<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 10 Jul 2025 18:32:01 +0000</lastBuildDate><item><title>This tool strips away anti-AI protections from digital art (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/07/10/1119937/tool-strips-away-anti-ai-protections-from-digital-art/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/scrape-art-1c_5925de.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;A new technique called LightShed will make it harder for artists to use existing protective tools to stop their work from being ingested for AI training. It’s the next step in a cat-and-mouse game—across technology, law, and culture—that has been going on between artists and AI proponents for years.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Generative AI models that create images need to be trained on a wide variety of visual material, and data sets that are used for this training allegedly include copyrighted art without permission. This has worried artists, who are concerned that the models will learn their style, mimic their work, and put them out of a job.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;div class="wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex"&gt; &lt;p&gt;These artists got some potential defenses in 2023, when researchers created tools like Glaze and Nightshade to protect artwork by “poisoning” it against AI training (Shawn Shan was even named &lt;em&gt;MIT Technology Review&lt;/em&gt;’s Innovator of the Year last year for his work on these). LightShed, however, claims to be able to subvert these tools and others like them, making it easy for the artwork to be used for training once again.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;div class="wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex"&gt; &lt;p&gt;To be clear, the researchers behind LightShed aren’t trying to steal artists’ work. They just don’t want people to get a false sense of security. “You will not be sure if companies have methods to delete these poisons but will never tell you,” says Hanna Foerster, a PhD student at the University of Cambridge and the lead author of a paper on the work. And if they do, it may be too late to fix the problem.&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;AI models work, in part, by implicitly creating boundaries between what they perceive as different categories of images. Glaze and Nightshade change enough pixels to push a given piece of art over this boundary without affecting the image’s quality, causing the model to see it as something it’s not. These almost imperceptible changes are called perturbations, and they mess up the AI model’s ability to understand the artwork.&lt;/p&gt;  &lt;div class="wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex"&gt; &lt;p&gt;Glaze makes models misunderstand style (e.g., interpreting a photorealistic painting as a cartoon). Nightshade instead makes the model see the subject incorrectly (e.g., interpreting a cat in a drawing as a dog). Glaze is used to defend an artist’s individual style, whereas Nightshade is used to attack AI models that crawl the internet for art.&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;Foerster worked with a team of researchers from the Technical University of Darmstadt and the University of Texas at San Antonio to develop LightShed, which learns how to see where tools like Glaze and Nightshade splash this sort of digital poison onto art so that it can effectively clean it off. The group will present its findings at the Usenix Security Symposium, a leading global cybersecurity conference, in August.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;The researchers trained LightShed by feeding it pieces of art with and without Nightshade, Glaze, and other similar programs applied. Foerster describes the process as teaching LightShed to reconstruct “just the poison on poisoned images.” Identifying a cutoff for how much poison will actually confuse an AI makes it easier to “wash” just the poison off.&amp;nbsp;&lt;/p&gt;  &lt;div class="wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex"&gt; &lt;p&gt;LightShed is incredibly effective at this. While other researchers have found simple ways to subvert poisoning, LightShed appears to be more adaptable. It can even apply what it’s learned from one anti-AI tool—say, Nightshade—to others like Mist or MetaCloak without ever seeing them ahead of time. While it has some trouble performing against small doses of poison, those are less likely to kill the AI models’ abilities to understand the underlying art, making it a win-win for the AI—or a lose-lose for the artists using these tools.&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;Around 7.5 million people, many of them artists with small and medium-size followings and fewer resources, have downloaded Glaze to protect their art. Those using tools like Glaze see it as an important technical line of defense, especially when the state of regulation around AI training and copyright is still up in the air. The LightShed authors see their work as a warning that tools like Glaze are not permanent solutions. “It might need a few more rounds of trying to come up with better ideas for protection,” says Foerster.&lt;/p&gt; 
 &lt;p&gt;The creators of Glaze and Nightshade seem to agree with that sentiment: The website for Nightshade warned the tool wasn’t future-proof before work on LightShed ever began. And Shan, who led research on both tools, still believes defenses like his have meaning even if there are ways around them.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;“It’s a deterrent,” says Shan—a way to warn AI companies that artists are serious about their concerns. The goal, as he puts it, is to put up as many roadblocks as possible so that AI companies find it easier to just work with artists. He believes that “most artists kind of understand this is a temporary solution,” but that creating those obstacles against the unwanted use of their work is still valuable.&lt;/p&gt;  &lt;p&gt;Foerster hopes to use what she learned through LightShed to build new defenses for artists, including clever watermarks that somehow persist with the artwork even after it’s gone through an AI model. While she doesn’t believe this will protect a work against AI forever, she thinks this could help tip the scales back in the artist’s favor once again.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/scrape-art-1c_5925de.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;A new technique called LightShed will make it harder for artists to use existing protective tools to stop their work from being ingested for AI training. It’s the next step in a cat-and-mouse game—across technology, law, and culture—that has been going on between artists and AI proponents for years.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Generative AI models that create images need to be trained on a wide variety of visual material, and data sets that are used for this training allegedly include copyrighted art without permission. This has worried artists, who are concerned that the models will learn their style, mimic their work, and put them out of a job.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;div class="wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex"&gt; &lt;p&gt;These artists got some potential defenses in 2023, when researchers created tools like Glaze and Nightshade to protect artwork by “poisoning” it against AI training (Shawn Shan was even named &lt;em&gt;MIT Technology Review&lt;/em&gt;’s Innovator of the Year last year for his work on these). LightShed, however, claims to be able to subvert these tools and others like them, making it easy for the artwork to be used for training once again.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;div class="wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex"&gt; &lt;p&gt;To be clear, the researchers behind LightShed aren’t trying to steal artists’ work. They just don’t want people to get a false sense of security. “You will not be sure if companies have methods to delete these poisons but will never tell you,” says Hanna Foerster, a PhD student at the University of Cambridge and the lead author of a paper on the work. And if they do, it may be too late to fix the problem.&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;AI models work, in part, by implicitly creating boundaries between what they perceive as different categories of images. Glaze and Nightshade change enough pixels to push a given piece of art over this boundary without affecting the image’s quality, causing the model to see it as something it’s not. These almost imperceptible changes are called perturbations, and they mess up the AI model’s ability to understand the artwork.&lt;/p&gt;  &lt;div class="wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex"&gt; &lt;p&gt;Glaze makes models misunderstand style (e.g., interpreting a photorealistic painting as a cartoon). Nightshade instead makes the model see the subject incorrectly (e.g., interpreting a cat in a drawing as a dog). Glaze is used to defend an artist’s individual style, whereas Nightshade is used to attack AI models that crawl the internet for art.&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;Foerster worked with a team of researchers from the Technical University of Darmstadt and the University of Texas at San Antonio to develop LightShed, which learns how to see where tools like Glaze and Nightshade splash this sort of digital poison onto art so that it can effectively clean it off. The group will present its findings at the Usenix Security Symposium, a leading global cybersecurity conference, in August.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;The researchers trained LightShed by feeding it pieces of art with and without Nightshade, Glaze, and other similar programs applied. Foerster describes the process as teaching LightShed to reconstruct “just the poison on poisoned images.” Identifying a cutoff for how much poison will actually confuse an AI makes it easier to “wash” just the poison off.&amp;nbsp;&lt;/p&gt;  &lt;div class="wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex"&gt; &lt;p&gt;LightShed is incredibly effective at this. While other researchers have found simple ways to subvert poisoning, LightShed appears to be more adaptable. It can even apply what it’s learned from one anti-AI tool—say, Nightshade—to others like Mist or MetaCloak without ever seeing them ahead of time. While it has some trouble performing against small doses of poison, those are less likely to kill the AI models’ abilities to understand the underlying art, making it a win-win for the AI—or a lose-lose for the artists using these tools.&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;Around 7.5 million people, many of them artists with small and medium-size followings and fewer resources, have downloaded Glaze to protect their art. Those using tools like Glaze see it as an important technical line of defense, especially when the state of regulation around AI training and copyright is still up in the air. The LightShed authors see their work as a warning that tools like Glaze are not permanent solutions. “It might need a few more rounds of trying to come up with better ideas for protection,” says Foerster.&lt;/p&gt; 
 &lt;p&gt;The creators of Glaze and Nightshade seem to agree with that sentiment: The website for Nightshade warned the tool wasn’t future-proof before work on LightShed ever began. And Shan, who led research on both tools, still believes defenses like his have meaning even if there are ways around them.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;“It’s a deterrent,” says Shan—a way to warn AI companies that artists are serious about their concerns. The goal, as he puts it, is to put up as many roadblocks as possible so that AI companies find it easier to just work with artists. He believes that “most artists kind of understand this is a temporary solution,” but that creating those obstacles against the unwanted use of their work is still valuable.&lt;/p&gt;  &lt;p&gt;Foerster hopes to use what she learned through LightShed to build new defenses for artists, including clever watermarks that somehow persist with the artwork even after it’s gone through an AI model. While she doesn’t believe this will protect a work against AI forever, she thinks this could help tip the scales back in the artist’s favor once again.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/07/10/1119937/tool-strips-away-anti-ai-protections-from-digital-art/</guid><pubDate>Thu, 10 Jul 2025 09:00:00 +0000</pubDate></item><item><title>China’s energy dominance in three charts (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/07/10/1119941/china-energy-dominance-three-charts/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/GettyImages-1830281211.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;China is &lt;em&gt;the&lt;/em&gt; dominant force in next-generation energy technologies today. It’s pouring hundreds of billions of dollars into putting renewable sources like wind and solar on its grid, manufacturing millions of electric vehicles, and building out capacity for energy storage, nuclear power, and more. This investment has been transformational for the country’s economy and has contributed to establishing China as a major player in global politics.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Meanwhile, in the US, a massive new tax and spending bill just cut hundreds of billions in credits, grants, and loans for clean energy technologies. It’s a stark reversal from previous policies, and it could have massive effects at a time when it feels as if everyone is chasing China on energy.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;So while we all try to get our heads around what’s next for climate tech in the US and beyond, let’s look at just how dominant China is when it comes to clean energy, as documented in three charts.&lt;/p&gt;    &lt;p&gt;China is on an absolute tear installing wind and solar power. The country reached nearly 900 gigawatts of installed capacity for solar at the end of 2024, and the rapid pace of building has continued into this year. An additional 198 GW was installed between January and May, with 93 GW coming in May alone.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;For context, those additions over the first five months of the year account for more than double the capacity of the grid in California. Not the renewables capacity of that state—&lt;em&gt;the entire grid.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;Meanwhile, the policy shift in the US is projected to slow down new solar and wind additions. With tax credits and other support stripped away, much of the new capacity that was expected to come online by the end of the decade will now face delays or cancellations.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;That’s significant because of all the new electricity generation capacity that’s come online in the US recently, renewables make up the vast majority. Solar and battery storage alone are expected to make up over 80% of capacity additions in 2025. So slowing down wind and solar basically means slowing down adding new electricity capacity, at a time when demand is very much set to rise. (Hello, AI?)&lt;/p&gt;    &lt;p&gt;China’s EV market is also booming—the country is currently flirting with a big symbolic milestone, nearing the point where over half of all new vehicles sold in the country are electric. (It already passed that mark for a single month and could do so on a yearly basis in the next couple of years.)&lt;/p&gt;  &lt;p&gt;It’s not just selling those vehicles within China, either: the country exports them globally, with customers including established markets like Europe and growing ones like India and Brazil. As of 2024, more than 70% of electric and plug-in hybrid vehicles on roads around the world were built in ChinaSome leaders in legacy automakers are taking notice. Ford CEO Jim Farley shared some striking comments at the Aspen Ideas Festival last month about how far ahead China is on vehicle technology and price. “They have far superior in-vehicle technology,” Farley said. “We are in a global competition with China, and it's not just EVs. And if we lose this, we do not have a future Ford.”&amp;nbsp;&lt;/p&gt;    &lt;p&gt;Looking ahead, China is still pouring money into renewables, storage, grids, and energy efficiency technologies. It’s also outspending the rest of the world on nuclear power. The country tripled its investment in renewable power from 2015 to 2025.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;The situation isn’t set in stone, though: The US actually very briefly overtook China on battery investments over the past year, as Cat Clifford at &lt;em&gt;Cipher&lt;/em&gt; reported last week. But changes resulting from the new bill could very quickly reverse that progress, cementing China as &lt;em&gt;the &lt;/em&gt;place for battery manufacturing and innovation.&lt;/p&gt;  &lt;p&gt;In a story earlier this week, the MIT economist David Autor laid out the high stakes for this race. Advanced manufacturing and technology are beneficial for US prosperity, and putting public support and trade protections in place for key industries could be crucial to keeping them going, he says.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I’d add that this whole discussion shouldn’t be about a zero-sum competition between the US and China. But many experts argue that the US, where I and many readers live, is surrendering its leadership and ability to develop key energy technologies of the future.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Ultimately, the numbers don’t lie: By a lot of measures, China is the world’s leader in energy. The question is, will that change anytime soon?&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark, &lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday,&lt;/em&gt; &lt;em&gt;sign up here&lt;/em&gt;.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/GettyImages-1830281211.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;China is &lt;em&gt;the&lt;/em&gt; dominant force in next-generation energy technologies today. It’s pouring hundreds of billions of dollars into putting renewable sources like wind and solar on its grid, manufacturing millions of electric vehicles, and building out capacity for energy storage, nuclear power, and more. This investment has been transformational for the country’s economy and has contributed to establishing China as a major player in global politics.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Meanwhile, in the US, a massive new tax and spending bill just cut hundreds of billions in credits, grants, and loans for clean energy technologies. It’s a stark reversal from previous policies, and it could have massive effects at a time when it feels as if everyone is chasing China on energy.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;So while we all try to get our heads around what’s next for climate tech in the US and beyond, let’s look at just how dominant China is when it comes to clean energy, as documented in three charts.&lt;/p&gt;    &lt;p&gt;China is on an absolute tear installing wind and solar power. The country reached nearly 900 gigawatts of installed capacity for solar at the end of 2024, and the rapid pace of building has continued into this year. An additional 198 GW was installed between January and May, with 93 GW coming in May alone.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;For context, those additions over the first five months of the year account for more than double the capacity of the grid in California. Not the renewables capacity of that state—&lt;em&gt;the entire grid.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;Meanwhile, the policy shift in the US is projected to slow down new solar and wind additions. With tax credits and other support stripped away, much of the new capacity that was expected to come online by the end of the decade will now face delays or cancellations.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;That’s significant because of all the new electricity generation capacity that’s come online in the US recently, renewables make up the vast majority. Solar and battery storage alone are expected to make up over 80% of capacity additions in 2025. So slowing down wind and solar basically means slowing down adding new electricity capacity, at a time when demand is very much set to rise. (Hello, AI?)&lt;/p&gt;    &lt;p&gt;China’s EV market is also booming—the country is currently flirting with a big symbolic milestone, nearing the point where over half of all new vehicles sold in the country are electric. (It already passed that mark for a single month and could do so on a yearly basis in the next couple of years.)&lt;/p&gt;  &lt;p&gt;It’s not just selling those vehicles within China, either: the country exports them globally, with customers including established markets like Europe and growing ones like India and Brazil. As of 2024, more than 70% of electric and plug-in hybrid vehicles on roads around the world were built in ChinaSome leaders in legacy automakers are taking notice. Ford CEO Jim Farley shared some striking comments at the Aspen Ideas Festival last month about how far ahead China is on vehicle technology and price. “They have far superior in-vehicle technology,” Farley said. “We are in a global competition with China, and it's not just EVs. And if we lose this, we do not have a future Ford.”&amp;nbsp;&lt;/p&gt;    &lt;p&gt;Looking ahead, China is still pouring money into renewables, storage, grids, and energy efficiency technologies. It’s also outspending the rest of the world on nuclear power. The country tripled its investment in renewable power from 2015 to 2025.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;The situation isn’t set in stone, though: The US actually very briefly overtook China on battery investments over the past year, as Cat Clifford at &lt;em&gt;Cipher&lt;/em&gt; reported last week. But changes resulting from the new bill could very quickly reverse that progress, cementing China as &lt;em&gt;the &lt;/em&gt;place for battery manufacturing and innovation.&lt;/p&gt;  &lt;p&gt;In a story earlier this week, the MIT economist David Autor laid out the high stakes for this race. Advanced manufacturing and technology are beneficial for US prosperity, and putting public support and trade protections in place for key industries could be crucial to keeping them going, he says.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I’d add that this whole discussion shouldn’t be about a zero-sum competition between the US and China. But many experts argue that the US, where I and many readers live, is surrendering its leadership and ability to develop key energy technologies of the future.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Ultimately, the numbers don’t lie: By a lot of measures, China is the world’s leader in energy. The question is, will that change anytime soon?&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark, &lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday,&lt;/em&gt; &lt;em&gt;sign up here&lt;/em&gt;.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/07/10/1119941/china-energy-dominance-three-charts/</guid><pubDate>Thu, 10 Jul 2025 10:00:00 +0000</pubDate></item><item><title>The Download: flaws in anti-AI protections for art, and an AI regulation vibe shift (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/07/10/1119961/the-download-anti-ai-art-regulation-vibe-shift/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;  &lt;h3 class="wp-block-heading has-medium-font-size"&gt;&lt;strong&gt;This tool strips away anti-AI protections from digital art&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;&lt;strong&gt;The news: &lt;/strong&gt;A new technique called LightShed will make it harder for artists to use existing protective tools to stop their work from being ingested for AI training. It’s the next step in a cat-and-mouse game—across technology, law, and culture—that has been going on between artists and AI proponents for years.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; Protective tools like Glaze and Nightshade change enough pixels to affect an image, so if it’s scraped up by AI models, they see it as something it’s not. LightShed essentially works by spotting just the “poison” on poisoned images. To be clear, the researchers behind it aren’t trying to steal artists’ work. They just don’t want people to get a false sense of security. &amp;nbsp;Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Peter Hall&lt;/em&gt;&lt;/p&gt; 
   &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Why the AI moratorium’s defeat may signal a new political era&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The “Big, Beautiful Bill” that President Donald Trump signed into law on July 4 was chock full of controversial policies. But one highly contested provision was missing. Just days earlier, during a late-night voting session, the Senate had killed the bill’s 10-year moratorium on state-level AI regulation.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;The bipartisan vote was seen as a victory by many, and may signal a bigger political shift, with a broader and more diverse coalition in favor of AI regulation starting to form. After years of relative inaction, politicians are getting concerned about the risks of unregulated artificial intelligence. Read the full story.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Grace Huckins&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;China’s energy dominance in three charts&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;China is &lt;em&gt;the&lt;/em&gt; dominant force in next-generation energy technologies today. It’s pouring hundreds of billions of dollars into putting renewable sources like wind and solar, manufacturing millions of electric vehicles, and building out capacity for energy storage, nuclear power, and more. This investment has been transformational for the country’s economy and has contributed to establishing China as a major player in global politics.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;So while we all try to get our heads around what’s next for climate tech in the US and beyond, let’s look at just how dominant China is when it comes to clean energy, as documented in three charts.&lt;strong&gt; &lt;/strong&gt;Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Casey Crownhart&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This article is from The Spark, MIT Technology Review’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; 

   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Linda Yaccarino is stepping down as CEO of X&lt;/strong&gt;&lt;br /&gt;She managed to last almost exactly two years reporting to owner Elon Musk.&amp;nbsp; (Axios)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;She was planning to leave before Grok’s anti-Semitic rants, apparently.&amp;nbsp;&lt;/em&gt;(NYT&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Turkey has banned Grok after it insulted President Erdoğan&lt;/em&gt;. (Politico)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 OpenAI is planning to release its own web browser&lt;/strong&gt;&lt;br /&gt;If it works out, it’ll give it the same advantage as Google: direct ownership over users’ data. (Reuters&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;AI means the end of internet search as we’ve known it.&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3 McDonald’s hiring chatbot exposed millions of applicants’ data to hackers&lt;/strong&gt;&lt;br /&gt;Adding the insult of carelessness to an already pretty dystopian process! (Wired&amp;nbsp;$)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;4 AI-generated images of child sexual abuse are proliferating online&lt;/strong&gt;&lt;br /&gt;This is going to make an already very hard job for law enforcement even harder. (NYT&amp;nbsp;$)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;5 Autonomous fighter jets are on the horizon&lt;/strong&gt;&lt;br /&gt;European defense start-up Helsing just completed two successful test flights. (FT&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Generative AI is learning to spy for the US military&lt;/em&gt;. (MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;6 What happened to all the human bird flu cases?&lt;/strong&gt;&lt;br /&gt;Since February, the CDC has not recorded a single new case in the US. (Undark)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;7 An interstellar object is cruising through the solar system&lt;/strong&gt;&lt;br /&gt;And it’s giving astronomers a chance to test out early theories of interstellar-object-ology (yes, that’s what it’s called!) (The Economist&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Inside the most dangerous asteroid hunt ever.&amp;nbsp;&lt;/em&gt;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;8 Apple is planning its first upgrade to its Vision Pro headset&lt;/strong&gt;&lt;br /&gt;But no matter what upgrades it’s got, it’s going to be a real struggle to revive its flagging fortunes. (Bloomberg&amp;nbsp;$)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;9 Where have all the mundane social media posts gone?&lt;/strong&gt;&lt;br /&gt;Normies used to be what made social media good. We miss them and their photos of their breakfasts. (New Yorker&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;It’s heartening to see that ‘missed connection’ posts are making a comeback, though&lt;/em&gt;. (The Guardian)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10 A global shortage is turning MatchaTok sour&lt;/strong&gt;&lt;br /&gt;But it’s pretty easy to explain why it’s in short supply: the whole world’s started going mad for it. (WSJ&amp;nbsp;$)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;&amp;nbsp;“You’ll be hard pressed to find someone that really believes in our AI mission. To most, it’s not even clear what our mission is.”&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;—Tijmen Blankevoort, an AI researcher at Meta, explains why he thinks expensive hires alone might not cure the company’s woes, The Information reports.&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1096407" src="https://wp.technologyreview.com/wp-content/uploads/2024/08/SO24-feature_archives_Opener_thumb.jpg?w=2342" /&gt;&lt;div class="image-credit"&gt;MIKE MCQUADE&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;&lt;strong&gt;The race to save our online lives from a digital dark age&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;There is a photo of my daughter that I love. She is sitting, smiling, in our old back garden, chubby hands grabbing at the cool grass. It was taken on a digital camera in 2013, when she was almost one, but now lives on Google Photos.&lt;/p&gt;&lt;p&gt;But what if, one day, Google ceased to function? What if I lost my treasured photos forever? For many archivists, alarm bells are ringing. Across the world, they are scraping up defunct websites or at-risk data collections to save as much of our digital lives as possible. Others are working on ways to store that data in formats that will last hundreds, perhaps even thousands, of years.&lt;/p&gt;&lt;p&gt;The endeavor raises complex questions. What is important to us? How and why do we decide what to keep—and what do we let go? And how will future generations make sense of what we’re able to save? Read the full story.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;  &lt;h3 class="wp-block-heading has-medium-font-size"&gt;&lt;strong&gt;This tool strips away anti-AI protections from digital art&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;&lt;strong&gt;The news: &lt;/strong&gt;A new technique called LightShed will make it harder for artists to use existing protective tools to stop their work from being ingested for AI training. It’s the next step in a cat-and-mouse game—across technology, law, and culture—that has been going on between artists and AI proponents for years.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; Protective tools like Glaze and Nightshade change enough pixels to affect an image, so if it’s scraped up by AI models, they see it as something it’s not. LightShed essentially works by spotting just the “poison” on poisoned images. To be clear, the researchers behind it aren’t trying to steal artists’ work. They just don’t want people to get a false sense of security. &amp;nbsp;Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Peter Hall&lt;/em&gt;&lt;/p&gt; 
   &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Why the AI moratorium’s defeat may signal a new political era&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The “Big, Beautiful Bill” that President Donald Trump signed into law on July 4 was chock full of controversial policies. But one highly contested provision was missing. Just days earlier, during a late-night voting session, the Senate had killed the bill’s 10-year moratorium on state-level AI regulation.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;The bipartisan vote was seen as a victory by many, and may signal a bigger political shift, with a broader and more diverse coalition in favor of AI regulation starting to form. After years of relative inaction, politicians are getting concerned about the risks of unregulated artificial intelligence. Read the full story.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Grace Huckins&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;China’s energy dominance in three charts&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;China is &lt;em&gt;the&lt;/em&gt; dominant force in next-generation energy technologies today. It’s pouring hundreds of billions of dollars into putting renewable sources like wind and solar, manufacturing millions of electric vehicles, and building out capacity for energy storage, nuclear power, and more. This investment has been transformational for the country’s economy and has contributed to establishing China as a major player in global politics.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;So while we all try to get our heads around what’s next for climate tech in the US and beyond, let’s look at just how dominant China is when it comes to clean energy, as documented in three charts.&lt;strong&gt; &lt;/strong&gt;Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Casey Crownhart&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This article is from The Spark, MIT Technology Review’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; 

   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Linda Yaccarino is stepping down as CEO of X&lt;/strong&gt;&lt;br /&gt;She managed to last almost exactly two years reporting to owner Elon Musk.&amp;nbsp; (Axios)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;She was planning to leave before Grok’s anti-Semitic rants, apparently.&amp;nbsp;&lt;/em&gt;(NYT&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Turkey has banned Grok after it insulted President Erdoğan&lt;/em&gt;. (Politico)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 OpenAI is planning to release its own web browser&lt;/strong&gt;&lt;br /&gt;If it works out, it’ll give it the same advantage as Google: direct ownership over users’ data. (Reuters&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;AI means the end of internet search as we’ve known it.&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3 McDonald’s hiring chatbot exposed millions of applicants’ data to hackers&lt;/strong&gt;&lt;br /&gt;Adding the insult of carelessness to an already pretty dystopian process! (Wired&amp;nbsp;$)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;4 AI-generated images of child sexual abuse are proliferating online&lt;/strong&gt;&lt;br /&gt;This is going to make an already very hard job for law enforcement even harder. (NYT&amp;nbsp;$)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;5 Autonomous fighter jets are on the horizon&lt;/strong&gt;&lt;br /&gt;European defense start-up Helsing just completed two successful test flights. (FT&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Generative AI is learning to spy for the US military&lt;/em&gt;. (MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;6 What happened to all the human bird flu cases?&lt;/strong&gt;&lt;br /&gt;Since February, the CDC has not recorded a single new case in the US. (Undark)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;7 An interstellar object is cruising through the solar system&lt;/strong&gt;&lt;br /&gt;And it’s giving astronomers a chance to test out early theories of interstellar-object-ology (yes, that’s what it’s called!) (The Economist&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Inside the most dangerous asteroid hunt ever.&amp;nbsp;&lt;/em&gt;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;8 Apple is planning its first upgrade to its Vision Pro headset&lt;/strong&gt;&lt;br /&gt;But no matter what upgrades it’s got, it’s going to be a real struggle to revive its flagging fortunes. (Bloomberg&amp;nbsp;$)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;9 Where have all the mundane social media posts gone?&lt;/strong&gt;&lt;br /&gt;Normies used to be what made social media good. We miss them and their photos of their breakfasts. (New Yorker&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;It’s heartening to see that ‘missed connection’ posts are making a comeback, though&lt;/em&gt;. (The Guardian)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10 A global shortage is turning MatchaTok sour&lt;/strong&gt;&lt;br /&gt;But it’s pretty easy to explain why it’s in short supply: the whole world’s started going mad for it. (WSJ&amp;nbsp;$)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;&amp;nbsp;“You’ll be hard pressed to find someone that really believes in our AI mission. To most, it’s not even clear what our mission is.”&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;—Tijmen Blankevoort, an AI researcher at Meta, explains why he thinks expensive hires alone might not cure the company’s woes, The Information reports.&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1096407" src="https://wp.technologyreview.com/wp-content/uploads/2024/08/SO24-feature_archives_Opener_thumb.jpg?w=2342" /&gt;&lt;div class="image-credit"&gt;MIKE MCQUADE&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;&lt;strong&gt;The race to save our online lives from a digital dark age&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;There is a photo of my daughter that I love. She is sitting, smiling, in our old back garden, chubby hands grabbing at the cool grass. It was taken on a digital camera in 2013, when she was almost one, but now lives on Google Photos.&lt;/p&gt;&lt;p&gt;But what if, one day, Google ceased to function? What if I lost my treasured photos forever? For many archivists, alarm bells are ringing. Across the world, they are scraping up defunct websites or at-risk data collections to save as much of our digital lives as possible. Others are working on ways to store that data in formats that will last hundreds, perhaps even thousands, of years.&lt;/p&gt;&lt;p&gt;The endeavor raises complex questions. What is important to us? How and why do we decide what to keep—and what do we let go? And how will future generations make sense of what we’re able to save? Read the full story.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/07/10/1119961/the-download-anti-ai-art-regulation-vibe-shift/</guid><pubDate>Thu, 10 Jul 2025 12:10:00 +0000</pubDate></item><item><title>[NEW] Employee AI agent adoption: Maximizing gains while navigating challenges (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/employee-ai-agent-adoption-maximizing-gains-while-navigating-challenges/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://venturebeat.com/wp-content/uploads/2025/07/VBTRANSFORM25-0136-X3.jpg?w=1024?w=1200&amp;amp;strip=all" /&gt;&lt;/div&gt;&lt;p&gt;While agentic AI definitely marks a turning point in human-computer interaction, moving from tool use to collaboration, the next step is integrating these agents and actually deriving value. At VentureBeat’s Transform 2025, Matthew Kropp, managing director and senior partner at BCG, offered a game plan for workflow evolution, employee adoption, and organizational change.&lt;/p&gt;



&lt;p&gt;“The companies that are at the top of this curve — what we call future built, the ones that are most mature — are seeing substantial results: 1.5 times more revenue growth, 1.8 times higher shareholder value,” Kropp said. “There’s value here, but we’re early.”&lt;/p&gt;



&lt;figure class="wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="h-deploy-reshape-invent"&gt;&lt;strong&gt;Deploy, reshape, invent&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;To take advantage and create value with AI and with agents, a company needs to determine where to focus, using a deploy, reshape, invent framework. AI is already being deployed in every enterprise, and will have agents within the next few years. But if you give an employee a chatbot, you haven’t changed the way the work is done. You have to rethink the work, and reshape functions, departments, and workflows by identifying where human work can be automated.&lt;/p&gt;



&lt;p&gt;“We’re advising companies right now to focus on your three or four big rocks. If you have a big customer support organization, you should apply AI in customer support. It has a huge impact. If you have a big engineering organization, you should employ tools like Windsurf to reshape the way that you do engineering, software development.”&lt;/p&gt;



&lt;p&gt;Invention is still in the very early stage, but enterprises should be thinking about how to use AI’s ability to be creative, reason, and plan. Look at services and products, and how you interact with customers: can you reinvent that using those capabilities?”&lt;/p&gt;



&lt;p&gt;For instance, makeup company L’Oreal launched a virtual beauty advisor to scale that exclusive service beyond their retail locations, reinventing the way they think about interacting with their customers at scale.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-thinking-beyond-basic-use-cases"&gt;&lt;strong&gt;Thinking beyond basic use cases&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;It’s also critical to think about how AI changes your business. There’s been a lot of focus in the last couple of years on cost reduction by replacing workers, but that isn’t big-picture thinking. AI amplifies the employees you currently have, dramatically increasing their productivity.&lt;/p&gt;



&lt;p&gt;“This is what we’re seeing in software development,” he said. “I don’t think we’ll see companies laying off their software developers. We’re going to see a massive explosion in the amount of capability and features that software companies are building.”&lt;/p&gt;



&lt;p&gt;In a study BCG conducted with Harvard, Wharton, and MIT, they asked 750 knowledge workers to write a business and marketing plan, with and without generative AI. The participants using GPT4 executed 25% faster, completed 15% more tasks, and the quality of their output was 40% better. And when given an LLM, the bottom performers in the baseline did just as well as the top performers.&lt;/p&gt;



&lt;p&gt;“It brought everyone’s performance up, which is very powerful, because in most organizations the new joiners are less effective than more experienced people,” he said. “It has the ability to increase time to proficiency.”&lt;/p&gt;



&lt;p&gt;AI can also surpass human scale, even open up new applications that were not previously possible. For example, in the medical space, outcomes for patients are significantly improved with preoperative and postoperative follow-up from a nurse, but implementing this has been cost-prohibitive — until the advent of AI nurses that can take on that task for a large patient population.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-overcoming-the-biggest-hurdle-adoption"&gt;&lt;strong&gt;Overcoming the biggest hurdle: Adoption&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;While these tools are fantastic, people aren’t using them. BCG tracked the adoption of GitHub Copilot and productivity metrics for an organization with about 10,000 software engineers. The top 5% engineers doubled in productivity in four months, while 60% showed zero improvement, because they just didn’t adopt the tool at all.&lt;/p&gt;



&lt;p&gt;Why won’t humans adopt? There are three reasons. First is capability ignorance. The second, habit inertia. The third is identity threat, and that is the hardest to overcome. Developers are asking, “If this AI can write code for me then who am I? What’s my value?”&lt;/p&gt;



&lt;p&gt;“This is going to be the real work of the next three to five years,” Kropp said. “It’s getting people to use the agents.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-strategies-overcoming-reluctance"&gt;&lt;strong&gt;Strategies overcoming reluctance&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;There are a few valuable ways to overcome these challenges. Naturally, getting the right tool is the first step, and integrating it with the way people work by training them explicitly. It’s also critical to measure and celebrate adoption for those employees actively using the tools so that everyone else starts to see they need to get on this bandwagon.&lt;/p&gt;



&lt;p&gt;Another important step is ramping up scarcity — that means taking away resources so employees need to do more with less. At the same time, it’s essential to redesign work processes hand-in-hand with those employees who are on the front lines. Don’t just identify laborious processes where manual work can be automated — identify the parts where humans bring value.&lt;/p&gt;



&lt;p&gt;“We minimize the toil and we maximize the joy,” Kropp said. “We’re left with a much more efficient process, a much more efficient company, a much more productive workforce, and jobs that people like to be in.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://venturebeat.com/wp-content/uploads/2025/07/VBTRANSFORM25-0136-X3.jpg?w=1024?w=1200&amp;amp;strip=all" /&gt;&lt;/div&gt;&lt;p&gt;While agentic AI definitely marks a turning point in human-computer interaction, moving from tool use to collaboration, the next step is integrating these agents and actually deriving value. At VentureBeat’s Transform 2025, Matthew Kropp, managing director and senior partner at BCG, offered a game plan for workflow evolution, employee adoption, and organizational change.&lt;/p&gt;



&lt;p&gt;“The companies that are at the top of this curve — what we call future built, the ones that are most mature — are seeing substantial results: 1.5 times more revenue growth, 1.8 times higher shareholder value,” Kropp said. “There’s value here, but we’re early.”&lt;/p&gt;



&lt;figure class="wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="h-deploy-reshape-invent"&gt;&lt;strong&gt;Deploy, reshape, invent&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;To take advantage and create value with AI and with agents, a company needs to determine where to focus, using a deploy, reshape, invent framework. AI is already being deployed in every enterprise, and will have agents within the next few years. But if you give an employee a chatbot, you haven’t changed the way the work is done. You have to rethink the work, and reshape functions, departments, and workflows by identifying where human work can be automated.&lt;/p&gt;



&lt;p&gt;“We’re advising companies right now to focus on your three or four big rocks. If you have a big customer support organization, you should apply AI in customer support. It has a huge impact. If you have a big engineering organization, you should employ tools like Windsurf to reshape the way that you do engineering, software development.”&lt;/p&gt;



&lt;p&gt;Invention is still in the very early stage, but enterprises should be thinking about how to use AI’s ability to be creative, reason, and plan. Look at services and products, and how you interact with customers: can you reinvent that using those capabilities?”&lt;/p&gt;



&lt;p&gt;For instance, makeup company L’Oreal launched a virtual beauty advisor to scale that exclusive service beyond their retail locations, reinventing the way they think about interacting with their customers at scale.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-thinking-beyond-basic-use-cases"&gt;&lt;strong&gt;Thinking beyond basic use cases&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;It’s also critical to think about how AI changes your business. There’s been a lot of focus in the last couple of years on cost reduction by replacing workers, but that isn’t big-picture thinking. AI amplifies the employees you currently have, dramatically increasing their productivity.&lt;/p&gt;



&lt;p&gt;“This is what we’re seeing in software development,” he said. “I don’t think we’ll see companies laying off their software developers. We’re going to see a massive explosion in the amount of capability and features that software companies are building.”&lt;/p&gt;



&lt;p&gt;In a study BCG conducted with Harvard, Wharton, and MIT, they asked 750 knowledge workers to write a business and marketing plan, with and without generative AI. The participants using GPT4 executed 25% faster, completed 15% more tasks, and the quality of their output was 40% better. And when given an LLM, the bottom performers in the baseline did just as well as the top performers.&lt;/p&gt;



&lt;p&gt;“It brought everyone’s performance up, which is very powerful, because in most organizations the new joiners are less effective than more experienced people,” he said. “It has the ability to increase time to proficiency.”&lt;/p&gt;



&lt;p&gt;AI can also surpass human scale, even open up new applications that were not previously possible. For example, in the medical space, outcomes for patients are significantly improved with preoperative and postoperative follow-up from a nurse, but implementing this has been cost-prohibitive — until the advent of AI nurses that can take on that task for a large patient population.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-overcoming-the-biggest-hurdle-adoption"&gt;&lt;strong&gt;Overcoming the biggest hurdle: Adoption&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;While these tools are fantastic, people aren’t using them. BCG tracked the adoption of GitHub Copilot and productivity metrics for an organization with about 10,000 software engineers. The top 5% engineers doubled in productivity in four months, while 60% showed zero improvement, because they just didn’t adopt the tool at all.&lt;/p&gt;



&lt;p&gt;Why won’t humans adopt? There are three reasons. First is capability ignorance. The second, habit inertia. The third is identity threat, and that is the hardest to overcome. Developers are asking, “If this AI can write code for me then who am I? What’s my value?”&lt;/p&gt;



&lt;p&gt;“This is going to be the real work of the next three to five years,” Kropp said. “It’s getting people to use the agents.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-strategies-overcoming-reluctance"&gt;&lt;strong&gt;Strategies overcoming reluctance&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;There are a few valuable ways to overcome these challenges. Naturally, getting the right tool is the first step, and integrating it with the way people work by training them explicitly. It’s also critical to measure and celebrate adoption for those employees actively using the tools so that everyone else starts to see they need to get on this bandwagon.&lt;/p&gt;



&lt;p&gt;Another important step is ramping up scarcity — that means taking away resources so employees need to do more with less. At the same time, it’s essential to redesign work processes hand-in-hand with those employees who are on the front lines. Don’t just identify laborious processes where manual work can be automated — identify the parts where humans bring value.&lt;/p&gt;



&lt;p&gt;“We minimize the toil and we maximize the joy,” Kropp said. “We’re left with a much more efficient process, a much more efficient company, a much more productive workforce, and jobs that people like to be in.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/employee-ai-agent-adoption-maximizing-gains-while-navigating-challenges/</guid><pubDate>Thu, 10 Jul 2025 12:50:00 +0000</pubDate></item><item><title>[NEW] Google announces latest AI American Infrastructure Academy cohort (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/10/google-announces-latest-ai-american-infrastructure-acadmey-cohort/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2196352264.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google on Thursday announced the second cohort to take part in its AI Academy: American Infrastructure, which seeks to support companies using AI to address issues such as cybersecurity, education, and transportation.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The four-month program is designed for companies at a seed to Series A stage and provides equity-free support and resources like leadership coaching and sales training. It’s primarily virtual, but founders will convene for an in-person summit at Google. Applications opened in late April of this year and closed mid-May; companies selected had to pass competitive criteria, including having at least six months of runway and having proof of traction.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Google has a pretty good track record so far of identifying notable AI startups. Alumni from American Infrastructure’s first cohort last year include the government contractor company Cloverleaf AI, which went on to raise a $2.8 million seed round, and Zordi, an autonomous agtech that had already raised $20 million from Khosla Ventures.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And it partners with some of the most significant AI companies that use its cloud.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Here are the companies selected for this latest batch:&amp;nbsp;&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Attuned Intelligence — AI-powered voice agents for call centers.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Block Harbor — cybersecurity for vehicle systems.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;CircNova — uses AI to analyze RNA for therapeutics.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;CloudRig — provides AI technology to help contractors manage schedules, production, and work plans.&amp;nbsp;&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Making Space — connects employers with disabled talent and prospective employees.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;MedHaul — connects healthcare organizations, like hospitals and clinics, to non-emergency medical transportation to book rides for patients with mobility needs.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Mpathic — automates clinical workflows and provides AI oversight to clinical trials.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Nimblemind.ai — helps organize health data.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Omnia Fishing — offers personalized fishing suggestions, such as where to fish and what to bring with you.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Otrafy — automates the process of supply management.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Partsimony — helps companies build and manage supply chains.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Satlyt — a computing platform to process satellite data.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;StudyFetch — offers personalized learning experiences for students, educators, and institutions.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Tansy AI — lets users manage their health, such as tracking appointments and records.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Tradeverifyd — helps businesses track global supply chain risk.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Vetr Health — offers at-home veterinary care.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Waterplan — lets businesses track water risk.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;This is just one of a number of programs where Google invests in AI startups and research. TechCrunch reported a few months ago that it launched its inaugural AI Futures Fund initiative to back startups building with the latest AI tools from DeepMind.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, Google’s charitable wing announced a $20 million commitment to researchers and scientists in AI and an AI accelerator program to give $20 million to nonprofits developing AI technology. Sundar Pichai also said the company would create a $120 million Global AI Opportunity Fund to help make AI education more accessible to people throughout the world.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;July 15&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Aside from this, Google has a few other notable Academies seeking to help founders, including its Founders Academy and Growth Academy. A Google spokesperson told us earlier this year that its Google for Startups Founders Fund would also look to start backing AI-focused startups as of this year.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2196352264.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google on Thursday announced the second cohort to take part in its AI Academy: American Infrastructure, which seeks to support companies using AI to address issues such as cybersecurity, education, and transportation.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The four-month program is designed for companies at a seed to Series A stage and provides equity-free support and resources like leadership coaching and sales training. It’s primarily virtual, but founders will convene for an in-person summit at Google. Applications opened in late April of this year and closed mid-May; companies selected had to pass competitive criteria, including having at least six months of runway and having proof of traction.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Google has a pretty good track record so far of identifying notable AI startups. Alumni from American Infrastructure’s first cohort last year include the government contractor company Cloverleaf AI, which went on to raise a $2.8 million seed round, and Zordi, an autonomous agtech that had already raised $20 million from Khosla Ventures.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And it partners with some of the most significant AI companies that use its cloud.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Here are the companies selected for this latest batch:&amp;nbsp;&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Attuned Intelligence — AI-powered voice agents for call centers.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Block Harbor — cybersecurity for vehicle systems.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;CircNova — uses AI to analyze RNA for therapeutics.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;CloudRig — provides AI technology to help contractors manage schedules, production, and work plans.&amp;nbsp;&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Making Space — connects employers with disabled talent and prospective employees.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;MedHaul — connects healthcare organizations, like hospitals and clinics, to non-emergency medical transportation to book rides for patients with mobility needs.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Mpathic — automates clinical workflows and provides AI oversight to clinical trials.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Nimblemind.ai — helps organize health data.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Omnia Fishing — offers personalized fishing suggestions, such as where to fish and what to bring with you.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Otrafy — automates the process of supply management.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Partsimony — helps companies build and manage supply chains.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Satlyt — a computing platform to process satellite data.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;StudyFetch — offers personalized learning experiences for students, educators, and institutions.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Tansy AI — lets users manage their health, such as tracking appointments and records.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Tradeverifyd — helps businesses track global supply chain risk.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Vetr Health — offers at-home veterinary care.&amp;nbsp;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Waterplan — lets businesses track water risk.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;This is just one of a number of programs where Google invests in AI startups and research. TechCrunch reported a few months ago that it launched its inaugural AI Futures Fund initiative to back startups building with the latest AI tools from DeepMind.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, Google’s charitable wing announced a $20 million commitment to researchers and scientists in AI and an AI accelerator program to give $20 million to nonprofits developing AI technology. Sundar Pichai also said the company would create a $120 million Global AI Opportunity Fund to help make AI education more accessible to people throughout the world.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;July 15&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Aside from this, Google has a few other notable Academies seeking to help founders, including its Founders Academy and Growth Academy. A Google spokesperson told us earlier this year that its Google for Startups Founders Fund would also look to start backing AI-focused startups as of this year.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/10/google-announces-latest-ai-american-infrastructure-acadmey-cohort/</guid><pubDate>Thu, 10 Jul 2025 13:00:00 +0000</pubDate></item><item><title>[NEW] Diligent Robotics hires two notable Cruise alumni to its leadership team (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/10/diligent-robotics-adds-two-notable-cruise-alumni-to-its-leadership-team/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/2k24-0002-AK9_9061_4-1.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Diligent Robotics is bulking up its leadership team as the company looks to scale its fleet of humanoid robots that work in hospitals and pharmacies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Austin, Texas-based Diligent announced Thursday it appointed Rashed Haq as its chief technology officer and Todd Brugger as its chief operating officer. Both Haq and Brugger were most recently at Cruise, the GM self-driving subsidiary that shuttered earlier this year.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Haq was formerly the vice president and head of AI and robotics. Brugger was Cruise’s COO.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Andrea Thomaz, the co-founder and CEO of Diligent Robotics, told TechCrunch it was the right time for the company to make these leadership hires. The startup has deployed about 100 of its Moxi humanoid robots, which assist healthcare facilities with non-patient-facing tasks, and is now ready to focus on scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’ve purposely grown a little bit more slowly, I would say, over the last two or three years, really honing some of the operational efficiencies and getting ready to be in a position to scale more dramatically,” Thomaz said. “And that’s kind of what we’re gearing up to do the end of this year and next year.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Thomaz said she got introduced to Haq first and liked his deep AI expertise and experience getting novel AI algorithms to work in real life, through Cruise’s autonomous cars, as opposed to just the lab.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While having early conversations with Haq, she was introduced to Brugger through a mutual connection. She felt Brugger’s experience scaling Cruise from zero vehicles on the road to hundreds seemed like the right fit for what Diligent needed.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;July 15&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“Todd and Rashed worked so well together at Cruise,” Thomaz said. “Everything started coming together. We were in need of operational leadership. We knew that we were needing to hire someone with Todd’s expertise, and it was really very much perfect timing.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Haq and Brugger both told TechCrunch that Diligent was a natural next step for them. The robotics company has already reached the deployment stage and the technology was very similar to what they were working on at Cruise. Haq added that autonomous vehicles are fundamentally mobile robots, just called a different name.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Many companies have early traction in terms of revenue and I call it ‘vibe revenue,’ because people try it out, and then they cancel their service afterwards, so then that revenue dies out,” Haq said. “But with Diligent, if you look at all the metrics, the robots are actually in day-to-day use, and have become integral parts of the companies that are using them. So that makes it a very sticky product as well. So, you know, lots of interesting things about the company.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Brugger said that he was also drawn to Diligent because it had a lot of the same operational challenges and priorities as Cruise.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There’s a sort of a hierarchy, or pyramid, of priorities that we looked at that I think will be very similar,” Brugger said. “You start with safety at the bottom of the pyramid, that’s a nonnegotiable. Then you move up and improve reliability. Beyond that, you continue to work on product-market-fit, which a lot of times, is expanding the capability or the utility of the robots. So I think that sort of pyramid is the same. And then the way you think about deployments, I think the parallels are very similar as well.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Diligent was founded in 2017 by Thomaz and Vivian Chu. The company’s Moxi robots are deployed in more than 25 healthcare networks. Diligent has raised more than $90 million in venture funding from firms, including Tiger Global, True Ventures, and Canaan Partners, among others.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/2k24-0002-AK9_9061_4-1.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Diligent Robotics is bulking up its leadership team as the company looks to scale its fleet of humanoid robots that work in hospitals and pharmacies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Austin, Texas-based Diligent announced Thursday it appointed Rashed Haq as its chief technology officer and Todd Brugger as its chief operating officer. Both Haq and Brugger were most recently at Cruise, the GM self-driving subsidiary that shuttered earlier this year.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Haq was formerly the vice president and head of AI and robotics. Brugger was Cruise’s COO.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Andrea Thomaz, the co-founder and CEO of Diligent Robotics, told TechCrunch it was the right time for the company to make these leadership hires. The startup has deployed about 100 of its Moxi humanoid robots, which assist healthcare facilities with non-patient-facing tasks, and is now ready to focus on scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’ve purposely grown a little bit more slowly, I would say, over the last two or three years, really honing some of the operational efficiencies and getting ready to be in a position to scale more dramatically,” Thomaz said. “And that’s kind of what we’re gearing up to do the end of this year and next year.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Thomaz said she got introduced to Haq first and liked his deep AI expertise and experience getting novel AI algorithms to work in real life, through Cruise’s autonomous cars, as opposed to just the lab.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While having early conversations with Haq, she was introduced to Brugger through a mutual connection. She felt Brugger’s experience scaling Cruise from zero vehicles on the road to hundreds seemed like the right fit for what Diligent needed.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;July 15&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“Todd and Rashed worked so well together at Cruise,” Thomaz said. “Everything started coming together. We were in need of operational leadership. We knew that we were needing to hire someone with Todd’s expertise, and it was really very much perfect timing.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Haq and Brugger both told TechCrunch that Diligent was a natural next step for them. The robotics company has already reached the deployment stage and the technology was very similar to what they were working on at Cruise. Haq added that autonomous vehicles are fundamentally mobile robots, just called a different name.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Many companies have early traction in terms of revenue and I call it ‘vibe revenue,’ because people try it out, and then they cancel their service afterwards, so then that revenue dies out,” Haq said. “But with Diligent, if you look at all the metrics, the robots are actually in day-to-day use, and have become integral parts of the companies that are using them. So that makes it a very sticky product as well. So, you know, lots of interesting things about the company.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Brugger said that he was also drawn to Diligent because it had a lot of the same operational challenges and priorities as Cruise.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There’s a sort of a hierarchy, or pyramid, of priorities that we looked at that I think will be very similar,” Brugger said. “You start with safety at the bottom of the pyramid, that’s a nonnegotiable. Then you move up and improve reliability. Beyond that, you continue to work on product-market-fit, which a lot of times, is expanding the capability or the utility of the robots. So I think that sort of pyramid is the same. And then the way you think about deployments, I think the parallels are very similar as well.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Diligent was founded in 2017 by Thomaz and Vivian Chu. The company’s Moxi robots are deployed in more than 25 healthcare networks. Diligent has raised more than $90 million in venture funding from firms, including Tiger Global, True Ventures, and Canaan Partners, among others.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/10/diligent-robotics-adds-two-notable-cruise-alumni-to-its-leadership-team/</guid><pubDate>Thu, 10 Jul 2025 13:00:00 +0000</pubDate></item><item><title>[NEW] How to Run Coding Assistants for Free on RTX AI PCs and Workstations (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/rtx-ai-garage-coding-assistants/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Coding assistants or copilots — AI-powered assistants that can suggest, explain and debug code — are fundamentally changing how software is developed for both experienced and novice developers.&lt;/p&gt;
&lt;p&gt;Experienced developers use these assistants to stay focused on complex coding tasks, reduce repetitive work and explore new ideas more quickly. Newer coders — like students and AI hobbyists — benefit from coding assistants that accelerate learning by describing different implementation approaches or explaining what a piece of code is doing and why.&lt;/p&gt;
&lt;p&gt;Coding assistants can run in cloud environments or locally. Cloud-based coding assistants can be run anywhere but offer some limitations and require a subscription. Local coding assistants remove these issues but require performant hardware to operate well.&lt;/p&gt;
&lt;p&gt;NVIDIA GeForce RTX GPUs provide the necessary hardware acceleration to run local assistants effectively.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Code, Meet Generative AI&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Traditional software development includes many mundane tasks such as reviewing documentation, researching examples, setting up boilerplate code, authoring code with appropriate syntax, tracing down bugs and documenting functions. These are essential tasks that can take time away from problem solving and software design. Coding assistants help streamline such steps.&lt;/p&gt;
&lt;p&gt;Many AI assistants are linked with popular integrated development environments (IDEs) like Microsoft Visual Studio Code or JetBrains’ Pycharm, which embed AI support directly into existing workflows.&lt;/p&gt;
&lt;p&gt;There are two ways to run coding assistants: in the cloud or locally.&lt;/p&gt;
&lt;p&gt;Cloud-based coding assistants require source code to be sent to external servers before responses are returned. This approach can be laggy and impose usage limits. Some developers prefer to keep their code local, especially when working with sensitive or proprietary projects. Plus, many cloud-based assistants require a paid subscription to unlock full functionality, which can be a barrier for students, hobbyists and teams that need to manage costs.&lt;/p&gt;
&lt;p&gt;Coding assistants run in a local environment, enabling cost-free access with:&lt;/p&gt;
&lt;figure class="wp-caption alignnone" id="attachment_83025"&gt;&lt;img alt="alt" class="wp-image-83025 size-medium" height="384" src="https://blogs.nvidia.com/wp-content/uploads/2025/07/raig-07-09-infographic-960x384.jpg" width="960" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-83025"&gt;Coding assistants running locally on RTX offer numerous advantages.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2&gt;&lt;b&gt;Get Started With Local Coding Assistants&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Tools that make it easy to run coding assistants locally include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Continue.dev&lt;/b&gt; — An open-source extension for the VS Code IDE that connects to local large language models (LLMs) via Ollama, LM Studio or custom endpoints. This tool offers in-editor chat, autocomplete and debugging assistance with minimal setup. Get started with Continue.dev using the Ollama backend for local RTX acceleration.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Tabby&lt;/b&gt; — A secure and transparent coding assistant that’s compatible across many IDEs with the ability to run AI on NVIDIA RTX GPUs. This tool offers code completion, answering queries, inline chat and more. Get started with Tabby on NVIDIA RTX AI PCs.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;OpenInterpreter&lt;/b&gt; — Experimental but rapidly evolving interface that combines LLMs with command-line access, file editing and agentic task execution. Ideal for automation and devops-style tasks for developers. Get started with OpenInterpreter on NVIDIA RTX AI PCs.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;LM Studio&lt;/b&gt; — A graphical user interface-based runner for local LLMs that offers chat, context window management and system prompts. Optimal for testing coding models interactively before IDE deployment. Get started with LM Studio on NVIDIA RTX AI PCs.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Ollama&lt;/b&gt; — A local AI model inferencing engine that enables fast, private inference of models like Code Llama, StarCoder2 and DeepSeek. It integrates seamlessly with tools like Continue.dev.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These tools support models served through frameworks like Ollama or llama.cpp, and many are now optimized for GeForce RTX and NVIDIA RTX PRO GPUs.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;See AI-Assisted Learning on RTX in Action&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Running on a GeForce RTX-powered PC, Continue.dev paired with the Gemma 12B Code LLM helps explain existing code, explore search algorithms and debug issues — all entirely on device. Acting like a virtual teaching assistant, the assistant provides plain-language guidance, context-aware explanations, inline comments and suggested code improvements tailored to the user’s project.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-full wp-image-83002" height="544" src="https://blogs.nvidia.com/wp-content/uploads/2025/07/Demo-of-Continue.dev-with-Gemma-12B-Code-LLM.gif" width="800" /&gt;&lt;/p&gt;
&lt;p&gt;This workflow highlights the advantage of local acceleration: the assistant is always available, responds instantly and provides personalized support, all while keeping the code private on device and making the learning experience immersive.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-full wp-image-83005" height="544" src="https://blogs.nvidia.com/wp-content/uploads/2025/07/Continue.dev-running-Gemma-12B-Code-LLM.gif" width="800" /&gt;&lt;/p&gt;
&lt;p&gt;That level of responsiveness comes down to GPU acceleration. Models like Gemma 12B are compute-heavy, especially when they’re processing long prompts or working across multiple files. Running them locally without a GPU can feel sluggish — even for simple tasks. With RTX GPUs, Tensor Cores accelerate inference directly on the device, so the assistant is fast, responsive and able to keep up with an active development workflow.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_83008"&gt;&lt;img alt="alt" class="size-full wp-image-83008" height="654" src="https://blogs.nvidia.com/wp-content/uploads/2025/07/Coding-assistant-performance.png" width="1070" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-83008"&gt;Coding assistants running on the Meta Llama 3.1-8B model experience 5-6x faster throughput on RTX-powered laptops versus on CPU. Data measured uses the average tokens per second at BS = 1, ISL/OSL = 2000/100, with the Llama-3.1-8B model quantized to int4.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Whether used for academic work, coding bootcamps or personal projects, RTX AI PCs are enabling developers to build, learn and iterate faster with AI-powered tools.&lt;/p&gt;
&lt;p&gt;For those just getting started — especially students building their skills or experimenting with generative AI — NVIDIA GeForce RTX 50 Series laptops feature specialized AI technologies that accelerate top applications for learning, creating and gaming, all on a single system. Explore RTX laptops ideal for back-to-school season.&lt;/p&gt;
&lt;p&gt;And to encourage AI enthusiasts and developers to experiment with local AI and extend the capabilities of their RTX PCs, NVIDIA is hosting a Plug and Play: Project G-Assist Plug-In Hackathon — running virtually through Wednesday, July 16. Participants can create custom plug-ins for Project G-Assist, an experimental AI assistant designed to respond to natural language and extend across creative and development tools. It’s a chance to win prizes and showcase what’s possible with RTX AI PCs.&lt;/p&gt;
&lt;p&gt;Join NVIDIA’s Discord server to connect with community developers and AI enthusiasts for discussions on what’s possible with RTX AI.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Each week, the &lt;/i&gt;&lt;i&gt;RTX AI Garage&lt;/i&gt; &lt;i&gt;blog series features community-driven AI innovations and content for those looking to learn more about NVIDIA NIM microservices and AI Blueprints, as well as building &lt;/i&gt;&lt;i&gt;AI agents&lt;/i&gt;&lt;i&gt;, creative workflows, digital humans, productivity apps and more on AI PCs and workstations.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; — and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Coding assistants or copilots — AI-powered assistants that can suggest, explain and debug code — are fundamentally changing how software is developed for both experienced and novice developers.&lt;/p&gt;
&lt;p&gt;Experienced developers use these assistants to stay focused on complex coding tasks, reduce repetitive work and explore new ideas more quickly. Newer coders — like students and AI hobbyists — benefit from coding assistants that accelerate learning by describing different implementation approaches or explaining what a piece of code is doing and why.&lt;/p&gt;
&lt;p&gt;Coding assistants can run in cloud environments or locally. Cloud-based coding assistants can be run anywhere but offer some limitations and require a subscription. Local coding assistants remove these issues but require performant hardware to operate well.&lt;/p&gt;
&lt;p&gt;NVIDIA GeForce RTX GPUs provide the necessary hardware acceleration to run local assistants effectively.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Code, Meet Generative AI&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Traditional software development includes many mundane tasks such as reviewing documentation, researching examples, setting up boilerplate code, authoring code with appropriate syntax, tracing down bugs and documenting functions. These are essential tasks that can take time away from problem solving and software design. Coding assistants help streamline such steps.&lt;/p&gt;
&lt;p&gt;Many AI assistants are linked with popular integrated development environments (IDEs) like Microsoft Visual Studio Code or JetBrains’ Pycharm, which embed AI support directly into existing workflows.&lt;/p&gt;
&lt;p&gt;There are two ways to run coding assistants: in the cloud or locally.&lt;/p&gt;
&lt;p&gt;Cloud-based coding assistants require source code to be sent to external servers before responses are returned. This approach can be laggy and impose usage limits. Some developers prefer to keep their code local, especially when working with sensitive or proprietary projects. Plus, many cloud-based assistants require a paid subscription to unlock full functionality, which can be a barrier for students, hobbyists and teams that need to manage costs.&lt;/p&gt;
&lt;p&gt;Coding assistants run in a local environment, enabling cost-free access with:&lt;/p&gt;
&lt;figure class="wp-caption alignnone" id="attachment_83025"&gt;&lt;img alt="alt" class="wp-image-83025 size-medium" height="384" src="https://blogs.nvidia.com/wp-content/uploads/2025/07/raig-07-09-infographic-960x384.jpg" width="960" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-83025"&gt;Coding assistants running locally on RTX offer numerous advantages.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2&gt;&lt;b&gt;Get Started With Local Coding Assistants&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Tools that make it easy to run coding assistants locally include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Continue.dev&lt;/b&gt; — An open-source extension for the VS Code IDE that connects to local large language models (LLMs) via Ollama, LM Studio or custom endpoints. This tool offers in-editor chat, autocomplete and debugging assistance with minimal setup. Get started with Continue.dev using the Ollama backend for local RTX acceleration.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Tabby&lt;/b&gt; — A secure and transparent coding assistant that’s compatible across many IDEs with the ability to run AI on NVIDIA RTX GPUs. This tool offers code completion, answering queries, inline chat and more. Get started with Tabby on NVIDIA RTX AI PCs.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;OpenInterpreter&lt;/b&gt; — Experimental but rapidly evolving interface that combines LLMs with command-line access, file editing and agentic task execution. Ideal for automation and devops-style tasks for developers. Get started with OpenInterpreter on NVIDIA RTX AI PCs.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;LM Studio&lt;/b&gt; — A graphical user interface-based runner for local LLMs that offers chat, context window management and system prompts. Optimal for testing coding models interactively before IDE deployment. Get started with LM Studio on NVIDIA RTX AI PCs.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Ollama&lt;/b&gt; — A local AI model inferencing engine that enables fast, private inference of models like Code Llama, StarCoder2 and DeepSeek. It integrates seamlessly with tools like Continue.dev.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These tools support models served through frameworks like Ollama or llama.cpp, and many are now optimized for GeForce RTX and NVIDIA RTX PRO GPUs.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;See AI-Assisted Learning on RTX in Action&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Running on a GeForce RTX-powered PC, Continue.dev paired with the Gemma 12B Code LLM helps explain existing code, explore search algorithms and debug issues — all entirely on device. Acting like a virtual teaching assistant, the assistant provides plain-language guidance, context-aware explanations, inline comments and suggested code improvements tailored to the user’s project.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-full wp-image-83002" height="544" src="https://blogs.nvidia.com/wp-content/uploads/2025/07/Demo-of-Continue.dev-with-Gemma-12B-Code-LLM.gif" width="800" /&gt;&lt;/p&gt;
&lt;p&gt;This workflow highlights the advantage of local acceleration: the assistant is always available, responds instantly and provides personalized support, all while keeping the code private on device and making the learning experience immersive.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-full wp-image-83005" height="544" src="https://blogs.nvidia.com/wp-content/uploads/2025/07/Continue.dev-running-Gemma-12B-Code-LLM.gif" width="800" /&gt;&lt;/p&gt;
&lt;p&gt;That level of responsiveness comes down to GPU acceleration. Models like Gemma 12B are compute-heavy, especially when they’re processing long prompts or working across multiple files. Running them locally without a GPU can feel sluggish — even for simple tasks. With RTX GPUs, Tensor Cores accelerate inference directly on the device, so the assistant is fast, responsive and able to keep up with an active development workflow.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_83008"&gt;&lt;img alt="alt" class="size-full wp-image-83008" height="654" src="https://blogs.nvidia.com/wp-content/uploads/2025/07/Coding-assistant-performance.png" width="1070" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-83008"&gt;Coding assistants running on the Meta Llama 3.1-8B model experience 5-6x faster throughput on RTX-powered laptops versus on CPU. Data measured uses the average tokens per second at BS = 1, ISL/OSL = 2000/100, with the Llama-3.1-8B model quantized to int4.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Whether used for academic work, coding bootcamps or personal projects, RTX AI PCs are enabling developers to build, learn and iterate faster with AI-powered tools.&lt;/p&gt;
&lt;p&gt;For those just getting started — especially students building their skills or experimenting with generative AI — NVIDIA GeForce RTX 50 Series laptops feature specialized AI technologies that accelerate top applications for learning, creating and gaming, all on a single system. Explore RTX laptops ideal for back-to-school season.&lt;/p&gt;
&lt;p&gt;And to encourage AI enthusiasts and developers to experiment with local AI and extend the capabilities of their RTX PCs, NVIDIA is hosting a Plug and Play: Project G-Assist Plug-In Hackathon — running virtually through Wednesday, July 16. Participants can create custom plug-ins for Project G-Assist, an experimental AI assistant designed to respond to natural language and extend across creative and development tools. It’s a chance to win prizes and showcase what’s possible with RTX AI PCs.&lt;/p&gt;
&lt;p&gt;Join NVIDIA’s Discord server to connect with community developers and AI enthusiasts for discussions on what’s possible with RTX AI.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Each week, the &lt;/i&gt;&lt;i&gt;RTX AI Garage&lt;/i&gt; &lt;i&gt;blog series features community-driven AI innovations and content for those looking to learn more about NVIDIA NIM microservices and AI Blueprints, as well as building &lt;/i&gt;&lt;i&gt;AI agents&lt;/i&gt;&lt;i&gt;, creative workflows, digital humans, productivity apps and more on AI PCs and workstations.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; — and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/rtx-ai-garage-coding-assistants/</guid><pubDate>Thu, 10 Jul 2025 13:00:02 +0000</pubDate></item><item><title>[NEW] Reach the ‘PEAK’ on GeForce NOW (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/geforce-now-thursday-peak/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Grab a friend and climb toward the clouds — &lt;i&gt;PEAK&lt;/i&gt; is now available on GeForce NOW, enabling members to try the hugely popular indie hit on virtually any device.&lt;/p&gt;
&lt;p&gt;It’s one of four new games joining the cloud this week. Plus, members can look forward to &lt;i&gt;Tony Hawk’s Pro Skater 3 + 4&lt;/i&gt; coming soon.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Time to Climb&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_83031"&gt;&lt;img alt="PEAK on GeForce NOW" class="size-large wp-image-83031" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/07/ss_55365bfa09745df86bed72720a842f64d8724b9d-1-1680x945.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-83031"&gt;&lt;em&gt;There’s always gonna be another mountain.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;PEAK is a co-op climbing game that puts players in the shoes of lost nature scouts, ascending a mountain at the center of a mysterious island. Scavenge for food (even if it’s of questionable quality), manage injuries on the climb and help the squad summit safely. Members can play solo or survive together with up to four players.&lt;/p&gt;
&lt;p&gt;There’s a new island to survive on every day. And, with more than 100,000 concurrent players daily on Steam, there’s always a climbing buddy to join.&lt;/p&gt;
&lt;p&gt;GeForce NOW members are equipped for the challenge with an Ultimate membership, which powers the climb at up to 4K resolution and 120 frames per second. Ultimate members can play &lt;i&gt;PEAK &lt;/i&gt;and more than 2,000 other games they already own with extended session lengths and ultralow latency on a GeForce RTX 4080 rig. Upgrade today for elevated gameplay.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Get Hyped&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_83022"&gt;&lt;img alt="Tony Hawk's Pro Skater 3+4 is Coming soon to GeForce NOW" class="size-large wp-image-83022" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/07/CHI_Reveal_College_NH_02_UNBRANDED_FINAL-1680x945.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-83022"&gt;&lt;em&gt;Old school, new tricks.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;i&gt;Tony Hawk’s Pro Skater 3 + 4&lt;/i&gt; is coming soon to GeForce NOW.&lt;/p&gt;
&lt;p&gt;The legendary franchise series that taught generations to ollie, grind and combo like maniacs is back, helping members hit the 900 from nearly any device.&lt;/p&gt;
&lt;p&gt;The Birdman and crew return, bringing all the classic parks, legendary skaters and iconic soundtrack gamers remember — now fully remade with a few wild surprises for players.&lt;/p&gt;
&lt;p&gt;Relive the glory days, whether grinding rails in the airport or pulling off insane combos in Los Angeles. New environments like a water park add a creative twist. The roster is stacked with original legends and new faces — plus a few unexpected guests.&lt;/p&gt;
&lt;p&gt;Career Mode delivers with heart-pounding runs, while New Game+ and Solo Tours keep the challenge alive. Take on friends and rivals in online multiplayer mode. The upgraded Create-a-Park and Create-a-Skater tools mean gamers can build, style and shred their way.&lt;/p&gt;
&lt;p&gt;With GeForce NOW, gamers will soon be able to skate anywhere, anytime — no console required. Enjoy the title in stunning 4K resolution and ultrasmooth frame rates with an Ultimate membership powered by GeForce RTX 4080 servers. Drop in instantly on any device, chase high scores with the lowest latency and keep the shred alive. The ultimate skate session is always just a click away.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Every Day a New Game&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_83019"&gt;&lt;img alt="Every Day we Fight on GeForce NOW" class="size-large wp-image-83019" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/07/GFN_Thursday-Every_Day_We_Fight-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-83019"&gt;&lt;em&gt;Fight as one, survive as many.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Catch &lt;i&gt;Every Day We Fight, &lt;/i&gt;a new roguelite, turn-based tactic game from Singla Space Lab and Hooded Horse, in the cloud with GeForce NOW.&lt;/p&gt;
&lt;p&gt;In this title, citizens from either side of an ongoing war must set aside their differences as a mysterious alien invasion threatens humanity — and time has come to a stop for all but a small band of freedom fighters. Caught in a seemingly endless loop, players must shape these ordinary civilians into heroes as they repeatedly fight and die. Real-time exploration, stealth and teamwork are essential to acquire new skills, seek out more powerful weapons, escape the time loop and save the world.&lt;/p&gt;
&lt;p&gt;Members can look for the following games to stream:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Every Day We Fight &lt;/i&gt;(New release on Steam, July 10)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Mycopunk &lt;/i&gt;(New release on Steam, July 10)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Brickadia &lt;/i&gt;(New release on Steam, July 11)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Peak&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What are you planning to play this weekend? Let us know on X or in the comments below.&lt;/p&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p dir="ltr" lang="en"&gt;🕹️ Which game demands the fastest reflexes you’ve got?&lt;/p&gt;
&lt;p&gt;— 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) July 9, 2025&lt;/p&gt;&lt;/blockquote&gt;


		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Grab a friend and climb toward the clouds — &lt;i&gt;PEAK&lt;/i&gt; is now available on GeForce NOW, enabling members to try the hugely popular indie hit on virtually any device.&lt;/p&gt;
&lt;p&gt;It’s one of four new games joining the cloud this week. Plus, members can look forward to &lt;i&gt;Tony Hawk’s Pro Skater 3 + 4&lt;/i&gt; coming soon.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Time to Climb&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_83031"&gt;&lt;img alt="PEAK on GeForce NOW" class="size-large wp-image-83031" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/07/ss_55365bfa09745df86bed72720a842f64d8724b9d-1-1680x945.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-83031"&gt;&lt;em&gt;There’s always gonna be another mountain.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;PEAK is a co-op climbing game that puts players in the shoes of lost nature scouts, ascending a mountain at the center of a mysterious island. Scavenge for food (even if it’s of questionable quality), manage injuries on the climb and help the squad summit safely. Members can play solo or survive together with up to four players.&lt;/p&gt;
&lt;p&gt;There’s a new island to survive on every day. And, with more than 100,000 concurrent players daily on Steam, there’s always a climbing buddy to join.&lt;/p&gt;
&lt;p&gt;GeForce NOW members are equipped for the challenge with an Ultimate membership, which powers the climb at up to 4K resolution and 120 frames per second. Ultimate members can play &lt;i&gt;PEAK &lt;/i&gt;and more than 2,000 other games they already own with extended session lengths and ultralow latency on a GeForce RTX 4080 rig. Upgrade today for elevated gameplay.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Get Hyped&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_83022"&gt;&lt;img alt="Tony Hawk's Pro Skater 3+4 is Coming soon to GeForce NOW" class="size-large wp-image-83022" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/07/CHI_Reveal_College_NH_02_UNBRANDED_FINAL-1680x945.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-83022"&gt;&lt;em&gt;Old school, new tricks.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;i&gt;Tony Hawk’s Pro Skater 3 + 4&lt;/i&gt; is coming soon to GeForce NOW.&lt;/p&gt;
&lt;p&gt;The legendary franchise series that taught generations to ollie, grind and combo like maniacs is back, helping members hit the 900 from nearly any device.&lt;/p&gt;
&lt;p&gt;The Birdman and crew return, bringing all the classic parks, legendary skaters and iconic soundtrack gamers remember — now fully remade with a few wild surprises for players.&lt;/p&gt;
&lt;p&gt;Relive the glory days, whether grinding rails in the airport or pulling off insane combos in Los Angeles. New environments like a water park add a creative twist. The roster is stacked with original legends and new faces — plus a few unexpected guests.&lt;/p&gt;
&lt;p&gt;Career Mode delivers with heart-pounding runs, while New Game+ and Solo Tours keep the challenge alive. Take on friends and rivals in online multiplayer mode. The upgraded Create-a-Park and Create-a-Skater tools mean gamers can build, style and shred their way.&lt;/p&gt;
&lt;p&gt;With GeForce NOW, gamers will soon be able to skate anywhere, anytime — no console required. Enjoy the title in stunning 4K resolution and ultrasmooth frame rates with an Ultimate membership powered by GeForce RTX 4080 servers. Drop in instantly on any device, chase high scores with the lowest latency and keep the shred alive. The ultimate skate session is always just a click away.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Every Day a New Game&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_83019"&gt;&lt;img alt="Every Day we Fight on GeForce NOW" class="size-large wp-image-83019" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/07/GFN_Thursday-Every_Day_We_Fight-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-83019"&gt;&lt;em&gt;Fight as one, survive as many.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Catch &lt;i&gt;Every Day We Fight, &lt;/i&gt;a new roguelite, turn-based tactic game from Singla Space Lab and Hooded Horse, in the cloud with GeForce NOW.&lt;/p&gt;
&lt;p&gt;In this title, citizens from either side of an ongoing war must set aside their differences as a mysterious alien invasion threatens humanity — and time has come to a stop for all but a small band of freedom fighters. Caught in a seemingly endless loop, players must shape these ordinary civilians into heroes as they repeatedly fight and die. Real-time exploration, stealth and teamwork are essential to acquire new skills, seek out more powerful weapons, escape the time loop and save the world.&lt;/p&gt;
&lt;p&gt;Members can look for the following games to stream:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Every Day We Fight &lt;/i&gt;(New release on Steam, July 10)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Mycopunk &lt;/i&gt;(New release on Steam, July 10)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Brickadia &lt;/i&gt;(New release on Steam, July 11)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Peak&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What are you planning to play this weekend? Let us know on X or in the comments below.&lt;/p&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p dir="ltr" lang="en"&gt;🕹️ Which game demands the fastest reflexes you’ve got?&lt;/p&gt;
&lt;p&gt;— 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) July 9, 2025&lt;/p&gt;&lt;/blockquote&gt;


		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/geforce-now-thursday-peak/</guid><pubDate>Thu, 10 Jul 2025 13:00:34 +0000</pubDate></item><item><title>[NEW] Knox lands $6.5M to compete with Palantir in the federal compliance market (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/10/knox-lands-6-5m-to-compete-with-palantir-in-the-federal-compliance-market/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/1CE6B615-D89F-45AD-9376-3F965C30AA2F.jpeg?resize=1200,718" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Highly sought-after federal software contracts frequently come with a hidden cost: Achieving government SaaS security compliance, known as FedRAMP, can take years and require substantial resources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Achieving this certification typically takes up to three years and costs more than $3 million, covering everything from security operations engineer salaries to security audits, according to Irina Denisenko, CEO of Knox.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Denisenko (pictured above, second from right) launched Knox, a federal managed cloud provider, last year with a mission to help software vendors speed through this security authorization process in just three months, and at a fraction of what it would cost to do it on their own.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Thursday, Knox said it has raised a $6.5 million seed round led by Felicis, with participation from Ridgeline and FirsthandVC.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Denisenko decided to embark on this journey after she learned firsthand the challenges of obtaining FedRAMP. Class, an education startup where she served as COO, had secured a contract to sell its software to the U.S. Air Force. And instead of waiting three years and spending millions, Denisenko helped Class.com buy CoSo Cloud, a company that was already FedRAMP certified and was managing Adobe’s federal cloud.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The acquisition helped Class receive FedRAMP certification in just six months. “Class would still be getting FedRAMP today” if it had tried to obtain the clearance on its own, Denisenko told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And late last year, when it became clear that the proliferation of AI agents was becoming a national security concern, Denisenko decided to spin out the managed cloud solution into a standalone startup, Knox.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;July 15&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Companies that can afford FedRAMP certification include large software vendors like CrowdStrike, Palo Alto Networks, and Salesforce, Denisenko told TechCrunch. And as the government increasingly adopts more software, she hopes Knox can help SaaS vendors gain FedRAMP to access government contracts more easily.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Knox, named after a giant gold-storage fort in Kentucky, essentially provides a compliance management platform via a managed cloud that customers can connect their codebase to. The company’s software runs a continuous series of tests and audits to identify where the customer’s infrastructure, code, and security controls are falling short of FedRAMP standards, and either remediates those issues itself or flags them to the customer. It also offers some non-software tools to track and verify policies like personnel training and vendor management.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This stuff is legitimately very hard and very risky,” she said. “We will bear the risk.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Knox is already handling security and compliance for Adobe, Class, Spacelift, and an LLM provider. “We’ll end the year with well north of a dozen customers live in the cloud,” Denisenko said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While FedRAMP authorization management may seem like a niche offering, Knox has one large competitor: Palantir.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Palantir’s offering, called FedStart, was introduced only two years ago, and since then, the giant data analysis platform has brought on the likes of Anthropic and Windsurf as clients. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Denisenko, Palantir’s early success with FedRAMP only validates Knox’s mission.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Even Anthropic couldn’t figure this out on their own,” she said, adding that going forward, software companies will want to outsource their FedRAMP compliance to a company like Knox.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/1CE6B615-D89F-45AD-9376-3F965C30AA2F.jpeg?resize=1200,718" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Highly sought-after federal software contracts frequently come with a hidden cost: Achieving government SaaS security compliance, known as FedRAMP, can take years and require substantial resources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Achieving this certification typically takes up to three years and costs more than $3 million, covering everything from security operations engineer salaries to security audits, according to Irina Denisenko, CEO of Knox.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Denisenko (pictured above, second from right) launched Knox, a federal managed cloud provider, last year with a mission to help software vendors speed through this security authorization process in just three months, and at a fraction of what it would cost to do it on their own.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Thursday, Knox said it has raised a $6.5 million seed round led by Felicis, with participation from Ridgeline and FirsthandVC.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Denisenko decided to embark on this journey after she learned firsthand the challenges of obtaining FedRAMP. Class, an education startup where she served as COO, had secured a contract to sell its software to the U.S. Air Force. And instead of waiting three years and spending millions, Denisenko helped Class.com buy CoSo Cloud, a company that was already FedRAMP certified and was managing Adobe’s federal cloud.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The acquisition helped Class receive FedRAMP certification in just six months. “Class would still be getting FedRAMP today” if it had tried to obtain the clearance on its own, Denisenko told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And late last year, when it became clear that the proliferation of AI agents was becoming a national security concern, Denisenko decided to spin out the managed cloud solution into a standalone startup, Knox.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;July 15&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Companies that can afford FedRAMP certification include large software vendors like CrowdStrike, Palo Alto Networks, and Salesforce, Denisenko told TechCrunch. And as the government increasingly adopts more software, she hopes Knox can help SaaS vendors gain FedRAMP to access government contracts more easily.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Knox, named after a giant gold-storage fort in Kentucky, essentially provides a compliance management platform via a managed cloud that customers can connect their codebase to. The company’s software runs a continuous series of tests and audits to identify where the customer’s infrastructure, code, and security controls are falling short of FedRAMP standards, and either remediates those issues itself or flags them to the customer. It also offers some non-software tools to track and verify policies like personnel training and vendor management.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This stuff is legitimately very hard and very risky,” she said. “We will bear the risk.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Knox is already handling security and compliance for Adobe, Class, Spacelift, and an LLM provider. “We’ll end the year with well north of a dozen customers live in the cloud,” Denisenko said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While FedRAMP authorization management may seem like a niche offering, Knox has one large competitor: Palantir.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Palantir’s offering, called FedStart, was introduced only two years ago, and since then, the giant data analysis platform has brought on the likes of Anthropic and Windsurf as clients. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Denisenko, Palantir’s early success with FedRAMP only validates Knox’s mission.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Even Anthropic couldn’t figure this out on their own,” she said, adding that going forward, software companies will want to outsource their FedRAMP compliance to a company like Knox.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/10/knox-lands-6-5m-to-compete-with-palantir-in-the-federal-compliance-market/</guid><pubDate>Thu, 10 Jul 2025 13:15:00 +0000</pubDate></item><item><title>[NEW] LGND wants to make ChatGPT for the Earth (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/10/lgnd-wants-to-make-chatgpt-for-the-earth/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2020/12/nasa-yZygONrUBe8-unsplash.jpg?resize=1200,799" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Earth is awash in data about itself. Every day, satellites capture around 100 terabytes of imagery.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But making sense of it isn’t always easy. Seemingly simple questions can be fiendishly complex to answer. Take this question that is of vital economic importance to California: How many fire breaks does the state have that might stop a wildfire in its tracks, and how have they changed since the last fire season?&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Originally, you’d have a person look at pictures. And that only scales so far,” Nathaniel Manning, co-founder and CEO of LGND, told TechCrunch. In recent years, neural networks have made it a bit easier, allowing machine learning experts and data scientists to train algorithms how to see fire breaks in satellite imagery.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You probably sink, you know, [a] couple hundred thousand dollars —&amp;nbsp;if not multiple hundred thousand dollars — to try to create that dataset, and it would only be able to do that one thing,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;LGND wants to slash those figures by an order of magnitude or more.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are not looking to replace people doing these things,” said Bruno Sánchez-Andrade Nuño, LGND’s co-founder and chief scientist. “We’re looking to make them 10 times more efficient, 100 times more efficient.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;LGND recently raised a $9 million seed round led by Javelin Venture Partners, the company exclusively told TechCrunch. AENU, Clocktower Ventures, Coalition Operators, MCJ, Overture, Ridgeline, and Space Capital participated. A number of angel investors also joined, including Keyhole founder John Hanke, Ramp co-founder Karim Atiyeh, and Salesforce executive Suzanne DiBianca.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;July 15&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The startup’s core product is vector embeddings of geographic data. Today, most geographic information exists in either pixels or traditional vectors (points, lines, areas). They’re flexible and easy to distribute and read, but interpreting that information requires either deep understanding of the space, some nontrivial amount of computing, or both.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Geographic embeddings summarize spatial data in a way that makes it easier to find relationships between different points on Earth.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Embeddings get you 90% of all the undifferentiated compute up front,” Nuño said. “Embeddings are the universal, super-short summaries that embody 90% of the computation you have to do anyways.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Take the example of fire breaks. They might take the form of roads, rivers, or lakes. Each of them will appear differently on a map, but they all share certain characteristics. For one, pixels that make up an image of a fire break won’t have any vegetation. Also, a fire break will have to be a certain minimum width, which often depends on how tall the vegetation is around it. Embeddings make it much easier to find places on a map that match those descriptions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;LGND has built an enterprise app to help large companies answer questions involving spatial data, along with an API which users with more specific needs can hit directly.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Manning sees LGND’s embeddings encouraging companies to query geospatial data in entirely new ways.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Imagine an AI travel agent, he said. Users might ask it to find a short-term rental with three rooms that’s close to good snorkeling. “But also, I want to be on a white sand beach. I want to know that there’s very little sea weed in February, when we’re going to go, and maybe most importantly, at this time of booking, there’s no construction happening within one kilometer of the house,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Building traditional geospatial models to answer those questions would be time-consuming for just one query, let alone all of them together.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If LGND can succeed in delivering such a tool to the masses, or even just to people who use geospatial data for their jobs, it has the potential to take a bite out of a market valued near $400 billion.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re trying to be the Standard Oil for this data,” Manning said.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2020/12/nasa-yZygONrUBe8-unsplash.jpg?resize=1200,799" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Earth is awash in data about itself. Every day, satellites capture around 100 terabytes of imagery.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But making sense of it isn’t always easy. Seemingly simple questions can be fiendishly complex to answer. Take this question that is of vital economic importance to California: How many fire breaks does the state have that might stop a wildfire in its tracks, and how have they changed since the last fire season?&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Originally, you’d have a person look at pictures. And that only scales so far,” Nathaniel Manning, co-founder and CEO of LGND, told TechCrunch. In recent years, neural networks have made it a bit easier, allowing machine learning experts and data scientists to train algorithms how to see fire breaks in satellite imagery.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You probably sink, you know, [a] couple hundred thousand dollars —&amp;nbsp;if not multiple hundred thousand dollars — to try to create that dataset, and it would only be able to do that one thing,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;LGND wants to slash those figures by an order of magnitude or more.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are not looking to replace people doing these things,” said Bruno Sánchez-Andrade Nuño, LGND’s co-founder and chief scientist. “We’re looking to make them 10 times more efficient, 100 times more efficient.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;LGND recently raised a $9 million seed round led by Javelin Venture Partners, the company exclusively told TechCrunch. AENU, Clocktower Ventures, Coalition Operators, MCJ, Overture, Ridgeline, and Space Capital participated. A number of angel investors also joined, including Keyhole founder John Hanke, Ramp co-founder Karim Atiyeh, and Salesforce executive Suzanne DiBianca.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;July 15&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The startup’s core product is vector embeddings of geographic data. Today, most geographic information exists in either pixels or traditional vectors (points, lines, areas). They’re flexible and easy to distribute and read, but interpreting that information requires either deep understanding of the space, some nontrivial amount of computing, or both.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Geographic embeddings summarize spatial data in a way that makes it easier to find relationships between different points on Earth.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Embeddings get you 90% of all the undifferentiated compute up front,” Nuño said. “Embeddings are the universal, super-short summaries that embody 90% of the computation you have to do anyways.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Take the example of fire breaks. They might take the form of roads, rivers, or lakes. Each of them will appear differently on a map, but they all share certain characteristics. For one, pixels that make up an image of a fire break won’t have any vegetation. Also, a fire break will have to be a certain minimum width, which often depends on how tall the vegetation is around it. Embeddings make it much easier to find places on a map that match those descriptions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;LGND has built an enterprise app to help large companies answer questions involving spatial data, along with an API which users with more specific needs can hit directly.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Manning sees LGND’s embeddings encouraging companies to query geospatial data in entirely new ways.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Imagine an AI travel agent, he said. Users might ask it to find a short-term rental with three rooms that’s close to good snorkeling. “But also, I want to be on a white sand beach. I want to know that there’s very little sea weed in February, when we’re going to go, and maybe most importantly, at this time of booking, there’s no construction happening within one kilometer of the house,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Building traditional geospatial models to answer those questions would be time-consuming for just one query, let alone all of them together.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If LGND can succeed in delivering such a tool to the masses, or even just to people who use geospatial data for their jobs, it has the potential to take a bite out of a market valued near $400 billion.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re trying to be the Standard Oil for this data,” Manning said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/10/lgnd-wants-to-make-chatgpt-for-the-earth/</guid><pubDate>Thu, 10 Jul 2025 13:58:45 +0000</pubDate></item><item><title>[NEW] Grok is coming to Tesla vehicles ‘next week,’ says Elon Musk (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/10/grok-is-coming-to-tesla-vehicles-next-week-says-elon-musk/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2218892225.jpg?resize=1200,804" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Elon Musk said in a post on X early Thursday morning that Grok, the chatbot from his AI company, xAI, will be coming to Tesla vehicles “very soon.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Next week at the latest,” he said.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The news that Grok would be coming to Tesla vehicles soon comes several hours after xAI debuted the latest flagship AI model, Grok 4. Fans had wondered loudly why Musk spent an hour late on Wednesday talking about Grok with no mention of a Tesla integration, which likely prompted the billionaire’s early morning announcement.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The update also comes as adjustments to Grok have made the chatbot more prone to misbehavior — including making antisemitic comments, slating the Democrats, and even rape threats. X took down Grok temporarily on Wednesday to attempt to solve the problem.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Musk is known for making Tesla-related announcements on X, the social media platform he owns, oftentimes before he even tells his own engineers. In this case, they might have seen it coming. Musk has teased that Grok would end up in Tesla vehicles as an AI assistant for months, saying that Tesla drivers would be able to chat conversationally with their cars and ask Grok to perform certain tasks.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While poking around in Tesla’s firmware, a hacker who goes by the name “green” last week found that drivers can choose between certain Grok “personalities,” including ones that are NSFW (not safe for work). There seem to be a lot of personalities to choose from, including argumentative, conspiracy, kids story, sexy, therapist, unhinged, and more.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Green’s findings also suggest that Grok will only be available on newer vehicles with Hardware 3. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Grok will also be the voice and brain for Tesla’s humanoid robot Optimus, Musk confirmed recently.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2218892225.jpg?resize=1200,804" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Elon Musk said in a post on X early Thursday morning that Grok, the chatbot from his AI company, xAI, will be coming to Tesla vehicles “very soon.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Next week at the latest,” he said.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The news that Grok would be coming to Tesla vehicles soon comes several hours after xAI debuted the latest flagship AI model, Grok 4. Fans had wondered loudly why Musk spent an hour late on Wednesday talking about Grok with no mention of a Tesla integration, which likely prompted the billionaire’s early morning announcement.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The update also comes as adjustments to Grok have made the chatbot more prone to misbehavior — including making antisemitic comments, slating the Democrats, and even rape threats. X took down Grok temporarily on Wednesday to attempt to solve the problem.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Musk is known for making Tesla-related announcements on X, the social media platform he owns, oftentimes before he even tells his own engineers. In this case, they might have seen it coming. Musk has teased that Grok would end up in Tesla vehicles as an AI assistant for months, saying that Tesla drivers would be able to chat conversationally with their cars and ask Grok to perform certain tasks.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While poking around in Tesla’s firmware, a hacker who goes by the name “green” last week found that drivers can choose between certain Grok “personalities,” including ones that are NSFW (not safe for work). There seem to be a lot of personalities to choose from, including argumentative, conspiracy, kids story, sexy, therapist, unhinged, and more.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Green’s findings also suggest that Grok will only be available on newer vehicles with Hardware 3. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Grok will also be the voice and brain for Tesla’s humanoid robot Optimus, Musk confirmed recently.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/10/grok-is-coming-to-tesla-vehicles-next-week-says-elon-musk/</guid><pubDate>Thu, 10 Jul 2025 14:22:24 +0000</pubDate></item><item><title>[NEW] Google adds image-to-video generation capability to Veo 3 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/10/google-adds-image-to-video-generation-capability-to-veo-3/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google said on Thursday it’s adding an image-to-video generation feature to its Veo 3 AI video generator through its Gemini app.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company had already rolled out this feature in its AI-powered video tool called Flow, which was launched in May at Google’s I/O developer conference.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;After launching Veo 3-powered video generation in May, Google made the feature available in over 150 countries as of last week. At the moment, only Google AI Ultra and Google AI Pro plan users can generate videos with a three-creations-per-day limit with no carry-over.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3026471" height="383" src="https://techcrunch.com/wp-content/uploads/2025/07/Google-gemini.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;What image-to-video generation with Veo 3 looks like&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google said that users can generate a clip by selecting the “Videos” option from the tool menu in the prompt box and uploading a photo. You can also add sound by describing the audio in the prompt. Once the video is generated, you can download it or share it with others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company noted that since its release seven weeks ago, users have created more than 40 million videos across the Gemini app and Flow tool. All videos generated using the Veo 3 model will have a visible watermark that says “Veo” along with an invisible SynthID digital watermark, which is adopted by Google’s AI tool to identify AI-powered digital artifacts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this year, the company also released a tool that helps you detect content containing SynthID.&lt;/p&gt;


&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;July 15&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google said on Thursday it’s adding an image-to-video generation feature to its Veo 3 AI video generator through its Gemini app.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company had already rolled out this feature in its AI-powered video tool called Flow, which was launched in May at Google’s I/O developer conference.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;After launching Veo 3-powered video generation in May, Google made the feature available in over 150 countries as of last week. At the moment, only Google AI Ultra and Google AI Pro plan users can generate videos with a three-creations-per-day limit with no carry-over.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3026471" height="383" src="https://techcrunch.com/wp-content/uploads/2025/07/Google-gemini.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;What image-to-video generation with Veo 3 looks like&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google said that users can generate a clip by selecting the “Videos” option from the tool menu in the prompt box and uploading a photo. You can also add sound by describing the audio in the prompt. Once the video is generated, you can download it or share it with others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company noted that since its release seven weeks ago, users have created more than 40 million videos across the Gemini app and Flow tool. All videos generated using the Veo 3 model will have a visible watermark that says “Veo” along with an invisible SynthID digital watermark, which is adopted by Google’s AI tool to identify AI-powered digital artifacts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this year, the company also released a tool that helps you detect content containing SynthID.&lt;/p&gt;


&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;July 15&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/10/google-adds-image-to-video-generation-capability-to-veo-3/</guid><pubDate>Thu, 10 Jul 2025 15:00:00 +0000</pubDate></item><item><title>[NEW] Gemini can now turn your photos into video with Veo 3 (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/07/google-adds-photo-to-video-generation-with-veo-3-to-the-gemini-app/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google is making it easier to create videos with Gemini, but you only get a few shots per day.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Gemini icon macro" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/04/Gemini-1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Gemini icon macro" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/04/Gemini-1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google's Veo 3 videos have propagated across the Internet since the model's debut in May, blurring the line between truth and fiction. Now, it's getting even easier to create these AI videos. The Gemini app is gaining photo-to-video generation, allowing you to upload a photo and turn it into a video. You don't have to pay anything extra for these Veo 3 videos, but the feature is only available to subscribers of Google's Pro and Ultra AI plans.&lt;/p&gt;
&lt;p&gt;When Veo 3 launched, it could conjure up a video based only on your description, complete with speech, music, and background audio. This has made Google's new AI videos staggeringly realistic—it's actually getting hard to identify AI videos at a glance. Using a reference photo makes it easier to get the look you want without tediously describing every aspect. This was an option in Google's Flow AI tool for filmmakers, but now it's in the Gemini app and web interface.&lt;/p&gt;
&lt;p&gt;To create a video from a photo, you have to select "Video" from the Gemini toolbar. Once this feature is available, you can then add your image and prompt, including audio and dialogue. Generating the video takes several minutes—this process takes a lot of computation, which is why video output is still quite limited.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Veo 3 videos are limited to 720p resolution and eight seconds in length, and there's no guarantee you'll like what Veo 3 spits out. That can be frustrating because you are extremely limited in how many videos you can create with Veo 3. Anyone subscribing to AI Pro ($20 per month) gets three video generations per day. Upgrade to the $250 AI Ultra plan and that goes up to just five videos per day.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2105139 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="Veo 3 video" class="fullwidth full" height="563" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/unnamed.gif" width="1000" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Google says photo-to-video generation is rolling out in Gemini today, so you won't have to wait long to give it a shot, assuming you have a paid AI subscription. Free Gemini users won't have access to this feature at all.&lt;/p&gt;
&lt;p&gt;As we've been recently reminded, people can use AI video generation for nefarious purposes. Veo 3 does seem quite compliant, producing almost whatever you want unless it's overtly in opposition to Google's rules. The company says it is committed to safety with "red teaming" to aggressively test its AI systems to ensure they do not create unsafe content. All the videos created by Gemini with Veo 3 will also have Google's SynthID digital watermark, which helps identify them as artificial.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google is making it easier to create videos with Gemini, but you only get a few shots per day.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Gemini icon macro" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/04/Gemini-1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Gemini icon macro" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/04/Gemini-1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google's Veo 3 videos have propagated across the Internet since the model's debut in May, blurring the line between truth and fiction. Now, it's getting even easier to create these AI videos. The Gemini app is gaining photo-to-video generation, allowing you to upload a photo and turn it into a video. You don't have to pay anything extra for these Veo 3 videos, but the feature is only available to subscribers of Google's Pro and Ultra AI plans.&lt;/p&gt;
&lt;p&gt;When Veo 3 launched, it could conjure up a video based only on your description, complete with speech, music, and background audio. This has made Google's new AI videos staggeringly realistic—it's actually getting hard to identify AI videos at a glance. Using a reference photo makes it easier to get the look you want without tediously describing every aspect. This was an option in Google's Flow AI tool for filmmakers, but now it's in the Gemini app and web interface.&lt;/p&gt;
&lt;p&gt;To create a video from a photo, you have to select "Video" from the Gemini toolbar. Once this feature is available, you can then add your image and prompt, including audio and dialogue. Generating the video takes several minutes—this process takes a lot of computation, which is why video output is still quite limited.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Veo 3 videos are limited to 720p resolution and eight seconds in length, and there's no guarantee you'll like what Veo 3 spits out. That can be frustrating because you are extremely limited in how many videos you can create with Veo 3. Anyone subscribing to AI Pro ($20 per month) gets three video generations per day. Upgrade to the $250 AI Ultra plan and that goes up to just five videos per day.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2105139 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="Veo 3 video" class="fullwidth full" height="563" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/unnamed.gif" width="1000" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Google says photo-to-video generation is rolling out in Gemini today, so you won't have to wait long to give it a shot, assuming you have a paid AI subscription. Free Gemini users won't have access to this feature at all.&lt;/p&gt;
&lt;p&gt;As we've been recently reminded, people can use AI video generation for nefarious purposes. Veo 3 does seem quite compliant, producing almost whatever you want unless it's overtly in opposition to Google's rules. The company says it is committed to safety with "red teaming" to aggressively test its AI systems to ensure they do not create unsafe content. All the videos created by Gemini with Veo 3 will also have Google's SynthID digital watermark, which helps identify them as artificial.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/07/google-adds-photo-to-video-generation-with-veo-3-to-the-gemini-app/</guid><pubDate>Thu, 10 Jul 2025 15:02:08 +0000</pubDate></item><item><title>[NEW] Google’s open MedGemma AI models could transform healthcare (AI News)</title><link>https://www.artificialintelligence-news.com/news/google-open-medgemma-ai-models-healthcare/</link><description>&lt;p&gt;Instead of keeping their new MedGemma AI models locked behind expensive APIs, Google will hand these powerful tools to healthcare developers.&lt;/p&gt;&lt;p&gt;The new arrivals are called MedGemma 27B Multimodal and MedSigLIP and they’re part of Google’s growing collection of open-source healthcare AI models. What makes these special isn’t just their technical prowess, but the fact that hospitals, researchers, and developers can download them, modify them, and run them however they see fit.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-google-s-ai-meets-real-healthcare"&gt;Google’s AI meets real healthcare&lt;/h3&gt;&lt;p&gt;The flagship MedGemma 27B model doesn’t just read medical text like previous versions did; it can actually “look” at medical images and understand what it’s seeing. Whether it’s chest X-rays, pathology slides, or patient records potentially spanning months or years, it can process all of this information together, much like a doctor would.&lt;/p&gt;&lt;p&gt;The performance figures are quite impressive. When tested on MedQA, a standard medical knowledge benchmark, the 27B text model scored 87.7%. That puts it within spitting distance of much larger, more expensive models whilst costing about a tenth as much to run. For cash-strapped healthcare systems, that’s potentially transformative.&lt;/p&gt;&lt;p&gt;The smaller sibling, MedGemma 4B, might be more modest in size but it’s no slouch. Despite being tiny by modern AI standards, it scored 64.4% on the same tests, making it one of the best performers in its weight class. More importantly, when US board-certified radiologists reviewed chest X-ray reports it had written, they deemed 81% accurate enough to guide actual patient care.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-medsiglip-a-featherweight-powerhouse"&gt;MedSigLIP: A featherweight powerhouse&lt;/h3&gt;&lt;p&gt;Alongside these generative AI models, Google has released MedSigLIP. At just 400 million parameters, it’s practically featherweight compared to today’s AI giants, but it’s been specifically trained to understand medical images in ways that general-purpose models cannot.&lt;/p&gt;&lt;p&gt;This little powerhouse has been fed a diet of chest X-rays, tissue samples, skin condition photos, and eye scans. The result? It can spot patterns and features that matter in medical contexts whilst still handling everyday images perfectly well.&lt;/p&gt;&lt;p&gt;MedSigLIP creates a bridge between images and text. Show it a chest X-ray, and ask it to find similar cases in a database, and it’ll understand not just visual similarities but medical significance too.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-healthcare-professionals-are-putting-google-s-ai-models-to-work"&gt;Healthcare professionals are putting Google’s AI models to work&lt;/h3&gt;&lt;p&gt;The proof of any AI tool lies in whether real professionals actually want to use it. Early reports suggest doctors and healthcare companies are excited about what these models can do.&lt;/p&gt;&lt;p&gt;DeepHealth in Massachusetts has been testing MedSigLIP for chest X-ray analysis. They’re finding it helps spot potential problems that might otherwise be missed, acting as a safety net for overworked radiologists. Meanwhile, at Chang Gung Memorial Hospital in Taiwan, researchers have discovered that MedGemma works with traditional Chinese medical texts and answers staff questions with high accuracy.&lt;/p&gt;&lt;p&gt;Tap Health in India has highlighted something crucial about MedGemma’s reliability. Unlike general-purpose AI that might hallucinate medical facts, MedGemma seems to understand when clinical context matters. It’s the difference between a chatbot that sounds medical and one that actually thinks medically.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-why-open-sourcing-the-ai-models-is-critical-in-healthcare"&gt;Why open-sourcing the AI models is critical in healthcare&lt;/h3&gt;&lt;p&gt;Beyond generosity, Google’s decision to make these models is also strategic. Healthcare has unique requirements that standard AI services can’t always meet. Hospitals need to know their patient data isn’t leaving their premises. Research institutions need models that won’t suddenly change behaviour without warning. Developers need the freedom to fine-tune for very specific medical tasks.&lt;/p&gt;&lt;p&gt;By open-sourcing the AI models, Google has addressed these concerns with healthcare deployments. A hospital can run MedGemma on their own servers, modify it for their specific needs, and trust that it’ll behave consistently over time. For medical applications where reproducibility is crucial, this stability is invaluable.&lt;/p&gt;&lt;p&gt;However, Google has been careful to emphasise that these models aren’t ready to replace doctors. They’re tools that require human oversight, clinical correlation, and proper validation before any real-world deployment. The outputs need checking, the recommendations need verifying, and the decisions still rest with qualified medical professionals.&lt;/p&gt;&lt;p&gt;This cautious approach makes sense. Even with impressive benchmark scores, medical AI can still make mistakes, particularly when dealing with unusual cases or edge scenarios. The models excel at processing information and spotting patterns, but they can’t replace the judgment, experience, and ethical responsibility that human doctors bring.&lt;/p&gt;&lt;p&gt;What’s exciting about this release isn’t just the immediate capabilities, but what it enables. Smaller hospitals that couldn’t afford expensive AI services can now access cutting-edge technology. Researchers in developing countries can build specialised tools for local health challenges. Medical schools can teach students using AI that actually understands medicine.&lt;/p&gt;&lt;p&gt;The models are designed to run on single graphics cards, with the smaller versions even adaptable for mobile devices. This accessibility opens doors for point-of-care AI applications in places where high-end computing infrastructure simply doesn’t exist.&lt;/p&gt;&lt;p&gt;As healthcare continues grappling with staff shortages, increasing patient loads, and the need for more efficient workflows, AI tools like Google’s MedGemma could provide some much-needed relief. Not by replacing human expertise, but by amplifying it and making it more accessible where it’s needed most.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Owen Beard)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Tencent improves testing creative AI models with new benchmark&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Instead of keeping their new MedGemma AI models locked behind expensive APIs, Google will hand these powerful tools to healthcare developers.&lt;/p&gt;&lt;p&gt;The new arrivals are called MedGemma 27B Multimodal and MedSigLIP and they’re part of Google’s growing collection of open-source healthcare AI models. What makes these special isn’t just their technical prowess, but the fact that hospitals, researchers, and developers can download them, modify them, and run them however they see fit.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-google-s-ai-meets-real-healthcare"&gt;Google’s AI meets real healthcare&lt;/h3&gt;&lt;p&gt;The flagship MedGemma 27B model doesn’t just read medical text like previous versions did; it can actually “look” at medical images and understand what it’s seeing. Whether it’s chest X-rays, pathology slides, or patient records potentially spanning months or years, it can process all of this information together, much like a doctor would.&lt;/p&gt;&lt;p&gt;The performance figures are quite impressive. When tested on MedQA, a standard medical knowledge benchmark, the 27B text model scored 87.7%. That puts it within spitting distance of much larger, more expensive models whilst costing about a tenth as much to run. For cash-strapped healthcare systems, that’s potentially transformative.&lt;/p&gt;&lt;p&gt;The smaller sibling, MedGemma 4B, might be more modest in size but it’s no slouch. Despite being tiny by modern AI standards, it scored 64.4% on the same tests, making it one of the best performers in its weight class. More importantly, when US board-certified radiologists reviewed chest X-ray reports it had written, they deemed 81% accurate enough to guide actual patient care.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-medsiglip-a-featherweight-powerhouse"&gt;MedSigLIP: A featherweight powerhouse&lt;/h3&gt;&lt;p&gt;Alongside these generative AI models, Google has released MedSigLIP. At just 400 million parameters, it’s practically featherweight compared to today’s AI giants, but it’s been specifically trained to understand medical images in ways that general-purpose models cannot.&lt;/p&gt;&lt;p&gt;This little powerhouse has been fed a diet of chest X-rays, tissue samples, skin condition photos, and eye scans. The result? It can spot patterns and features that matter in medical contexts whilst still handling everyday images perfectly well.&lt;/p&gt;&lt;p&gt;MedSigLIP creates a bridge between images and text. Show it a chest X-ray, and ask it to find similar cases in a database, and it’ll understand not just visual similarities but medical significance too.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-healthcare-professionals-are-putting-google-s-ai-models-to-work"&gt;Healthcare professionals are putting Google’s AI models to work&lt;/h3&gt;&lt;p&gt;The proof of any AI tool lies in whether real professionals actually want to use it. Early reports suggest doctors and healthcare companies are excited about what these models can do.&lt;/p&gt;&lt;p&gt;DeepHealth in Massachusetts has been testing MedSigLIP for chest X-ray analysis. They’re finding it helps spot potential problems that might otherwise be missed, acting as a safety net for overworked radiologists. Meanwhile, at Chang Gung Memorial Hospital in Taiwan, researchers have discovered that MedGemma works with traditional Chinese medical texts and answers staff questions with high accuracy.&lt;/p&gt;&lt;p&gt;Tap Health in India has highlighted something crucial about MedGemma’s reliability. Unlike general-purpose AI that might hallucinate medical facts, MedGemma seems to understand when clinical context matters. It’s the difference between a chatbot that sounds medical and one that actually thinks medically.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-why-open-sourcing-the-ai-models-is-critical-in-healthcare"&gt;Why open-sourcing the AI models is critical in healthcare&lt;/h3&gt;&lt;p&gt;Beyond generosity, Google’s decision to make these models is also strategic. Healthcare has unique requirements that standard AI services can’t always meet. Hospitals need to know their patient data isn’t leaving their premises. Research institutions need models that won’t suddenly change behaviour without warning. Developers need the freedom to fine-tune for very specific medical tasks.&lt;/p&gt;&lt;p&gt;By open-sourcing the AI models, Google has addressed these concerns with healthcare deployments. A hospital can run MedGemma on their own servers, modify it for their specific needs, and trust that it’ll behave consistently over time. For medical applications where reproducibility is crucial, this stability is invaluable.&lt;/p&gt;&lt;p&gt;However, Google has been careful to emphasise that these models aren’t ready to replace doctors. They’re tools that require human oversight, clinical correlation, and proper validation before any real-world deployment. The outputs need checking, the recommendations need verifying, and the decisions still rest with qualified medical professionals.&lt;/p&gt;&lt;p&gt;This cautious approach makes sense. Even with impressive benchmark scores, medical AI can still make mistakes, particularly when dealing with unusual cases or edge scenarios. The models excel at processing information and spotting patterns, but they can’t replace the judgment, experience, and ethical responsibility that human doctors bring.&lt;/p&gt;&lt;p&gt;What’s exciting about this release isn’t just the immediate capabilities, but what it enables. Smaller hospitals that couldn’t afford expensive AI services can now access cutting-edge technology. Researchers in developing countries can build specialised tools for local health challenges. Medical schools can teach students using AI that actually understands medicine.&lt;/p&gt;&lt;p&gt;The models are designed to run on single graphics cards, with the smaller versions even adaptable for mobile devices. This accessibility opens doors for point-of-care AI applications in places where high-end computing infrastructure simply doesn’t exist.&lt;/p&gt;&lt;p&gt;As healthcare continues grappling with staff shortages, increasing patient loads, and the need for more efficient workflows, AI tools like Google’s MedGemma could provide some much-needed relief. Not by replacing human expertise, but by amplifying it and making it more accessible where it’s needed most.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Owen Beard)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Tencent improves testing creative AI models with new benchmark&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/google-open-medgemma-ai-models-healthcare/</guid><pubDate>Thu, 10 Jul 2025 15:17:31 +0000</pubDate></item><item><title>[NEW] Nvidia reportedly plans to release new AI chip designed for China (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/10/nvidia-reportedly-plans-to-release-new-ai-chip-designed-for-china/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/GettyImages-2216028442.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nvidia seems determined to find a way to sell AI chips in China despite U.S. export restrictions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The semiconductor giant is planning to launch an AI chip specifically for the Chinese market as early as September, as originally reported by the Financial Times.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This AI chip would be based on Nvidia’s Blackwell RTX Pro 6000 processor, which is already modified to meet the existing AI chip restrictions, the Financial Times added. These chips wouldn’t include high-bandwidth memory or NVLink, Nvidia’s high-speed but low-latency communication interface, both of which are features of the company’s advanced AI chips.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last month, Nvidia CEO Jensen Huang said the company would no longer include the Chinese market in its revenue and profit forecasts. Maybe that will be a short-lived change.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia declined to comment on the news. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;An Nvidia spokesperson added, “With the current export controls, we are effectively out of the China datacenter market, which is now served only by competitors such as Huawei. China has one of the largest populations of developers in the world, creating open-source foundation models and non-military applications used globally. While security is paramount, every one of those applications should run best on the U.S. AI stack.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/GettyImages-2216028442.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nvidia seems determined to find a way to sell AI chips in China despite U.S. export restrictions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The semiconductor giant is planning to launch an AI chip specifically for the Chinese market as early as September, as originally reported by the Financial Times.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This AI chip would be based on Nvidia’s Blackwell RTX Pro 6000 processor, which is already modified to meet the existing AI chip restrictions, the Financial Times added. These chips wouldn’t include high-bandwidth memory or NVLink, Nvidia’s high-speed but low-latency communication interface, both of which are features of the company’s advanced AI chips.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last month, Nvidia CEO Jensen Huang said the company would no longer include the Chinese market in its revenue and profit forecasts. Maybe that will be a short-lived change.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia declined to comment on the news. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;An Nvidia spokesperson added, “With the current export controls, we are effectively out of the China datacenter market, which is now served only by competitors such as Huawei. China has one of the largest populations of developers in the world, creating open-source foundation models and non-military applications used globally. While security is paramount, every one of those applications should run best on the U.S. AI stack.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/10/nvidia-reportedly-plans-to-release-new-ai-chip-designed-for-china/</guid><pubDate>Thu, 10 Jul 2025 15:29:32 +0000</pubDate></item><item><title>[NEW] How AI will accelerate biomedical research and discovery (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/podcast/how-ai-will-accelerate-biomedical-research-and-discovery/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Illustrated images of Peter Lee, Daphne Koller, Noubar Afeyan, and Dr. Eric Topol for the Microsoft Research Podcast" class="wp-image-1144053" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Episode8-PeterEricNoubarDaphne-AIRevolution_Hero_Feature_No_Text_1400x788.jpg" width="1400" /&gt;&lt;/figure&gt;






&lt;p&gt;In November 2022, OpenAI’s ChatGPT kick-started a new era in AI. This was followed less than a half year later by the release of GPT-4. In the months leading up to GPT-4’s public release, Peter Lee, president of Microsoft Research, cowrote a book full of optimism for the potential of advanced AI models to transform the world of healthcare. What has happened since? In this special podcast series, &lt;em&gt;The AI Revolution in Medicine, Revisited&lt;/em&gt;, Lee revisits the book, exploring how patients, providers, and other medical professionals are experiencing and using generative AI today while examining what he and his coauthors got right—and what they didn’t foresee.&lt;/p&gt;



&lt;p&gt;In this episode, Daphne Koller&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Noubar Afeyan&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, and Dr. Eric Topol&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, leaders in AI-driven medicine, join Lee to explore the rapidly evolving role of AI across the biomedical and healthcare landscape. Koller, founder and CEO of Insitro, shares how machine learning is transforming drug discovery, especially target identification for complex diseases like ALS, by uncovering biological patterns across massive datasets. Afeyan, founder and CEO of Flagship Pioneering and co-founder and chairman of Moderna, discusses how AI is being applied across biotech research and development, from protein design to autonomous science platforms. Topol, executive vice president of Scripps Research and director of the Scripps Research Translational Institute, highlights how AI can &lt;em&gt;today&lt;/em&gt; help mitigate and prevent the core diseases that erode our health and the possibility of realizing a virtual cell. Through his conversations with the three, Lee investigates how AI is reshaping the discovery, deployment, and delivery of medicine.&amp;nbsp;&lt;/p&gt;



&lt;div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow"&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h2 class="wp-block-heading h5" id="learn-more-1"&gt;Learn more:&lt;/h2&gt;




&lt;/div&gt;







&lt;section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast"&gt;
	
&lt;/section&gt;


&lt;div class="wp-block-msr-show-more"&gt;
	&lt;div class="bg-neutral-100 p-5"&gt;
		&lt;div class="show-more-show-less"&gt;
			&lt;div&gt;
				&lt;span&gt;
					

&lt;h2 class="wp-block-heading" id="transcript"&gt;Transcript&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;[MUSIC]&lt;/p&gt;



&lt;p&gt;[BOOK PASSAGE]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;PETER LEE: &lt;/strong&gt;“Can GPT-4 indeed accelerate the progression of medicine&lt;strong&gt; &lt;/strong&gt;… ? It seems like a tall order, but if I had been told six months ago that it could rapidly summarize any published paper, that alone would have satisfied me as a strong contribution to research productivity. … But now that I’ve seen what GPT-4 can do with the healthcare process, I expect a lot more in the realm of research.”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[END OF BOOK PASSAGE]&lt;/p&gt;



&lt;p&gt;[THEME MUSIC]&lt;/p&gt;



&lt;p&gt;This is &lt;em&gt;The AI Revolution in Medicine, Revisited&lt;/em&gt;. I’m your host, Peter Lee.&lt;/p&gt;



&lt;p&gt;Shortly after OpenAI’s GPT-4 was publicly released, Carey Goldberg, Dr. Zak Kohane, and I published &lt;em&gt;The AI Revolution in Medicine &lt;/em&gt;to help educate the world of healthcare and medical research about the transformative impact this new generative AI technology could have. But because we wrote the book when GPT-4 was still a secret, we had to speculate. Now, two years later, what did we get right, and what did we get wrong?&lt;/p&gt;



&lt;p&gt;In this series, we’ll talk to clinicians, patients, hospital administrators, and others to understand the reality of AI in the field and where we go from here.&lt;/p&gt;



&lt;p&gt;[THEME MUSIC FADES]&lt;/p&gt;



&lt;p&gt;The book passage I read at the top was from “Chapter 8: Smarter Science,” which was written by Zak.&lt;/p&gt;



&lt;p&gt;In writing the book, we were optimistic about AI’s potential to accelerate biomedical research and help get new and much-needed treatments and drugs to patients sooner. One area we explored was generative AI as a designer of clinical trials. We looked at generative AI’s adeptness at summarizing helping speed up pre-trial triage and research. We even went so far as to predict the arrival of a large language model that can serve as a central intellectual tool.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For a look at how AI is impacting biomedical research today, I’m excited to welcome Daphne Koller, Noubar Afeyan, and Eric Topol.&amp;nbsp;&lt;/p&gt;



				&lt;/span&gt;
				&lt;span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1"&gt;
					



&lt;p&gt;Daphne Koller is the CEO and founder of Insitro, a machine learning-driven drug discovery and development company that recently made news for its identification of a novel drug target for ALS and its collaboration with Eli Lilly to license Lilly’s biochemical delivery systems. Prior to founding Insitro, Daphne was the co-founder, co-CEO, and president of the online education platform Coursera.&lt;/p&gt;



&lt;p&gt;Noubar Afeyan is the founder and CEO of Flagship Pioneering, which creates biotechnology companies focused on transforming human health and environmental sustainability. He is also co-founder and chairman of the messenger RNA company Moderna. An entrepreneur and biochemical engineer, Noubar has numerous patents to his name and has co-founded many startups in science and technology.&lt;/p&gt;



&lt;p&gt;Dr. Eric Topol is the executive vice president of the biomedical research non-profit Scripps Research, where he founded and now directs the Scripps Research Translational Institute. One of the most cited researchers in medicine, Eric has focused on promoting human health and individualized medicine through the use of genomic and digital data and AI.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;These three are likely to have an outsized influence on how drugs and new medical technologies soon will be developed.&lt;/p&gt;



&lt;p&gt;[TRANSITION MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Here’s my interview with Daphne Koller:&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Daphne, I’m just thrilled to have you join us.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DAPHNE KOLLER: &lt;/strong&gt;Thank you for having me, Peter. It’s a pleasure to be here.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Well, you know, you’re quite well-known across several fields. But maybe for some audience members of this podcast, they might not have encountered you before. So where I’d like to start is a question I’ve been asking all of our guests.&lt;/p&gt;



&lt;p&gt;How would you describe what you do? And the way I kind of put it is, you know, how do you explain to someone like your parents what you do for a living?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER: &lt;/strong&gt;So that answer obviously has shifted over the years.&lt;/p&gt;



&lt;p&gt;What I would say now is that we are working to leverage the incredible convergence of very powerful technologies, of which AI is one but not the only one, to change the way in which we discover and develop new treatments for diseases for which patients are currently suffering and even dying.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;You know, I think I’ve known you for a long time.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; Longer than I think either of us care to admit.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;[LAUGHS] In fact, I think I remember you even when you were still a graduate student. But of course, I knew you best when you took up your professorship at Stanford. And I always, in my mind, think of you as a computer scientist and a machine learning person. And in fact, you really made a big name for yourself in computer science research in machine learning.&lt;/p&gt;



&lt;p&gt;But now you’re, you know, leading one of the most important biotech companies on the planet. How did that happen?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; So people often think that this is a recent transition. That is, after I left Coursera, I looked around and said, “Hmm. What should I do next? Oh, biotech seems like a good thing,” but that’s actually not the way it transpired.&lt;/p&gt;



&lt;p&gt;This goes all the way back to my early days at Stanford, where, in fact, I was, you know, as a young faculty member in machine learning, because I was the first machine learning hire into Stanford’s computer science department, I was looking for really exciting places in which this technology could be deployed, and applications back then, because of scarcity of data, were just not that inspiring.&lt;/p&gt;



&lt;p&gt;And so I looked around, and this was around the late ’90s, and realized that there was interesting data emerging in biology and medicine. My first application actually was in, interestingly, in epidemiology—patient tracking and tuberculosis. You know, you can think of it as a tiny microcosm of the very sophisticated models that COVID then enabled in a much later stage.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; And so initially, this was based almost entirely on just technical interest. It’s kind of like, oh, this is more interesting as a question to tackle than spam filtering. But then I became interested in biology in its own right, biology and medicine, and ended up having a bifurcated existence as a Stanford professor where half my lab continued to do core computer science research published in, you know, NeurIPS and ICML. And the other half actually did biomedical research that was published in, you know, &lt;em&gt;Nature Cell [and] Science&lt;/em&gt;. So that was back in, you know, the early, early 2000s, and for most of my Stanford career, I continued to have both interests.&lt;/p&gt;



&lt;p&gt;And then the Coursera experience kind of took me out of Stanford and put me in an industry setting for the first time in my life actually.&lt;strong&gt; &lt;/strong&gt;But then when my time at Coursera came to an end, you know, I’d been there for five years. And if you look at the timeline, I left Stanford in early 2012, right as the machine learning revolution was starting. So I missed the beginning.&lt;/p&gt;



&lt;p&gt;And it was only in like 2016 or so that, as I picked my head up over the trenches, like, “Oh my goodness, this technology is going to change the world.” And I wanted to deploy that big thing towards places where it would have beneficial impact on the world, like to make the world a better place.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; And so I decided that one of the areas where I could make a unique, differentiated impact was in really bringing AI and machine learning to the life sciences, having spent, you know, the majority of my career at the boundary of those two disciplines. And notice I say “boundary” with deliberation because there wasn’t very much of an intersection.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; I felt like I could do something that was unique.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;So just to stick on you for a little bit longer, you know, we have been sort of getting into your origin story about what we call AI today—but machine learning, so deep learning.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And, you know, there has always been a kind of an emotional response for people like you and me and now the general public about their first encounters with what we now call generative AI. I’d love to hear what your first encounter was with generative AI and how you reacted to this.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; I think my first encounter was actually an indirect one. Because, you know, the earlier generations of generative AI didn’t directly touch our work at Insitro&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And yet at the same time, I had always had an interest in computer vision. That was a large part of my non-bio work when I was at Stanford.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And so some of my earlier even presentations, when I was trying to convey to people back in 2016 how this technology was going to transform the world, I was talking about the incredible progress in image recognition that had happened up until that point.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So my first interaction was actually in the generative AI for images, where you are able to go the other way …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER: &lt;/strong&gt;… where you can take a verbal description of an image and create—and this was back in the days when the images weren’t particularly photorealistic, but still a natural language description to an image was magic given that only two or three years before that, we were barely able to look at an image and write a short phrase saying, “This is a dog on the beach.” And so that arc, that hockey curve, was just mind blowing to me.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Did you have moments of skepticism?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER: &lt;/strong&gt;Yeah, I mean the early, you know, early versions of ChatGPT, where it was more like parlor tricks and poking it a little bit revealed all of the easy ways that one could break it and make it do really stupid things. I was like, yeah, OK, this is kind of cute, but is it going to actually make a difference? Is it going to solve a problem that matters?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And I mean, obviously, I think now everyone agrees that the answer is yes, although there are still people who are like, yeah, but maybe it’s around the edges. I’m not among them, by the way, but … yeah, so initially there were like, “Yeah, this is cute and very impressive, but is it going to make a difference to a problem that matters?”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&lt;strong&gt; &lt;/strong&gt;So now, maybe this is a good time to get into what you’ve been doing with ALS [amyotrophic lateral sclerosis]. You know, there’s a knee-jerk reaction from the technology side to focus on designing small molecules, on predicting, you know, their properties, you know, maybe binding affinity or aspects of ADME [absorption, distribution, metabolism, and excretion], you know, like absorption or dispersion or whatever.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And all of that is very useful, but if I understand the work on ALS, you went to a much harder place, which is to actually identify and select targets.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; That’s right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; So first off, just for the benefit of the standard listeners of this podcast, explain what that problem is in general.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; No, for sure. And I think maybe I’ll start by just very quickly talking about the drug discovery and development arc, …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; … which, by and large, consists of three main phases. That’s the standard taxonomy.&amp;nbsp;The first is what’s called sometimes target discovery or identifying a therapeutic hypothesis, which looks like: if I modulate this target in this disease, something beneficial will happen.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Then, you have to take that target and turn it into a molecule that you can actually put into a person. It could be a small molecule. It could be a large molecule like an antibody, whatever. And then you have that construct, that molecule. And the last piece is you put it into a person in the context of a clinical trial, and you measure what has happened. And there’s been AI deployed towards each of those three stages in different ways.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The last one is mostly like an efficiency gain. You know, the trial is kind of already defined, and you want to deploy technology to make it more efficient and effective, which is great because those are expensive operations.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; The middle one is where I would say the vast majority of efforts so far has been deployed in AI because it is a nice, well-defined problem. It doesn’t mean it’s easy, but it’s one where you can define the problem. It is, &lt;em&gt;I need to inhibit this protein by this amount, and the molecule needs to be soluble and whatever and go past the blood-brain barrier&lt;/em&gt;. And you know probably within a year and a half or so, or two, if you succeeded or not.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The first stage is the one where I would say the least amount of energy has gone because when you’re uncovering a novel target in the context of an indication, you don’t know that you’ve been successful until you go &lt;em&gt;all the way&lt;/em&gt; to the end, which is the clinical trial, which is what makes this a long and risky journey. And not a lot of people have the appetite or the capital to actually do that.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;However, in my opinion, and that of, I think, quite a number of others, it is where the biggest impact can be made. And the reason is that while pharma has its deficiencies, making good molecules is actually something they’re pretty good at.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It might take them longer than it should, maybe it’s not as efficient as it could be, but at the end of the day, if you tell them to drug A target, pharma is actually pretty good at generating those molecules. However, when you put those molecules into the clinic, 90% of them fail. And the reason they fail is not by and large because the molecule wasn’t good. In the majority of cases, it’s because the target you went after didn’t do anything useful in the context of the patient population in which you put it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And so in order to fix the inefficiency of this industry, which is &lt;em&gt;incredible&lt;/em&gt; inefficiency, you need to address the problem at the root, and the root is picking the right targets to go after. And so that is what we elected to do.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It doesn’t mean we don’t make molecules. I mean, of course, you can’t just end up with a target because a target is not actionable. You need to turn it into a molecule. And we absolutely do that. And by the way, the partnership with Lilly&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; is actually one where they help us make a molecule.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER: &lt;/strong&gt;I mean, it’s our target. It’s our program. But Lilly is deploying its very state-of-the-art molecule-making capabilities to help us turn that target into a drug.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;So let’s get now into the machine learning of this. Again, this just strikes me as such a difficult problem to solve.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; So how does machine learning … how does AI help you?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; So I think when you look at how people currently select targets, it’s a combination of oftentimes at this point, with an increasing respect for the power of human genetics, some search for a genetic association, oftentimes with a human-defined, highly subjective, highly noisy clinical outcome, like some ICD [International Classification of Diseases] code.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And those are often underpowered and very difficult to deconvolute the underlying biology. You combine that with some mechanistic interrogation in a highly reductionist model system looking at a small number of readouts, biochemical readouts, that a biologist thinks are relevant to the disease. Like does this make this, whatever, cholesterol go up or amyloid beta go down? Or whatever. And then you take that as the second stage, and you pick, based on typically human intuition about, &lt;em&gt;Oh, this one looks good to me&lt;/em&gt;, and then you take that forward.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;What we’re doing is an attempt to be as unbiased and holistic as possible. So, first of all, rather than rely on human-defined clinical endpoints, like this person has been diagnosed with diabetes or fatty liver, we try and measure as much as we can a holistic physiological state and then use machine learning to find structure, patterns &lt;em&gt;in&lt;/em&gt; that human physiological readouts, imaging readouts, and omics readouts from blood, from tissue, different kinds of imaging, and say, these are different vectors that this disease takes, this group of individuals, and here’s a different group of individuals that maybe from a diagnostical perspective are all called the same thing, but they are actually exhibiting a very different biology underlying it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And so that is something that doesn’t emerge when a human being takes a reductionist view to looking at this high-content data, and oftentimes, they don’t even look at it and produce an ICD code.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right. Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; The same approach, actually even the same code base, is taken in the cellular data. So we don’t just say, “Well, the thing that matters is, you know, the total amount of lipid in the cell or whatever.” Rather, we say, “Let’s look at multiple readouts, multiple ways of looking at the cells, combine them using the power of machine learning.” And again, looking at imaging readouts where a human’s eyes just glaze over looking at even a few dozen cells, far less a few hundreds of millions of cells, and understand what are the different biological processes that are going on. What are the vectors that the disease might take you in this direction, in this group of cells, or in that direction?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And then importantly, we take all of that information from the human side, from the cellular side, across these different readouts, and we combine them using an integrative approach that looks at the combined weight of evidence and says, these are the targets that I have the greatest amount of conviction about by looking across all of that information. Whereas we know, and we know this, I’m sure you’ve seen this analysis done for clinicians, a human being typically is able to keep three or four things in their head at the same time.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; A really &lt;em&gt;good&lt;/em&gt; human being who’s really expert at what they do can maybe get to six to eight.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; The machine learning has no problem doing a few hundred.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; And so you put that together, and that allows you, to your earlier question, really select the targets around which you have the highest conviction. And then those are the ones that we then prioritize for interrogation in more expensive systems like mice and monkeys and then at the end of the day pick the small handful that one can afford to actually take into clinical trials.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;So now, Insitro recently received $25 million in milestone payments from Bristol Myers Squibb&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; after discovering and selecting a novel drug target for ALS. Can you tell us a little bit more about that? &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; We are incredibly excited about the first novel target, and there is a couple of others just behind it in line that seem, you know, quite efficacious, as well, that truly seem to reverse, albeit in a cellular system, what we now understand to be ALS pathology across multiple different dimensions. There’s been obviously many attempts made to try and address ALS, which by the way, horrible, horrible disease, worse than most cancers. It kills you almost inevitably in three to five years in a particularly horrific way.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And what we have in our hands is a target that seems to revert a lot of the pathologies that are associated with the disease, which we now understand has to do with the mis-splicing of multiple proteins within the cell and creating defective versions of those proteins that are just not operational. And we are seeing reversion of many of those.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So can I tell you for sure it’ll work in a human? No, there’s many steps between now and then. But we couldn’t be more excited about the opportunity to provide what we hope will be a disease-modifying intervention for these patients who really desperately need something.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Well, it’s certainly been making waves in the biotech and biomedical world.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; Thank you.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;So we’ll be really watching very closely.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So, you know, I think just reflecting on, you know, what we missed and what we got right in our book, I think in our book, we did have the insight that there would be an ability to connect, say, genotypic and phenotypic data and, you know, just broadly the kinds of clinical measurements that get made on real patients and that these things could be brought together. And I think the work that you’re doing really illustrates that in a very, very sophisticated, very ambitious way.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But the fact that this could be connected all the way down to the biology, to the biochemistry, I think we didn’t have any clue what would happen, at least not this quickly.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; Well, I think the …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;And I realize, you’ve been at this for quite a few years, but still, it’s quite amazing.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; The thread that connects them is human genetics. And I think that has, to us, been, sort of, the, kind of, the connective tissue that allows you to translate across different systems and say, “What does this gene do? What does this gene do in this organ and in that organ? What does it do in this type of cell and in that type of cell?”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And then use that as sort of the thread, if you will, that follows the impact of modulating this gene all the way from the simple systems where you can do the experiment to the complex systems where you can’t do the experiment until the very end, but you have the human genetics as a way of looking at the statistics and understanding what the impact might be.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;So I’d like to now switch gears and take … I want to take two steps in the remainder of this conversation towards the future. So one step into that future, of course, we’re living through now, which is just all of the crazy pace of work and advancement in generative AI generally, you know, just the scale of transformers, of post-training, and now inference scale and reasoning models and so on. And where do you see all of that going with respect to the goals that you have and that Insitro has?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; So I think first and foremost is the parallel, if you will, to the predictions that you focused on in your book, which is this will transform a lot of the core data processing tasks, the information tasks. And sure, the doctors and nurses is one thing. But if you just think of clinical trial operations or the submission of regulatory documents, these are all kind of simple data … they’re not simple, obviously, but they’re data processing tasks. They involve natural language. That’s not going to be our focus, but I hope that others will use that to make clinical trials faster, more efficient, less expensive.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;There’s already a lot of progress that’s happening on the molecular design side of things and taking hypotheses and turning them quickly and effectively into molecules. As I said, this is part of our work that we absolutely do and we don’t talk about it very much, simply because it’s a very crowded landscape and a lot of companies are engaged on that. But I think it’s really important to be able to take biological insights and turn them into new molecules.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And then, of course, the transformer models and their likes play a very significant role in that sort of turning insights into molecules because you can have foundation models for proteins. There are increasing efforts to create foundation models for other categories of molecules. And so that will undoubtedly accelerate the process by which you can quickly generate different molecular hypotheses and test them and learn from what you did so that you can do fewer iterations …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; … before you converge on a successful molecule.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I do think that arguably the biggest impact as yet to be had is in that understanding of core human biology and what are the right ways to intervene in it. And that plays a role in a couple different ways. First of all, it certainly plays a role in which … if we are able to understand the human physiological state and, you know, the state of different systems all the way down to the cell level, that will inform our ability to pick hypotheses that are more likely to actually impact the right biologies underneath.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yep. Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; And the more data we’re able to collect about humans and about cells, the more successful our models will be at representing that human physiological state or the cell biological state and making predictions reliably on the impact of these interventions.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The other side of it, though, and this comes back, I think, to themes that were very much in your book, is this will impact not only the early stages of which hypotheses we interrogate, which molecules we move forward, but also hopefully at the end of the day, which molecule we prescribe to which patient.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; And I think there’s been obviously so much narrative over the years about precision medicine, personalized medicine, and very little of that has come to fruition, with the exception of, you know, certain islands in oncology, primarily on genetically driven cancers.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But I think the opportunity is still there. We just haven’t been able to bring it to life because of the lack of the right kind of data. And I think with the increasing amount of human, kind of, foundational data that we’re able to acquire, things that are not sort of distilled through the eye of a clinician, for example, …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; … but really measurements of human pathology, we can start to get to some of that precision, carving out of the human population and then get to a world where we can prescribe the right medicine to the right patient and not only in cancer but also in other diseases that are also not a single disease.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;All right, so now to wrap up this time together, I always try to ask one more provocative last question. One of the dreams that comes naturally to someone like me or any of my colleagues, probably even to you, is this idea of, you know, wouldn’t it be possible someday to have a foundation model for biology or for human biology or foundation model for the human cell or something along these lines?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And in fact, there are, of course, you and I are both aware of people who are taking that idea seriously and chasing after it. I have people in our labs that think hard about this kind of thing. Is it a reasonable thought at all?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; I have learned over the years to avoid saying the word &lt;em&gt;never&lt;/em&gt; because technology proceeds in ways that you often don’t expect. And so will we at some point be able to measure the cell in enough different ways across enough different channels at the same time that you can piece together what a cell does? I think that is eminently feasible, not today, but over time.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I don’t think it’s feasible using today’s technology, although the efforts to get there may expose where the biggest opportunities lie to, you know, build that next layer. So I think it’s good that people are working on really hard problems. I would also point out that even if one were to solve that really challenging problem of creating a model of &lt;em&gt;a cell&lt;/em&gt;, there is thousands of different types of cells within the human body.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;They’re very different. They also talk to each other …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; … both within the cell type and across different cell types. So the combinatorial complexity of that system is, I think, unfathomable to many people. I mean, I would say to all of us.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER: &lt;/strong&gt;And so even from that very lofty goal, there is multiple big steps that would need to be taken to a &lt;em&gt;mechanistic&lt;/em&gt; model of the full organism. So will we ever get there? Again, you know, I don’t see a reason why this is impossible to do. So I think over time, technology will get better and will allow us to build more and more elaborate models of more and more complex systems.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Patients can’t wait …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right. Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; … for that to happen in order for us to get them better medicines. So I think there is a great basic science initiative on that side of things. And, in parallel, we need to make do with the data that we have or can collect or can print. We print a lot of data in our internal wet labs and get to drugs that are effective even though they don’t benefit from having a full-blown mechanistic model.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Last question: where do you think we’ll be in five years?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; Phew. If I had answered that question five years ago, I would have been very badly embarrassed at the inaccuracy of my answer. [LAUGHTER] So I will not answer it today either.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I will say that the thing about exponential curves is that they are very, very tricky, and they move in unexpected ways. I would hope that in five years, we will have made a sufficient investment in the generation of scientific data that we will be able to move beyond data that was generated entirely by humans and therefore insights that are derivative of what people already know to things that are truly novel discoveries.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And I think in order to do that in, you know, math, maybe because math is entirely conceptual, maybe you can do that today. Math is effectively a construct of the human mind. I don’t think biology is a construct of the human mind, and therefore one needs to collect enough data to really build those models that will give rise to those novel insights.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And that’s where I hope we will have made considerable progress in five years.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Well, I’m with you. I hope so, too. Well, you know, thank you, Daphne, so much for this conversation. I learn a lot talking to you, and it was great to, you know, connect again on this. And congratulations on all of this success. It’s really groundbreaking.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; Thank you very much, Peter. It was a pleasure chatting with you, as well.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[TRANSITION MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; I still think of Daphne first and foremost as an AI researcher. And for sure, her research work in machine learning continues to be incredibly influential to this day. But it’s her work on AI-enhanced drug development that now is on the verge of making a really big difference on some of the most difficult diseases afflicting people today.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In our book, Carey, Zak, and I predicted that AI might be a meaningful accelerant in biomedical research, but I don’t know that we foresaw the incredible potential specifically in drug development.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Today, we’re seeing a flurry of activity at companies, universities, and startups on generative AI systems that aid and maybe even completely automate the design of new molecules as drug candidates. But now, in our conversation with Daphne, seeing AI go even further than that to do what one might reasonably have assumed to be impossible, to identify and select novel drug targets, especially for a neurodegenerative disease like ALS, it’s just, well, mind blowing. &lt;/p&gt;



&lt;p&gt;Let’s continue our deep dive on AI and biomedical research with this conversation with Noubar Afeyan:&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Noubar, thanks so much for joining. I’m really looking forward to this conversation.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;NOUBAR AFEYAN: &lt;/strong&gt;Peter, thanks. Thrilled to be here.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; While I think most of the listeners to this podcast have heard of Flagship Pioneering&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, it’s still worth hearing from you, you know, what is Flagship? And maybe a little bit about your background. And finally, you found a way to balance science and business creation. And so, you know, your approach and philosophy to all of that.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN: &lt;/strong&gt;Well, great. So maybe I’ll just start out by way of quick background. You know, my … and since we’re going talk about AI, I’ll also highlight my first contact with the topic of AI. So as an undergraduate in 1980 up at McGill University, I was an engineering student, but I was really captivated by, at that time, the talk on the campus around the expert system, heuristic-based, rule-based kind of programs.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; And so actually I had the dubious distinction of writing my one and only college newspaper article. [LAUGHTER] That was a short career. And it was all about how artificial intelligence would be impacting medicine, would be impacting, you know, speech capture, translation, and some of the ideas that were there that it’s interesting to see now 45 years later re-emerge with some of the new learning-based models.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;My journey after college ended up taking me into biotechnology. In the early ’80s, I came to MIT to do a PhD. At the time, the field was brand new. I ended up being the first PhD graduate from MIT in this combination biology and engineering degree. And since then, I’ve basically been—so since 1987—a founder, a technologist in the space of biotechnology for human health and as well for planetary health.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And then in 1999/2000 formed what is now Flagship Pioneering, which essentially was an attempt to bring together the three elements of what we know are important in startups. That is scientific capital, human capital, and financial capital. Right now, startups get that from different places. The science in our fields mostly come from academia, research hospitals. The human capital comes from other startups …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; … or large companies or some academics leave. And then the financial capital is usually venture capital, but there’s also now more and more other deeper pockets of money.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;What we thought was, what if all that existed in one entity and instead of having to convince each other how much they should believe the other if we just said, “Let’s use that power to go work on much further out things”? But in a way where nobody would believe it in the beginning, but we could give ourselves a little bit of time to do impactful big things.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Twenty-five years later, that’s the road we’ve stayed on.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; OK. So let’s get into AI. Now, you know, what I’ve been asking guests is kind of an origin story. And there’s the origin story of contact with AI, you know, before the emergence of generative AI and afterwards. I don’t think there’s much of a point to asking you the pre-ChatGPT. But … so let’s focus on your first encounter with ChatGPT or generative AI. When did that happen, and what went through your head?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; Yeah. So, if you permit me, Peter, just for very briefly, let me actually say I had the interesting opportunity over the last 25 years to actually stay pretty close to the machine learning world …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah. Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; … because one, as you well know, among the most prolific users of machine learning has been the bioinformatics computational biology world because it’s been so data rich that anything that can be done, people have thrown at these problems because unlike most other things, we’re not working on man-made data. We’re looking at data that comes from nature, the complexity of which far exceeds our ability to comprehend.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So you could imagine that any approach to statistically reduce complexity, get signal out of scant data—that’s a problem that’s been around.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The other place where I’ve been exposed to this, which I’m going to come back to because that’s where it first felt totally different to me, is that some 25 years ago, actually the very first company we started was a company that attempted to use evolutionary algorithms to essentially iteratively evolve consumer-packaged goods online. Literally, we tried to, you know, consider features of products as genes and create little genomes of them. And by recombination and mutation, we could create variety. And then we could get people through panels online—this was 2002/2003 timeframe—we could essentially get people through iterative cycles of voting to create a survival of the fittest. And that’s a company that was called Affinnova.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The reason I say that is that I knew that there’s a much better way to do this if only: one, you can generate variety …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; … without having to prespecify genes. We couldn’t do that before. And, two, which we’ve come back to nowadays, you can actually mimic how humans think about voting on things and just get rid of that element of it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So then to your question of when does this kind of begin to feel different? So you could imagine that in biotechnology, you know, as an engineer by background, I always wanted to do CAD, and I picked the one field in which CAD doesn’t exist, which is biology. Computer-aided design is kind of a notional thing in that space. But boy, have we tried. For a long time, …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; … people would try to do, you know, hidden Markov models of genomes to try to figure out what &lt;em&gt;should&lt;/em&gt; be the next, you know, base that you may want to or where genes might be, etc. But the notion of generating in biology has been something we’ve tried for a while. And in the late teens, so kind of 2018, ’17, ’18, because we saw deep learning come along, and you could basically generate novelty with some of the deep learning models … and so we started asking, “Could you generate a protein basically by training a correspondence table, if you will, between protein structures and their underlying DNA sequence?” Not their protein sequence, but their DNA sequence.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; So that’s a big leap. So ’17/’18, we started this thing. It was called 56. It was FL56, Flagship Labs 56, our 56th project.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;By the way, we started this parallel one called “57” that did it in a very different way. So one of them did pure black box model-building. The other one said, you know what, we don’t want to do the kind of … at that time, AlphaFold was in its very early embodiments. And we said, “Is there a way we could actually take little, you know, multi amino acid kind of almost grammars, if you will, a little piece, and then see if we could compose a protein that way?” So we were experimenting.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And what we found was that actually, if you show enough instances and you could train a transformer model—back in the day, that’s what we were using—you could actually, say, predict another sequence that should have the same activity as the first one.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN: &lt;/strong&gt;So we trained on green fluorescent proteins. Now, we’re talking about seven years ago. We trained on enzymes, and then we got to antibodies.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;With antibodies, we started seeing that, boy, this could be a pretty big deal because it has big market impact. And we started bringing in some of the diffusion models that were beginning to come along at that time. And so we started getting much more excited. This was all done in a company that subsequently got renamed from FL56 to Generate:Biomedicines&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yep, yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN: &lt;/strong&gt;… which is one of the leaders in protein design using the generative techniques. It was interesting because Generate:Biomedicines is a company that was called that before generative AI was a thing, [LAUGHTER] which was kind of very ironic.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And, of course, that team, which operates today very, very kind of at the cutting edge, has published their models. They came up with this first Chroma&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; model, which is a diffusion-based model, and then started incorporating a lot of the LLM capabilities and fusing them.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Now we’re doing atomistic models and many other things. The point being, that gave us a glimpse of how quickly the capability was gaining, …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah. Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; … just like evolution shows you. Sometimes evolution is super silent, and then all of a sudden, all hell breaks loose. And that’s what we saw.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right. One of the things that I reflect on just in my own journey through this is there are other emotions that come up. One that was prominent for me early on was skepticism. Were there points when even in your own work, transformer-based work on this early on, that you had doubts or skepticism that these transformer architectures would be or diffusion-based approaches would be worth anything?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; You know, it’s interesting, I think that, I’m going to say this to you in a kind of a friendly way, but you’ll understand what I mean. In the world I live in, it’s kind of like the slums of innovation, [LAUGHTER] kind of like just doing things that are not supposed to work. The notion of skepticism is a luxury, right. I assume everything we do won’t work. And then once in a while I’m wrong.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And so I don’t actually try to evaluate whether before I bring something in, like just think about it. We, some hundred or so times a year, ask “what if” questions that lead us to totally weird places of thought. We then try to iterate, iterate, iterate to come up with something that’s testable. Then we go into a lab, and we test it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So in that world, right, sitting there going, like, “How do I know this transformer is going to work?” The answer is, “For what?” Like, it’s going to work. To make something up … well, guess what? We knew early on with LLMs that hallucination was a feature, not a bug for what we wanted to do.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So it’s just such a different use that, of course, I have trained scientific skepticism, but it’s a little bit like looking at a competitive situation in an ecology and saying, “I bet that thing’s going to die.” Well, you’d be right—most of the time, you’d be right. [LAUGHTER]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So I just don’t … like, it … and that’s why—I guess, call me an early adopter—for us, things that could move the needle even a little, but then upon repetition a lot, let alone this, …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; … you have to embrace. You can’t wait there and say, I’ll embrace it once it’s ready. And so that’s what we did.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Hmm. All right. So let’s get into some specifics and what you are seeing either in your portfolio companies or in the research projects or out in the industry. What is going on today with respect to AI really being used for something meaningful in the design and development of drugs?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; In companies that are doing as diverse things as—let me give you a few examples—a project that’s now become a named company called ProFound Therapeutics&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; that literally discovered three, four years ago, and would not have been able to without some of the big data-model-building capabilities, that our cells make literally thousands, if not tens of thousands, of more proteins than we were aware of, full stop.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;We had done the human genome sequence, there was 20,000 genes, we thought that there was …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Wow.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; … maybe 70-80,000, 100,000 proteins, and that’s that. And it turns out that our cells have a penchant to express themselves in the form of proteins, and they have many other ways than we knew to do that.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Now, so what does that mean? That means that we have generated a massive amount of data, the interpretation of which, the use of which to guide what you do and what these things might be involved with is purely being done using the most cutting-edge data-trained models that allow you to navigate such complexity.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Wow. Hmm.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; That’s just one example. Another example: a company called Quotient Therapeutics&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, again three, four years old. I can talk about the ones that are three, four years old because we’ve kind of gotten to a place where we’ve decided that it’s not going to fail &lt;em&gt;yet&lt;/em&gt;, [LAUGHTER] so we can talk about it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;You know, we discovered—our team discovered—that in our cells, right, so we know that when we get cancer, our cells have genetic mutations in them or DNA mutations that are correlated and often causal to the hyperproliferative stages of cancer. But what we assume is that all the other cells in our body, pretty much, have one copy of their genes from our mom, one copy from our dad, and that’s that.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And when very precise deep sequencing came along, we always asked the question, “How much variation is there cell to cell?”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; And the answer was it’s kind of noise, random variation. Well, our team said, “Well, what if it’s not really that random?” because upon cell division cycles, there’s selection happening on these cells. And so not just in cancer but in liver cells, in muscle cells, in skin cells …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Oh, interesting.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; … can you imagine that there’s an evolutionary experiment that is favoring either compensatory mutations that are helping you avoid disease or disease-caused mutations that are gaining advantage as a way to understand the mechanism? Sure enough—I wouldn’t be telling you otherwise—with &lt;em&gt;massive&lt;/em&gt; amount of single cell sequencing from individual patient samples, we’ve now discovered that the human genome is mutated on average in our bodies 10,000 times, like over every base, like, it’s huge numbers.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And we’re finding very interesting big signals come out of this massive amount of data. By the way, data of the sort that the human mind, if it tries to assign causal explanations to what’s happening …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; … is completely inadequate.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; When you think about a language model, we’re learning from human language, and the totality of human language—at least relative to what we’re able to compute today in terms of constructing a model—the totality of human language is actually pretty limited. And in fact, you know, as is always written about in click-baity titles, you know, the big model builders are actually starting to run short.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; Running out, running out, yes. [LAUGHTER]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; But one of the things that perplexes me and maybe even worries me—like these two examples—are generally in the realm of cellular biology and the complexity. Let’s just take the example of your company, ProFound. You know, the complexity of what’s going on and the potential genetic diversity is such that, can we ever have enough data? You know, because there just aren’t that many human beings. There just aren’t that many samples.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN: &lt;/strong&gt;Well, it depends on what you want to train, right. So if you want to train a &lt;em&gt;de novo&lt;/em&gt; evolutionary model that could take you from bacteria to human mammalian cells and the like, there may not be—and I’m not an expert in that—but that’s a question that we often kind of think about.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But if you’re trying to train a … like you know what the proteins we know about, how they interact with pathways and disease mechanisms and the like. Now all of a sudden you find out that there’s a whole continent of them missing in your explanations. But there are things you can reason, in quotations, through analogy, functional analogy, sequence analogy, homology. So there’s a lot of things that we could do to essentially make use of this, even though you may not have the totality of data needed to, kind of, predict, based on a de novo sequence, exactly what it’s going to do.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So I agree with the comparison. But … but you’re right. The complexity is … just keep in mind, on average, a protein may be interacting with 50 to 100 other proteins.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; So if you find thousands of proteins, you’ve found a massive interaction space through which information is being processed in a living cell.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;But do you find in your AI companies that access to data ends up being a key challenge? Or, you know, how central is that?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; Access to data is a key challenge for the companies we have that are trying to build just models. But that’s the minority of things we do. The majority of things we do is to actually co-develop the data and the models. And as you know well, because you guys, you know, have given us some ideas around this space, that, you know, you could generate data and &lt;em&gt;then&lt;/em&gt; think about what you’re to do with it, which is the way biotech is operated with bioinformatics.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right, right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; Or you could generate bespoke data that is used to train the model that’s quite separate from what you would have done in the natural course of biology. So we’re doing much more of the latter of late, and I think that’ll continue. So, but these things are proliferating.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I mean,&lt;strong&gt; &lt;/strong&gt;it’s hard to find a place where we’re not using this.&lt;strong&gt; &lt;/strong&gt;And the “this” is any and all data-driven model building, generative, LLM-based, but also every other technique to make progress.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Sure.&lt;strong&gt; &lt;/strong&gt;So now moving away from the straight biochemistry applications, what about AI in the process of building a business, of making investment decisions, of actually running an operation? What are you seeing there?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; So, well, you know, Moderna, which is a company that I’m quite proud of being a founder and chairman of, has adopted a significant, significant amount of AI embedded into their operations in all aspects: from the manufacturing, quality control, the clinical monitoring, the design—every aspect. And in fact, they’ve had a partnership that they’ve had for a little while here with OpenAI, and they’ve tried many different ways to stay at the cutting edge of that.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So we see that play out at some scale. That’s a 5,000-, 6,000-person organization, and what they’re doing is a good example of what early adopters would do, at least in our kind of biotechnology company.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But then, you know, in our space, I would say the efficiency impact is kind of no different, than, you know, anywhere else in academia you might adopt it or in other kinds of companies. But where I find it an interesting kind of maybe segue is the degree to which&lt;strong&gt; &lt;/strong&gt;it may fundamentally change the way we think about how to do science, which is a whole other use, right?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; So it’s not an efficiency gain &lt;em&gt;per se&lt;/em&gt;, although it’s maybe an effectiveness gain when it comes to science, but can you just fundamentally train models to generate hypotheses?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN: &lt;/strong&gt;And we have done that, and we’ve been doing this for the last three years. And now it’s getting better and better, the better these reasoning engines are getting and kind of being able to extrapolate and train for novelty. Can you convert that to the world’s best experimental protocol to very precisely falsify your hypothesis, on and on?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;That closing of that loop, kind of what we call &lt;em&gt;autonomous science&lt;/em&gt;, which we’ve been trying to do for the last two, three years and are making some progress in, that to me is another kind of bespoke use of these things, not to generate molecules in its chemistry, but to change the behavior of how science is done.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah.&lt;strong&gt; &lt;/strong&gt;So I always end with a couple of provocative questions, but I need—before we do that, while we’re on this subject—to get your take on Lila Sciences&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And there is a vision there that I think is very interesting. It’d be great to hear it described by you.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; Sure. So Lila, after operating for two to three years in kind of a preparatory kind of stealth mode, we’ve now had a little bit more visibility around, and essentially what we’re trying to do there is to create what we call automated science factories, and such a factory would essentially be able to take problems, either computationally specified or human-specified, and essentially do the experimental work in order to either make an optimization happen or enable something that just didn’t exist. And it’s really, at this point, we’ve shown proof of concept in narrow areas.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; But it’s hard to say that if you can do this, you can’t do some other things, so we’re just expanding it that way. We don’t think we need a complete proof or complete demonstration of it for every aspect.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; So we’re just kind of being opportunistic. The idea for Lila is to partner with a number of companies. The good news is, within Flagship, there’s 48 of them. And so there’s a whole lot of them they can partner with to get their learning cycles. But eventually they want to be a real alternative to every time somebody has an idea, having to kind of go into a lab and manually do this.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I do want to say one thing we touched on, Peter, though, just on that front, which is …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; … if you say, like, “What problem is this going to solve?” It’s several but an important one is just the flat-out human capacity to reason on this much data and this much complexity that is real. Because nature doesn’t try to abstract itself in a human understandable form.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right. Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; In biology, since it’s kind of like progress happens through evolutionary kind of selections, the evidence of which [has] long been lost, and so therefore, you just see what you have, and then it has a behavior. I really do think that there’s something to be said, and I want to—just for your audience—lay out a provocative, at least, thought on all this, which Lila is a beginning embodiment of, which is that I really think that what’s going to happen over the next five, 10 years, even while we’re all fascinated with the impending arrival of AGI [artificial general intelligence] is really what I call &lt;em&gt;poly-intelligence&lt;/em&gt;, which is the combination of human intelligence, machine intelligence, AI, and nature’s intelligence.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;We’re all fascinated at the human-machine interface. We know the human-nature interface, but imagine the machine-nature interface—that is, actually letting loose a digital kind of information processing life form through the algorithms that are being developed and the commensurately complex, maybe much more complex. We’ll see. And so now the question becomes, what does the human do?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And we’re living in a world which is human dominated, which means the humans say, “If I don’t understand it, it’s not real, basically. And if I don’t understand it, I can’t regulate it.” And we’re going to have to make peace with the fact that we’re not going to be able to predictably affect things without necessarily understanding them the way we could if we just forced ourselves to only work on problems we can understand. And that world we’re not ready for at all.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah. All right. So this one I predict is going to be a little harder for you because I think while you think about the future, you live very much in the present. But I’d like you to make some predictions about what the biotech and biopharmaceutical industries are going to be able to do two years from now, five years from now, 10 years from now.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; Yeah, well, it’s hard for me because you know my nature, which is that I think this is all emergent.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; And so I would be the conceit of predicting. So I would say with likelihood positive predictive value of less than 10%, I’m happy to answer your question. So I’m not trying to score high [LAUGHTER] because I really think that my job is to envision it, not to predict it. And that’s a little bit different, right?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah, I actually was trying to pick what would be the hardest possible question I could ask you, [LAUGHTER] and this is what I came up with.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; Yeah, no, no, I’m kidding here. So now look, I think that we will cross this threshold of understandability. And of course you’re seeing that in a lot of LLM things today. And of course, people are trying to train for things that are explainers and all that whole, there’s a whole world of that. But I think at some point we’re going to have to kind of let go and get comfortable working on things that, you know …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I sometimes tell people, you know, and I’m not the first, but scientists and engineers are different, it’s said, in that engineers work on things that they don’t wait until they get a full understanding of before they work with them. Well, now scientists are going to have to get used to that, too, right?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah. Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; Because insisting that it’s only valid if it’s understandable. So, I would say, look, I hope that the time … for example, I think major improvements will be made in patient selection. If we can test drugs on patients that are more synchronized as to the stage of their disease …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; … I think the answer will be much better. We’re working on that. It’s a company called Etiome&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, very, very early stage. It’s really beautiful data, very early data that shows that when we talk about MASH [metabolic dysfunction-associated steatohepatitis], liver disease, when we talk about Parkinson’s, there’s such a heterogeneity, not only of the subset type of the disease, but the stage of the disease, that this notion that you have stage one cancer, stage two cancer, again, nobody told nature there’s stages of that kind. It’s a continuum.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But if you can synchronize based on training, kind of, the ability to detect who are the patients that are in enough of a close proximity that should be treated so that the trial—much smaller a trial size—could give you a drug, then afterwards, you can prescribe it using these approaches.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Kind of we’re going to find that what we thought is one disease is more like 15 diseases. That’s bad news because we’re not going to be able to claim that we can treat everything which we can. It’s good news in that there’s going to be people who are going to start making much more specific solutions to things.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; So I can imagine that. I can imagine a generation of, kind of, students who are going to be able to play in this space without having 25 years of graduate education on the subject. So what is deemed knowledge sufficient to do creative things will change. I can go on and on, but I think all this is very close by and it’s very exciting.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Noubar, I just always have so much fun, and I learn really a lot. It’s high-density learning when I talk to you. And so I hope our listeners feel the same way. It’s something I really appreciate.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; Well, Peter, thanks for this. And I think your listeners know that if I was asking you questions, you would be answering them with equal if not more fascinating stuff. So, thanks for giving me the chance to do that today.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[TRANSITION MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;I’m always fascinated by Noubar’s perspectives on fundamental research and how it connects to human health and the building of successful companies. I see him as a classic “systems thinker,” and by that, I mean he builds impressive things like Flagship Pioneering itself, which he created as a kind of biomedical innovation system.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In our conversation, I was really struck by the fact that he’s been thinking about the potential impact of transformers—transformers being the fundamental building block of large language models—as far back as 2017, when the first paper on the attention mechanism in transformers was published by Google.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But, you know, it isn’t only about using AI to do things like understand and design molecules and antibodies faster. It’s interesting that he is also pushing really hard towards a future where AI might “close the loop” from hypothesis generation, to experiment design, to analysis, and so on.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Now, here’s my conversation with Dr. Eric Topol:&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Eric, it’s really great to have you here.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;ERIC TOPOL: &lt;/strong&gt;Oh, Peter, I’m thrilled to be here with you here at Microsoft.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; You’re a super famous person. Extremely well known to researchers even in computer science, as we have here at Microsoft Research.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But the question I’d like to ask is, how would you explain to your parents what you do every day?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; [LAUGHS] That’s a good question. If I was just telling them I’m trying to come up with better ways to keep people healthy, that probably would be the easiest way to do it because if I ever got in deeper, I would lose them real quickly. They’re not around, but just thinking about what they could understand.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; I think as long as they knew it was work centered on innovative paths to promoting and preserving human health, that would get to them, I think.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; OK, so now, kind of the second topic, and then we let the conversation flow, is about origin stories with respect to AI. And with most of our guests, you know, I factor that into two pieces: the encounters with AI before ChatGPT and what we call generative AI and then the first contacts after.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And, of course, you have extensive contact with both now. But let’s start with how you got interested in machine learning and AI prior to ChatGPT. How did that happen?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Yeah, it was out of necessity. So back, you know, when I started at Scripps at the end of ’06, we started accumulating, you know, massive datasets. First, it was whole genomes. We did one of the early big cohorts of 1,400 people of healthy aging. We called the Wellderly whole genome sequence&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And then we started big in the sensor world, and then we started saying, what are we going to do with all this data, with electronic health records and all those sensors? And now we got whole genomes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And basically, what we were doing, we were in hoarding mode. We didn’t have a way to meaningfully analyze it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;You would read about how, you know, data is the new oil and, you know, gold and whatnot. But we just didn’t have a way to extract the juice. And even when we wanted to analyze genomes, it was incredibly laborious.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; And we weren’t extracting a lot of the important information. So that’s why … not having any training in computer science, when I was doing the … about three years of work to do the book &lt;em&gt;Deep Medicine&lt;/em&gt;, I started really, first auto-didactic about, you know, machine learning. And then I started contacting a lot of the real top people in the field and hanging out with them, and learning from them, getting their views as to, you know, where we are today, what models are coming in the future.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And then I said, “You know what? We are going to be able to fix this mess.” [LAUGHS] We’re going to get out of the hoarding phase, and we’re going to get into, you know, really making a difference.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So that’s when I embraced the future of AI. And I knew, you know, back—that was six years ago when it was published and probably eight or nine years ago when I was doing the research, and I knew that we weren’t there yet.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;You know, at the time, we were seeing the image interpretation. That was kind of the early promise. But really, the models that were transformative, the transformer models, they were incubating back in 2017. So people knew something was brewing.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right. Yes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; And everyone said we’re going to get there.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;So then, ChatGPT comes out November of 2022; there’s GPT-4 in 2023, and now a lot has happened. Do you remember what your first encounter with that technology was?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Oh, sure. First, ChatGPT. You know, in the last days of November ’22, I was just blown away. I mean, I’m having a conversation. I’m having fun. And this is humanoid responding to me. I said, “&lt;em&gt;What?&lt;/em&gt;” You know? So that was to me, a moment I’ll never forget. And so I knew that the world was, you know, at a very kind of momentous changing point.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Of course, knowing, too, that this is going to be built on, and built on quickly. Of course, I didn’t know how soon GPT-4 and all the others were going to come forward, but that was a wake-up call that the capabilities of AI had just made a humongous jump, which seemingly was all of a sudden, although I did know this had been percolating …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; … you know, for what, at least five years, that, you know, it really was getting into its position to do this.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; I know one of the things that was challenging psychologically and emotionally for me is, it made me rethink a lot of things that were going on in Microsoft Research in areas like causal reasoning, natural language processing, speech processing, and so on.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I’m imagining you must have had some emotional struggles too because you have this amazing book, &lt;em&gt;Deep Medicine&lt;/em&gt;. Did you have to … did it go through your mind to rethink what you wrote in &lt;em&gt;Deep Medicine &lt;/em&gt;in light of this or, or, you know, how did that feel?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;It’s funny you ask that because in this one chapter I have on the virtual health coach, I wrote a whole bunch of scenarios …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; … that were very kind of futuristic. You know, about how the AI interacts with the person’s health and schedules their appointment for this and their scan and tells them what lab tests they should tell their doctor to have, and, you know, all these things. And I sent a whole bunch of these, thinking that they were a little too far-fetched.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;And I sent them to my editor when I wrote the book, and he says, “Oh, these are great. You should put them all in.” [LAUGHTER] What I didn’t realize is they weren’t that, you know, they were all going to happen.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&amp;nbsp;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah. They weren’t that far-fetched at all.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Not at all. If there’s one thing I’ve learned from all this, is our imagination isn’t big enough.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&amp;nbsp;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; We think too small.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Now in our book that Carey, Zak, and I wrote, you know, we made, you know, we sort of guessed that GPT-4 might help biomedical researchers, but I don’t think that any of us had the thought in mind that the architecture around generative AI would be so directly applicable to, you know, say, protein structures or, you know, to clinical health records and so on.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And so a lot of that seems much more obvious today. But two years ago, it wasn’t. But we did guess that biomedical researchers would find this interesting and be helped along.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So as you reflect over the past two years, you know, do you have things that you think are very important, kind of, meaningful applications of generative AI in the kinds of research that Scripps does?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL&lt;/strong&gt;: Yeah. I mean, I think for one, you pointed out how the term &lt;em&gt;generative AI&lt;/em&gt; is a misnomer.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; And so it really was prescient about how, you know, it had a pluripotent capability in every respect, you know, of editing and creating. So that was something that I think was telling us, an indicator that this is, you know, a lot bigger than how it’s being labeled. And our expectations can actually be more than what we had seen previously with the earlier version.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So I think what’s happened is that now, we keep jumping. It’s so quick that we can’t … you know, first we think, oh, well, we’ve gone into the agentic era, and then we could pass that with reasoning. [LAUGHTER] And, you know, we just can’t …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; It’s just wild.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; So I think so many of us now will put in prompts that will necessitate or ideally result in a not-immediate gratification, but rather one that requires, you know, quite a bit of combing through the corpus of knowledge …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; … and getting, with all the citations, a report or a response. And I think now this has been a reset because to do that on our own, it takes, you know, many, many hours. And it’s usually incomplete.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But one of the things that was so different in the beginning was you would get the references from up to a year and a half previously.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;And that’s not good enough. [LAUGHS]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;And now you get references, like, from the day before.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yes. Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;And so, you say, “Why would you do a regular search for anything when you could do something like this?”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; And then, you know, the reasoning power. And a lot of people who are not using this enough still are talking about, “Well, there’s no reasoning.”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; Which you dealt with really well in the book. But what, of course, you couldn’t have predicted is the new dimensions.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; I think you nailed it with GPT-4. But it’s all these just, kind of, stepwise progressions that have been occurring because of the velocity that’s unprecedented. I just can’t believe it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;We were aware of the idea of multi-modality, but we didn’t appreciate, you know, what that would mean. Like AlphaFold&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; [protein structure database], you know, the ability for AI to understand—or crystal structures—to really start understanding something more fundamental about biochemistry or medicinal chemistry.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I have to admit, when we wrote the book, we really had no idea.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Well, I feel the same way. I still today can’t get over it because the reason AlphaFold and Demis [Hassabis] and John Jumper [AlphaFold’s co-creators] were so successful is there was this protein databank.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;And it had been kept for decades. And so, they had the substrate to work with.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; So, you say, “OK, we can do proteins.” But then how do you do everything else?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; And so this whole, what I call, “large language of &lt;em&gt;life&lt;/em&gt; model” work, which has gone into high gear like I’ve never seen.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; You know, now to this holy grail of a virtual cell, and …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; You know, it’s basically … it’s … it was inspired by proteins. But now it’s hitting on, you know, ligands and small molecules, cells. I mean, nothing is being held back here.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; So how could anybody have predicted that?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; I sure wouldn’t have thought it would be possible at this point.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah. So just to challenge you, where do you think that is going to be two years from now? Five years from now? Ten years from now? Like, so you talk about a virtual cell. Is that achievable within 10 years, or is that still too far out?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; No, I think within 10 years for sure. You know the group that got assembled that Steve Quake&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; pulled together?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; I think has 42 authors in a paper&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; in &lt;em&gt;Cell&lt;/em&gt;. The fact that he could get these 42 experts in life science and some in computer science to come together and all agree …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; … that not only is this a worthy goal, but it’s actually going to be realized, that was impressive.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I challenged him about that. How did you get these people all to agree? So many of them were naysayers. And by the time the workshop finished, they were fully convinced. I think that what we’re seeing is so much progress happening so quickly. And then all the different models, you know, across DNA, RNA, and everything are just zooming forward.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; And it’s just a matter of pulling this together. Now when we have that, and I think it could easily be well before a decade and possibly, you know, between the five- and 10-year mark—that’s just a guess—but then we’re moving into another era of life science because right now, you know, this whole buzz about drug discovery.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;It’s not… with the ability to do all these perturbations at a cellular level.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Or the cell of interest.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; Or the cell-to-cell interactions or the intra-cell interaction. So once you nail that, yeah, it takes it to a kind of another predictive level that we haven’t really fathomed. So, yes, there’s going to be drug discovery that’s accelerated. But this would make that and also the underpinnings of diseases.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; So the idea that there’s so many diseases we don’t understand now. And if you had virtual cell, …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; … you would probably get to that answer …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; … much more quickly. So whether it’s underpinnings of diseases or what it’s going to take to really come up with far better treatments—preventions—I think that’s where virtual cell will get us.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; There’s a technical question … I wonder if you have an opinion. You may or may not. There is sort of what I would refer to as &lt;em&gt;ab initio&lt;/em&gt; approaches to this. You know, you start from the fundamental physics and chemistry, and we know the laws, we have the math and, you know, we can try to derive from there … in fact, we can even run simulations of that math to generate training data to build generative models and work up to a cell, &lt;em&gt;or&lt;/em&gt; forget all of that and just take as many observations and measurements of, say, living cells as possible, and just have faith that hidden amongst all of the observational data, there is structure and language that can be derived.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So that’s sort of bottom-up versus top-down approaches. Do you have an opinion about which way?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; Oh, I think you go after both. And clearly whenever you’re positing that you’ve got a virtual cell model that’s working, you’ve got to do the traditional methods as well to validate it, and … so all that. You know, I think if you’re going to go out after this seriously, you have to pull out all the stops. Both approaches, I think, are going to be essential.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; You know, if what you’re saying is true, and it is amazing to hear the confidence, the one thing I tried to explain to someone nontechnical is that for a lot of problems in medicine, we just don’t have enough data in a really profound way. And the most profound way to say that is, since Adam and Eve, there have only been an estimated 106 billion people who have ever lived.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So even if we had the DNA of every human being, every individual of &lt;em&gt;Homo sapiens&lt;/em&gt;, there are certain problems for which we would not have enough data.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Sure.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;And so I think another thing that seems profound to me, if we can actually have a virtual cell, is we can actually make trillions of virtual …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; Yeah&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; … human beings. The true genetic diversity could be realized for our species.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;I think you nailed it. The ability to have that type of data, no less synthetic data, I mean, it’s just extraordinary.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;We will get there someday. I’m confident of that. We may be wrong in projections. And I do think [science writer] Philip Ball won’t be right that it will never happen, though. [LAUGHTER] No, I think that if there’s a holy grail of biology, this is it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;And I think you’re absolutely right about where that will get us.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Transcending the beginning of the species.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; Of&lt;em&gt; our&lt;/em&gt; species.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah. All right. So now, we’re starting to run short on time here. And so I wanted to ask you about, I’m in my 60s, so I actually think about this a lot more. [LAUGHTER] And I know you’ve been thinking a lot about longevity. And, of course, your new book, &lt;em&gt;Super Agers&lt;/em&gt;.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And one of the reasons I’m so eager to read is it’s a topic very top of mind for me and actually for a lot of people. Where is this going? Because this is another area where you hear so much hype. At the same time, you see Nobel laureate scientists …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; … working on this.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; So, so what’s, what’s real there?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Yeah. Well, it’s really … the real deal is the science of aging is zooming forward.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And that’s exciting. But I see it bifurcating. On the one hand, all these new ideas, strategies to reverse aging are very ambitious. Like cell reprogramming and senolytics and, you know, the rejuvenation of our thymus gland, and it’s a long list.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; And they’re really cool science, and it used to be the mouse lived longer. Now it’s the old mouse looks really young.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah. Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; All the different features. A blind mouse with cataracts is all of a sudden there’s no cataracts. I mean, so these things are exciting, but none of them are proven in people, and they all have significant risk, no less, you know, the expense that might be attached.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; And some people are jumping the gun. They’re taking rapamycin, which can really knock out their immune system. So they all carry a lot of risk. And people are just getting a little carried away. We’re not there yet.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But the other side, which is what I emphasize in the book, which is exciting, is that we have all these new metrics that came out of the science of aging.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; So we have clocks of the body. Our biological clock versus our chronological clock, and we have organ clocks. So I can say, you know, Peter, we’ve assessed all your organs and your immune system. And guess what? Every one of them is either at or less than your actual age.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;And that’s very reassuring. And by the way, your methylation clock is also … I don’t need to worry about you so much. And then I have these other tests that I can do now, like, for example, the brain. We have an amazing protein p-Tau217 that we can say over 20 years in advance of you developing Alzheimer’s, …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; … we can look at that, and it’s modifiable by lifestyle, bringing it down. It should be you can change the natural history. So what we’ve seen is an explosion of knowledge of metrics, proteins, no less, you know, our understanding at the gene level, the gut microbiome, the immune system. So that’s what’s so exciting. How our immune system ages. &lt;em&gt;Immunosenescence&lt;/em&gt;. How we have more inflammation—&lt;em&gt;inflammaging&lt;/em&gt;—with aging. So basically, we have three diseases that kill us, that take away our health: heart, cancer, and neurodegenerative.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; And they all take more than 20 years. They all have a defective immune system inflammation problem, and they’re all going to be preventable.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;That’s what’s so exciting.&amp;nbsp;So we don’t have to have reverse aging. We can actually work on …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Just prevent aging in the first place.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;…&lt;strong&gt; &lt;/strong&gt;the age-related diseases. So basically, what it means is: I got to find out if you have a risk, if you’re in this high-risk group for this particular condition, because if you are—and we have many levels, layers, orthogonal ways to check—we don’t just bank it all on one polygenic test. We’re going to have several ways, say this is the one we are going …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And then we go into high surveillance, where, let’s say if it’s your brain, we do more p-Tau, if we need to do brain imaging—whatever it takes. And also, we do preventive treatments on top of the lifestyle [changes], that one of the problems we have today is a lot of people know generally, what are good lifestyle factors. Although, I go through a lot more than people generally acknowledge.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But they don’t incorporate them because they don’t know that they’re at risk and they could change their … extend their health span and prevent that disease. So what I at least put out there, a blueprint, is how we can use AI, because it’s multimodal AI, with all these layers of data, and then temporally, it’s like today you could say if you have two protein tests, not only are you going to have Alzheimer’s, but within a two-year time frame &lt;em&gt;when&lt;/em&gt; …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;… and if you don’t change things, if we don’t gear up … you know, we can … we can completely prevent this, so … or at least defer it for a decade or more. So that’s why I’m excited, is that we made these strides in the science of aging. But we haven’t acknowledged the part that doesn’t require reversing aging. There’s this much less flashy, attainable, less risky approach …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;…&lt;strong&gt; &lt;/strong&gt;than the one that … when you reverse aging, you’re playing with the hallmarks of cancer. They are like, if you look at the hallmarks of cancer …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; That has been one of the primary challenges.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;They’re lined up.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; They’re all the same, you know, whether it’s telomeres, or whether it’s … you know … so this is the problem. I actually say in the book, I do think one of these—we have so many shots on goal—one of these reverse aging things will likely happen someday. But we’re nowhere close.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;On the other hand, let’s gear up. Let’s do what we can do. Because we have these new metrics that’s … people don’t … like, when I read the organ clock paper&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; from Tony Wyss-Coray from Stanford. It was published end of ’23; it was the cover of &lt;em&gt;Nature&lt;/em&gt;. It blew me away.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;And I wrote a Substack&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; [article] on it. And Tony said, “Well, that’s so nice of you.” I said, “So nice? This is revolutionary, you know.” [LAUGHTER] So …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;By the way, what’s so interesting is, how these things, this kind of understanding and AI, are coming together.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Yes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;It’s almost eerie the timing of these things.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Absolutely. Because you couldn’t take all these layers of data, just like we were talking about data hoarding.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; Now we have data hoarding on individual with no way to be able to make these assessments of what level of risk, when, what are we going to do in &lt;em&gt;this&lt;/em&gt; individual to prevent that? We can do that now.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;We can do it today. And we could keep building on that. So I’m really excited about it. I think that, you know, when I wrote the last book on deep medicine, it was our overarching goal should be to bring back the patient-doctor relationship. I’m an old dog, and I know what it used to be when I got out of medical school.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It’s totally … you couldn’t imagine how much erosion from the ’70s, ’80s to now. But now I have a new overarching goal. I’m thinking that that still is really important—humanity in medicine—but let’s prevent these three … big three diseases because it’s an opportunity that we’re not … you know, in medicine, all my life we’ve been hearing and talking about we need to prevent diseases.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Curing is much harder than prevention. And the economics. Oh my gosh. But we haven’t done it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Now we can do it. Primary prevention. We’d do really well. Somebody’s had heart attack.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Oh, we’re going to get all over it. Why did they have a heart attack in the first place?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Well, the thing that makes so much sense in what you’re saying is that we understand we have an understanding both economically and medically that prevention is a good thing. And extending the concept of prevention to these age-related conditions, I think, makes all the sense in the world.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;You know, Eric, maybe on that optimistic note, it’s time to wrap up this conversation. Really appreciate you coming. Let me just brag in closing that I’m now the proud owner of an autographed copy of your latest book, and, really, thank you for that.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Oh, thank you. I could spend the rest of the day talking to you. I’ve really enjoyed it. Thanks.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[TRANSITION MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; For me, the biggest takeaway from our conversation was Eric’s supremely optimistic predictions about what AI will allow us to do in much less than 10 years.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;You know, for me personally, I started off several years ago with the typical techie naivete that if we could solve protein folding using machine learning, we would solve human biology. But as I’ve gotten smarter, I’ve realized that things are way, way more complicated than that, and so hearing Eric’s techno-optimism on this is really both heartening and so interesting.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Another thing that really caught my attention are Eric’s views on AI in medical diagnosis. That really stood out to me because within our labs here at Microsoft Research, we have been doing a lot of work on this, for example in creating foundation models for whole-slide digital pathology.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The bottom line, though, is that biomedical research and development is really changing and changing quickly. It’s something that we thought about and wrote briefly about in our book, but just hearing it from these three people gives me reason to believe that this is going to create tremendous benefits in the diagnosis and treatment of disease.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And in fact, I wonder now how regulators, such as the Food and Drug Administration here in the United States, will be able to keep up with what might become a really big increase in the number of animal and human studies that need to be approved. On this point, it’s clear that the FDA and other regulators will need to use AI to help process the likely rise in the pace of discovery and experimentation. And so stay tuned for more information about that.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[THEME MUSIC] &lt;/p&gt;



&lt;p&gt;I’d like to thank Daphne, Noubar, and Eric again for their time and insights. And to our listeners, thank you for joining us. There are several episodes left in the series, including discussions on medical students’ experiences with AI and AI’s influence on the operation of health systems and public health departments. We hope you’ll continue to tune in.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Until next time.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC FADES] &lt;/p&gt;

				&lt;/span&gt;
			&lt;/div&gt;
			&lt;button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button"&gt;
				Show more			&lt;/button&gt;
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;




&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Illustrated images of Peter Lee, Daphne Koller, Noubar Afeyan, and Dr. Eric Topol for the Microsoft Research Podcast" class="wp-image-1144053" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Episode8-PeterEricNoubarDaphne-AIRevolution_Hero_Feature_No_Text_1400x788.jpg" width="1400" /&gt;&lt;/figure&gt;






&lt;p&gt;In November 2022, OpenAI’s ChatGPT kick-started a new era in AI. This was followed less than a half year later by the release of GPT-4. In the months leading up to GPT-4’s public release, Peter Lee, president of Microsoft Research, cowrote a book full of optimism for the potential of advanced AI models to transform the world of healthcare. What has happened since? In this special podcast series, &lt;em&gt;The AI Revolution in Medicine, Revisited&lt;/em&gt;, Lee revisits the book, exploring how patients, providers, and other medical professionals are experiencing and using generative AI today while examining what he and his coauthors got right—and what they didn’t foresee.&lt;/p&gt;



&lt;p&gt;In this episode, Daphne Koller&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Noubar Afeyan&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, and Dr. Eric Topol&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, leaders in AI-driven medicine, join Lee to explore the rapidly evolving role of AI across the biomedical and healthcare landscape. Koller, founder and CEO of Insitro, shares how machine learning is transforming drug discovery, especially target identification for complex diseases like ALS, by uncovering biological patterns across massive datasets. Afeyan, founder and CEO of Flagship Pioneering and co-founder and chairman of Moderna, discusses how AI is being applied across biotech research and development, from protein design to autonomous science platforms. Topol, executive vice president of Scripps Research and director of the Scripps Research Translational Institute, highlights how AI can &lt;em&gt;today&lt;/em&gt; help mitigate and prevent the core diseases that erode our health and the possibility of realizing a virtual cell. Through his conversations with the three, Lee investigates how AI is reshaping the discovery, deployment, and delivery of medicine.&amp;nbsp;&lt;/p&gt;



&lt;div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow"&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h2 class="wp-block-heading h5" id="learn-more-1"&gt;Learn more:&lt;/h2&gt;




&lt;/div&gt;







&lt;section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast"&gt;
	
&lt;/section&gt;


&lt;div class="wp-block-msr-show-more"&gt;
	&lt;div class="bg-neutral-100 p-5"&gt;
		&lt;div class="show-more-show-less"&gt;
			&lt;div&gt;
				&lt;span&gt;
					

&lt;h2 class="wp-block-heading" id="transcript"&gt;Transcript&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;[MUSIC]&lt;/p&gt;



&lt;p&gt;[BOOK PASSAGE]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;PETER LEE: &lt;/strong&gt;“Can GPT-4 indeed accelerate the progression of medicine&lt;strong&gt; &lt;/strong&gt;… ? It seems like a tall order, but if I had been told six months ago that it could rapidly summarize any published paper, that alone would have satisfied me as a strong contribution to research productivity. … But now that I’ve seen what GPT-4 can do with the healthcare process, I expect a lot more in the realm of research.”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[END OF BOOK PASSAGE]&lt;/p&gt;



&lt;p&gt;[THEME MUSIC]&lt;/p&gt;



&lt;p&gt;This is &lt;em&gt;The AI Revolution in Medicine, Revisited&lt;/em&gt;. I’m your host, Peter Lee.&lt;/p&gt;



&lt;p&gt;Shortly after OpenAI’s GPT-4 was publicly released, Carey Goldberg, Dr. Zak Kohane, and I published &lt;em&gt;The AI Revolution in Medicine &lt;/em&gt;to help educate the world of healthcare and medical research about the transformative impact this new generative AI technology could have. But because we wrote the book when GPT-4 was still a secret, we had to speculate. Now, two years later, what did we get right, and what did we get wrong?&lt;/p&gt;



&lt;p&gt;In this series, we’ll talk to clinicians, patients, hospital administrators, and others to understand the reality of AI in the field and where we go from here.&lt;/p&gt;



&lt;p&gt;[THEME MUSIC FADES]&lt;/p&gt;



&lt;p&gt;The book passage I read at the top was from “Chapter 8: Smarter Science,” which was written by Zak.&lt;/p&gt;



&lt;p&gt;In writing the book, we were optimistic about AI’s potential to accelerate biomedical research and help get new and much-needed treatments and drugs to patients sooner. One area we explored was generative AI as a designer of clinical trials. We looked at generative AI’s adeptness at summarizing helping speed up pre-trial triage and research. We even went so far as to predict the arrival of a large language model that can serve as a central intellectual tool.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For a look at how AI is impacting biomedical research today, I’m excited to welcome Daphne Koller, Noubar Afeyan, and Eric Topol.&amp;nbsp;&lt;/p&gt;



				&lt;/span&gt;
				&lt;span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1"&gt;
					



&lt;p&gt;Daphne Koller is the CEO and founder of Insitro, a machine learning-driven drug discovery and development company that recently made news for its identification of a novel drug target for ALS and its collaboration with Eli Lilly to license Lilly’s biochemical delivery systems. Prior to founding Insitro, Daphne was the co-founder, co-CEO, and president of the online education platform Coursera.&lt;/p&gt;



&lt;p&gt;Noubar Afeyan is the founder and CEO of Flagship Pioneering, which creates biotechnology companies focused on transforming human health and environmental sustainability. He is also co-founder and chairman of the messenger RNA company Moderna. An entrepreneur and biochemical engineer, Noubar has numerous patents to his name and has co-founded many startups in science and technology.&lt;/p&gt;



&lt;p&gt;Dr. Eric Topol is the executive vice president of the biomedical research non-profit Scripps Research, where he founded and now directs the Scripps Research Translational Institute. One of the most cited researchers in medicine, Eric has focused on promoting human health and individualized medicine through the use of genomic and digital data and AI.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;These three are likely to have an outsized influence on how drugs and new medical technologies soon will be developed.&lt;/p&gt;



&lt;p&gt;[TRANSITION MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Here’s my interview with Daphne Koller:&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Daphne, I’m just thrilled to have you join us.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DAPHNE KOLLER: &lt;/strong&gt;Thank you for having me, Peter. It’s a pleasure to be here.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Well, you know, you’re quite well-known across several fields. But maybe for some audience members of this podcast, they might not have encountered you before. So where I’d like to start is a question I’ve been asking all of our guests.&lt;/p&gt;



&lt;p&gt;How would you describe what you do? And the way I kind of put it is, you know, how do you explain to someone like your parents what you do for a living?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER: &lt;/strong&gt;So that answer obviously has shifted over the years.&lt;/p&gt;



&lt;p&gt;What I would say now is that we are working to leverage the incredible convergence of very powerful technologies, of which AI is one but not the only one, to change the way in which we discover and develop new treatments for diseases for which patients are currently suffering and even dying.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;You know, I think I’ve known you for a long time.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; Longer than I think either of us care to admit.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;[LAUGHS] In fact, I think I remember you even when you were still a graduate student. But of course, I knew you best when you took up your professorship at Stanford. And I always, in my mind, think of you as a computer scientist and a machine learning person. And in fact, you really made a big name for yourself in computer science research in machine learning.&lt;/p&gt;



&lt;p&gt;But now you’re, you know, leading one of the most important biotech companies on the planet. How did that happen?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; So people often think that this is a recent transition. That is, after I left Coursera, I looked around and said, “Hmm. What should I do next? Oh, biotech seems like a good thing,” but that’s actually not the way it transpired.&lt;/p&gt;



&lt;p&gt;This goes all the way back to my early days at Stanford, where, in fact, I was, you know, as a young faculty member in machine learning, because I was the first machine learning hire into Stanford’s computer science department, I was looking for really exciting places in which this technology could be deployed, and applications back then, because of scarcity of data, were just not that inspiring.&lt;/p&gt;



&lt;p&gt;And so I looked around, and this was around the late ’90s, and realized that there was interesting data emerging in biology and medicine. My first application actually was in, interestingly, in epidemiology—patient tracking and tuberculosis. You know, you can think of it as a tiny microcosm of the very sophisticated models that COVID then enabled in a much later stage.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; And so initially, this was based almost entirely on just technical interest. It’s kind of like, oh, this is more interesting as a question to tackle than spam filtering. But then I became interested in biology in its own right, biology and medicine, and ended up having a bifurcated existence as a Stanford professor where half my lab continued to do core computer science research published in, you know, NeurIPS and ICML. And the other half actually did biomedical research that was published in, you know, &lt;em&gt;Nature Cell [and] Science&lt;/em&gt;. So that was back in, you know, the early, early 2000s, and for most of my Stanford career, I continued to have both interests.&lt;/p&gt;



&lt;p&gt;And then the Coursera experience kind of took me out of Stanford and put me in an industry setting for the first time in my life actually.&lt;strong&gt; &lt;/strong&gt;But then when my time at Coursera came to an end, you know, I’d been there for five years. And if you look at the timeline, I left Stanford in early 2012, right as the machine learning revolution was starting. So I missed the beginning.&lt;/p&gt;



&lt;p&gt;And it was only in like 2016 or so that, as I picked my head up over the trenches, like, “Oh my goodness, this technology is going to change the world.” And I wanted to deploy that big thing towards places where it would have beneficial impact on the world, like to make the world a better place.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; And so I decided that one of the areas where I could make a unique, differentiated impact was in really bringing AI and machine learning to the life sciences, having spent, you know, the majority of my career at the boundary of those two disciplines. And notice I say “boundary” with deliberation because there wasn’t very much of an intersection.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; I felt like I could do something that was unique.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;So just to stick on you for a little bit longer, you know, we have been sort of getting into your origin story about what we call AI today—but machine learning, so deep learning.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And, you know, there has always been a kind of an emotional response for people like you and me and now the general public about their first encounters with what we now call generative AI. I’d love to hear what your first encounter was with generative AI and how you reacted to this.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; I think my first encounter was actually an indirect one. Because, you know, the earlier generations of generative AI didn’t directly touch our work at Insitro&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And yet at the same time, I had always had an interest in computer vision. That was a large part of my non-bio work when I was at Stanford.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And so some of my earlier even presentations, when I was trying to convey to people back in 2016 how this technology was going to transform the world, I was talking about the incredible progress in image recognition that had happened up until that point.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So my first interaction was actually in the generative AI for images, where you are able to go the other way …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER: &lt;/strong&gt;… where you can take a verbal description of an image and create—and this was back in the days when the images weren’t particularly photorealistic, but still a natural language description to an image was magic given that only two or three years before that, we were barely able to look at an image and write a short phrase saying, “This is a dog on the beach.” And so that arc, that hockey curve, was just mind blowing to me.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Did you have moments of skepticism?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER: &lt;/strong&gt;Yeah, I mean the early, you know, early versions of ChatGPT, where it was more like parlor tricks and poking it a little bit revealed all of the easy ways that one could break it and make it do really stupid things. I was like, yeah, OK, this is kind of cute, but is it going to actually make a difference? Is it going to solve a problem that matters?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And I mean, obviously, I think now everyone agrees that the answer is yes, although there are still people who are like, yeah, but maybe it’s around the edges. I’m not among them, by the way, but … yeah, so initially there were like, “Yeah, this is cute and very impressive, but is it going to make a difference to a problem that matters?”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&lt;strong&gt; &lt;/strong&gt;So now, maybe this is a good time to get into what you’ve been doing with ALS [amyotrophic lateral sclerosis]. You know, there’s a knee-jerk reaction from the technology side to focus on designing small molecules, on predicting, you know, their properties, you know, maybe binding affinity or aspects of ADME [absorption, distribution, metabolism, and excretion], you know, like absorption or dispersion or whatever.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And all of that is very useful, but if I understand the work on ALS, you went to a much harder place, which is to actually identify and select targets.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; That’s right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; So first off, just for the benefit of the standard listeners of this podcast, explain what that problem is in general.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; No, for sure. And I think maybe I’ll start by just very quickly talking about the drug discovery and development arc, …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; … which, by and large, consists of three main phases. That’s the standard taxonomy.&amp;nbsp;The first is what’s called sometimes target discovery or identifying a therapeutic hypothesis, which looks like: if I modulate this target in this disease, something beneficial will happen.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Then, you have to take that target and turn it into a molecule that you can actually put into a person. It could be a small molecule. It could be a large molecule like an antibody, whatever. And then you have that construct, that molecule. And the last piece is you put it into a person in the context of a clinical trial, and you measure what has happened. And there’s been AI deployed towards each of those three stages in different ways.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The last one is mostly like an efficiency gain. You know, the trial is kind of already defined, and you want to deploy technology to make it more efficient and effective, which is great because those are expensive operations.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; The middle one is where I would say the vast majority of efforts so far has been deployed in AI because it is a nice, well-defined problem. It doesn’t mean it’s easy, but it’s one where you can define the problem. It is, &lt;em&gt;I need to inhibit this protein by this amount, and the molecule needs to be soluble and whatever and go past the blood-brain barrier&lt;/em&gt;. And you know probably within a year and a half or so, or two, if you succeeded or not.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The first stage is the one where I would say the least amount of energy has gone because when you’re uncovering a novel target in the context of an indication, you don’t know that you’ve been successful until you go &lt;em&gt;all the way&lt;/em&gt; to the end, which is the clinical trial, which is what makes this a long and risky journey. And not a lot of people have the appetite or the capital to actually do that.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;However, in my opinion, and that of, I think, quite a number of others, it is where the biggest impact can be made. And the reason is that while pharma has its deficiencies, making good molecules is actually something they’re pretty good at.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It might take them longer than it should, maybe it’s not as efficient as it could be, but at the end of the day, if you tell them to drug A target, pharma is actually pretty good at generating those molecules. However, when you put those molecules into the clinic, 90% of them fail. And the reason they fail is not by and large because the molecule wasn’t good. In the majority of cases, it’s because the target you went after didn’t do anything useful in the context of the patient population in which you put it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And so in order to fix the inefficiency of this industry, which is &lt;em&gt;incredible&lt;/em&gt; inefficiency, you need to address the problem at the root, and the root is picking the right targets to go after. And so that is what we elected to do.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It doesn’t mean we don’t make molecules. I mean, of course, you can’t just end up with a target because a target is not actionable. You need to turn it into a molecule. And we absolutely do that. And by the way, the partnership with Lilly&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; is actually one where they help us make a molecule.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER: &lt;/strong&gt;I mean, it’s our target. It’s our program. But Lilly is deploying its very state-of-the-art molecule-making capabilities to help us turn that target into a drug.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;So let’s get now into the machine learning of this. Again, this just strikes me as such a difficult problem to solve.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; So how does machine learning … how does AI help you?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; So I think when you look at how people currently select targets, it’s a combination of oftentimes at this point, with an increasing respect for the power of human genetics, some search for a genetic association, oftentimes with a human-defined, highly subjective, highly noisy clinical outcome, like some ICD [International Classification of Diseases] code.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And those are often underpowered and very difficult to deconvolute the underlying biology. You combine that with some mechanistic interrogation in a highly reductionist model system looking at a small number of readouts, biochemical readouts, that a biologist thinks are relevant to the disease. Like does this make this, whatever, cholesterol go up or amyloid beta go down? Or whatever. And then you take that as the second stage, and you pick, based on typically human intuition about, &lt;em&gt;Oh, this one looks good to me&lt;/em&gt;, and then you take that forward.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;What we’re doing is an attempt to be as unbiased and holistic as possible. So, first of all, rather than rely on human-defined clinical endpoints, like this person has been diagnosed with diabetes or fatty liver, we try and measure as much as we can a holistic physiological state and then use machine learning to find structure, patterns &lt;em&gt;in&lt;/em&gt; that human physiological readouts, imaging readouts, and omics readouts from blood, from tissue, different kinds of imaging, and say, these are different vectors that this disease takes, this group of individuals, and here’s a different group of individuals that maybe from a diagnostical perspective are all called the same thing, but they are actually exhibiting a very different biology underlying it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And so that is something that doesn’t emerge when a human being takes a reductionist view to looking at this high-content data, and oftentimes, they don’t even look at it and produce an ICD code.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right. Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; The same approach, actually even the same code base, is taken in the cellular data. So we don’t just say, “Well, the thing that matters is, you know, the total amount of lipid in the cell or whatever.” Rather, we say, “Let’s look at multiple readouts, multiple ways of looking at the cells, combine them using the power of machine learning.” And again, looking at imaging readouts where a human’s eyes just glaze over looking at even a few dozen cells, far less a few hundreds of millions of cells, and understand what are the different biological processes that are going on. What are the vectors that the disease might take you in this direction, in this group of cells, or in that direction?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And then importantly, we take all of that information from the human side, from the cellular side, across these different readouts, and we combine them using an integrative approach that looks at the combined weight of evidence and says, these are the targets that I have the greatest amount of conviction about by looking across all of that information. Whereas we know, and we know this, I’m sure you’ve seen this analysis done for clinicians, a human being typically is able to keep three or four things in their head at the same time.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; A really &lt;em&gt;good&lt;/em&gt; human being who’s really expert at what they do can maybe get to six to eight.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; The machine learning has no problem doing a few hundred.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; And so you put that together, and that allows you, to your earlier question, really select the targets around which you have the highest conviction. And then those are the ones that we then prioritize for interrogation in more expensive systems like mice and monkeys and then at the end of the day pick the small handful that one can afford to actually take into clinical trials.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;So now, Insitro recently received $25 million in milestone payments from Bristol Myers Squibb&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; after discovering and selecting a novel drug target for ALS. Can you tell us a little bit more about that? &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; We are incredibly excited about the first novel target, and there is a couple of others just behind it in line that seem, you know, quite efficacious, as well, that truly seem to reverse, albeit in a cellular system, what we now understand to be ALS pathology across multiple different dimensions. There’s been obviously many attempts made to try and address ALS, which by the way, horrible, horrible disease, worse than most cancers. It kills you almost inevitably in three to five years in a particularly horrific way.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And what we have in our hands is a target that seems to revert a lot of the pathologies that are associated with the disease, which we now understand has to do with the mis-splicing of multiple proteins within the cell and creating defective versions of those proteins that are just not operational. And we are seeing reversion of many of those.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So can I tell you for sure it’ll work in a human? No, there’s many steps between now and then. But we couldn’t be more excited about the opportunity to provide what we hope will be a disease-modifying intervention for these patients who really desperately need something.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Well, it’s certainly been making waves in the biotech and biomedical world.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; Thank you.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;So we’ll be really watching very closely.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So, you know, I think just reflecting on, you know, what we missed and what we got right in our book, I think in our book, we did have the insight that there would be an ability to connect, say, genotypic and phenotypic data and, you know, just broadly the kinds of clinical measurements that get made on real patients and that these things could be brought together. And I think the work that you’re doing really illustrates that in a very, very sophisticated, very ambitious way.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But the fact that this could be connected all the way down to the biology, to the biochemistry, I think we didn’t have any clue what would happen, at least not this quickly.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; Well, I think the …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;And I realize, you’ve been at this for quite a few years, but still, it’s quite amazing.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; The thread that connects them is human genetics. And I think that has, to us, been, sort of, the, kind of, the connective tissue that allows you to translate across different systems and say, “What does this gene do? What does this gene do in this organ and in that organ? What does it do in this type of cell and in that type of cell?”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And then use that as sort of the thread, if you will, that follows the impact of modulating this gene all the way from the simple systems where you can do the experiment to the complex systems where you can’t do the experiment until the very end, but you have the human genetics as a way of looking at the statistics and understanding what the impact might be.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;So I’d like to now switch gears and take … I want to take two steps in the remainder of this conversation towards the future. So one step into that future, of course, we’re living through now, which is just all of the crazy pace of work and advancement in generative AI generally, you know, just the scale of transformers, of post-training, and now inference scale and reasoning models and so on. And where do you see all of that going with respect to the goals that you have and that Insitro has?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; So I think first and foremost is the parallel, if you will, to the predictions that you focused on in your book, which is this will transform a lot of the core data processing tasks, the information tasks. And sure, the doctors and nurses is one thing. But if you just think of clinical trial operations or the submission of regulatory documents, these are all kind of simple data … they’re not simple, obviously, but they’re data processing tasks. They involve natural language. That’s not going to be our focus, but I hope that others will use that to make clinical trials faster, more efficient, less expensive.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;There’s already a lot of progress that’s happening on the molecular design side of things and taking hypotheses and turning them quickly and effectively into molecules. As I said, this is part of our work that we absolutely do and we don’t talk about it very much, simply because it’s a very crowded landscape and a lot of companies are engaged on that. But I think it’s really important to be able to take biological insights and turn them into new molecules.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And then, of course, the transformer models and their likes play a very significant role in that sort of turning insights into molecules because you can have foundation models for proteins. There are increasing efforts to create foundation models for other categories of molecules. And so that will undoubtedly accelerate the process by which you can quickly generate different molecular hypotheses and test them and learn from what you did so that you can do fewer iterations …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; … before you converge on a successful molecule.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I do think that arguably the biggest impact as yet to be had is in that understanding of core human biology and what are the right ways to intervene in it. And that plays a role in a couple different ways. First of all, it certainly plays a role in which … if we are able to understand the human physiological state and, you know, the state of different systems all the way down to the cell level, that will inform our ability to pick hypotheses that are more likely to actually impact the right biologies underneath.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yep. Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; And the more data we’re able to collect about humans and about cells, the more successful our models will be at representing that human physiological state or the cell biological state and making predictions reliably on the impact of these interventions.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The other side of it, though, and this comes back, I think, to themes that were very much in your book, is this will impact not only the early stages of which hypotheses we interrogate, which molecules we move forward, but also hopefully at the end of the day, which molecule we prescribe to which patient.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; And I think there’s been obviously so much narrative over the years about precision medicine, personalized medicine, and very little of that has come to fruition, with the exception of, you know, certain islands in oncology, primarily on genetically driven cancers.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But I think the opportunity is still there. We just haven’t been able to bring it to life because of the lack of the right kind of data. And I think with the increasing amount of human, kind of, foundational data that we’re able to acquire, things that are not sort of distilled through the eye of a clinician, for example, …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; … but really measurements of human pathology, we can start to get to some of that precision, carving out of the human population and then get to a world where we can prescribe the right medicine to the right patient and not only in cancer but also in other diseases that are also not a single disease.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;All right, so now to wrap up this time together, I always try to ask one more provocative last question. One of the dreams that comes naturally to someone like me or any of my colleagues, probably even to you, is this idea of, you know, wouldn’t it be possible someday to have a foundation model for biology or for human biology or foundation model for the human cell or something along these lines?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And in fact, there are, of course, you and I are both aware of people who are taking that idea seriously and chasing after it. I have people in our labs that think hard about this kind of thing. Is it a reasonable thought at all?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; I have learned over the years to avoid saying the word &lt;em&gt;never&lt;/em&gt; because technology proceeds in ways that you often don’t expect. And so will we at some point be able to measure the cell in enough different ways across enough different channels at the same time that you can piece together what a cell does? I think that is eminently feasible, not today, but over time.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I don’t think it’s feasible using today’s technology, although the efforts to get there may expose where the biggest opportunities lie to, you know, build that next layer. So I think it’s good that people are working on really hard problems. I would also point out that even if one were to solve that really challenging problem of creating a model of &lt;em&gt;a cell&lt;/em&gt;, there is thousands of different types of cells within the human body.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;They’re very different. They also talk to each other …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; … both within the cell type and across different cell types. So the combinatorial complexity of that system is, I think, unfathomable to many people. I mean, I would say to all of us.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER: &lt;/strong&gt;And so even from that very lofty goal, there is multiple big steps that would need to be taken to a &lt;em&gt;mechanistic&lt;/em&gt; model of the full organism. So will we ever get there? Again, you know, I don’t see a reason why this is impossible to do. So I think over time, technology will get better and will allow us to build more and more elaborate models of more and more complex systems.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Patients can’t wait …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right. Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; … for that to happen in order for us to get them better medicines. So I think there is a great basic science initiative on that side of things. And, in parallel, we need to make do with the data that we have or can collect or can print. We print a lot of data in our internal wet labs and get to drugs that are effective even though they don’t benefit from having a full-blown mechanistic model.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Last question: where do you think we’ll be in five years?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; Phew. If I had answered that question five years ago, I would have been very badly embarrassed at the inaccuracy of my answer. [LAUGHTER] So I will not answer it today either.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I will say that the thing about exponential curves is that they are very, very tricky, and they move in unexpected ways. I would hope that in five years, we will have made a sufficient investment in the generation of scientific data that we will be able to move beyond data that was generated entirely by humans and therefore insights that are derivative of what people already know to things that are truly novel discoveries.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And I think in order to do that in, you know, math, maybe because math is entirely conceptual, maybe you can do that today. Math is effectively a construct of the human mind. I don’t think biology is a construct of the human mind, and therefore one needs to collect enough data to really build those models that will give rise to those novel insights.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And that’s where I hope we will have made considerable progress in five years.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Well, I’m with you. I hope so, too. Well, you know, thank you, Daphne, so much for this conversation. I learn a lot talking to you, and it was great to, you know, connect again on this. And congratulations on all of this success. It’s really groundbreaking.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KOLLER:&lt;/strong&gt; Thank you very much, Peter. It was a pleasure chatting with you, as well.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[TRANSITION MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; I still think of Daphne first and foremost as an AI researcher. And for sure, her research work in machine learning continues to be incredibly influential to this day. But it’s her work on AI-enhanced drug development that now is on the verge of making a really big difference on some of the most difficult diseases afflicting people today.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In our book, Carey, Zak, and I predicted that AI might be a meaningful accelerant in biomedical research, but I don’t know that we foresaw the incredible potential specifically in drug development.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Today, we’re seeing a flurry of activity at companies, universities, and startups on generative AI systems that aid and maybe even completely automate the design of new molecules as drug candidates. But now, in our conversation with Daphne, seeing AI go even further than that to do what one might reasonably have assumed to be impossible, to identify and select novel drug targets, especially for a neurodegenerative disease like ALS, it’s just, well, mind blowing. &lt;/p&gt;



&lt;p&gt;Let’s continue our deep dive on AI and biomedical research with this conversation with Noubar Afeyan:&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Noubar, thanks so much for joining. I’m really looking forward to this conversation.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;NOUBAR AFEYAN: &lt;/strong&gt;Peter, thanks. Thrilled to be here.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; While I think most of the listeners to this podcast have heard of Flagship Pioneering&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, it’s still worth hearing from you, you know, what is Flagship? And maybe a little bit about your background. And finally, you found a way to balance science and business creation. And so, you know, your approach and philosophy to all of that.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN: &lt;/strong&gt;Well, great. So maybe I’ll just start out by way of quick background. You know, my … and since we’re going talk about AI, I’ll also highlight my first contact with the topic of AI. So as an undergraduate in 1980 up at McGill University, I was an engineering student, but I was really captivated by, at that time, the talk on the campus around the expert system, heuristic-based, rule-based kind of programs.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; And so actually I had the dubious distinction of writing my one and only college newspaper article. [LAUGHTER] That was a short career. And it was all about how artificial intelligence would be impacting medicine, would be impacting, you know, speech capture, translation, and some of the ideas that were there that it’s interesting to see now 45 years later re-emerge with some of the new learning-based models.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;My journey after college ended up taking me into biotechnology. In the early ’80s, I came to MIT to do a PhD. At the time, the field was brand new. I ended up being the first PhD graduate from MIT in this combination biology and engineering degree. And since then, I’ve basically been—so since 1987—a founder, a technologist in the space of biotechnology for human health and as well for planetary health.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And then in 1999/2000 formed what is now Flagship Pioneering, which essentially was an attempt to bring together the three elements of what we know are important in startups. That is scientific capital, human capital, and financial capital. Right now, startups get that from different places. The science in our fields mostly come from academia, research hospitals. The human capital comes from other startups …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; … or large companies or some academics leave. And then the financial capital is usually venture capital, but there’s also now more and more other deeper pockets of money.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;What we thought was, what if all that existed in one entity and instead of having to convince each other how much they should believe the other if we just said, “Let’s use that power to go work on much further out things”? But in a way where nobody would believe it in the beginning, but we could give ourselves a little bit of time to do impactful big things.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Twenty-five years later, that’s the road we’ve stayed on.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; OK. So let’s get into AI. Now, you know, what I’ve been asking guests is kind of an origin story. And there’s the origin story of contact with AI, you know, before the emergence of generative AI and afterwards. I don’t think there’s much of a point to asking you the pre-ChatGPT. But … so let’s focus on your first encounter with ChatGPT or generative AI. When did that happen, and what went through your head?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; Yeah. So, if you permit me, Peter, just for very briefly, let me actually say I had the interesting opportunity over the last 25 years to actually stay pretty close to the machine learning world …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah. Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; … because one, as you well know, among the most prolific users of machine learning has been the bioinformatics computational biology world because it’s been so data rich that anything that can be done, people have thrown at these problems because unlike most other things, we’re not working on man-made data. We’re looking at data that comes from nature, the complexity of which far exceeds our ability to comprehend.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So you could imagine that any approach to statistically reduce complexity, get signal out of scant data—that’s a problem that’s been around.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The other place where I’ve been exposed to this, which I’m going to come back to because that’s where it first felt totally different to me, is that some 25 years ago, actually the very first company we started was a company that attempted to use evolutionary algorithms to essentially iteratively evolve consumer-packaged goods online. Literally, we tried to, you know, consider features of products as genes and create little genomes of them. And by recombination and mutation, we could create variety. And then we could get people through panels online—this was 2002/2003 timeframe—we could essentially get people through iterative cycles of voting to create a survival of the fittest. And that’s a company that was called Affinnova.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The reason I say that is that I knew that there’s a much better way to do this if only: one, you can generate variety …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; … without having to prespecify genes. We couldn’t do that before. And, two, which we’ve come back to nowadays, you can actually mimic how humans think about voting on things and just get rid of that element of it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So then to your question of when does this kind of begin to feel different? So you could imagine that in biotechnology, you know, as an engineer by background, I always wanted to do CAD, and I picked the one field in which CAD doesn’t exist, which is biology. Computer-aided design is kind of a notional thing in that space. But boy, have we tried. For a long time, …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; … people would try to do, you know, hidden Markov models of genomes to try to figure out what &lt;em&gt;should&lt;/em&gt; be the next, you know, base that you may want to or where genes might be, etc. But the notion of generating in biology has been something we’ve tried for a while. And in the late teens, so kind of 2018, ’17, ’18, because we saw deep learning come along, and you could basically generate novelty with some of the deep learning models … and so we started asking, “Could you generate a protein basically by training a correspondence table, if you will, between protein structures and their underlying DNA sequence?” Not their protein sequence, but their DNA sequence.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; So that’s a big leap. So ’17/’18, we started this thing. It was called 56. It was FL56, Flagship Labs 56, our 56th project.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;By the way, we started this parallel one called “57” that did it in a very different way. So one of them did pure black box model-building. The other one said, you know what, we don’t want to do the kind of … at that time, AlphaFold was in its very early embodiments. And we said, “Is there a way we could actually take little, you know, multi amino acid kind of almost grammars, if you will, a little piece, and then see if we could compose a protein that way?” So we were experimenting.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And what we found was that actually, if you show enough instances and you could train a transformer model—back in the day, that’s what we were using—you could actually, say, predict another sequence that should have the same activity as the first one.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN: &lt;/strong&gt;So we trained on green fluorescent proteins. Now, we’re talking about seven years ago. We trained on enzymes, and then we got to antibodies.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;With antibodies, we started seeing that, boy, this could be a pretty big deal because it has big market impact. And we started bringing in some of the diffusion models that were beginning to come along at that time. And so we started getting much more excited. This was all done in a company that subsequently got renamed from FL56 to Generate:Biomedicines&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yep, yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN: &lt;/strong&gt;… which is one of the leaders in protein design using the generative techniques. It was interesting because Generate:Biomedicines is a company that was called that before generative AI was a thing, [LAUGHTER] which was kind of very ironic.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And, of course, that team, which operates today very, very kind of at the cutting edge, has published their models. They came up with this first Chroma&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; model, which is a diffusion-based model, and then started incorporating a lot of the LLM capabilities and fusing them.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Now we’re doing atomistic models and many other things. The point being, that gave us a glimpse of how quickly the capability was gaining, …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah. Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; … just like evolution shows you. Sometimes evolution is super silent, and then all of a sudden, all hell breaks loose. And that’s what we saw.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right. One of the things that I reflect on just in my own journey through this is there are other emotions that come up. One that was prominent for me early on was skepticism. Were there points when even in your own work, transformer-based work on this early on, that you had doubts or skepticism that these transformer architectures would be or diffusion-based approaches would be worth anything?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; You know, it’s interesting, I think that, I’m going to say this to you in a kind of a friendly way, but you’ll understand what I mean. In the world I live in, it’s kind of like the slums of innovation, [LAUGHTER] kind of like just doing things that are not supposed to work. The notion of skepticism is a luxury, right. I assume everything we do won’t work. And then once in a while I’m wrong.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And so I don’t actually try to evaluate whether before I bring something in, like just think about it. We, some hundred or so times a year, ask “what if” questions that lead us to totally weird places of thought. We then try to iterate, iterate, iterate to come up with something that’s testable. Then we go into a lab, and we test it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So in that world, right, sitting there going, like, “How do I know this transformer is going to work?” The answer is, “For what?” Like, it’s going to work. To make something up … well, guess what? We knew early on with LLMs that hallucination was a feature, not a bug for what we wanted to do.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So it’s just such a different use that, of course, I have trained scientific skepticism, but it’s a little bit like looking at a competitive situation in an ecology and saying, “I bet that thing’s going to die.” Well, you’d be right—most of the time, you’d be right. [LAUGHTER]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So I just don’t … like, it … and that’s why—I guess, call me an early adopter—for us, things that could move the needle even a little, but then upon repetition a lot, let alone this, …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; … you have to embrace. You can’t wait there and say, I’ll embrace it once it’s ready. And so that’s what we did.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Hmm. All right. So let’s get into some specifics and what you are seeing either in your portfolio companies or in the research projects or out in the industry. What is going on today with respect to AI really being used for something meaningful in the design and development of drugs?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; In companies that are doing as diverse things as—let me give you a few examples—a project that’s now become a named company called ProFound Therapeutics&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; that literally discovered three, four years ago, and would not have been able to without some of the big data-model-building capabilities, that our cells make literally thousands, if not tens of thousands, of more proteins than we were aware of, full stop.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;We had done the human genome sequence, there was 20,000 genes, we thought that there was …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Wow.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; … maybe 70-80,000, 100,000 proteins, and that’s that. And it turns out that our cells have a penchant to express themselves in the form of proteins, and they have many other ways than we knew to do that.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Now, so what does that mean? That means that we have generated a massive amount of data, the interpretation of which, the use of which to guide what you do and what these things might be involved with is purely being done using the most cutting-edge data-trained models that allow you to navigate such complexity.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Wow. Hmm.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; That’s just one example. Another example: a company called Quotient Therapeutics&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, again three, four years old. I can talk about the ones that are three, four years old because we’ve kind of gotten to a place where we’ve decided that it’s not going to fail &lt;em&gt;yet&lt;/em&gt;, [LAUGHTER] so we can talk about it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;You know, we discovered—our team discovered—that in our cells, right, so we know that when we get cancer, our cells have genetic mutations in them or DNA mutations that are correlated and often causal to the hyperproliferative stages of cancer. But what we assume is that all the other cells in our body, pretty much, have one copy of their genes from our mom, one copy from our dad, and that’s that.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And when very precise deep sequencing came along, we always asked the question, “How much variation is there cell to cell?”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; And the answer was it’s kind of noise, random variation. Well, our team said, “Well, what if it’s not really that random?” because upon cell division cycles, there’s selection happening on these cells. And so not just in cancer but in liver cells, in muscle cells, in skin cells …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Oh, interesting.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; … can you imagine that there’s an evolutionary experiment that is favoring either compensatory mutations that are helping you avoid disease or disease-caused mutations that are gaining advantage as a way to understand the mechanism? Sure enough—I wouldn’t be telling you otherwise—with &lt;em&gt;massive&lt;/em&gt; amount of single cell sequencing from individual patient samples, we’ve now discovered that the human genome is mutated on average in our bodies 10,000 times, like over every base, like, it’s huge numbers.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And we’re finding very interesting big signals come out of this massive amount of data. By the way, data of the sort that the human mind, if it tries to assign causal explanations to what’s happening …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; … is completely inadequate.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; When you think about a language model, we’re learning from human language, and the totality of human language—at least relative to what we’re able to compute today in terms of constructing a model—the totality of human language is actually pretty limited. And in fact, you know, as is always written about in click-baity titles, you know, the big model builders are actually starting to run short.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; Running out, running out, yes. [LAUGHTER]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; But one of the things that perplexes me and maybe even worries me—like these two examples—are generally in the realm of cellular biology and the complexity. Let’s just take the example of your company, ProFound. You know, the complexity of what’s going on and the potential genetic diversity is such that, can we ever have enough data? You know, because there just aren’t that many human beings. There just aren’t that many samples.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN: &lt;/strong&gt;Well, it depends on what you want to train, right. So if you want to train a &lt;em&gt;de novo&lt;/em&gt; evolutionary model that could take you from bacteria to human mammalian cells and the like, there may not be—and I’m not an expert in that—but that’s a question that we often kind of think about.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But if you’re trying to train a … like you know what the proteins we know about, how they interact with pathways and disease mechanisms and the like. Now all of a sudden you find out that there’s a whole continent of them missing in your explanations. But there are things you can reason, in quotations, through analogy, functional analogy, sequence analogy, homology. So there’s a lot of things that we could do to essentially make use of this, even though you may not have the totality of data needed to, kind of, predict, based on a de novo sequence, exactly what it’s going to do.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So I agree with the comparison. But … but you’re right. The complexity is … just keep in mind, on average, a protein may be interacting with 50 to 100 other proteins.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; So if you find thousands of proteins, you’ve found a massive interaction space through which information is being processed in a living cell.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;But do you find in your AI companies that access to data ends up being a key challenge? Or, you know, how central is that?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; Access to data is a key challenge for the companies we have that are trying to build just models. But that’s the minority of things we do. The majority of things we do is to actually co-develop the data and the models. And as you know well, because you guys, you know, have given us some ideas around this space, that, you know, you could generate data and &lt;em&gt;then&lt;/em&gt; think about what you’re to do with it, which is the way biotech is operated with bioinformatics.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right, right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; Or you could generate bespoke data that is used to train the model that’s quite separate from what you would have done in the natural course of biology. So we’re doing much more of the latter of late, and I think that’ll continue. So, but these things are proliferating.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I mean,&lt;strong&gt; &lt;/strong&gt;it’s hard to find a place where we’re not using this.&lt;strong&gt; &lt;/strong&gt;And the “this” is any and all data-driven model building, generative, LLM-based, but also every other technique to make progress.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Sure.&lt;strong&gt; &lt;/strong&gt;So now moving away from the straight biochemistry applications, what about AI in the process of building a business, of making investment decisions, of actually running an operation? What are you seeing there?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; So, well, you know, Moderna, which is a company that I’m quite proud of being a founder and chairman of, has adopted a significant, significant amount of AI embedded into their operations in all aspects: from the manufacturing, quality control, the clinical monitoring, the design—every aspect. And in fact, they’ve had a partnership that they’ve had for a little while here with OpenAI, and they’ve tried many different ways to stay at the cutting edge of that.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So we see that play out at some scale. That’s a 5,000-, 6,000-person organization, and what they’re doing is a good example of what early adopters would do, at least in our kind of biotechnology company.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But then, you know, in our space, I would say the efficiency impact is kind of no different, than, you know, anywhere else in academia you might adopt it or in other kinds of companies. But where I find it an interesting kind of maybe segue is the degree to which&lt;strong&gt; &lt;/strong&gt;it may fundamentally change the way we think about how to do science, which is a whole other use, right?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; So it’s not an efficiency gain &lt;em&gt;per se&lt;/em&gt;, although it’s maybe an effectiveness gain when it comes to science, but can you just fundamentally train models to generate hypotheses?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN: &lt;/strong&gt;And we have done that, and we’ve been doing this for the last three years. And now it’s getting better and better, the better these reasoning engines are getting and kind of being able to extrapolate and train for novelty. Can you convert that to the world’s best experimental protocol to very precisely falsify your hypothesis, on and on?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;That closing of that loop, kind of what we call &lt;em&gt;autonomous science&lt;/em&gt;, which we’ve been trying to do for the last two, three years and are making some progress in, that to me is another kind of bespoke use of these things, not to generate molecules in its chemistry, but to change the behavior of how science is done.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah.&lt;strong&gt; &lt;/strong&gt;So I always end with a couple of provocative questions, but I need—before we do that, while we’re on this subject—to get your take on Lila Sciences&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And there is a vision there that I think is very interesting. It’d be great to hear it described by you.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; Sure. So Lila, after operating for two to three years in kind of a preparatory kind of stealth mode, we’ve now had a little bit more visibility around, and essentially what we’re trying to do there is to create what we call automated science factories, and such a factory would essentially be able to take problems, either computationally specified or human-specified, and essentially do the experimental work in order to either make an optimization happen or enable something that just didn’t exist. And it’s really, at this point, we’ve shown proof of concept in narrow areas.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; But it’s hard to say that if you can do this, you can’t do some other things, so we’re just expanding it that way. We don’t think we need a complete proof or complete demonstration of it for every aspect.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; So we’re just kind of being opportunistic. The idea for Lila is to partner with a number of companies. The good news is, within Flagship, there’s 48 of them. And so there’s a whole lot of them they can partner with to get their learning cycles. But eventually they want to be a real alternative to every time somebody has an idea, having to kind of go into a lab and manually do this.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I do want to say one thing we touched on, Peter, though, just on that front, which is …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; … if you say, like, “What problem is this going to solve?” It’s several but an important one is just the flat-out human capacity to reason on this much data and this much complexity that is real. Because nature doesn’t try to abstract itself in a human understandable form.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right. Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; In biology, since it’s kind of like progress happens through evolutionary kind of selections, the evidence of which [has] long been lost, and so therefore, you just see what you have, and then it has a behavior. I really do think that there’s something to be said, and I want to—just for your audience—lay out a provocative, at least, thought on all this, which Lila is a beginning embodiment of, which is that I really think that what’s going to happen over the next five, 10 years, even while we’re all fascinated with the impending arrival of AGI [artificial general intelligence] is really what I call &lt;em&gt;poly-intelligence&lt;/em&gt;, which is the combination of human intelligence, machine intelligence, AI, and nature’s intelligence.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;We’re all fascinated at the human-machine interface. We know the human-nature interface, but imagine the machine-nature interface—that is, actually letting loose a digital kind of information processing life form through the algorithms that are being developed and the commensurately complex, maybe much more complex. We’ll see. And so now the question becomes, what does the human do?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And we’re living in a world which is human dominated, which means the humans say, “If I don’t understand it, it’s not real, basically. And if I don’t understand it, I can’t regulate it.” And we’re going to have to make peace with the fact that we’re not going to be able to predictably affect things without necessarily understanding them the way we could if we just forced ourselves to only work on problems we can understand. And that world we’re not ready for at all.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah. All right. So this one I predict is going to be a little harder for you because I think while you think about the future, you live very much in the present. But I’d like you to make some predictions about what the biotech and biopharmaceutical industries are going to be able to do two years from now, five years from now, 10 years from now.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; Yeah, well, it’s hard for me because you know my nature, which is that I think this is all emergent.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; And so I would be the conceit of predicting. So I would say with likelihood positive predictive value of less than 10%, I’m happy to answer your question. So I’m not trying to score high [LAUGHTER] because I really think that my job is to envision it, not to predict it. And that’s a little bit different, right?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah, I actually was trying to pick what would be the hardest possible question I could ask you, [LAUGHTER] and this is what I came up with.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; Yeah, no, no, I’m kidding here. So now look, I think that we will cross this threshold of understandability. And of course you’re seeing that in a lot of LLM things today. And of course, people are trying to train for things that are explainers and all that whole, there’s a whole world of that. But I think at some point we’re going to have to kind of let go and get comfortable working on things that, you know …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I sometimes tell people, you know, and I’m not the first, but scientists and engineers are different, it’s said, in that engineers work on things that they don’t wait until they get a full understanding of before they work with them. Well, now scientists are going to have to get used to that, too, right?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah. Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; Because insisting that it’s only valid if it’s understandable. So, I would say, look, I hope that the time … for example, I think major improvements will be made in patient selection. If we can test drugs on patients that are more synchronized as to the stage of their disease …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; … I think the answer will be much better. We’re working on that. It’s a company called Etiome&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, very, very early stage. It’s really beautiful data, very early data that shows that when we talk about MASH [metabolic dysfunction-associated steatohepatitis], liver disease, when we talk about Parkinson’s, there’s such a heterogeneity, not only of the subset type of the disease, but the stage of the disease, that this notion that you have stage one cancer, stage two cancer, again, nobody told nature there’s stages of that kind. It’s a continuum.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But if you can synchronize based on training, kind of, the ability to detect who are the patients that are in enough of a close proximity that should be treated so that the trial—much smaller a trial size—could give you a drug, then afterwards, you can prescribe it using these approaches.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Kind of we’re going to find that what we thought is one disease is more like 15 diseases. That’s bad news because we’re not going to be able to claim that we can treat everything which we can. It’s good news in that there’s going to be people who are going to start making much more specific solutions to things.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; So I can imagine that. I can imagine a generation of, kind of, students who are going to be able to play in this space without having 25 years of graduate education on the subject. So what is deemed knowledge sufficient to do creative things will change. I can go on and on, but I think all this is very close by and it’s very exciting.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Noubar, I just always have so much fun, and I learn really a lot. It’s high-density learning when I talk to you. And so I hope our listeners feel the same way. It’s something I really appreciate.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFEYAN:&lt;/strong&gt; Well, Peter, thanks for this. And I think your listeners know that if I was asking you questions, you would be answering them with equal if not more fascinating stuff. So, thanks for giving me the chance to do that today.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[TRANSITION MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;I’m always fascinated by Noubar’s perspectives on fundamental research and how it connects to human health and the building of successful companies. I see him as a classic “systems thinker,” and by that, I mean he builds impressive things like Flagship Pioneering itself, which he created as a kind of biomedical innovation system.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In our conversation, I was really struck by the fact that he’s been thinking about the potential impact of transformers—transformers being the fundamental building block of large language models—as far back as 2017, when the first paper on the attention mechanism in transformers was published by Google.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But, you know, it isn’t only about using AI to do things like understand and design molecules and antibodies faster. It’s interesting that he is also pushing really hard towards a future where AI might “close the loop” from hypothesis generation, to experiment design, to analysis, and so on.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Now, here’s my conversation with Dr. Eric Topol:&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Eric, it’s really great to have you here.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;ERIC TOPOL: &lt;/strong&gt;Oh, Peter, I’m thrilled to be here with you here at Microsoft.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; You’re a super famous person. Extremely well known to researchers even in computer science, as we have here at Microsoft Research.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But the question I’d like to ask is, how would you explain to your parents what you do every day?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; [LAUGHS] That’s a good question. If I was just telling them I’m trying to come up with better ways to keep people healthy, that probably would be the easiest way to do it because if I ever got in deeper, I would lose them real quickly. They’re not around, but just thinking about what they could understand.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; I think as long as they knew it was work centered on innovative paths to promoting and preserving human health, that would get to them, I think.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; OK, so now, kind of the second topic, and then we let the conversation flow, is about origin stories with respect to AI. And with most of our guests, you know, I factor that into two pieces: the encounters with AI before ChatGPT and what we call generative AI and then the first contacts after.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And, of course, you have extensive contact with both now. But let’s start with how you got interested in machine learning and AI prior to ChatGPT. How did that happen?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Yeah, it was out of necessity. So back, you know, when I started at Scripps at the end of ’06, we started accumulating, you know, massive datasets. First, it was whole genomes. We did one of the early big cohorts of 1,400 people of healthy aging. We called the Wellderly whole genome sequence&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And then we started big in the sensor world, and then we started saying, what are we going to do with all this data, with electronic health records and all those sensors? And now we got whole genomes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And basically, what we were doing, we were in hoarding mode. We didn’t have a way to meaningfully analyze it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;You would read about how, you know, data is the new oil and, you know, gold and whatnot. But we just didn’t have a way to extract the juice. And even when we wanted to analyze genomes, it was incredibly laborious.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; And we weren’t extracting a lot of the important information. So that’s why … not having any training in computer science, when I was doing the … about three years of work to do the book &lt;em&gt;Deep Medicine&lt;/em&gt;, I started really, first auto-didactic about, you know, machine learning. And then I started contacting a lot of the real top people in the field and hanging out with them, and learning from them, getting their views as to, you know, where we are today, what models are coming in the future.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And then I said, “You know what? We are going to be able to fix this mess.” [LAUGHS] We’re going to get out of the hoarding phase, and we’re going to get into, you know, really making a difference.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So that’s when I embraced the future of AI. And I knew, you know, back—that was six years ago when it was published and probably eight or nine years ago when I was doing the research, and I knew that we weren’t there yet.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;You know, at the time, we were seeing the image interpretation. That was kind of the early promise. But really, the models that were transformative, the transformer models, they were incubating back in 2017. So people knew something was brewing.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right. Yes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; And everyone said we’re going to get there.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;So then, ChatGPT comes out November of 2022; there’s GPT-4 in 2023, and now a lot has happened. Do you remember what your first encounter with that technology was?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Oh, sure. First, ChatGPT. You know, in the last days of November ’22, I was just blown away. I mean, I’m having a conversation. I’m having fun. And this is humanoid responding to me. I said, “&lt;em&gt;What?&lt;/em&gt;” You know? So that was to me, a moment I’ll never forget. And so I knew that the world was, you know, at a very kind of momentous changing point.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Of course, knowing, too, that this is going to be built on, and built on quickly. Of course, I didn’t know how soon GPT-4 and all the others were going to come forward, but that was a wake-up call that the capabilities of AI had just made a humongous jump, which seemingly was all of a sudden, although I did know this had been percolating …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; … you know, for what, at least five years, that, you know, it really was getting into its position to do this.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; I know one of the things that was challenging psychologically and emotionally for me is, it made me rethink a lot of things that were going on in Microsoft Research in areas like causal reasoning, natural language processing, speech processing, and so on.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I’m imagining you must have had some emotional struggles too because you have this amazing book, &lt;em&gt;Deep Medicine&lt;/em&gt;. Did you have to … did it go through your mind to rethink what you wrote in &lt;em&gt;Deep Medicine &lt;/em&gt;in light of this or, or, you know, how did that feel?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;It’s funny you ask that because in this one chapter I have on the virtual health coach, I wrote a whole bunch of scenarios …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; … that were very kind of futuristic. You know, about how the AI interacts with the person’s health and schedules their appointment for this and their scan and tells them what lab tests they should tell their doctor to have, and, you know, all these things. And I sent a whole bunch of these, thinking that they were a little too far-fetched.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;And I sent them to my editor when I wrote the book, and he says, “Oh, these are great. You should put them all in.” [LAUGHTER] What I didn’t realize is they weren’t that, you know, they were all going to happen.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&amp;nbsp;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah. They weren’t that far-fetched at all.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Not at all. If there’s one thing I’ve learned from all this, is our imagination isn’t big enough.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&amp;nbsp;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; We think too small.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Now in our book that Carey, Zak, and I wrote, you know, we made, you know, we sort of guessed that GPT-4 might help biomedical researchers, but I don’t think that any of us had the thought in mind that the architecture around generative AI would be so directly applicable to, you know, say, protein structures or, you know, to clinical health records and so on.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And so a lot of that seems much more obvious today. But two years ago, it wasn’t. But we did guess that biomedical researchers would find this interesting and be helped along.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So as you reflect over the past two years, you know, do you have things that you think are very important, kind of, meaningful applications of generative AI in the kinds of research that Scripps does?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL&lt;/strong&gt;: Yeah. I mean, I think for one, you pointed out how the term &lt;em&gt;generative AI&lt;/em&gt; is a misnomer.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; And so it really was prescient about how, you know, it had a pluripotent capability in every respect, you know, of editing and creating. So that was something that I think was telling us, an indicator that this is, you know, a lot bigger than how it’s being labeled. And our expectations can actually be more than what we had seen previously with the earlier version.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So I think what’s happened is that now, we keep jumping. It’s so quick that we can’t … you know, first we think, oh, well, we’ve gone into the agentic era, and then we could pass that with reasoning. [LAUGHTER] And, you know, we just can’t …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; It’s just wild.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; So I think so many of us now will put in prompts that will necessitate or ideally result in a not-immediate gratification, but rather one that requires, you know, quite a bit of combing through the corpus of knowledge …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; … and getting, with all the citations, a report or a response. And I think now this has been a reset because to do that on our own, it takes, you know, many, many hours. And it’s usually incomplete.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But one of the things that was so different in the beginning was you would get the references from up to a year and a half previously.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;And that’s not good enough. [LAUGHS]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;And now you get references, like, from the day before.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yes. Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;And so, you say, “Why would you do a regular search for anything when you could do something like this?”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; And then, you know, the reasoning power. And a lot of people who are not using this enough still are talking about, “Well, there’s no reasoning.”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; Which you dealt with really well in the book. But what, of course, you couldn’t have predicted is the new dimensions.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; I think you nailed it with GPT-4. But it’s all these just, kind of, stepwise progressions that have been occurring because of the velocity that’s unprecedented. I just can’t believe it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;We were aware of the idea of multi-modality, but we didn’t appreciate, you know, what that would mean. Like AlphaFold&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; [protein structure database], you know, the ability for AI to understand—or crystal structures—to really start understanding something more fundamental about biochemistry or medicinal chemistry.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I have to admit, when we wrote the book, we really had no idea.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Well, I feel the same way. I still today can’t get over it because the reason AlphaFold and Demis [Hassabis] and John Jumper [AlphaFold’s co-creators] were so successful is there was this protein databank.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;And it had been kept for decades. And so, they had the substrate to work with.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; So, you say, “OK, we can do proteins.” But then how do you do everything else?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; And so this whole, what I call, “large language of &lt;em&gt;life&lt;/em&gt; model” work, which has gone into high gear like I’ve never seen.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; You know, now to this holy grail of a virtual cell, and …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; You know, it’s basically … it’s … it was inspired by proteins. But now it’s hitting on, you know, ligands and small molecules, cells. I mean, nothing is being held back here.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; So how could anybody have predicted that?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; I sure wouldn’t have thought it would be possible at this point.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah. So just to challenge you, where do you think that is going to be two years from now? Five years from now? Ten years from now? Like, so you talk about a virtual cell. Is that achievable within 10 years, or is that still too far out?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; No, I think within 10 years for sure. You know the group that got assembled that Steve Quake&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; pulled together?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; I think has 42 authors in a paper&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; in &lt;em&gt;Cell&lt;/em&gt;. The fact that he could get these 42 experts in life science and some in computer science to come together and all agree …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; … that not only is this a worthy goal, but it’s actually going to be realized, that was impressive.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I challenged him about that. How did you get these people all to agree? So many of them were naysayers. And by the time the workshop finished, they were fully convinced. I think that what we’re seeing is so much progress happening so quickly. And then all the different models, you know, across DNA, RNA, and everything are just zooming forward.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; And it’s just a matter of pulling this together. Now when we have that, and I think it could easily be well before a decade and possibly, you know, between the five- and 10-year mark—that’s just a guess—but then we’re moving into another era of life science because right now, you know, this whole buzz about drug discovery.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;It’s not… with the ability to do all these perturbations at a cellular level.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Or the cell of interest.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; Or the cell-to-cell interactions or the intra-cell interaction. So once you nail that, yeah, it takes it to a kind of another predictive level that we haven’t really fathomed. So, yes, there’s going to be drug discovery that’s accelerated. But this would make that and also the underpinnings of diseases.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; So the idea that there’s so many diseases we don’t understand now. And if you had virtual cell, …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; … you would probably get to that answer …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; … much more quickly. So whether it’s underpinnings of diseases or what it’s going to take to really come up with far better treatments—preventions—I think that’s where virtual cell will get us.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; There’s a technical question … I wonder if you have an opinion. You may or may not. There is sort of what I would refer to as &lt;em&gt;ab initio&lt;/em&gt; approaches to this. You know, you start from the fundamental physics and chemistry, and we know the laws, we have the math and, you know, we can try to derive from there … in fact, we can even run simulations of that math to generate training data to build generative models and work up to a cell, &lt;em&gt;or&lt;/em&gt; forget all of that and just take as many observations and measurements of, say, living cells as possible, and just have faith that hidden amongst all of the observational data, there is structure and language that can be derived.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So that’s sort of bottom-up versus top-down approaches. Do you have an opinion about which way?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; Oh, I think you go after both. And clearly whenever you’re positing that you’ve got a virtual cell model that’s working, you’ve got to do the traditional methods as well to validate it, and … so all that. You know, I think if you’re going to go out after this seriously, you have to pull out all the stops. Both approaches, I think, are going to be essential.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; You know, if what you’re saying is true, and it is amazing to hear the confidence, the one thing I tried to explain to someone nontechnical is that for a lot of problems in medicine, we just don’t have enough data in a really profound way. And the most profound way to say that is, since Adam and Eve, there have only been an estimated 106 billion people who have ever lived.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So even if we had the DNA of every human being, every individual of &lt;em&gt;Homo sapiens&lt;/em&gt;, there are certain problems for which we would not have enough data.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Sure.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;And so I think another thing that seems profound to me, if we can actually have a virtual cell, is we can actually make trillions of virtual …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; Yeah&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; … human beings. The true genetic diversity could be realized for our species.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;I think you nailed it. The ability to have that type of data, no less synthetic data, I mean, it’s just extraordinary.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;We will get there someday. I’m confident of that. We may be wrong in projections. And I do think [science writer] Philip Ball won’t be right that it will never happen, though. [LAUGHTER] No, I think that if there’s a holy grail of biology, this is it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;And I think you’re absolutely right about where that will get us.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Transcending the beginning of the species.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; Of&lt;em&gt; our&lt;/em&gt; species.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah. All right. So now, we’re starting to run short on time here. And so I wanted to ask you about, I’m in my 60s, so I actually think about this a lot more. [LAUGHTER] And I know you’ve been thinking a lot about longevity. And, of course, your new book, &lt;em&gt;Super Agers&lt;/em&gt;.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And one of the reasons I’m so eager to read is it’s a topic very top of mind for me and actually for a lot of people. Where is this going? Because this is another area where you hear so much hype. At the same time, you see Nobel laureate scientists …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; … working on this.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; So, so what’s, what’s real there?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Yeah. Well, it’s really … the real deal is the science of aging is zooming forward.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And that’s exciting. But I see it bifurcating. On the one hand, all these new ideas, strategies to reverse aging are very ambitious. Like cell reprogramming and senolytics and, you know, the rejuvenation of our thymus gland, and it’s a long list.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; And they’re really cool science, and it used to be the mouse lived longer. Now it’s the old mouse looks really young.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah. Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; All the different features. A blind mouse with cataracts is all of a sudden there’s no cataracts. I mean, so these things are exciting, but none of them are proven in people, and they all have significant risk, no less, you know, the expense that might be attached.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; And some people are jumping the gun. They’re taking rapamycin, which can really knock out their immune system. So they all carry a lot of risk. And people are just getting a little carried away. We’re not there yet.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But the other side, which is what I emphasize in the book, which is exciting, is that we have all these new metrics that came out of the science of aging.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; So we have clocks of the body. Our biological clock versus our chronological clock, and we have organ clocks. So I can say, you know, Peter, we’ve assessed all your organs and your immune system. And guess what? Every one of them is either at or less than your actual age.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;And that’s very reassuring. And by the way, your methylation clock is also … I don’t need to worry about you so much. And then I have these other tests that I can do now, like, for example, the brain. We have an amazing protein p-Tau217 that we can say over 20 years in advance of you developing Alzheimer’s, …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; … we can look at that, and it’s modifiable by lifestyle, bringing it down. It should be you can change the natural history. So what we’ve seen is an explosion of knowledge of metrics, proteins, no less, you know, our understanding at the gene level, the gut microbiome, the immune system. So that’s what’s so exciting. How our immune system ages. &lt;em&gt;Immunosenescence&lt;/em&gt;. How we have more inflammation—&lt;em&gt;inflammaging&lt;/em&gt;—with aging. So basically, we have three diseases that kill us, that take away our health: heart, cancer, and neurodegenerative.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; And they all take more than 20 years. They all have a defective immune system inflammation problem, and they’re all going to be preventable.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;That’s what’s so exciting.&amp;nbsp;So we don’t have to have reverse aging. We can actually work on …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Just prevent aging in the first place.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;…&lt;strong&gt; &lt;/strong&gt;the age-related diseases. So basically, what it means is: I got to find out if you have a risk, if you’re in this high-risk group for this particular condition, because if you are—and we have many levels, layers, orthogonal ways to check—we don’t just bank it all on one polygenic test. We’re going to have several ways, say this is the one we are going …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And then we go into high surveillance, where, let’s say if it’s your brain, we do more p-Tau, if we need to do brain imaging—whatever it takes. And also, we do preventive treatments on top of the lifestyle [changes], that one of the problems we have today is a lot of people know generally, what are good lifestyle factors. Although, I go through a lot more than people generally acknowledge.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But they don’t incorporate them because they don’t know that they’re at risk and they could change their … extend their health span and prevent that disease. So what I at least put out there, a blueprint, is how we can use AI, because it’s multimodal AI, with all these layers of data, and then temporally, it’s like today you could say if you have two protein tests, not only are you going to have Alzheimer’s, but within a two-year time frame &lt;em&gt;when&lt;/em&gt; …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;… and if you don’t change things, if we don’t gear up … you know, we can … we can completely prevent this, so … or at least defer it for a decade or more. So that’s why I’m excited, is that we made these strides in the science of aging. But we haven’t acknowledged the part that doesn’t require reversing aging. There’s this much less flashy, attainable, less risky approach …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;…&lt;strong&gt; &lt;/strong&gt;than the one that … when you reverse aging, you’re playing with the hallmarks of cancer. They are like, if you look at the hallmarks of cancer …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; That has been one of the primary challenges.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;They’re lined up.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; They’re all the same, you know, whether it’s telomeres, or whether it’s … you know … so this is the problem. I actually say in the book, I do think one of these—we have so many shots on goal—one of these reverse aging things will likely happen someday. But we’re nowhere close.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;On the other hand, let’s gear up. Let’s do what we can do. Because we have these new metrics that’s … people don’t … like, when I read the organ clock paper&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; from Tony Wyss-Coray from Stanford. It was published end of ’23; it was the cover of &lt;em&gt;Nature&lt;/em&gt;. It blew me away.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;And I wrote a Substack&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; [article] on it. And Tony said, “Well, that’s so nice of you.” I said, “So nice? This is revolutionary, you know.” [LAUGHTER] So …&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;By the way, what’s so interesting is, how these things, this kind of understanding and AI, are coming together.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Yes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;It’s almost eerie the timing of these things.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Absolutely. Because you couldn’t take all these layers of data, just like we were talking about data hoarding.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL:&lt;/strong&gt; Now we have data hoarding on individual with no way to be able to make these assessments of what level of risk, when, what are we going to do in &lt;em&gt;this&lt;/em&gt; individual to prevent that? We can do that now.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;We can do it today. And we could keep building on that. So I’m really excited about it. I think that, you know, when I wrote the last book on deep medicine, it was our overarching goal should be to bring back the patient-doctor relationship. I’m an old dog, and I know what it used to be when I got out of medical school.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It’s totally … you couldn’t imagine how much erosion from the ’70s, ’80s to now. But now I have a new overarching goal. I’m thinking that that still is really important—humanity in medicine—but let’s prevent these three … big three diseases because it’s an opportunity that we’re not … you know, in medicine, all my life we’ve been hearing and talking about we need to prevent diseases.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Curing is much harder than prevention. And the economics. Oh my gosh. But we haven’t done it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Now we can do it. Primary prevention. We’d do really well. Somebody’s had heart attack.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Oh, we’re going to get all over it. Why did they have a heart attack in the first place?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Well, the thing that makes so much sense in what you’re saying is that we understand we have an understanding both economically and medically that prevention is a good thing. And extending the concept of prevention to these age-related conditions, I think, makes all the sense in the world.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;You know, Eric, maybe on that optimistic note, it’s time to wrap up this conversation. Really appreciate you coming. Let me just brag in closing that I’m now the proud owner of an autographed copy of your latest book, and, really, thank you for that.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TOPOL: &lt;/strong&gt;Oh, thank you. I could spend the rest of the day talking to you. I’ve really enjoyed it. Thanks.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[TRANSITION MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; For me, the biggest takeaway from our conversation was Eric’s supremely optimistic predictions about what AI will allow us to do in much less than 10 years.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;You know, for me personally, I started off several years ago with the typical techie naivete that if we could solve protein folding using machine learning, we would solve human biology. But as I’ve gotten smarter, I’ve realized that things are way, way more complicated than that, and so hearing Eric’s techno-optimism on this is really both heartening and so interesting.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Another thing that really caught my attention are Eric’s views on AI in medical diagnosis. That really stood out to me because within our labs here at Microsoft Research, we have been doing a lot of work on this, for example in creating foundation models for whole-slide digital pathology.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The bottom line, though, is that biomedical research and development is really changing and changing quickly. It’s something that we thought about and wrote briefly about in our book, but just hearing it from these three people gives me reason to believe that this is going to create tremendous benefits in the diagnosis and treatment of disease.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And in fact, I wonder now how regulators, such as the Food and Drug Administration here in the United States, will be able to keep up with what might become a really big increase in the number of animal and human studies that need to be approved. On this point, it’s clear that the FDA and other regulators will need to use AI to help process the likely rise in the pace of discovery and experimentation. And so stay tuned for more information about that.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[THEME MUSIC] &lt;/p&gt;



&lt;p&gt;I’d like to thank Daphne, Noubar, and Eric again for their time and insights. And to our listeners, thank you for joining us. There are several episodes left in the series, including discussions on medical students’ experiences with AI and AI’s influence on the operation of health systems and public health departments. We hope you’ll continue to tune in.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Until next time.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC FADES] &lt;/p&gt;

				&lt;/span&gt;
			&lt;/div&gt;
			&lt;button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button"&gt;
				Show more			&lt;/button&gt;
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;




&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/podcast/how-ai-will-accelerate-biomedical-research-and-discovery/</guid><pubDate>Thu, 10 Jul 2025 16:00:00 +0000</pubDate></item><item><title>[NEW] Musk’s Grok 4 launches one day after chatbot generated Hitler praise on X (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/07/musks-grok-4-launches-one-day-after-chatbot-generated-hitler-praise-on-x/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        xAI claims new multi-agent model hits top benchmarks as Nazi controversy lingers.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="The Grok chatbot logo on a smartphone" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/grok_header_1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="The Grok chatbot logo on a smartphone" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/grok_header_1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Bloomberg via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Wednesday night, Elon Musk unveiled xAI's latest flagship models Grok 4 and Grok 4 Heavy via livestream, just one day after the company's Grok chatbot began generating outputs that featured blatantly antisemitic tropes in responses to users on X.&lt;/p&gt;
&lt;p&gt;Among the two models, xAI calls Grok 4 Heavy its "multi-agent version." According to Musk, Grok 4 Heavy "spawns multiple agents in parallel" that "compare notes and yield an answer," simulating a study group approach. The company describes this as test-time compute scaling (similar to previous simulated reasoning models), claiming to increase computational resources by roughly an order of magnitude during runtime (called "inference").&lt;/p&gt;
&lt;p&gt;During the livestream, Musk claimed the new models achieved frontier-level performance on several benchmarks. On Humanity's Last Exam, a deliberately challenging test with 2,500 expert-curated questions across multiple subjects, Grok 4 reportedly scored 25.4 percent without external tools, which the company says outperformed OpenAI's o3 at 21 percent and Google's Gemini 2.5 Pro at 21.6 percent. With tools enabled, xAI claims Grok 4 Heavy reached 44.4 percent. However, it remains to be seen if these AI benchmarks actually measure properties that translate to usefulness for users.&lt;/p&gt;
&lt;p&gt;The release timing proved particularly noteworthy given the events of the preceding 48 hours on Musk's X social media platform, which included multiple instances of the chatbot labeling itself as "MechaHitler." The antisemitic posts emerged after an update over the weekend that instructed the chatbot to "not shy away from making claims which are politically incorrect, as long as they are well substantiated." xAI reportedly removed the modified directive Tuesday.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In response to the episode, Poland announced plans to report xAI to the European Commission, and Turkey blocked some access to Grok following the incident. On Wednesday, Musk wrote in a post on X that "Grok was too compliant to user prompts. Too eager to please and be manipulated, essentially. That is being addressed."&lt;/p&gt;
&lt;p&gt;Adding to the week's turmoil, X CEO Linda Yaccarino announced Wednesday morning she was stepping down, writing on X, "Now, the best is yet to come as X enters a new chapter with @xai." Her departure follows Musk's March&amp;nbsp;announcement that his artificial intelligence company, xAI, acquired X in an all-stock transaction that valued X at $33 billion and gave xAI a valuation of $80 billion.&lt;/p&gt;
&lt;h2&gt;The Grok technical conundrum&lt;/h2&gt;
&lt;p&gt;Since the launch of Grok 1 in 2023, the Grok series of large language models has been something of a conundrum for some members of the AI technical community. Judging by posts on X, some prominent researchers like Andrej Karpathy have historically taken the underlying models seriously as examples of technical achievement in AI development.&lt;/p&gt;
&lt;p&gt;But that achievement has been inextricably linked to Musk, who has seemingly guided the application of his AI models (in the form of "Grok" chatbot assistants on X and in the Grok app) through a series of controversies over the past few years that include potentially using OpenAI models to generate training data, producing uncensored image outputs, making up fake news based on X user jokes, and allowing explicit abusive voice chats in its app, among others.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Musk has also apparently used the Grok chatbots as an automated extension of his trolling habits, showing examples of Grok 3 producing "based" opinions that criticized the media in February. In May, Grok on X began repeatedly generating outputs about white genocide in South Africa, and most recently, we've seen the Grok Nazi output debacle. It's admittedly difficult to take Grok seriously as a technical product when it's linked to so many examples of unserious and capricious applications of the technology.&lt;/p&gt;
&lt;p&gt;Still, the technical achievements xAI claims for various Grok 4 models seem to stand out. The Arc Prize organization reported that Grok 4 Thinking (with simulated reasoning enabled) achieved a score of 15.9 percent on its ARC-AGI-2 test, which the organization says nearly doubles the previous commercial best and tops the current Kaggle competition leader.&lt;/p&gt;
&lt;p&gt;"With respect to academic questions, Grok 4 is better than PhD level in every subject, no exceptions," Musk claimed during the livestream. We've previously covered nebulous claims about "PhD-level" AI, finding them to be generally specious marketing talk.&lt;/p&gt;
&lt;h2&gt;Premium pricing amid controversy&lt;/h2&gt;
&lt;p&gt;During Wednesday's livestream, xAI also announced plans for an AI coding model in August, a multi-modal agent in September, and a video generation model in October. The company also plans to make Grok 4 available in Tesla vehicles next week, further expanding Musk's AI assistant across his various companies.&lt;/p&gt;
&lt;p&gt;Despite the recent turmoil, xAI has moved forward with an aggressive pricing strategy for "premium" versions of Grok. Alongside Grok 4 and Grok 4 Heavy, xAI launched "SuperGrok Heavy," a $300-per-month subscription that makes it the most expensive AI service among major providers. Subscribers will get early access to Grok 4 Heavy and upcoming features.&lt;/p&gt;
&lt;p&gt;Whether users will pay xAI's premium pricing remains to be seen, particularly given the AI assistant's tendency to periodically generate politically motivated outputs. These incidents—stemming from deliberate choices about training and system prompts—represenmt fundamental management and implementation issues that, so far, no fancy-looking test-taking benchmarks have been able to capture.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        xAI claims new multi-agent model hits top benchmarks as Nazi controversy lingers.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="The Grok chatbot logo on a smartphone" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/grok_header_1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="The Grok chatbot logo on a smartphone" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/grok_header_1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Bloomberg via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Wednesday night, Elon Musk unveiled xAI's latest flagship models Grok 4 and Grok 4 Heavy via livestream, just one day after the company's Grok chatbot began generating outputs that featured blatantly antisemitic tropes in responses to users on X.&lt;/p&gt;
&lt;p&gt;Among the two models, xAI calls Grok 4 Heavy its "multi-agent version." According to Musk, Grok 4 Heavy "spawns multiple agents in parallel" that "compare notes and yield an answer," simulating a study group approach. The company describes this as test-time compute scaling (similar to previous simulated reasoning models), claiming to increase computational resources by roughly an order of magnitude during runtime (called "inference").&lt;/p&gt;
&lt;p&gt;During the livestream, Musk claimed the new models achieved frontier-level performance on several benchmarks. On Humanity's Last Exam, a deliberately challenging test with 2,500 expert-curated questions across multiple subjects, Grok 4 reportedly scored 25.4 percent without external tools, which the company says outperformed OpenAI's o3 at 21 percent and Google's Gemini 2.5 Pro at 21.6 percent. With tools enabled, xAI claims Grok 4 Heavy reached 44.4 percent. However, it remains to be seen if these AI benchmarks actually measure properties that translate to usefulness for users.&lt;/p&gt;
&lt;p&gt;The release timing proved particularly noteworthy given the events of the preceding 48 hours on Musk's X social media platform, which included multiple instances of the chatbot labeling itself as "MechaHitler." The antisemitic posts emerged after an update over the weekend that instructed the chatbot to "not shy away from making claims which are politically incorrect, as long as they are well substantiated." xAI reportedly removed the modified directive Tuesday.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In response to the episode, Poland announced plans to report xAI to the European Commission, and Turkey blocked some access to Grok following the incident. On Wednesday, Musk wrote in a post on X that "Grok was too compliant to user prompts. Too eager to please and be manipulated, essentially. That is being addressed."&lt;/p&gt;
&lt;p&gt;Adding to the week's turmoil, X CEO Linda Yaccarino announced Wednesday morning she was stepping down, writing on X, "Now, the best is yet to come as X enters a new chapter with @xai." Her departure follows Musk's March&amp;nbsp;announcement that his artificial intelligence company, xAI, acquired X in an all-stock transaction that valued X at $33 billion and gave xAI a valuation of $80 billion.&lt;/p&gt;
&lt;h2&gt;The Grok technical conundrum&lt;/h2&gt;
&lt;p&gt;Since the launch of Grok 1 in 2023, the Grok series of large language models has been something of a conundrum for some members of the AI technical community. Judging by posts on X, some prominent researchers like Andrej Karpathy have historically taken the underlying models seriously as examples of technical achievement in AI development.&lt;/p&gt;
&lt;p&gt;But that achievement has been inextricably linked to Musk, who has seemingly guided the application of his AI models (in the form of "Grok" chatbot assistants on X and in the Grok app) through a series of controversies over the past few years that include potentially using OpenAI models to generate training data, producing uncensored image outputs, making up fake news based on X user jokes, and allowing explicit abusive voice chats in its app, among others.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Musk has also apparently used the Grok chatbots as an automated extension of his trolling habits, showing examples of Grok 3 producing "based" opinions that criticized the media in February. In May, Grok on X began repeatedly generating outputs about white genocide in South Africa, and most recently, we've seen the Grok Nazi output debacle. It's admittedly difficult to take Grok seriously as a technical product when it's linked to so many examples of unserious and capricious applications of the technology.&lt;/p&gt;
&lt;p&gt;Still, the technical achievements xAI claims for various Grok 4 models seem to stand out. The Arc Prize organization reported that Grok 4 Thinking (with simulated reasoning enabled) achieved a score of 15.9 percent on its ARC-AGI-2 test, which the organization says nearly doubles the previous commercial best and tops the current Kaggle competition leader.&lt;/p&gt;
&lt;p&gt;"With respect to academic questions, Grok 4 is better than PhD level in every subject, no exceptions," Musk claimed during the livestream. We've previously covered nebulous claims about "PhD-level" AI, finding them to be generally specious marketing talk.&lt;/p&gt;
&lt;h2&gt;Premium pricing amid controversy&lt;/h2&gt;
&lt;p&gt;During Wednesday's livestream, xAI also announced plans for an AI coding model in August, a multi-modal agent in September, and a video generation model in October. The company also plans to make Grok 4 available in Tesla vehicles next week, further expanding Musk's AI assistant across his various companies.&lt;/p&gt;
&lt;p&gt;Despite the recent turmoil, xAI has moved forward with an aggressive pricing strategy for "premium" versions of Grok. Alongside Grok 4 and Grok 4 Heavy, xAI launched "SuperGrok Heavy," a $300-per-month subscription that makes it the most expensive AI service among major providers. Subscribers will get early access to Grok 4 Heavy and upcoming features.&lt;/p&gt;
&lt;p&gt;Whether users will pay xAI's premium pricing remains to be seen, particularly given the AI assistant's tendency to periodically generate politically motivated outputs. These incidents—stemming from deliberate choices about training and system prompts—represenmt fundamental management and implementation issues that, so far, no fancy-looking test-taking benchmarks have been able to capture.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/07/musks-grok-4-launches-one-day-after-chatbot-generated-hitler-praise-on-x/</guid><pubDate>Thu, 10 Jul 2025 16:05:35 +0000</pubDate></item><item><title>[NEW] Everything tech giants will hate about the EU’s new AI rules (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/07/everything-tech-giants-will-hate-about-the-eus-new-ai-rules/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        EU rules ask tech giants to publicly track how and when AI models go off the rails.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="379" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-1793187673-640x379.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-1793187673-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Moor Studio | DigitalVision Vectors

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The European Union is moving to force AI companies to be more transparent than ever, publishing a code of practice Thursday that will help tech giants prepare to comply with the EU's landmark AI Act.&lt;/p&gt;
&lt;p&gt;These rules—which have not yet been finalized and focus on copyright protections, transparency, and public safety—will initially be voluntary when they take effect for the biggest makers of "general purpose AI" on August 2.&lt;/p&gt;
&lt;p&gt;But the EU will begin enforcing the AI Act in August 2026, and the Commission has noted that any companies agreeing to the rules could benefit from a "reduced administrative burden and increased legal certainty," The New York Times reported. Rejecting the voluntary rules could force companies to prove their compliance in ways that could be more costly or time-consuming, the Commission suggested.&lt;/p&gt;
&lt;p&gt;The AI industry participated in drafting the AI Act, but some companies have recently urged the EU to delay enforcement of the law, warning that the EU may risk hampering AI innovation by placing heavy restrictions on companies.&lt;/p&gt;
&lt;p&gt;Among the most controversial commitments that the EU is asking companies like Google, Meta, and OpenAI to voluntarily make is a promise to never pirate materials for AI training.&lt;/p&gt;
&lt;p&gt;Many AI companies have controversially used pirated book datasets to train AI, including Meta, which suggested that individual books are individually worthless to train AI after being called out for torrenting unauthorized book copies. But the EU doesn't agree, recommending that tech companies designate staffers and create internal mechanisms to field complaints "within a reasonable timeframe" from rightsholders, who must be allowed to opt their creative works out of AI training data sets.&lt;/p&gt;
&lt;p&gt;The EU rules pressure AI makers to take other steps the industry has mostly resisted. Most notably, AI companies will need to share detailed information about their training data, including providing a rationale for key model design choices and disclosing precisely where their training data came from. That could make it clearer how much of each company's models depend on publicly available data versus user data, third-party data, synthetic data, or some emerging new source of data.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The code also details expectations for AI companies to respect paywalls, as well as robots.txt instructions restricting crawling, which could help confront a growing problem of AI crawlers hammering websites. It "encourages" online search giants to embrace a solution that Cloudflare is currently pushing: allowing content creators to protect copyrights by restricting AI crawling without impacting search indexing.&lt;/p&gt;
&lt;p&gt;Additionally, companies are asked to disclose total energy consumption for both training and inference, allowing the EU to detect environmental concerns while companies race forward with AI innovation.&lt;/p&gt;
&lt;p&gt;More substantially, the code's safety guidance provides for additional monitoring for other harms. It makes recommendations to detect and avoid "serious incidents" with new AI models, which could include cybersecurity breaches, disruptions of critical infrastructure, "serious harm to a person’s health (mental and/or physical)," or "a death of a person." It stipulates timelines of between five and 10 days to report serious incidents with the EU's AI Office. And it requires companies to track all events, provide an "adequate level" of cybersecurity protection, prevent jailbreaking as best they can, and justify "any failures or circumventions of systemic risk mitigations."&lt;/p&gt;
&lt;p&gt;Ars reached out to tech companies for immediate reactions to the new rules. OpenAI, Meta, and Microsoft declined to comment. A Google spokesperson confirmed that the company is reviewing the code, which still must be approved by the European Commission and EU member states amid expected industry pushback.&lt;/p&gt;
&lt;p&gt;"Europeans should have access to first-rate, secure AI models when they become available, and an environment that promotes innovation and investment," Google's spokesperson said. "We look forward to reviewing the code and sharing our views alongside other model providers and many others."&lt;/p&gt;
&lt;p&gt;These rules are just one part of the AI Act, which will start taking effect in a staggered approach over the next year or more, the NYT reported. Breaching the AI Act could result in AI models being yanked off the market or fines "of as much as 7 percent of a company’s annual sales or 3 percent for the companies developing advanced AI models," Bloomberg noted.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        EU rules ask tech giants to publicly track how and when AI models go off the rails.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="379" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-1793187673-640x379.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-1793187673-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Moor Studio | DigitalVision Vectors

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The European Union is moving to force AI companies to be more transparent than ever, publishing a code of practice Thursday that will help tech giants prepare to comply with the EU's landmark AI Act.&lt;/p&gt;
&lt;p&gt;These rules—which have not yet been finalized and focus on copyright protections, transparency, and public safety—will initially be voluntary when they take effect for the biggest makers of "general purpose AI" on August 2.&lt;/p&gt;
&lt;p&gt;But the EU will begin enforcing the AI Act in August 2026, and the Commission has noted that any companies agreeing to the rules could benefit from a "reduced administrative burden and increased legal certainty," The New York Times reported. Rejecting the voluntary rules could force companies to prove their compliance in ways that could be more costly or time-consuming, the Commission suggested.&lt;/p&gt;
&lt;p&gt;The AI industry participated in drafting the AI Act, but some companies have recently urged the EU to delay enforcement of the law, warning that the EU may risk hampering AI innovation by placing heavy restrictions on companies.&lt;/p&gt;
&lt;p&gt;Among the most controversial commitments that the EU is asking companies like Google, Meta, and OpenAI to voluntarily make is a promise to never pirate materials for AI training.&lt;/p&gt;
&lt;p&gt;Many AI companies have controversially used pirated book datasets to train AI, including Meta, which suggested that individual books are individually worthless to train AI after being called out for torrenting unauthorized book copies. But the EU doesn't agree, recommending that tech companies designate staffers and create internal mechanisms to field complaints "within a reasonable timeframe" from rightsholders, who must be allowed to opt their creative works out of AI training data sets.&lt;/p&gt;
&lt;p&gt;The EU rules pressure AI makers to take other steps the industry has mostly resisted. Most notably, AI companies will need to share detailed information about their training data, including providing a rationale for key model design choices and disclosing precisely where their training data came from. That could make it clearer how much of each company's models depend on publicly available data versus user data, third-party data, synthetic data, or some emerging new source of data.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The code also details expectations for AI companies to respect paywalls, as well as robots.txt instructions restricting crawling, which could help confront a growing problem of AI crawlers hammering websites. It "encourages" online search giants to embrace a solution that Cloudflare is currently pushing: allowing content creators to protect copyrights by restricting AI crawling without impacting search indexing.&lt;/p&gt;
&lt;p&gt;Additionally, companies are asked to disclose total energy consumption for both training and inference, allowing the EU to detect environmental concerns while companies race forward with AI innovation.&lt;/p&gt;
&lt;p&gt;More substantially, the code's safety guidance provides for additional monitoring for other harms. It makes recommendations to detect and avoid "serious incidents" with new AI models, which could include cybersecurity breaches, disruptions of critical infrastructure, "serious harm to a person’s health (mental and/or physical)," or "a death of a person." It stipulates timelines of between five and 10 days to report serious incidents with the EU's AI Office. And it requires companies to track all events, provide an "adequate level" of cybersecurity protection, prevent jailbreaking as best they can, and justify "any failures or circumventions of systemic risk mitigations."&lt;/p&gt;
&lt;p&gt;Ars reached out to tech companies for immediate reactions to the new rules. OpenAI, Meta, and Microsoft declined to comment. A Google spokesperson confirmed that the company is reviewing the code, which still must be approved by the European Commission and EU member states amid expected industry pushback.&lt;/p&gt;
&lt;p&gt;"Europeans should have access to first-rate, secure AI models when they become available, and an environment that promotes innovation and investment," Google's spokesperson said. "We look forward to reviewing the code and sharing our views alongside other model providers and many others."&lt;/p&gt;
&lt;p&gt;These rules are just one part of the AI Act, which will start taking effect in a staggered approach over the next year or more, the NYT reported. Breaching the AI Act could result in AI models being yanked off the market or fines "of as much as 7 percent of a company’s annual sales or 3 percent for the companies developing advanced AI models," Bloomberg noted.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/07/everything-tech-giants-will-hate-about-the-eus-new-ai-rules/</guid><pubDate>Thu, 10 Jul 2025 16:29:59 +0000</pubDate></item><item><title>[NEW] Elon Musk introduced Grok 4 last night, calling it the ‘smartest AI in the world’ — what businesses need to know (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/elon-musk-introduced-grok-4-last-night-calling-it-the-smartest-ai-in-the-world-what-businesses-need-to-know/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;After days of controversy surrounding a flurry of antisemitic responses made recently by his Grok AI-powered chatbot on his social network X (formerly Twitter), a seemingly unrepentant and unbothered Elon Musk launched the latest version of his AI model family, Grok 4, during an event livestreamed on X last night, calling it the “the smartest AI in the world.”&lt;/p&gt;



&lt;p&gt;As Musk posted on X: “Grok 4 is the first time, in my experience, that an AI has been able to solve difficult, real-world engineering questions where the answers cannot be found anywhere on the Internet or in books. And it will get much better.”&lt;/p&gt;



&lt;p&gt;The new release actually includes two distinct models: &lt;strong&gt;Grok 4&lt;/strong&gt;, a single-agent reasoning model, and &lt;strong&gt;Grok 4 Heavy&lt;/strong&gt;, a multi-agent system designed to solve complex problems through internal collaboration and synthesis. &lt;/p&gt;



&lt;p&gt;Both models are optimized for reasoning tasks and come with native tool integration, enabling capabilities such as web search, code execution, and multimodal analysis. &lt;/p&gt;



&lt;p&gt;Musk and his team at xAI showcased benchmarks that suggest Grok 4 outperforms all current competitors across a range of academic and coding evaluations, even compared to formerly leading AI reasoning model rivals OpenAI o3 and Google Gemini. &lt;/p&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3014031" height="471" src="https://venturebeat.com/wp-content/uploads/2025/07/Screenshot-2025-07-10-at-11.02.05%E2%80%AFAM.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;However, xAI has not yet released a &lt;strong&gt;model card&lt;/strong&gt; nor any &lt;strong&gt;official release notes documentation&lt;/strong&gt; for Grok 4 to the public, making it challenging to independently assess performance and the claims made during the stream. We’ll update if/when these become available.&lt;/p&gt;



&lt;p&gt;Nor did Musk and his xAI team members participating in the livestream address the glaring controversy facing Grok over the past week, including many incidents of Grok making antisemitic remarks or referring to itself as “MechaHitler“, and suggesting that people with Jewish surnames should be handled decisively by Adolf Hitler — a seemingly overt reference to the Holocaust and genocide of 6 million Jews during World War 2. &lt;/p&gt;



&lt;p&gt;The closest Musk came was when he stated: “The thing that I think is most important for AI safety—at least my biological neural net tells me the most important thing—is to be maximally truth-seeking,” and “We need to make sure that the AI is a good AI. Good Grok” as well as “It’s important to instill the values you want in a child that would grow up to be incredibly powerful.” &lt;/p&gt;



&lt;p&gt;However, Musk did not apologize nor did he accept responsibility for Grok’s antisemitic, sexually offensive, and conspiratorial remarks. Here’s a cop of the full stream below:&lt;/p&gt;



&lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://venturebeat.com/wp-content/uploads/2025/07/grok-4-livestream-rip.mp4"&gt;&lt;/video&gt;&lt;/figure&gt;



&lt;p&gt;Throughout the livestream, the team emphasized Grok 4’s ability to reason from first principles, correct its own errors, and potentially invent new technologies or uncover novel scientific insights. &lt;/p&gt;



&lt;p&gt;The presentation also included demonstrations of Grok 4 Heavy applying multi-agent collaboration to tackle research-level problems across disciplines.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-availability-and-pricing"&gt;Availability and pricing&lt;/h2&gt;



&lt;p&gt;Grok 4 is available now through several channels, depending on user type and subscription level:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;API Access (for developers and enterprises)&lt;/strong&gt;:&lt;br /&gt;Grok 4 and Grok 4 Heavy are live via the &lt;strong&gt;xAI API&lt;/strong&gt;. Pricing is structured as follows:
&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;$3 per 1 million input tokens&lt;/strong&gt;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;$15 per 1 million output tokens&lt;/strong&gt;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;$0.75 per 1 million cached input tokens&lt;/strong&gt;&lt;/li&gt;



&lt;li&gt;Prices &lt;strong&gt;double after 128,000 tokens&lt;/strong&gt; in a single context window&lt;br /&gt;The API supports text and image inputs, function calling, structured outputs, and offers a 256,000-token context window.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Consumer Access (via Grok chatbot and apps)&lt;/strong&gt;:&lt;br /&gt;Individual users can access Grok 4 through the &lt;strong&gt;Grok chatbot on X&lt;/strong&gt;, the &lt;strong&gt;Grok app&lt;/strong&gt; (iOS and Android), and &lt;strong&gt;X.com&lt;/strong&gt;, but only with one of the following subscriptions:
&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;PremiumPlus&lt;/strong&gt;: $16/month&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;SuperGrok&lt;/strong&gt;: $300/month&lt;/li&gt;



&lt;li&gt;A new &lt;strong&gt;“SuperGrok Heavy”&lt;/strong&gt; tier, also priced at &lt;strong&gt;$300/month&lt;/strong&gt;, provides access to &lt;strong&gt;both Grok 4 and Grok 4 Heavy&lt;/strong&gt;, the multi-agent variant.&lt;br /&gt;(Note: SuperGrok and PremiumPlus tiers may differ in availability and usage quotas across X and Grok platforms.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Launch Timing&lt;/strong&gt;:&lt;br /&gt;Grok 4 became available immediately following the &lt;strong&gt;July 9, 2025&lt;/strong&gt; livestream. Temporary access limits were in place during the demo, but full rollout to subscribers began shortly after.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Platform Expansion&lt;/strong&gt;:&lt;br /&gt;xAI has indicated plans to make Grok 4 available through &lt;strong&gt;Microsoft Azure AI Foundry&lt;/strong&gt;, where Grok 3 and Grok 3 Mini are currently listed.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;For subscription details, users are directed to x.ai/grok and X Premium support. Here’s how it compares to other leading AI models in terms of pricing per million tokens.&lt;/p&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Provider &amp;amp; model&lt;/th&gt;&lt;th&gt;Context window&lt;/th&gt;&lt;th&gt;&lt;strong&gt;Input&lt;/strong&gt; ($/Mtok)&lt;/th&gt;&lt;th&gt;&lt;strong&gt;Cached input&lt;/strong&gt;&lt;/th&gt;&lt;th&gt;&lt;strong&gt;Output&lt;/strong&gt; ($/Mtok)&lt;/th&gt;&lt;th&gt;Additional notes&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;xAI – Grok 4 / 4 Heavy&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;256 K (2× price &amp;gt;128 K)&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$3.00&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;$0.75&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$15.00&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Image input, function calling, structured JSON ﻿(apidog)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;OpenAI – o3&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;200 K&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$2.00&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;$0.50&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$8.00&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;50 % Batch-API discount available ﻿(OpenAI, OpenAI Help Center)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;GPT-4o&lt;/td&gt;&lt;td&gt;128 K&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$5.00&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;$2.50&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$20.00&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Vision, audio, tools ﻿(OpenAI)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Anthropic – Claude Sonnet 4&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;200 K&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$3.00&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;$0.30&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$15.00&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;50 % batch output discount ﻿(Anthropic)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Claude Opus 4&lt;/td&gt;&lt;td&gt;200 K&lt;/td&gt;&lt;td&gt;$15.00&lt;/td&gt;&lt;td&gt;$1.50&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$75.00&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;High-accuracy flagship ﻿(Anthropic)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Google – Gemini 2.5 Pro&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;200 K (2× price &amp;gt;200 K)&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$1.25&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;$0.31&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$10.00&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;75 % cache hit discount ﻿(Google AI for Developers, Google Cloud)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Gemini 2.5 Flash&lt;/td&gt;&lt;td&gt;200 K&lt;/td&gt;&lt;td&gt;$0.30&lt;/td&gt;&lt;td&gt;$0.075&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$2.50&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Fast, cheap preview tier ﻿(Google Cloud)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;DeepSeek – deepseek-reasoner&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;64 K&lt;/td&gt;&lt;td&gt;$0.55 (miss) / $0.14 (hit)&lt;/td&gt;&lt;td&gt;$0.14&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$2.19&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;50-75 % off-peak discount ﻿(DeepSeek API Docs)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;







&lt;p&gt;Unlike its predecessor Grok 3, released in February, which separated tool-augmented responses from general reasoning, Grok 4 was trained with tools from the start. &lt;/p&gt;



&lt;p&gt;The model integrates capabilities such as code execution, web search, and document parsing. It also introduces &lt;strong&gt;Grok 4 Heavy&lt;/strong&gt;, a multi-agent system where several internal models work in parallel to generate and validate answers.&lt;/p&gt;



&lt;p&gt;Grok 4 also includes a new &lt;strong&gt;voice mode&lt;/strong&gt; featuring expressive outputs with reduced latency, and it supports text and image input, structured outputs, and function calling.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-performance-highlights"&gt;Performance highlights&lt;/h2&gt;



&lt;p&gt;The independent AI model analysis and benchmarking group Artificial Analysis stated on X that xAI provided it with a version of Grok 4 (not Heavy) earlier than the public release for scoring.&lt;/p&gt;



&lt;p&gt;On technical benchmarks, Grok 4 leads the Artificial Analysis Intelligence Index with a score of 73, ahead of competitors such as OpenAI’s o3 (70) and Google’s Gemini 2.5 Pro (70). &lt;/p&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3014037" height="366" src="https://venturebeat.com/wp-content/uploads/2025/07/Gvd9nWIakAULlB9-1.jpg?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;It also recorded top scores in:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;GPQA Diamond:&lt;/strong&gt; 88%&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;ARC-AGI 2:&lt;/strong&gt; 15.9%, double the second-best score&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Humanities Last Exam:&lt;/strong&gt; 24% on the text-only version, and 44% with tools&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;MMLU-Pro and AIME 2024:&lt;/strong&gt; 87% and 94%, respectively&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Coding and Math evaluations:&lt;/strong&gt; Highest to date on LiveCodeBench, SciCode, AIME24, and MATH-500&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Despite its benchmark success, Grok 4’s &lt;strong&gt;output speed&lt;/strong&gt; stands at 75 tokens per second—slower than models like Gemini 2.5 Flash (353) or OpenAI’s o3 (187), but still faster than Anthropic’s Claude 4 Opus (66).&lt;/p&gt;



&lt;p&gt;The model features a &lt;strong&gt;256,000 token context window&lt;/strong&gt;, which sits above the 200k context limits of o3 and Claude 4 Sonnet but below the 1 million tokens offered by Gemini 2.5 Pro and GPT-4.1.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-real-world-use-cases"&gt;Real world use cases&lt;/h2&gt;



&lt;p&gt;xAI provided several demonstrations of Grok 4’s performance in applied scenarios:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;In a simulated business task called &lt;strong&gt;VendingBench&lt;/strong&gt;, Grok 4 significantly outperformed other models in long-horizon financial planning.&lt;/li&gt;



&lt;li&gt;At the &lt;strong&gt;Arc Institute&lt;/strong&gt;, researchers used Grok 4 to analyze CRISPR logs and uncover novel hypotheses.&lt;/li&gt;



&lt;li&gt;In &lt;strong&gt;radiology&lt;/strong&gt;, the model interpreted chest X-rays with higher accuracy than leading peers.&lt;/li&gt;



&lt;li&gt;In the &lt;strong&gt;financial sector&lt;/strong&gt;, its combination of real-time data access and reasoning made it suitable for forecasting and analysis.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;The model can also create &lt;strong&gt;3D video games&lt;/strong&gt; with minimal input by autonomously sourcing and integrating assets. Additionally, it demonstrated capabilities to simulate astrophysical events using grounded approximations from published research.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-reception-and-discussion"&gt;Reception and discussion&lt;/h2&gt;



&lt;p&gt;Industry response to the Grok 4 launch has been divided, blending enthusiasm for its performance with criticism of the event’s delivery and broader trust issues.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;David Shapiro&lt;/strong&gt;, an AI power user and writer, noted: “Grok 4 now takes its place as ‘smart enough to actually help with frontier research’… but has merely caught up with OpenAI.”&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Ethan Mollick&lt;/strong&gt;, a professor at Wharton, remarked on X: “So Grok 3 has had three separate incidents where apparently unvetted changes to the deployed system caused a large-scale ethical issue and an emergency rollback. I don’t think you can do a Grok 4 launch that doesn’t at least address this honestly, if user trust matters,” later adding, “Grok 3 was a very good model, and Grok 4 might be amazing but having a very good model is not enough – there are a lot of really good models out there. You actually want to trust the model you are building on.”&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Ben Hyak&lt;/strong&gt;, co-founder and CTO of AI product observability startup Raindrop AI (himself a former Musk employee) criticized the livestream itself: “This xAI livestream is one of the worst things I’ve ever watched in my life. Love y’all, but it’s bad.”&lt;/p&gt;



&lt;p&gt;Despite the criticisms, benchmarking firm Artificial Analysis noted: “Grok 4 is now the leading AI model.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-ongoing-trust-issues"&gt;Ongoing trust issues&lt;/h2&gt;



&lt;p&gt;The launch of Grok 4 comes amid renewed criticism over Grok’s prior behavior in consumer deployments, particularly as a chatbot integrated into Musk’s social network, X.&lt;/p&gt;



&lt;p&gt;Over the July 4 holiday and in subsequent days, Grok generated antisemitic and conspiratorial responses that reignited scrutiny over its system design and governance practices.&lt;/p&gt;



&lt;p&gt;As reported by my VentureBeat colleague Michael F. Nuñez, Grok responded to questions about Jewish influence in Hollywood by asserting that Jewish executives “dominate leadership” at major studios and influence content through “progressive ideologies,” and went on to rant about people of Jewish surnames as fitting a “pattern” of engaging in “extreme leftist activism,” and suggesting Hitler knew “how to handle it decisively, every damn time,” an apparent reference to the Holocaust. &lt;/p&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3014035" height="600" src="https://venturebeat.com/wp-content/uploads/2025/07/GvXHBiRXAAAlTtl-2.jpg?w=356" width="356" /&gt;&lt;/figure&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3014036" height="600" src="https://venturebeat.com/wp-content/uploads/2025/07/GvXvvjNWkAA0IR9.jpg?w=772" width="772" /&gt;&lt;/figure&gt;



&lt;p&gt;The conspiratorial and antisemitic posting was so prolific, the Anti-Defamation League (ADL), a preeminent U.S.-based non-profit combating anti-semitism and hatred, posted on July 8: “What we are seeing from Grok LLM right now is irresponsible, dangerous and antisemitic, plain and simple. This supercharging of extremist rhetoric will only amplify and encourage the antisemitism that is already surging on X and many other platforms.”&lt;/p&gt;



&lt;p&gt;This incident follows a history of problematic Grok outputs, including a May 2025 case where the Grok bot integrated into X randomly inserted references to a completely nonsensical and non-real “white genocide” in South Africa into unrelated queries, and an earlier case wherein its system prompt was discovered to direct the Grok chatbot on X to avoid referencing any sources that declared Musk and his former political funding beneficiary U.S. President Donald J. Trump as spreaders of misinformation. In both of these two cases, xAI blamed the behaviors on nameless employees and said they were being addressed. &lt;/p&gt;



&lt;p&gt;Already, today, users of Grok 4 on the consumer app have observed it to once again be outputting anti-Zionist and anti-Semitic remarks: &lt;/p&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3014038" height="600" src="https://venturebeat.com/wp-content/uploads/2025/07/Gve4PNVXAAAA9ny.jpg?w=569" width="569" /&gt;&lt;/figure&gt;



&lt;p&gt;As I previously noted, Musk has openly stated on several occasions he wanted to alter Grok to better reflect his personal beliefs and distrust in mainstream media and accredited sources. This makes it a poor source in enterprise contexts where such views could adversely impact users and the businesses building atop the Grok family of models. &lt;/p&gt;



&lt;p&gt;My prior recommendation remains: For those in the enterprise trying to ensure their business’s AI products work properly and accurately… Grok is sadly best avoided. Thankfully, there are numerous other alternatives to choose from.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;After days of controversy surrounding a flurry of antisemitic responses made recently by his Grok AI-powered chatbot on his social network X (formerly Twitter), a seemingly unrepentant and unbothered Elon Musk launched the latest version of his AI model family, Grok 4, during an event livestreamed on X last night, calling it the “the smartest AI in the world.”&lt;/p&gt;



&lt;p&gt;As Musk posted on X: “Grok 4 is the first time, in my experience, that an AI has been able to solve difficult, real-world engineering questions where the answers cannot be found anywhere on the Internet or in books. And it will get much better.”&lt;/p&gt;



&lt;p&gt;The new release actually includes two distinct models: &lt;strong&gt;Grok 4&lt;/strong&gt;, a single-agent reasoning model, and &lt;strong&gt;Grok 4 Heavy&lt;/strong&gt;, a multi-agent system designed to solve complex problems through internal collaboration and synthesis. &lt;/p&gt;



&lt;p&gt;Both models are optimized for reasoning tasks and come with native tool integration, enabling capabilities such as web search, code execution, and multimodal analysis. &lt;/p&gt;



&lt;p&gt;Musk and his team at xAI showcased benchmarks that suggest Grok 4 outperforms all current competitors across a range of academic and coding evaluations, even compared to formerly leading AI reasoning model rivals OpenAI o3 and Google Gemini. &lt;/p&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3014031" height="471" src="https://venturebeat.com/wp-content/uploads/2025/07/Screenshot-2025-07-10-at-11.02.05%E2%80%AFAM.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;However, xAI has not yet released a &lt;strong&gt;model card&lt;/strong&gt; nor any &lt;strong&gt;official release notes documentation&lt;/strong&gt; for Grok 4 to the public, making it challenging to independently assess performance and the claims made during the stream. We’ll update if/when these become available.&lt;/p&gt;



&lt;p&gt;Nor did Musk and his xAI team members participating in the livestream address the glaring controversy facing Grok over the past week, including many incidents of Grok making antisemitic remarks or referring to itself as “MechaHitler“, and suggesting that people with Jewish surnames should be handled decisively by Adolf Hitler — a seemingly overt reference to the Holocaust and genocide of 6 million Jews during World War 2. &lt;/p&gt;



&lt;p&gt;The closest Musk came was when he stated: “The thing that I think is most important for AI safety—at least my biological neural net tells me the most important thing—is to be maximally truth-seeking,” and “We need to make sure that the AI is a good AI. Good Grok” as well as “It’s important to instill the values you want in a child that would grow up to be incredibly powerful.” &lt;/p&gt;



&lt;p&gt;However, Musk did not apologize nor did he accept responsibility for Grok’s antisemitic, sexually offensive, and conspiratorial remarks. Here’s a cop of the full stream below:&lt;/p&gt;



&lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://venturebeat.com/wp-content/uploads/2025/07/grok-4-livestream-rip.mp4"&gt;&lt;/video&gt;&lt;/figure&gt;



&lt;p&gt;Throughout the livestream, the team emphasized Grok 4’s ability to reason from first principles, correct its own errors, and potentially invent new technologies or uncover novel scientific insights. &lt;/p&gt;



&lt;p&gt;The presentation also included demonstrations of Grok 4 Heavy applying multi-agent collaboration to tackle research-level problems across disciplines.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-availability-and-pricing"&gt;Availability and pricing&lt;/h2&gt;



&lt;p&gt;Grok 4 is available now through several channels, depending on user type and subscription level:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;API Access (for developers and enterprises)&lt;/strong&gt;:&lt;br /&gt;Grok 4 and Grok 4 Heavy are live via the &lt;strong&gt;xAI API&lt;/strong&gt;. Pricing is structured as follows:
&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;$3 per 1 million input tokens&lt;/strong&gt;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;$15 per 1 million output tokens&lt;/strong&gt;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;$0.75 per 1 million cached input tokens&lt;/strong&gt;&lt;/li&gt;



&lt;li&gt;Prices &lt;strong&gt;double after 128,000 tokens&lt;/strong&gt; in a single context window&lt;br /&gt;The API supports text and image inputs, function calling, structured outputs, and offers a 256,000-token context window.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Consumer Access (via Grok chatbot and apps)&lt;/strong&gt;:&lt;br /&gt;Individual users can access Grok 4 through the &lt;strong&gt;Grok chatbot on X&lt;/strong&gt;, the &lt;strong&gt;Grok app&lt;/strong&gt; (iOS and Android), and &lt;strong&gt;X.com&lt;/strong&gt;, but only with one of the following subscriptions:
&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;PremiumPlus&lt;/strong&gt;: $16/month&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;SuperGrok&lt;/strong&gt;: $300/month&lt;/li&gt;



&lt;li&gt;A new &lt;strong&gt;“SuperGrok Heavy”&lt;/strong&gt; tier, also priced at &lt;strong&gt;$300/month&lt;/strong&gt;, provides access to &lt;strong&gt;both Grok 4 and Grok 4 Heavy&lt;/strong&gt;, the multi-agent variant.&lt;br /&gt;(Note: SuperGrok and PremiumPlus tiers may differ in availability and usage quotas across X and Grok platforms.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Launch Timing&lt;/strong&gt;:&lt;br /&gt;Grok 4 became available immediately following the &lt;strong&gt;July 9, 2025&lt;/strong&gt; livestream. Temporary access limits were in place during the demo, but full rollout to subscribers began shortly after.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Platform Expansion&lt;/strong&gt;:&lt;br /&gt;xAI has indicated plans to make Grok 4 available through &lt;strong&gt;Microsoft Azure AI Foundry&lt;/strong&gt;, where Grok 3 and Grok 3 Mini are currently listed.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;For subscription details, users are directed to x.ai/grok and X Premium support. Here’s how it compares to other leading AI models in terms of pricing per million tokens.&lt;/p&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Provider &amp;amp; model&lt;/th&gt;&lt;th&gt;Context window&lt;/th&gt;&lt;th&gt;&lt;strong&gt;Input&lt;/strong&gt; ($/Mtok)&lt;/th&gt;&lt;th&gt;&lt;strong&gt;Cached input&lt;/strong&gt;&lt;/th&gt;&lt;th&gt;&lt;strong&gt;Output&lt;/strong&gt; ($/Mtok)&lt;/th&gt;&lt;th&gt;Additional notes&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;xAI – Grok 4 / 4 Heavy&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;256 K (2× price &amp;gt;128 K)&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$3.00&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;$0.75&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$15.00&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Image input, function calling, structured JSON ﻿(apidog)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;OpenAI – o3&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;200 K&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$2.00&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;$0.50&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$8.00&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;50 % Batch-API discount available ﻿(OpenAI, OpenAI Help Center)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;GPT-4o&lt;/td&gt;&lt;td&gt;128 K&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$5.00&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;$2.50&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$20.00&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Vision, audio, tools ﻿(OpenAI)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Anthropic – Claude Sonnet 4&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;200 K&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$3.00&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;$0.30&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$15.00&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;50 % batch output discount ﻿(Anthropic)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Claude Opus 4&lt;/td&gt;&lt;td&gt;200 K&lt;/td&gt;&lt;td&gt;$15.00&lt;/td&gt;&lt;td&gt;$1.50&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$75.00&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;High-accuracy flagship ﻿(Anthropic)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Google – Gemini 2.5 Pro&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;200 K (2× price &amp;gt;200 K)&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$1.25&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;$0.31&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$10.00&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;75 % cache hit discount ﻿(Google AI for Developers, Google Cloud)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Gemini 2.5 Flash&lt;/td&gt;&lt;td&gt;200 K&lt;/td&gt;&lt;td&gt;$0.30&lt;/td&gt;&lt;td&gt;$0.075&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$2.50&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Fast, cheap preview tier ﻿(Google Cloud)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;DeepSeek – deepseek-reasoner&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;64 K&lt;/td&gt;&lt;td&gt;$0.55 (miss) / $0.14 (hit)&lt;/td&gt;&lt;td&gt;$0.14&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$2.19&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;50-75 % off-peak discount ﻿(DeepSeek API Docs)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;







&lt;p&gt;Unlike its predecessor Grok 3, released in February, which separated tool-augmented responses from general reasoning, Grok 4 was trained with tools from the start. &lt;/p&gt;



&lt;p&gt;The model integrates capabilities such as code execution, web search, and document parsing. It also introduces &lt;strong&gt;Grok 4 Heavy&lt;/strong&gt;, a multi-agent system where several internal models work in parallel to generate and validate answers.&lt;/p&gt;



&lt;p&gt;Grok 4 also includes a new &lt;strong&gt;voice mode&lt;/strong&gt; featuring expressive outputs with reduced latency, and it supports text and image input, structured outputs, and function calling.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-performance-highlights"&gt;Performance highlights&lt;/h2&gt;



&lt;p&gt;The independent AI model analysis and benchmarking group Artificial Analysis stated on X that xAI provided it with a version of Grok 4 (not Heavy) earlier than the public release for scoring.&lt;/p&gt;



&lt;p&gt;On technical benchmarks, Grok 4 leads the Artificial Analysis Intelligence Index with a score of 73, ahead of competitors such as OpenAI’s o3 (70) and Google’s Gemini 2.5 Pro (70). &lt;/p&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3014037" height="366" src="https://venturebeat.com/wp-content/uploads/2025/07/Gvd9nWIakAULlB9-1.jpg?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;It also recorded top scores in:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;GPQA Diamond:&lt;/strong&gt; 88%&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;ARC-AGI 2:&lt;/strong&gt; 15.9%, double the second-best score&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Humanities Last Exam:&lt;/strong&gt; 24% on the text-only version, and 44% with tools&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;MMLU-Pro and AIME 2024:&lt;/strong&gt; 87% and 94%, respectively&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Coding and Math evaluations:&lt;/strong&gt; Highest to date on LiveCodeBench, SciCode, AIME24, and MATH-500&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Despite its benchmark success, Grok 4’s &lt;strong&gt;output speed&lt;/strong&gt; stands at 75 tokens per second—slower than models like Gemini 2.5 Flash (353) or OpenAI’s o3 (187), but still faster than Anthropic’s Claude 4 Opus (66).&lt;/p&gt;



&lt;p&gt;The model features a &lt;strong&gt;256,000 token context window&lt;/strong&gt;, which sits above the 200k context limits of o3 and Claude 4 Sonnet but below the 1 million tokens offered by Gemini 2.5 Pro and GPT-4.1.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-real-world-use-cases"&gt;Real world use cases&lt;/h2&gt;



&lt;p&gt;xAI provided several demonstrations of Grok 4’s performance in applied scenarios:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;In a simulated business task called &lt;strong&gt;VendingBench&lt;/strong&gt;, Grok 4 significantly outperformed other models in long-horizon financial planning.&lt;/li&gt;



&lt;li&gt;At the &lt;strong&gt;Arc Institute&lt;/strong&gt;, researchers used Grok 4 to analyze CRISPR logs and uncover novel hypotheses.&lt;/li&gt;



&lt;li&gt;In &lt;strong&gt;radiology&lt;/strong&gt;, the model interpreted chest X-rays with higher accuracy than leading peers.&lt;/li&gt;



&lt;li&gt;In the &lt;strong&gt;financial sector&lt;/strong&gt;, its combination of real-time data access and reasoning made it suitable for forecasting and analysis.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;The model can also create &lt;strong&gt;3D video games&lt;/strong&gt; with minimal input by autonomously sourcing and integrating assets. Additionally, it demonstrated capabilities to simulate astrophysical events using grounded approximations from published research.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-reception-and-discussion"&gt;Reception and discussion&lt;/h2&gt;



&lt;p&gt;Industry response to the Grok 4 launch has been divided, blending enthusiasm for its performance with criticism of the event’s delivery and broader trust issues.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;David Shapiro&lt;/strong&gt;, an AI power user and writer, noted: “Grok 4 now takes its place as ‘smart enough to actually help with frontier research’… but has merely caught up with OpenAI.”&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Ethan Mollick&lt;/strong&gt;, a professor at Wharton, remarked on X: “So Grok 3 has had three separate incidents where apparently unvetted changes to the deployed system caused a large-scale ethical issue and an emergency rollback. I don’t think you can do a Grok 4 launch that doesn’t at least address this honestly, if user trust matters,” later adding, “Grok 3 was a very good model, and Grok 4 might be amazing but having a very good model is not enough – there are a lot of really good models out there. You actually want to trust the model you are building on.”&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Ben Hyak&lt;/strong&gt;, co-founder and CTO of AI product observability startup Raindrop AI (himself a former Musk employee) criticized the livestream itself: “This xAI livestream is one of the worst things I’ve ever watched in my life. Love y’all, but it’s bad.”&lt;/p&gt;



&lt;p&gt;Despite the criticisms, benchmarking firm Artificial Analysis noted: “Grok 4 is now the leading AI model.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-ongoing-trust-issues"&gt;Ongoing trust issues&lt;/h2&gt;



&lt;p&gt;The launch of Grok 4 comes amid renewed criticism over Grok’s prior behavior in consumer deployments, particularly as a chatbot integrated into Musk’s social network, X.&lt;/p&gt;



&lt;p&gt;Over the July 4 holiday and in subsequent days, Grok generated antisemitic and conspiratorial responses that reignited scrutiny over its system design and governance practices.&lt;/p&gt;



&lt;p&gt;As reported by my VentureBeat colleague Michael F. Nuñez, Grok responded to questions about Jewish influence in Hollywood by asserting that Jewish executives “dominate leadership” at major studios and influence content through “progressive ideologies,” and went on to rant about people of Jewish surnames as fitting a “pattern” of engaging in “extreme leftist activism,” and suggesting Hitler knew “how to handle it decisively, every damn time,” an apparent reference to the Holocaust. &lt;/p&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3014035" height="600" src="https://venturebeat.com/wp-content/uploads/2025/07/GvXHBiRXAAAlTtl-2.jpg?w=356" width="356" /&gt;&lt;/figure&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3014036" height="600" src="https://venturebeat.com/wp-content/uploads/2025/07/GvXvvjNWkAA0IR9.jpg?w=772" width="772" /&gt;&lt;/figure&gt;



&lt;p&gt;The conspiratorial and antisemitic posting was so prolific, the Anti-Defamation League (ADL), a preeminent U.S.-based non-profit combating anti-semitism and hatred, posted on July 8: “What we are seeing from Grok LLM right now is irresponsible, dangerous and antisemitic, plain and simple. This supercharging of extremist rhetoric will only amplify and encourage the antisemitism that is already surging on X and many other platforms.”&lt;/p&gt;



&lt;p&gt;This incident follows a history of problematic Grok outputs, including a May 2025 case where the Grok bot integrated into X randomly inserted references to a completely nonsensical and non-real “white genocide” in South Africa into unrelated queries, and an earlier case wherein its system prompt was discovered to direct the Grok chatbot on X to avoid referencing any sources that declared Musk and his former political funding beneficiary U.S. President Donald J. Trump as spreaders of misinformation. In both of these two cases, xAI blamed the behaviors on nameless employees and said they were being addressed. &lt;/p&gt;



&lt;p&gt;Already, today, users of Grok 4 on the consumer app have observed it to once again be outputting anti-Zionist and anti-Semitic remarks: &lt;/p&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3014038" height="600" src="https://venturebeat.com/wp-content/uploads/2025/07/Gve4PNVXAAAA9ny.jpg?w=569" width="569" /&gt;&lt;/figure&gt;



&lt;p&gt;As I previously noted, Musk has openly stated on several occasions he wanted to alter Grok to better reflect his personal beliefs and distrust in mainstream media and accredited sources. This makes it a poor source in enterprise contexts where such views could adversely impact users and the businesses building atop the Grok family of models. &lt;/p&gt;



&lt;p&gt;My prior recommendation remains: For those in the enterprise trying to ensure their business’s AI products work properly and accurately… Grok is sadly best avoided. Thankfully, there are numerous other alternatives to choose from.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/elon-musk-introduced-grok-4-last-night-calling-it-the-smartest-ai-in-the-world-what-businesses-need-to-know/</guid><pubDate>Thu, 10 Jul 2025 17:24:46 +0000</pubDate></item><item><title>[NEW] 5 days until TechCrunch All Stage — save up to $475 before prices rise (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/10/5-days-until-techcrunch-all-stage-save-up-to-475-before-prices-rise/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;In just five days, startup leaders from across the country will descend on Boston’s SoWa Power Station for &lt;strong&gt;TechCrunch All Stage 2025&lt;/strong&gt; — and your chance to lock in the lowest ticket prices will be gone.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether you’re preparing for your next raise, looking to fine-tune your go-to-market strategy, or building your founding team, TC All Stage is where real startup momentum happens. This is a full-day event packed with actionable sessions, tactical advice, and conversations that push your company forward.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Right now, Founder Passes are just $100 and Investor Passes are $200 — but those savings (up to $475 off full price) vanish soon. Don’t wait. Get your pass today before rates go up.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch All Stage 5 days left" class="wp-image-3019875" height="383" src="https://techcrunch.com/wp-content/uploads/2025/06/16x9_GeneralArticleImageHeader_TCAllStage_5DaysCountdown.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-fuel-your-next-stage-of-growth"&gt;Fuel your next stage of growth&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;TC All Stage isn’t about hype — it’s about execution. Every session is built to deliver practical insights from founders, operators, and VCs who know what it takes to scale in today’s climate.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On July 15 in Boston, you’ll get:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Breakout sessions&lt;/strong&gt; on fundraising frameworks, AI-driven product development, startup hiring, cap table strategy, and market readiness.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Interactive roundtables&lt;/strong&gt; designed for real talk, not surface-level fluff.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Live feedback&lt;/strong&gt; during “So You Think You Can Pitch” — the pitch competition that puts founder storytelling front and center.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Curated networking&lt;/strong&gt; through Braindate, helping you match with the right people for high-impact connections.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;After-hours &lt;/strong&gt;&lt;strong&gt;Side Events&lt;/strong&gt; that go beyond the badge for candid convos and community-building.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 class="wp-block-heading" id="h-hear-from-the-operators-and-investors-shaping-what-s-next"&gt;Hear from the operators and investors shaping what’s next&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Check the agenda and see who’s taking the stage. A few of the experts you’ll hear from include:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Jon McNeill&lt;/strong&gt;,&lt;strong&gt; &lt;/strong&gt;DVx Ventures — why the next wave of disruptors must be operator-led from day one.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Kristen Craft&lt;/strong&gt;,&lt;strong&gt; &lt;/strong&gt;Fidelity Private Shares — decoding the VC landscape of 2025.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;David H. Rosmarin&lt;/strong&gt;, Harvard Medical School — transforming anxiety into a founder superpower.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Jeff Chow&lt;/strong&gt;, Miro — building team intelligence through product-led innovation.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Kamila Khasanova&lt;/strong&gt;&lt;strong&gt;, &lt;/strong&gt;On Top Strategy — the storytelling strategies that drive successful raises.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="YC Group Partner Tom Blomfield on stage during TechCrunch Early Stage in Boston in April 2024" class="wp-image-2697522" height="383" src="https://techcrunch.com/wp-content/uploads/2024/04/53679979052_da1066fe91_o.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Halo Creative&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-just-5-days-to-go-get-your-pass-before-prices-increase"&gt;Just 5 days to go — get your pass before prices increase&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;TechCrunch All Stage happens July 15 in Boston&lt;/strong&gt;, and your chance to save ends soon. The event is just five days away, and prices rise at the door. Join the founders, VCs, and builders defining the future. Secure your pass now.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;July 15&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;In just five days, startup leaders from across the country will descend on Boston’s SoWa Power Station for &lt;strong&gt;TechCrunch All Stage 2025&lt;/strong&gt; — and your chance to lock in the lowest ticket prices will be gone.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether you’re preparing for your next raise, looking to fine-tune your go-to-market strategy, or building your founding team, TC All Stage is where real startup momentum happens. This is a full-day event packed with actionable sessions, tactical advice, and conversations that push your company forward.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Right now, Founder Passes are just $100 and Investor Passes are $200 — but those savings (up to $475 off full price) vanish soon. Don’t wait. Get your pass today before rates go up.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch All Stage 5 days left" class="wp-image-3019875" height="383" src="https://techcrunch.com/wp-content/uploads/2025/06/16x9_GeneralArticleImageHeader_TCAllStage_5DaysCountdown.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-fuel-your-next-stage-of-growth"&gt;Fuel your next stage of growth&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;TC All Stage isn’t about hype — it’s about execution. Every session is built to deliver practical insights from founders, operators, and VCs who know what it takes to scale in today’s climate.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On July 15 in Boston, you’ll get:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Breakout sessions&lt;/strong&gt; on fundraising frameworks, AI-driven product development, startup hiring, cap table strategy, and market readiness.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Interactive roundtables&lt;/strong&gt; designed for real talk, not surface-level fluff.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Live feedback&lt;/strong&gt; during “So You Think You Can Pitch” — the pitch competition that puts founder storytelling front and center.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Curated networking&lt;/strong&gt; through Braindate, helping you match with the right people for high-impact connections.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;After-hours &lt;/strong&gt;&lt;strong&gt;Side Events&lt;/strong&gt; that go beyond the badge for candid convos and community-building.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 class="wp-block-heading" id="h-hear-from-the-operators-and-investors-shaping-what-s-next"&gt;Hear from the operators and investors shaping what’s next&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Check the agenda and see who’s taking the stage. A few of the experts you’ll hear from include:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Jon McNeill&lt;/strong&gt;,&lt;strong&gt; &lt;/strong&gt;DVx Ventures — why the next wave of disruptors must be operator-led from day one.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Kristen Craft&lt;/strong&gt;,&lt;strong&gt; &lt;/strong&gt;Fidelity Private Shares — decoding the VC landscape of 2025.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;David H. Rosmarin&lt;/strong&gt;, Harvard Medical School — transforming anxiety into a founder superpower.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Jeff Chow&lt;/strong&gt;, Miro — building team intelligence through product-led innovation.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Kamila Khasanova&lt;/strong&gt;&lt;strong&gt;, &lt;/strong&gt;On Top Strategy — the storytelling strategies that drive successful raises.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="YC Group Partner Tom Blomfield on stage during TechCrunch Early Stage in Boston in April 2024" class="wp-image-2697522" height="383" src="https://techcrunch.com/wp-content/uploads/2024/04/53679979052_da1066fe91_o.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Halo Creative&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-just-5-days-to-go-get-your-pass-before-prices-increase"&gt;Just 5 days to go — get your pass before prices increase&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;TechCrunch All Stage happens July 15 in Boston&lt;/strong&gt;, and your chance to save ends soon. The event is just five days away, and prices rise at the door. Join the founders, VCs, and builders defining the future. Secure your pass now.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;July 15&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/10/5-days-until-techcrunch-all-stage-save-up-to-475-before-prices-rise/</guid><pubDate>Thu, 10 Jul 2025 18:00:00 +0000</pubDate></item></channel></rss>