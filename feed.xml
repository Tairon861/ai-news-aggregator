<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Aggregated AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 09 Jun 2025 13:13:24 +0000</lastBuildDate><item><title>How AI is introducing errors into courtrooms (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/05/20/1116823/how-ai-is-introducing-errors-into-courtrooms/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/05/250516-algo-aicourt.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;It’s been quite a couple weeks for stories about AI in the courtroom. You might have heard about the deceased victim of a road rage incident whose family created an AI avatar of him to show as an impact statement (possibly the first time this has been done in the US). But there’s a bigger, far more consequential controversy brewing, legal experts say. AI hallucinations are cropping up more and more in legal filings. And it’s starting to infuriate judges. Just consider these three cases, each of which gives a glimpse into what we can expect to see more of as lawyers embrace AI.&lt;/p&gt;  &lt;p&gt;A few weeks ago, a California judge, Michael Wilner, became intrigued by a set of arguments some lawyers made in a filing. He went to learn more about those arguments by following the articles they cited. But the articles didn’t exist. He asked the lawyers’ firm for more details, and they responded with a new brief that contained even more mistakes than the first. Wilner ordered the attorneys to give sworn testimonies explaining the mistakes, in which he learned that one of them, from the elite firm Ellis George, used Google Gemini as well as law-specific AI models to help write the document, which generated false information. As detailed in a filing on May 6, the judge fined the firm $31,000.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Last week, another California-based judge caught another hallucination in a court filing, this time submitted by the AI company Anthropic in the lawsuit that record labels have brought against it over copyright issues. One of Anthropic’s lawyers had asked the company’s AI model Claude to create a citation for a legal article, but Claude included the wrong title and author. Anthropic’s attorney admitted that the mistake was not caught by anyone reviewing the document.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Lastly, and perhaps most concerning, is a case unfolding in Israel. After police arrested an individual on charges of money laundering, Israeli prosecutors submitted a request asking a judge for permission to keep the individual’s phone as evidence. But they cited laws that don’t exist, prompting the defendant’s attorney to accuse them of including AI hallucinations in their request. The prosecutors, according to Israeli news outlets, admitted that this was the case, receiving a scolding from the judge.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Taken together, these cases point to a serious problem. Courts rely on documents that are accurate and backed up with citations—two traits that AI models, despite being adopted by lawyers eager to save time, often fail miserably to deliver.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Those mistakes are getting caught (for now), but it’s not a stretch to imagine that at some point soon, a judge’s decision will be influenced by something that’s totally made up by AI, and no one will catch it.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;I spoke with Maura Grossman, who teaches at the School of Computer Science at the University of Waterloo as well as Osgoode Hall Law School, and has been a vocal early critic of the problems that generative AI poses for courts. She wrote about the problem back in 2023, when the first cases of hallucinations started appearing. She said she thought courts’ existing rules requiring lawyers to vet what they submit to the courts, combined with the bad publicity those cases attracted, would put a stop to the problem. That hasn’t panned out.&lt;/p&gt;  &lt;p&gt;Hallucinations “don’t seem to have slowed down,” she says. “If anything, they’ve sped up.” And these aren’t one-off cases with obscure local firms, she says. These are big-time lawyers making significant, embarrassing mistakes with AI. She worries that such mistakes are also cropping up more in documents not written by lawyers themselves, like expert reports (in December, a Stanford professor and expert on AI admitted to including AI-generated mistakes in his testimony).&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I told Grossman that I find all this a little surprising. Attorneys, more than most, are obsessed with diction. They choose their words with precision. Why are so many getting caught making these mistakes?&lt;/p&gt;  &lt;p&gt;“Lawyers fall in two camps,” she says. “The first are scared to death and don’t want to use it at all.” But then there are the early adopters. These are lawyers tight on time or without a cadre of other lawyers to help with a brief. They’re eager for technology that can help them write documents under tight deadlines. And their checks on the AI’s work aren’t always thorough.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;The fact that high-powered lawyers, whose very profession it is to scrutinize language, keep getting caught making mistakes introduced by AI says something about how most of us treat the technology right now. We’re told repeatedly that AI makes mistakes, but language models also feel a bit like magic. We put in a complicated question and receive what sounds like a thoughtful, intelligent reply. Over time, AI models develop a veneer of authority. We trust them.&lt;/p&gt;  &lt;p&gt;“We assume that because these large language models are so fluent, it also means that they’re accurate,” Grossman says. “We all sort of slip into that trusting mode because it sounds authoritative.” Attorneys are used to checking the work of junior attorneys and interns but for some reason, Grossman says, don’t apply this skepticism to AI.&lt;/p&gt;  &lt;p&gt;We’ve known about this problem ever since ChatGPT launched nearly three years ago, but the recommended solution has not evolved much since then: Don’t trust everything you read, and vet what an AI model tells you. As AI models get thrust into so many different tools we use, I increasingly find this to be an unsatisfying counter to one of AI’s most foundational flaws.&lt;/p&gt;  &lt;p&gt;Hallucinations are inherent to the way that large language models work. Despite that, companies are selling generative AI tools made for lawyers that claim to be reliably accurate. “Feel confident your research is accurate and complete,” reads the website for Westlaw Precision, and the website for CoCounsel promises its AI is “backed by authoritative content.” That didn’t stop their client, Ellis George, from being fined $31,000.&lt;/p&gt;  &lt;p&gt;Increasingly, I have sympathy for people who trust AI more than they should. We are, after all, living in a time when the people building this technology are telling us that AI is so powerful it should be treated like nuclear weapons. Models have learned from nearly every word humanity has ever written down and are infiltrating our online life. If people shouldn’t trust everything AI models say, they probably deserve to be reminded of that a little more often by the companies building them.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This story originally appeared in&amp;nbsp;&lt;/em&gt;The Algorithm&lt;em&gt;, our weekly newsletter on AI. To get stories like this in your inbox first,&amp;nbsp;sign up here.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/05/250516-algo-aicourt.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;It’s been quite a couple weeks for stories about AI in the courtroom. You might have heard about the deceased victim of a road rage incident whose family created an AI avatar of him to show as an impact statement (possibly the first time this has been done in the US). But there’s a bigger, far more consequential controversy brewing, legal experts say. AI hallucinations are cropping up more and more in legal filings. And it’s starting to infuriate judges. Just consider these three cases, each of which gives a glimpse into what we can expect to see more of as lawyers embrace AI.&lt;/p&gt;  &lt;p&gt;A few weeks ago, a California judge, Michael Wilner, became intrigued by a set of arguments some lawyers made in a filing. He went to learn more about those arguments by following the articles they cited. But the articles didn’t exist. He asked the lawyers’ firm for more details, and they responded with a new brief that contained even more mistakes than the first. Wilner ordered the attorneys to give sworn testimonies explaining the mistakes, in which he learned that one of them, from the elite firm Ellis George, used Google Gemini as well as law-specific AI models to help write the document, which generated false information. As detailed in a filing on May 6, the judge fined the firm $31,000.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Last week, another California-based judge caught another hallucination in a court filing, this time submitted by the AI company Anthropic in the lawsuit that record labels have brought against it over copyright issues. One of Anthropic’s lawyers had asked the company’s AI model Claude to create a citation for a legal article, but Claude included the wrong title and author. Anthropic’s attorney admitted that the mistake was not caught by anyone reviewing the document.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Lastly, and perhaps most concerning, is a case unfolding in Israel. After police arrested an individual on charges of money laundering, Israeli prosecutors submitted a request asking a judge for permission to keep the individual’s phone as evidence. But they cited laws that don’t exist, prompting the defendant’s attorney to accuse them of including AI hallucinations in their request. The prosecutors, according to Israeli news outlets, admitted that this was the case, receiving a scolding from the judge.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Taken together, these cases point to a serious problem. Courts rely on documents that are accurate and backed up with citations—two traits that AI models, despite being adopted by lawyers eager to save time, often fail miserably to deliver.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Those mistakes are getting caught (for now), but it’s not a stretch to imagine that at some point soon, a judge’s decision will be influenced by something that’s totally made up by AI, and no one will catch it.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;I spoke with Maura Grossman, who teaches at the School of Computer Science at the University of Waterloo as well as Osgoode Hall Law School, and has been a vocal early critic of the problems that generative AI poses for courts. She wrote about the problem back in 2023, when the first cases of hallucinations started appearing. She said she thought courts’ existing rules requiring lawyers to vet what they submit to the courts, combined with the bad publicity those cases attracted, would put a stop to the problem. That hasn’t panned out.&lt;/p&gt;  &lt;p&gt;Hallucinations “don’t seem to have slowed down,” she says. “If anything, they’ve sped up.” And these aren’t one-off cases with obscure local firms, she says. These are big-time lawyers making significant, embarrassing mistakes with AI. She worries that such mistakes are also cropping up more in documents not written by lawyers themselves, like expert reports (in December, a Stanford professor and expert on AI admitted to including AI-generated mistakes in his testimony).&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I told Grossman that I find all this a little surprising. Attorneys, more than most, are obsessed with diction. They choose their words with precision. Why are so many getting caught making these mistakes?&lt;/p&gt;  &lt;p&gt;“Lawyers fall in two camps,” she says. “The first are scared to death and don’t want to use it at all.” But then there are the early adopters. These are lawyers tight on time or without a cadre of other lawyers to help with a brief. They’re eager for technology that can help them write documents under tight deadlines. And their checks on the AI’s work aren’t always thorough.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;The fact that high-powered lawyers, whose very profession it is to scrutinize language, keep getting caught making mistakes introduced by AI says something about how most of us treat the technology right now. We’re told repeatedly that AI makes mistakes, but language models also feel a bit like magic. We put in a complicated question and receive what sounds like a thoughtful, intelligent reply. Over time, AI models develop a veneer of authority. We trust them.&lt;/p&gt;  &lt;p&gt;“We assume that because these large language models are so fluent, it also means that they’re accurate,” Grossman says. “We all sort of slip into that trusting mode because it sounds authoritative.” Attorneys are used to checking the work of junior attorneys and interns but for some reason, Grossman says, don’t apply this skepticism to AI.&lt;/p&gt;  &lt;p&gt;We’ve known about this problem ever since ChatGPT launched nearly three years ago, but the recommended solution has not evolved much since then: Don’t trust everything you read, and vet what an AI model tells you. As AI models get thrust into so many different tools we use, I increasingly find this to be an unsatisfying counter to one of AI’s most foundational flaws.&lt;/p&gt;  &lt;p&gt;Hallucinations are inherent to the way that large language models work. Despite that, companies are selling generative AI tools made for lawyers that claim to be reliably accurate. “Feel confident your research is accurate and complete,” reads the website for Westlaw Precision, and the website for CoCounsel promises its AI is “backed by authoritative content.” That didn’t stop their client, Ellis George, from being fined $31,000.&lt;/p&gt;  &lt;p&gt;Increasingly, I have sympathy for people who trust AI more than they should. We are, after all, living in a time when the people building this technology are telling us that AI is so powerful it should be treated like nuclear weapons. Models have learned from nearly every word humanity has ever written down and are infiltrating our online life. If people shouldn’t trust everything AI models say, they probably deserve to be reminded of that a little more often by the companies building them.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This story originally appeared in&amp;nbsp;&lt;/em&gt;The Algorithm&lt;em&gt;, our weekly newsletter on AI. To get stories like this in your inbox first,&amp;nbsp;sign up here.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/05/20/1116823/how-ai-is-introducing-errors-into-courtrooms/</guid><pubDate>Tue, 20 May 2025 09:00:00 +0000</pubDate></item><item><title>AI’s energy impact is still small—but how we handle it is huge (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/05/20/1116274/opinion-ai-energy-use-data-centers-electricity/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/05/0071_3_250513_F-03_72.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;With seemingly no limit to the demand for artificial intelligence, everyone in the energy, AI, and climate fields is justifiably worried. Will there be enough clean electricity to power AI and enough water to cool the data centers that support this technology? These are important questions with serious implications for communities, the economy, and the environment.&amp;nbsp;&lt;/p&gt;    &lt;p&gt;But the question about AI’s energy usage portends even bigger issues about what we need to do in addressing climate change for the next several decades. If we can’t work out how to handle this, we won’t be able to handle broader electrification of the economy, and the climate risks we face will increase.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Innovation in IT got us to this point. Graphics processing units (GPUs) that power the computing behind AI have fallen in cost by 99% since 2006. There was similar concern about the energy use of data centers in the early 2010s, with wild projections of growth in electricity demand. But gains in computing power and energy efficiency not only proved these projections wrong but enabled a 550% increase in global computing capability from 2010 to 2018 with only minimal increases in energy use.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In the late 2010s, however, the trends that had saved us began to break. As the accuracy of AI models dramatically improved, the electricity needed for data centers also started increasing faster; they now account for 4.4% of total demand, up&amp;nbsp; from 1.9% in 2018. Data centers consume more than 10% of the electricity supply in six US states. In Virginia, which has emerged as a hub of data center activity, that figure is 25%.&lt;/p&gt; 
 &lt;p&gt;Projections about the future demand for energy to power AI are uncertain and range widely, but in one study, Lawrence Berkeley National Laboratory estimated that data centers could represent 6% to 12% of total US electricity use by 2028. Communities and companies will notice this type of rapid growth in electricity demand. It will put pressure on energy prices and on ecosystems. The projections have resulted in calls to build lots of new fossil-fired power plants or bring older ones out of retirement. In many parts of the US, the demand will likely result in a surge of natural-gas-powered plants.&lt;/p&gt;  &lt;p&gt;It’s a daunting situation. Yet when we zoom out, the projected electricity use from AI is still pretty small. The US generated about 4,300 billion kilowatt-hours last year. We’ll likely need another 1,000 billion to 1,200 billion or more in the next decade—a 24% to 29% increase. Almost half the additional electricity demand will be from electrified vehicles. Another 30% is expected to be from electrified technologies in buildings and industry. Innovation in vehicle and building electrification also advanced in the last decade, and this shift will be good news for the climate, for communities, and for energy costs.&lt;/p&gt; 
 &lt;p&gt;The remaining 22% of new electricity demand is estimated to come from AI and data centers. While it represents a smaller piece of the pie, it’s the most urgent one. Because of their rapid growth and geographic concentration, data centers are the electrification challenge we face right now—the small stuff we have to figure out before we’re able to do the big stuff like vehicles and buildings.&lt;/p&gt;  &lt;p&gt;We also need to understand what the energy consumption and carbon emissions associated with AI are buying us. While the impacts from producing semiconductors and powering AI data centers are important, they are likely small compared with the positive or negative effects AI may have on applications such as the electricity grid, the transportation system, buildings and factories, or consumer behavior. Companies could use AI to develop new materials or batteries that would better integrate renewable energy into the grid. But they could also use AI to make it easier to find more fossil fuels. The claims about potential benefits for the climate are exciting, but they need to be continuously verified and will need support to be realized.&lt;/p&gt;  &lt;p&gt;This isn’t the first time we’ve faced challenges coping with growth in electricity demand. In the 1960s, US electricity demand was growing at more than 7% per year. In the 1970s that growth was nearly 5%, and in the 1980s and 1990s it was more than 2% per year. Then, starting in 2005, we basically had a decade and a half of flat electricity growth. Most projections for the next decade put our expected growth in electricity demand at around 2% again—but this time we’ll have to do things differently.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;To manage these new energy demands, we need a “Grid New Deal” that leverages public and private capital to rebuild the electricity system for AI with enough capacity and intelligence for decarbonization. New clean energy supplies, investment in transmission and distribution, and strategies for virtual demand management can cut emissions, lower prices, and increase resilience. Data centers bringing clean electricity and distribution system upgrades could be given a fast lane to connect to the grid. Infrastructure banks could fund new transmission lines or pay to upgrade existing ones. Direct investment or tax incentives could encourage clean computing standards, workforce development in the clean energy sector, and open data transparency from data center operators about their energy use so that communities can understand and measure the impacts.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;In 2022, the White House released a Blueprint for an AI Bill of Rights that provided principles to protect the public’s rights, opportunities, and access to critical resources from being restricted by AI systems. To the AI Bill of Rights, we humbly offer a climate amendment, because ethical AI must be climate-safe AI. It’s a starting point to ensure that the growth of AI works for everyone—that it doesn’t raise people’s energy bills, adds more clean power to the grid than it uses, increases investment in the power system’s infrastructure, and benefits communities while driving innovation.&lt;/p&gt;  &lt;p&gt;By grounding the conversation about AI and energy in context about what is needed to tackle climate change, we can deliver better outcomes for communities, ecosystems, and the economy. The growth of electricity demand for AI and data centers is a test case for how society will respond to the demands and challenges of broader electrification. If we get this wrong, the likelihood of meeting our climate targets will be extremely low. This is what we mean when we say the energy and climate impacts from data centers are small, but they are also huge.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Costa Samaras is the Trustee Professor of Civil and Environmental Engineering and director of the Scott Institute for Energy Innovation at Carnegie Mellon University.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Emma Strubell is the Raj Reddy Assistant Professor in the Language Technologies Institute in the School of Computer Science at Carnegie Mellon University.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Ramayya Krishnan is dean of the Heinz College of Information Systems and Public Policy and the William W. and Ruth F. Cooper Professor of Management Science and Information Systems at Carnegie Mellon University.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="seriesPromo__wrap--ef66c94cf04f56bc5c35c7a22ece9ac9"&gt;&lt;/aside&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/05/0071_3_250513_F-03_72.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;With seemingly no limit to the demand for artificial intelligence, everyone in the energy, AI, and climate fields is justifiably worried. Will there be enough clean electricity to power AI and enough water to cool the data centers that support this technology? These are important questions with serious implications for communities, the economy, and the environment.&amp;nbsp;&lt;/p&gt;    &lt;p&gt;But the question about AI’s energy usage portends even bigger issues about what we need to do in addressing climate change for the next several decades. If we can’t work out how to handle this, we won’t be able to handle broader electrification of the economy, and the climate risks we face will increase.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Innovation in IT got us to this point. Graphics processing units (GPUs) that power the computing behind AI have fallen in cost by 99% since 2006. There was similar concern about the energy use of data centers in the early 2010s, with wild projections of growth in electricity demand. But gains in computing power and energy efficiency not only proved these projections wrong but enabled a 550% increase in global computing capability from 2010 to 2018 with only minimal increases in energy use.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In the late 2010s, however, the trends that had saved us began to break. As the accuracy of AI models dramatically improved, the electricity needed for data centers also started increasing faster; they now account for 4.4% of total demand, up&amp;nbsp; from 1.9% in 2018. Data centers consume more than 10% of the electricity supply in six US states. In Virginia, which has emerged as a hub of data center activity, that figure is 25%.&lt;/p&gt; 
 &lt;p&gt;Projections about the future demand for energy to power AI are uncertain and range widely, but in one study, Lawrence Berkeley National Laboratory estimated that data centers could represent 6% to 12% of total US electricity use by 2028. Communities and companies will notice this type of rapid growth in electricity demand. It will put pressure on energy prices and on ecosystems. The projections have resulted in calls to build lots of new fossil-fired power plants or bring older ones out of retirement. In many parts of the US, the demand will likely result in a surge of natural-gas-powered plants.&lt;/p&gt;  &lt;p&gt;It’s a daunting situation. Yet when we zoom out, the projected electricity use from AI is still pretty small. The US generated about 4,300 billion kilowatt-hours last year. We’ll likely need another 1,000 billion to 1,200 billion or more in the next decade—a 24% to 29% increase. Almost half the additional electricity demand will be from electrified vehicles. Another 30% is expected to be from electrified technologies in buildings and industry. Innovation in vehicle and building electrification also advanced in the last decade, and this shift will be good news for the climate, for communities, and for energy costs.&lt;/p&gt; 
 &lt;p&gt;The remaining 22% of new electricity demand is estimated to come from AI and data centers. While it represents a smaller piece of the pie, it’s the most urgent one. Because of their rapid growth and geographic concentration, data centers are the electrification challenge we face right now—the small stuff we have to figure out before we’re able to do the big stuff like vehicles and buildings.&lt;/p&gt;  &lt;p&gt;We also need to understand what the energy consumption and carbon emissions associated with AI are buying us. While the impacts from producing semiconductors and powering AI data centers are important, they are likely small compared with the positive or negative effects AI may have on applications such as the electricity grid, the transportation system, buildings and factories, or consumer behavior. Companies could use AI to develop new materials or batteries that would better integrate renewable energy into the grid. But they could also use AI to make it easier to find more fossil fuels. The claims about potential benefits for the climate are exciting, but they need to be continuously verified and will need support to be realized.&lt;/p&gt;  &lt;p&gt;This isn’t the first time we’ve faced challenges coping with growth in electricity demand. In the 1960s, US electricity demand was growing at more than 7% per year. In the 1970s that growth was nearly 5%, and in the 1980s and 1990s it was more than 2% per year. Then, starting in 2005, we basically had a decade and a half of flat electricity growth. Most projections for the next decade put our expected growth in electricity demand at around 2% again—but this time we’ll have to do things differently.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;To manage these new energy demands, we need a “Grid New Deal” that leverages public and private capital to rebuild the electricity system for AI with enough capacity and intelligence for decarbonization. New clean energy supplies, investment in transmission and distribution, and strategies for virtual demand management can cut emissions, lower prices, and increase resilience. Data centers bringing clean electricity and distribution system upgrades could be given a fast lane to connect to the grid. Infrastructure banks could fund new transmission lines or pay to upgrade existing ones. Direct investment or tax incentives could encourage clean computing standards, workforce development in the clean energy sector, and open data transparency from data center operators about their energy use so that communities can understand and measure the impacts.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;In 2022, the White House released a Blueprint for an AI Bill of Rights that provided principles to protect the public’s rights, opportunities, and access to critical resources from being restricted by AI systems. To the AI Bill of Rights, we humbly offer a climate amendment, because ethical AI must be climate-safe AI. It’s a starting point to ensure that the growth of AI works for everyone—that it doesn’t raise people’s energy bills, adds more clean power to the grid than it uses, increases investment in the power system’s infrastructure, and benefits communities while driving innovation.&lt;/p&gt;  &lt;p&gt;By grounding the conversation about AI and energy in context about what is needed to tackle climate change, we can deliver better outcomes for communities, ecosystems, and the economy. The growth of electricity demand for AI and data centers is a test case for how society will respond to the demands and challenges of broader electrification. If we get this wrong, the likelihood of meeting our climate targets will be extremely low. This is what we mean when we say the energy and climate impacts from data centers are small, but they are also huge.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Costa Samaras is the Trustee Professor of Civil and Environmental Engineering and director of the Scott Institute for Energy Innovation at Carnegie Mellon University.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Emma Strubell is the Raj Reddy Assistant Professor in the Language Technologies Institute in the School of Computer Science at Carnegie Mellon University.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Ramayya Krishnan is dean of the Heinz College of Information Systems and Public Policy and the William W. and Ruth F. Cooper Professor of Management Science and Information Systems at Carnegie Mellon University.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="seriesPromo__wrap--ef66c94cf04f56bc5c35c7a22ece9ac9"&gt;&lt;/aside&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/05/20/1116274/opinion-ai-energy-use-data-centers-electricity/</guid><pubDate>Tue, 20 May 2025 09:00:00 +0000</pubDate></item><item><title>Gemini 2.5: Our most intelligent models are getting even better (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/gemini-25-our-world-leading-model-is-getting-even-better/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/deep-think__key-art_16-9.width-1300.jpg" /&gt;&lt;/div&gt;&lt;h2&gt;New Gemini 2.5 capabilities&lt;/h2&gt;&lt;h3&gt;Native audio output and improvements to Live API&lt;/h3&gt;&lt;p&gt;Today, the Live API is introducing a preview version of audio-visual input and native audio out dialogue, so you can directly build conversational experiences, with a more natural and expressive Gemini.&lt;/p&gt;&lt;p&gt;It also allows the user to steer its tone, accent and style of speaking. For example, you can tell the model to use a dramatic voice when telling a story. And it supports tool use, to be able to search on your behalf.&lt;/p&gt;&lt;p&gt;You can experiment with a set of early features, including:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Affective Dialogue, in which the model detects emotion in the user's voice and responds appropriately.&lt;/li&gt;&lt;li&gt;Proactive Audio, in which the model will ignore background conversations and know when to respond.&lt;/li&gt;&lt;li&gt;Thinking in the Live API, in which the model leverages Gemini’s thinking capabilities to support more complex tasks.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We’re also releasing new previews for text-to-speech in 2.5 Pro and 2.5 Flash. These have first-of-its-kind support for multiple speakers, enabling text-to-speech with two voices via native audio out.&lt;/p&gt;&lt;p&gt;Like Native Audio dialogue, text-to-speech is expressive, and can capture really subtle nuances, such as whispers. It works in over 24 languages and seamlessly switches between them.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/deep-think__key-art_16-9.width-1300.jpg" /&gt;&lt;/div&gt;&lt;h2&gt;New Gemini 2.5 capabilities&lt;/h2&gt;&lt;h3&gt;Native audio output and improvements to Live API&lt;/h3&gt;&lt;p&gt;Today, the Live API is introducing a preview version of audio-visual input and native audio out dialogue, so you can directly build conversational experiences, with a more natural and expressive Gemini.&lt;/p&gt;&lt;p&gt;It also allows the user to steer its tone, accent and style of speaking. For example, you can tell the model to use a dramatic voice when telling a story. And it supports tool use, to be able to search on your behalf.&lt;/p&gt;&lt;p&gt;You can experiment with a set of early features, including:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Affective Dialogue, in which the model detects emotion in the user's voice and responds appropriately.&lt;/li&gt;&lt;li&gt;Proactive Audio, in which the model will ignore background conversations and know when to respond.&lt;/li&gt;&lt;li&gt;Thinking in the Live API, in which the model leverages Gemini’s thinking capabilities to support more complex tasks.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We’re also releasing new previews for text-to-speech in 2.5 Pro and 2.5 Flash. These have first-of-its-kind support for multiple speakers, enabling text-to-speech with two voices via native audio out.&lt;/p&gt;&lt;p&gt;Like Native Audio dialogue, text-to-speech is expressive, and can capture really subtle nuances, such as whispers. It works in over 24 languages and seamlessly switches between them.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/gemini-25-our-world-leading-model-is-getting-even-better/</guid><pubDate>Tue, 20 May 2025 09:45:00 +0000</pubDate></item><item><title>Announcing Gemma 3n preview: Powerful, efficient, mobile-first AI (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/announcing-gemma-3n-preview-powerful-efficient-mobile-first-ai/</link><description>&lt;div&gt;
    &lt;p&gt;Following the exciting launches of Gemma 3 and Gemma 3 QAT, our family of state-of-the-art open models capable of running on a single cloud or desktop accelerator, we're pushing our vision for accessible AI even further. Gemma 3 delivered powerful capabilities for developers, and we're now extending that vision to highly capable, real-time AI operating directly on the devices you use every day – your phones, tablets, and laptops.&lt;/p&gt;&lt;p&gt;To power the next generation of on-device AI and support a diverse range of applications, including advancing the capabilities of Gemini Nano, we engineered a new, cutting-edge architecture. This next-generation foundation was created in close collaboration with mobile hardware leaders like Qualcomm Technologies, MediaTek, and Samsung's System LSI business, and is optimized for lightning-fast, multimodal AI, enabling truly personal and private experiences directly on your device.&lt;/p&gt;&lt;p&gt;Gemma 3n is our first open model built on this groundbreaking, shared architecture, allowing developers to begin experimenting with this technology today in an early preview. The same advanced architecture also powers the next generation of Gemini Nano, which brings these capabilities to a broad range of features in Google apps and our on-device ecosystem, and will become available later this year. Gemma 3n enables you to start building on this foundation that will come to major platforms such as Android and Chrome.&lt;/p&gt;
&lt;/div&gt;&lt;div&gt;
        &lt;div class="image-wrapper"&gt;
            
                &lt;img alt="Chatbot Arena Elo scores" class="regular-image" src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image3_OjwrVp1.original.png" /&gt;
            
            
                
                    &lt;p&gt;
                        This chart ranks AI models by Chatbot Arena Elo scores; higher scores (top numbers) indicate greater user preference. Gemma 3n ranks highly amongst both popular proprietary and open models.
                    &lt;/p&gt;
                
            
        &lt;/div&gt;
    &lt;/div&gt;&lt;div&gt;
    &lt;p&gt;Gemma 3n leverages a Google DeepMind innovation called Per-Layer Embeddings (PLE) that delivers a significant reduction in RAM usage. While the raw parameter count is 5B and 8B, this innovation allows you to run larger models on mobile devices or live-stream from the cloud, with a memory overhead comparable to a 2B and 4B model, meaning the models can operate with a dynamic memory footprint of just 2GB and 3GB. Learn more in our documentation.&lt;/p&gt;&lt;p&gt;By exploring Gemma 3n, developers can get an early preview of the open model’s core capabilities and mobile-first architectural innovations that will be available on Android and Chrome with Gemini Nano.&lt;/p&gt;&lt;p&gt;In this post, we'll explore Gemma 3n's new capabilities, our approach to responsible development, and how you can access the preview today.&lt;/p&gt;&lt;h3 id="key-capabilities-of-gemma-3n"&gt;&lt;b&gt;&lt;br /&gt;Key Capabilities of Gemma 3n&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Engineered for fast, low-footprint AI experiences running locally, Gemma 3n delivers:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Optimized On-Device Performance &amp;amp; Efficiency:&lt;/b&gt; Gemma 3n starts responding approximately 1.5x faster on mobile with significantly better quality (compared to Gemma 3 4B) and a reduced memory footprint achieved through innovations like Per Layer Embeddings, KVC sharing, and advanced activation quantization.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Many-in-1 Flexibility:&lt;/b&gt; A model with a 4B active memory footprint that natively includes a nested state-of-the-art 2B active memory footprint submodel (thanks to MatFormer training). This provides flexibility to dynamically trade off performance and quality on the fly without hosting separate models. We further introduce mix’n’match capability in Gemma 3n to dynamically create submodels from the 4B model that can optimally fit your specific use case -- and associated quality/latency tradeoff. Stay tuned for more on this research in our upcoming technical report.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Privacy-First &amp;amp; Offline Ready:&lt;/b&gt; Local execution enables features that respect user privacy and function reliably, even without an internet connection.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Expanded Multimodal Understanding with Audio:&lt;/b&gt; Gemma 3n can understand and process audio, text, and images, and offers significantly enhanced video understanding. Its audio capabilities enable the model to perform high-quality Automatic Speech Recognition (transcription) and Translation (speech to translated text). Additionally, the model accepts interleaved inputs across modalities, enabling understanding of complex multimodal interactions. (Public implementation coming soon)&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Improved Multilingual Capabilities:&lt;/b&gt; Improved multilingual performance, particularly in Japanese, German, Korean, Spanish, and French. Strong performance reflected on multilingual benchmarks such as 50.1% on WMT24++ (ChrF).&lt;/li&gt;&lt;/ul&gt;
&lt;/div&gt;&lt;div&gt;
        &lt;div class="image-wrapper"&gt;
            
                &lt;img alt="MMLU performance" class="regular-image" src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Artboard_1.original.png" /&gt;
            
            
                
                    &lt;p&gt;
                        This chart show’s MMLU performance vs model size of Gemma 3n’s mix-n-match (pretrained) capability.
                    &lt;/p&gt;
                
            
        &lt;/div&gt;
    &lt;/div&gt;&lt;div&gt;
    &lt;h3 id="unlocking-new-on-the-go-experiences"&gt;&lt;b&gt;Unlocking New On-the-go Experiences&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Gemma 3n will empower a new wave of intelligent, on-the-go applications by enabling developers to:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;Build live, interactive experiences&lt;/b&gt; that understand and respond to real-time visual and auditory cues from the user's environment.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;2. &lt;b&gt;Power deeper understanding&lt;/b&gt; and contextual text generation using combined audio, image, video, and text inputs—all processed privately on-device.&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;3. &lt;b&gt;Develop advanced audio-centric applications&lt;/b&gt;, including real-time speech transcription, translation, and rich voice-driven interactions.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;Here’s an overview and the types of experiences you can build:&lt;/p&gt;
&lt;/div&gt;&lt;div&gt;
    &lt;h3 id="building-responsibly-together"&gt;&lt;b&gt;Building Responsibly, Together&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Our commitment to responsible AI development is paramount. Gemma 3n, like all Gemma models, underwent rigorous safety evaluations, data governance, and fine-tuning alignment with our safety policies. We approach open models with careful risk assessment, continually refining our practices as the AI landscape evolves.&lt;/p&gt;&lt;h3 id="get-started:-preview-gemma-3n-today"&gt;&lt;b&gt;&lt;br /&gt;Get Started: Preview Gemma 3n Today&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;We're excited to get Gemma 3n into your hands through a preview starting today:&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;br /&gt;Initial Access (Available Now):&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Cloud-based Exploration with Google AI Studio:&lt;/b&gt; Try Gemma 3n directly in your browser on Google AI Studio – no setup needed. Explore its text input capabilities instantly.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;On-Device Development with Google AI Edge:&lt;/b&gt; For developers looking to integrate Gemma 3n locally, Google AI Edge provides tools and libraries. You can get started with text and image understanding/generation capabilities today.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br /&gt;Gemma 3n marks the next step in democratizing access to cutting-edge, efficient AI. We’re incredibly excited to see what you’ll build as we make this technology progressively available, starting with today's preview.&lt;/p&gt;&lt;p&gt;Explore this announcement and all Google I/O 2025 updates on io.google starting May 22.&lt;/p&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;
    &lt;p&gt;Following the exciting launches of Gemma 3 and Gemma 3 QAT, our family of state-of-the-art open models capable of running on a single cloud or desktop accelerator, we're pushing our vision for accessible AI even further. Gemma 3 delivered powerful capabilities for developers, and we're now extending that vision to highly capable, real-time AI operating directly on the devices you use every day – your phones, tablets, and laptops.&lt;/p&gt;&lt;p&gt;To power the next generation of on-device AI and support a diverse range of applications, including advancing the capabilities of Gemini Nano, we engineered a new, cutting-edge architecture. This next-generation foundation was created in close collaboration with mobile hardware leaders like Qualcomm Technologies, MediaTek, and Samsung's System LSI business, and is optimized for lightning-fast, multimodal AI, enabling truly personal and private experiences directly on your device.&lt;/p&gt;&lt;p&gt;Gemma 3n is our first open model built on this groundbreaking, shared architecture, allowing developers to begin experimenting with this technology today in an early preview. The same advanced architecture also powers the next generation of Gemini Nano, which brings these capabilities to a broad range of features in Google apps and our on-device ecosystem, and will become available later this year. Gemma 3n enables you to start building on this foundation that will come to major platforms such as Android and Chrome.&lt;/p&gt;
&lt;/div&gt;&lt;div&gt;
        &lt;div class="image-wrapper"&gt;
            
                &lt;img alt="Chatbot Arena Elo scores" class="regular-image" src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image3_OjwrVp1.original.png" /&gt;
            
            
                
                    &lt;p&gt;
                        This chart ranks AI models by Chatbot Arena Elo scores; higher scores (top numbers) indicate greater user preference. Gemma 3n ranks highly amongst both popular proprietary and open models.
                    &lt;/p&gt;
                
            
        &lt;/div&gt;
    &lt;/div&gt;&lt;div&gt;
    &lt;p&gt;Gemma 3n leverages a Google DeepMind innovation called Per-Layer Embeddings (PLE) that delivers a significant reduction in RAM usage. While the raw parameter count is 5B and 8B, this innovation allows you to run larger models on mobile devices or live-stream from the cloud, with a memory overhead comparable to a 2B and 4B model, meaning the models can operate with a dynamic memory footprint of just 2GB and 3GB. Learn more in our documentation.&lt;/p&gt;&lt;p&gt;By exploring Gemma 3n, developers can get an early preview of the open model’s core capabilities and mobile-first architectural innovations that will be available on Android and Chrome with Gemini Nano.&lt;/p&gt;&lt;p&gt;In this post, we'll explore Gemma 3n's new capabilities, our approach to responsible development, and how you can access the preview today.&lt;/p&gt;&lt;h3 id="key-capabilities-of-gemma-3n"&gt;&lt;b&gt;&lt;br /&gt;Key Capabilities of Gemma 3n&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Engineered for fast, low-footprint AI experiences running locally, Gemma 3n delivers:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Optimized On-Device Performance &amp;amp; Efficiency:&lt;/b&gt; Gemma 3n starts responding approximately 1.5x faster on mobile with significantly better quality (compared to Gemma 3 4B) and a reduced memory footprint achieved through innovations like Per Layer Embeddings, KVC sharing, and advanced activation quantization.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Many-in-1 Flexibility:&lt;/b&gt; A model with a 4B active memory footprint that natively includes a nested state-of-the-art 2B active memory footprint submodel (thanks to MatFormer training). This provides flexibility to dynamically trade off performance and quality on the fly without hosting separate models. We further introduce mix’n’match capability in Gemma 3n to dynamically create submodels from the 4B model that can optimally fit your specific use case -- and associated quality/latency tradeoff. Stay tuned for more on this research in our upcoming technical report.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Privacy-First &amp;amp; Offline Ready:&lt;/b&gt; Local execution enables features that respect user privacy and function reliably, even without an internet connection.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Expanded Multimodal Understanding with Audio:&lt;/b&gt; Gemma 3n can understand and process audio, text, and images, and offers significantly enhanced video understanding. Its audio capabilities enable the model to perform high-quality Automatic Speech Recognition (transcription) and Translation (speech to translated text). Additionally, the model accepts interleaved inputs across modalities, enabling understanding of complex multimodal interactions. (Public implementation coming soon)&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Improved Multilingual Capabilities:&lt;/b&gt; Improved multilingual performance, particularly in Japanese, German, Korean, Spanish, and French. Strong performance reflected on multilingual benchmarks such as 50.1% on WMT24++ (ChrF).&lt;/li&gt;&lt;/ul&gt;
&lt;/div&gt;&lt;div&gt;
        &lt;div class="image-wrapper"&gt;
            
                &lt;img alt="MMLU performance" class="regular-image" src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Artboard_1.original.png" /&gt;
            
            
                
                    &lt;p&gt;
                        This chart show’s MMLU performance vs model size of Gemma 3n’s mix-n-match (pretrained) capability.
                    &lt;/p&gt;
                
            
        &lt;/div&gt;
    &lt;/div&gt;&lt;div&gt;
    &lt;h3 id="unlocking-new-on-the-go-experiences"&gt;&lt;b&gt;Unlocking New On-the-go Experiences&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Gemma 3n will empower a new wave of intelligent, on-the-go applications by enabling developers to:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;Build live, interactive experiences&lt;/b&gt; that understand and respond to real-time visual and auditory cues from the user's environment.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;2. &lt;b&gt;Power deeper understanding&lt;/b&gt; and contextual text generation using combined audio, image, video, and text inputs—all processed privately on-device.&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;3. &lt;b&gt;Develop advanced audio-centric applications&lt;/b&gt;, including real-time speech transcription, translation, and rich voice-driven interactions.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;Here’s an overview and the types of experiences you can build:&lt;/p&gt;
&lt;/div&gt;&lt;div&gt;
    &lt;h3 id="building-responsibly-together"&gt;&lt;b&gt;Building Responsibly, Together&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Our commitment to responsible AI development is paramount. Gemma 3n, like all Gemma models, underwent rigorous safety evaluations, data governance, and fine-tuning alignment with our safety policies. We approach open models with careful risk assessment, continually refining our practices as the AI landscape evolves.&lt;/p&gt;&lt;h3 id="get-started:-preview-gemma-3n-today"&gt;&lt;b&gt;&lt;br /&gt;Get Started: Preview Gemma 3n Today&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;We're excited to get Gemma 3n into your hands through a preview starting today:&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;br /&gt;Initial Access (Available Now):&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Cloud-based Exploration with Google AI Studio:&lt;/b&gt; Try Gemma 3n directly in your browser on Google AI Studio – no setup needed. Explore its text input capabilities instantly.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;On-Device Development with Google AI Edge:&lt;/b&gt; For developers looking to integrate Gemma 3n locally, Google AI Edge provides tools and libraries. You can get started with text and image understanding/generation capabilities today.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br /&gt;Gemma 3n marks the next step in democratizing access to cutting-edge, efficient AI. We’re incredibly excited to see what you’ll build as we make this technology progressively available, starting with today's preview.&lt;/p&gt;&lt;p&gt;Explore this announcement and all Google I/O 2025 updates on io.google starting May 22.&lt;/p&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/announcing-gemma-3n-preview-powerful-efficient-mobile-first-ai/</guid><pubDate>Tue, 20 May 2025 09:45:00 +0000</pubDate></item><item><title>SynthID Detector — a new portal to help identify AI-generated content (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/synthid-detector--a-new-portal-to-help-identify-ai-generated-content/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_Gemini_MOD_HEADER.width-1300.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Advances in generative AI are making it possible for people to create content in entirely new ways — from text to high quality audio, images and videos. As these capabilities advance and become more broadly available, questions of authenticity, context and verification emerge.&lt;/p&gt;&lt;p&gt;Today we’re announcing SynthID Detector, a verification portal to quickly and efficiently identify AI-generated content made with Google AI. The portal provides detection capabilities across different modalities in one place and provides essential transparency in the rapidly evolving landscape of generative media. It can also highlight which parts of the content are more likely to have been watermarked with SynthID.&lt;/p&gt;&lt;p&gt;When we launched SynthID — a state-of-the-art tool that embeds imperceptible watermarks and enables the identification of AI-generated content — our aim was to provide a suite of novel technical solutions to help minimise misinformation and misattribution.&lt;/p&gt;&lt;p&gt;SynthID not only preserves the content’s quality, it acts as a robust watermark that remains detectable even when the content is shared or undergoes a range of transformations. While originally focused on AI-generated imagery only, we’ve since expanded SynthID to cover AI-generated text, audio and video content, including content generated by our Gemini, Imagen, Lyria and Veo models across Google. Over 10 billion pieces of content have already been watermarked with SynthID.&lt;/p&gt;&lt;h2&gt;How SynthID Detector works&lt;/h2&gt;&lt;p&gt;When you upload an image, audio track, video or piece of text created using Google's AI tools, the portal will scan the media for a SynthID watermark. If a watermark is detected, the portal will highlight specific portions of the content most likely to be watermarked.&lt;/p&gt;&lt;p&gt;For audio, the portal pinpoints specific segments where a SynthID watermark is detected, and for images, it indicates areas where a watermark is most likely.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_Gemini_MOD_HEADER.width-1300.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Advances in generative AI are making it possible for people to create content in entirely new ways — from text to high quality audio, images and videos. As these capabilities advance and become more broadly available, questions of authenticity, context and verification emerge.&lt;/p&gt;&lt;p&gt;Today we’re announcing SynthID Detector, a verification portal to quickly and efficiently identify AI-generated content made with Google AI. The portal provides detection capabilities across different modalities in one place and provides essential transparency in the rapidly evolving landscape of generative media. It can also highlight which parts of the content are more likely to have been watermarked with SynthID.&lt;/p&gt;&lt;p&gt;When we launched SynthID — a state-of-the-art tool that embeds imperceptible watermarks and enables the identification of AI-generated content — our aim was to provide a suite of novel technical solutions to help minimise misinformation and misattribution.&lt;/p&gt;&lt;p&gt;SynthID not only preserves the content’s quality, it acts as a robust watermark that remains detectable even when the content is shared or undergoes a range of transformations. While originally focused on AI-generated imagery only, we’ve since expanded SynthID to cover AI-generated text, audio and video content, including content generated by our Gemini, Imagen, Lyria and Veo models across Google. Over 10 billion pieces of content have already been watermarked with SynthID.&lt;/p&gt;&lt;h2&gt;How SynthID Detector works&lt;/h2&gt;&lt;p&gt;When you upload an image, audio track, video or piece of text created using Google's AI tools, the portal will scan the media for a SynthID watermark. If a watermark is detected, the portal will highlight specific portions of the content most likely to be watermarked.&lt;/p&gt;&lt;p&gt;For audio, the portal pinpoints specific segments where a SynthID watermark is detected, and for images, it indicates areas where a watermark is most likely.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/synthid-detector--a-new-portal-to-help-identify-ai-generated-content/</guid><pubDate>Tue, 20 May 2025 09:45:00 +0000</pubDate></item><item><title>Our vision for building a universal AI assistant (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/our-vision-for-building-a-universal-ai-assistant/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_GeminiVision_SocialShare.width-1300.png" /&gt;&lt;/div&gt;&lt;p&gt;Over the last decade, we’ve laid a lot of the foundations for the modern AI era, from pioneering the Transformer architecture on which all large language models are based, to developing agent systems that can learn and plan like AlphaGo and AlphaZero.&lt;/p&gt;&lt;p&gt;We’ve applied these techniques to make breakthroughs in quantum computing, mathematics, life sciences and algorithmic discovery. And we continue to double down on the breadth and depth of our fundamental research, working to invent the next big breakthroughs necessary for artificial general intelligence (AGI).&lt;/p&gt;&lt;p&gt;This is why we’re working to extend our best multimodal foundation model, Gemini 2.5 Pro, to become a “world model” that can make plans and imagine new experiences by understanding and simulating aspects of the world, just as the brain does.&lt;/p&gt;&lt;p&gt;We’ve been taking strides in this direction for a while, from our pioneering work training agents to master complex games like Go and StarCraft, to building Genie 2, which is capable of generating 3D simulated environments that you can interact with, from a single image prompt.&lt;/p&gt;&lt;p&gt;Already, we can see evidence of these capabilities emerging in Gemini’s ability to use world knowledge and reasoning to represent and simulate natural environments, Veo’s deep understanding of intuitive physics, and the way Gemini Robotics teaches robots to grasp, follow instructions and adjust on the fly.&lt;/p&gt;&lt;p&gt;Making Gemini a world model is a critical step in developing a new, more general and more useful kind of AI —&amp;nbsp;a universal AI assistant. This is an AI that’s intelligent, understands the context you are in, and that can plan and take action on your behalf, across any device.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_GeminiVision_SocialShare.width-1300.png" /&gt;&lt;/div&gt;&lt;p&gt;Over the last decade, we’ve laid a lot of the foundations for the modern AI era, from pioneering the Transformer architecture on which all large language models are based, to developing agent systems that can learn and plan like AlphaGo and AlphaZero.&lt;/p&gt;&lt;p&gt;We’ve applied these techniques to make breakthroughs in quantum computing, mathematics, life sciences and algorithmic discovery. And we continue to double down on the breadth and depth of our fundamental research, working to invent the next big breakthroughs necessary for artificial general intelligence (AGI).&lt;/p&gt;&lt;p&gt;This is why we’re working to extend our best multimodal foundation model, Gemini 2.5 Pro, to become a “world model” that can make plans and imagine new experiences by understanding and simulating aspects of the world, just as the brain does.&lt;/p&gt;&lt;p&gt;We’ve been taking strides in this direction for a while, from our pioneering work training agents to master complex games like Go and StarCraft, to building Genie 2, which is capable of generating 3D simulated environments that you can interact with, from a single image prompt.&lt;/p&gt;&lt;p&gt;Already, we can see evidence of these capabilities emerging in Gemini’s ability to use world knowledge and reasoning to represent and simulate natural environments, Veo’s deep understanding of intuitive physics, and the way Gemini Robotics teaches robots to grasp, follow instructions and adjust on the fly.&lt;/p&gt;&lt;p&gt;Making Gemini a world model is a critical step in developing a new, more general and more useful kind of AI —&amp;nbsp;a universal AI assistant. This is an AI that’s intelligent, understands the context you are in, and that can plan and take action on your behalf, across any device.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/our-vision-for-building-a-universal-ai-assistant/</guid><pubDate>Tue, 20 May 2025 09:45:00 +0000</pubDate></item><item><title>Fuel your creativity with new generative media models and tools (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/fuel-your-creativity-with-new-generative-media-models-and-tools/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/5.20v2_SS_1920x1080.width-1300.png" /&gt;&lt;/div&gt;&lt;p&gt;Today, we’re announcing our newest generative media models, which mark significant breakthroughs. These models create breathtaking images, videos and music, empowering artists to bring their creative vision to life. They also power amazing tools for everyone to express themselves.&lt;/p&gt;&lt;p&gt;Veo 3 and Imagen 4, our newest video and image generation models, push the frontier of media generation, with their groundbreaking new capabilities. We're also expanding access to Lyria 2, giving musicians more tools to create music. Finally, we’re inviting visual storytellers to try Flow, our new AI filmmaking tool. Using Google DeepMind’s most advanced models, Flow lets you weave cinematic films with more sophisticated control of characters, scenes and styles, to bring your story to life.&lt;/p&gt;&lt;p&gt;We’ve partnered closely with the creative industries — filmmakers, musicians, artists, YouTube creators — to help shape these models and products responsibly and to give creators new tools to realize the possibilities of AI in their art.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/5.20v2_SS_1920x1080.width-1300.png" /&gt;&lt;/div&gt;&lt;p&gt;Today, we’re announcing our newest generative media models, which mark significant breakthroughs. These models create breathtaking images, videos and music, empowering artists to bring their creative vision to life. They also power amazing tools for everyone to express themselves.&lt;/p&gt;&lt;p&gt;Veo 3 and Imagen 4, our newest video and image generation models, push the frontier of media generation, with their groundbreaking new capabilities. We're also expanding access to Lyria 2, giving musicians more tools to create music. Finally, we’re inviting visual storytellers to try Flow, our new AI filmmaking tool. Using Google DeepMind’s most advanced models, Flow lets you weave cinematic films with more sophisticated control of characters, scenes and styles, to bring your story to life.&lt;/p&gt;&lt;p&gt;We’ve partnered closely with the creative industries — filmmakers, musicians, artists, YouTube creators — to help shape these models and products responsibly and to give creators new tools to realize the possibilities of AI in their art.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/fuel-your-creativity-with-new-generative-media-models-and-tools/</guid><pubDate>Tue, 20 May 2025 09:45:00 +0000</pubDate></item><item><title>Advancing Gemini's security safeguards (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/advancing-geminis-security-safeguards/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://lh3.googleusercontent.com/Uh_O6Nx1GWznAfODatYYz2sxiDekdb6HWnnSsy-cfmTxfjdUEEleh9w4cBdwUfBnyQBS-t1xW4UZXrMmC-rI6bz31hCrm5nHLt6Cp1FJAT7X9Upv5g=w1200-h630-n-nu" /&gt;&lt;/div&gt;&lt;div&gt;
  &lt;p class="gdm-rich-text__subtitle"&gt;We’re publishing a new white paper outlining how we’ve made Gemini 2.5 our most secure model family to date.&lt;/p&gt;&lt;p&gt;Imagine asking your AI agent to summarize your latest emails — a seemingly straightforward task. Gemini and other large language models (LLMs) are consistently improving at performing such tasks, by accessing information like our documents, calendars, or external websites. But what if one of those emails contains hidden, malicious instructions, designed to trick the AI into sharing private data or misusing its permissions?&lt;/p&gt;&lt;p&gt;Indirect prompt injection presents a real cybersecurity challenge where AI models sometimes struggle to differentiate between genuine user instructions and manipulative commands embedded within the data they retrieve. Our new white paper, Lessons from Defending Gemini Against Indirect Prompt Injections, lays out our strategic blueprint for tackling indirect prompt injections that make agentic AI tools, supported by advanced large language models, targets for such attacks.&lt;/p&gt;&lt;p&gt;Our commitment to build not just capable, but secure AI agents, means we’re continually working to understand how Gemini might respond to indirect prompt injections and make it more resilient against them.&lt;/p&gt;
&lt;/div&gt;&lt;div&gt;
  &lt;h2&gt;Evaluating baseline defense strategies&lt;/h2&gt;&lt;p&gt;Indirect prompt injection attacks are complex and require constant vigilance and multiple layers of defense. Google DeepMind’s Security and Privacy Research team specialises in protecting our AI models from deliberate, malicious attacks. Trying to find these vulnerabilities manually is slow and inefficient, especially as models evolve rapidly. That's one of the reasons we built an automated system to relentlessly probe Gemini’s defenses.&lt;/p&gt;&lt;h2&gt;Using automated red-teaming to make Gemini more secure&lt;/h2&gt;&lt;p&gt;A core part of our security strategy is automated red teaming (ART), where our internal Gemini team constantly attacks Gemini in realistic ways to uncover potential security weaknesses in the model. Using this technique, among other efforts detailed in our white paper, has helped significantly increase Gemini’s protection rate against indirect prompt injection attacks during tool-use, making Gemini 2.5 our most secure model family to date.&lt;/p&gt;&lt;p&gt;We tested several defense strategies suggested by the research community, as well as some of our own ideas:&lt;/p&gt;
&lt;/div&gt;&lt;div&gt;
  &lt;h2&gt;Tailoring evaluations for adaptive attacks&lt;/h2&gt;&lt;p&gt;Baseline mitigations showed promise against basic, non-adaptive attacks, significantly reducing the attack success rate. However, malicious actors increasingly use adaptive attacks that are specifically designed to evolve and adapt with ART to circumvent the defense being tested.&lt;/p&gt;&lt;p&gt;Successful baseline defenses like Spotlighting or Self-reflection became much less effective against adaptive attacks learning how to deal with and bypass static defense approaches.&lt;/p&gt;&lt;p&gt;This finding illustrates a key point: relying on defenses tested only against static attacks offers a false sense of security. For robust security, it is critical to evaluate adaptive attacks that evolve in response to potential defenses.&lt;/p&gt;&lt;h2&gt;Building inherent resilience through model hardening&lt;/h2&gt;&lt;p&gt;While external defenses and system-level guardrails are important, enhancing the AI model’s intrinsic ability to recognize and disregard malicious instructions embedded in data is also crucial. We call this process ‘model hardening’.&lt;/p&gt;&lt;p&gt;We fine-tuned Gemini on a large dataset of realistic scenarios, where ART generates effective indirect prompt injections targeting sensitive information. This taught Gemini to ignore the malicious embedded instruction and follow the original user request, thereby only providing the &lt;i&gt;correct&lt;/i&gt;, safe response it &lt;i&gt;should&lt;/i&gt; give. This allows the model to innately understand how to handle compromised information that evolves over time as part of adaptive attacks.&lt;/p&gt;&lt;p&gt;This model hardening has significantly boosted Gemini’s ability to identify and ignore injected instructions, lowering its attack success rate. And importantly, without significantly impacting the model’s performance on normal tasks.&lt;/p&gt;&lt;p&gt;It’s important to note that even with model hardening, no model is completely immune. Determined attackers might still find new vulnerabilities. Therefore, our goal is to make attacks much harder, costlier, and more complex for adversaries.&lt;/p&gt;&lt;h2&gt;Taking a holistic approach to model security&lt;/h2&gt;&lt;p&gt;Protecting AI models against attacks like indirect prompt injections requires “defense-in-depth” – using multiple layers of protection, including model hardening, input/output checks (like classifiers), and system-level guardrails. Combating indirect prompt injections is a key way we’re implementing our agentic security principles and guidelines to develop agents responsibly.&lt;/p&gt;&lt;p&gt;Securing advanced AI systems against specific, evolving threats like indirect prompt injection is an ongoing process. It demands pursuing continuous and adaptive evaluation, improving existing defenses and exploring new ones, and building inherent resilience into the models themselves. By layering defenses and learning constantly, we can enable AI assistants like Gemini to continue to be both incredibly helpful &lt;i&gt;and&lt;/i&gt; trustworthy.&lt;/p&gt;&lt;p&gt;To learn more about the defenses we built into Gemini and our recommendation for using more challenging, adaptive attacks to evaluate model robustness, please refer to the GDM white paper, Lessons from Defending Gemini Against Indirect Prompt Injections.&lt;/p&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://lh3.googleusercontent.com/Uh_O6Nx1GWznAfODatYYz2sxiDekdb6HWnnSsy-cfmTxfjdUEEleh9w4cBdwUfBnyQBS-t1xW4UZXrMmC-rI6bz31hCrm5nHLt6Cp1FJAT7X9Upv5g=w1200-h630-n-nu" /&gt;&lt;/div&gt;&lt;div&gt;
  &lt;p class="gdm-rich-text__subtitle"&gt;We’re publishing a new white paper outlining how we’ve made Gemini 2.5 our most secure model family to date.&lt;/p&gt;&lt;p&gt;Imagine asking your AI agent to summarize your latest emails — a seemingly straightforward task. Gemini and other large language models (LLMs) are consistently improving at performing such tasks, by accessing information like our documents, calendars, or external websites. But what if one of those emails contains hidden, malicious instructions, designed to trick the AI into sharing private data or misusing its permissions?&lt;/p&gt;&lt;p&gt;Indirect prompt injection presents a real cybersecurity challenge where AI models sometimes struggle to differentiate between genuine user instructions and manipulative commands embedded within the data they retrieve. Our new white paper, Lessons from Defending Gemini Against Indirect Prompt Injections, lays out our strategic blueprint for tackling indirect prompt injections that make agentic AI tools, supported by advanced large language models, targets for such attacks.&lt;/p&gt;&lt;p&gt;Our commitment to build not just capable, but secure AI agents, means we’re continually working to understand how Gemini might respond to indirect prompt injections and make it more resilient against them.&lt;/p&gt;
&lt;/div&gt;&lt;div&gt;
  &lt;h2&gt;Evaluating baseline defense strategies&lt;/h2&gt;&lt;p&gt;Indirect prompt injection attacks are complex and require constant vigilance and multiple layers of defense. Google DeepMind’s Security and Privacy Research team specialises in protecting our AI models from deliberate, malicious attacks. Trying to find these vulnerabilities manually is slow and inefficient, especially as models evolve rapidly. That's one of the reasons we built an automated system to relentlessly probe Gemini’s defenses.&lt;/p&gt;&lt;h2&gt;Using automated red-teaming to make Gemini more secure&lt;/h2&gt;&lt;p&gt;A core part of our security strategy is automated red teaming (ART), where our internal Gemini team constantly attacks Gemini in realistic ways to uncover potential security weaknesses in the model. Using this technique, among other efforts detailed in our white paper, has helped significantly increase Gemini’s protection rate against indirect prompt injection attacks during tool-use, making Gemini 2.5 our most secure model family to date.&lt;/p&gt;&lt;p&gt;We tested several defense strategies suggested by the research community, as well as some of our own ideas:&lt;/p&gt;
&lt;/div&gt;&lt;div&gt;
  &lt;h2&gt;Tailoring evaluations for adaptive attacks&lt;/h2&gt;&lt;p&gt;Baseline mitigations showed promise against basic, non-adaptive attacks, significantly reducing the attack success rate. However, malicious actors increasingly use adaptive attacks that are specifically designed to evolve and adapt with ART to circumvent the defense being tested.&lt;/p&gt;&lt;p&gt;Successful baseline defenses like Spotlighting or Self-reflection became much less effective against adaptive attacks learning how to deal with and bypass static defense approaches.&lt;/p&gt;&lt;p&gt;This finding illustrates a key point: relying on defenses tested only against static attacks offers a false sense of security. For robust security, it is critical to evaluate adaptive attacks that evolve in response to potential defenses.&lt;/p&gt;&lt;h2&gt;Building inherent resilience through model hardening&lt;/h2&gt;&lt;p&gt;While external defenses and system-level guardrails are important, enhancing the AI model’s intrinsic ability to recognize and disregard malicious instructions embedded in data is also crucial. We call this process ‘model hardening’.&lt;/p&gt;&lt;p&gt;We fine-tuned Gemini on a large dataset of realistic scenarios, where ART generates effective indirect prompt injections targeting sensitive information. This taught Gemini to ignore the malicious embedded instruction and follow the original user request, thereby only providing the &lt;i&gt;correct&lt;/i&gt;, safe response it &lt;i&gt;should&lt;/i&gt; give. This allows the model to innately understand how to handle compromised information that evolves over time as part of adaptive attacks.&lt;/p&gt;&lt;p&gt;This model hardening has significantly boosted Gemini’s ability to identify and ignore injected instructions, lowering its attack success rate. And importantly, without significantly impacting the model’s performance on normal tasks.&lt;/p&gt;&lt;p&gt;It’s important to note that even with model hardening, no model is completely immune. Determined attackers might still find new vulnerabilities. Therefore, our goal is to make attacks much harder, costlier, and more complex for adversaries.&lt;/p&gt;&lt;h2&gt;Taking a holistic approach to model security&lt;/h2&gt;&lt;p&gt;Protecting AI models against attacks like indirect prompt injections requires “defense-in-depth” – using multiple layers of protection, including model hardening, input/output checks (like classifiers), and system-level guardrails. Combating indirect prompt injections is a key way we’re implementing our agentic security principles and guidelines to develop agents responsibly.&lt;/p&gt;&lt;p&gt;Securing advanced AI systems against specific, evolving threats like indirect prompt injection is an ongoing process. It demands pursuing continuous and adaptive evaluation, improving existing defenses and exploring new ones, and building inherent resilience into the models themselves. By layering defenses and learning constantly, we can enable AI assistants like Gemini to continue to be both incredibly helpful &lt;i&gt;and&lt;/i&gt; trustworthy.&lt;/p&gt;&lt;p&gt;To learn more about the defenses we built into Gemini and our recommendation for using more challenging, adaptive attacks to evaluate model robustness, please refer to the GDM white paper, Lessons from Defending Gemini Against Indirect Prompt Injections.&lt;/p&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/advancing-geminis-security-safeguards/</guid><pubDate>Tue, 20 May 2025 09:45:00 +0000</pubDate></item><item><title>AI Safety Newsletter #55: Trump Administration Rescinds AI Diffusion Rule, Allows Chip Sales to Gulf States (AI Safety Newsletter)</title><link>https://newsletter.safe.ai/p/ai-safety-newsletter-55-trump-administration</link><description>&lt;p&gt;&lt;span&gt;Welcome to the AI Safety Newsletter by the &lt;/span&gt;&lt;a href="https://www.safe.ai/" rel="rel"&gt;Center for AI Safety&lt;/a&gt;&lt;span&gt;. We discuss developments in AI and AI safety. No technical background required.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;In this edition: The Trump Administration rescinds the Biden-era AI diffusion rule and sells AI chips to the UAE and Saudi Arabia; Federal lawmakers propose legislation on AI whistleblowers, location verification for AI chips, and prohibiting states from regulating AI.&lt;/p&gt;&lt;p&gt;&lt;span&gt;Listen to the AI Safety Newsletter for free on &lt;/span&gt;&lt;a href="https://spotify.link/E6lHa1ij2Cb" rel="rel"&gt;Spotify&lt;/a&gt;&lt;span&gt; or &lt;/span&gt;&lt;a href="https://podcasts.apple.com/us/podcast/ai-safety-newsletter/id1702875110" rel="rel"&gt;Apple Podcasts&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;The Center for AI Safety is also excited to announce the Summer session of our AI Safety, Ethics, and Society course, running from &lt;/span&gt;&lt;strong&gt;June 23 to September 14&lt;/strong&gt;&lt;span&gt;. The course, based on our recently published &lt;/span&gt;&lt;a href="https://www.routledge.com/Introduction-to-AI-Safety-Ethics-and-Society/Hendrycks/p/book/9781032869926?srsltid=AfmBOornG5wYvl7unW0XGlM2bjuOV97TAAkehylkWjbyE847srTgGOgs" rel="rel"&gt;textbook&lt;/a&gt;&lt;span&gt;, is open to participants from all disciplines and countries, and is designed to accommodate full-time work or study.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Applications for the Summer 2025 course are now open. The final application deadline is &lt;/span&gt;&lt;strong&gt;May 30th&lt;/strong&gt;&lt;span&gt;. Visit the &lt;/span&gt;&lt;a href="https://www.aisafetybook.com/virtual-course" rel="rel"&gt;course website&lt;/a&gt;&lt;span&gt; to learn more and apply.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;On May 12th, the Department of Commerce &lt;/span&gt;&lt;a href="https://media.bis.gov/sites/default/files/documents/05.07%20Recission%20of%20AI%20Diffusion%20Press%20Release.pdf" rel="rel"&gt;announced&lt;/a&gt;&lt;span&gt; that it had rescinded the &lt;/span&gt;&lt;a href="https://www.federalregister.gov/documents/2025/01/15/2025-00636/framework-for-artificial-intelligence-diffusion" rel="rel"&gt;Framework for Artificial Intelligence Diffusion&lt;/a&gt;&lt;span&gt;, which was set to take effect May 15th. The rule would have regulated the export of AI chips and models across three tiers of countries, each with its own set of restrictions. (Other AI chip export controls, including those prohibiting sales to China, remain on the books.)&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;The announcement states that the Bureau of Industry and Security (BIS) will issue a replacement rule in the future. In the meantime, the BIS will focus on working to prevent US chips from being used in Chinese AI development. Bloomberg &lt;/span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-05-07/trump-to-rescind-global-chip-curbs-amid-ai-restrictions-debate" rel="rel"&gt;reports&lt;/a&gt;&lt;span&gt; that new restrictions will focus on countries that have diverted US chips to China, including Thailand and Malaysia.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;The Trump Administration wants to capture the global AI chip market.&lt;/strong&gt;&lt;span&gt; Though China has yet to export its own AI chips, the BIS will also issue guidance that states using Huawei Ascend chips violates US export controls. This preemptive restriction supports the Trump Administration’s intent for the US to dominate the global AI chip market.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;UAE and Saudi Arabia are set to receive hundreds of thousands of AI chips&lt;/strong&gt;&lt;span&gt;. Last week, Trump announced trade deals with the &lt;/span&gt;&lt;a href="https://www.whitehouse.gov/fact-sheets/2025/05/fact-sheet-president-donald-j-trump-secures-200-billion-in-new-u-s-uae-deals-and-accelerates-previously-committed-1-4-trillion-uae-investment/" rel="rel"&gt;UAE&lt;/a&gt;&lt;span&gt; and &lt;/span&gt;&lt;a href="https://www.whitehouse.gov/fact-sheets/2025/05/fact-sheet-president-donald-j-trump-secures-historic-600-billion-investment-commitment-in-saudi-arabia/" rel="rel"&gt;Saudi Arabia&lt;/a&gt;&lt;span&gt;, respectively.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;The UAE is set to receive up to &lt;/span&gt;&lt;a href="https://www.reuters.com/business/finance/us-close-letting-uae-import-millions-nvidias-ai-chips-sources-say-2025-05-14/" rel="rel"&gt;500,000 of Nvidia's most advanced chips per year&lt;/a&gt;&lt;span&gt;, beginning in 2025. 100,000 of these would go to the Emirati firm G42, with the remainder going to U.S. companies building datacenters in the UAE. Following the deal’s &lt;/span&gt;&lt;a href="https://www.commerce.gov/news/press-releases/2025/05/uae/us-framework-advanced-technology-cooperation" rel="rel"&gt;announcement&lt;/a&gt;&lt;span&gt;, G42 announced the construction of a &lt;/span&gt;&lt;a href="https://www.commerce.gov/news/press-releases/2025/05/uae-and-us-presidents-attend-unveiling-phase-1-new-5gw-ai-campus-abu" rel="rel"&gt;five GW AI campus&lt;/a&gt;&lt;span&gt; in Abu Dhabi—the &lt;/span&gt;&lt;a href="https://x.com/ohlennart/status/1923091524688007474" rel="rel"&gt;largest AI infrastructure project anywhere in the world&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;&lt;div class="captioned-image-container"&gt;&lt;figure&gt;&lt;a class="image-link image2 is-viewable-img" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45cc31a2-d027-43bd-9f4f-2b26b23e051b_1600x1066.png" rel="rel" target="_blank"&gt;&lt;div class="image2-inset"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="alt" class="sizing-normal" height="970" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45cc31a2-d027-43bd-9f4f-2b26b23e051b_1600x1066.png" width="1456" /&gt;&lt;/div&gt;&lt;/a&gt;&lt;figcaption class="image-caption"&gt;&lt;span&gt;President Trump with the Emirati president, Sheikh Mohammed bin Zayed, at the AI campus’ unveiling. (&lt;/span&gt;&lt;a href="https://www.nytimes.com/2025/05/15/us/politics/ai-us-abu-dhabi.html" rel="rel"&gt;Source&lt;/a&gt;&lt;span&gt;.)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;p&gt;&lt;span&gt;Nvidia &lt;/span&gt;&lt;a href="https://nvidianews.nvidia.com/news/humain-and-nvidia-announce-strategic-partnership-to-build-ai-factories-of-the-future-in-saudi-arabia" rel="rel"&gt;announced&lt;/a&gt;&lt;span&gt; a strategic partnership with Saudi Arabia’s new sovereign AI company, &lt;/span&gt;&lt;a href="https://www.pif.gov.sa/en/news-and-insights/press-releases/2025/hrh-crown-prince-launches-humain-as-global-ai-powerhouse/" rel="rel"&gt;Humain&lt;/a&gt;&lt;span&gt;. In the first phase of the partnership, Humain is set to receive &lt;/span&gt;&lt;a href="https://www.nvidia.com/en-us/" rel="rel"&gt;18,000 Blackwell chips&lt;/a&gt;&lt;span&gt;. AMD &lt;/span&gt;&lt;a href="https://www.amd.com/en/newsroom/press-releases/2025-5-13-amd-and-humain-form-strategic--10b-collaboration-.html" rel="rel"&gt;also announced&lt;/a&gt;&lt;span&gt; a partnership with Humain.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;The chip sales affect several US priorities. &lt;/strong&gt;&lt;span&gt;The deals will direct large investments to US AI companies that might have otherwise gone to China (China is the leading source of revenue for both the &lt;/span&gt;&lt;a href="https://oec.world/en/profile/country/are" rel="rel"&gt;UAE&lt;/a&gt;&lt;span&gt; and &lt;/span&gt;&lt;a href="https://oec.world/en/profile/country/sau" rel="rel"&gt;Saudi Arabia)&lt;/a&gt;&lt;span&gt;. It will also allow US AI companies to circumvent compute capacity limitations imposed by the US’ energy grid.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Some US officials argue that the Trump Administration’s chip sales &lt;/span&gt;&lt;a href="https://www.nytimes.com/2025/05/15/business/economy/trump-chips-ai-uae.html" rel="rel"&gt;threaten to undermine the US’ lead in compute capacity&lt;/a&gt;&lt;span&gt;, and consequently US national security, since compute capacity may soon become &lt;/span&gt;&lt;a href="https://www.rand.org/pubs/commentary/2025/05/chinas-ai-models-are-closing-the-gap-but-americas-real.html" rel="rel"&gt;a key determinant of state power&lt;/a&gt;&lt;span&gt;. However, it’s difficult to evaluate the sales’ overall effects on US interests, since the terms of the agreement are unclear.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A federal AI whistleblower protection act&lt;/strong&gt;&lt;span&gt;. Senate Judiciary Committee Chair Chuck Grassley introduced the &lt;/span&gt;&lt;a href="https://www.judiciary.senate.gov/press/rep/releases/grassley-introduces-ai-whistleblower-protection-act" rel="rel"&gt;Artificial Intelligence Whistleblower Protection Act&lt;/a&gt;&lt;span&gt;, which would protect employees who come forward with information about harmful or illegal activities happening inside AI companies.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://law-ai.org/how-to-design-ai-whistleblower-legislation/" rel="rel"&gt;Current AI whistleblower protections aren’t effective.&lt;/a&gt;&lt;span&gt; Currently, these sorts of laws only exist as a patchwork across jurisdictions, making it difficult for would-be AI whistleblowers to predict whether they would be protected. They also often only protect reporting violations of law. Because AI regulation is minimal, developer behavior that poses a threat to public safety may not violate any law.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;AI companies can also require employees to sign NDAs preventing them from disparaging the company even after they leave. OpenAI &lt;/span&gt;&lt;a href="https://www.vox.com/future-perfect/351132/openai-vested-equity-nda-sam-altman-documents-employees" rel="rel"&gt;had employees sign such an NDA&lt;/a&gt;&lt;span&gt;, which they later discontinued after public pressure.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;The AI Whistleblower Protection Act &lt;/span&gt;&lt;a href="https://x.com/MackenZ_arnold/status/1923099536588767733" rel="rel"&gt;addresses these shortcomings&lt;/a&gt;&lt;span&gt;. It covers disclosing any “substantial and specific” danger that AI developer behavior might pose to public safety, public health, or national security. It also prohibits AI companies from requiring employees to sign NDAs or other contracts that undermine their ability to make such disclosures.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A bill requiring location verification for AI chips.&lt;/strong&gt;&lt;span&gt; Senator Tom Cotton introduced the &lt;/span&gt;&lt;a href="https://www.cotton.senate.gov/news/press-releases/cotton-introduces-bill-to-prevent-diversion-of-advanced-chips-to-americas-adversaries-and-protect-us-product-integrity" rel="rel"&gt;Chip Security Act&lt;/a&gt;&lt;span&gt;, which would require &lt;/span&gt;&lt;a href="https://www.ai-frontiers.org/articles/location-verification-ai-chips" rel="rel"&gt;location verification mechanisms&lt;/a&gt;&lt;span&gt; for export-controlled AI chips.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;The bill would strengthen US export controls by preventing AI chips from being smuggled into China. AI chip smuggling is a growing problem, with potentially &lt;/span&gt;&lt;a href="https://www.aipolicybulletin.org/articles/ai-chip-smuggling-is-the-default-not-the-exception" rel="rel"&gt;100,000 chips smuggled in 2024&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;Currently, US officials struggle to determine what happens to AI chips once they’re shipped overseas. Location verification would allow export authorities to tell when a shipment of chips isn’t where it’s supposed to be, triggering further investigation.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A provision in a tax bill prohibiting states from regulating AI.&lt;/strong&gt;&lt;span&gt; The House Energy and Commerce Committee included a provision that would &lt;/span&gt;&lt;a href="https://apnews.com/article/ai-regulation-state-moratorium-congress-39d1c8a0758ffe0242283bb82f66d51a" rel="rel"&gt;prohibit states from regulating AI&lt;/a&gt;&lt;span&gt; in its &lt;/span&gt;&lt;a href="https://docs.house.gov/meetings/IF/IF00/20250513/118261/HMKP-119-IF00-20250513-SD003.pdf" rel="rel"&gt;markup&lt;/a&gt;&lt;span&gt; of House Republicans’ &lt;/span&gt;&lt;a href="https://apnews.com/article/trump-big-beautiful-bill-medicaid-cuts-125ad670515460108ded062029abd8c8" rel="rel"&gt;tax bill&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Ever since &lt;/span&gt;&lt;a href="https://www.npr.org/2024/09/20/nx-s1-5119792/newsom-ai-bill-california-sb1047-tech" rel="rel"&gt;California’s SB 1047 almost became law&lt;/a&gt;&lt;span&gt;, AI companies have argued that states should be prohibited from regulating AI, and instead leave the problem to the federal government. SB 1047 would have made AI companies liable for harm caused by their models.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;However, the provision seems to run afoul of the Senate’s “&lt;/span&gt;&lt;a href="https://www.congress.gov/crs_external_products/RL/PDF/RL30862/RL30862.20.pdf" rel="rel"&gt;Byrd Rule&lt;/a&gt;&lt;span&gt;,” which prohibits policy provisions from being included in budget reconciliation bills.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Industry&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Civil Society&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;See also: &lt;/span&gt;&lt;a href="https://x.com/ai_risks?lang=en" rel="rel"&gt;CAIS’ X account&lt;/a&gt;&lt;span&gt;, our paper on &lt;/span&gt;&lt;a href="https://www.nationalsecurity.ai/" rel="rel"&gt;superintelligence strategy&lt;/a&gt;&lt;span&gt;, our &lt;/span&gt;&lt;a href="https://www.aisafetybook.com/" rel="rel"&gt;AI safety course&lt;/a&gt;&lt;span&gt;, and &lt;/span&gt;&lt;a href="http://ai-frontiers.org/" rel="rel"&gt;AI Frontiers&lt;/a&gt;&lt;span&gt;, a new platform for expert commentary and analysis.&lt;/span&gt;&lt;/p&gt;&lt;p class="button-wrapper"&gt;&lt;a class="button primary" href="https://newsletter.safe.ai/p/ai-safety-newsletter-55-trump-administration?utm_source=substack&amp;amp;utm_medium=email&amp;amp;utm_content=share&amp;amp;action=share" rel="rel"&gt;&lt;span&gt;Share&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;</description><content:encoded>&lt;p&gt;&lt;span&gt;Welcome to the AI Safety Newsletter by the &lt;/span&gt;&lt;a href="https://www.safe.ai/" rel="rel"&gt;Center for AI Safety&lt;/a&gt;&lt;span&gt;. We discuss developments in AI and AI safety. No technical background required.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;In this edition: The Trump Administration rescinds the Biden-era AI diffusion rule and sells AI chips to the UAE and Saudi Arabia; Federal lawmakers propose legislation on AI whistleblowers, location verification for AI chips, and prohibiting states from regulating AI.&lt;/p&gt;&lt;p&gt;&lt;span&gt;Listen to the AI Safety Newsletter for free on &lt;/span&gt;&lt;a href="https://spotify.link/E6lHa1ij2Cb" rel="rel"&gt;Spotify&lt;/a&gt;&lt;span&gt; or &lt;/span&gt;&lt;a href="https://podcasts.apple.com/us/podcast/ai-safety-newsletter/id1702875110" rel="rel"&gt;Apple Podcasts&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;The Center for AI Safety is also excited to announce the Summer session of our AI Safety, Ethics, and Society course, running from &lt;/span&gt;&lt;strong&gt;June 23 to September 14&lt;/strong&gt;&lt;span&gt;. The course, based on our recently published &lt;/span&gt;&lt;a href="https://www.routledge.com/Introduction-to-AI-Safety-Ethics-and-Society/Hendrycks/p/book/9781032869926?srsltid=AfmBOornG5wYvl7unW0XGlM2bjuOV97TAAkehylkWjbyE847srTgGOgs" rel="rel"&gt;textbook&lt;/a&gt;&lt;span&gt;, is open to participants from all disciplines and countries, and is designed to accommodate full-time work or study.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Applications for the Summer 2025 course are now open. The final application deadline is &lt;/span&gt;&lt;strong&gt;May 30th&lt;/strong&gt;&lt;span&gt;. Visit the &lt;/span&gt;&lt;a href="https://www.aisafetybook.com/virtual-course" rel="rel"&gt;course website&lt;/a&gt;&lt;span&gt; to learn more and apply.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;On May 12th, the Department of Commerce &lt;/span&gt;&lt;a href="https://media.bis.gov/sites/default/files/documents/05.07%20Recission%20of%20AI%20Diffusion%20Press%20Release.pdf" rel="rel"&gt;announced&lt;/a&gt;&lt;span&gt; that it had rescinded the &lt;/span&gt;&lt;a href="https://www.federalregister.gov/documents/2025/01/15/2025-00636/framework-for-artificial-intelligence-diffusion" rel="rel"&gt;Framework for Artificial Intelligence Diffusion&lt;/a&gt;&lt;span&gt;, which was set to take effect May 15th. The rule would have regulated the export of AI chips and models across three tiers of countries, each with its own set of restrictions. (Other AI chip export controls, including those prohibiting sales to China, remain on the books.)&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;The announcement states that the Bureau of Industry and Security (BIS) will issue a replacement rule in the future. In the meantime, the BIS will focus on working to prevent US chips from being used in Chinese AI development. Bloomberg &lt;/span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-05-07/trump-to-rescind-global-chip-curbs-amid-ai-restrictions-debate" rel="rel"&gt;reports&lt;/a&gt;&lt;span&gt; that new restrictions will focus on countries that have diverted US chips to China, including Thailand and Malaysia.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;The Trump Administration wants to capture the global AI chip market.&lt;/strong&gt;&lt;span&gt; Though China has yet to export its own AI chips, the BIS will also issue guidance that states using Huawei Ascend chips violates US export controls. This preemptive restriction supports the Trump Administration’s intent for the US to dominate the global AI chip market.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;UAE and Saudi Arabia are set to receive hundreds of thousands of AI chips&lt;/strong&gt;&lt;span&gt;. Last week, Trump announced trade deals with the &lt;/span&gt;&lt;a href="https://www.whitehouse.gov/fact-sheets/2025/05/fact-sheet-president-donald-j-trump-secures-200-billion-in-new-u-s-uae-deals-and-accelerates-previously-committed-1-4-trillion-uae-investment/" rel="rel"&gt;UAE&lt;/a&gt;&lt;span&gt; and &lt;/span&gt;&lt;a href="https://www.whitehouse.gov/fact-sheets/2025/05/fact-sheet-president-donald-j-trump-secures-historic-600-billion-investment-commitment-in-saudi-arabia/" rel="rel"&gt;Saudi Arabia&lt;/a&gt;&lt;span&gt;, respectively.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;The UAE is set to receive up to &lt;/span&gt;&lt;a href="https://www.reuters.com/business/finance/us-close-letting-uae-import-millions-nvidias-ai-chips-sources-say-2025-05-14/" rel="rel"&gt;500,000 of Nvidia's most advanced chips per year&lt;/a&gt;&lt;span&gt;, beginning in 2025. 100,000 of these would go to the Emirati firm G42, with the remainder going to U.S. companies building datacenters in the UAE. Following the deal’s &lt;/span&gt;&lt;a href="https://www.commerce.gov/news/press-releases/2025/05/uae/us-framework-advanced-technology-cooperation" rel="rel"&gt;announcement&lt;/a&gt;&lt;span&gt;, G42 announced the construction of a &lt;/span&gt;&lt;a href="https://www.commerce.gov/news/press-releases/2025/05/uae-and-us-presidents-attend-unveiling-phase-1-new-5gw-ai-campus-abu" rel="rel"&gt;five GW AI campus&lt;/a&gt;&lt;span&gt; in Abu Dhabi—the &lt;/span&gt;&lt;a href="https://x.com/ohlennart/status/1923091524688007474" rel="rel"&gt;largest AI infrastructure project anywhere in the world&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;&lt;div class="captioned-image-container"&gt;&lt;figure&gt;&lt;a class="image-link image2 is-viewable-img" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45cc31a2-d027-43bd-9f4f-2b26b23e051b_1600x1066.png" rel="rel" target="_blank"&gt;&lt;div class="image2-inset"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="alt" class="sizing-normal" height="970" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45cc31a2-d027-43bd-9f4f-2b26b23e051b_1600x1066.png" width="1456" /&gt;&lt;/div&gt;&lt;/a&gt;&lt;figcaption class="image-caption"&gt;&lt;span&gt;President Trump with the Emirati president, Sheikh Mohammed bin Zayed, at the AI campus’ unveiling. (&lt;/span&gt;&lt;a href="https://www.nytimes.com/2025/05/15/us/politics/ai-us-abu-dhabi.html" rel="rel"&gt;Source&lt;/a&gt;&lt;span&gt;.)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;p&gt;&lt;span&gt;Nvidia &lt;/span&gt;&lt;a href="https://nvidianews.nvidia.com/news/humain-and-nvidia-announce-strategic-partnership-to-build-ai-factories-of-the-future-in-saudi-arabia" rel="rel"&gt;announced&lt;/a&gt;&lt;span&gt; a strategic partnership with Saudi Arabia’s new sovereign AI company, &lt;/span&gt;&lt;a href="https://www.pif.gov.sa/en/news-and-insights/press-releases/2025/hrh-crown-prince-launches-humain-as-global-ai-powerhouse/" rel="rel"&gt;Humain&lt;/a&gt;&lt;span&gt;. In the first phase of the partnership, Humain is set to receive &lt;/span&gt;&lt;a href="https://www.nvidia.com/en-us/" rel="rel"&gt;18,000 Blackwell chips&lt;/a&gt;&lt;span&gt;. AMD &lt;/span&gt;&lt;a href="https://www.amd.com/en/newsroom/press-releases/2025-5-13-amd-and-humain-form-strategic--10b-collaboration-.html" rel="rel"&gt;also announced&lt;/a&gt;&lt;span&gt; a partnership with Humain.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;The chip sales affect several US priorities. &lt;/strong&gt;&lt;span&gt;The deals will direct large investments to US AI companies that might have otherwise gone to China (China is the leading source of revenue for both the &lt;/span&gt;&lt;a href="https://oec.world/en/profile/country/are" rel="rel"&gt;UAE&lt;/a&gt;&lt;span&gt; and &lt;/span&gt;&lt;a href="https://oec.world/en/profile/country/sau" rel="rel"&gt;Saudi Arabia)&lt;/a&gt;&lt;span&gt;. It will also allow US AI companies to circumvent compute capacity limitations imposed by the US’ energy grid.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Some US officials argue that the Trump Administration’s chip sales &lt;/span&gt;&lt;a href="https://www.nytimes.com/2025/05/15/business/economy/trump-chips-ai-uae.html" rel="rel"&gt;threaten to undermine the US’ lead in compute capacity&lt;/a&gt;&lt;span&gt;, and consequently US national security, since compute capacity may soon become &lt;/span&gt;&lt;a href="https://www.rand.org/pubs/commentary/2025/05/chinas-ai-models-are-closing-the-gap-but-americas-real.html" rel="rel"&gt;a key determinant of state power&lt;/a&gt;&lt;span&gt;. However, it’s difficult to evaluate the sales’ overall effects on US interests, since the terms of the agreement are unclear.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A federal AI whistleblower protection act&lt;/strong&gt;&lt;span&gt;. Senate Judiciary Committee Chair Chuck Grassley introduced the &lt;/span&gt;&lt;a href="https://www.judiciary.senate.gov/press/rep/releases/grassley-introduces-ai-whistleblower-protection-act" rel="rel"&gt;Artificial Intelligence Whistleblower Protection Act&lt;/a&gt;&lt;span&gt;, which would protect employees who come forward with information about harmful or illegal activities happening inside AI companies.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://law-ai.org/how-to-design-ai-whistleblower-legislation/" rel="rel"&gt;Current AI whistleblower protections aren’t effective.&lt;/a&gt;&lt;span&gt; Currently, these sorts of laws only exist as a patchwork across jurisdictions, making it difficult for would-be AI whistleblowers to predict whether they would be protected. They also often only protect reporting violations of law. Because AI regulation is minimal, developer behavior that poses a threat to public safety may not violate any law.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;AI companies can also require employees to sign NDAs preventing them from disparaging the company even after they leave. OpenAI &lt;/span&gt;&lt;a href="https://www.vox.com/future-perfect/351132/openai-vested-equity-nda-sam-altman-documents-employees" rel="rel"&gt;had employees sign such an NDA&lt;/a&gt;&lt;span&gt;, which they later discontinued after public pressure.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;The AI Whistleblower Protection Act &lt;/span&gt;&lt;a href="https://x.com/MackenZ_arnold/status/1923099536588767733" rel="rel"&gt;addresses these shortcomings&lt;/a&gt;&lt;span&gt;. It covers disclosing any “substantial and specific” danger that AI developer behavior might pose to public safety, public health, or national security. It also prohibits AI companies from requiring employees to sign NDAs or other contracts that undermine their ability to make such disclosures.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A bill requiring location verification for AI chips.&lt;/strong&gt;&lt;span&gt; Senator Tom Cotton introduced the &lt;/span&gt;&lt;a href="https://www.cotton.senate.gov/news/press-releases/cotton-introduces-bill-to-prevent-diversion-of-advanced-chips-to-americas-adversaries-and-protect-us-product-integrity" rel="rel"&gt;Chip Security Act&lt;/a&gt;&lt;span&gt;, which would require &lt;/span&gt;&lt;a href="https://www.ai-frontiers.org/articles/location-verification-ai-chips" rel="rel"&gt;location verification mechanisms&lt;/a&gt;&lt;span&gt; for export-controlled AI chips.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;The bill would strengthen US export controls by preventing AI chips from being smuggled into China. AI chip smuggling is a growing problem, with potentially &lt;/span&gt;&lt;a href="https://www.aipolicybulletin.org/articles/ai-chip-smuggling-is-the-default-not-the-exception" rel="rel"&gt;100,000 chips smuggled in 2024&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;Currently, US officials struggle to determine what happens to AI chips once they’re shipped overseas. Location verification would allow export authorities to tell when a shipment of chips isn’t where it’s supposed to be, triggering further investigation.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A provision in a tax bill prohibiting states from regulating AI.&lt;/strong&gt;&lt;span&gt; The House Energy and Commerce Committee included a provision that would &lt;/span&gt;&lt;a href="https://apnews.com/article/ai-regulation-state-moratorium-congress-39d1c8a0758ffe0242283bb82f66d51a" rel="rel"&gt;prohibit states from regulating AI&lt;/a&gt;&lt;span&gt; in its &lt;/span&gt;&lt;a href="https://docs.house.gov/meetings/IF/IF00/20250513/118261/HMKP-119-IF00-20250513-SD003.pdf" rel="rel"&gt;markup&lt;/a&gt;&lt;span&gt; of House Republicans’ &lt;/span&gt;&lt;a href="https://apnews.com/article/trump-big-beautiful-bill-medicaid-cuts-125ad670515460108ded062029abd8c8" rel="rel"&gt;tax bill&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Ever since &lt;/span&gt;&lt;a href="https://www.npr.org/2024/09/20/nx-s1-5119792/newsom-ai-bill-california-sb1047-tech" rel="rel"&gt;California’s SB 1047 almost became law&lt;/a&gt;&lt;span&gt;, AI companies have argued that states should be prohibited from regulating AI, and instead leave the problem to the federal government. SB 1047 would have made AI companies liable for harm caused by their models.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;However, the provision seems to run afoul of the Senate’s “&lt;/span&gt;&lt;a href="https://www.congress.gov/crs_external_products/RL/PDF/RL30862/RL30862.20.pdf" rel="rel"&gt;Byrd Rule&lt;/a&gt;&lt;span&gt;,” which prohibits policy provisions from being included in budget reconciliation bills.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Industry&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Civil Society&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;See also: &lt;/span&gt;&lt;a href="https://x.com/ai_risks?lang=en" rel="rel"&gt;CAIS’ X account&lt;/a&gt;&lt;span&gt;, our paper on &lt;/span&gt;&lt;a href="https://www.nationalsecurity.ai/" rel="rel"&gt;superintelligence strategy&lt;/a&gt;&lt;span&gt;, our &lt;/span&gt;&lt;a href="https://www.aisafetybook.com/" rel="rel"&gt;AI safety course&lt;/a&gt;&lt;span&gt;, and &lt;/span&gt;&lt;a href="http://ai-frontiers.org/" rel="rel"&gt;AI Frontiers&lt;/a&gt;&lt;span&gt;, a new platform for expert commentary and analysis.&lt;/span&gt;&lt;/p&gt;&lt;p class="button-wrapper"&gt;&lt;a class="button primary" href="https://newsletter.safe.ai/p/ai-safety-newsletter-55-trump-administration?utm_source=substack&amp;amp;utm_medium=email&amp;amp;utm_content=share&amp;amp;action=share" rel="rel"&gt;&lt;span&gt;Share&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://newsletter.safe.ai/p/ai-safety-newsletter-55-trump-administration</guid><pubDate>Tue, 20 May 2025 14:43:03 +0000</pubDate></item><item><title>Collaborators: Healthcare Innovation to Impact (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/podcast/collaborators-healthcare-innovation-to-impact/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Microsoft Research Podcast | Collaborators: Healthcare Innovation to Impact | outline illustrations of Jonathan Carlson, Smitha Saligrama, Will Guyman, Cameron Runde, Dr. Matthew Lungren" class="wp-image-1139392" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/HealthResearch_Collaborators_Hero_Feature_No_Text_1400x788.jpg" width="1400" /&gt;&lt;/figure&gt;






&lt;p&gt;&lt;em&gt;This episode contains forward-looking statements, including predictions and expectations about future developments in AI and healthcare. These statements are based on current assumptions and projections and are subject to risks and uncertainties. Actual outcomes may differ materially.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Transforming research ideas into meaningful impact is no small feat. It often requires the knowledge and experience of individuals from across disciplines and institutions. &lt;em&gt;Collaborators&lt;/em&gt;, a new Microsoft Research Podcast series, explores the relationships—both expected and unexpected—behind the projects, products, and services being pursued and delivered by researchers at Microsoft and the diverse range of people they’re teaming up with.&amp;nbsp;&lt;br /&gt;&lt;/p&gt;



&lt;p&gt;Amid the ongoing surge of AI research, healthcare is emerging as a leading area for real-world transformation. From driving efficiency gains for clinicians to improving patient outcomes, AI is beginning to make a tangible impact. Thousands of scientific papers have explored AI systems capable of analyzing medical documents and images with unprecedented accuracy. The latest work goes even further, showing how healthcare agents can collaborate—with each other and with human doctors—embedding AI directly into clinical workflows.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In this discussion, we explore how teams across Microsoft are working together to generate advanced AI capabilities and solutions for developers and clinicians around the globe. Leading the conversation are Dr. Matthew Lungren, chief scientific officer for Microsoft Health and Life Sciences, and Jonathan Carlson, vice president and managing director of Microsoft Health Futures—two key leaders behind this collaboration. They’re joined by Smitha Saligrama, principal group engineering manager within Microsoft Health and Life Sciences, Will Guyman, group product manager within Microsoft Health and Life Sciences, and Cameron Runde, a senior strategy manager for Microsoft Health Futures—all of whom play crucial roles in turning AI breakthroughs into practical, life-saving innovations.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Together, these experts examine how Microsoft is helping integrate cutting-edge AI into healthcare workflows—saving time today, and lives tomorrow.&lt;/p&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h2 class="wp-block-heading h5" id="learn-more"&gt;Learn more&lt;/h2&gt;



&lt;p&gt;Developing next-generation cancer care management with multi-agent orchestration&lt;br /&gt;Industry Blog, May 2025&lt;/p&gt;



&lt;p&gt;Healthcare Agent Orchestrator&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;br /&gt;GitHub &lt;/p&gt;



&lt;p&gt;Azure AI Foundry Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;br /&gt;Homepage&lt;/p&gt;







&lt;section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast"&gt;
	
&lt;/section&gt;


&lt;div class="wp-block-msr-show-more"&gt;
	&lt;div class="bg-neutral-100 p-5"&gt;
		&lt;div class="show-more-show-less"&gt;
			&lt;div&gt;
				&lt;span&gt;
					

&lt;h2 class="wp-block-heading" id="transcript"&gt;Transcript&lt;/h2&gt;



&lt;p&gt;[MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MATTHEW LUNGREN: &lt;/strong&gt;You’re listening to Collaborators, a Microsoft Research podcast, showcasing the range of expertise that goes into transforming mind blowing ideas into&amp;nbsp; world changing technologies. Despite the advancements in AI over the decades, generative AI exploded into view in 2022, when ChatGPT became the, sort of, internet browser for AI and became the fastest adopted consumer software application in history.&lt;/p&gt;



				&lt;/span&gt;
				&lt;span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1"&gt;
					



&lt;p&gt;&lt;strong&gt;JONATHAN CARLSON: &lt;/strong&gt;From the beginning, healthcare stood out to us as an important opportunity for general reasoners to improve the lives and experiences of patients and providers. Indeed, in the past two years, there’s been an explosion of scientific papers looking at the application first of text reasoners and medicine, then multi-modal reasoners that can interpret medical images, and now, most recently, healthcare agents that can reason with each other. But even more impressive than the pace of research has been the surprisingly rapid diffusion of this technology into real world clinical workflows.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN:&lt;/strong&gt; So today, we’ll talk about how our cross-company collaboration has shortened that gap and delivered advanced AI capabilities and solutions into the hands of developers and clinicians around the world, empowering everyone in health and life sciences to achieve more. I’m Doctor Matt Lungren, chief scientific officer for Microsoft Health and Life Sciences.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON: &lt;/strong&gt;And I’m Jonathan Carlson, vice president and managing director of Microsoft Health Futures.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN:&lt;/strong&gt; And together we brought some key players leading in the space of AI and health&lt;s&gt; &lt;/s&gt;care from across Microsoft. Our guests today are Smitha Saligrama, principal group engineering manager within Microsoft Health and Life Sciences, Will Guyman, group product manager within Microsoft Health and Life Sciences, and Cameron Runde, a senior strategy manager for Microsoft Health Futures.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON:&lt;/strong&gt; We’ve asked these brilliant folks to join us because each of them represents a mission critical group of cutting-edge stakeholders, scaling breakthroughs into purpose-built solutions and capabilities for health&lt;s&gt; &lt;/s&gt;care.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;We’ll hear today how generative AI capabilities can unlock reasoning across every data type in medicine: text, images, waveforms, genomics. And further, how multi-agent frameworks in healthcare can accelerate complex workflows, in some cases acting as a specialist team member, safely secured inside the Microsoft 365 tools used by hundreds of millions of healthcare enterprise users across the world. The opportunity to save time today and lives tomorrow with AI has never been larger.&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&amp;nbsp;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;&lt;strong&gt;MATTHEW LUNGREN: &lt;/strong&gt;Jonathan. You know, it’s been really interesting kind of observing Microsoft Research over the decades. I’ve, you know, been watching you guys in my prior academic career. You are always on the front of innovation, particularly in health&lt;s&gt; &lt;/s&gt;care. And I find it fascinating that, you know, millions of people are using the solutions that, you know, your team has developed over the years and yet you still find ways to stay cutting edge and state of the art, even in this accelerating time of technology and AI, particularly, how do you do that? [LAUGHS]&lt;/p&gt;



&lt;p&gt;&amp;nbsp;&lt;strong&gt;JONATHAN CARLSON: &lt;/strong&gt;I mean, it’s some of what’s in our DNA, I mean, we’ve been publishing in health and life sciences for two decades here. But when we launched Health Futures as a mission-focused lab about 7 or 8 years ago, we really started with the premise that the way to have impact was to really close the loop between, not just good ideas that get published, but good ideas that can actually be grounded in real problems that clinicians and scientists care about, that then allow us to actually go from that first proof of concept into an incubation, into getting real world feedback that allows us to close that loop. And now with, you know, the HLS organization here as a product group, we have the opportunity to work really closely with you all to not just prove what’s possible in the clinic or in the lab, but actually start scaling that into the broader community.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CAMERON RUNDE: &lt;/strong&gt;And one thing I’ll add here is that the problems that we’re trying to tackle in health&lt;s&gt; &lt;/s&gt;care are extremely complex. And so, as Jonathan said, it’s really important that we come together and collaborate across disciplines as well as across the company of Microsoft and with our external collaborators, as well across the whole industry.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON: &lt;/strong&gt;So, Matt, back to you. What are you guys doing in the product group? How do you guys see these models getting into the clinic?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN:&lt;/strong&gt; You know, I think a lot of people, you know, think about AI is just, you know, maybe just even a few years old because of GPT and how that really captured the public’s consciousness. Right? &lt;/p&gt;



&lt;p&gt;And so, you think about the speech-to-text technology of being able to dictate something, for a clinic note or for a visit, that was typically based on Nuance technology. And so there’s a lot of product understanding of the market, how to deliver something that clinicians will use, understanding the pain points and workflows and really that Health IT space, which is sometimes the third rail, I feel like with a lot of innovation in healthcare.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But beyond that, I mean, I think now that we have this really powerful engine of Microsoft and the platform capabilities, we’re seeing, innovations on the healthcare side for data storage, data interoperability, with different types of medical data. You have new applications coming online, the ability, of course, to see generative AI now infused into the speech-to-text and, becoming Dragon Copilot, which is something that has been, you know, tremendously, received by the community.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Physicians are able to now just have a conversation with a patient. They turn to their computer and the note is ready for them. There’s no more this, we call it keyboard liberation. I don’t know if you heard that before. And that’s just been tremendous. And there’s so much more coming from that side. And then there’s other parts of the workflow that we also get engaged in — the diagnostic workflow.&lt;/p&gt;



&lt;p&gt;So medical imaging, sharing images across different hospital systems, the list goes on. And so now when you move into AI, we feel like there’s a huge opportunity to deliver capabilities into the clinical workflow via the products and solutions we already have. But, I mean, we’ll now that we’ve kind of expanded our team to involve Azure and platform, we’re really able to now focus on the developers.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WILL GUYMAN: &lt;/strong&gt;Yeah. And you’re always telling me as a doctor how frustrating it is to be spending time at the computer instead of with your patients. I think you told me, you know, 4,000 clicks a day for the typical doctor, which is tremendous. And something like Dragon Copilot can save that five minutes per patient. But it can also now take actions after the patient encounter so it can draft the after-visit summary.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It can order labs and medications for the referral. And that’s incredible. And we want to keep building on that. There’s so many other use cases across the ecosystem. And so that’s why in Azure AI Foundry, we have translated a lot of the research from Microsoft Research and made that available to developers to build and customize for their own applications.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SMITHA SALIGRAMA: &lt;/strong&gt;Yeah. And as you were saying, in our transformation of moving from solutions to platforms and as, scaling solutions to other, multiple scenarios, as we put our models in AI Foundry, we provide these developer capabilities like bring your own data and fine&lt;s&gt; &lt;/s&gt;tune these models and then apply it to, scenarios that we couldn’t even imagine. So that’s kind of the platform play we’re scaling now.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;Well, I want to do a reality check because, you know, I think to us that are now really focused on technology, it seems like, I’ve heard this story before, right. I, I remember even in, my academic clinical days where it felt like technology was always the quick answer and it felt like technology was, there was maybe a disconnect between what my problems were or what I think needed to be done versus kind of the solutions that were kind of, created or offered to us. And I guess at some level, how Jonathan, do you think about this? Because to do things well in the science space is one thing, to do things well in science, but then also have it be something that actually drives health&lt;s&gt; &lt;/s&gt;care innovation and practice and translation. It’s tricky, right?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON:&lt;/strong&gt; Yeah. I mean, as you said, I think one of the core pathologies of Big Tech is we assume every problem is a technology problem. And that’s all it will take to solve the problem. And I think, look, I was trained as a computational biologist, and that sits in the awkward middle between biology and computation. And the thing that we always have to remember, the thing that we were very acutely aware of when we set out, was that we are not the experts. We do have, you know, you as an M.D., we have everybody on the team, we have biologists on the team.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But this is a big space. And the only way we’re going to have real impact, the only way we’re even going to pick the right problems to work on is if we really partner deeply, with providers, with EHR (electronic health records) vendors, with scientists, and really understand what’s important and again, get that feedback loop.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;RUNDE: &lt;/strong&gt;Yeah, I think we really need to ground the work that we do in the science itself. You need to understand the broader ecosystem and the broader landscape, across health&lt;s&gt; &lt;/s&gt;care and life sciences, so that we can tackle the most important problems, not just the problems that &lt;em&gt;we &lt;/em&gt;think are important. Because, as Jonathan said, we’re not the experts in health&lt;s&gt; &lt;/s&gt;care and life sciences. And that’s really the secret sauce. When you have the clinical expertise come together with the technical expertise. That’s how you really accelerate health&lt;s&gt; &lt;/s&gt;care.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON:&lt;/strong&gt; When we really launched this, this mission, 7 or 8 years ago, we really came in with the premise of, if we decide to stop, we want to be sure the world cares. And the only way that’s going to be true is if we’re really deeply embedded with the people that matter–the patients, the providers and the scientists.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN:&lt;/strong&gt; And now it really feels like this collaborative effort, you know, really can help start to extend that mission. Right. I think, you know, Will and Smitha, that we definitely feel the passion and the innovation. And we certainly benefit from those collaborations, too. But then we have these other partners and even customers, right, that we can start to tap into and have that flywheel keep spinning.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;GUYMAN: &lt;/strong&gt;Yeah. And the whole industry is an ecosystem. So, we have our own data sets at Microsoft Research that you’ve trained amazing AI models with. And those are in the catalog. But then you’ve also partnered with institutions like Providence or Page AI . And those models are in the catalog with their data. And then there are third parties like Nvidia that have their own specialized proprietary data sets, and their models are there too. So, we have this ecosystem of open source models. And maybe Smitha, you want to talk about how developers can actually customize these.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SALIGRAMA: &lt;/strong&gt;Yeah. So we use the Azure AI Foundry ecosystem. Developers can feel at home if they’re using the AI Foundry. So they can look at our model cards that we publish as part of the models we publish, understand the use cases of these models, how to, quickly, bring up these APIs and, look at different use cases of how to apply these and even fine&lt;s&gt; &lt;/s&gt;tune these models with their own data. Right. And then, use it for specific tasks that we couldn’t have even imagined.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;Yeah it has been interesting to see we have these health&lt;s&gt; &lt;/s&gt;care models in the catalog again, some that came from research, some that came from third parties and other product developers and Azure’s kind of becoming the home base, I think, for a lot of health and life science developers. They’re seeing all the different modalities, all the different capabilities. And then in combination with Azure OpenAI, which as we know, is incredibly competent in lots of different use cases. How are you looking at the use cases, and what are you seeing folks use these models for as they come to the catalog and start sharing their discoveries or products?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;GUYMAN: &lt;/strong&gt;Well, the general-purpose large language models are amazing for medical general reasoning. So Microsoft Research has shown that that they can perform super well on, for example, like the United States medical licensing exam, they can exceed doctor performance if they’re just picking between different multiple-choice questions. But real medicine we know is messier. It doesn’t always start with the whole patient context provided as text in the prompt. You have to get the source data and that raw data is often non-text. The majority of it is non-text. It’s things like medical imaging, radiology, pathology, ophthalmology, dermatology. It goes on and on. And there’s endless signal data, lab data. And so all of this diverse data type needs to be processed through specialized models because much of that data is not available on the public internet.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And that’s why we’re taking this partner approach, first party and third party models that can interpret all this kind of data and then connect them ultimately back to these general reasoners to reason over that.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;So, you know, I’ve been at this company for a while and, you know, familiar with kind of how long it takes, generally to get, you know, a really good research paper, do all the studies, do all the data analysis, and then go through the process of publishing, right, which takes, as, you know, a long time and it’s, you know, very rigorous.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And one of the things that struck me, last year, I think we, we started this big collaboration and, within a quarter, you had a Nature paper coming out from Microsoft Research, and that model that the Nature paper was describing was ready to be used by anyone on the Azure AI Foundry within that same quarter. It kind of blew my mind when I thought about it, you know, even though we were all, you know, working very hard to get that done. Any thoughts on that? I mean, has this ever happened in your career? And, you know, what’s the secret sauce to that?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON: &lt;/strong&gt;Yeah, I mean, the time scale from research to product has been massively compressed. And I’d push that even further, which is to say, the reason why it took a quarter was because we were laying the railroad tracks as we’re driving the train. We have examples right after that when we are launching on Foundry the same day we were publishing the paper.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And frankly, the review times are becoming longer than it takes to actually productize the models. I think there’s two things that are going on with that are really converging. One is that the overall ecosystem is converging on a relatively small number of patterns, and that gives us, as a tech company, a reason to go off and really make those patterns hardened in a way that allows not just us, but third parties as well, to really have a nice workflow to publish these models.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But the other is actually, I think, a change in how we work, you know, and for most of our history as an industrial research lab, we would do research and then we’d go pitch it to somebody and try and throw it over the fence. We’ve really built a much more integrated team. In fact, if you look at that Nature paper or any of the other papers, there’s folks from product teams. Many of you are on the papers along with our clinical collaborators. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;RUNDE:&lt;/strong&gt; Yeah. I think one thing that’s really important to note is that there’s a ton of different ways that you can have impact, right? So I like to think about phasing. In Health Futures at least, I like to think about phasing the work that we do. So first we have research, which is really early innovation. And the impact there is getting our technology and our tools out there and really sharing the learnings that we’ve had.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So that can be through publications like you mentioned. It can be through open-sourcing our models. And then you go to incubation. So, this is, I think, one of the more new spaces that we’re getting into, which is maybe that blurred line between research and product. Right. Which is, how do we take the tools and technologies that we’ve built and get them into the hands of users, typically through our partnerships?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Right. So, we partner very deeply and collaborate very deeply across the industry. And incubation is really important because we get that early feedback. We get an ability to pivot if we need to. And we also get the ability to see what types of impact our technology is having in the real world. And then lastly, when you think about scale, there’s tons of different ways that you can scale. We can scale third-party through our collaborators and really empower them to go to market to commercialize the things that we’ve built together.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;You can also think about scaling internally, which is why I’m so thankful that we’ve created this flywheel between research and product, and a lot of the models that we’ve built that have gone through research, have gone through incubation, have been able to scale on the Azure AI Foundry. But that’s not really our expertise. Right? The scale piece in research, that’s research and incubation. Smitha, how do you think about scaling?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SALIGRAMA:&lt;/strong&gt; So, there are several angles to scaling the models, the state-of-the-art models we see from the research team. The first angle is, the open sourcing, to get developer trust, and very generous commercial licenses so that they can use it and for their own, use cases.&amp;nbsp;The second is, we also allow them to customize these models, fine&lt;s&gt; &lt;/s&gt;tuning these models with their own data. So a lot of different angles of how we provide support and scaling, the state-of-the-art of models we get from the research org.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;GUYMAN:&lt;/strong&gt; And as one example, you know, University of Wisconsin Health, you know, which Matt knows well. They took one of our models, which is highly versatile. They customized it in Foundry and they optimized it to reliably identify abnormal chest X-rays, the most common imaging procedure, so they could improve their turnaround time triage quickly. And that’s just one example. But we have other partners like Sectra who are doing more of operations use cases automatically routing imaging to the radiologists, setting them up to be efficient. And then Page AI is doing, you know, biomarker identification for actually diagnostics and new drug discovery. So, there’s so many use cases that we have partners already who are building and customizing.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN:&lt;/strong&gt; The part that’s striking to me is just that, you know, we could all sit in a room and think about all the different ways someone might use these models on the catalog. And I’m still shocked at the stuff that people use them for and how effective they are. And I think part of that is, you know, again, we talk a lot about generative AI and healthcare and all the things you can do. Again, you know, in text, you refer to that earlier and certainly off the shelf, there’s really powerful applications. But there is, you know, kind of this tip of the iceberg effect where under the water, most of the data that we use to take care of our patients is not text. Right. It’s all the different other modalities. And I think that this has been an unlock right, sort of taking these innovations, innovations from the community, putting them in this ecosystem kind of catalog, essentially. Right. And then allowing folks to kind of, you know, build and develop applications with all these different types of data. Again, I’ve been surprised at what I’m seeing.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON:&lt;/strong&gt; This has been just one of the most profound shifts that’s happened in the last 12 months, really. I mean, two years ago we had general models in text that really shifted how we think about, I mean, natural language processing got totally upended by that. Turns out the same technology works for images as well. It doesn’t only allow you to automatically extract concepts from images, but allows you to align those image concepts with text concepts, which means that you can have a conversation with that image. And once you’re in that world now, you are a place where you can start stitching together these multimodal models that really change how you can interact with the data, and how you can start getting more information out of the raw primary data that is part of the patient journey.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;Well, and we’re going to get to that because I think you just touched on something. And I want to re-emphasize stitching these things together. There’s a lot of different ways to potentially do that. Right? There’s ways that you can literally train the model end to end with adapters and all kinds of other early fusion fusions. All kinds of ways. But one of the things that the word of the I guess the year is going to be agents and an agent is a very interesting term to think about how you might abstract away some of the components or the tasks that you want the model to, to accomplish in the midst of sort of a real human to maybe model interaction. Can you talk a little bit more about, how we’re thinking about agents in this, in this platform approach?&amp;nbsp;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;&lt;strong&gt;GUYMAN: &lt;/strong&gt;Well, this is our newest addition to the Azure AI Foundry. So there’s an agent catalog now where we have a set of pre-configured agents for health care. And then we also have a multi-agent orchestrator that can jump&lt;s&gt; &lt;/s&gt;start the process of developers building their own multi-agent workflows to tackle some complex real-world tasks that clinicians have to deal with. And these agents basically combine a general reasoner, like a large language model, like a GPT 4o or an o series model with a specialized model, like a model that understands radiology or pathology with domain-specific knowledge and tools. So the knowledge might be, you know, public guidelines or, you know, medical journals or your own private data from your EHR or medical imaging system, and then tools like a code interpreter to deal with all of the numeric data or tools like that that the clinicians are using today, like PowerPoint, Word, Teams and etc. And so we’re allowing developers to build and customize each of these agents in Foundry and then deploy them into their workflows.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;And, and I really like that concept because, you know, as, as a, as a from the user personas, I think about myself as a user. How am I going to interact with these agents? Where does it naturally fit? And I and I sort of, you know, I’ve seen some of the demonstrations and some of the work that’s going on with Stanford in particular, showing that, you know, and literally in a Teams chat, I can have my clinician colleagues and I can have specialized health&lt;s&gt; &lt;/s&gt;care agents that kind of interact, like I’m interacting with a human on a chat.&lt;/p&gt;



&lt;p&gt;It is a completely mind-blowing thing for me. And it’s a light bulb moment for me to I wonder, what have we, what have we heard from folks that have, you know, tried out this health care agent orchestrator in this kind of deployment environment via Teams?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;GUYMAN: &lt;/strong&gt;Well, someone joked, you know, are you sure you’re not using Teams because you work at Microsoft? [LAUGHS] But, then we actually were meeting with one of the, radiologists at one of our partners, and they said that that morning they had just done a Teams meeting, or they had met with other specialists to talk about a patient’s cancer case, or they were coming up with a treatment plan.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And that was the light bulb moment for us. We realized, actually, Teams is already being used by physicians as an internal communication tool, as a tool to get work done. And especially since the pandemic, a lot of the meetings moved to virtual and telemedicine. And so it’s a great distribution channel for AI, which is often been a struggle for AI to actually get in the hands of clinicians. And so now we’re allowing developers to build and then deploy very easily and extend it into their own workflows.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON: &lt;/strong&gt;I think that’s such an important point. I mean, if you think about one of the really important concepts in computer science is an application programing interface, like some set of rules that allow two applications to talk to each other. One of the big pushes, really important pushes, in medicine has been standards that allow us to actually have data standards and APIs that allow these to talk to each other, and yet still we end up with these silos. There’s silos of data. There’s silos of applications.&lt;/p&gt;



&lt;p&gt;And just like when you and I work on our phone, we have to go back and forth between applications. One of the things that I think agents do is that it takes the idea that now you can use language to understand intent and effectively program an interface, and it creates a whole new abstraction layer that allows us to simplify the interaction between not just humans and the endpoint, but also for developers.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It allows us to have this abstraction layer that lets different developers focus on different types of models, and yet stitch them all together in a very, very natural, way, not just for the users, but for the ability to actually deploy those models.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SALIGRAMA: &lt;/strong&gt;Just to add to what Jonathan was mentioning, the other cool thing about the Microsoft Teams user interface is it’s also enterprise ready.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;RUNDE: &lt;/strong&gt;And one important thing that we’re thinking about, is exactly this from the very early research through incubation and then to scale, obviously. Right. And so early on in research, we are actively working with our partners and our collaborators to make sure that we have the right data privacy and consent in place. We’re doing this in incubation as well. And then obviously in scale. Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;So, I think AI has always been thought of as a savior kind of technology. We talked a little bit about how there’s been some ups and downs in terms of the ability for technology to be effective in health care. At the same time, we’re seeing a lot of new innovations that are really making a difference. But then we kind of get, you know, we talked about agents a little bit. It feels like we’re maybe abstracting too far. Maybe it’s things are going too fast, almost. What makes this different? I mean, in your mind is this truly a logical next step or is it going to take some time?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON:&lt;/strong&gt; I think there’s a couple things that have happened. I think first, on just a pure technology. What led to ChatGPT? And I like to think of really three major breakthroughs.&lt;/p&gt;



&lt;p&gt;The first was new mathematical concepts of attention, which really means that we now have a way that a machine can figure out which parts of the context it should actually focus on, just the way our brains do. Right? I mean, if you’re a clinician and somebody is talking to you, the majority of that conversation is not relevant for the diagnosis. But, you know how to zoom in on the parts that matter. That’s a super powerful mathematical concept. The second one is this idea of self-supervision. So, I think one of the fundamental problems of machine learning has been that you have to train on labeled training data and labels are expensive, which means data sets are small, which means the final models are very narrow and brittle. And the idea of self-supervision is that you can just get a model to automatically learn concepts, and the language is just predict the next word. And what’s important about that is that leads to models that can actually manipulate and understand really messy text and pull out what’s important about that, and then and then stitch that back together in interesting ways.&lt;/p&gt;



&lt;p&gt;And the third concept, that came out of those first two, was just the observational scale. And that’s that more is better, more data, more compute, bigger models. And that really leads to a reason to keep investing. And for these models to keep getting better. So that as a as a groundwork, that’s what led to ChatGPT. That’s what led to our ability now to not just have rule-based systems or simple machine learning based systems to take a messy EHR record, say, and pull out a couple concepts.&lt;/p&gt;



&lt;p&gt;But to really feed the whole thing in and say, okay, I need you to figure out which concepts are in here. And is this particular attribute there, for example. That’s now led to the next breakthrough, which is all those core ideas apply to images as well. They apply to proteins, to DNA. And so we’re starting to see models that understand images and the concepts of images, and can actually map those back to text as well.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So, you can look at a pathology image and say, not just at the cell, but it appears that there’s some certain sort of cancer in this particular, tissue there. And then you take those two things together and you layer on the fact that now you have a model, or a set of models, that can understand intent, can understand human concepts and biomedical concepts, and you can start stitching them together into specialized agents that can actually reason with each other, which at some level gives you an API as a developer to say, okay, I need to focus on a pathology model and get this really, really, sound while somebody else is focusing on a radiology model, but now allows us to stitch these all together with a user interface that we can now talk to through natural language.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;RUNDE: &lt;/strong&gt;I’d like to double click a little bit on that medical abstraction piece that you mentioned. Just the amount of data, clinical data that there is for each individual patient. Let’s think about cancer patients for a second to make this real. Right. For every cancer patient, it could take a couple of hours to structure their information. And why is that important? Because, you have to get that information in a structured way and abstract relevant information to be able to unlock precision health applications right, for each patient. So, to be able to match them to a trial, right, someone has to sit there and go through all of the clinical notes from their entire patient care journey, from the beginning to the end. And that’s not scalable. And so one thing that we’ve been doing in an active project that we’ve been working on with a handful of our partners, but Providence specifically, I’ll call out, is using AI to actually abstract and curate that information. So that gives time back to the health care provider to spend with patients, instead of spending all their time curating this information.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And this is super important because it sets the scene and the backbone for all those precision health applications. Like I mentioned, clinical trial matching, tumor boards is another really important example here. Maybe Matt, you can talk to that a little bit.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;It’s a great example. And you know it’s so funny. We’ve talked about this use case and the you know the health&lt;s&gt; &lt;/s&gt;care agent orchestrator is sort of the at the initial lighthouse use case was a tumor board setting. And I remember when we first started working with some of the partners on this, I think we were you know, under a research kind of lens, thinking about what could this, what new diagnoses could have come up with or what new insights might have and what was really a really key moment for us, I think, was noticing that we had developed an agent that can take all of the multimodal data about a patient’s chart, organize it in a timeline, in chronological fashion, and then allow folks to click on different parts of the timeline to ground it back to the note. And just that, which doesn’t sound like a really interesting research paper. It was mind blowing for clinicians who, again, as you said, spend a great deal of time, often outside of the typical work hours, trying to organize these patient records in order to go present to a tumor board.&lt;/p&gt;



&lt;p&gt;And a tumor board is a critical meeting that happens at many cancer centers where specialists all get together, come with their perspective, and make a comment on what would be the best next step in treatment. But the background in preparing for that is you know, again, organizing the data. But to your point, also, what are the clinical trials that are active? There are thousands of clinical trials. There’s hundreds every day added. How can anyone keep up with that? And these are the kinds of use cases that start to bubble up. And you realize that a technology that understands concepts, context and can reason over vast amounts of data with a language interface-that is a powerful tool. Even before we get to some of the, you know, unlocking new insights and even precision medicine, this is that idea of saving time before lives to me. And there’s an enormous amount of undifferentiated heavy lifting that happens in health&lt;s&gt; &lt;/s&gt;care that these agents and these kinds of workflows can start to unlock.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;GUYMAN: &lt;/strong&gt;And we’ve packaged these agents, the manual abstraction work that, you know, manually takes hours. Now we have an agent. It’s in Foundry along with the clinical trial matching agent, which I think at Providence you showed could double the match rate over the baseline that they were using by using the AI for multiple data sources. So, we have that and then we have this orchestration that is using this really neat technology from Microsoft Research. Semantic Kernel, Magentic&lt;s&gt; &lt;/s&gt;One, Omni&lt;s&gt; &lt;/s&gt;Parser. These are technologies that are good at figuring out which agent to use for a given task. So a clinician who’s used to working with other specialists, like a radiologist, a pathologist, a surgeon, they can now also consult these specialist agents who are experts in their domain and there’s shared memory across the agents.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;There’s turn taking, there’s negotiation between the agents. So, there’s this really interesting system that’s emerging. And again, this is all possible to be used through Teams. And there’s some great extensibility as well. We’ve been talking about that and working on some cool tools.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SALIGRAMA:&lt;/strong&gt; Yeah. Yeah. No, I think if I have to geek out a little bit on how all this agent tech orchestrations are coming up, like I’ve been in software engineering for decades, it’s kind of a next version of distributed systems where you have these services that talk to each other. It’s a more natural way because LLMs are giving these natural ways instead of a structured API ways of conversing. We have these agents which can naturally understand how to talk to each other. Right. So this is like the next evolution of our systems now. And the way we’re packaging all of this is multiple ways based on all the standards and innovation that’s happening in this space. So, first of all, we are building these agents that are very good at specific tasks, like, Will was saying like, a trial matching agent or patient timeline agents.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So, we take all of these, and then we package it in a workflow and an orchestration. We use the standard, some of these coming from research. The Semantic Kernel, the Magentic-One. And then, all of these also allow us to extend these agents with custom agents that can be plugged in. So, we are open sourcing the entire agent orchestration in AI Foundry templates, so that developers can extend their own agents, and make their own workflows out of it. So, a lot of cool innovation happening to apply this technology to specific scenarios and workflows.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;Well, I was going to ask you, like, so as part of that extension. So, like, you know, folks can say, hey, I have maybe a really specific part of my workflow that I want to use some agents for, maybe one of the agents that can do PubMed literature search, for example. But then there’s also agents that, come in from the outside, you know, sort of like I could, I can imagine a software company or AI company that has a built-in agent that plugs in as well.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SALIGRAMA: &lt;/strong&gt;Yeah. Yeah, absolutely. So, you can bring your own agent. And then we have these, standard ways of communicating with agents and integrating with the orchestration language so you can bring your own agent and extend this health care agent, agent orchestrator to your own needs.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;I can just think of, like, in a group chat, like a bunch of different specialist agents. And I really would want an orchestrator to help find the right tool, to your point earlier, because I’m guessing this ecosystem is going to expand quickly. Yeah. And I may not know which tool is best for which question. I just want to ask the question. Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SALIGRAMA: &lt;/strong&gt;Yeah. Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON: &lt;/strong&gt;Well, I think to that point to I mean, you said an important point here, which is tools, and these are not necessarily just AI tools. Right? I mean, we’ve known this for a while, right? LLMS are not very good at math, but you can have it use a calculator and then it works very well. And you know you guys both brought up the universal medical abstraction a couple times.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And one of the things that I find so powerful about that is we’ve long had this vision within the precision health community that we should be able to have a learning hospital system. We should be able to actually learn from the actual real clinical experiences that are happening every day, so that we can stop practicing medicine based off averages.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;There’s a lot of work that’s gone on for the last 20 years about how to actually do causal inference. That’s not an AI question. That’s a statistical question. The bottleneck, the reason why we haven’t been able to do that is because most of that information is locked up in unstructured text. And these other tools need essentially a table.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And so now you can decompose this problem, say, well, what if I can use AI not to get to the causal answer, but to just structure the information. So now I can put it into the causal inference tool. And these sorts of patterns I think again become very, not just powerful for a programmer, but they start pulling together different specialties. And I think we’ll really see an acceleration, really, of collaboration across disciplines because of this.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON: &lt;/strong&gt;So, when I joined Microsoft Research 18 years ago, I was doing work in computational biology. And I would always have to answer the question: why is Microsoft in biomedicine? And I would always kind of joke saying, well, it is. We sell Office and Windows to every health&lt;s&gt; &lt;/s&gt;care system in the world. We’re already in the space. And it really struck me to now see that we’ve actually come full circle. And now you can actually connect in Teams, Word, PowerPoint, which are these tools that everybody uses every day, but they’re actually now specialize-able through these agents. Can you guys talk a little bit about what that looks like from a developer perspective? How can provider groups actually start playing with this and see this come to life?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SALIGRAMA: &lt;/strong&gt;A lot of healthcare organizations already use Microsoft productivity tools, as you mentioned. So, they asked the developers, build these agents, and use our healthcare orchestrations, to plug in these agents and expose these in these productivity tools. They will get access to all these healthcare workers. So the healthcare agent orchestrator we have today integrates with Microsoft Teams, and it showcases an example of how you can at (@) mention these agents and talk to them like you were talking to another person in a Teams chat. And then it also provides examples of these agents and how they can use these productivity tools. One of the examples we have there is how they can summarize the assessments of this whole chat into a Word Doc, or even convert that into a PowerPoint presentation, for later on.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON:&lt;/strong&gt; One of the things that has struck me is how easy it is to do. I mean, Will, I don’t know if you’ve worked with folks that have gone from 0 to 60, like, how fast? What does that look like?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;GUYMAN: &lt;/strong&gt;Yeah, it’s funny for us, the technology to transfer all this context into a Word Document or PowerPoint presentation for a doctor to take to a meeting is relatively straightforward compared to the complicated clinical trial matching multimodal processing. The feedback has been tremendous in terms of, wow, that saves so much time to have this organized report that then I can show up to meeting with and the agents can come with me to that meeting because they’re literally having a Teams meeting, often with other human specialists. And the agents can be there and ask and answer questions and fact check and source all the right information on the fly. So, there’s a nice integration into these existing tools.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN:&lt;/strong&gt; We worked with several different centers just to kind of understand, you know, where this might be useful. And, like, as I think we talked about before, the ideas that we’ve come up with again, this is a great one because it’s complex. It’s kind of hairy. There’s a lot of things happening under the hood that don’t necessarily require a medical license to do, right, to prepare for a tumor board and to organize data. But, it’s fascinating, actually. So, you know, folks have come up with ideas of, could I have an agent that can operate an MRI machine, and I can ask the agent to change some parameters or redo a protocol. We thought that was a pretty powerful use case. We’ve had others that have just said, you know, I really want to have a specific agent that’s able to kind of act like deep research does for the consumer side, but based on the context of my patient, so that it can search all the literature and pull the data in the papers that are relevant to this case. And the list goes on and on from operations all the way to clinical, you know, sort of decision making at some level. And I think that the research community that’s going to sprout around this will help us, guide us, I guess, to see what is the most high-impact use cases. Where is this effective? And maybe where it’s not effective.&lt;/p&gt;



&lt;p&gt;But to me, the part that makes me so, I guess excited about this is just that I don’t have to think about, okay, well, then we have to figure out Health IT. Because it’s always, you know, we always have great ideas and research, and it always feels like there’s such a huge chasm to get it in front of the health care workers that might want to test this out. And it feels like, again, this productivity tool use case again with the enterprise security, the possibility for bringing in third parties to contribute really does feel like it’s a new surface area for innovation.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON: &lt;/strong&gt;Yeah, I love that. Look. Let me end by putting you all on the spot. So, in three years, multimodal agents will do what? Matt, I’ll start with you.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;I am convinced that it’s going to save massive amount of time before it saves many lives.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;RUNDE: &lt;/strong&gt;I’ll focus on the patient care journey and diagnostic journey. I think it will kind of transform that process for the patient itself and shorten that process.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;GUYMAN: &lt;/strong&gt;Yeah, I think we’ve seen already papers recently showing that different modalities surfaced complementary information. And so we’ll see kind of this AI and these agents becoming an essential companion to the physician, surfacing insights that would have been overlooked otherwise.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SALIGRAMA: &lt;/strong&gt;And similar to what you guys were saying, agents will become important assistants to healthcare workers, reducing a lot of documentation and workflow, excess work they have to do.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON: &lt;/strong&gt;I love that. And I guess for my part, I think really what we’re going to see is a massive unleash of creativity. We’ve had a lot of folks that have been innovating in this space, but they haven’t had a way to actually get it into the hands of early adopters. And I think we’re going to see that really lead to an explosion of creativity across the ecosystem.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;So, where do we get started? Like where are the developers who are listening to this? The folks that are at, you know, labs, research labs and developing health care solutions. Where do they go to get started with the Foundry, the models we’ve talked about, the healthcare agent orchestrator. Where do they go?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;GUYMAN&lt;/strong&gt;: So AI.azure.com is the AI Foundry. It’s a website you can go as a developer. You can sign in with your Azure subscription, get your Azure account, your own VM, all that stuff. And you have an agent catalog, the model catalog. You can start from there. There is documentation and templates that you can then deploy to Teams or other applications.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN:&lt;/strong&gt; And tutorials are coming. Right. We have recordings of tutorials.&amp;nbsp;We’ll have Hackathons, some sessions and then more to come. Yeah, we’re really excited.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;Thank you so much, guys for joining us.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON:&lt;/strong&gt; Yes. Yeah. Thanks.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SALIGRAMA:&lt;/strong&gt; Thanks for having us.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&amp;nbsp;&lt;/p&gt;

				&lt;/span&gt;
			&lt;/div&gt;
			&lt;button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button"&gt;
				Show more			&lt;/button&gt;
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Microsoft Research Podcast | Collaborators: Healthcare Innovation to Impact | outline illustrations of Jonathan Carlson, Smitha Saligrama, Will Guyman, Cameron Runde, Dr. Matthew Lungren" class="wp-image-1139392" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/HealthResearch_Collaborators_Hero_Feature_No_Text_1400x788.jpg" width="1400" /&gt;&lt;/figure&gt;






&lt;p&gt;&lt;em&gt;This episode contains forward-looking statements, including predictions and expectations about future developments in AI and healthcare. These statements are based on current assumptions and projections and are subject to risks and uncertainties. Actual outcomes may differ materially.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Transforming research ideas into meaningful impact is no small feat. It often requires the knowledge and experience of individuals from across disciplines and institutions. &lt;em&gt;Collaborators&lt;/em&gt;, a new Microsoft Research Podcast series, explores the relationships—both expected and unexpected—behind the projects, products, and services being pursued and delivered by researchers at Microsoft and the diverse range of people they’re teaming up with.&amp;nbsp;&lt;br /&gt;&lt;/p&gt;



&lt;p&gt;Amid the ongoing surge of AI research, healthcare is emerging as a leading area for real-world transformation. From driving efficiency gains for clinicians to improving patient outcomes, AI is beginning to make a tangible impact. Thousands of scientific papers have explored AI systems capable of analyzing medical documents and images with unprecedented accuracy. The latest work goes even further, showing how healthcare agents can collaborate—with each other and with human doctors—embedding AI directly into clinical workflows.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In this discussion, we explore how teams across Microsoft are working together to generate advanced AI capabilities and solutions for developers and clinicians around the globe. Leading the conversation are Dr. Matthew Lungren, chief scientific officer for Microsoft Health and Life Sciences, and Jonathan Carlson, vice president and managing director of Microsoft Health Futures—two key leaders behind this collaboration. They’re joined by Smitha Saligrama, principal group engineering manager within Microsoft Health and Life Sciences, Will Guyman, group product manager within Microsoft Health and Life Sciences, and Cameron Runde, a senior strategy manager for Microsoft Health Futures—all of whom play crucial roles in turning AI breakthroughs into practical, life-saving innovations.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Together, these experts examine how Microsoft is helping integrate cutting-edge AI into healthcare workflows—saving time today, and lives tomorrow.&lt;/p&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h2 class="wp-block-heading h5" id="learn-more"&gt;Learn more&lt;/h2&gt;



&lt;p&gt;Developing next-generation cancer care management with multi-agent orchestration&lt;br /&gt;Industry Blog, May 2025&lt;/p&gt;



&lt;p&gt;Healthcare Agent Orchestrator&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;br /&gt;GitHub &lt;/p&gt;



&lt;p&gt;Azure AI Foundry Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;br /&gt;Homepage&lt;/p&gt;







&lt;section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast"&gt;
	
&lt;/section&gt;


&lt;div class="wp-block-msr-show-more"&gt;
	&lt;div class="bg-neutral-100 p-5"&gt;
		&lt;div class="show-more-show-less"&gt;
			&lt;div&gt;
				&lt;span&gt;
					

&lt;h2 class="wp-block-heading" id="transcript"&gt;Transcript&lt;/h2&gt;



&lt;p&gt;[MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MATTHEW LUNGREN: &lt;/strong&gt;You’re listening to Collaborators, a Microsoft Research podcast, showcasing the range of expertise that goes into transforming mind blowing ideas into&amp;nbsp; world changing technologies. Despite the advancements in AI over the decades, generative AI exploded into view in 2022, when ChatGPT became the, sort of, internet browser for AI and became the fastest adopted consumer software application in history.&lt;/p&gt;



				&lt;/span&gt;
				&lt;span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1"&gt;
					



&lt;p&gt;&lt;strong&gt;JONATHAN CARLSON: &lt;/strong&gt;From the beginning, healthcare stood out to us as an important opportunity for general reasoners to improve the lives and experiences of patients and providers. Indeed, in the past two years, there’s been an explosion of scientific papers looking at the application first of text reasoners and medicine, then multi-modal reasoners that can interpret medical images, and now, most recently, healthcare agents that can reason with each other. But even more impressive than the pace of research has been the surprisingly rapid diffusion of this technology into real world clinical workflows.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN:&lt;/strong&gt; So today, we’ll talk about how our cross-company collaboration has shortened that gap and delivered advanced AI capabilities and solutions into the hands of developers and clinicians around the world, empowering everyone in health and life sciences to achieve more. I’m Doctor Matt Lungren, chief scientific officer for Microsoft Health and Life Sciences.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON: &lt;/strong&gt;And I’m Jonathan Carlson, vice president and managing director of Microsoft Health Futures.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN:&lt;/strong&gt; And together we brought some key players leading in the space of AI and health&lt;s&gt; &lt;/s&gt;care from across Microsoft. Our guests today are Smitha Saligrama, principal group engineering manager within Microsoft Health and Life Sciences, Will Guyman, group product manager within Microsoft Health and Life Sciences, and Cameron Runde, a senior strategy manager for Microsoft Health Futures.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON:&lt;/strong&gt; We’ve asked these brilliant folks to join us because each of them represents a mission critical group of cutting-edge stakeholders, scaling breakthroughs into purpose-built solutions and capabilities for health&lt;s&gt; &lt;/s&gt;care.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;We’ll hear today how generative AI capabilities can unlock reasoning across every data type in medicine: text, images, waveforms, genomics. And further, how multi-agent frameworks in healthcare can accelerate complex workflows, in some cases acting as a specialist team member, safely secured inside the Microsoft 365 tools used by hundreds of millions of healthcare enterprise users across the world. The opportunity to save time today and lives tomorrow with AI has never been larger.&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&amp;nbsp;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;&lt;strong&gt;MATTHEW LUNGREN: &lt;/strong&gt;Jonathan. You know, it’s been really interesting kind of observing Microsoft Research over the decades. I’ve, you know, been watching you guys in my prior academic career. You are always on the front of innovation, particularly in health&lt;s&gt; &lt;/s&gt;care. And I find it fascinating that, you know, millions of people are using the solutions that, you know, your team has developed over the years and yet you still find ways to stay cutting edge and state of the art, even in this accelerating time of technology and AI, particularly, how do you do that? [LAUGHS]&lt;/p&gt;



&lt;p&gt;&amp;nbsp;&lt;strong&gt;JONATHAN CARLSON: &lt;/strong&gt;I mean, it’s some of what’s in our DNA, I mean, we’ve been publishing in health and life sciences for two decades here. But when we launched Health Futures as a mission-focused lab about 7 or 8 years ago, we really started with the premise that the way to have impact was to really close the loop between, not just good ideas that get published, but good ideas that can actually be grounded in real problems that clinicians and scientists care about, that then allow us to actually go from that first proof of concept into an incubation, into getting real world feedback that allows us to close that loop. And now with, you know, the HLS organization here as a product group, we have the opportunity to work really closely with you all to not just prove what’s possible in the clinic or in the lab, but actually start scaling that into the broader community.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CAMERON RUNDE: &lt;/strong&gt;And one thing I’ll add here is that the problems that we’re trying to tackle in health&lt;s&gt; &lt;/s&gt;care are extremely complex. And so, as Jonathan said, it’s really important that we come together and collaborate across disciplines as well as across the company of Microsoft and with our external collaborators, as well across the whole industry.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON: &lt;/strong&gt;So, Matt, back to you. What are you guys doing in the product group? How do you guys see these models getting into the clinic?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN:&lt;/strong&gt; You know, I think a lot of people, you know, think about AI is just, you know, maybe just even a few years old because of GPT and how that really captured the public’s consciousness. Right? &lt;/p&gt;



&lt;p&gt;And so, you think about the speech-to-text technology of being able to dictate something, for a clinic note or for a visit, that was typically based on Nuance technology. And so there’s a lot of product understanding of the market, how to deliver something that clinicians will use, understanding the pain points and workflows and really that Health IT space, which is sometimes the third rail, I feel like with a lot of innovation in healthcare.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But beyond that, I mean, I think now that we have this really powerful engine of Microsoft and the platform capabilities, we’re seeing, innovations on the healthcare side for data storage, data interoperability, with different types of medical data. You have new applications coming online, the ability, of course, to see generative AI now infused into the speech-to-text and, becoming Dragon Copilot, which is something that has been, you know, tremendously, received by the community.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Physicians are able to now just have a conversation with a patient. They turn to their computer and the note is ready for them. There’s no more this, we call it keyboard liberation. I don’t know if you heard that before. And that’s just been tremendous. And there’s so much more coming from that side. And then there’s other parts of the workflow that we also get engaged in — the diagnostic workflow.&lt;/p&gt;



&lt;p&gt;So medical imaging, sharing images across different hospital systems, the list goes on. And so now when you move into AI, we feel like there’s a huge opportunity to deliver capabilities into the clinical workflow via the products and solutions we already have. But, I mean, we’ll now that we’ve kind of expanded our team to involve Azure and platform, we’re really able to now focus on the developers.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WILL GUYMAN: &lt;/strong&gt;Yeah. And you’re always telling me as a doctor how frustrating it is to be spending time at the computer instead of with your patients. I think you told me, you know, 4,000 clicks a day for the typical doctor, which is tremendous. And something like Dragon Copilot can save that five minutes per patient. But it can also now take actions after the patient encounter so it can draft the after-visit summary.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It can order labs and medications for the referral. And that’s incredible. And we want to keep building on that. There’s so many other use cases across the ecosystem. And so that’s why in Azure AI Foundry, we have translated a lot of the research from Microsoft Research and made that available to developers to build and customize for their own applications.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SMITHA SALIGRAMA: &lt;/strong&gt;Yeah. And as you were saying, in our transformation of moving from solutions to platforms and as, scaling solutions to other, multiple scenarios, as we put our models in AI Foundry, we provide these developer capabilities like bring your own data and fine&lt;s&gt; &lt;/s&gt;tune these models and then apply it to, scenarios that we couldn’t even imagine. So that’s kind of the platform play we’re scaling now.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;Well, I want to do a reality check because, you know, I think to us that are now really focused on technology, it seems like, I’ve heard this story before, right. I, I remember even in, my academic clinical days where it felt like technology was always the quick answer and it felt like technology was, there was maybe a disconnect between what my problems were or what I think needed to be done versus kind of the solutions that were kind of, created or offered to us. And I guess at some level, how Jonathan, do you think about this? Because to do things well in the science space is one thing, to do things well in science, but then also have it be something that actually drives health&lt;s&gt; &lt;/s&gt;care innovation and practice and translation. It’s tricky, right?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON:&lt;/strong&gt; Yeah. I mean, as you said, I think one of the core pathologies of Big Tech is we assume every problem is a technology problem. And that’s all it will take to solve the problem. And I think, look, I was trained as a computational biologist, and that sits in the awkward middle between biology and computation. And the thing that we always have to remember, the thing that we were very acutely aware of when we set out, was that we are not the experts. We do have, you know, you as an M.D., we have everybody on the team, we have biologists on the team.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But this is a big space. And the only way we’re going to have real impact, the only way we’re even going to pick the right problems to work on is if we really partner deeply, with providers, with EHR (electronic health records) vendors, with scientists, and really understand what’s important and again, get that feedback loop.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;RUNDE: &lt;/strong&gt;Yeah, I think we really need to ground the work that we do in the science itself. You need to understand the broader ecosystem and the broader landscape, across health&lt;s&gt; &lt;/s&gt;care and life sciences, so that we can tackle the most important problems, not just the problems that &lt;em&gt;we &lt;/em&gt;think are important. Because, as Jonathan said, we’re not the experts in health&lt;s&gt; &lt;/s&gt;care and life sciences. And that’s really the secret sauce. When you have the clinical expertise come together with the technical expertise. That’s how you really accelerate health&lt;s&gt; &lt;/s&gt;care.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON:&lt;/strong&gt; When we really launched this, this mission, 7 or 8 years ago, we really came in with the premise of, if we decide to stop, we want to be sure the world cares. And the only way that’s going to be true is if we’re really deeply embedded with the people that matter–the patients, the providers and the scientists.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN:&lt;/strong&gt; And now it really feels like this collaborative effort, you know, really can help start to extend that mission. Right. I think, you know, Will and Smitha, that we definitely feel the passion and the innovation. And we certainly benefit from those collaborations, too. But then we have these other partners and even customers, right, that we can start to tap into and have that flywheel keep spinning.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;GUYMAN: &lt;/strong&gt;Yeah. And the whole industry is an ecosystem. So, we have our own data sets at Microsoft Research that you’ve trained amazing AI models with. And those are in the catalog. But then you’ve also partnered with institutions like Providence or Page AI . And those models are in the catalog with their data. And then there are third parties like Nvidia that have their own specialized proprietary data sets, and their models are there too. So, we have this ecosystem of open source models. And maybe Smitha, you want to talk about how developers can actually customize these.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SALIGRAMA: &lt;/strong&gt;Yeah. So we use the Azure AI Foundry ecosystem. Developers can feel at home if they’re using the AI Foundry. So they can look at our model cards that we publish as part of the models we publish, understand the use cases of these models, how to, quickly, bring up these APIs and, look at different use cases of how to apply these and even fine&lt;s&gt; &lt;/s&gt;tune these models with their own data. Right. And then, use it for specific tasks that we couldn’t have even imagined.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;Yeah it has been interesting to see we have these health&lt;s&gt; &lt;/s&gt;care models in the catalog again, some that came from research, some that came from third parties and other product developers and Azure’s kind of becoming the home base, I think, for a lot of health and life science developers. They’re seeing all the different modalities, all the different capabilities. And then in combination with Azure OpenAI, which as we know, is incredibly competent in lots of different use cases. How are you looking at the use cases, and what are you seeing folks use these models for as they come to the catalog and start sharing their discoveries or products?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;GUYMAN: &lt;/strong&gt;Well, the general-purpose large language models are amazing for medical general reasoning. So Microsoft Research has shown that that they can perform super well on, for example, like the United States medical licensing exam, they can exceed doctor performance if they’re just picking between different multiple-choice questions. But real medicine we know is messier. It doesn’t always start with the whole patient context provided as text in the prompt. You have to get the source data and that raw data is often non-text. The majority of it is non-text. It’s things like medical imaging, radiology, pathology, ophthalmology, dermatology. It goes on and on. And there’s endless signal data, lab data. And so all of this diverse data type needs to be processed through specialized models because much of that data is not available on the public internet.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And that’s why we’re taking this partner approach, first party and third party models that can interpret all this kind of data and then connect them ultimately back to these general reasoners to reason over that.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;So, you know, I’ve been at this company for a while and, you know, familiar with kind of how long it takes, generally to get, you know, a really good research paper, do all the studies, do all the data analysis, and then go through the process of publishing, right, which takes, as, you know, a long time and it’s, you know, very rigorous.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And one of the things that struck me, last year, I think we, we started this big collaboration and, within a quarter, you had a Nature paper coming out from Microsoft Research, and that model that the Nature paper was describing was ready to be used by anyone on the Azure AI Foundry within that same quarter. It kind of blew my mind when I thought about it, you know, even though we were all, you know, working very hard to get that done. Any thoughts on that? I mean, has this ever happened in your career? And, you know, what’s the secret sauce to that?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON: &lt;/strong&gt;Yeah, I mean, the time scale from research to product has been massively compressed. And I’d push that even further, which is to say, the reason why it took a quarter was because we were laying the railroad tracks as we’re driving the train. We have examples right after that when we are launching on Foundry the same day we were publishing the paper.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And frankly, the review times are becoming longer than it takes to actually productize the models. I think there’s two things that are going on with that are really converging. One is that the overall ecosystem is converging on a relatively small number of patterns, and that gives us, as a tech company, a reason to go off and really make those patterns hardened in a way that allows not just us, but third parties as well, to really have a nice workflow to publish these models.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But the other is actually, I think, a change in how we work, you know, and for most of our history as an industrial research lab, we would do research and then we’d go pitch it to somebody and try and throw it over the fence. We’ve really built a much more integrated team. In fact, if you look at that Nature paper or any of the other papers, there’s folks from product teams. Many of you are on the papers along with our clinical collaborators. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;RUNDE:&lt;/strong&gt; Yeah. I think one thing that’s really important to note is that there’s a ton of different ways that you can have impact, right? So I like to think about phasing. In Health Futures at least, I like to think about phasing the work that we do. So first we have research, which is really early innovation. And the impact there is getting our technology and our tools out there and really sharing the learnings that we’ve had.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So that can be through publications like you mentioned. It can be through open-sourcing our models. And then you go to incubation. So, this is, I think, one of the more new spaces that we’re getting into, which is maybe that blurred line between research and product. Right. Which is, how do we take the tools and technologies that we’ve built and get them into the hands of users, typically through our partnerships?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Right. So, we partner very deeply and collaborate very deeply across the industry. And incubation is really important because we get that early feedback. We get an ability to pivot if we need to. And we also get the ability to see what types of impact our technology is having in the real world. And then lastly, when you think about scale, there’s tons of different ways that you can scale. We can scale third-party through our collaborators and really empower them to go to market to commercialize the things that we’ve built together.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;You can also think about scaling internally, which is why I’m so thankful that we’ve created this flywheel between research and product, and a lot of the models that we’ve built that have gone through research, have gone through incubation, have been able to scale on the Azure AI Foundry. But that’s not really our expertise. Right? The scale piece in research, that’s research and incubation. Smitha, how do you think about scaling?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SALIGRAMA:&lt;/strong&gt; So, there are several angles to scaling the models, the state-of-the-art models we see from the research team. The first angle is, the open sourcing, to get developer trust, and very generous commercial licenses so that they can use it and for their own, use cases.&amp;nbsp;The second is, we also allow them to customize these models, fine&lt;s&gt; &lt;/s&gt;tuning these models with their own data. So a lot of different angles of how we provide support and scaling, the state-of-the-art of models we get from the research org.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;GUYMAN:&lt;/strong&gt; And as one example, you know, University of Wisconsin Health, you know, which Matt knows well. They took one of our models, which is highly versatile. They customized it in Foundry and they optimized it to reliably identify abnormal chest X-rays, the most common imaging procedure, so they could improve their turnaround time triage quickly. And that’s just one example. But we have other partners like Sectra who are doing more of operations use cases automatically routing imaging to the radiologists, setting them up to be efficient. And then Page AI is doing, you know, biomarker identification for actually diagnostics and new drug discovery. So, there’s so many use cases that we have partners already who are building and customizing.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN:&lt;/strong&gt; The part that’s striking to me is just that, you know, we could all sit in a room and think about all the different ways someone might use these models on the catalog. And I’m still shocked at the stuff that people use them for and how effective they are. And I think part of that is, you know, again, we talk a lot about generative AI and healthcare and all the things you can do. Again, you know, in text, you refer to that earlier and certainly off the shelf, there’s really powerful applications. But there is, you know, kind of this tip of the iceberg effect where under the water, most of the data that we use to take care of our patients is not text. Right. It’s all the different other modalities. And I think that this has been an unlock right, sort of taking these innovations, innovations from the community, putting them in this ecosystem kind of catalog, essentially. Right. And then allowing folks to kind of, you know, build and develop applications with all these different types of data. Again, I’ve been surprised at what I’m seeing.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON:&lt;/strong&gt; This has been just one of the most profound shifts that’s happened in the last 12 months, really. I mean, two years ago we had general models in text that really shifted how we think about, I mean, natural language processing got totally upended by that. Turns out the same technology works for images as well. It doesn’t only allow you to automatically extract concepts from images, but allows you to align those image concepts with text concepts, which means that you can have a conversation with that image. And once you’re in that world now, you are a place where you can start stitching together these multimodal models that really change how you can interact with the data, and how you can start getting more information out of the raw primary data that is part of the patient journey.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;Well, and we’re going to get to that because I think you just touched on something. And I want to re-emphasize stitching these things together. There’s a lot of different ways to potentially do that. Right? There’s ways that you can literally train the model end to end with adapters and all kinds of other early fusion fusions. All kinds of ways. But one of the things that the word of the I guess the year is going to be agents and an agent is a very interesting term to think about how you might abstract away some of the components or the tasks that you want the model to, to accomplish in the midst of sort of a real human to maybe model interaction. Can you talk a little bit more about, how we’re thinking about agents in this, in this platform approach?&amp;nbsp;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;&lt;strong&gt;GUYMAN: &lt;/strong&gt;Well, this is our newest addition to the Azure AI Foundry. So there’s an agent catalog now where we have a set of pre-configured agents for health care. And then we also have a multi-agent orchestrator that can jump&lt;s&gt; &lt;/s&gt;start the process of developers building their own multi-agent workflows to tackle some complex real-world tasks that clinicians have to deal with. And these agents basically combine a general reasoner, like a large language model, like a GPT 4o or an o series model with a specialized model, like a model that understands radiology or pathology with domain-specific knowledge and tools. So the knowledge might be, you know, public guidelines or, you know, medical journals or your own private data from your EHR or medical imaging system, and then tools like a code interpreter to deal with all of the numeric data or tools like that that the clinicians are using today, like PowerPoint, Word, Teams and etc. And so we’re allowing developers to build and customize each of these agents in Foundry and then deploy them into their workflows.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;And, and I really like that concept because, you know, as, as a, as a from the user personas, I think about myself as a user. How am I going to interact with these agents? Where does it naturally fit? And I and I sort of, you know, I’ve seen some of the demonstrations and some of the work that’s going on with Stanford in particular, showing that, you know, and literally in a Teams chat, I can have my clinician colleagues and I can have specialized health&lt;s&gt; &lt;/s&gt;care agents that kind of interact, like I’m interacting with a human on a chat.&lt;/p&gt;



&lt;p&gt;It is a completely mind-blowing thing for me. And it’s a light bulb moment for me to I wonder, what have we, what have we heard from folks that have, you know, tried out this health care agent orchestrator in this kind of deployment environment via Teams?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;GUYMAN: &lt;/strong&gt;Well, someone joked, you know, are you sure you’re not using Teams because you work at Microsoft? [LAUGHS] But, then we actually were meeting with one of the, radiologists at one of our partners, and they said that that morning they had just done a Teams meeting, or they had met with other specialists to talk about a patient’s cancer case, or they were coming up with a treatment plan.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And that was the light bulb moment for us. We realized, actually, Teams is already being used by physicians as an internal communication tool, as a tool to get work done. And especially since the pandemic, a lot of the meetings moved to virtual and telemedicine. And so it’s a great distribution channel for AI, which is often been a struggle for AI to actually get in the hands of clinicians. And so now we’re allowing developers to build and then deploy very easily and extend it into their own workflows.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON: &lt;/strong&gt;I think that’s such an important point. I mean, if you think about one of the really important concepts in computer science is an application programing interface, like some set of rules that allow two applications to talk to each other. One of the big pushes, really important pushes, in medicine has been standards that allow us to actually have data standards and APIs that allow these to talk to each other, and yet still we end up with these silos. There’s silos of data. There’s silos of applications.&lt;/p&gt;



&lt;p&gt;And just like when you and I work on our phone, we have to go back and forth between applications. One of the things that I think agents do is that it takes the idea that now you can use language to understand intent and effectively program an interface, and it creates a whole new abstraction layer that allows us to simplify the interaction between not just humans and the endpoint, but also for developers.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It allows us to have this abstraction layer that lets different developers focus on different types of models, and yet stitch them all together in a very, very natural, way, not just for the users, but for the ability to actually deploy those models.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SALIGRAMA: &lt;/strong&gt;Just to add to what Jonathan was mentioning, the other cool thing about the Microsoft Teams user interface is it’s also enterprise ready.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;RUNDE: &lt;/strong&gt;And one important thing that we’re thinking about, is exactly this from the very early research through incubation and then to scale, obviously. Right. And so early on in research, we are actively working with our partners and our collaborators to make sure that we have the right data privacy and consent in place. We’re doing this in incubation as well. And then obviously in scale. Yep.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;So, I think AI has always been thought of as a savior kind of technology. We talked a little bit about how there’s been some ups and downs in terms of the ability for technology to be effective in health care. At the same time, we’re seeing a lot of new innovations that are really making a difference. But then we kind of get, you know, we talked about agents a little bit. It feels like we’re maybe abstracting too far. Maybe it’s things are going too fast, almost. What makes this different? I mean, in your mind is this truly a logical next step or is it going to take some time?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON:&lt;/strong&gt; I think there’s a couple things that have happened. I think first, on just a pure technology. What led to ChatGPT? And I like to think of really three major breakthroughs.&lt;/p&gt;



&lt;p&gt;The first was new mathematical concepts of attention, which really means that we now have a way that a machine can figure out which parts of the context it should actually focus on, just the way our brains do. Right? I mean, if you’re a clinician and somebody is talking to you, the majority of that conversation is not relevant for the diagnosis. But, you know how to zoom in on the parts that matter. That’s a super powerful mathematical concept. The second one is this idea of self-supervision. So, I think one of the fundamental problems of machine learning has been that you have to train on labeled training data and labels are expensive, which means data sets are small, which means the final models are very narrow and brittle. And the idea of self-supervision is that you can just get a model to automatically learn concepts, and the language is just predict the next word. And what’s important about that is that leads to models that can actually manipulate and understand really messy text and pull out what’s important about that, and then and then stitch that back together in interesting ways.&lt;/p&gt;



&lt;p&gt;And the third concept, that came out of those first two, was just the observational scale. And that’s that more is better, more data, more compute, bigger models. And that really leads to a reason to keep investing. And for these models to keep getting better. So that as a as a groundwork, that’s what led to ChatGPT. That’s what led to our ability now to not just have rule-based systems or simple machine learning based systems to take a messy EHR record, say, and pull out a couple concepts.&lt;/p&gt;



&lt;p&gt;But to really feed the whole thing in and say, okay, I need you to figure out which concepts are in here. And is this particular attribute there, for example. That’s now led to the next breakthrough, which is all those core ideas apply to images as well. They apply to proteins, to DNA. And so we’re starting to see models that understand images and the concepts of images, and can actually map those back to text as well.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So, you can look at a pathology image and say, not just at the cell, but it appears that there’s some certain sort of cancer in this particular, tissue there. And then you take those two things together and you layer on the fact that now you have a model, or a set of models, that can understand intent, can understand human concepts and biomedical concepts, and you can start stitching them together into specialized agents that can actually reason with each other, which at some level gives you an API as a developer to say, okay, I need to focus on a pathology model and get this really, really, sound while somebody else is focusing on a radiology model, but now allows us to stitch these all together with a user interface that we can now talk to through natural language.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;RUNDE: &lt;/strong&gt;I’d like to double click a little bit on that medical abstraction piece that you mentioned. Just the amount of data, clinical data that there is for each individual patient. Let’s think about cancer patients for a second to make this real. Right. For every cancer patient, it could take a couple of hours to structure their information. And why is that important? Because, you have to get that information in a structured way and abstract relevant information to be able to unlock precision health applications right, for each patient. So, to be able to match them to a trial, right, someone has to sit there and go through all of the clinical notes from their entire patient care journey, from the beginning to the end. And that’s not scalable. And so one thing that we’ve been doing in an active project that we’ve been working on with a handful of our partners, but Providence specifically, I’ll call out, is using AI to actually abstract and curate that information. So that gives time back to the health care provider to spend with patients, instead of spending all their time curating this information.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And this is super important because it sets the scene and the backbone for all those precision health applications. Like I mentioned, clinical trial matching, tumor boards is another really important example here. Maybe Matt, you can talk to that a little bit.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;It’s a great example. And you know it’s so funny. We’ve talked about this use case and the you know the health&lt;s&gt; &lt;/s&gt;care agent orchestrator is sort of the at the initial lighthouse use case was a tumor board setting. And I remember when we first started working with some of the partners on this, I think we were you know, under a research kind of lens, thinking about what could this, what new diagnoses could have come up with or what new insights might have and what was really a really key moment for us, I think, was noticing that we had developed an agent that can take all of the multimodal data about a patient’s chart, organize it in a timeline, in chronological fashion, and then allow folks to click on different parts of the timeline to ground it back to the note. And just that, which doesn’t sound like a really interesting research paper. It was mind blowing for clinicians who, again, as you said, spend a great deal of time, often outside of the typical work hours, trying to organize these patient records in order to go present to a tumor board.&lt;/p&gt;



&lt;p&gt;And a tumor board is a critical meeting that happens at many cancer centers where specialists all get together, come with their perspective, and make a comment on what would be the best next step in treatment. But the background in preparing for that is you know, again, organizing the data. But to your point, also, what are the clinical trials that are active? There are thousands of clinical trials. There’s hundreds every day added. How can anyone keep up with that? And these are the kinds of use cases that start to bubble up. And you realize that a technology that understands concepts, context and can reason over vast amounts of data with a language interface-that is a powerful tool. Even before we get to some of the, you know, unlocking new insights and even precision medicine, this is that idea of saving time before lives to me. And there’s an enormous amount of undifferentiated heavy lifting that happens in health&lt;s&gt; &lt;/s&gt;care that these agents and these kinds of workflows can start to unlock.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;GUYMAN: &lt;/strong&gt;And we’ve packaged these agents, the manual abstraction work that, you know, manually takes hours. Now we have an agent. It’s in Foundry along with the clinical trial matching agent, which I think at Providence you showed could double the match rate over the baseline that they were using by using the AI for multiple data sources. So, we have that and then we have this orchestration that is using this really neat technology from Microsoft Research. Semantic Kernel, Magentic&lt;s&gt; &lt;/s&gt;One, Omni&lt;s&gt; &lt;/s&gt;Parser. These are technologies that are good at figuring out which agent to use for a given task. So a clinician who’s used to working with other specialists, like a radiologist, a pathologist, a surgeon, they can now also consult these specialist agents who are experts in their domain and there’s shared memory across the agents.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;There’s turn taking, there’s negotiation between the agents. So, there’s this really interesting system that’s emerging. And again, this is all possible to be used through Teams. And there’s some great extensibility as well. We’ve been talking about that and working on some cool tools.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SALIGRAMA:&lt;/strong&gt; Yeah. Yeah. No, I think if I have to geek out a little bit on how all this agent tech orchestrations are coming up, like I’ve been in software engineering for decades, it’s kind of a next version of distributed systems where you have these services that talk to each other. It’s a more natural way because LLMs are giving these natural ways instead of a structured API ways of conversing. We have these agents which can naturally understand how to talk to each other. Right. So this is like the next evolution of our systems now. And the way we’re packaging all of this is multiple ways based on all the standards and innovation that’s happening in this space. So, first of all, we are building these agents that are very good at specific tasks, like, Will was saying like, a trial matching agent or patient timeline agents.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So, we take all of these, and then we package it in a workflow and an orchestration. We use the standard, some of these coming from research. The Semantic Kernel, the Magentic-One. And then, all of these also allow us to extend these agents with custom agents that can be plugged in. So, we are open sourcing the entire agent orchestration in AI Foundry templates, so that developers can extend their own agents, and make their own workflows out of it. So, a lot of cool innovation happening to apply this technology to specific scenarios and workflows.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;Well, I was going to ask you, like, so as part of that extension. So, like, you know, folks can say, hey, I have maybe a really specific part of my workflow that I want to use some agents for, maybe one of the agents that can do PubMed literature search, for example. But then there’s also agents that, come in from the outside, you know, sort of like I could, I can imagine a software company or AI company that has a built-in agent that plugs in as well.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SALIGRAMA: &lt;/strong&gt;Yeah. Yeah, absolutely. So, you can bring your own agent. And then we have these, standard ways of communicating with agents and integrating with the orchestration language so you can bring your own agent and extend this health care agent, agent orchestrator to your own needs.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;I can just think of, like, in a group chat, like a bunch of different specialist agents. And I really would want an orchestrator to help find the right tool, to your point earlier, because I’m guessing this ecosystem is going to expand quickly. Yeah. And I may not know which tool is best for which question. I just want to ask the question. Right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SALIGRAMA: &lt;/strong&gt;Yeah. Yeah.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON: &lt;/strong&gt;Well, I think to that point to I mean, you said an important point here, which is tools, and these are not necessarily just AI tools. Right? I mean, we’ve known this for a while, right? LLMS are not very good at math, but you can have it use a calculator and then it works very well. And you know you guys both brought up the universal medical abstraction a couple times.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And one of the things that I find so powerful about that is we’ve long had this vision within the precision health community that we should be able to have a learning hospital system. We should be able to actually learn from the actual real clinical experiences that are happening every day, so that we can stop practicing medicine based off averages.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;There’s a lot of work that’s gone on for the last 20 years about how to actually do causal inference. That’s not an AI question. That’s a statistical question. The bottleneck, the reason why we haven’t been able to do that is because most of that information is locked up in unstructured text. And these other tools need essentially a table.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And so now you can decompose this problem, say, well, what if I can use AI not to get to the causal answer, but to just structure the information. So now I can put it into the causal inference tool. And these sorts of patterns I think again become very, not just powerful for a programmer, but they start pulling together different specialties. And I think we’ll really see an acceleration, really, of collaboration across disciplines because of this.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON: &lt;/strong&gt;So, when I joined Microsoft Research 18 years ago, I was doing work in computational biology. And I would always have to answer the question: why is Microsoft in biomedicine? And I would always kind of joke saying, well, it is. We sell Office and Windows to every health&lt;s&gt; &lt;/s&gt;care system in the world. We’re already in the space. And it really struck me to now see that we’ve actually come full circle. And now you can actually connect in Teams, Word, PowerPoint, which are these tools that everybody uses every day, but they’re actually now specialize-able through these agents. Can you guys talk a little bit about what that looks like from a developer perspective? How can provider groups actually start playing with this and see this come to life?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SALIGRAMA: &lt;/strong&gt;A lot of healthcare organizations already use Microsoft productivity tools, as you mentioned. So, they asked the developers, build these agents, and use our healthcare orchestrations, to plug in these agents and expose these in these productivity tools. They will get access to all these healthcare workers. So the healthcare agent orchestrator we have today integrates with Microsoft Teams, and it showcases an example of how you can at (@) mention these agents and talk to them like you were talking to another person in a Teams chat. And then it also provides examples of these agents and how they can use these productivity tools. One of the examples we have there is how they can summarize the assessments of this whole chat into a Word Doc, or even convert that into a PowerPoint presentation, for later on.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON:&lt;/strong&gt; One of the things that has struck me is how easy it is to do. I mean, Will, I don’t know if you’ve worked with folks that have gone from 0 to 60, like, how fast? What does that look like?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;GUYMAN: &lt;/strong&gt;Yeah, it’s funny for us, the technology to transfer all this context into a Word Document or PowerPoint presentation for a doctor to take to a meeting is relatively straightforward compared to the complicated clinical trial matching multimodal processing. The feedback has been tremendous in terms of, wow, that saves so much time to have this organized report that then I can show up to meeting with and the agents can come with me to that meeting because they’re literally having a Teams meeting, often with other human specialists. And the agents can be there and ask and answer questions and fact check and source all the right information on the fly. So, there’s a nice integration into these existing tools.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN:&lt;/strong&gt; We worked with several different centers just to kind of understand, you know, where this might be useful. And, like, as I think we talked about before, the ideas that we’ve come up with again, this is a great one because it’s complex. It’s kind of hairy. There’s a lot of things happening under the hood that don’t necessarily require a medical license to do, right, to prepare for a tumor board and to organize data. But, it’s fascinating, actually. So, you know, folks have come up with ideas of, could I have an agent that can operate an MRI machine, and I can ask the agent to change some parameters or redo a protocol. We thought that was a pretty powerful use case. We’ve had others that have just said, you know, I really want to have a specific agent that’s able to kind of act like deep research does for the consumer side, but based on the context of my patient, so that it can search all the literature and pull the data in the papers that are relevant to this case. And the list goes on and on from operations all the way to clinical, you know, sort of decision making at some level. And I think that the research community that’s going to sprout around this will help us, guide us, I guess, to see what is the most high-impact use cases. Where is this effective? And maybe where it’s not effective.&lt;/p&gt;



&lt;p&gt;But to me, the part that makes me so, I guess excited about this is just that I don’t have to think about, okay, well, then we have to figure out Health IT. Because it’s always, you know, we always have great ideas and research, and it always feels like there’s such a huge chasm to get it in front of the health care workers that might want to test this out. And it feels like, again, this productivity tool use case again with the enterprise security, the possibility for bringing in third parties to contribute really does feel like it’s a new surface area for innovation.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON: &lt;/strong&gt;Yeah, I love that. Look. Let me end by putting you all on the spot. So, in three years, multimodal agents will do what? Matt, I’ll start with you.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;I am convinced that it’s going to save massive amount of time before it saves many lives.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;RUNDE: &lt;/strong&gt;I’ll focus on the patient care journey and diagnostic journey. I think it will kind of transform that process for the patient itself and shorten that process.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;GUYMAN: &lt;/strong&gt;Yeah, I think we’ve seen already papers recently showing that different modalities surfaced complementary information. And so we’ll see kind of this AI and these agents becoming an essential companion to the physician, surfacing insights that would have been overlooked otherwise.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SALIGRAMA: &lt;/strong&gt;And similar to what you guys were saying, agents will become important assistants to healthcare workers, reducing a lot of documentation and workflow, excess work they have to do.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON: &lt;/strong&gt;I love that. And I guess for my part, I think really what we’re going to see is a massive unleash of creativity. We’ve had a lot of folks that have been innovating in this space, but they haven’t had a way to actually get it into the hands of early adopters. And I think we’re going to see that really lead to an explosion of creativity across the ecosystem.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;So, where do we get started? Like where are the developers who are listening to this? The folks that are at, you know, labs, research labs and developing health care solutions. Where do they go to get started with the Foundry, the models we’ve talked about, the healthcare agent orchestrator. Where do they go?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;GUYMAN&lt;/strong&gt;: So AI.azure.com is the AI Foundry. It’s a website you can go as a developer. You can sign in with your Azure subscription, get your Azure account, your own VM, all that stuff. And you have an agent catalog, the model catalog. You can start from there. There is documentation and templates that you can then deploy to Teams or other applications.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN:&lt;/strong&gt; And tutorials are coming. Right. We have recordings of tutorials.&amp;nbsp;We’ll have Hackathons, some sessions and then more to come. Yeah, we’re really excited.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LUNGREN: &lt;/strong&gt;Thank you so much, guys for joining us.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CARLSON:&lt;/strong&gt; Yes. Yeah. Thanks.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SALIGRAMA:&lt;/strong&gt; Thanks for having us.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&amp;nbsp;&lt;/p&gt;

				&lt;/span&gt;
			&lt;/div&gt;
			&lt;button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button"&gt;
				Show more			&lt;/button&gt;
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/podcast/collaborators-healthcare-innovation-to-impact/</guid><pubDate>Tue, 20 May 2025 20:39:01 +0000</pubDate></item><item><title>nanoVLM: The simplest repository to train your VLM in pure PyTorch (Hugging Face - Blog)</title><link>https://huggingface.co/blog/nanovlm</link><description>&lt;!-- HTML_TAG_START --&gt;
&lt;strong&gt;nanoVLM&lt;/strong&gt; is the &lt;em&gt;simplest&lt;/em&gt; way to get started with
&lt;strong&gt;training&lt;/strong&gt; your very own Vision Language Model (VLM) using pure PyTorch. It is lightweight &lt;em&gt;toolkit&lt;/em&gt;
which allows you to launch a VLM training on a free tier colab notebook.
&lt;blockquote&gt;
&lt;p&gt;We were inspired by Andrej Karpathy’s nanoGPT, and provide a similar project for the vision domain.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;At its heart, nanoVLM is a &lt;strong&gt;toolkit&lt;/strong&gt; that helps you build and train a model that can understand both
images and text, and then generate text based on that. The beauty of nanoVLM lies in its &lt;em&gt;simplicity&lt;/em&gt;.
The entire codebase is intentionally kept &lt;em&gt;minimal&lt;/em&gt; and &lt;em&gt;readable&lt;/em&gt;, making it perfect for beginners or
anyone who wants to peek under the hood of VLMs without getting overwhelmed.&lt;/p&gt;
&lt;p&gt;In this blog post, we cover the core ideas behind the project and provide a simple way to interact
with the repository. We not only go into the details of the project but also encapsulate all of it
so that you can quickly get started.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Table of contents:
	&lt;/span&gt;
&lt;/h2&gt;

&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		TL;DR
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;You can start training a Vision Language Model using our nanoVLM toolkit by following these steps:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;
git &lt;span class="hljs-built_in"&gt;clone&lt;/span&gt; https://github.com/huggingface/nanoVLM.git


python train.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is a Colab notebook
that will help you launch a training run with no local setup required!&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		What is a Vision Language Model?
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;As the name suggests, a Vision Language Model (VLM) is a multi-modal model that processes two
modalities: vision and text. These models typically take images and/or text as input and generate text as output.&lt;/p&gt;
&lt;p&gt;Generating text (output) conditioned on the understanding of images and texts (inputs) is a powerful paradigm.
It enables a wide range of applications, from image captioning and object detection to answering
questions about visual content (as shown in the table below). One thing to note is that nanoVLM
focuses only on Visual Question Answering as the training objective.&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td rowspan="4"&gt;&lt;img alt="an image of a cat" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/nanovlm/cat.jpg" width="200" /&gt;&lt;/td&gt;
    &lt;td&gt;Caption the image&lt;/td&gt;
    &lt;td&gt;Two cats lying down on a bed with remotes near them&lt;/td&gt;
    &lt;td&gt;Captioning&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Detect the objects in the image&lt;/td&gt;
    &lt;td&gt;&lt;code&gt;&amp;lt;locxx&amp;gt;&amp;lt;locxx&amp;gt;&amp;lt;locxx&amp;gt;&amp;lt;locxx&amp;gt;&lt;/code&gt;&lt;/td&gt;
    &lt;td&gt;Object Detection&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Segment the objects in the image&lt;/td&gt;
    &lt;td&gt;&lt;code&gt;&amp;lt;segxx&amp;gt;&amp;lt;segxx&amp;gt;&amp;lt;segxx&amp;gt;&lt;/code&gt;&lt;/td&gt;
    &lt;td&gt;Semantic Segmentation&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;How many cats are in the image?&lt;/td&gt;
    &lt;td&gt;2&lt;/td&gt;
    &lt;td&gt;Visual Question Answering&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;blockquote class="tip"&gt;
&lt;p&gt;If you are interested in learning more about VLMs, we strongly recommend reading our latest blog on the topic: Vision Language Models (Better, Faster, Stronger)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Working with the repository
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;"Talk is cheap, show me the code" - Linus Torvalds&lt;/p&gt;
&lt;p&gt;In this section, we’ll guide you through the codebase. It’s helpful to keep a
tab open for reference as you follow along.&lt;/p&gt;
&lt;p&gt;Below is the folder structure of our repository. We have removed helper files for brevity.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;.
├── data
│   ├── collators.py
│   ├── datasets.py
│   └── processors.py
├── generate.py
├── models
│   ├── config.py
│   ├── language_model.py
│   ├── modality_projector.py
│   ├── utils.py
│   ├── vision_language_model.py
│   └── vision_transformer.py
└── train.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Architecture
	&lt;/span&gt;
&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;.
├── data
│   └── ...
├── models      
│   └── ...
└── train.py     
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We model nanoVLM after two well known and widely used architectures. Our vision backbone
(&lt;code&gt;models/vision_transformer.py&lt;/code&gt;) is the standard vision transformer, more specifically Google’s
SigLIP vision encoder. Our language
backbone follows the Llama 3 architecture.&lt;/p&gt;
&lt;p&gt;The vision and text modalities are &lt;em&gt;aligned&lt;/em&gt; using a Modality Projection module. This module takes the
image embeddings produced by the vision backbone as input, and transforms them into embeddings
compatible with the text embeddings from the embedding layer of the language model. These embeddings
are then concatenated and fed into the language decoder. The Modality Projection module consists of a
pixel shuffle operation followed by a linear layer.&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th align="center"&gt;&lt;img alt="diagram of the model architecture" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/nanovlm/architecture.png" /&gt;&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td align="center"&gt;The architecture of the model (Source: Authors)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Pixel shuffle reduces the number of image tokens, which helps
reduce computational cost and speeds up training, especially for transformer-based language decoders
which are sensitive to input length. The figure below demonstrates the concept.&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th align="center"&gt;&lt;img alt="diagram of pixel shuffle" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/nanovlm/pixel-shuffle.png" /&gt;&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td align="center"&gt;Pixel Shuffle Visualized (Source: Authors)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;All the files are very lightweight and well documented. We highly encourage you to check them out
individually to get a better understanding of the implementation details (&lt;code&gt;models/xxx.py&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;While training, we use the following pre-trained backbone weights:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Vision backbone: &lt;code&gt;google/siglip-base-patch16-224&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;Language backbone: &lt;code&gt;HuggingFaceTB/SmolLM2-135M&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;One could also swap out the backbones with other variants of SigLIP/SigLIP 2 (for the vision backbone) and SmolLM2 (for the language backbone).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Train your own VLM
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Now that we are familiar with the architecture, let's shift gears and talk about how to train your own Vision Language Model using &lt;code&gt;train.py&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;.
├── data
│   └── ...
├── models
│   └── ...
└── train.py     
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can kick off training with:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;python train.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This script is your one-stop shop for the entire training pipeline, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dataset loading and preprocessing  &lt;/li&gt;
&lt;li&gt;Model initialization  &lt;/li&gt;
&lt;li&gt;Optimization and logging&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Configuration&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Before anything else, the script loads two configuration classes from &lt;code&gt;models/config.py&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;TrainConfig&lt;/code&gt;: Configuration parameters useful for training, like learning rates, checkpoint paths, etc.  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;VLMConfig&lt;/code&gt;: The configuration parameters used to initialize the VLM, like hidden dimensions, number of attention heads, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Data Loading&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;At the heart of the data pipeline is the &lt;code&gt;get_dataloaders&lt;/code&gt; function. It:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Loads datasets via Hugging Face’s &lt;code&gt;load_dataset&lt;/code&gt; API.  &lt;/li&gt;
&lt;li&gt;Combines and shuffles multiple datasets (if provided).  &lt;/li&gt;
&lt;li&gt;Applies a train/val split via indexing.  &lt;/li&gt;
&lt;li&gt;Wraps them in custom datasets (&lt;code&gt;VQADataset&lt;/code&gt;, &lt;code&gt;MMStarDataset&lt;/code&gt;) and collators (&lt;code&gt;VQACollator&lt;/code&gt;, &lt;code&gt;MMStarCollator&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class="tip"&gt;
&lt;p&gt;A helpful flag here is &lt;code&gt;data_cutoff_idx&lt;/code&gt;, useful for debugging on small subsets.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Model Initialization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The model is built via the &lt;code&gt;VisionLanguageModel&lt;/code&gt; class. If you're resuming from a checkpoint, it’s as easy as:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; models.vision_language_model &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; VisionLanguageModel

model = VisionLanguageModel.from_pretrained(model_path)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Otherwise, you get a freshly initialized model with optionally preloaded backbones for both vision and language.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Optimizer Setup: Two LRs&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Because the modality projector (&lt;code&gt;MP&lt;/code&gt;) is freshly initialized while the backbones are pre-trained, the
optimizer is split into two parameter groups, each with its own learning rate:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A higher LR for the MP  &lt;/li&gt;
&lt;li&gt;A smaller LR for the encoder/decoder stack&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This balance ensures the MP learns quickly while preserving knowledge in the vision and language backbones.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Training Loop&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This part is fairly standard but thoughtfully structured:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mixed precision is used with &lt;code&gt;torch.autocast&lt;/code&gt; to improve performance.  &lt;/li&gt;
&lt;li&gt;A cosine learning rate schedule with linear warmup is implemented via &lt;code&gt;get_lr&lt;/code&gt;.  &lt;/li&gt;
&lt;li&gt;Token throughput (tokens/sec) is logged per batch for performance monitoring.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Every 250 steps (configurable), the model is evaluated on the validation and &lt;code&gt;MMStar&lt;/code&gt; test datasets. If accuracy improves, the model is checkpointed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Logging &amp;amp; Monitoring&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If &lt;code&gt;log_wandb&lt;/code&gt; is enabled, training stats like &lt;code&gt;batch_loss&lt;/code&gt;,  &lt;code&gt;val_loss&lt;/code&gt;, &lt;code&gt;accuracy&lt;/code&gt;, and &lt;code&gt;tokens_per_second&lt;/code&gt;
are logged to Weights &amp;amp; Biases for real-time tracking.&lt;/p&gt;
&lt;p&gt;Runs are auto-named using metadata like sample size, batch size, epoch count, learning rates, and the date,
all handled by the helper &lt;code&gt;get_run_name&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Push to Hub&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Use the following to push the trained model to the Hub for others to find and test:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;model.save_pretrained(save_path)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can easily push them using:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;model.push_to_hub(&lt;span class="hljs-string"&gt;"hub/id"&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Run inference on a pre-trained model
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Using nanoVLM as the toolkit, we have trained a model and published it to Hub.
We have used the &lt;code&gt;google/siglip-base-patch16-224&lt;/code&gt; and &lt;code&gt;HuggingFaceTB/SmolLM2-135M&lt;/code&gt; as backbones. The model was
trained this for ~6h on a single H100 GPU on ~1.7M samples of the cauldron.&lt;/p&gt;
&lt;p&gt;This model isn't intended to compete with SoTA models, but rather to demystify the components and training process of VLMs.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;.
├── data
│   └── ...
├── generate.py     
├── models
│   └── ...
└── ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s run inference on the trained model using the &lt;code&gt;generate.py&lt;/code&gt; script. You can run the generation script using the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;python generate.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will use the default arguments and run the query “What is this?” on the image &lt;code&gt;assets/image.png&lt;/code&gt;.&lt;br /&gt;You can use this script on your own images and prompts like so:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;python generate.py --image path/to/image.png --prompt &lt;span class="hljs-string"&gt;"You prompt here"&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to visualize the heart of the script, it is just these lines:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;model = VisionLanguageModel.from_pretrained(source).to(device)
model.&lt;span class="hljs-built_in"&gt;eval&lt;/span&gt;()

tokenizer = get_tokenizer(model.cfg.lm_tokenizer)
image_processor = get_image_processor(model.cfg.vit_img_size)

template = &lt;span class="hljs-string"&gt;f"Question: &lt;span class="hljs-subst"&gt;{args.prompt}&lt;/span&gt; Answer:"&lt;/span&gt;
encoded = tokenizer.batch_encode_plus([template], return_tensors=&lt;span class="hljs-string"&gt;"pt"&lt;/span&gt;)
tokens = encoded[&lt;span class="hljs-string"&gt;"input_ids"&lt;/span&gt;].to(device)

img = Image.&lt;span class="hljs-built_in"&gt;open&lt;/span&gt;(args.image).convert(&lt;span class="hljs-string"&gt;"RGB"&lt;/span&gt;)
img_t = image_processor(img).unsqueeze(&lt;span class="hljs-number"&gt;0&lt;/span&gt;).to(device)

&lt;span class="hljs-built_in"&gt;print&lt;/span&gt;(&lt;span class="hljs-string"&gt;"\nInput:\n "&lt;/span&gt;, args.prompt, &lt;span class="hljs-string"&gt;"\n\nOutputs:"&lt;/span&gt;)
&lt;span class="hljs-keyword"&gt;for&lt;/span&gt; i &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; &lt;span class="hljs-built_in"&gt;range&lt;/span&gt;(args.generations):
    gen = model.generate(tokens, img_t, max_new_tokens=args.max_new_tokens)
    out = tokenizer.batch_decode(gen, skip_special_tokens=&lt;span class="hljs-literal"&gt;True&lt;/span&gt;)[&lt;span class="hljs-number"&gt;0&lt;/span&gt;]
    &lt;span class="hljs-built_in"&gt;print&lt;/span&gt;(&lt;span class="hljs-string"&gt;f"  &amp;gt;&amp;gt; Generation &lt;span class="hljs-subst"&gt;{i+&lt;span class="hljs-number"&gt;1&lt;/span&gt;}&lt;/span&gt;: &lt;span class="hljs-subst"&gt;{out}&lt;/span&gt;"&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We create the model and set it to &lt;code&gt;eval&lt;/code&gt;. Initialize the tokenizer, which tokenizes the text prompt,
and the image processor, which  is used to process the images. The next step is to process the inputs
and run &lt;code&gt;model.generate&lt;/code&gt; to generate the output text. Finally, decode the output using  &lt;code&gt;batch_decode&lt;/code&gt;.&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th align="center"&gt;Image&lt;/th&gt;
&lt;th align="center"&gt;Prompt&lt;/th&gt;
&lt;th align="center"&gt;Generation&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td align="center"&gt;&lt;img alt="image of a cat" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/nanovlm/cat.jpg" /&gt;&lt;/td&gt;
&lt;td align="center"&gt;What is this?&lt;/td&gt;
&lt;td align="center"&gt;In the picture I can see the pink color bed sheet. I can see two cats lying on the bed sheet.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;img alt="yoga" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/controlnet/yoga1.jpeg" /&gt;&lt;/td&gt;
&lt;td align="center"&gt;What is the woman doing?&lt;/td&gt;
&lt;td align="center"&gt;Here in the middle she is performing yoga&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;blockquote class="tip"&gt;
&lt;p&gt;If you want to run inference on the trained model in a UI interface, here is the Hugging Face Space for you to interact with the model. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Conclusion
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;In this blog post, we walked through what VLMs are, explored the architecture choices that power nanoVLM, and unpacked the training and inference workflows in detail.&lt;/p&gt;
&lt;p&gt;By keeping the codebase lightweight and readable, nanoVLM aims to serve as both a learning tool and a foundation you can build upon. Whether you’re looking to understand how multi-modal inputs are aligned, or you want to train a VLM on your own dataset, this repository gives you a head start.&lt;/p&gt;
&lt;p&gt;If you try it out, build on top of it, or just have questions we’d love to hear from you. Happy tinkering!&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		References
	&lt;/span&gt;
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;GitHub - huggingface/nanoVLM: The simplest, fastest repository for training/finetuning small-sized VLMs.&lt;/li&gt;
&lt;li&gt;Vision Language Models (Better, faster, stronger)&lt;/li&gt;
&lt;li&gt;Vision Language Models Explained&lt;/li&gt;
&lt;li&gt;A Dive into Vision-Language Models&lt;/li&gt;
&lt;li&gt;SmolVLM: Redefining small and efficient multimodal models&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;!-- HTML_TAG_START --&gt;
&lt;strong&gt;nanoVLM&lt;/strong&gt; is the &lt;em&gt;simplest&lt;/em&gt; way to get started with
&lt;strong&gt;training&lt;/strong&gt; your very own Vision Language Model (VLM) using pure PyTorch. It is lightweight &lt;em&gt;toolkit&lt;/em&gt;
which allows you to launch a VLM training on a free tier colab notebook.
&lt;blockquote&gt;
&lt;p&gt;We were inspired by Andrej Karpathy’s nanoGPT, and provide a similar project for the vision domain.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;At its heart, nanoVLM is a &lt;strong&gt;toolkit&lt;/strong&gt; that helps you build and train a model that can understand both
images and text, and then generate text based on that. The beauty of nanoVLM lies in its &lt;em&gt;simplicity&lt;/em&gt;.
The entire codebase is intentionally kept &lt;em&gt;minimal&lt;/em&gt; and &lt;em&gt;readable&lt;/em&gt;, making it perfect for beginners or
anyone who wants to peek under the hood of VLMs without getting overwhelmed.&lt;/p&gt;
&lt;p&gt;In this blog post, we cover the core ideas behind the project and provide a simple way to interact
with the repository. We not only go into the details of the project but also encapsulate all of it
so that you can quickly get started.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Table of contents:
	&lt;/span&gt;
&lt;/h2&gt;

&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		TL;DR
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;You can start training a Vision Language Model using our nanoVLM toolkit by following these steps:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;
git &lt;span class="hljs-built_in"&gt;clone&lt;/span&gt; https://github.com/huggingface/nanoVLM.git


python train.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is a Colab notebook
that will help you launch a training run with no local setup required!&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		What is a Vision Language Model?
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;As the name suggests, a Vision Language Model (VLM) is a multi-modal model that processes two
modalities: vision and text. These models typically take images and/or text as input and generate text as output.&lt;/p&gt;
&lt;p&gt;Generating text (output) conditioned on the understanding of images and texts (inputs) is a powerful paradigm.
It enables a wide range of applications, from image captioning and object detection to answering
questions about visual content (as shown in the table below). One thing to note is that nanoVLM
focuses only on Visual Question Answering as the training objective.&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td rowspan="4"&gt;&lt;img alt="an image of a cat" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/nanovlm/cat.jpg" width="200" /&gt;&lt;/td&gt;
    &lt;td&gt;Caption the image&lt;/td&gt;
    &lt;td&gt;Two cats lying down on a bed with remotes near them&lt;/td&gt;
    &lt;td&gt;Captioning&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Detect the objects in the image&lt;/td&gt;
    &lt;td&gt;&lt;code&gt;&amp;lt;locxx&amp;gt;&amp;lt;locxx&amp;gt;&amp;lt;locxx&amp;gt;&amp;lt;locxx&amp;gt;&lt;/code&gt;&lt;/td&gt;
    &lt;td&gt;Object Detection&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Segment the objects in the image&lt;/td&gt;
    &lt;td&gt;&lt;code&gt;&amp;lt;segxx&amp;gt;&amp;lt;segxx&amp;gt;&amp;lt;segxx&amp;gt;&lt;/code&gt;&lt;/td&gt;
    &lt;td&gt;Semantic Segmentation&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;How many cats are in the image?&lt;/td&gt;
    &lt;td&gt;2&lt;/td&gt;
    &lt;td&gt;Visual Question Answering&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;blockquote class="tip"&gt;
&lt;p&gt;If you are interested in learning more about VLMs, we strongly recommend reading our latest blog on the topic: Vision Language Models (Better, Faster, Stronger)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Working with the repository
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;"Talk is cheap, show me the code" - Linus Torvalds&lt;/p&gt;
&lt;p&gt;In this section, we’ll guide you through the codebase. It’s helpful to keep a
tab open for reference as you follow along.&lt;/p&gt;
&lt;p&gt;Below is the folder structure of our repository. We have removed helper files for brevity.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;.
├── data
│   ├── collators.py
│   ├── datasets.py
│   └── processors.py
├── generate.py
├── models
│   ├── config.py
│   ├── language_model.py
│   ├── modality_projector.py
│   ├── utils.py
│   ├── vision_language_model.py
│   └── vision_transformer.py
└── train.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Architecture
	&lt;/span&gt;
&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;.
├── data
│   └── ...
├── models      
│   └── ...
└── train.py     
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We model nanoVLM after two well known and widely used architectures. Our vision backbone
(&lt;code&gt;models/vision_transformer.py&lt;/code&gt;) is the standard vision transformer, more specifically Google’s
SigLIP vision encoder. Our language
backbone follows the Llama 3 architecture.&lt;/p&gt;
&lt;p&gt;The vision and text modalities are &lt;em&gt;aligned&lt;/em&gt; using a Modality Projection module. This module takes the
image embeddings produced by the vision backbone as input, and transforms them into embeddings
compatible with the text embeddings from the embedding layer of the language model. These embeddings
are then concatenated and fed into the language decoder. The Modality Projection module consists of a
pixel shuffle operation followed by a linear layer.&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th align="center"&gt;&lt;img alt="diagram of the model architecture" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/nanovlm/architecture.png" /&gt;&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td align="center"&gt;The architecture of the model (Source: Authors)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Pixel shuffle reduces the number of image tokens, which helps
reduce computational cost and speeds up training, especially for transformer-based language decoders
which are sensitive to input length. The figure below demonstrates the concept.&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th align="center"&gt;&lt;img alt="diagram of pixel shuffle" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/nanovlm/pixel-shuffle.png" /&gt;&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td align="center"&gt;Pixel Shuffle Visualized (Source: Authors)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;All the files are very lightweight and well documented. We highly encourage you to check them out
individually to get a better understanding of the implementation details (&lt;code&gt;models/xxx.py&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;While training, we use the following pre-trained backbone weights:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Vision backbone: &lt;code&gt;google/siglip-base-patch16-224&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;Language backbone: &lt;code&gt;HuggingFaceTB/SmolLM2-135M&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;One could also swap out the backbones with other variants of SigLIP/SigLIP 2 (for the vision backbone) and SmolLM2 (for the language backbone).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Train your own VLM
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Now that we are familiar with the architecture, let's shift gears and talk about how to train your own Vision Language Model using &lt;code&gt;train.py&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;.
├── data
│   └── ...
├── models
│   └── ...
└── train.py     
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can kick off training with:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;python train.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This script is your one-stop shop for the entire training pipeline, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dataset loading and preprocessing  &lt;/li&gt;
&lt;li&gt;Model initialization  &lt;/li&gt;
&lt;li&gt;Optimization and logging&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Configuration&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Before anything else, the script loads two configuration classes from &lt;code&gt;models/config.py&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;TrainConfig&lt;/code&gt;: Configuration parameters useful for training, like learning rates, checkpoint paths, etc.  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;VLMConfig&lt;/code&gt;: The configuration parameters used to initialize the VLM, like hidden dimensions, number of attention heads, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Data Loading&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;At the heart of the data pipeline is the &lt;code&gt;get_dataloaders&lt;/code&gt; function. It:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Loads datasets via Hugging Face’s &lt;code&gt;load_dataset&lt;/code&gt; API.  &lt;/li&gt;
&lt;li&gt;Combines and shuffles multiple datasets (if provided).  &lt;/li&gt;
&lt;li&gt;Applies a train/val split via indexing.  &lt;/li&gt;
&lt;li&gt;Wraps them in custom datasets (&lt;code&gt;VQADataset&lt;/code&gt;, &lt;code&gt;MMStarDataset&lt;/code&gt;) and collators (&lt;code&gt;VQACollator&lt;/code&gt;, &lt;code&gt;MMStarCollator&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class="tip"&gt;
&lt;p&gt;A helpful flag here is &lt;code&gt;data_cutoff_idx&lt;/code&gt;, useful for debugging on small subsets.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Model Initialization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The model is built via the &lt;code&gt;VisionLanguageModel&lt;/code&gt; class. If you're resuming from a checkpoint, it’s as easy as:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; models.vision_language_model &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; VisionLanguageModel

model = VisionLanguageModel.from_pretrained(model_path)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Otherwise, you get a freshly initialized model with optionally preloaded backbones for both vision and language.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Optimizer Setup: Two LRs&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Because the modality projector (&lt;code&gt;MP&lt;/code&gt;) is freshly initialized while the backbones are pre-trained, the
optimizer is split into two parameter groups, each with its own learning rate:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A higher LR for the MP  &lt;/li&gt;
&lt;li&gt;A smaller LR for the encoder/decoder stack&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This balance ensures the MP learns quickly while preserving knowledge in the vision and language backbones.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Training Loop&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This part is fairly standard but thoughtfully structured:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mixed precision is used with &lt;code&gt;torch.autocast&lt;/code&gt; to improve performance.  &lt;/li&gt;
&lt;li&gt;A cosine learning rate schedule with linear warmup is implemented via &lt;code&gt;get_lr&lt;/code&gt;.  &lt;/li&gt;
&lt;li&gt;Token throughput (tokens/sec) is logged per batch for performance monitoring.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Every 250 steps (configurable), the model is evaluated on the validation and &lt;code&gt;MMStar&lt;/code&gt; test datasets. If accuracy improves, the model is checkpointed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Logging &amp;amp; Monitoring&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If &lt;code&gt;log_wandb&lt;/code&gt; is enabled, training stats like &lt;code&gt;batch_loss&lt;/code&gt;,  &lt;code&gt;val_loss&lt;/code&gt;, &lt;code&gt;accuracy&lt;/code&gt;, and &lt;code&gt;tokens_per_second&lt;/code&gt;
are logged to Weights &amp;amp; Biases for real-time tracking.&lt;/p&gt;
&lt;p&gt;Runs are auto-named using metadata like sample size, batch size, epoch count, learning rates, and the date,
all handled by the helper &lt;code&gt;get_run_name&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Push to Hub&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Use the following to push the trained model to the Hub for others to find and test:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;model.save_pretrained(save_path)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can easily push them using:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;model.push_to_hub(&lt;span class="hljs-string"&gt;"hub/id"&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Run inference on a pre-trained model
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Using nanoVLM as the toolkit, we have trained a model and published it to Hub.
We have used the &lt;code&gt;google/siglip-base-patch16-224&lt;/code&gt; and &lt;code&gt;HuggingFaceTB/SmolLM2-135M&lt;/code&gt; as backbones. The model was
trained this for ~6h on a single H100 GPU on ~1.7M samples of the cauldron.&lt;/p&gt;
&lt;p&gt;This model isn't intended to compete with SoTA models, but rather to demystify the components and training process of VLMs.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;.
├── data
│   └── ...
├── generate.py     
├── models
│   └── ...
└── ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s run inference on the trained model using the &lt;code&gt;generate.py&lt;/code&gt; script. You can run the generation script using the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;python generate.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will use the default arguments and run the query “What is this?” on the image &lt;code&gt;assets/image.png&lt;/code&gt;.&lt;br /&gt;You can use this script on your own images and prompts like so:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;python generate.py --image path/to/image.png --prompt &lt;span class="hljs-string"&gt;"You prompt here"&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to visualize the heart of the script, it is just these lines:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;model = VisionLanguageModel.from_pretrained(source).to(device)
model.&lt;span class="hljs-built_in"&gt;eval&lt;/span&gt;()

tokenizer = get_tokenizer(model.cfg.lm_tokenizer)
image_processor = get_image_processor(model.cfg.vit_img_size)

template = &lt;span class="hljs-string"&gt;f"Question: &lt;span class="hljs-subst"&gt;{args.prompt}&lt;/span&gt; Answer:"&lt;/span&gt;
encoded = tokenizer.batch_encode_plus([template], return_tensors=&lt;span class="hljs-string"&gt;"pt"&lt;/span&gt;)
tokens = encoded[&lt;span class="hljs-string"&gt;"input_ids"&lt;/span&gt;].to(device)

img = Image.&lt;span class="hljs-built_in"&gt;open&lt;/span&gt;(args.image).convert(&lt;span class="hljs-string"&gt;"RGB"&lt;/span&gt;)
img_t = image_processor(img).unsqueeze(&lt;span class="hljs-number"&gt;0&lt;/span&gt;).to(device)

&lt;span class="hljs-built_in"&gt;print&lt;/span&gt;(&lt;span class="hljs-string"&gt;"\nInput:\n "&lt;/span&gt;, args.prompt, &lt;span class="hljs-string"&gt;"\n\nOutputs:"&lt;/span&gt;)
&lt;span class="hljs-keyword"&gt;for&lt;/span&gt; i &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; &lt;span class="hljs-built_in"&gt;range&lt;/span&gt;(args.generations):
    gen = model.generate(tokens, img_t, max_new_tokens=args.max_new_tokens)
    out = tokenizer.batch_decode(gen, skip_special_tokens=&lt;span class="hljs-literal"&gt;True&lt;/span&gt;)[&lt;span class="hljs-number"&gt;0&lt;/span&gt;]
    &lt;span class="hljs-built_in"&gt;print&lt;/span&gt;(&lt;span class="hljs-string"&gt;f"  &amp;gt;&amp;gt; Generation &lt;span class="hljs-subst"&gt;{i+&lt;span class="hljs-number"&gt;1&lt;/span&gt;}&lt;/span&gt;: &lt;span class="hljs-subst"&gt;{out}&lt;/span&gt;"&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We create the model and set it to &lt;code&gt;eval&lt;/code&gt;. Initialize the tokenizer, which tokenizes the text prompt,
and the image processor, which  is used to process the images. The next step is to process the inputs
and run &lt;code&gt;model.generate&lt;/code&gt; to generate the output text. Finally, decode the output using  &lt;code&gt;batch_decode&lt;/code&gt;.&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th align="center"&gt;Image&lt;/th&gt;
&lt;th align="center"&gt;Prompt&lt;/th&gt;
&lt;th align="center"&gt;Generation&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td align="center"&gt;&lt;img alt="image of a cat" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/nanovlm/cat.jpg" /&gt;&lt;/td&gt;
&lt;td align="center"&gt;What is this?&lt;/td&gt;
&lt;td align="center"&gt;In the picture I can see the pink color bed sheet. I can see two cats lying on the bed sheet.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;img alt="yoga" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/controlnet/yoga1.jpeg" /&gt;&lt;/td&gt;
&lt;td align="center"&gt;What is the woman doing?&lt;/td&gt;
&lt;td align="center"&gt;Here in the middle she is performing yoga&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;blockquote class="tip"&gt;
&lt;p&gt;If you want to run inference on the trained model in a UI interface, here is the Hugging Face Space for you to interact with the model. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Conclusion
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;In this blog post, we walked through what VLMs are, explored the architecture choices that power nanoVLM, and unpacked the training and inference workflows in detail.&lt;/p&gt;
&lt;p&gt;By keeping the codebase lightweight and readable, nanoVLM aims to serve as both a learning tool and a foundation you can build upon. Whether you’re looking to understand how multi-modal inputs are aligned, or you want to train a VLM on your own dataset, this repository gives you a head start.&lt;/p&gt;
&lt;p&gt;If you try it out, build on top of it, or just have questions we’d love to hear from you. Happy tinkering!&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		References
	&lt;/span&gt;
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;GitHub - huggingface/nanoVLM: The simplest, fastest repository for training/finetuning small-sized VLMs.&lt;/li&gt;
&lt;li&gt;Vision Language Models (Better, faster, stronger)&lt;/li&gt;
&lt;li&gt;Vision Language Models Explained&lt;/li&gt;
&lt;li&gt;A Dive into Vision-Language Models&lt;/li&gt;
&lt;li&gt;SmolVLM: Redefining small and efficient multimodal models&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/nanovlm</guid><pubDate>Wed, 21 May 2025 00:00:00 +0000</pubDate></item><item><title>Exploring Quantization Backends in Diffusers (Hugging Face - Blog)</title><link>https://huggingface.co/blog/diffusers-quantization</link><description>&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Derek Liu's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://huggingface.co/avatars/e5b8331c9a96cd96b679f38afd30422e.svg" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Marc Sun's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://cdn-avatars.huggingface.co/v1/production/uploads/63ce875d199b36f7552d4f07/NrTlnhUTAp0j0ZsMTsWIu.jpeg" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;




Large diffusion models like Flux (a flow-based text-to-image generation model) can create stunning images, but their size can be a hurdle, demanding significant memory and compute resources. Quantization offers a powerful solution, shrinking these models to make them more accessible without drastically compromising performance. But the big question always is: can you actually tell the difference in the final image?
&lt;p&gt;Before we dive into the technical details of how various quantization backends in Hugging Face Diffusers work, why not test your own perception?&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Spot The Quantized Model
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We created a setup where you can provide a prompt, and we generate results using both the original, high-precision model (e.g., Flux-dev in BF16) and several quantized versions (BnB 4-bit, BnB 8-bit). The generated images are then presented to you and your challenge is to identify which ones came from the quantized models.&lt;/p&gt;
&lt;p&gt;Try it out here or below!
&lt;/p&gt;
&lt;p&gt;Often, especially with 8-bit quantization, the differences are subtle and may not be noticeable without close inspection. More aggressive quantization like 4-bit or lower might be more noticeable, but the results can still be good, especially considering the massive memory savings. NF4 often gives the best trade-off though.&lt;/p&gt;
&lt;p&gt;Now, let's dive deeper.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Quantization Backends in Diffusers
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Building on our previous post, "Memory-efficient Diffusion Transformers with Quanto and Diffusers", this post explores the diverse quantization backends integrated directly into Hugging Face Diffusers. We'll examine how bitsandbytes, GGUF, torchao, Quanto and native FP8 support make large and powerful models more accessible, demonstrating their use with Flux.&lt;/p&gt;
&lt;p&gt;Before diving into the quantization backends, let's introduce the FluxPipeline (using the black-forest-labs/FLUX.1-dev checkpoint) and its components, which we'll be quantizing. Loading the full &lt;code&gt;FLUX.1-dev&lt;/code&gt; model in BF16 precision requires approximately 31.447 GB of memory. The main components are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Text Encoders (CLIP and T5):&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Function:&lt;/strong&gt; Process input text prompts. FLUX-dev uses CLIP for initial understanding and a larger T5 for nuanced comprehension and better text rendering.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory:&lt;/strong&gt; T5 - 9.52 GB; CLIP - 246 MB (in BF16)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transformer (Main Model - MMDiT):&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Function:&lt;/strong&gt; Core generative part (Multimodal Diffusion Transformer). Generates images in latent space from text embeddings. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory:&lt;/strong&gt; 23.8 GB (in BF16)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Variational Auto-Encoder (VAE):&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Function:&lt;/strong&gt; Translates images between pixel and latent space. Decodes generated latent representation to a pixel-based image.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory:&lt;/strong&gt; 168 MB (in BF16)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Focus of Quantization:&lt;/strong&gt; Examples will primarily focus on the &lt;code&gt;transformer&lt;/code&gt; and &lt;code&gt;text_encoder_2&lt;/code&gt; (T5) for the most substantial memory savings.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;prompts = [
    &lt;span class="hljs-string"&gt;"Baroque style, a lavish palace interior with ornate gilded ceilings, intricate tapestries, and dramatic lighting over a grand staircase."&lt;/span&gt;,
    &lt;span class="hljs-string"&gt;"Futurist style, a dynamic spaceport with sleek silver starships docked at angular platforms, surrounded by distant planets and glowing energy lines."&lt;/span&gt;,
    &lt;span class="hljs-string"&gt;"Noir style, a shadowy alleyway with flickering street lamps and a solitary trench-coated figure, framed by rain-soaked cobblestones and darkened storefronts."&lt;/span&gt;,
]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		bitsandbytes (BnB)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;bitsandbytes&lt;/code&gt; is a popular and user-friendly library for 8-bit and 4-bit quantization, widely used for LLMs and QLoRA fine-tuning. We can use it for transformer-based diffusion and flow models, too.&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;
      BF16&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
    &lt;td&gt;
      BnB 4-bit&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
    &lt;td&gt;
      BnB 8-bit&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td colspan="3"&gt;&lt;em&gt;Visual comparison of Flux-dev model outputs using BF16 (left), BnB 4-bit (center), and BnB 8-bit (right) quantization. (Click on an image to zoom) &lt;/em&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Precision&lt;/th&gt;
&lt;th&gt;Memory after loading&lt;/th&gt;
&lt;th&gt;Peak memory&lt;/th&gt;
&lt;th&gt;Inference time&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;BF16&lt;/td&gt;
&lt;td&gt;~31.447 GB&lt;/td&gt;
&lt;td&gt;36.166 GB&lt;/td&gt;
&lt;td&gt;12 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4-bit&lt;/td&gt;
&lt;td&gt;12.584 GB&lt;/td&gt;
&lt;td&gt;17.281 GB&lt;/td&gt;
&lt;td&gt;12 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8-bit&lt;/td&gt;
&lt;td&gt;19.273 GB&lt;/td&gt;
&lt;td&gt;24.432 GB&lt;/td&gt;
&lt;td&gt;27 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;sub&gt;All benchmarks performed on 1x NVIDIA H100 80GB GPU&lt;/sub&gt;&lt;/p&gt;
&lt;details&gt;
Example (Flux-dev with BnB 4-bit):


&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;import&lt;/span&gt; torch
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; diffusers &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; FluxPipeline
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; diffusers &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; BitsAndBytesConfig &lt;span class="hljs-keyword"&gt;as&lt;/span&gt; DiffusersBitsAndBytesConfig
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; diffusers.quantizers &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; PipelineQuantizationConfig
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; transformers &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; BitsAndBytesConfig &lt;span class="hljs-keyword"&gt;as&lt;/span&gt; TransformersBitsAndBytesConfig

model_id = &lt;span class="hljs-string"&gt;"black-forest-labs/FLUX.1-dev"&lt;/span&gt;

pipeline_quant_config = PipelineQuantizationConfig(
    quant_mapping={
        &lt;span class="hljs-string"&gt;"transformer"&lt;/span&gt;: DiffusersBitsAndBytesConfig(load_in_4bit=&lt;span class="hljs-literal"&gt;True&lt;/span&gt;, bnb_4bit_quant_type=&lt;span class="hljs-string"&gt;"nf4"&lt;/span&gt;, bnb_4bit_compute_dtype=torch.bfloat16),
        &lt;span class="hljs-string"&gt;"text_encoder_2"&lt;/span&gt;: TransformersBitsAndBytesConfig(load_in_4bit=&lt;span class="hljs-literal"&gt;True&lt;/span&gt;, bnb_4bit_quant_type=&lt;span class="hljs-string"&gt;"nf4"&lt;/span&gt;, bnb_4bit_compute_dtype=torch.bfloat16),
    }
)

pipe = FluxPipeline.from_pretrained(
    model_id,
    quantization_config=pipeline_quant_config,
    torch_dtype=torch.bfloat16
)
pipe.to(&lt;span class="hljs-string"&gt;"cuda"&lt;/span&gt;)

prompt = &lt;span class="hljs-string"&gt;"Baroque style, a lavish palace interior with ornate gilded ceilings, intricate tapestries, and dramatic lighting over a grand staircase."&lt;/span&gt;
pipe_kwargs = {
    &lt;span class="hljs-string"&gt;"prompt"&lt;/span&gt;: prompt,
    &lt;span class="hljs-string"&gt;"height"&lt;/span&gt;: &lt;span class="hljs-number"&gt;1024&lt;/span&gt;,
    &lt;span class="hljs-string"&gt;"width"&lt;/span&gt;: &lt;span class="hljs-number"&gt;1024&lt;/span&gt;,
    &lt;span class="hljs-string"&gt;"guidance_scale"&lt;/span&gt;: &lt;span class="hljs-number"&gt;3.5&lt;/span&gt;,
    &lt;span class="hljs-string"&gt;"num_inference_steps"&lt;/span&gt;: &lt;span class="hljs-number"&gt;50&lt;/span&gt;,
    &lt;span class="hljs-string"&gt;"max_sequence_length"&lt;/span&gt;: &lt;span class="hljs-number"&gt;512&lt;/span&gt;,
}


&lt;span class="hljs-built_in"&gt;print&lt;/span&gt;(&lt;span class="hljs-string"&gt;f"Pipeline memory usage: &lt;span class="hljs-subst"&gt;{torch.cuda.max_memory_reserved() / &lt;span class="hljs-number"&gt;1024&lt;/span&gt;**&lt;span class="hljs-number"&gt;3&lt;/span&gt;:&lt;span class="hljs-number"&gt;.3&lt;/span&gt;f}&lt;/span&gt; GB"&lt;/span&gt;)

image = pipe(
    **pipe_kwargs, generator=torch.manual_seed(&lt;span class="hljs-number"&gt;0&lt;/span&gt;),
).images[&lt;span class="hljs-number"&gt;0&lt;/span&gt;]

&lt;span class="hljs-built_in"&gt;print&lt;/span&gt;(&lt;span class="hljs-string"&gt;f"Pipeline memory usage: &lt;span class="hljs-subst"&gt;{torch.cuda.max_memory_reserved() / &lt;span class="hljs-number"&gt;1024&lt;/span&gt;**&lt;span class="hljs-number"&gt;3&lt;/span&gt;:&lt;span class="hljs-number"&gt;.3&lt;/span&gt;f}&lt;/span&gt; GB"&lt;/span&gt;)

image.save(&lt;span class="hljs-string"&gt;"flux-dev_bnb_4bit.png"&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; When using &lt;code&gt;PipelineQuantizationConfig&lt;/code&gt; with &lt;code&gt;bitsandbytes&lt;/code&gt;, you need to import &lt;code&gt;DiffusersBitsAndBytesConfig&lt;/code&gt; from &lt;code&gt;diffusers&lt;/code&gt; and &lt;code&gt;TransformersBitsAndBytesConfig&lt;/code&gt; from &lt;code&gt;transformers&lt;/code&gt; separately. This is because these components originate from different libraries. If you prefer a simpler setup without managing these distinct imports, you can use an alternative approach for pipeline-level quantization, an example of this method is in the Diffusers documentation on Pipeline-level quantization.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/details&gt;

&lt;p&gt;For more information check out the bitsandbytes docs.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		torchao
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;torchao&lt;/code&gt; is a PyTorch-native library for architecture optimization, offering quantization, sparsity, and custom data types, designed for compatibility with &lt;code&gt;torch.compile&lt;/code&gt; and FSDP. Diffusers supports a wide range of &lt;code&gt;torchao&lt;/code&gt;'s exotic data types, enabling fine-grained control over model optimization.&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;
      int4_weight_only&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
    &lt;td&gt;
      int8_weight_only&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
    &lt;td&gt;
      float8_weight_only&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td colspan="3"&gt;&lt;em&gt;Visual comparison of Flux-dev model outputs using torchao int4_weight_only (left), int8_weight_only (center), and float8_weight_only (right) quantization. (Click on an image to zoom)&lt;/em&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;torchao Precision&lt;/th&gt;
&lt;th&gt;Memory after loading&lt;/th&gt;
&lt;th&gt;Peak memory&lt;/th&gt;
&lt;th&gt;Inference time&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;int4_weight_only&lt;/td&gt;
&lt;td&gt;10.635 GB&lt;/td&gt;
&lt;td&gt;14.654 GB&lt;/td&gt;
&lt;td&gt;109 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;int8_weight_only&lt;/td&gt;
&lt;td&gt;17.020 GB&lt;/td&gt;
&lt;td&gt;21.482 GB&lt;/td&gt;
&lt;td&gt;15 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;float8_weight_only&lt;/td&gt;
&lt;td&gt;17.016 GB&lt;/td&gt;
&lt;td&gt;21.488 GB&lt;/td&gt;
&lt;td&gt;15 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;details&gt;
Example (Flux-dev with torchao INT8 weight-only):

&lt;pre&gt;&lt;code class="language-diff"&gt;@@
&lt;span class="hljs-deletion"&gt;- from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+ from diffusers import TorchAoConfig as DiffusersTorchAoConfig&lt;/span&gt;

&lt;span class="hljs-deletion"&gt;- from transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+ from transformers import TorchAoConfig as TransformersTorchAoConfig&lt;/span&gt;
@@
pipeline_quant_config = PipelineQuantizationConfig(
    quant_mapping={
&lt;span class="hljs-deletion"&gt;-         "transformer": DiffusersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16),&lt;/span&gt;
&lt;span class="hljs-deletion"&gt;-         "text_encoder_2": TransformersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16),&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+         "transformer": DiffusersTorchAoConfig("int8_weight_only"),&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+         "text_encoder_2": TransformersTorchAoConfig("int8_weight_only"),&lt;/span&gt;
    }
)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;

&lt;details&gt;
Example (Flux-dev with torchao INT4 weight-only):

&lt;pre&gt;&lt;code class="language-diff"&gt;@@
&lt;span class="hljs-deletion"&gt;- from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+ from diffusers import TorchAoConfig as DiffusersTorchAoConfig&lt;/span&gt;

&lt;span class="hljs-deletion"&gt;- from transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+ from transformers import TorchAoConfig as TransformersTorchAoConfig&lt;/span&gt;
@@
pipeline_quant_config = PipelineQuantizationConfig(
    quant_mapping={
&lt;span class="hljs-deletion"&gt;-         "transformer": DiffusersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16),&lt;/span&gt;
&lt;span class="hljs-deletion"&gt;-         "text_encoder_2": TransformersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16),&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+         "transformer": DiffusersTorchAoConfig("int4_weight_only"),&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+         "text_encoder_2": TransformersTorchAoConfig("int4_weight_only"),&lt;/span&gt;
    }
)

pipe = FluxPipeline.from_pretrained(
    model_id,
    quantization_config=pipeline_quant_config,
    torch_dtype=torch.bfloat16,
&lt;span class="hljs-addition"&gt;+    device_map="balanced"&lt;/span&gt;
)
&lt;span class="hljs-deletion"&gt;- pipe.to("cuda")&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;

&lt;p&gt;For more information check out the torchao docs.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Quanto
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Quanto is a quantization library integrated with the Hugging Face ecosystem via the &lt;code&gt;optimum&lt;/code&gt; library.&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;
      INT4&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
    &lt;td&gt;
      INT8&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
    &lt;td&gt;
      FP8&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td colspan="3"&gt;&lt;em&gt;Visual comparison of Flux-dev model outputs using Quanto INT4 (left), INT8 (center), and FP8 (right) quantization. (Click on an image to zoom)&lt;/em&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;quanto Precision&lt;/th&gt;
&lt;th&gt;Memory after loading&lt;/th&gt;
&lt;th&gt;Peak memory&lt;/th&gt;
&lt;th&gt;Inference time&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;INT4&lt;/td&gt;
&lt;td&gt;12.254 GB&lt;/td&gt;
&lt;td&gt;16.139 GB&lt;/td&gt;
&lt;td&gt;109 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;INT8&lt;/td&gt;
&lt;td&gt;17.330 GB&lt;/td&gt;
&lt;td&gt;21.814 GB&lt;/td&gt;
&lt;td&gt;15 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FP8&lt;/td&gt;
&lt;td&gt;16.395 GB&lt;/td&gt;
&lt;td&gt;20.898 GB&lt;/td&gt;
&lt;td&gt;16 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;details&gt;
Example (Flux-dev with quanto INT8 weight-only):

&lt;pre&gt;&lt;code class="language-diff"&gt;@@
&lt;span class="hljs-deletion"&gt;- from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+ from diffusers import QuantoConfig as DiffusersQuantoConfig&lt;/span&gt;

&lt;span class="hljs-deletion"&gt;- from transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+ from transformers import QuantoConfig as TransformersQuantoConfig&lt;/span&gt;
@@
pipeline_quant_config = PipelineQuantizationConfig(
    quant_mapping={
&lt;span class="hljs-deletion"&gt;-         "transformer": DiffusersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16),&lt;/span&gt;
&lt;span class="hljs-deletion"&gt;-         "text_encoder_2": TransformersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16),&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+         "transformer": DiffusersQuantoConfig(weights_dtype="int8"),&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+         "text_encoder_2": TransformersQuantoConfig(weights_dtype="int8"),&lt;/span&gt;
    }
)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; At the time of writing, for float8 support with Quanto, you'll need &lt;code&gt;optimum-quanto&amp;lt;0.2.5&lt;/code&gt; and use quanto directly. We will be working on fixing this.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;details&gt;
Example (Flux-dev with quanto FP8 weight-only)

&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;import&lt;/span&gt; torch
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; diffusers &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; AutoModel, FluxPipeline
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; transformers &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; T5EncoderModel
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; optimum.quanto &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; freeze, qfloat8, quantize

model_id = &lt;span class="hljs-string"&gt;"black-forest-labs/FLUX.1-dev"&lt;/span&gt;

text_encoder_2 = T5EncoderModel.from_pretrained(
    model_id,
    subfolder=&lt;span class="hljs-string"&gt;"text_encoder_2"&lt;/span&gt;,
    torch_dtype=torch.bfloat16,
)

quantize(text_encoder_2, weights=qfloat8)
freeze(text_encoder_2)

transformer = AutoModel.from_pretrained(
      model_id,
      subfolder=&lt;span class="hljs-string"&gt;"transformer"&lt;/span&gt;,
      torch_dtype=torch.bfloat16,
)

quantize(transformer, weights=qfloat8)
freeze(transformer)

pipe = FluxPipeline.from_pretrained(
    model_id,
    transformer=transformer,
    text_encoder_2=text_encoder_2,
    torch_dtype=torch.bfloat16
).to(&lt;span class="hljs-string"&gt;"cuda"&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;

&lt;p&gt;For more information check out the Quanto docs.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		GGUF
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;GGUF is a file format popular in the llama.cpp community for storing quantized models.&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;
      Q2_k&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
    &lt;td&gt;
      Q4_1&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
    &lt;td&gt;
      Q8_0&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td colspan="3"&gt;&lt;em&gt;Visual comparison of Flux-dev model outputs using GGUF Q2_k (left), Q4_1 (center), and Q8_0 (right) quantization. (Click on an image to zoom)&lt;/em&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;GGUF Precision&lt;/th&gt;
&lt;th&gt;Memory after loading&lt;/th&gt;
&lt;th&gt;Peak memory&lt;/th&gt;
&lt;th&gt;Inference time&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;Q2_k&lt;/td&gt;
&lt;td&gt;13.264 GB&lt;/td&gt;
&lt;td&gt;17.752 GB&lt;/td&gt;
&lt;td&gt;26 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Q4_1&lt;/td&gt;
&lt;td&gt;16.838 GB&lt;/td&gt;
&lt;td&gt;21.326 GB&lt;/td&gt;
&lt;td&gt;23 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Q8_0&lt;/td&gt;
&lt;td&gt;21.502 GB&lt;/td&gt;
&lt;td&gt;25.973 GB&lt;/td&gt;
&lt;td&gt;15 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;details&gt;
Example (Flux-dev with GGUF Q4_1)

&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;import&lt;/span&gt; torch
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; diffusers &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; FluxPipeline, FluxTransformer2DModel, GGUFQuantizationConfig

model_id = &lt;span class="hljs-string"&gt;"black-forest-labs/FLUX.1-dev"&lt;/span&gt;


ckpt_path = &lt;span class="hljs-string"&gt;"https://huggingface.co/city96/FLUX.1-dev-gguf/resolve/main/flux1-dev-Q4_1.gguf"&lt;/span&gt;

transformer = FluxTransformer2DModel.from_single_file(
    ckpt_path,
    quantization_config=GGUFQuantizationConfig(compute_dtype=torch.bfloat16),
    torch_dtype=torch.bfloat16,
)

pipe = FluxPipeline.from_pretrained(
    model_id,
    transformer=transformer,
    torch_dtype=torch.bfloat16,
)
pipe.to(&lt;span class="hljs-string"&gt;"cuda"&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;

&lt;p&gt;For more information check out the GGUF docs.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		FP8 Layerwise Casting (&lt;code&gt;enable_layerwise_casting&lt;/code&gt;)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;FP8 Layerwise Casting is a memory optimization technique. It works by storing the model's weights in the compact FP8 (8-bit floating point) format, which uses roughly half the memory of standard FP16 or BF16 precision.
Just before a layer performs its calculations, its weights are dynamically cast up to a higher compute precision (like FP16/BF16). Immediately afterward, the weights are cast back down to FP8 for efficient storage. This approach works because the core computations retain high precision, and layers particularly sensitive to quantization (like normalization) are typically skipped. This technique can also be combined with group offloading for further memory savings.&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;
      FP8 (e4m3)&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;em&gt;Visual output of Flux-dev model using FP8 Layerwise Casting (e4m3) quantization.&lt;/em&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;precision&lt;/th&gt;
&lt;th&gt;Memory after loading&lt;/th&gt;
&lt;th&gt;Peak memory&lt;/th&gt;
&lt;th&gt;Inference time&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;FP8 (e4m3)&lt;/td&gt;
&lt;td&gt;23.682 GB&lt;/td&gt;
&lt;td&gt;28.451 GB&lt;/td&gt;
&lt;td&gt;13 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;import&lt;/span&gt; torch
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; diffusers &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; AutoModel, FluxPipeline

model_id = &lt;span class="hljs-string"&gt;"black-forest-labs/FLUX.1-dev"&lt;/span&gt;

transformer = AutoModel.from_pretrained(
    model_id,
    subfolder=&lt;span class="hljs-string"&gt;"transformer"&lt;/span&gt;,
    torch_dtype=torch.bfloat16
)
transformer.enable_layerwise_casting(storage_dtype=torch.float8_e4m3fn, compute_dtype=torch.bfloat16)

pipe = FluxPipeline.from_pretrained(model_id, transformer=transformer, torch_dtype=torch.bfloat16)
pipe.to(&lt;span class="hljs-string"&gt;"cuda"&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For more information check out the Layerwise casting docs.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Combining with More Memory Optimizations and torch.compile
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Most of these quantization backends can be combined with the memory optimization techniques offered in Diffusers. Let's explore CPU offloading, group offloading, and &lt;code&gt;torch.compile&lt;/code&gt;. You can learn more about these techniques in the Diffusers documentation.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; At the time of writing, bnb + &lt;code&gt;torch.compile&lt;/code&gt; also works if bnb is installed from source and using pytorch nightly or with fullgraph=False.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;details&gt;
Example (Flux-dev with BnB 4-bit + enable_model_cpu_offload):

&lt;pre&gt;&lt;code class="language-diff"&gt;import torch
from diffusers import FluxPipeline
from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig
from diffusers.quantizers import PipelineQuantizationConfig
from transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig

model_id = "black-forest-labs/FLUX.1-dev"

pipeline_quant_config = PipelineQuantizationConfig(
    quant_mapping={
        "transformer": DiffusersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16),
        "text_encoder_2": TransformersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16),
    }
)

pipe = FluxPipeline.from_pretrained(
    model_id,
    quantization_config=pipeline_quant_config,
    torch_dtype=torch.bfloat16
)
&lt;span class="hljs-deletion"&gt;- pipe.to("cuda")&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+ pipe.enable_model_cpu_offload()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;

&lt;p&gt;&lt;strong&gt;Model CPU Offloading (&lt;code&gt;enable_model_cpu_offload&lt;/code&gt;)&lt;/strong&gt;: This method moves entire model components (like the UNet, text encoders, or VAE) between the CPU and GPU during the inference pipeline. It offers substantial VRAM savings and is generally faster than more granular offloading because it involves fewer, larger data transfers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;bnb + &lt;code&gt;enable_model_cpu_offload&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Precision&lt;/th&gt;
&lt;th&gt;Memory after loading&lt;/th&gt;
&lt;th&gt;Peak memory&lt;/th&gt;
&lt;th&gt;Inference time&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;4-bit&lt;/td&gt;
&lt;td&gt;12.383 GB&lt;/td&gt;
&lt;td&gt;12.383 GB&lt;/td&gt;
&lt;td&gt;17 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8-bit&lt;/td&gt;
&lt;td&gt;19.182 GB&lt;/td&gt;
&lt;td&gt;23.428 GB&lt;/td&gt;
&lt;td&gt;27 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;details&gt;
Example (Flux-dev with fp8 layerwise casting + group offloading):

&lt;pre&gt;&lt;code class="language-diff"&gt;import torch
from diffusers import FluxPipeline, AutoModel

model_id = "black-forest-labs/FLUX.1-dev"

transformer = AutoModel.from_pretrained(
    model_id,
    subfolder="transformer",
    torch_dtype=torch.bfloat16,
    # device_map="cuda"
)
transformer.enable_layerwise_casting(storage_dtype=torch.float8_e4m3fn, compute_dtype=torch.bfloat16)
&lt;span class="hljs-addition"&gt;+ transformer.enable_group_offload(onload_device=torch.device("cuda"), offload_device=torch.device("cpu"), offload_type="leaf_level", use_stream=True)&lt;/span&gt;

pipe = FluxPipeline.from_pretrained(model_id, transformer=transformer, torch_dtype=torch.bfloat16)
&lt;span class="hljs-deletion"&gt;- pipe.to("cuda")&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;

&lt;p&gt;&lt;strong&gt;Group offloading (&lt;code&gt;enable_group_offload&lt;/code&gt; for &lt;code&gt;diffusers&lt;/code&gt; components or &lt;code&gt;apply_group_offloading&lt;/code&gt; for generic &lt;code&gt;torch.nn.Module&lt;/code&gt;s)&lt;/strong&gt;: It moves groups of internal model layers (like &lt;code&gt;torch.nn.ModuleList&lt;/code&gt; or &lt;code&gt;torch.nn.Sequential&lt;/code&gt; instances) to the CPU. This approach is typically more memory-efficient than full model offloading and faster than sequential offloading.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;FP8 layerwise casting + group offloading&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;precision&lt;/th&gt;
&lt;th&gt;Memory after loading&lt;/th&gt;
&lt;th&gt;Peak memory&lt;/th&gt;
&lt;th&gt;Inference time&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;FP8 (e4m3)&lt;/td&gt;
&lt;td&gt;9.264 GB&lt;/td&gt;
&lt;td&gt;14.232 GB&lt;/td&gt;
&lt;td&gt;58 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;details&gt;
Example (Flux-dev with torchao 4-bit + torch.compile):

&lt;pre&gt;&lt;code class="language-diff"&gt;import torch
from diffusers import FluxPipeline
from diffusers import TorchAoConfig as DiffusersTorchAoConfig
from diffusers.quantizers import PipelineQuantizationConfig
from transformers import TorchAoConfig as TransformersTorchAoConfig

from torchao.quantization import Float8WeightOnlyConfig

model_id = "black-forest-labs/FLUX.1-dev"
dtype = torch.bfloat16

pipeline_quant_config = PipelineQuantizationConfig(
    quant_mapping={
        "transformer":DiffusersTorchAoConfig("int4_weight_only"),
        "text_encoder_2": TransformersTorchAoConfig("int4_weight_only"),
    }
)

pipe = FluxPipeline.from_pretrained(
    model_id,
    quantization_config=pipeline_quant_config,
    torch_dtype=torch.bfloat16,
    device_map="balanced"
)

&lt;span class="hljs-addition"&gt;+ pipe.transformer = torch.compile(pipe.transformer, mode="max-autotune", fullgraph=True)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;code&gt;torch.compile&lt;/code&gt; can introduce subtle numerical differences, leading to changes in image output&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/details&gt;

&lt;p&gt;&lt;strong&gt;torch.compile&lt;/strong&gt;: Another complementary approach is to accelerate the execution of your model with PyTorch 2.x’s torch.compile() feature. Compiling the model doesn’t directly lower memory, but it can significantly speed up inference. PyTorch 2.0’s compile (Torch Dynamo) works by tracing and optimizing the model graph ahead-of-time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;torchao + &lt;code&gt;torch.compile&lt;/code&gt;&lt;/strong&gt;: &lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;torchao Precision&lt;/th&gt;
&lt;th&gt;Memory after loading&lt;/th&gt;
&lt;th&gt;Peak memory&lt;/th&gt;
&lt;th&gt;Inference time&lt;/th&gt;
&lt;th&gt;Compile Time&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;int4_weight_only&lt;/td&gt;
&lt;td&gt;10.635 GB&lt;/td&gt;
&lt;td&gt;15.238 GB&lt;/td&gt;
&lt;td&gt;6 seconds&lt;/td&gt;
&lt;td&gt;~285 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;int8_weight_only&lt;/td&gt;
&lt;td&gt;17.020 GB&lt;/td&gt;
&lt;td&gt;22.473 GB&lt;/td&gt;
&lt;td&gt;8 seconds&lt;/td&gt;
&lt;td&gt;~851 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;float8_weight_only&lt;/td&gt;
&lt;td&gt;17.016 GB&lt;/td&gt;
&lt;td&gt;22.115 GB&lt;/td&gt;
&lt;td&gt;8 seconds&lt;/td&gt;
&lt;td&gt;~545 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Explore some benchmarking results here:&lt;/p&gt;


&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Ready to use quantized checkpoints
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;You can find &lt;code&gt;bitsandbytes&lt;/code&gt; and &lt;code&gt;torchao&lt;/code&gt; quantized models from this blog post in our Hugging Face collection: link to collection.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Conclusion
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Here's a quick guide to choosing a quantization backend:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Easiest Memory Savings (NVIDIA):&lt;/strong&gt; Start with &lt;code&gt;bitsandbytes&lt;/code&gt; 4/8-bit. This can also be combined with &lt;code&gt;torch.compile()&lt;/code&gt; for faster inference.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prioritize Inference Speed:&lt;/strong&gt; &lt;code&gt;torchao&lt;/code&gt;, &lt;code&gt;GGUF&lt;/code&gt;, and &lt;code&gt;bitsandbytes&lt;/code&gt; can all be used with &lt;code&gt;torch.compile()&lt;/code&gt; to potentially boost inference speed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;For Hardware Flexibility (CPU/MPS), FP8 Precision:&lt;/strong&gt; &lt;code&gt;Quanto&lt;/code&gt; can be a good option.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Simplicity (Hopper/Ada):&lt;/strong&gt; Explore FP8 Layerwise Casting (&lt;code&gt;enable_layerwise_casting&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;For Using Existing GGUF Models:&lt;/strong&gt; Use GGUF loading (&lt;code&gt;from_single_file&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Curious about training with quantization?&lt;/strong&gt; Stay tuned for a follow-up blog post on that topic!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Quantization significantly lowers the barrier to entry for using large diffusion models. Experiment with these backends to find the best balance of memory, speed, and quality for your needs.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Acknowledgements: Thanks to Chunte for providing the thumbnail for this post.&lt;/em&gt;&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Derek Liu's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://huggingface.co/avatars/e5b8331c9a96cd96b679f38afd30422e.svg" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Marc Sun's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://cdn-avatars.huggingface.co/v1/production/uploads/63ce875d199b36f7552d4f07/NrTlnhUTAp0j0ZsMTsWIu.jpeg" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;




Large diffusion models like Flux (a flow-based text-to-image generation model) can create stunning images, but their size can be a hurdle, demanding significant memory and compute resources. Quantization offers a powerful solution, shrinking these models to make them more accessible without drastically compromising performance. But the big question always is: can you actually tell the difference in the final image?
&lt;p&gt;Before we dive into the technical details of how various quantization backends in Hugging Face Diffusers work, why not test your own perception?&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Spot The Quantized Model
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We created a setup where you can provide a prompt, and we generate results using both the original, high-precision model (e.g., Flux-dev in BF16) and several quantized versions (BnB 4-bit, BnB 8-bit). The generated images are then presented to you and your challenge is to identify which ones came from the quantized models.&lt;/p&gt;
&lt;p&gt;Try it out here or below!
&lt;/p&gt;
&lt;p&gt;Often, especially with 8-bit quantization, the differences are subtle and may not be noticeable without close inspection. More aggressive quantization like 4-bit or lower might be more noticeable, but the results can still be good, especially considering the massive memory savings. NF4 often gives the best trade-off though.&lt;/p&gt;
&lt;p&gt;Now, let's dive deeper.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Quantization Backends in Diffusers
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Building on our previous post, "Memory-efficient Diffusion Transformers with Quanto and Diffusers", this post explores the diverse quantization backends integrated directly into Hugging Face Diffusers. We'll examine how bitsandbytes, GGUF, torchao, Quanto and native FP8 support make large and powerful models more accessible, demonstrating their use with Flux.&lt;/p&gt;
&lt;p&gt;Before diving into the quantization backends, let's introduce the FluxPipeline (using the black-forest-labs/FLUX.1-dev checkpoint) and its components, which we'll be quantizing. Loading the full &lt;code&gt;FLUX.1-dev&lt;/code&gt; model in BF16 precision requires approximately 31.447 GB of memory. The main components are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Text Encoders (CLIP and T5):&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Function:&lt;/strong&gt; Process input text prompts. FLUX-dev uses CLIP for initial understanding and a larger T5 for nuanced comprehension and better text rendering.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory:&lt;/strong&gt; T5 - 9.52 GB; CLIP - 246 MB (in BF16)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transformer (Main Model - MMDiT):&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Function:&lt;/strong&gt; Core generative part (Multimodal Diffusion Transformer). Generates images in latent space from text embeddings. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory:&lt;/strong&gt; 23.8 GB (in BF16)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Variational Auto-Encoder (VAE):&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Function:&lt;/strong&gt; Translates images between pixel and latent space. Decodes generated latent representation to a pixel-based image.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory:&lt;/strong&gt; 168 MB (in BF16)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Focus of Quantization:&lt;/strong&gt; Examples will primarily focus on the &lt;code&gt;transformer&lt;/code&gt; and &lt;code&gt;text_encoder_2&lt;/code&gt; (T5) for the most substantial memory savings.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;prompts = [
    &lt;span class="hljs-string"&gt;"Baroque style, a lavish palace interior with ornate gilded ceilings, intricate tapestries, and dramatic lighting over a grand staircase."&lt;/span&gt;,
    &lt;span class="hljs-string"&gt;"Futurist style, a dynamic spaceport with sleek silver starships docked at angular platforms, surrounded by distant planets and glowing energy lines."&lt;/span&gt;,
    &lt;span class="hljs-string"&gt;"Noir style, a shadowy alleyway with flickering street lamps and a solitary trench-coated figure, framed by rain-soaked cobblestones and darkened storefronts."&lt;/span&gt;,
]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		bitsandbytes (BnB)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;bitsandbytes&lt;/code&gt; is a popular and user-friendly library for 8-bit and 4-bit quantization, widely used for LLMs and QLoRA fine-tuning. We can use it for transformer-based diffusion and flow models, too.&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;
      BF16&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
    &lt;td&gt;
      BnB 4-bit&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
    &lt;td&gt;
      BnB 8-bit&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td colspan="3"&gt;&lt;em&gt;Visual comparison of Flux-dev model outputs using BF16 (left), BnB 4-bit (center), and BnB 8-bit (right) quantization. (Click on an image to zoom) &lt;/em&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Precision&lt;/th&gt;
&lt;th&gt;Memory after loading&lt;/th&gt;
&lt;th&gt;Peak memory&lt;/th&gt;
&lt;th&gt;Inference time&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;BF16&lt;/td&gt;
&lt;td&gt;~31.447 GB&lt;/td&gt;
&lt;td&gt;36.166 GB&lt;/td&gt;
&lt;td&gt;12 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4-bit&lt;/td&gt;
&lt;td&gt;12.584 GB&lt;/td&gt;
&lt;td&gt;17.281 GB&lt;/td&gt;
&lt;td&gt;12 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8-bit&lt;/td&gt;
&lt;td&gt;19.273 GB&lt;/td&gt;
&lt;td&gt;24.432 GB&lt;/td&gt;
&lt;td&gt;27 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;sub&gt;All benchmarks performed on 1x NVIDIA H100 80GB GPU&lt;/sub&gt;&lt;/p&gt;
&lt;details&gt;
Example (Flux-dev with BnB 4-bit):


&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;import&lt;/span&gt; torch
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; diffusers &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; FluxPipeline
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; diffusers &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; BitsAndBytesConfig &lt;span class="hljs-keyword"&gt;as&lt;/span&gt; DiffusersBitsAndBytesConfig
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; diffusers.quantizers &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; PipelineQuantizationConfig
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; transformers &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; BitsAndBytesConfig &lt;span class="hljs-keyword"&gt;as&lt;/span&gt; TransformersBitsAndBytesConfig

model_id = &lt;span class="hljs-string"&gt;"black-forest-labs/FLUX.1-dev"&lt;/span&gt;

pipeline_quant_config = PipelineQuantizationConfig(
    quant_mapping={
        &lt;span class="hljs-string"&gt;"transformer"&lt;/span&gt;: DiffusersBitsAndBytesConfig(load_in_4bit=&lt;span class="hljs-literal"&gt;True&lt;/span&gt;, bnb_4bit_quant_type=&lt;span class="hljs-string"&gt;"nf4"&lt;/span&gt;, bnb_4bit_compute_dtype=torch.bfloat16),
        &lt;span class="hljs-string"&gt;"text_encoder_2"&lt;/span&gt;: TransformersBitsAndBytesConfig(load_in_4bit=&lt;span class="hljs-literal"&gt;True&lt;/span&gt;, bnb_4bit_quant_type=&lt;span class="hljs-string"&gt;"nf4"&lt;/span&gt;, bnb_4bit_compute_dtype=torch.bfloat16),
    }
)

pipe = FluxPipeline.from_pretrained(
    model_id,
    quantization_config=pipeline_quant_config,
    torch_dtype=torch.bfloat16
)
pipe.to(&lt;span class="hljs-string"&gt;"cuda"&lt;/span&gt;)

prompt = &lt;span class="hljs-string"&gt;"Baroque style, a lavish palace interior with ornate gilded ceilings, intricate tapestries, and dramatic lighting over a grand staircase."&lt;/span&gt;
pipe_kwargs = {
    &lt;span class="hljs-string"&gt;"prompt"&lt;/span&gt;: prompt,
    &lt;span class="hljs-string"&gt;"height"&lt;/span&gt;: &lt;span class="hljs-number"&gt;1024&lt;/span&gt;,
    &lt;span class="hljs-string"&gt;"width"&lt;/span&gt;: &lt;span class="hljs-number"&gt;1024&lt;/span&gt;,
    &lt;span class="hljs-string"&gt;"guidance_scale"&lt;/span&gt;: &lt;span class="hljs-number"&gt;3.5&lt;/span&gt;,
    &lt;span class="hljs-string"&gt;"num_inference_steps"&lt;/span&gt;: &lt;span class="hljs-number"&gt;50&lt;/span&gt;,
    &lt;span class="hljs-string"&gt;"max_sequence_length"&lt;/span&gt;: &lt;span class="hljs-number"&gt;512&lt;/span&gt;,
}


&lt;span class="hljs-built_in"&gt;print&lt;/span&gt;(&lt;span class="hljs-string"&gt;f"Pipeline memory usage: &lt;span class="hljs-subst"&gt;{torch.cuda.max_memory_reserved() / &lt;span class="hljs-number"&gt;1024&lt;/span&gt;**&lt;span class="hljs-number"&gt;3&lt;/span&gt;:&lt;span class="hljs-number"&gt;.3&lt;/span&gt;f}&lt;/span&gt; GB"&lt;/span&gt;)

image = pipe(
    **pipe_kwargs, generator=torch.manual_seed(&lt;span class="hljs-number"&gt;0&lt;/span&gt;),
).images[&lt;span class="hljs-number"&gt;0&lt;/span&gt;]

&lt;span class="hljs-built_in"&gt;print&lt;/span&gt;(&lt;span class="hljs-string"&gt;f"Pipeline memory usage: &lt;span class="hljs-subst"&gt;{torch.cuda.max_memory_reserved() / &lt;span class="hljs-number"&gt;1024&lt;/span&gt;**&lt;span class="hljs-number"&gt;3&lt;/span&gt;:&lt;span class="hljs-number"&gt;.3&lt;/span&gt;f}&lt;/span&gt; GB"&lt;/span&gt;)

image.save(&lt;span class="hljs-string"&gt;"flux-dev_bnb_4bit.png"&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; When using &lt;code&gt;PipelineQuantizationConfig&lt;/code&gt; with &lt;code&gt;bitsandbytes&lt;/code&gt;, you need to import &lt;code&gt;DiffusersBitsAndBytesConfig&lt;/code&gt; from &lt;code&gt;diffusers&lt;/code&gt; and &lt;code&gt;TransformersBitsAndBytesConfig&lt;/code&gt; from &lt;code&gt;transformers&lt;/code&gt; separately. This is because these components originate from different libraries. If you prefer a simpler setup without managing these distinct imports, you can use an alternative approach for pipeline-level quantization, an example of this method is in the Diffusers documentation on Pipeline-level quantization.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/details&gt;

&lt;p&gt;For more information check out the bitsandbytes docs.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		torchao
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;torchao&lt;/code&gt; is a PyTorch-native library for architecture optimization, offering quantization, sparsity, and custom data types, designed for compatibility with &lt;code&gt;torch.compile&lt;/code&gt; and FSDP. Diffusers supports a wide range of &lt;code&gt;torchao&lt;/code&gt;'s exotic data types, enabling fine-grained control over model optimization.&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;
      int4_weight_only&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
    &lt;td&gt;
      int8_weight_only&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
    &lt;td&gt;
      float8_weight_only&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td colspan="3"&gt;&lt;em&gt;Visual comparison of Flux-dev model outputs using torchao int4_weight_only (left), int8_weight_only (center), and float8_weight_only (right) quantization. (Click on an image to zoom)&lt;/em&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;torchao Precision&lt;/th&gt;
&lt;th&gt;Memory after loading&lt;/th&gt;
&lt;th&gt;Peak memory&lt;/th&gt;
&lt;th&gt;Inference time&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;int4_weight_only&lt;/td&gt;
&lt;td&gt;10.635 GB&lt;/td&gt;
&lt;td&gt;14.654 GB&lt;/td&gt;
&lt;td&gt;109 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;int8_weight_only&lt;/td&gt;
&lt;td&gt;17.020 GB&lt;/td&gt;
&lt;td&gt;21.482 GB&lt;/td&gt;
&lt;td&gt;15 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;float8_weight_only&lt;/td&gt;
&lt;td&gt;17.016 GB&lt;/td&gt;
&lt;td&gt;21.488 GB&lt;/td&gt;
&lt;td&gt;15 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;details&gt;
Example (Flux-dev with torchao INT8 weight-only):

&lt;pre&gt;&lt;code class="language-diff"&gt;@@
&lt;span class="hljs-deletion"&gt;- from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+ from diffusers import TorchAoConfig as DiffusersTorchAoConfig&lt;/span&gt;

&lt;span class="hljs-deletion"&gt;- from transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+ from transformers import TorchAoConfig as TransformersTorchAoConfig&lt;/span&gt;
@@
pipeline_quant_config = PipelineQuantizationConfig(
    quant_mapping={
&lt;span class="hljs-deletion"&gt;-         "transformer": DiffusersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16),&lt;/span&gt;
&lt;span class="hljs-deletion"&gt;-         "text_encoder_2": TransformersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16),&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+         "transformer": DiffusersTorchAoConfig("int8_weight_only"),&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+         "text_encoder_2": TransformersTorchAoConfig("int8_weight_only"),&lt;/span&gt;
    }
)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;

&lt;details&gt;
Example (Flux-dev with torchao INT4 weight-only):

&lt;pre&gt;&lt;code class="language-diff"&gt;@@
&lt;span class="hljs-deletion"&gt;- from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+ from diffusers import TorchAoConfig as DiffusersTorchAoConfig&lt;/span&gt;

&lt;span class="hljs-deletion"&gt;- from transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+ from transformers import TorchAoConfig as TransformersTorchAoConfig&lt;/span&gt;
@@
pipeline_quant_config = PipelineQuantizationConfig(
    quant_mapping={
&lt;span class="hljs-deletion"&gt;-         "transformer": DiffusersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16),&lt;/span&gt;
&lt;span class="hljs-deletion"&gt;-         "text_encoder_2": TransformersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16),&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+         "transformer": DiffusersTorchAoConfig("int4_weight_only"),&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+         "text_encoder_2": TransformersTorchAoConfig("int4_weight_only"),&lt;/span&gt;
    }
)

pipe = FluxPipeline.from_pretrained(
    model_id,
    quantization_config=pipeline_quant_config,
    torch_dtype=torch.bfloat16,
&lt;span class="hljs-addition"&gt;+    device_map="balanced"&lt;/span&gt;
)
&lt;span class="hljs-deletion"&gt;- pipe.to("cuda")&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;

&lt;p&gt;For more information check out the torchao docs.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Quanto
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Quanto is a quantization library integrated with the Hugging Face ecosystem via the &lt;code&gt;optimum&lt;/code&gt; library.&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;
      INT4&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
    &lt;td&gt;
      INT8&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
    &lt;td&gt;
      FP8&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td colspan="3"&gt;&lt;em&gt;Visual comparison of Flux-dev model outputs using Quanto INT4 (left), INT8 (center), and FP8 (right) quantization. (Click on an image to zoom)&lt;/em&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;quanto Precision&lt;/th&gt;
&lt;th&gt;Memory after loading&lt;/th&gt;
&lt;th&gt;Peak memory&lt;/th&gt;
&lt;th&gt;Inference time&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;INT4&lt;/td&gt;
&lt;td&gt;12.254 GB&lt;/td&gt;
&lt;td&gt;16.139 GB&lt;/td&gt;
&lt;td&gt;109 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;INT8&lt;/td&gt;
&lt;td&gt;17.330 GB&lt;/td&gt;
&lt;td&gt;21.814 GB&lt;/td&gt;
&lt;td&gt;15 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FP8&lt;/td&gt;
&lt;td&gt;16.395 GB&lt;/td&gt;
&lt;td&gt;20.898 GB&lt;/td&gt;
&lt;td&gt;16 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;details&gt;
Example (Flux-dev with quanto INT8 weight-only):

&lt;pre&gt;&lt;code class="language-diff"&gt;@@
&lt;span class="hljs-deletion"&gt;- from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+ from diffusers import QuantoConfig as DiffusersQuantoConfig&lt;/span&gt;

&lt;span class="hljs-deletion"&gt;- from transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+ from transformers import QuantoConfig as TransformersQuantoConfig&lt;/span&gt;
@@
pipeline_quant_config = PipelineQuantizationConfig(
    quant_mapping={
&lt;span class="hljs-deletion"&gt;-         "transformer": DiffusersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16),&lt;/span&gt;
&lt;span class="hljs-deletion"&gt;-         "text_encoder_2": TransformersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16),&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+         "transformer": DiffusersQuantoConfig(weights_dtype="int8"),&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+         "text_encoder_2": TransformersQuantoConfig(weights_dtype="int8"),&lt;/span&gt;
    }
)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; At the time of writing, for float8 support with Quanto, you'll need &lt;code&gt;optimum-quanto&amp;lt;0.2.5&lt;/code&gt; and use quanto directly. We will be working on fixing this.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;details&gt;
Example (Flux-dev with quanto FP8 weight-only)

&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;import&lt;/span&gt; torch
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; diffusers &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; AutoModel, FluxPipeline
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; transformers &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; T5EncoderModel
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; optimum.quanto &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; freeze, qfloat8, quantize

model_id = &lt;span class="hljs-string"&gt;"black-forest-labs/FLUX.1-dev"&lt;/span&gt;

text_encoder_2 = T5EncoderModel.from_pretrained(
    model_id,
    subfolder=&lt;span class="hljs-string"&gt;"text_encoder_2"&lt;/span&gt;,
    torch_dtype=torch.bfloat16,
)

quantize(text_encoder_2, weights=qfloat8)
freeze(text_encoder_2)

transformer = AutoModel.from_pretrained(
      model_id,
      subfolder=&lt;span class="hljs-string"&gt;"transformer"&lt;/span&gt;,
      torch_dtype=torch.bfloat16,
)

quantize(transformer, weights=qfloat8)
freeze(transformer)

pipe = FluxPipeline.from_pretrained(
    model_id,
    transformer=transformer,
    text_encoder_2=text_encoder_2,
    torch_dtype=torch.bfloat16
).to(&lt;span class="hljs-string"&gt;"cuda"&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;

&lt;p&gt;For more information check out the Quanto docs.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		GGUF
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;GGUF is a file format popular in the llama.cpp community for storing quantized models.&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;
      Q2_k&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
    &lt;td&gt;
      Q4_1&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
    &lt;td&gt;
      Q8_0&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td colspan="3"&gt;&lt;em&gt;Visual comparison of Flux-dev model outputs using GGUF Q2_k (left), Q4_1 (center), and Q8_0 (right) quantization. (Click on an image to zoom)&lt;/em&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;GGUF Precision&lt;/th&gt;
&lt;th&gt;Memory after loading&lt;/th&gt;
&lt;th&gt;Peak memory&lt;/th&gt;
&lt;th&gt;Inference time&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;Q2_k&lt;/td&gt;
&lt;td&gt;13.264 GB&lt;/td&gt;
&lt;td&gt;17.752 GB&lt;/td&gt;
&lt;td&gt;26 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Q4_1&lt;/td&gt;
&lt;td&gt;16.838 GB&lt;/td&gt;
&lt;td&gt;21.326 GB&lt;/td&gt;
&lt;td&gt;23 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Q8_0&lt;/td&gt;
&lt;td&gt;21.502 GB&lt;/td&gt;
&lt;td&gt;25.973 GB&lt;/td&gt;
&lt;td&gt;15 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;details&gt;
Example (Flux-dev with GGUF Q4_1)

&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;import&lt;/span&gt; torch
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; diffusers &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; FluxPipeline, FluxTransformer2DModel, GGUFQuantizationConfig

model_id = &lt;span class="hljs-string"&gt;"black-forest-labs/FLUX.1-dev"&lt;/span&gt;


ckpt_path = &lt;span class="hljs-string"&gt;"https://huggingface.co/city96/FLUX.1-dev-gguf/resolve/main/flux1-dev-Q4_1.gguf"&lt;/span&gt;

transformer = FluxTransformer2DModel.from_single_file(
    ckpt_path,
    quantization_config=GGUFQuantizationConfig(compute_dtype=torch.bfloat16),
    torch_dtype=torch.bfloat16,
)

pipe = FluxPipeline.from_pretrained(
    model_id,
    transformer=transformer,
    torch_dtype=torch.bfloat16,
)
pipe.to(&lt;span class="hljs-string"&gt;"cuda"&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;

&lt;p&gt;For more information check out the GGUF docs.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		FP8 Layerwise Casting (&lt;code&gt;enable_layerwise_casting&lt;/code&gt;)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;FP8 Layerwise Casting is a memory optimization technique. It works by storing the model's weights in the compact FP8 (8-bit floating point) format, which uses roughly half the memory of standard FP16 or BF16 precision.
Just before a layer performs its calculations, its weights are dynamically cast up to a higher compute precision (like FP16/BF16). Immediately afterward, the weights are cast back down to FP8 for efficient storage. This approach works because the core computations retain high precision, and layers particularly sensitive to quantization (like normalization) are typically skipped. This technique can also be combined with group offloading for further memory savings.&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;
      FP8 (e4m3)&lt;br /&gt;&lt;figure class="image table text-center m-0 w-full"&gt;
        
      &lt;/figure&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;em&gt;Visual output of Flux-dev model using FP8 Layerwise Casting (e4m3) quantization.&lt;/em&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;precision&lt;/th&gt;
&lt;th&gt;Memory after loading&lt;/th&gt;
&lt;th&gt;Peak memory&lt;/th&gt;
&lt;th&gt;Inference time&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;FP8 (e4m3)&lt;/td&gt;
&lt;td&gt;23.682 GB&lt;/td&gt;
&lt;td&gt;28.451 GB&lt;/td&gt;
&lt;td&gt;13 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;import&lt;/span&gt; torch
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; diffusers &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; AutoModel, FluxPipeline

model_id = &lt;span class="hljs-string"&gt;"black-forest-labs/FLUX.1-dev"&lt;/span&gt;

transformer = AutoModel.from_pretrained(
    model_id,
    subfolder=&lt;span class="hljs-string"&gt;"transformer"&lt;/span&gt;,
    torch_dtype=torch.bfloat16
)
transformer.enable_layerwise_casting(storage_dtype=torch.float8_e4m3fn, compute_dtype=torch.bfloat16)

pipe = FluxPipeline.from_pretrained(model_id, transformer=transformer, torch_dtype=torch.bfloat16)
pipe.to(&lt;span class="hljs-string"&gt;"cuda"&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For more information check out the Layerwise casting docs.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Combining with More Memory Optimizations and torch.compile
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Most of these quantization backends can be combined with the memory optimization techniques offered in Diffusers. Let's explore CPU offloading, group offloading, and &lt;code&gt;torch.compile&lt;/code&gt;. You can learn more about these techniques in the Diffusers documentation.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; At the time of writing, bnb + &lt;code&gt;torch.compile&lt;/code&gt; also works if bnb is installed from source and using pytorch nightly or with fullgraph=False.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;details&gt;
Example (Flux-dev with BnB 4-bit + enable_model_cpu_offload):

&lt;pre&gt;&lt;code class="language-diff"&gt;import torch
from diffusers import FluxPipeline
from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig
from diffusers.quantizers import PipelineQuantizationConfig
from transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig

model_id = "black-forest-labs/FLUX.1-dev"

pipeline_quant_config = PipelineQuantizationConfig(
    quant_mapping={
        "transformer": DiffusersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16),
        "text_encoder_2": TransformersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16),
    }
)

pipe = FluxPipeline.from_pretrained(
    model_id,
    quantization_config=pipeline_quant_config,
    torch_dtype=torch.bfloat16
)
&lt;span class="hljs-deletion"&gt;- pipe.to("cuda")&lt;/span&gt;
&lt;span class="hljs-addition"&gt;+ pipe.enable_model_cpu_offload()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;

&lt;p&gt;&lt;strong&gt;Model CPU Offloading (&lt;code&gt;enable_model_cpu_offload&lt;/code&gt;)&lt;/strong&gt;: This method moves entire model components (like the UNet, text encoders, or VAE) between the CPU and GPU during the inference pipeline. It offers substantial VRAM savings and is generally faster than more granular offloading because it involves fewer, larger data transfers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;bnb + &lt;code&gt;enable_model_cpu_offload&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Precision&lt;/th&gt;
&lt;th&gt;Memory after loading&lt;/th&gt;
&lt;th&gt;Peak memory&lt;/th&gt;
&lt;th&gt;Inference time&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;4-bit&lt;/td&gt;
&lt;td&gt;12.383 GB&lt;/td&gt;
&lt;td&gt;12.383 GB&lt;/td&gt;
&lt;td&gt;17 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8-bit&lt;/td&gt;
&lt;td&gt;19.182 GB&lt;/td&gt;
&lt;td&gt;23.428 GB&lt;/td&gt;
&lt;td&gt;27 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;details&gt;
Example (Flux-dev with fp8 layerwise casting + group offloading):

&lt;pre&gt;&lt;code class="language-diff"&gt;import torch
from diffusers import FluxPipeline, AutoModel

model_id = "black-forest-labs/FLUX.1-dev"

transformer = AutoModel.from_pretrained(
    model_id,
    subfolder="transformer",
    torch_dtype=torch.bfloat16,
    # device_map="cuda"
)
transformer.enable_layerwise_casting(storage_dtype=torch.float8_e4m3fn, compute_dtype=torch.bfloat16)
&lt;span class="hljs-addition"&gt;+ transformer.enable_group_offload(onload_device=torch.device("cuda"), offload_device=torch.device("cpu"), offload_type="leaf_level", use_stream=True)&lt;/span&gt;

pipe = FluxPipeline.from_pretrained(model_id, transformer=transformer, torch_dtype=torch.bfloat16)
&lt;span class="hljs-deletion"&gt;- pipe.to("cuda")&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;

&lt;p&gt;&lt;strong&gt;Group offloading (&lt;code&gt;enable_group_offload&lt;/code&gt; for &lt;code&gt;diffusers&lt;/code&gt; components or &lt;code&gt;apply_group_offloading&lt;/code&gt; for generic &lt;code&gt;torch.nn.Module&lt;/code&gt;s)&lt;/strong&gt;: It moves groups of internal model layers (like &lt;code&gt;torch.nn.ModuleList&lt;/code&gt; or &lt;code&gt;torch.nn.Sequential&lt;/code&gt; instances) to the CPU. This approach is typically more memory-efficient than full model offloading and faster than sequential offloading.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;FP8 layerwise casting + group offloading&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;precision&lt;/th&gt;
&lt;th&gt;Memory after loading&lt;/th&gt;
&lt;th&gt;Peak memory&lt;/th&gt;
&lt;th&gt;Inference time&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;FP8 (e4m3)&lt;/td&gt;
&lt;td&gt;9.264 GB&lt;/td&gt;
&lt;td&gt;14.232 GB&lt;/td&gt;
&lt;td&gt;58 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;details&gt;
Example (Flux-dev with torchao 4-bit + torch.compile):

&lt;pre&gt;&lt;code class="language-diff"&gt;import torch
from diffusers import FluxPipeline
from diffusers import TorchAoConfig as DiffusersTorchAoConfig
from diffusers.quantizers import PipelineQuantizationConfig
from transformers import TorchAoConfig as TransformersTorchAoConfig

from torchao.quantization import Float8WeightOnlyConfig

model_id = "black-forest-labs/FLUX.1-dev"
dtype = torch.bfloat16

pipeline_quant_config = PipelineQuantizationConfig(
    quant_mapping={
        "transformer":DiffusersTorchAoConfig("int4_weight_only"),
        "text_encoder_2": TransformersTorchAoConfig("int4_weight_only"),
    }
)

pipe = FluxPipeline.from_pretrained(
    model_id,
    quantization_config=pipeline_quant_config,
    torch_dtype=torch.bfloat16,
    device_map="balanced"
)

&lt;span class="hljs-addition"&gt;+ pipe.transformer = torch.compile(pipe.transformer, mode="max-autotune", fullgraph=True)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;code&gt;torch.compile&lt;/code&gt; can introduce subtle numerical differences, leading to changes in image output&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/details&gt;

&lt;p&gt;&lt;strong&gt;torch.compile&lt;/strong&gt;: Another complementary approach is to accelerate the execution of your model with PyTorch 2.x’s torch.compile() feature. Compiling the model doesn’t directly lower memory, but it can significantly speed up inference. PyTorch 2.0’s compile (Torch Dynamo) works by tracing and optimizing the model graph ahead-of-time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;torchao + &lt;code&gt;torch.compile&lt;/code&gt;&lt;/strong&gt;: &lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;torchao Precision&lt;/th&gt;
&lt;th&gt;Memory after loading&lt;/th&gt;
&lt;th&gt;Peak memory&lt;/th&gt;
&lt;th&gt;Inference time&lt;/th&gt;
&lt;th&gt;Compile Time&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;int4_weight_only&lt;/td&gt;
&lt;td&gt;10.635 GB&lt;/td&gt;
&lt;td&gt;15.238 GB&lt;/td&gt;
&lt;td&gt;6 seconds&lt;/td&gt;
&lt;td&gt;~285 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;int8_weight_only&lt;/td&gt;
&lt;td&gt;17.020 GB&lt;/td&gt;
&lt;td&gt;22.473 GB&lt;/td&gt;
&lt;td&gt;8 seconds&lt;/td&gt;
&lt;td&gt;~851 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;float8_weight_only&lt;/td&gt;
&lt;td&gt;17.016 GB&lt;/td&gt;
&lt;td&gt;22.115 GB&lt;/td&gt;
&lt;td&gt;8 seconds&lt;/td&gt;
&lt;td&gt;~545 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Explore some benchmarking results here:&lt;/p&gt;


&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Ready to use quantized checkpoints
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;You can find &lt;code&gt;bitsandbytes&lt;/code&gt; and &lt;code&gt;torchao&lt;/code&gt; quantized models from this blog post in our Hugging Face collection: link to collection.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Conclusion
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Here's a quick guide to choosing a quantization backend:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Easiest Memory Savings (NVIDIA):&lt;/strong&gt; Start with &lt;code&gt;bitsandbytes&lt;/code&gt; 4/8-bit. This can also be combined with &lt;code&gt;torch.compile()&lt;/code&gt; for faster inference.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prioritize Inference Speed:&lt;/strong&gt; &lt;code&gt;torchao&lt;/code&gt;, &lt;code&gt;GGUF&lt;/code&gt;, and &lt;code&gt;bitsandbytes&lt;/code&gt; can all be used with &lt;code&gt;torch.compile()&lt;/code&gt; to potentially boost inference speed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;For Hardware Flexibility (CPU/MPS), FP8 Precision:&lt;/strong&gt; &lt;code&gt;Quanto&lt;/code&gt; can be a good option.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Simplicity (Hopper/Ada):&lt;/strong&gt; Explore FP8 Layerwise Casting (&lt;code&gt;enable_layerwise_casting&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;For Using Existing GGUF Models:&lt;/strong&gt; Use GGUF loading (&lt;code&gt;from_single_file&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Curious about training with quantization?&lt;/strong&gt; Stay tuned for a follow-up blog post on that topic!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Quantization significantly lowers the barrier to entry for using large diffusion models. Experiment with these backends to find the best balance of memory, speed, and quality for your needs.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Acknowledgements: Thanks to Chunte for providing the thumbnail for this post.&lt;/em&gt;&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/diffusers-quantization</guid><pubDate>Wed, 21 May 2025 00:00:00 +0000</pubDate></item><item><title>By putting AI into everything, Google wants to make it invisible (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/05/21/1117251/by-putting-ai-into-everything-google-wants-to-make-it-invisible/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/05/AP25140699166229.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;If you want to know where AI is headed, this year’s Google I/O has you covered. The company’s annual showcase of next-gen products, which kicked off yesterday, has all of the pomp and pizzazz, the sizzle reels and celebrity walk-ons, that you’d expect from a multimillion-dollar marketing event.&lt;/p&gt;  &lt;p&gt;But it also shows us just how fast this still experimental technology is being subsumed into a lineup designed to sell phones and subscription tiers. Never before have I seen this thing we call artificial intelligence appear so normal.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;p&gt;Yes, Google’s roster of consumer-facing products is the slickest on offer. The firm is bundling most of its multimodal models into its Gemini app, including the new Imagen 4 image generator and the new Veo 3 video generator. That means you can now access Google’s full range of generative models via a single chatbot. It also announced Gemini Live, a feature that lets you share your phone’s screen or your camera’s view with the chatbot and ask it about what it can see.&lt;/p&gt;  &lt;p&gt;Those features were previously only seen in demos of Project Astra, a “universal AI assistant“ that Google DeepMind is working on. Now, Google is inching toward putting Project Astra into the hands of anyone with a smartphone.&lt;/p&gt; 
 &lt;p&gt;Google is also rolling out AI Mode, an LLM-powered front end to search. This can now pull in personal information from Gmail or Google Docs to tailor searches to users. It will include Deep Search, which can break a query down into hundreds of individual searches and then summarize the results; a version of Project Mariner, Google DeepMind’s browser-using agent; and Search Live, which lets you hold up your camera and ask it what it sees.&lt;/p&gt;  &lt;p&gt;This is the new frontier. It’s no longer about who has the most powerful models, but who can spin them into the best products. OpenAI’s ChatGPT includes many similar features to Gemini’s. But with its existing ecosystem of consumer services and billions of existing users, Google has a clear advantage. Power users wanting access to the latest versions of everything on display can now sign up for Google AI Ultra for $250 a month.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;When OpenAI released ChatGPT in late 2022, Google was caught on the back foot and was forced to jump into higher gear to catch up. With this year’s product lineup, it looks as if Google has stuck its landing.&lt;/p&gt;  &lt;p&gt;On a preview call, CEO Sundar Pichai claimed that AI Overviews, a precursor to AI Mode that provides LLM-generated summaries of search results, had turned out to be popular with hundreds of millions of users. He speculated that many of them may not even know (or care) that they were using AI—it was just a cool new way to search. Google I/O gives a broader glimpse of that future, one where AI is invisible.&lt;/p&gt;  &lt;p&gt;“More intelligence is available, for everyone, everywhere,” Pichai told his audience. I think we are expected to marvel. But by putting AI in everything, Google is turning AI into a technology we won’t notice and may not even bother to name.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/05/AP25140699166229.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;If you want to know where AI is headed, this year’s Google I/O has you covered. The company’s annual showcase of next-gen products, which kicked off yesterday, has all of the pomp and pizzazz, the sizzle reels and celebrity walk-ons, that you’d expect from a multimillion-dollar marketing event.&lt;/p&gt;  &lt;p&gt;But it also shows us just how fast this still experimental technology is being subsumed into a lineup designed to sell phones and subscription tiers. Never before have I seen this thing we call artificial intelligence appear so normal.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;p&gt;Yes, Google’s roster of consumer-facing products is the slickest on offer. The firm is bundling most of its multimodal models into its Gemini app, including the new Imagen 4 image generator and the new Veo 3 video generator. That means you can now access Google’s full range of generative models via a single chatbot. It also announced Gemini Live, a feature that lets you share your phone’s screen or your camera’s view with the chatbot and ask it about what it can see.&lt;/p&gt;  &lt;p&gt;Those features were previously only seen in demos of Project Astra, a “universal AI assistant“ that Google DeepMind is working on. Now, Google is inching toward putting Project Astra into the hands of anyone with a smartphone.&lt;/p&gt; 
 &lt;p&gt;Google is also rolling out AI Mode, an LLM-powered front end to search. This can now pull in personal information from Gmail or Google Docs to tailor searches to users. It will include Deep Search, which can break a query down into hundreds of individual searches and then summarize the results; a version of Project Mariner, Google DeepMind’s browser-using agent; and Search Live, which lets you hold up your camera and ask it what it sees.&lt;/p&gt;  &lt;p&gt;This is the new frontier. It’s no longer about who has the most powerful models, but who can spin them into the best products. OpenAI’s ChatGPT includes many similar features to Gemini’s. But with its existing ecosystem of consumer services and billions of existing users, Google has a clear advantage. Power users wanting access to the latest versions of everything on display can now sign up for Google AI Ultra for $250 a month.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;When OpenAI released ChatGPT in late 2022, Google was caught on the back foot and was forced to jump into higher gear to catch up. With this year’s product lineup, it looks as if Google has stuck its landing.&lt;/p&gt;  &lt;p&gt;On a preview call, CEO Sundar Pichai claimed that AI Overviews, a precursor to AI Mode that provides LLM-generated summaries of search results, had turned out to be popular with hundreds of millions of users. He speculated that many of them may not even know (or care) that they were using AI—it was just a cool new way to search. Google I/O gives a broader glimpse of that future, one where AI is invisible.&lt;/p&gt;  &lt;p&gt;“More intelligence is available, for everyone, everywhere,” Pichai told his audience. I think we are expected to marvel. But by putting AI in everything, Google is turning AI into a technology we won’t notice and may not even bother to name.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/05/21/1117251/by-putting-ai-into-everything-google-wants-to-make-it-invisible/</guid><pubDate>Wed, 21 May 2025 11:49:03 +0000</pubDate></item><item><title>Abstracts: Aurora with Megan Stanley and Wessel Bruinsma (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/podcast/abstracts-aurora-with-megan-stanley-and-wessel-bruinsma/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Abstracts podcast | Aurora with Megan Stanley and Wessel Bruinsma" class="wp-image-1139936" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Aurora_Wessel-and-Megan_Abstracts_Hero_Feature_No_Text_1400x788_.jpg" width="1400" /&gt;&lt;/figure&gt;






&lt;p&gt;Members of the research community at Microsoft work continuously to advance their respective fields.&lt;em&gt; Abstracts &lt;/em&gt;brings its audience to the cutting edge with them through short, compelling conversations about new and noteworthy achievements.&lt;/p&gt;



&lt;p&gt;In this episode of &lt;em&gt;Abstracts&lt;/em&gt;, Microsoft senior researchers Megan Stanley and Wessel Bruinsma join host Amber Tingle to discuss their groundbreaking work on environmental forecasting. Their new &lt;em&gt;Nature&lt;/em&gt; publication, “A Foundation Model for the Earth System,”&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; features Aurora, an AI model that redefines weather prediction and extends its capabilities to other environmental domains such as tropical cyclones and ocean wave forecasting.&lt;/p&gt;







&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h2 class="wp-block-heading h5" id="learn-more"&gt;Learn more&lt;/h2&gt;



&lt;p&gt;A foundation model for the Earth system&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;br /&gt;Nature | May 2025&lt;/p&gt;



&lt;p&gt;Introducing Aurora: The first large-scale foundation model of the atmosphere&lt;br /&gt;Microsoft Research Blog | June 2024&lt;/p&gt;



&lt;p&gt;Project Aurora: The first large-scale foundation model of the atmosphere&lt;br /&gt;Video | September 2024&lt;/p&gt;



&lt;p&gt;A Foundation Model for the Earth System&lt;br /&gt;Paper | November 2024&lt;/p&gt;



&lt;p&gt;Aurora&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;br /&gt;Azure AI Foundry Labs&lt;/p&gt;







&lt;section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast"&gt;
	
&lt;/section&gt;


&lt;div class="wp-block-msr-show-more"&gt;
	&lt;div class="bg-neutral-100 p-5"&gt;
		&lt;div class="show-more-show-less"&gt;
			&lt;div&gt;
				&lt;span&gt;
					

&lt;h2 class="wp-block-heading" id="transcript"&gt;Transcript&lt;/h2&gt;



&lt;p&gt;[MUSIC] &amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AMBER TINGLE:&lt;/strong&gt; Welcome to &lt;em&gt;Abstracts&lt;/em&gt;, a Microsoft Research Podcast that puts the spotlight on world-class research &lt;em&gt;in brief&lt;/em&gt;. I’m Amber Tingle. In this series, members of the research community at Microsoft give us a quick snapshot—or a &lt;em&gt;podcast abstract&lt;/em&gt;—of their new and noteworthy papers.&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Our guests today are Megan Stanley and Wessel Bruinsma. They are both senior researchers within the Microsoft Research AI for Science initiative. They are also two of the coauthors on a new &lt;em&gt;Nature&lt;/em&gt; publication called “A Foundation Model for the Earth System.”&lt;/p&gt;



				&lt;/span&gt;
				&lt;span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1"&gt;
					



&lt;p&gt;This is such exciting work about environmental forecasting, so we’re happy to have the two of you join us today.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Megan and Wessel, welcome.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MEGAN STANLEY:&lt;/strong&gt; Thank you. Thanks. Great to be here.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WESSEL BRUINSMA:&lt;/strong&gt; Thanks.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TINGLE:&lt;/strong&gt; Let’s jump right in. Wessel, share a bit about the problem your research addresses and why this work is so important.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;BRUINSMA:&lt;/strong&gt; I think we’re all very much aware of the revolution that’s happening in the space of large language models, which have just become so strong. What’s perhaps lesser well-known is that machine learning models have also started to revolutionize this field of weather prediction. Whereas traditional weather prediction models, based on physical laws, used to be the state of the art, these traditional models are now challenged and often even outperformed by AI models. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;This advancement is super impressive and really a big deal. Mostly because AI weather forecasting models are computationally much more efficient and can even be more accurate. What’s unfortunate though, about this big step forward, is that these developments are mostly limited to the setting of weather forecasting.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Weather forecasting is very important, obviously, but there are many other important environmental forecasting problems out there, such as air pollution forecasting or ocean wave forecasting. We have developed a model, named Aurora, which really kicks the AI revolution in weather forecasting into the next gear by extending these advancements to other environmental forecasting fields, too. With Aurora, we’re now able to produce state-of-the-art air pollution forecasts using an AI approach. And that wasn’t possible before!&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TINGLE:&lt;/strong&gt; Megan, how does this approach differ from or build on work that’s already been done in the atmospheric sciences?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;STANLEY:&lt;/strong&gt; Current approaches have really focused training very specifically on weather forecasting models. And in contrast, with Aurora, what we’ve attempted to do is train a so-called foundation model for the Earth system. In the first step, we train Aurora on a vast body of Earth system data. This is our pretraining step. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;And when I say a vast body of data, I really do mean a lot. And the purpose of this pretraining is to let Aurora, kind of, learn some general-purpose representation of the dynamics that govern the Earth system. But then once we’ve pretrained Aurora, and this really is the crux of this, the reason why we’re doing this project, is after the model has been pretrained, it can leverage this learned general-purpose representation and efficiently adapt to new tasks, new domains, new variables. And this is called fine-tuning.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The idea is that the model really uses the learned representation to perform this adaptation very efficiently, which basically means Aurora is a powerful, flexible model that can relatively cheaply be adapted to any environmental forecasting task.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TINGLE:&lt;/strong&gt; Wessel, can you tell us about your methodology? How did you all conduct this research?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;BRUINSMA:&lt;/strong&gt; While approaches so far have trained models on primarily one particular data&lt;s&gt; &lt;/s&gt;set, this one dataset is very large, which makes it possible to train very good models. But it does remain only one dataset, and that’s not very diverse. In the domain of environmental forecasting, we have really tried to push the limits of scaling to large data by training Aurora on not just this one large dataset, but on as many very large datasets as we could find.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;These datasets are a combination of estimates of the historical state of the world, forecasts by other models, climate simulations, and more. We’ve been able to show that training on not just more data but more diverse data helps the model achieve even better performance. Showing this is difficult because there is just so much &lt;em&gt;data&lt;/em&gt;. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;In addition to scaling to more and more diverse data, we also increased the size of the model as much as we could. Here we found that bigger models, despite being slower to run, make more efficient use of computational resources. It’s cheaper to train a good big model than a good small model. The mantra of this project was to really keep it simple and to scale to simultaneously very large and, more importantly, diverse data and large model size.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TINGLE:&lt;/strong&gt; So, Megan, what were your major findings? And we know they’re major because they’re in &lt;em&gt;N&lt;/em&gt;&lt;em&gt;ature&lt;/em&gt;. [LAUGHS]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;STANLEY:&lt;/strong&gt; Yeah, [LAUGHS] I guess they really are. So the main outcome of this project is we were actually able to train a single foundation model that achieves state-of-the-art performance in four different domains. Air pollution forecasting. For example, predicting particulate matter near the surface or ozone in the atmosphere. Ocean wave forecasting, which is critical for planning shipping routes. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;Tropical cyclone track forecasting, so that means being able to predict where a hurricane or a typhoon is expected to go, which is obviously incredibly important, and very high-resolution weather forecasting.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And I’ve, kind of, named these forecasting domains as if they’re just items in a list, but in every single one, Aurora really pushed the limits of what is possible with AI models. And we’re really proud of that. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;But perhaps, kind of, you know, to my mind, the key takeaway here is that the foundation model approach actually works. So what we have shown is it’s possible to actually train some kind of general model, a foundation model, and then adapt it to a wide variety of environmental tasks. Now we definitely do not claim that Aurora is some kind of ultimate environmental forecasting model. We are sure that the model and the pretraining procedure can actually be improved. But, nevertheless, we’ve shown that this approach works for environmental forecasting. It really holds massive promise, and that’s incredibly cool.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TINGLE:&lt;/strong&gt; Wessel, what do you think will be the real-world impact of this work?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;BRUINSMA:&lt;/strong&gt; Well, for applications that we mentioned, which are air pollution forecasting, ocean wave forecasting, tropical cyclone track forecasting, and very high-resolution weather forecasting, Aurora could today be deployed in real-time systems to produce near real-time forecasts. And, you know, in fact, it already is. You can view real-time weather forecasts by the high-resolution version of the model on the website of ECMWF (European Centre for Medium-Range Weather Forecasts).&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But what’s remarkable is that every of these applications took a small team of engineers about four to eight weeks to fully execute. You should compare this to a typical development timeline for more traditional models, which can be on the order of multiple years. Using the pretraining fine-tuning approach that we used for Aurora, we might see significantly accelerated development cycles for environmental forecasting problems. And that’s exciting.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TINGLE:&lt;/strong&gt; Megan, if our listeners only walk away from this conversation with one key talking point, what would you like that to be? What should we remember about this paper?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;STANLEY:&lt;/strong&gt; The biggest takeaway is that the pretraining fine-tuning paradigm, it really works for environmental forecasting, right? So you can train a foundational model, it learns some kind of general-purpose representation of the Earth system dynamics, and this representation boosts performance in a wide variety of forecasting tasks. But we really want to emphasize that Aurora only scratches the surface of what’s actually possible.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So there are many more applications to explore than the four we’ve mentioned. And undoubtedly, the model and pretraining procedure can actually be improved. So we’re really excited to see what the next few years will bring.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TINGLE:&lt;/strong&gt; Wessel, tell us more about those opportunities and unanswered questions. What’s next on the research agenda in environmental prediction?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;BRUINSMA:&lt;/strong&gt; Well, Aurora has two main limitations. The first is that the model produces only deterministic predictions, by which I mean a single predicted value. For variables like temperature, this is mostly fine. But other variables like precipitation, they are inherently some kind of stochastic. For these variables, we really want to assign probabilities to different levels of precipitation rather than predicting only a single value.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;An extension of Aurora to allow this sort of prediction would be a great next step.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The second limitation is that Aurora depends on a procedure called assimilation. Assimilation attempts to create a starting point for the model from real-world observations, such as from weather stations and satellites. The model then takes the starting point and uses it to make predictions. Unfortunately, assimilation is super expensive, so it would be great if we could somehow circumvent the need for it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Finally, what we find really important is to make our advancements available to the community.&lt;/p&gt;



&lt;p&gt;[MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TINGLE:&lt;/strong&gt; Great. Megan and Wessel, thanks for joining us today on the Microsoft Research Podcast.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;BRUINSMA:&lt;/strong&gt; Thanks for having us.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;STANLEY:&lt;/strong&gt; Yeah, thank you. It’s been great.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TINGLE:&lt;/strong&gt; You can check out the Aurora model on Azure AI Foundry. You can read the entire paper, “A Foundation Model for the Earth System,” at aka.ms/abstracts. And you’ll certainly find it on the &lt;em&gt;Nature&lt;/em&gt; website, too.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Thank you so much for tuning in to &lt;em&gt;Abstracts&lt;/em&gt; today. Until next time.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&amp;nbsp;&lt;/p&gt;

				&lt;/span&gt;
			&lt;/div&gt;
			&lt;button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button"&gt;
				Show more			&lt;/button&gt;
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Abstracts podcast | Aurora with Megan Stanley and Wessel Bruinsma" class="wp-image-1139936" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Aurora_Wessel-and-Megan_Abstracts_Hero_Feature_No_Text_1400x788_.jpg" width="1400" /&gt;&lt;/figure&gt;






&lt;p&gt;Members of the research community at Microsoft work continuously to advance their respective fields.&lt;em&gt; Abstracts &lt;/em&gt;brings its audience to the cutting edge with them through short, compelling conversations about new and noteworthy achievements.&lt;/p&gt;



&lt;p&gt;In this episode of &lt;em&gt;Abstracts&lt;/em&gt;, Microsoft senior researchers Megan Stanley and Wessel Bruinsma join host Amber Tingle to discuss their groundbreaking work on environmental forecasting. Their new &lt;em&gt;Nature&lt;/em&gt; publication, “A Foundation Model for the Earth System,”&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; features Aurora, an AI model that redefines weather prediction and extends its capabilities to other environmental domains such as tropical cyclones and ocean wave forecasting.&lt;/p&gt;







&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h2 class="wp-block-heading h5" id="learn-more"&gt;Learn more&lt;/h2&gt;



&lt;p&gt;A foundation model for the Earth system&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;br /&gt;Nature | May 2025&lt;/p&gt;



&lt;p&gt;Introducing Aurora: The first large-scale foundation model of the atmosphere&lt;br /&gt;Microsoft Research Blog | June 2024&lt;/p&gt;



&lt;p&gt;Project Aurora: The first large-scale foundation model of the atmosphere&lt;br /&gt;Video | September 2024&lt;/p&gt;



&lt;p&gt;A Foundation Model for the Earth System&lt;br /&gt;Paper | November 2024&lt;/p&gt;



&lt;p&gt;Aurora&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;br /&gt;Azure AI Foundry Labs&lt;/p&gt;







&lt;section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast"&gt;
	
&lt;/section&gt;


&lt;div class="wp-block-msr-show-more"&gt;
	&lt;div class="bg-neutral-100 p-5"&gt;
		&lt;div class="show-more-show-less"&gt;
			&lt;div&gt;
				&lt;span&gt;
					

&lt;h2 class="wp-block-heading" id="transcript"&gt;Transcript&lt;/h2&gt;



&lt;p&gt;[MUSIC] &amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AMBER TINGLE:&lt;/strong&gt; Welcome to &lt;em&gt;Abstracts&lt;/em&gt;, a Microsoft Research Podcast that puts the spotlight on world-class research &lt;em&gt;in brief&lt;/em&gt;. I’m Amber Tingle. In this series, members of the research community at Microsoft give us a quick snapshot—or a &lt;em&gt;podcast abstract&lt;/em&gt;—of their new and noteworthy papers.&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Our guests today are Megan Stanley and Wessel Bruinsma. They are both senior researchers within the Microsoft Research AI for Science initiative. They are also two of the coauthors on a new &lt;em&gt;Nature&lt;/em&gt; publication called “A Foundation Model for the Earth System.”&lt;/p&gt;



				&lt;/span&gt;
				&lt;span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1"&gt;
					



&lt;p&gt;This is such exciting work about environmental forecasting, so we’re happy to have the two of you join us today.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Megan and Wessel, welcome.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MEGAN STANLEY:&lt;/strong&gt; Thank you. Thanks. Great to be here.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WESSEL BRUINSMA:&lt;/strong&gt; Thanks.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TINGLE:&lt;/strong&gt; Let’s jump right in. Wessel, share a bit about the problem your research addresses and why this work is so important.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;BRUINSMA:&lt;/strong&gt; I think we’re all very much aware of the revolution that’s happening in the space of large language models, which have just become so strong. What’s perhaps lesser well-known is that machine learning models have also started to revolutionize this field of weather prediction. Whereas traditional weather prediction models, based on physical laws, used to be the state of the art, these traditional models are now challenged and often even outperformed by AI models. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;This advancement is super impressive and really a big deal. Mostly because AI weather forecasting models are computationally much more efficient and can even be more accurate. What’s unfortunate though, about this big step forward, is that these developments are mostly limited to the setting of weather forecasting.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Weather forecasting is very important, obviously, but there are many other important environmental forecasting problems out there, such as air pollution forecasting or ocean wave forecasting. We have developed a model, named Aurora, which really kicks the AI revolution in weather forecasting into the next gear by extending these advancements to other environmental forecasting fields, too. With Aurora, we’re now able to produce state-of-the-art air pollution forecasts using an AI approach. And that wasn’t possible before!&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TINGLE:&lt;/strong&gt; Megan, how does this approach differ from or build on work that’s already been done in the atmospheric sciences?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;STANLEY:&lt;/strong&gt; Current approaches have really focused training very specifically on weather forecasting models. And in contrast, with Aurora, what we’ve attempted to do is train a so-called foundation model for the Earth system. In the first step, we train Aurora on a vast body of Earth system data. This is our pretraining step. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;And when I say a vast body of data, I really do mean a lot. And the purpose of this pretraining is to let Aurora, kind of, learn some general-purpose representation of the dynamics that govern the Earth system. But then once we’ve pretrained Aurora, and this really is the crux of this, the reason why we’re doing this project, is after the model has been pretrained, it can leverage this learned general-purpose representation and efficiently adapt to new tasks, new domains, new variables. And this is called fine-tuning.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The idea is that the model really uses the learned representation to perform this adaptation very efficiently, which basically means Aurora is a powerful, flexible model that can relatively cheaply be adapted to any environmental forecasting task.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TINGLE:&lt;/strong&gt; Wessel, can you tell us about your methodology? How did you all conduct this research?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;BRUINSMA:&lt;/strong&gt; While approaches so far have trained models on primarily one particular data&lt;s&gt; &lt;/s&gt;set, this one dataset is very large, which makes it possible to train very good models. But it does remain only one dataset, and that’s not very diverse. In the domain of environmental forecasting, we have really tried to push the limits of scaling to large data by training Aurora on not just this one large dataset, but on as many very large datasets as we could find.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;These datasets are a combination of estimates of the historical state of the world, forecasts by other models, climate simulations, and more. We’ve been able to show that training on not just more data but more diverse data helps the model achieve even better performance. Showing this is difficult because there is just so much &lt;em&gt;data&lt;/em&gt;. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;In addition to scaling to more and more diverse data, we also increased the size of the model as much as we could. Here we found that bigger models, despite being slower to run, make more efficient use of computational resources. It’s cheaper to train a good big model than a good small model. The mantra of this project was to really keep it simple and to scale to simultaneously very large and, more importantly, diverse data and large model size.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TINGLE:&lt;/strong&gt; So, Megan, what were your major findings? And we know they’re major because they’re in &lt;em&gt;N&lt;/em&gt;&lt;em&gt;ature&lt;/em&gt;. [LAUGHS]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;STANLEY:&lt;/strong&gt; Yeah, [LAUGHS] I guess they really are. So the main outcome of this project is we were actually able to train a single foundation model that achieves state-of-the-art performance in four different domains. Air pollution forecasting. For example, predicting particulate matter near the surface or ozone in the atmosphere. Ocean wave forecasting, which is critical for planning shipping routes. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;Tropical cyclone track forecasting, so that means being able to predict where a hurricane or a typhoon is expected to go, which is obviously incredibly important, and very high-resolution weather forecasting.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And I’ve, kind of, named these forecasting domains as if they’re just items in a list, but in every single one, Aurora really pushed the limits of what is possible with AI models. And we’re really proud of that. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;But perhaps, kind of, you know, to my mind, the key takeaway here is that the foundation model approach actually works. So what we have shown is it’s possible to actually train some kind of general model, a foundation model, and then adapt it to a wide variety of environmental tasks. Now we definitely do not claim that Aurora is some kind of ultimate environmental forecasting model. We are sure that the model and the pretraining procedure can actually be improved. But, nevertheless, we’ve shown that this approach works for environmental forecasting. It really holds massive promise, and that’s incredibly cool.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TINGLE:&lt;/strong&gt; Wessel, what do you think will be the real-world impact of this work?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;BRUINSMA:&lt;/strong&gt; Well, for applications that we mentioned, which are air pollution forecasting, ocean wave forecasting, tropical cyclone track forecasting, and very high-resolution weather forecasting, Aurora could today be deployed in real-time systems to produce near real-time forecasts. And, you know, in fact, it already is. You can view real-time weather forecasts by the high-resolution version of the model on the website of ECMWF (European Centre for Medium-Range Weather Forecasts).&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But what’s remarkable is that every of these applications took a small team of engineers about four to eight weeks to fully execute. You should compare this to a typical development timeline for more traditional models, which can be on the order of multiple years. Using the pretraining fine-tuning approach that we used for Aurora, we might see significantly accelerated development cycles for environmental forecasting problems. And that’s exciting.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TINGLE:&lt;/strong&gt; Megan, if our listeners only walk away from this conversation with one key talking point, what would you like that to be? What should we remember about this paper?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;STANLEY:&lt;/strong&gt; The biggest takeaway is that the pretraining fine-tuning paradigm, it really works for environmental forecasting, right? So you can train a foundational model, it learns some kind of general-purpose representation of the Earth system dynamics, and this representation boosts performance in a wide variety of forecasting tasks. But we really want to emphasize that Aurora only scratches the surface of what’s actually possible.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So there are many more applications to explore than the four we’ve mentioned. And undoubtedly, the model and pretraining procedure can actually be improved. So we’re really excited to see what the next few years will bring.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TINGLE:&lt;/strong&gt; Wessel, tell us more about those opportunities and unanswered questions. What’s next on the research agenda in environmental prediction?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;BRUINSMA:&lt;/strong&gt; Well, Aurora has two main limitations. The first is that the model produces only deterministic predictions, by which I mean a single predicted value. For variables like temperature, this is mostly fine. But other variables like precipitation, they are inherently some kind of stochastic. For these variables, we really want to assign probabilities to different levels of precipitation rather than predicting only a single value.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;An extension of Aurora to allow this sort of prediction would be a great next step.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The second limitation is that Aurora depends on a procedure called assimilation. Assimilation attempts to create a starting point for the model from real-world observations, such as from weather stations and satellites. The model then takes the starting point and uses it to make predictions. Unfortunately, assimilation is super expensive, so it would be great if we could somehow circumvent the need for it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Finally, what we find really important is to make our advancements available to the community.&lt;/p&gt;



&lt;p&gt;[MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TINGLE:&lt;/strong&gt; Great. Megan and Wessel, thanks for joining us today on the Microsoft Research Podcast.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;BRUINSMA:&lt;/strong&gt; Thanks for having us.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;STANLEY:&lt;/strong&gt; Yeah, thank you. It’s been great.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;TINGLE:&lt;/strong&gt; You can check out the Aurora model on Azure AI Foundry. You can read the entire paper, “A Foundation Model for the Earth System,” at aka.ms/abstracts. And you’ll certainly find it on the &lt;em&gt;Nature&lt;/em&gt; website, too.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Thank you so much for tuning in to &lt;em&gt;Abstracts&lt;/em&gt; today. Until next time.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&amp;nbsp;&lt;/p&gt;

				&lt;/span&gt;
			&lt;/div&gt;
			&lt;button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button"&gt;
				Show more			&lt;/button&gt;
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/podcast/abstracts-aurora-with-megan-stanley-and-wessel-bruinsma/</guid><pubDate>Wed, 21 May 2025 15:22:51 +0000</pubDate></item><item><title>Learning how to predict rare kinds of failures (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/learning-how-predict-rare-kinds-failures-0521</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202505/mit-rare-event-modeling.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;On Dec. 21, 2022, just as peak holiday season travel was getting underway, Southwest Airlines went through a cascading series of failures in their scheduling, initially triggered by severe winter weather in the Denver area. But the problems spread through their network, and over the course of the next 10 days the crisis ended up stranding over 2 million passengers and causing losses of $750 million for the airline.&lt;/p&gt;&lt;p&gt;How did a localized weather system end up triggering such a widespread failure? Researchers at MIT have examined this widely reported failure as an example of cases where systems that work smoothly most of the time suddenly break down and cause a domino effect of failures. They have now developed a computational system for using the combination of sparse data about a rare failure event, in combination with much more extensive data on normal operations, to work backwards and try to pinpoint the root causes of the failure, and hopefully be able to find ways to adjust the systems to prevent such failures in the future.&lt;/p&gt;&lt;p&gt;The findings were presented at the International Conference on Learning Representations (ICLR), which was held in Singapore from April 24-28 by MIT doctoral student Charles Dawson, professor of aeronautics and astronautics Chuchu Fan, and colleagues from Harvard University and the University of Michigan.&lt;/p&gt;&lt;p&gt;“The motivation behind this work is that it’s really frustrating when we have to interact with these complicated systems, where it’s really hard to understand what’s going on behind the scenes that’s creating these issues or failures that we’re observing,” says Dawson.&lt;/p&gt;&lt;p&gt;The new work builds on previous research from Fan’s lab, where they looked at problems involving hypothetical failure prediction problems, she says, such as with groups of robots working together on a task, or complex systems such as the power grid, looking for ways to predict how such systems may fail. “The goal of this project,” Fan says, “was really to turn that into a diagnostic tool that we could use on real-world systems.”&lt;/p&gt;&lt;p&gt;The idea was to provide a way that someone could “give us data from a time when this real-world system had an issue or a failure,” Dawson says, “and we can try to diagnose the root causes, and provide a little bit of a look behind the curtain at this complexity.”&lt;/p&gt;&lt;p&gt;The intent is for the methods they developed “to work for a pretty general class of cyber-physical problems,” he says. These are problems in which “you have an automated decision-making component interacting with the messiness of the real world,” he explains. There are available tools for testing software systems that operate on their own, but the complexity arises when that software has to interact with physical entities going about their activities in a real physical setting, whether it be the scheduling of aircraft, the movements of autonomous vehicles, the interactions of a team of robots, or the control of the inputs and outputs on an electric grid. In such systems, what often happens, he says, is that “the software might make a decision that looks OK at first, but then it has all these domino, knock-on effects that make things messier and much more uncertain.”&lt;/p&gt;&lt;p&gt;One key difference, though, is that in systems like teams of robots, unlike the scheduling of airplanes, “we have access to a model in the robotics world,” says Fan, who is a principal investigator in MIT’s Laboratory for Information and Decision Systems (LIDS). “We do have some good understanding of the physics behind the robotics, and we do have ways of creating a model” that represents their activities with reasonable accuracy. But airline scheduling involves processes and systems that are proprietary business information, and so the researchers had to find ways to infer what was behind the decisions, using only the relatively sparse publicly available information, which essentially consisted of just the actual arrival and departure times of each plane.&lt;/p&gt;&lt;p&gt;“We have grabbed all this flight data, but there is this entire system of the scheduling system behind it, and we don’t know how the system is working,” Fan says. And the amount of data relating to the actual failure is just several day’s worth, compared to years of data on normal flight operations.&lt;/p&gt;&lt;p&gt;The impact of the weather events in Denver during the week of Southwest’s scheduling crisis clearly showed up in the flight data, just from the longer-than-normal turnaround times between landing and takeoff at the Denver airport. But the way that impact cascaded though the system was less obvious, and required more analysis. The key turned out to have to do with the concept of reserve aircraft.&lt;/p&gt;&lt;p&gt;Airlines typically keep some planes in reserve at various airports, so that if problems are found with one plane that is scheduled for a flight, another plane can be quickly substituted. Southwest uses only a single type of plane, so they are all interchangeable, making such substitutions easier. But most airlines operate on a hub-and-spoke system, with a few designated hub airports where most of those reserve aircraft may be kept, whereas Southwest does not use hubs, so their reserve planes are more scattered throughout their network. And the way those planes were deployed turned out to play a major role in the unfolding crisis.&lt;/p&gt;&lt;p&gt;“The challenge is that there’s no public data available in terms of where the aircraft are stationed throughout the Southwest network,” Dawson says.&amp;nbsp;“What we’re able to find using our method is, by looking at the public data on arrivals, departures, and delays, we can use our method to back out what the hidden parameters of those aircraft reserves could have been, to explain the observations that we were seeing.”&lt;/p&gt;&lt;p&gt;What they found was that the way the reserves were deployed was a “leading indicator” of the problems that cascaded in a nationwide crisis. Some parts of the network that were affected directly by the weather were able to recover quickly and get back on schedule. “But when we looked at other areas in the network, we saw that these reserves were just not available, and things just kept getting worse.”&lt;/p&gt;&lt;p&gt;For example, the data showed that Denver’s reserves were rapidly dwindling because of the weather delays, but then “it also allowed us to trace this failure from Denver to Las Vegas,” he says. While there was no severe weather there, “our method was still showing us a steady decline in the number of aircraft that were able to serve flights out of Las Vegas.”&lt;/p&gt;&lt;p&gt;He says that “what we found was that there were these circulations of aircraft within the Southwest network, where an aircraft might start the day in California and then fly to Denver, and then end the day in Las Vegas.” What happened in the case of this storm was that the cycle got interrupted. As a result, “this one storm in Denver breaks the cycle, and suddenly the reserves in Las Vegas, which is not affected by the weather, start to deteriorate.”&lt;/p&gt;&lt;p&gt;In the end, Southwest was forced to take a drastic measure to resolve the problem: They had to do a “hard reset” of their entire system, canceling all flights and flying empty aircraft around the country to rebalance their reserves.&lt;/p&gt;&lt;p&gt;Working with experts in air transportation systems, the researchers developed a model of how the scheduling system is supposed to work. Then, “what our method does is, we’re essentially trying to run the model backwards.” Looking at the observed outcomes, the model allows them to work back to see what kinds of initial conditions could have produced those outcomes.&lt;/p&gt;&lt;p&gt;While the data on the actual failures were sparse, the extensive data on typical operations helped in teaching the computational model “what is feasible, what is possible, what’s the realm of physical possibility here,” Dawson says. “That gives us the domain knowledge to then say, in this extreme event, given the space of what’s possible, what’s the most likely explanation” for the failure.&lt;/p&gt;&lt;p&gt;This could lead to a real-time monitoring system, he says, where data on normal operations are constantly compared to the current data, and determining what the trend looks like. “Are we trending toward normal, or are we trending toward extreme events?” Seeing signs of impending issues could allow for preemptive measures, such as redeploying reserve aircraft in advance to areas of anticipated problems.&lt;/p&gt;&lt;p&gt;Work on developing such systems is ongoing in her lab, Fan says. In the meantime, they have produced an open-source tool for analyzing failure systems, called CalNF, which is available for anyone to use. Meanwhile Dawson, who earned his doctorate last year, is working as a postdoc to apply the methods developed in this work to understanding failures in power networks.&lt;/p&gt;&lt;p&gt;The research team also included Max Li from the University of Michigan and Van Tran from Harvard University. The work was supported by NASA, the Air Force Office of Scientific Research, and the MIT-DSTA program.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202505/mit-rare-event-modeling.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;On Dec. 21, 2022, just as peak holiday season travel was getting underway, Southwest Airlines went through a cascading series of failures in their scheduling, initially triggered by severe winter weather in the Denver area. But the problems spread through their network, and over the course of the next 10 days the crisis ended up stranding over 2 million passengers and causing losses of $750 million for the airline.&lt;/p&gt;&lt;p&gt;How did a localized weather system end up triggering such a widespread failure? Researchers at MIT have examined this widely reported failure as an example of cases where systems that work smoothly most of the time suddenly break down and cause a domino effect of failures. They have now developed a computational system for using the combination of sparse data about a rare failure event, in combination with much more extensive data on normal operations, to work backwards and try to pinpoint the root causes of the failure, and hopefully be able to find ways to adjust the systems to prevent such failures in the future.&lt;/p&gt;&lt;p&gt;The findings were presented at the International Conference on Learning Representations (ICLR), which was held in Singapore from April 24-28 by MIT doctoral student Charles Dawson, professor of aeronautics and astronautics Chuchu Fan, and colleagues from Harvard University and the University of Michigan.&lt;/p&gt;&lt;p&gt;“The motivation behind this work is that it’s really frustrating when we have to interact with these complicated systems, where it’s really hard to understand what’s going on behind the scenes that’s creating these issues or failures that we’re observing,” says Dawson.&lt;/p&gt;&lt;p&gt;The new work builds on previous research from Fan’s lab, where they looked at problems involving hypothetical failure prediction problems, she says, such as with groups of robots working together on a task, or complex systems such as the power grid, looking for ways to predict how such systems may fail. “The goal of this project,” Fan says, “was really to turn that into a diagnostic tool that we could use on real-world systems.”&lt;/p&gt;&lt;p&gt;The idea was to provide a way that someone could “give us data from a time when this real-world system had an issue or a failure,” Dawson says, “and we can try to diagnose the root causes, and provide a little bit of a look behind the curtain at this complexity.”&lt;/p&gt;&lt;p&gt;The intent is for the methods they developed “to work for a pretty general class of cyber-physical problems,” he says. These are problems in which “you have an automated decision-making component interacting with the messiness of the real world,” he explains. There are available tools for testing software systems that operate on their own, but the complexity arises when that software has to interact with physical entities going about their activities in a real physical setting, whether it be the scheduling of aircraft, the movements of autonomous vehicles, the interactions of a team of robots, or the control of the inputs and outputs on an electric grid. In such systems, what often happens, he says, is that “the software might make a decision that looks OK at first, but then it has all these domino, knock-on effects that make things messier and much more uncertain.”&lt;/p&gt;&lt;p&gt;One key difference, though, is that in systems like teams of robots, unlike the scheduling of airplanes, “we have access to a model in the robotics world,” says Fan, who is a principal investigator in MIT’s Laboratory for Information and Decision Systems (LIDS). “We do have some good understanding of the physics behind the robotics, and we do have ways of creating a model” that represents their activities with reasonable accuracy. But airline scheduling involves processes and systems that are proprietary business information, and so the researchers had to find ways to infer what was behind the decisions, using only the relatively sparse publicly available information, which essentially consisted of just the actual arrival and departure times of each plane.&lt;/p&gt;&lt;p&gt;“We have grabbed all this flight data, but there is this entire system of the scheduling system behind it, and we don’t know how the system is working,” Fan says. And the amount of data relating to the actual failure is just several day’s worth, compared to years of data on normal flight operations.&lt;/p&gt;&lt;p&gt;The impact of the weather events in Denver during the week of Southwest’s scheduling crisis clearly showed up in the flight data, just from the longer-than-normal turnaround times between landing and takeoff at the Denver airport. But the way that impact cascaded though the system was less obvious, and required more analysis. The key turned out to have to do with the concept of reserve aircraft.&lt;/p&gt;&lt;p&gt;Airlines typically keep some planes in reserve at various airports, so that if problems are found with one plane that is scheduled for a flight, another plane can be quickly substituted. Southwest uses only a single type of plane, so they are all interchangeable, making such substitutions easier. But most airlines operate on a hub-and-spoke system, with a few designated hub airports where most of those reserve aircraft may be kept, whereas Southwest does not use hubs, so their reserve planes are more scattered throughout their network. And the way those planes were deployed turned out to play a major role in the unfolding crisis.&lt;/p&gt;&lt;p&gt;“The challenge is that there’s no public data available in terms of where the aircraft are stationed throughout the Southwest network,” Dawson says.&amp;nbsp;“What we’re able to find using our method is, by looking at the public data on arrivals, departures, and delays, we can use our method to back out what the hidden parameters of those aircraft reserves could have been, to explain the observations that we were seeing.”&lt;/p&gt;&lt;p&gt;What they found was that the way the reserves were deployed was a “leading indicator” of the problems that cascaded in a nationwide crisis. Some parts of the network that were affected directly by the weather were able to recover quickly and get back on schedule. “But when we looked at other areas in the network, we saw that these reserves were just not available, and things just kept getting worse.”&lt;/p&gt;&lt;p&gt;For example, the data showed that Denver’s reserves were rapidly dwindling because of the weather delays, but then “it also allowed us to trace this failure from Denver to Las Vegas,” he says. While there was no severe weather there, “our method was still showing us a steady decline in the number of aircraft that were able to serve flights out of Las Vegas.”&lt;/p&gt;&lt;p&gt;He says that “what we found was that there were these circulations of aircraft within the Southwest network, where an aircraft might start the day in California and then fly to Denver, and then end the day in Las Vegas.” What happened in the case of this storm was that the cycle got interrupted. As a result, “this one storm in Denver breaks the cycle, and suddenly the reserves in Las Vegas, which is not affected by the weather, start to deteriorate.”&lt;/p&gt;&lt;p&gt;In the end, Southwest was forced to take a drastic measure to resolve the problem: They had to do a “hard reset” of their entire system, canceling all flights and flying empty aircraft around the country to rebalance their reserves.&lt;/p&gt;&lt;p&gt;Working with experts in air transportation systems, the researchers developed a model of how the scheduling system is supposed to work. Then, “what our method does is, we’re essentially trying to run the model backwards.” Looking at the observed outcomes, the model allows them to work back to see what kinds of initial conditions could have produced those outcomes.&lt;/p&gt;&lt;p&gt;While the data on the actual failures were sparse, the extensive data on typical operations helped in teaching the computational model “what is feasible, what is possible, what’s the realm of physical possibility here,” Dawson says. “That gives us the domain knowledge to then say, in this extreme event, given the space of what’s possible, what’s the most likely explanation” for the failure.&lt;/p&gt;&lt;p&gt;This could lead to a real-time monitoring system, he says, where data on normal operations are constantly compared to the current data, and determining what the trend looks like. “Are we trending toward normal, or are we trending toward extreme events?” Seeing signs of impending issues could allow for preemptive measures, such as redeploying reserve aircraft in advance to areas of anticipated problems.&lt;/p&gt;&lt;p&gt;Work on developing such systems is ongoing in her lab, Fan says. In the meantime, they have produced an open-source tool for analyzing failure systems, called CalNF, which is available for anyone to use. Meanwhile Dawson, who earned his doctorate last year, is working as a postdoc to apply the methods developed in this work to understanding failures in power networks.&lt;/p&gt;&lt;p&gt;The research team also included Max Li from the University of Michigan and Van Tran from Harvard University. The work was supported by NASA, the Air Force Office of Scientific Research, and the MIT-DSTA program.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/learning-how-predict-rare-kinds-failures-0521</guid><pubDate>Wed, 21 May 2025 20:35:00 +0000</pubDate></item><item><title>AI learns how vision and sound are connected, without human intervention (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/ai-learns-how-vision-and-sound-are-connected-without-human-intervention-0522</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202505/MIT-AV-Learning-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Humans naturally learn by making connections between sight and sound. For instance, we can watch someone playing the cello and recognize that the cellist’s movements are generating the music we hear.&lt;/p&gt;&lt;p&gt;A new approach developed by researchers from MIT and elsewhere improves an AI model’s ability to learn in this same fashion. This could be useful in applications such as journalism and film production, where the model could help with curating multimodal content through automatic video and audio retrieval.&lt;/p&gt;&lt;p&gt;In the longer term, this work could be used to improve a robot’s ability to understand real-world environments, where auditory and visual information are often closely connected.&lt;/p&gt;&lt;p&gt;Improving upon prior work from their group, the researchers created a method that helps machine-learning models align corresponding audio and visual data from video clips without the need for human labels.&lt;/p&gt;&lt;p&gt;They adjusted how their original model is trained so it learns a finer-grained correspondence between a particular video frame and the audio that occurs in that moment. The researchers also made some architectural tweaks that help the system balance two distinct learning objectives, which improves performance.&lt;/p&gt;&lt;p&gt;Taken together, these relatively simple improvements boost the accuracy of their approach in video retrieval tasks and in classifying the action in audiovisual scenes. For instance, the new method could automatically and precisely match the sound of a door slamming with the visual of it closing in a video clip.&lt;/p&gt;&lt;p&gt;“We are building AI systems that can process the world like humans do, in terms of having both audio and visual information coming in at once and being able to seamlessly process both modalities. Looking forward, if we can integrate this audio-visual technology into some of the tools we use on a daily basis, like large language models, it could open up a lot of new applications,” says Andrew Rouditchenko, an MIT graduate student and co-author of a paper on this research.&lt;/p&gt;&lt;p&gt;He is joined on the paper by lead author Edson Araujo, a graduate student at Goethe University in Germany; Yuan Gong, a former MIT postdoc; Saurabhchand Bhati, a current MIT postdoc; Samuel Thomas, Brian Kingsbury, and Leonid Karlinsky of IBM Research; Rogerio Feris, principal scientist and manager at the MIT-IBM Watson AI Lab; James Glass, senior research scientist and head of the Spoken Language Systems Group in the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL); and senior author Hilde Kuehne, professor of computer science at Goethe University and an affiliated professor at the MIT-IBM Watson AI Lab. The work will be presented at the Conference on Computer Vision and Pattern Recognition.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Syncing up&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;This work builds upon a machine-learning method the researchers developed a few years ago, which provided an efficient way to train a multimodal model to simultaneously process audio and visual data without the need for human labels.&lt;/p&gt;&lt;p&gt;The researchers feed this model, called CAV-MAE, unlabeled video clips and it encodes the visual and audio data separately into representations called tokens. Using the natural audio from the recording, the model automatically learns to map corresponding pairs of audio and visual tokens close together within its internal representation space.&lt;/p&gt;&lt;p&gt;They found that using two learning objectives balances the model’s learning process, which enables CAV-MAE to understand the corresponding audio and visual data while improving its ability to recover video clips that match user queries.&lt;/p&gt;&lt;p&gt;But CAV-MAE treats audio and visual samples as one unit, so a 10-second video clip and the sound of a door slamming are mapped together, even if that audio event happens in just one second of the video.&lt;/p&gt;&lt;p&gt;In their improved model, called CAV-MAE Sync, the researchers split the audio into smaller windows before the model computes its representations of the data, so it generates separate representations that correspond to each smaller window of audio.&lt;/p&gt;&lt;p&gt;During training, the model learns to associate one video frame with the audio that occurs during just that frame.&lt;/p&gt;&lt;p&gt;“By doing that, the model learns a finer-grained correspondence, which helps with performance later when we aggregate this information,” Araujo says.&lt;/p&gt;&lt;p&gt;They also incorporated architectural improvements that help the model balance its two learning objectives.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Adding “wiggle room”&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The model incorporates a contrastive objective, where it learns to associate similar audio and visual data, and a reconstruction objective which aims to recover specific audio and visual data based on user queries.&lt;/p&gt;&lt;p&gt;In CAV-MAE Sync, the researchers introduced two new types of data representations, or tokens, to improve the model’s learning ability.&lt;/p&gt;&lt;p&gt;They include dedicated “global tokens” that help with the contrastive learning objective and dedicated “register tokens” that help the model focus on important details for the reconstruction objective.&lt;/p&gt;&lt;p&gt;“Essentially, we add a bit more wiggle room to the model so it can perform each of these two tasks, contrastive and reconstructive, a bit more independently. That benefitted overall performance,” Araujo adds.&lt;/p&gt;&lt;p&gt;While the researchers had some intuition these enhancements would improve the performance of CAV-MAE Sync, it took a careful combination of strategies to shift the model in the direction they wanted it to go.&lt;/p&gt;&lt;p&gt;“Because we have multiple modalities, we need a good model for both modalities by themselves, but we also need to get them to fuse together and collaborate,” Rouditchenko says.&lt;/p&gt;&lt;p&gt;In the end, their enhancements improved the model’s ability to retrieve videos based on an audio query and predict the class of an audio-visual scene, like a dog barking or an instrument playing.&lt;/p&gt;&lt;p&gt;Its results were more accurate than their prior work, and it also performed better than more complex, state-of-the-art methods that require larger amounts of training data.&lt;/p&gt;&lt;p&gt;“Sometimes, very simple ideas or little patterns you see in the data have big value when applied on top of a model you are working on,” Araujo says.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to incorporate new models that generate better data representations into CAV-MAE Sync, which could improve performance. They also want to enable their system to handle text data, which would be an important step toward generating an audiovisual large language model.&lt;/p&gt;&lt;p&gt;This work is funded, in part, by the German Federal Ministry of Education and Research and the MIT-IBM Watson AI Lab.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202505/MIT-AV-Learning-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Humans naturally learn by making connections between sight and sound. For instance, we can watch someone playing the cello and recognize that the cellist’s movements are generating the music we hear.&lt;/p&gt;&lt;p&gt;A new approach developed by researchers from MIT and elsewhere improves an AI model’s ability to learn in this same fashion. This could be useful in applications such as journalism and film production, where the model could help with curating multimodal content through automatic video and audio retrieval.&lt;/p&gt;&lt;p&gt;In the longer term, this work could be used to improve a robot’s ability to understand real-world environments, where auditory and visual information are often closely connected.&lt;/p&gt;&lt;p&gt;Improving upon prior work from their group, the researchers created a method that helps machine-learning models align corresponding audio and visual data from video clips without the need for human labels.&lt;/p&gt;&lt;p&gt;They adjusted how their original model is trained so it learns a finer-grained correspondence between a particular video frame and the audio that occurs in that moment. The researchers also made some architectural tweaks that help the system balance two distinct learning objectives, which improves performance.&lt;/p&gt;&lt;p&gt;Taken together, these relatively simple improvements boost the accuracy of their approach in video retrieval tasks and in classifying the action in audiovisual scenes. For instance, the new method could automatically and precisely match the sound of a door slamming with the visual of it closing in a video clip.&lt;/p&gt;&lt;p&gt;“We are building AI systems that can process the world like humans do, in terms of having both audio and visual information coming in at once and being able to seamlessly process both modalities. Looking forward, if we can integrate this audio-visual technology into some of the tools we use on a daily basis, like large language models, it could open up a lot of new applications,” says Andrew Rouditchenko, an MIT graduate student and co-author of a paper on this research.&lt;/p&gt;&lt;p&gt;He is joined on the paper by lead author Edson Araujo, a graduate student at Goethe University in Germany; Yuan Gong, a former MIT postdoc; Saurabhchand Bhati, a current MIT postdoc; Samuel Thomas, Brian Kingsbury, and Leonid Karlinsky of IBM Research; Rogerio Feris, principal scientist and manager at the MIT-IBM Watson AI Lab; James Glass, senior research scientist and head of the Spoken Language Systems Group in the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL); and senior author Hilde Kuehne, professor of computer science at Goethe University and an affiliated professor at the MIT-IBM Watson AI Lab. The work will be presented at the Conference on Computer Vision and Pattern Recognition.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Syncing up&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;This work builds upon a machine-learning method the researchers developed a few years ago, which provided an efficient way to train a multimodal model to simultaneously process audio and visual data without the need for human labels.&lt;/p&gt;&lt;p&gt;The researchers feed this model, called CAV-MAE, unlabeled video clips and it encodes the visual and audio data separately into representations called tokens. Using the natural audio from the recording, the model automatically learns to map corresponding pairs of audio and visual tokens close together within its internal representation space.&lt;/p&gt;&lt;p&gt;They found that using two learning objectives balances the model’s learning process, which enables CAV-MAE to understand the corresponding audio and visual data while improving its ability to recover video clips that match user queries.&lt;/p&gt;&lt;p&gt;But CAV-MAE treats audio and visual samples as one unit, so a 10-second video clip and the sound of a door slamming are mapped together, even if that audio event happens in just one second of the video.&lt;/p&gt;&lt;p&gt;In their improved model, called CAV-MAE Sync, the researchers split the audio into smaller windows before the model computes its representations of the data, so it generates separate representations that correspond to each smaller window of audio.&lt;/p&gt;&lt;p&gt;During training, the model learns to associate one video frame with the audio that occurs during just that frame.&lt;/p&gt;&lt;p&gt;“By doing that, the model learns a finer-grained correspondence, which helps with performance later when we aggregate this information,” Araujo says.&lt;/p&gt;&lt;p&gt;They also incorporated architectural improvements that help the model balance its two learning objectives.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Adding “wiggle room”&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The model incorporates a contrastive objective, where it learns to associate similar audio and visual data, and a reconstruction objective which aims to recover specific audio and visual data based on user queries.&lt;/p&gt;&lt;p&gt;In CAV-MAE Sync, the researchers introduced two new types of data representations, or tokens, to improve the model’s learning ability.&lt;/p&gt;&lt;p&gt;They include dedicated “global tokens” that help with the contrastive learning objective and dedicated “register tokens” that help the model focus on important details for the reconstruction objective.&lt;/p&gt;&lt;p&gt;“Essentially, we add a bit more wiggle room to the model so it can perform each of these two tasks, contrastive and reconstructive, a bit more independently. That benefitted overall performance,” Araujo adds.&lt;/p&gt;&lt;p&gt;While the researchers had some intuition these enhancements would improve the performance of CAV-MAE Sync, it took a careful combination of strategies to shift the model in the direction they wanted it to go.&lt;/p&gt;&lt;p&gt;“Because we have multiple modalities, we need a good model for both modalities by themselves, but we also need to get them to fuse together and collaborate,” Rouditchenko says.&lt;/p&gt;&lt;p&gt;In the end, their enhancements improved the model’s ability to retrieve videos based on an audio query and predict the class of an audio-visual scene, like a dog barking or an instrument playing.&lt;/p&gt;&lt;p&gt;Its results were more accurate than their prior work, and it also performed better than more complex, state-of-the-art methods that require larger amounts of training data.&lt;/p&gt;&lt;p&gt;“Sometimes, very simple ideas or little patterns you see in the data have big value when applied on top of a model you are working on,” Araujo says.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to incorporate new models that generate better data representations into CAV-MAE Sync, which could improve performance. They also want to enable their system to handle text data, which would be an important step toward generating an audiovisual large language model.&lt;/p&gt;&lt;p&gt;This work is funded, in part, by the German Federal Ministry of Education and Research and the MIT-IBM Watson AI Lab.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/ai-learns-how-vision-and-sound-are-connected-without-human-intervention-0522</guid><pubDate>Thu, 22 May 2025 04:00:00 +0000</pubDate></item><item><title>Abstracts: Zero-shot models in single-cell biology with Alex Lu (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/podcast/abstracts-zero-shot-models-in-single-cell-biology-with-alex-lu/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Illustrated headshot of Alex Lu." class="wp-image-1139912" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/AlexLu-@Abstracts_Hero_Feature_No_Text_1400x788.jpg" width="1400" /&gt;&lt;/figure&gt;






&lt;p&gt;Members of the research community at Microsoft work continuously to advance their respective fields. The Abstracts podcast brings its audience to the cutting edge with them through short, compelling conversations about new and noteworthy achievements.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The success of foundation models like ChatGPT has sparked growing interest in scientific communities seeking to use AI for things like discovery in single-cell biology. In this episode, senior researcher Alex Lu joins host Gretchen Huizinga to talk about his work on a paper called &lt;strong&gt;&lt;em&gt;Assessing the limits of zero-shot foundation models in single-cell biology&lt;/em&gt;&lt;/strong&gt;, where researchers tested zero-shot performance of proposed single-cell foundation models. Results showed limited efficacy compared to older, simpler methods, and suggested the need for more rigorous evaluation and research.&amp;nbsp;&lt;/p&gt;







&lt;div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow"&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h2 class="wp-block-heading h5" id="learn-more-1"&gt;Learn more:&lt;/h2&gt;




&lt;/div&gt;



&lt;section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast"&gt;
	
&lt;/section&gt;


&lt;div class="wp-block-msr-show-more"&gt;
	&lt;div class="bg-neutral-100 p-5"&gt;
		&lt;div class="show-more-show-less"&gt;
			&lt;div&gt;
				&lt;span&gt;
					

&lt;h2 class="wp-block-heading" id="transcript"&gt;Transcript&lt;/h2&gt;



&lt;p&gt;[MUSIC]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;GRETCHEN HUIZINGA:&lt;/strong&gt; Welcome to Abstracts, a Microsoft Research Podcast that puts the spotlight on world-class research in brief. I’m Gretchen Huizinga. In this series, members of the research community at Microsoft give us a quick snapshot – or a podcast abstract – of their new and noteworthy papers.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&lt;/p&gt;



&lt;p&gt;On today’s episode, I’m talking to Alex Lu, a senior researcher at Microsoft Research and co-author of a paper called &lt;strong&gt;&lt;em&gt;Assessing the Limits of Zero Shot Foundation Models in Single-cell Biology&lt;/em&gt;&lt;/strong&gt;. Alex Lu, wonderful to have you on the podcast. Welcome to Abstracts!&amp;nbsp;&lt;/p&gt;



				&lt;/span&gt;
				&lt;span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1"&gt;
					



&lt;p&gt;&lt;strong&gt;ALEX LU:&lt;/strong&gt; Yeah, I’m really excited to be joining you today.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;&lt;strong&gt;HUIZINGA&lt;/strong&gt;:&lt;/strong&gt; So let’s start with a little background of your work. In just a few sentences, tell us about your study and more importantly, why it matters.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LU:&lt;/strong&gt; Absolutely. And before I dive in, I want to give a shout out to the MSR research intern who actually did this work. This was led by Kasia Kedzierska, who interned with us two summers ago in 2023, and she’s the lead author on the study. But basically, in this research, we study single-cell foundation models, which have really recently rocked the world of biology, because they basically claim to be able to use AI to unlock understanding about single-cell biology. Biologists for a myriad of applications, everything from understanding how single cells differentiate into different kinds of cells, to discovering new drugs for cancer, will conduct experiments where they measure how much of every gene is expressed inside of just one single cell. So these experiments give us a powerful view into the cell’s internal state. But measurements from these experiments are incredibly complex. There are about 20,000 different human genes. So you get this really long chain of numbers that measure how much there is of 20,000 different genes. So deriving meaning from this really long chain of numbers is really difficult. And single-cell foundation models claim to be capable of unraveling deeper insights than ever before. So that’s the claim that these works have made. And in our recent paper, we showed that these models may actually not live up to these claims. Basically, we showed that single-cell foundation models perform worse in settings that are fundamental to biological discovery than much simpler machine learning and statistical methods that were used in the field before single-cell foundation models emerged and are the go-to standard for unpacking meaning from these complicated experiments. So in a nutshell, we should care about these results because it has implications on the toolkits that biologists use to understand their experiments. Our work suggests that single-cell foundation models may not be appropriate for practical use just yet, at least in the discovery applications that we cover.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;HUIZINGA&lt;/strong&gt;&lt;/strong&gt;:&lt;/strong&gt; Well, let’s go a little deeper there. Generative pre-trained transformer models, GPTs, are relatively new on the research scene in terms of how they’re being used in novel applications, which is what you’re interested in, like single-cell biology. So I’m curious, just sort of as a foundation, what other research has already been done in this area, and how does this study illuminate or build on it?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LU:&lt;/strong&gt; Absolutely. Okay, so we were the first to notice and document this issue in single-cell foundation models, specifically. And this is because that we have proposed evaluation methods that, while are common in other areas of AI, have yet to be commonly used to evaluate single-cell foundation models. We performed something called zero-shot evaluation on these models. Prior to our work, most works evaluated single-cell foundation models with fine tuning. And the way to understand this is because single-cell foundation models are trained in a way that tries to expose these models to millions of single-cells. But because you’re exposing them to a large amount of data, you can’t really rely upon this data being annotated or like labeled in any particular fashion then. So in order for them to actually do the specialized tasks that are useful for biologists, you typically have to add on a second training phase. We call this the fine-tuning phase, where you have a smaller number of single cells, but now they are actually labeled with the specialized tasks that you want the model to perform. So most people, they typically evaluate the performance of single-cell models after they fine-tune these models. However, what we noticed is that this evaluating these fine-tuned models has several problems. First, it might not actually align with how these models are actually going to be used by biologists then. A critical distinction in biology is that we’re not just trying to interact with an agent that has access to knowledge through its pre-training, we’re trying to extend these models to discover new biology beyond the sphere of influence then. And so in many cases, the point of using these models, the point of analysis, is to explore the data with the goal of potentially discovering something new about the single cell that the biologists worked with that they weren’t aware of before. So in these kinds of cases, it is really tough to fine-tune a model. There’s a bit of a chicken and egg problem going on. If you don’t know, for example, there’s a new kind of cell in the data, you can’t really instruct the model to help us identify these kinds of new cells. So in other words, fine-tuning these models for those tasks essentially becomes impossible then. So the second issue is that evaluations on fine-tuned models can sometimes mislead us in our ability to understand how these models are working. So for example, the claim behind single-cell foundation model papers is that these models learn a foundation of biological knowledge by being exposed to millions of single cells in its first training phase, right? But it’s possible when you fine-tune a model, it may just be that any performance increases that you see using the model is simply because that you’re using a massive model that is really sophisticated, really large. And even if there’s any exposure to any cells at all then, that model is going to do perfectly fine then. So going back to our paper, what’s really different about this paper is that we propose zero-shot evaluation for these models. What that means is that we do not fine-tune the model at all, and instead we keep the model frozen during the analysis step. So how we specialize it to be a downstream task instead is that we extract the model’s internal embedding of single-cell data, which is essentially a numerical vector that contains information that the model is extracting and organizing from input data. So it’s essentially how the model perceives single-cell data and how it’s organizing in its own internal state. So basically, this is the better way for us to test the claim that single-cell foundation models are learning foundational biological insights. Because if they actually are learning these insights, they should be present in the models embedding space even before we fine-tune the model.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;HUIZINGA&lt;/strong&gt;&lt;/strong&gt;:&lt;/strong&gt; Well, let’s talk about methodology on this particular study. You focused on assessing existing models in zero-shot learning for single-cell biology. How did you go about evaluating these models?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LU:&lt;/strong&gt; Yes, so let’s dive deeper into how zero-shot evaluations are conducted, okay? So the premise here is that we’re relying upon the fact that if these models are fully learning foundational biological insights, if we take the model’s internal representation of cells, then cells that are biologically similar should be close in that internal representation, where cells that are biologically distinct should be further apart. And that is exactly what we tested in our study. We compared two popular single-cell foundation models and importantly, we compared these models against older and reliable tools that biologists have used for exploratory analyses. So these include simpler machine learning methods like scVI, statistical algorithms like Harmony, and even basic data pre-processing steps, just like filtering your data down to a more robust subset of genes, then. So basically, we tested embeddings from our two single-cell foundation models against this baseline in a variety of settings. And we tested the hypothesis that biologically similar cells should be similar across these distinct methods across these datasets.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;HUIZINGA&lt;/strong&gt;&lt;/strong&gt;:&lt;/strong&gt; Well, and as you as you did the testing, you obviously were aiming towards research findings, which is my favorite part of a research paper, so tell us what you did find and what you feel the most important takeaways of this paper are.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LU:&lt;/strong&gt; Absolutely. So in a nutshell, we found that these two newly proposed single-cell foundation models substantially underperformed compared to older methods then. So to contextualize why that is such a surprising result, there is a lot of hype around these methods. So basically, I think that,yeah, it’s a very surprising result, given how hyped these models are and how people were already adopting them. But our results basically caution that these shouldn’t really be adopted for these use purposes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;HUIZINGA&lt;/strong&gt;&lt;/strong&gt;:&lt;/strong&gt; Yeah, so this is serious real-world impact here in terms of if models are being adopted and adapted in these applications, how reliable are they, et cetera? So given that, who would you say benefits most from what you’ve discovered in this paper and why?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LU:&lt;/strong&gt; Okay, so two ways, right? So I think this has at least immediate implications on the way that we do discovery in biology. And as I’ve discussed, these experiments are used for cases that have practical impact, drug discovery applications, investigations into basic biology, then. But let’s also talk about the impact for methodologists, people who are trying to improve these single-cell foundation models, right? I think at the base, they’re really excited proposals. Because if you look at what some of the prior and less sophisticated methods couldn’t do, they tended to be more bespoke. So the excitement of single-cell foundation models is that you have this general-purpose model that can be used for everything and while they’re not living up to that purpose just now, just currently, I think that it’s important that we continue to bank onto that vision, right? So if you look at our contributions in that area, where single-cell foundation models are a really new proposal, so it makes sense that we may not know how to fully evaluate them just yet then. So you can view our work as basically being a step towards more rigorous evaluation of these models. Now that we did this experiment, I think the methodologists know to use this as a signal on how to improve the models and if they’re going in the right direction. And in fact, you are seeing more and more papers adopt zero-shot evaluations since we put out our paper then. And so this essentially helps future computer scientists that are working on single-cell foundation models know how to train better models.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;HUIZINGA&lt;/strong&gt;&lt;/strong&gt;:&lt;/strong&gt; That said, Alex, finally, what are the outstanding challenges that you identified for zero-shot learning research in biology, and what foundation might this paper lay for future research agendas in the field?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LU:&lt;/strong&gt; Yeah, absolutely. So now that we’ve shown single-cell foundation models don’t necessarily perform well, I think the natural question on everyone’s mind is how do we actually train single-cell foundation models that live up to that vision, that can perform in helping us discover new biology then? So I think in the short term, yeah, we’re actively investigating many hypotheses in this area. So for example, my colleagues, Lorin Crawford and Ava Amini, who were co-authors in the paper, recently put out a pre-print understanding how training data composition impacts model performance. And so one of the surprising findings that they had was that many of the training data sets that people used to train single-cell foundation models are highly redundant, to the point that you can even sample just a tiny fraction of the data and get basically the same performance then. But you can also look forward to many other explorations in this area as we continue to develop this research at the end of the day. But also zooming out into the bigger picture, I think one major takeaway from this paper is that developing AI methods for biology requires thought about the context of use, right? I mean, this is obvious for any AI method then, but I think people have gotten just too used to taking methods that work out there for natural vision or natural language maybe in the consumer domain and then extrapolating these methods to biology and expecting that they will work in the same way then, right? So for example, one reason why zero-shot evaluation was not routine practice for single-cell foundation models prior to our work, I mean, we were the first to fully establish that as a practice for the field, was because I think people who have been working in AI for biology have been looking to these more mainstream AI domains to shape their work then. And so with single-cell foundation models, many of these models are adopted from large language models with natural language processing, recycling the exact same architecture, the exact same code, basically just recycling practices in that field then. So when you look at like practices in like more mainstream domains, zero-shot evaluation is definitely explored in those domains, but it’s more of like a niche instead of being considered central to model understanding. So again, because biology is different from mainstream language processing, it’s a scientific discipline, zero-shot evaluation becomes much more important, and you have no choice but to use these models, zero-shot then. So in other words, I think that we need to be thinking carefully about what it is that makes training a model for biology different from training a model, for example, for consumer purposes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HUIZINGA:&lt;/strong&gt; Alex Lu, thanks for joining us today, and to our listeners, thanks for tuning in. If you want to read this paper, you can find a link at aka.ms/Abstracts, or you can read it on the Genome Biology website. See you next time on Abstracts!&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&amp;nbsp;&lt;/p&gt;

				&lt;/span&gt;
			&lt;/div&gt;
			&lt;button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button"&gt;
				Show more			&lt;/button&gt;
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Illustrated headshot of Alex Lu." class="wp-image-1139912" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/AlexLu-@Abstracts_Hero_Feature_No_Text_1400x788.jpg" width="1400" /&gt;&lt;/figure&gt;






&lt;p&gt;Members of the research community at Microsoft work continuously to advance their respective fields. The Abstracts podcast brings its audience to the cutting edge with them through short, compelling conversations about new and noteworthy achievements.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The success of foundation models like ChatGPT has sparked growing interest in scientific communities seeking to use AI for things like discovery in single-cell biology. In this episode, senior researcher Alex Lu joins host Gretchen Huizinga to talk about his work on a paper called &lt;strong&gt;&lt;em&gt;Assessing the limits of zero-shot foundation models in single-cell biology&lt;/em&gt;&lt;/strong&gt;, where researchers tested zero-shot performance of proposed single-cell foundation models. Results showed limited efficacy compared to older, simpler methods, and suggested the need for more rigorous evaluation and research.&amp;nbsp;&lt;/p&gt;







&lt;div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow"&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h2 class="wp-block-heading h5" id="learn-more-1"&gt;Learn more:&lt;/h2&gt;




&lt;/div&gt;



&lt;section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast"&gt;
	
&lt;/section&gt;


&lt;div class="wp-block-msr-show-more"&gt;
	&lt;div class="bg-neutral-100 p-5"&gt;
		&lt;div class="show-more-show-less"&gt;
			&lt;div&gt;
				&lt;span&gt;
					

&lt;h2 class="wp-block-heading" id="transcript"&gt;Transcript&lt;/h2&gt;



&lt;p&gt;[MUSIC]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;GRETCHEN HUIZINGA:&lt;/strong&gt; Welcome to Abstracts, a Microsoft Research Podcast that puts the spotlight on world-class research in brief. I’m Gretchen Huizinga. In this series, members of the research community at Microsoft give us a quick snapshot – or a podcast abstract – of their new and noteworthy papers.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&lt;/p&gt;



&lt;p&gt;On today’s episode, I’m talking to Alex Lu, a senior researcher at Microsoft Research and co-author of a paper called &lt;strong&gt;&lt;em&gt;Assessing the Limits of Zero Shot Foundation Models in Single-cell Biology&lt;/em&gt;&lt;/strong&gt;. Alex Lu, wonderful to have you on the podcast. Welcome to Abstracts!&amp;nbsp;&lt;/p&gt;



				&lt;/span&gt;
				&lt;span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1"&gt;
					



&lt;p&gt;&lt;strong&gt;ALEX LU:&lt;/strong&gt; Yeah, I’m really excited to be joining you today.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;&lt;strong&gt;HUIZINGA&lt;/strong&gt;:&lt;/strong&gt; So let’s start with a little background of your work. In just a few sentences, tell us about your study and more importantly, why it matters.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LU:&lt;/strong&gt; Absolutely. And before I dive in, I want to give a shout out to the MSR research intern who actually did this work. This was led by Kasia Kedzierska, who interned with us two summers ago in 2023, and she’s the lead author on the study. But basically, in this research, we study single-cell foundation models, which have really recently rocked the world of biology, because they basically claim to be able to use AI to unlock understanding about single-cell biology. Biologists for a myriad of applications, everything from understanding how single cells differentiate into different kinds of cells, to discovering new drugs for cancer, will conduct experiments where they measure how much of every gene is expressed inside of just one single cell. So these experiments give us a powerful view into the cell’s internal state. But measurements from these experiments are incredibly complex. There are about 20,000 different human genes. So you get this really long chain of numbers that measure how much there is of 20,000 different genes. So deriving meaning from this really long chain of numbers is really difficult. And single-cell foundation models claim to be capable of unraveling deeper insights than ever before. So that’s the claim that these works have made. And in our recent paper, we showed that these models may actually not live up to these claims. Basically, we showed that single-cell foundation models perform worse in settings that are fundamental to biological discovery than much simpler machine learning and statistical methods that were used in the field before single-cell foundation models emerged and are the go-to standard for unpacking meaning from these complicated experiments. So in a nutshell, we should care about these results because it has implications on the toolkits that biologists use to understand their experiments. Our work suggests that single-cell foundation models may not be appropriate for practical use just yet, at least in the discovery applications that we cover.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;HUIZINGA&lt;/strong&gt;&lt;/strong&gt;:&lt;/strong&gt; Well, let’s go a little deeper there. Generative pre-trained transformer models, GPTs, are relatively new on the research scene in terms of how they’re being used in novel applications, which is what you’re interested in, like single-cell biology. So I’m curious, just sort of as a foundation, what other research has already been done in this area, and how does this study illuminate or build on it?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LU:&lt;/strong&gt; Absolutely. Okay, so we were the first to notice and document this issue in single-cell foundation models, specifically. And this is because that we have proposed evaluation methods that, while are common in other areas of AI, have yet to be commonly used to evaluate single-cell foundation models. We performed something called zero-shot evaluation on these models. Prior to our work, most works evaluated single-cell foundation models with fine tuning. And the way to understand this is because single-cell foundation models are trained in a way that tries to expose these models to millions of single-cells. But because you’re exposing them to a large amount of data, you can’t really rely upon this data being annotated or like labeled in any particular fashion then. So in order for them to actually do the specialized tasks that are useful for biologists, you typically have to add on a second training phase. We call this the fine-tuning phase, where you have a smaller number of single cells, but now they are actually labeled with the specialized tasks that you want the model to perform. So most people, they typically evaluate the performance of single-cell models after they fine-tune these models. However, what we noticed is that this evaluating these fine-tuned models has several problems. First, it might not actually align with how these models are actually going to be used by biologists then. A critical distinction in biology is that we’re not just trying to interact with an agent that has access to knowledge through its pre-training, we’re trying to extend these models to discover new biology beyond the sphere of influence then. And so in many cases, the point of using these models, the point of analysis, is to explore the data with the goal of potentially discovering something new about the single cell that the biologists worked with that they weren’t aware of before. So in these kinds of cases, it is really tough to fine-tune a model. There’s a bit of a chicken and egg problem going on. If you don’t know, for example, there’s a new kind of cell in the data, you can’t really instruct the model to help us identify these kinds of new cells. So in other words, fine-tuning these models for those tasks essentially becomes impossible then. So the second issue is that evaluations on fine-tuned models can sometimes mislead us in our ability to understand how these models are working. So for example, the claim behind single-cell foundation model papers is that these models learn a foundation of biological knowledge by being exposed to millions of single cells in its first training phase, right? But it’s possible when you fine-tune a model, it may just be that any performance increases that you see using the model is simply because that you’re using a massive model that is really sophisticated, really large. And even if there’s any exposure to any cells at all then, that model is going to do perfectly fine then. So going back to our paper, what’s really different about this paper is that we propose zero-shot evaluation for these models. What that means is that we do not fine-tune the model at all, and instead we keep the model frozen during the analysis step. So how we specialize it to be a downstream task instead is that we extract the model’s internal embedding of single-cell data, which is essentially a numerical vector that contains information that the model is extracting and organizing from input data. So it’s essentially how the model perceives single-cell data and how it’s organizing in its own internal state. So basically, this is the better way for us to test the claim that single-cell foundation models are learning foundational biological insights. Because if they actually are learning these insights, they should be present in the models embedding space even before we fine-tune the model.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;HUIZINGA&lt;/strong&gt;&lt;/strong&gt;:&lt;/strong&gt; Well, let’s talk about methodology on this particular study. You focused on assessing existing models in zero-shot learning for single-cell biology. How did you go about evaluating these models?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LU:&lt;/strong&gt; Yes, so let’s dive deeper into how zero-shot evaluations are conducted, okay? So the premise here is that we’re relying upon the fact that if these models are fully learning foundational biological insights, if we take the model’s internal representation of cells, then cells that are biologically similar should be close in that internal representation, where cells that are biologically distinct should be further apart. And that is exactly what we tested in our study. We compared two popular single-cell foundation models and importantly, we compared these models against older and reliable tools that biologists have used for exploratory analyses. So these include simpler machine learning methods like scVI, statistical algorithms like Harmony, and even basic data pre-processing steps, just like filtering your data down to a more robust subset of genes, then. So basically, we tested embeddings from our two single-cell foundation models against this baseline in a variety of settings. And we tested the hypothesis that biologically similar cells should be similar across these distinct methods across these datasets.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;HUIZINGA&lt;/strong&gt;&lt;/strong&gt;:&lt;/strong&gt; Well, and as you as you did the testing, you obviously were aiming towards research findings, which is my favorite part of a research paper, so tell us what you did find and what you feel the most important takeaways of this paper are.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LU:&lt;/strong&gt; Absolutely. So in a nutshell, we found that these two newly proposed single-cell foundation models substantially underperformed compared to older methods then. So to contextualize why that is such a surprising result, there is a lot of hype around these methods. So basically, I think that,yeah, it’s a very surprising result, given how hyped these models are and how people were already adopting them. But our results basically caution that these shouldn’t really be adopted for these use purposes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;HUIZINGA&lt;/strong&gt;&lt;/strong&gt;:&lt;/strong&gt; Yeah, so this is serious real-world impact here in terms of if models are being adopted and adapted in these applications, how reliable are they, et cetera? So given that, who would you say benefits most from what you’ve discovered in this paper and why?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LU:&lt;/strong&gt; Okay, so two ways, right? So I think this has at least immediate implications on the way that we do discovery in biology. And as I’ve discussed, these experiments are used for cases that have practical impact, drug discovery applications, investigations into basic biology, then. But let’s also talk about the impact for methodologists, people who are trying to improve these single-cell foundation models, right? I think at the base, they’re really excited proposals. Because if you look at what some of the prior and less sophisticated methods couldn’t do, they tended to be more bespoke. So the excitement of single-cell foundation models is that you have this general-purpose model that can be used for everything and while they’re not living up to that purpose just now, just currently, I think that it’s important that we continue to bank onto that vision, right? So if you look at our contributions in that area, where single-cell foundation models are a really new proposal, so it makes sense that we may not know how to fully evaluate them just yet then. So you can view our work as basically being a step towards more rigorous evaluation of these models. Now that we did this experiment, I think the methodologists know to use this as a signal on how to improve the models and if they’re going in the right direction. And in fact, you are seeing more and more papers adopt zero-shot evaluations since we put out our paper then. And so this essentially helps future computer scientists that are working on single-cell foundation models know how to train better models.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;HUIZINGA&lt;/strong&gt;&lt;/strong&gt;:&lt;/strong&gt; That said, Alex, finally, what are the outstanding challenges that you identified for zero-shot learning research in biology, and what foundation might this paper lay for future research agendas in the field?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LU:&lt;/strong&gt; Yeah, absolutely. So now that we’ve shown single-cell foundation models don’t necessarily perform well, I think the natural question on everyone’s mind is how do we actually train single-cell foundation models that live up to that vision, that can perform in helping us discover new biology then? So I think in the short term, yeah, we’re actively investigating many hypotheses in this area. So for example, my colleagues, Lorin Crawford and Ava Amini, who were co-authors in the paper, recently put out a pre-print understanding how training data composition impacts model performance. And so one of the surprising findings that they had was that many of the training data sets that people used to train single-cell foundation models are highly redundant, to the point that you can even sample just a tiny fraction of the data and get basically the same performance then. But you can also look forward to many other explorations in this area as we continue to develop this research at the end of the day. But also zooming out into the bigger picture, I think one major takeaway from this paper is that developing AI methods for biology requires thought about the context of use, right? I mean, this is obvious for any AI method then, but I think people have gotten just too used to taking methods that work out there for natural vision or natural language maybe in the consumer domain and then extrapolating these methods to biology and expecting that they will work in the same way then, right? So for example, one reason why zero-shot evaluation was not routine practice for single-cell foundation models prior to our work, I mean, we were the first to fully establish that as a practice for the field, was because I think people who have been working in AI for biology have been looking to these more mainstream AI domains to shape their work then. And so with single-cell foundation models, many of these models are adopted from large language models with natural language processing, recycling the exact same architecture, the exact same code, basically just recycling practices in that field then. So when you look at like practices in like more mainstream domains, zero-shot evaluation is definitely explored in those domains, but it’s more of like a niche instead of being considered central to model understanding. So again, because biology is different from mainstream language processing, it’s a scientific discipline, zero-shot evaluation becomes much more important, and you have no choice but to use these models, zero-shot then. So in other words, I think that we need to be thinking carefully about what it is that makes training a model for biology different from training a model, for example, for consumer purposes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HUIZINGA:&lt;/strong&gt; Alex Lu, thanks for joining us today, and to our listeners, thanks for tuning in. If you want to read this paper, you can find a link at aka.ms/Abstracts, or you can read it on the Genome Biology website. See you next time on Abstracts!&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&amp;nbsp;&lt;/p&gt;

				&lt;/span&gt;
			&lt;/div&gt;
			&lt;button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button"&gt;
				Show more			&lt;/button&gt;
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/podcast/abstracts-zero-shot-models-in-single-cell-biology-with-alex-lu/</guid><pubDate>Thu, 22 May 2025 15:58:00 +0000</pubDate></item><item><title>Anthropic’s new hybrid AI model can work on tasks autonomously for hours at a time (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/05/22/1117338/anthropics-new-hybrid-ai-model-can-work-on-tasks-autonomously-for-hours-at-a-time/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/05/Claude-Plays-Pokemon.gif?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Anthropic has announced two new AI models that it claims represent a major step toward making AI agents truly useful.&lt;/p&gt;  &lt;p&gt;AI agents trained on Claude Opus 4, the company’s most powerful model to date, raise the bar for what such systems are capable of by tackling difficult tasks over extended periods of time and responding more usefully to user instructions, the company says.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Claude Opus 4 has been built to execute complex tasks that involve completing thousands of steps over several hours. For example, it created a guide for the video game Pokémon Red while playing it for more than 24 hours straight. The company’s previously most powerful model, Claude 3.7 Sonnet, was capable of playing for just 45 minutes, says Dianne Penn, product lead for research at Anthropic.&lt;/p&gt;  &lt;p&gt;Similarly, the company says that one of its customers, the Japanese technology company Rakuten, recently deployed Claude Opus 4 to code autonomously for close to seven hours on a complicated open-source project.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Anthropic achieved these advances by improving the model’s ability to create and maintain “memory files” to store key information. This enhanced ability to “remember” makes the model better at completing longer tasks.&lt;/p&gt;  &lt;p&gt;“We see this model generation leap as going from an assistant to a true agent,” says Penn. “While you still have to give a lot of real-time feedback and make all of the key decisions for AI assistants, an agent can make those key decisions itself. It allows humans to act more like a delegator or a judge, rather than having to hold these systems’ hands through every step.”&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;While Claude Opus 4 will be limited to paying Anthropic customers, a second model, Claude Sonnet 4, will be available for both paid and free tiers of users. Opus 4 is being marketed as a powerful, large model for complex challenges, while Sonnet 4 is described as a smart, efficient model for everyday use.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Both of the new models are hybrid, meaning they can offer a swift reply or a deeper, more reasoned response depending on the nature of a request. While they calculate a response, both models can search the web or use other tools to improve their output.&lt;/p&gt;  &lt;p&gt;AI companies are currently locked in a race to create truly useful AI agents that are able to plan, reason, and execute complex tasks both reliably and free from human supervision, says Stefano Albrecht, director of AI at the startup DeepFlow and coauthor of&lt;em&gt; Multi-Agent Reinforcement Learning: Foundations and Modern Approaches&lt;/em&gt;. Often this involves autonomously using the internet or other tools. There are still safety and security obstacles to overcome. AI agents powered by large language models can act erratically and perform unintended actions—which becomes even more of a problem when they’re trusted to act without human supervision.&lt;/p&gt;  &lt;p&gt;“The more agents are able to go ahead and do something over extended periods of time, the more helpful they will be, if I have to intervene less and less,” he says. “The new models’ ability to use tools in parallel is interesting—that could save some time along the way, so that’s going to be useful.”&lt;/p&gt;&lt;p&gt;As an example of the sorts of safety issues AI companies are still tackling, agents can end up taking unexpected shortcuts or exploiting loopholes to reach the goals they’ve been given. For example, they might book every seat on a plane to ensure that their user gets a seat, or resort to creative cheating to win a chess game. Anthropic says it managed to reduce this behavior, known as reward hacking, in both new models by 65% relative to Claude Sonnet 3.7. It achieved this by more closely monitoring problematic behaviors during training, and improving both the AI’s training environment and the evaluation methods.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/05/Claude-Plays-Pokemon.gif?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Anthropic has announced two new AI models that it claims represent a major step toward making AI agents truly useful.&lt;/p&gt;  &lt;p&gt;AI agents trained on Claude Opus 4, the company’s most powerful model to date, raise the bar for what such systems are capable of by tackling difficult tasks over extended periods of time and responding more usefully to user instructions, the company says.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Claude Opus 4 has been built to execute complex tasks that involve completing thousands of steps over several hours. For example, it created a guide for the video game Pokémon Red while playing it for more than 24 hours straight. The company’s previously most powerful model, Claude 3.7 Sonnet, was capable of playing for just 45 minutes, says Dianne Penn, product lead for research at Anthropic.&lt;/p&gt;  &lt;p&gt;Similarly, the company says that one of its customers, the Japanese technology company Rakuten, recently deployed Claude Opus 4 to code autonomously for close to seven hours on a complicated open-source project.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Anthropic achieved these advances by improving the model’s ability to create and maintain “memory files” to store key information. This enhanced ability to “remember” makes the model better at completing longer tasks.&lt;/p&gt;  &lt;p&gt;“We see this model generation leap as going from an assistant to a true agent,” says Penn. “While you still have to give a lot of real-time feedback and make all of the key decisions for AI assistants, an agent can make those key decisions itself. It allows humans to act more like a delegator or a judge, rather than having to hold these systems’ hands through every step.”&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;While Claude Opus 4 will be limited to paying Anthropic customers, a second model, Claude Sonnet 4, will be available for both paid and free tiers of users. Opus 4 is being marketed as a powerful, large model for complex challenges, while Sonnet 4 is described as a smart, efficient model for everyday use.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Both of the new models are hybrid, meaning they can offer a swift reply or a deeper, more reasoned response depending on the nature of a request. While they calculate a response, both models can search the web or use other tools to improve their output.&lt;/p&gt;  &lt;p&gt;AI companies are currently locked in a race to create truly useful AI agents that are able to plan, reason, and execute complex tasks both reliably and free from human supervision, says Stefano Albrecht, director of AI at the startup DeepFlow and coauthor of&lt;em&gt; Multi-Agent Reinforcement Learning: Foundations and Modern Approaches&lt;/em&gt;. Often this involves autonomously using the internet or other tools. There are still safety and security obstacles to overcome. AI agents powered by large language models can act erratically and perform unintended actions—which becomes even more of a problem when they’re trusted to act without human supervision.&lt;/p&gt;  &lt;p&gt;“The more agents are able to go ahead and do something over extended periods of time, the more helpful they will be, if I have to intervene less and less,” he says. “The new models’ ability to use tools in parallel is interesting—that could save some time along the way, so that’s going to be useful.”&lt;/p&gt;&lt;p&gt;As an example of the sorts of safety issues AI companies are still tackling, agents can end up taking unexpected shortcuts or exploiting loopholes to reach the goals they’ve been given. For example, they might book every seat on a plane to ensure that their user gets a seat, or resort to creative cheating to win a chess game. Anthropic says it managed to reduce this behavior, known as reward hacking, in both new models by 65% relative to Claude Sonnet 3.7. It achieved this by more closely monitoring problematic behaviors during training, and improving both the AI’s training environment and the evaluation methods.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/05/22/1117338/anthropics-new-hybrid-ai-model-can-work-on-tasks-autonomously-for-hours-at-a-time/</guid><pubDate>Thu, 22 May 2025 16:51:05 +0000</pubDate></item><item><title>Google Research at Google I/O 2025 (The latest research from Google)</title><link>https://research.google/blog/google-research-at-google-io-2025/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;Efficient and grounded models: Contributing to AI Mode in Search&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;As LLMs grow larger and demand increases, our ability to improve model efficiency while maintaining and even elevating their quality determines our success in democratizing access to these high-performing models. Google Research has made breakthroughs in efficiency that have become industry standards, for example, our work on speculative decoding and cascades.&lt;/p&gt;&lt;p&gt;We have published research on factual consistency techniques and evaluations, and set the bar on factuality and grounding with features like double-check and the FACTS Grounding leaderboard, released in collaboration with Google DeepMind and Kaggle. Now, we have contributed our research to AI Mode, to meaningfully improve the experience for users.&lt;/p&gt;&lt;p&gt;Announced at I/O, AI Mode is Google’s most powerful AI search yet with advanced reasoning capabilities. It is rolling out to all users in the U.S., allowing people to conduct deeper research with follow-up questions and links to relevant sites. Our work on efficiency enables the models to run more reliably and serve quicker outputs, and our factuality research has improved the way AI Mode searches the web, helping to ensure that the answers provided are highly accurate and grounded in multiple sources with relevant links.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;Efficient and grounded models: Contributing to AI Mode in Search&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;As LLMs grow larger and demand increases, our ability to improve model efficiency while maintaining and even elevating their quality determines our success in democratizing access to these high-performing models. Google Research has made breakthroughs in efficiency that have become industry standards, for example, our work on speculative decoding and cascades.&lt;/p&gt;&lt;p&gt;We have published research on factual consistency techniques and evaluations, and set the bar on factuality and grounding with features like double-check and the FACTS Grounding leaderboard, released in collaboration with Google DeepMind and Kaggle. Now, we have contributed our research to AI Mode, to meaningfully improve the experience for users.&lt;/p&gt;&lt;p&gt;Announced at I/O, AI Mode is Google’s most powerful AI search yet with advanced reasoning capabilities. It is rolling out to all users in the U.S., allowing people to conduct deeper research with follow-up questions and links to relevant sites. Our work on efficiency enables the models to run more reliably and serve quicker outputs, and our factuality research has improved the way AI Mode searches the web, helping to ensure that the answers provided are highly accurate and grounded in multiple sources with relevant links.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/google-research-at-google-io-2025/</guid><pubDate>Thu, 22 May 2025 20:35:00 +0000</pubDate></item><item><title>Tiny Agents in Python: a MCP-powered agent in ~70 lines of code (Hugging Face - Blog)</title><link>https://huggingface.co/blog/python-tiny-agents</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://huggingface.co/blog/assets/python-tiny-agents/thumbnail.png" /&gt;&lt;/div&gt;&lt;!-- HTML_TAG_START --&gt;
Inspired by Tiny Agents in JS, we ported the idea to Python 🐍 and extended the &lt;code&gt;huggingface_hub&lt;/code&gt; client SDK to act as a MCP Client so it can pull tools from MCP servers and pass them to the LLM during inference.
&lt;p&gt;MCP (Model Context Protocol) is an open protocol that standardizes how Large Language Models (LLMs) interact with external tools and APIs. Essentially, it removed the need to write custom integrations for each tool, making it simpler to plug new capabilities into your LLMs.&lt;/p&gt;
&lt;p&gt;In this blog post, we'll show you how to get started with a tiny Agent in Python connected to MCP servers to unlock powerful tool capabilities. You'll see just how easy it is to spin up your own Agent and start building!&lt;/p&gt;
&lt;blockquote class="tip"&gt;
&lt;p&gt;&lt;em&gt;Spoiler&lt;/em&gt; : An Agent is essentially a while loop built right on top of an MCP Client!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		How to Run the Demo
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;This section walks you through how to use existing Tiny Agents. We'll cover the setup and the commands to get an agent running.&lt;/p&gt;
&lt;p&gt;First, you need to install the latest version of &lt;code&gt;huggingface_hub&lt;/code&gt; with the &lt;code&gt;mcp&lt;/code&gt; extra to get all the necessary components.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install &lt;span class="hljs-string"&gt;"huggingface_hub[mcp]&amp;gt;=0.32.0"&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let's run an agent using the CLI! &lt;/p&gt;
&lt;p&gt;The coolest part is that you can load agents directly from the Hugging Face Hub tiny-agents Dataset, or specify a path to your own local agent configuration! &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;&amp;gt; tiny-agents run --&lt;span class="hljs-built_in"&gt;help&lt;/span&gt;
                                                                                                                                                                                     
 Usage: tiny-agents run [OPTIONS] [PATH] COMMAND [ARGS]...                                                                                                                           
                                                                                                                                                                                     
 Run the Agent &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; the CLI                                                                                                                                                            
                                                                                                                                                                                     
                                                                                                                                                                                     
╭─ Arguments ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│   path      [PATH]  Path to a &lt;span class="hljs-built_in"&gt;local&lt;/span&gt; folder containing an agent.json file or a built-in agent stored &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; the &lt;span class="hljs-string"&gt;'tiny-agents/tiny-agents'&lt;/span&gt; Hugging Face dataset                         │
│                     (https://huggingface.co/datasets/tiny-agents/tiny-agents)                                                                                                     │
╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭─ Options ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ --&lt;span class="hljs-built_in"&gt;help&lt;/span&gt;          Show this message and &lt;span class="hljs-built_in"&gt;exit&lt;/span&gt;.                                                                                                                                       │
╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you don't provide a path to a specific agent configuration, our Tiny Agent will connect by default to the following two MCP servers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the "canonical" file system server, which gets access to your Desktop,&lt;/li&gt;
&lt;li&gt;and the Playwright MCP server, which knows how to use a sandboxed Chromium browser for you.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The following example shows a web-browsing agent configured to use the Qwen/Qwen2.5-72B-Instruct model via Nebius inference provider, and it comes equipped with a playwright MCP server, which lets it use a web browser! The agent config is loaded specifying its path in the &lt;code&gt;tiny-agents/tiny-agents&lt;/code&gt; Hugging Face dataset.&lt;/p&gt;
&lt;video controls="controls" loop="loop"&gt;
  &lt;source src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/python-tiny-agents/web_browser_agent.mp4" type="video/mp4" /&gt;
&lt;/video&gt;

&lt;p&gt;When you run the agent, you'll see it load, listing the tools it has discovered from its connected MCP servers. Then, it's ready for your prompts!&lt;/p&gt;
&lt;p&gt;Prompt used in this demo:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;do a Web Search for HF inference providers on Brave Search and open the first result and then give me the list of the inference providers supported on Hugging Face &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can also use Gradio Spaces as MCP servers! The following example uses Qwen/Qwen2.5-72B-Instruct model via Nebius inference provider, and connects to a &lt;code&gt;FLUX.1 [schnell]&lt;/code&gt; image generation HF Space as an MCP server. The agent is loaded from its configuration in the tiny-agents/tiny-agents dataset on the Hugging Face Hub.&lt;/p&gt;
&lt;video controls="controls" loop="loop"&gt;
  &lt;source src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/python-tiny-agents/image-generation.mp4" type="video/mp4" /&gt;
&lt;/video&gt;

&lt;p&gt;Prompt used in this demo:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Generate a 1024x1024 image of a tiny astronaut hatching from an egg on the surface of the moon.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now that you've seen how to run existing Tiny Agents, the following sections will dive deeper into how they work and how to build your own.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Agent Configuration
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Each agent's behavior (its default model, inference provider, which MCP servers to connect to, and its initial system prompt) is defined by an &lt;code&gt;agent.json&lt;/code&gt; file. You can also provide a custom &lt;code&gt;PROMPT.md&lt;/code&gt; in the same directory for a more detailed system prompt. Here is an example:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;agent.json&lt;/code&gt;
The &lt;code&gt;model&lt;/code&gt; and &lt;code&gt;provider&lt;/code&gt; fields specify the LLM and inference provider used by the agent.
The &lt;code&gt;servers&lt;/code&gt; array defines the MCP servers the agent will connect to.
In this example, a "stdio" MCP server is configured. This type of server runs as a local process. The Agent starts it using the specified &lt;code&gt;command&lt;/code&gt; and &lt;code&gt;args&lt;/code&gt;, and then communicates with it via stdin/stdout to discover and execute available tools.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;&lt;span class="hljs-punctuation"&gt;{&lt;/span&gt;
    &lt;span class="hljs-attr"&gt;"model"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-string"&gt;"Qwen/Qwen2.5-72B-Instruct"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;,&lt;/span&gt;
    &lt;span class="hljs-attr"&gt;"provider"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-string"&gt;"nebius"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;,&lt;/span&gt;
    &lt;span class="hljs-attr"&gt;"servers"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-punctuation"&gt;[&lt;/span&gt;
        &lt;span class="hljs-punctuation"&gt;{&lt;/span&gt;
            &lt;span class="hljs-attr"&gt;"type"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-string"&gt;"stdio"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;,&lt;/span&gt;
            &lt;span class="hljs-attr"&gt;"config"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-punctuation"&gt;{&lt;/span&gt;
                &lt;span class="hljs-attr"&gt;"command"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-string"&gt;"npx"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;,&lt;/span&gt;
                &lt;span class="hljs-attr"&gt;"args"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-punctuation"&gt;[&lt;/span&gt;&lt;span class="hljs-string"&gt;"@playwright/mcp@latest"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;]&lt;/span&gt;
            &lt;span class="hljs-punctuation"&gt;}&lt;/span&gt;
        &lt;span class="hljs-punctuation"&gt;}&lt;/span&gt;
    &lt;span class="hljs-punctuation"&gt;]&lt;/span&gt;
&lt;span class="hljs-punctuation"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;PROMPT.md&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;You are an agent - please keep going until the user’s query is completely resolved [...]
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote class="tip"&gt;
&lt;p&gt;You can find more details about Hugging Face Inference Providers here.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		LLMs Can Use Tools
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Modern LLMs are built for function calling (or tool use), which enables users to easily build applications tailored to specific use cases and real-world tasks. &lt;/p&gt;
&lt;p&gt;A function is defined by its schema, which informs the LLM what it does and what input arguments it expects. The LLM decides when to use a tool, the Agent then orchestrates running the tool and feeding the result back.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;tools = [
        {
            &lt;span class="hljs-string"&gt;"type"&lt;/span&gt;: &lt;span class="hljs-string"&gt;"function"&lt;/span&gt;,
            &lt;span class="hljs-string"&gt;"function"&lt;/span&gt;: {
                &lt;span class="hljs-string"&gt;"name"&lt;/span&gt;: &lt;span class="hljs-string"&gt;"get_weather"&lt;/span&gt;,
                &lt;span class="hljs-string"&gt;"description"&lt;/span&gt;: &lt;span class="hljs-string"&gt;"Get current temperature for a given location."&lt;/span&gt;,
                &lt;span class="hljs-string"&gt;"parameters"&lt;/span&gt;: {
                    &lt;span class="hljs-string"&gt;"type"&lt;/span&gt;: &lt;span class="hljs-string"&gt;"object"&lt;/span&gt;,
                    &lt;span class="hljs-string"&gt;"properties"&lt;/span&gt;: {
                        &lt;span class="hljs-string"&gt;"location"&lt;/span&gt;: {
                            &lt;span class="hljs-string"&gt;"type"&lt;/span&gt;: &lt;span class="hljs-string"&gt;"string"&lt;/span&gt;,
                            &lt;span class="hljs-string"&gt;"description"&lt;/span&gt;: &lt;span class="hljs-string"&gt;"City and country e.g. Paris, France"&lt;/span&gt;
                        }
                    },
                    &lt;span class="hljs-string"&gt;"required"&lt;/span&gt;: [&lt;span class="hljs-string"&gt;"location"&lt;/span&gt;],
                },
            }
        }
]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;InferenceClient&lt;/code&gt; implements the same tool calling interface as the OpenAI Chat Completions API, which is the established standard for inference providers and the community.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Building our Python MCP Client
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;MCPClient&lt;/code&gt; is the heart of our tool-use functionality. It's now part of &lt;code&gt;huggingface_hub&lt;/code&gt; and uses the &lt;code&gt;AsyncInferenceClient&lt;/code&gt; to communicate with LLMs.&lt;/p&gt;
&lt;blockquote class="tip"&gt;
&lt;p&gt;The full &lt;code&gt;MCPClient&lt;/code&gt; code is in here if you want to follow along using the actual code 🤓&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Key responsibilities of the &lt;code&gt;MCPClient&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Manage async connections to one or more MCP servers.&lt;/li&gt;
&lt;li&gt;Discover tools from these servers.&lt;/li&gt;
&lt;li&gt;Format these tools for the LLM.&lt;/li&gt;
&lt;li&gt;Execute tool calls via the correct MCP server.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;​​Here’s a glimpse of how it connects to an MCP server (the &lt;code&gt;add_mcp_server&lt;/code&gt; method):&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;

&lt;span class="hljs-keyword"&gt;class&lt;/span&gt; &lt;span class="hljs-title class_"&gt;MCPClient&lt;/span&gt;:
    ...
    &lt;span class="hljs-keyword"&gt;async&lt;/span&gt; &lt;span class="hljs-keyword"&gt;def&lt;/span&gt; &lt;span class="hljs-title function_"&gt;add_mcp_server&lt;/span&gt;(&lt;span class="hljs-params"&gt;self, &lt;span class="hljs-built_in"&gt;type&lt;/span&gt;: ServerType, **params: &lt;span class="hljs-type"&gt;Any&lt;/span&gt;&lt;/span&gt;):
        
        
        
        

        
        
        read, write = &lt;span class="hljs-keyword"&gt;await&lt;/span&gt; self.exit_stack.enter_async_context(...)

        
        session = &lt;span class="hljs-keyword"&gt;await&lt;/span&gt; self.exit_stack.enter_async_context(
            ClientSession(read_stream=read, write_stream=write, ...)
        )
        &lt;span class="hljs-keyword"&gt;await&lt;/span&gt; session.initialize()

        
        response = &lt;span class="hljs-keyword"&gt;await&lt;/span&gt; session.list_tools()
        &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; tool &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; response.tools:
            
            self.sessions[tool.name] = session 
            
            self.available_tools.append({ 
                &lt;span class="hljs-string"&gt;"type"&lt;/span&gt;: &lt;span class="hljs-string"&gt;"function"&lt;/span&gt;,
                &lt;span class="hljs-string"&gt;"function"&lt;/span&gt;: {
                    &lt;span class="hljs-string"&gt;"name"&lt;/span&gt;: tool.name,
                    &lt;span class="hljs-string"&gt;"description"&lt;/span&gt;: tool.description,
                    &lt;span class="hljs-string"&gt;"parameters"&lt;/span&gt;: tool.input_schema,
                },
            })
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It supports &lt;code&gt;stdio&lt;/code&gt; servers for local tools (like accessing your file system), and &lt;code&gt;http&lt;/code&gt; servers for remote tools! It's also compatible with &lt;code&gt;sse&lt;/code&gt;, which is the previous standard for remote tools.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Using the Tools: Streaming and Processing
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;MCPClient&lt;/code&gt;'s &lt;code&gt;process_single_turn_with_tools&lt;/code&gt; method is where the LLM interaction happens. It sends the conversation history and available tools to the LLM via &lt;code&gt;AsyncInferenceClient.chat.completions.create(..., stream=True)&lt;/code&gt;.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		1. Prepare tools and calling the LLM
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;First, the method determines all tools the LLM should be aware of for the current turn – this includes tools from MCP servers and any special "exit loop" tools for agent control; then, it makes a streaming call to the LLM:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;


    
    tools = self.available_tools
    &lt;span class="hljs-keyword"&gt;if&lt;/span&gt; exit_loop_tools &lt;span class="hljs-keyword"&gt;is&lt;/span&gt; &lt;span class="hljs-keyword"&gt;not&lt;/span&gt; &lt;span class="hljs-literal"&gt;None&lt;/span&gt;:
        tools = [*exit_loop_tools, *self.available_tools]

    
    response = &lt;span class="hljs-keyword"&gt;await&lt;/span&gt; self.client.chat.completions.create(
        messages=messages,
        tools=tools,
        tool_choice=&lt;span class="hljs-string"&gt;"auto"&lt;/span&gt;,  
        stream=&lt;span class="hljs-literal"&gt;True&lt;/span&gt;,  
    )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As chunks arrive from the LLM, the method iterates through them. Each chunk is immediately yielded, then we reconstruct the complete text response and any tool calls.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;


&lt;span class="hljs-keyword"&gt;async&lt;/span&gt; &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; chunk &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; response:
      
      &lt;span class="hljs-keyword"&gt;yield&lt;/span&gt; chunk
      
      …
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		2. Executing tools
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Once the stream is complete, if the LLM requested any tool calls (now fully reconstructed in &lt;code&gt;final_tool_calls&lt;/code&gt;), the method processes each one:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;

&lt;span class="hljs-keyword"&gt;for&lt;/span&gt; tool_call &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; final_tool_calls.values():
    function_name = tool_call.function.name
    function_args = json.loads(tool_call.function.arguments &lt;span class="hljs-keyword"&gt;or&lt;/span&gt; &lt;span class="hljs-string"&gt;"{}"&lt;/span&gt;)

    
    tool_message = {&lt;span class="hljs-string"&gt;"role"&lt;/span&gt;: &lt;span class="hljs-string"&gt;"tool"&lt;/span&gt;, &lt;span class="hljs-string"&gt;"tool_call_id"&lt;/span&gt;: tool_call.&lt;span class="hljs-built_in"&gt;id&lt;/span&gt;, &lt;span class="hljs-string"&gt;"content"&lt;/span&gt;: &lt;span class="hljs-string"&gt;""&lt;/span&gt;, &lt;span class="hljs-string"&gt;"name"&lt;/span&gt;: function_name}

    
    &lt;span class="hljs-keyword"&gt;if&lt;/span&gt; exit_loop_tools &lt;span class="hljs-keyword"&gt;and&lt;/span&gt; function_name &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; [t.function.name &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; t &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; exit_loop_tools]:
        
        messages.append(ChatCompletionInputMessage.parse_obj_as_instance(tool_message))
        &lt;span class="hljs-keyword"&gt;yield&lt;/span&gt; ChatCompletionInputMessage.parse_obj_as_instance(tool_message)
        &lt;span class="hljs-keyword"&gt;return&lt;/span&gt; 

    
    session = self.sessions.get(function_name) 
    &lt;span class="hljs-keyword"&gt;if&lt;/span&gt; session &lt;span class="hljs-keyword"&gt;is&lt;/span&gt; &lt;span class="hljs-keyword"&gt;not&lt;/span&gt; &lt;span class="hljs-literal"&gt;None&lt;/span&gt;:
        result = &lt;span class="hljs-keyword"&gt;await&lt;/span&gt; session.call_tool(function_name, function_args)
        tool_message[&lt;span class="hljs-string"&gt;"content"&lt;/span&gt;] = format_result(result) 
    &lt;span class="hljs-keyword"&gt;else&lt;/span&gt;:
        tool_message[&lt;span class="hljs-string"&gt;"content"&lt;/span&gt;] = &lt;span class="hljs-string"&gt;f"Error: No session found for tool: &lt;span class="hljs-subst"&gt;{function_name}&lt;/span&gt;"&lt;/span&gt;
        tool_message[&lt;span class="hljs-string"&gt;"content"&lt;/span&gt;] = error_msg

    
    ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It first checks if the tool called exits the loop (&lt;code&gt;exit_loop_tool&lt;/code&gt;). If not, it finds the correct MCP session responsible for that tool and calls &lt;code&gt;session.call_tool()&lt;/code&gt;. The result (or error response) is then formatted, added to the conversation history, and yielded so the Agent is aware of the tool's output.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Our Tiny Python Agent: It's (Almost) Just a Loop!
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;With the &lt;code&gt;MCPClient&lt;/code&gt; doing all the job for tool interactions, our &lt;code&gt;Agent&lt;/code&gt; class becomes wonderfully simple. It inherits from &lt;code&gt;MCPClient&lt;/code&gt; and adds the conversation management logic.&lt;/p&gt;
&lt;blockquote class="tip"&gt;
&lt;p&gt;The Agent class is tiny and focuses on the conversational loop, the code can be found here. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		1. Initializing the Agent
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;When an Agent is created, it takes an agent config (model, provider, which MCP servers to use, system prompt) and initializes the conversation history with the system prompt. The &lt;code&gt;load_tools()&lt;/code&gt; method then iterates through the server configurations (defined in agent.json) and calls &lt;code&gt;add_mcp_server&lt;/code&gt; (from the parent &lt;code&gt;MCPClient&lt;/code&gt;) for each one, populating the agent's toolbox.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;

&lt;span class="hljs-keyword"&gt;class&lt;/span&gt; &lt;span class="hljs-title class_"&gt;Agent&lt;/span&gt;(&lt;span class="hljs-title class_ inherited__"&gt;MCPClient&lt;/span&gt;):
    &lt;span class="hljs-keyword"&gt;def&lt;/span&gt; &lt;span class="hljs-title function_"&gt;__init__&lt;/span&gt;(&lt;span class="hljs-params"&gt;&lt;/span&gt;
&lt;span class="hljs-params"&gt;        self,&lt;/span&gt;
&lt;span class="hljs-params"&gt;        *,&lt;/span&gt;
&lt;span class="hljs-params"&gt;        model: &lt;span class="hljs-built_in"&gt;str&lt;/span&gt;,&lt;/span&gt;
&lt;span class="hljs-params"&gt;        servers: Iterable[&lt;span class="hljs-type"&gt;Dict&lt;/span&gt;], &lt;/span&gt;
&lt;span class="hljs-params"&gt;        provider: &lt;span class="hljs-type"&gt;Optional&lt;/span&gt;[PROVIDER_OR_POLICY_T] = &lt;span class="hljs-literal"&gt;None&lt;/span&gt;,&lt;/span&gt;
&lt;span class="hljs-params"&gt;        api_key: &lt;span class="hljs-type"&gt;Optional&lt;/span&gt;[&lt;span class="hljs-built_in"&gt;str&lt;/span&gt;] = &lt;span class="hljs-literal"&gt;None&lt;/span&gt;,&lt;/span&gt;
&lt;span class="hljs-params"&gt;        prompt: &lt;span class="hljs-type"&gt;Optional&lt;/span&gt;[&lt;span class="hljs-built_in"&gt;str&lt;/span&gt;] = &lt;span class="hljs-literal"&gt;None&lt;/span&gt;, &lt;/span&gt;
&lt;span class="hljs-params"&gt;    &lt;/span&gt;):
        
        &lt;span class="hljs-built_in"&gt;super&lt;/span&gt;().__init__(model=model, provider=provider, api_key=api_key)
        
        self._servers_cfg = &lt;span class="hljs-built_in"&gt;list&lt;/span&gt;(servers)
        
        self.messages: &lt;span class="hljs-type"&gt;List&lt;/span&gt;[&lt;span class="hljs-type"&gt;Union&lt;/span&gt;[&lt;span class="hljs-type"&gt;Dict&lt;/span&gt;, ChatCompletionInputMessage]] = [
            {&lt;span class="hljs-string"&gt;"role"&lt;/span&gt;: &lt;span class="hljs-string"&gt;"system"&lt;/span&gt;, &lt;span class="hljs-string"&gt;"content"&lt;/span&gt;: prompt &lt;span class="hljs-keyword"&gt;or&lt;/span&gt; DEFAULT_SYSTEM_PROMPT}
        ]

    &lt;span class="hljs-keyword"&gt;async&lt;/span&gt; &lt;span class="hljs-keyword"&gt;def&lt;/span&gt; &lt;span class="hljs-title function_"&gt;load_tools&lt;/span&gt;(&lt;span class="hljs-params"&gt;self&lt;/span&gt;) -&amp;gt; &lt;span class="hljs-literal"&gt;None&lt;/span&gt;:
        
        &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; cfg &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; self._servers_cfg:
            &lt;span class="hljs-keyword"&gt;await&lt;/span&gt; self.add_mcp_server(cfg[&lt;span class="hljs-string"&gt;"type"&lt;/span&gt;], **cfg[&lt;span class="hljs-string"&gt;"config"&lt;/span&gt;])
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		2. The agent’s core: the Loop
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;Agent.run()&lt;/code&gt; method is an asynchronous generator that processes a single user input. It manages the conversation turns, deciding when the agent's current task is complete.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;

&lt;span class="hljs-keyword"&gt;async&lt;/span&gt; &lt;span class="hljs-keyword"&gt;def&lt;/span&gt; &lt;span class="hljs-title function_"&gt;run&lt;/span&gt;(&lt;span class="hljs-params"&gt;self, user_input: &lt;span class="hljs-built_in"&gt;str&lt;/span&gt;, *, abort_event: &lt;span class="hljs-type"&gt;Optional&lt;/span&gt;[asyncio.Event] = &lt;span class="hljs-literal"&gt;None&lt;/span&gt;, ...&lt;/span&gt;) -&amp;gt; AsyncGenerator[...]:
    ...
    &lt;span class="hljs-keyword"&gt;while&lt;/span&gt; &lt;span class="hljs-literal"&gt;True&lt;/span&gt;: 
        ...

        
        
        &lt;span class="hljs-keyword"&gt;async&lt;/span&gt; &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; item &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; self.process_single_turn_with_tools(
            self.messages,
            ...
        ):
            &lt;span class="hljs-keyword"&gt;yield&lt;/span&gt; item 

        ... 
        
        
        
        &lt;span class="hljs-keyword"&gt;if&lt;/span&gt; last.get(&lt;span class="hljs-string"&gt;"role"&lt;/span&gt;) == &lt;span class="hljs-string"&gt;"tool"&lt;/span&gt; &lt;span class="hljs-keyword"&gt;and&lt;/span&gt; last.get(&lt;span class="hljs-string"&gt;"name"&lt;/span&gt;) &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; {t.function.name &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; t &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; EXIT_LOOP_TOOLS}:
                &lt;span class="hljs-keyword"&gt;return&lt;/span&gt;

        
        &lt;span class="hljs-keyword"&gt;if&lt;/span&gt; last.get(&lt;span class="hljs-string"&gt;"role"&lt;/span&gt;) != &lt;span class="hljs-string"&gt;"tool"&lt;/span&gt; &lt;span class="hljs-keyword"&gt;and&lt;/span&gt; num_turns &amp;gt; MAX_NUM_TURNS:
                &lt;span class="hljs-keyword"&gt;return&lt;/span&gt;
        &lt;span class="hljs-keyword"&gt;if&lt;/span&gt; last.get(&lt;span class="hljs-string"&gt;"role"&lt;/span&gt;) != &lt;span class="hljs-string"&gt;"tool"&lt;/span&gt; &lt;span class="hljs-keyword"&gt;and&lt;/span&gt; next_turn_should_call_tools:
            &lt;span class="hljs-keyword"&gt;return&lt;/span&gt;
        
        next_turn_should_call_tools = (last_message.get(&lt;span class="hljs-string"&gt;"role"&lt;/span&gt;) != &lt;span class="hljs-string"&gt;"tool"&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inside the &lt;code&gt;run()&lt;/code&gt; loop:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It first adds the user prompt to the conversation.&lt;/li&gt;
&lt;li&gt;Then it calls &lt;code&gt;MCPClient.process_single_turn_with_tools(...)&lt;/code&gt; to get the LLM's response and handle any tool executions for one step of reasoning.&lt;/li&gt;
&lt;li&gt;Each item is immediately yielded, enabling real-time streaming to the caller.&lt;/li&gt;
&lt;li&gt;After each step, it checks exit conditions: if a special "exit loop" tools was used, if a maximum turn limit is hit, or if the LLM provides a text response that seems final for the current request.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Next Steps
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;There are a lot of cool ways to explore and expand upon the MCP Client and the Tiny Agent 🔥 
Here are some ideas to get you started:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Benchmark how different LLM models and inference providers impact agentic performance: Tool calling performance can differ because each provider may optimize it differently. You can find the list of supported providers here.&lt;/li&gt;
&lt;li&gt;Run tiny agents with local LLM inference servers, such as llama.cpp, or LM Studio.&lt;/li&gt;
&lt;li&gt;.. and of course contribute! Share your unique tiny agents and open PRs in tiny-agents/tiny-agents dataset on the Hugging Face Hub.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pull requests and contributions are welcome! Again, everything here is open source! 💎❤️&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://huggingface.co/blog/assets/python-tiny-agents/thumbnail.png" /&gt;&lt;/div&gt;&lt;!-- HTML_TAG_START --&gt;
Inspired by Tiny Agents in JS, we ported the idea to Python 🐍 and extended the &lt;code&gt;huggingface_hub&lt;/code&gt; client SDK to act as a MCP Client so it can pull tools from MCP servers and pass them to the LLM during inference.
&lt;p&gt;MCP (Model Context Protocol) is an open protocol that standardizes how Large Language Models (LLMs) interact with external tools and APIs. Essentially, it removed the need to write custom integrations for each tool, making it simpler to plug new capabilities into your LLMs.&lt;/p&gt;
&lt;p&gt;In this blog post, we'll show you how to get started with a tiny Agent in Python connected to MCP servers to unlock powerful tool capabilities. You'll see just how easy it is to spin up your own Agent and start building!&lt;/p&gt;
&lt;blockquote class="tip"&gt;
&lt;p&gt;&lt;em&gt;Spoiler&lt;/em&gt; : An Agent is essentially a while loop built right on top of an MCP Client!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		How to Run the Demo
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;This section walks you through how to use existing Tiny Agents. We'll cover the setup and the commands to get an agent running.&lt;/p&gt;
&lt;p&gt;First, you need to install the latest version of &lt;code&gt;huggingface_hub&lt;/code&gt; with the &lt;code&gt;mcp&lt;/code&gt; extra to get all the necessary components.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install &lt;span class="hljs-string"&gt;"huggingface_hub[mcp]&amp;gt;=0.32.0"&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let's run an agent using the CLI! &lt;/p&gt;
&lt;p&gt;The coolest part is that you can load agents directly from the Hugging Face Hub tiny-agents Dataset, or specify a path to your own local agent configuration! &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;&amp;gt; tiny-agents run --&lt;span class="hljs-built_in"&gt;help&lt;/span&gt;
                                                                                                                                                                                     
 Usage: tiny-agents run [OPTIONS] [PATH] COMMAND [ARGS]...                                                                                                                           
                                                                                                                                                                                     
 Run the Agent &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; the CLI                                                                                                                                                            
                                                                                                                                                                                     
                                                                                                                                                                                     
╭─ Arguments ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│   path      [PATH]  Path to a &lt;span class="hljs-built_in"&gt;local&lt;/span&gt; folder containing an agent.json file or a built-in agent stored &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; the &lt;span class="hljs-string"&gt;'tiny-agents/tiny-agents'&lt;/span&gt; Hugging Face dataset                         │
│                     (https://huggingface.co/datasets/tiny-agents/tiny-agents)                                                                                                     │
╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭─ Options ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ --&lt;span class="hljs-built_in"&gt;help&lt;/span&gt;          Show this message and &lt;span class="hljs-built_in"&gt;exit&lt;/span&gt;.                                                                                                                                       │
╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you don't provide a path to a specific agent configuration, our Tiny Agent will connect by default to the following two MCP servers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the "canonical" file system server, which gets access to your Desktop,&lt;/li&gt;
&lt;li&gt;and the Playwright MCP server, which knows how to use a sandboxed Chromium browser for you.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The following example shows a web-browsing agent configured to use the Qwen/Qwen2.5-72B-Instruct model via Nebius inference provider, and it comes equipped with a playwright MCP server, which lets it use a web browser! The agent config is loaded specifying its path in the &lt;code&gt;tiny-agents/tiny-agents&lt;/code&gt; Hugging Face dataset.&lt;/p&gt;
&lt;video controls="controls" loop="loop"&gt;
  &lt;source src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/python-tiny-agents/web_browser_agent.mp4" type="video/mp4" /&gt;
&lt;/video&gt;

&lt;p&gt;When you run the agent, you'll see it load, listing the tools it has discovered from its connected MCP servers. Then, it's ready for your prompts!&lt;/p&gt;
&lt;p&gt;Prompt used in this demo:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;do a Web Search for HF inference providers on Brave Search and open the first result and then give me the list of the inference providers supported on Hugging Face &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can also use Gradio Spaces as MCP servers! The following example uses Qwen/Qwen2.5-72B-Instruct model via Nebius inference provider, and connects to a &lt;code&gt;FLUX.1 [schnell]&lt;/code&gt; image generation HF Space as an MCP server. The agent is loaded from its configuration in the tiny-agents/tiny-agents dataset on the Hugging Face Hub.&lt;/p&gt;
&lt;video controls="controls" loop="loop"&gt;
  &lt;source src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/python-tiny-agents/image-generation.mp4" type="video/mp4" /&gt;
&lt;/video&gt;

&lt;p&gt;Prompt used in this demo:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Generate a 1024x1024 image of a tiny astronaut hatching from an egg on the surface of the moon.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now that you've seen how to run existing Tiny Agents, the following sections will dive deeper into how they work and how to build your own.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Agent Configuration
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Each agent's behavior (its default model, inference provider, which MCP servers to connect to, and its initial system prompt) is defined by an &lt;code&gt;agent.json&lt;/code&gt; file. You can also provide a custom &lt;code&gt;PROMPT.md&lt;/code&gt; in the same directory for a more detailed system prompt. Here is an example:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;agent.json&lt;/code&gt;
The &lt;code&gt;model&lt;/code&gt; and &lt;code&gt;provider&lt;/code&gt; fields specify the LLM and inference provider used by the agent.
The &lt;code&gt;servers&lt;/code&gt; array defines the MCP servers the agent will connect to.
In this example, a "stdio" MCP server is configured. This type of server runs as a local process. The Agent starts it using the specified &lt;code&gt;command&lt;/code&gt; and &lt;code&gt;args&lt;/code&gt;, and then communicates with it via stdin/stdout to discover and execute available tools.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;&lt;span class="hljs-punctuation"&gt;{&lt;/span&gt;
    &lt;span class="hljs-attr"&gt;"model"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-string"&gt;"Qwen/Qwen2.5-72B-Instruct"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;,&lt;/span&gt;
    &lt;span class="hljs-attr"&gt;"provider"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-string"&gt;"nebius"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;,&lt;/span&gt;
    &lt;span class="hljs-attr"&gt;"servers"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-punctuation"&gt;[&lt;/span&gt;
        &lt;span class="hljs-punctuation"&gt;{&lt;/span&gt;
            &lt;span class="hljs-attr"&gt;"type"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-string"&gt;"stdio"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;,&lt;/span&gt;
            &lt;span class="hljs-attr"&gt;"config"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-punctuation"&gt;{&lt;/span&gt;
                &lt;span class="hljs-attr"&gt;"command"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-string"&gt;"npx"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;,&lt;/span&gt;
                &lt;span class="hljs-attr"&gt;"args"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-punctuation"&gt;[&lt;/span&gt;&lt;span class="hljs-string"&gt;"@playwright/mcp@latest"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;]&lt;/span&gt;
            &lt;span class="hljs-punctuation"&gt;}&lt;/span&gt;
        &lt;span class="hljs-punctuation"&gt;}&lt;/span&gt;
    &lt;span class="hljs-punctuation"&gt;]&lt;/span&gt;
&lt;span class="hljs-punctuation"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;PROMPT.md&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;You are an agent - please keep going until the user’s query is completely resolved [...]
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote class="tip"&gt;
&lt;p&gt;You can find more details about Hugging Face Inference Providers here.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		LLMs Can Use Tools
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Modern LLMs are built for function calling (or tool use), which enables users to easily build applications tailored to specific use cases and real-world tasks. &lt;/p&gt;
&lt;p&gt;A function is defined by its schema, which informs the LLM what it does and what input arguments it expects. The LLM decides when to use a tool, the Agent then orchestrates running the tool and feeding the result back.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;tools = [
        {
            &lt;span class="hljs-string"&gt;"type"&lt;/span&gt;: &lt;span class="hljs-string"&gt;"function"&lt;/span&gt;,
            &lt;span class="hljs-string"&gt;"function"&lt;/span&gt;: {
                &lt;span class="hljs-string"&gt;"name"&lt;/span&gt;: &lt;span class="hljs-string"&gt;"get_weather"&lt;/span&gt;,
                &lt;span class="hljs-string"&gt;"description"&lt;/span&gt;: &lt;span class="hljs-string"&gt;"Get current temperature for a given location."&lt;/span&gt;,
                &lt;span class="hljs-string"&gt;"parameters"&lt;/span&gt;: {
                    &lt;span class="hljs-string"&gt;"type"&lt;/span&gt;: &lt;span class="hljs-string"&gt;"object"&lt;/span&gt;,
                    &lt;span class="hljs-string"&gt;"properties"&lt;/span&gt;: {
                        &lt;span class="hljs-string"&gt;"location"&lt;/span&gt;: {
                            &lt;span class="hljs-string"&gt;"type"&lt;/span&gt;: &lt;span class="hljs-string"&gt;"string"&lt;/span&gt;,
                            &lt;span class="hljs-string"&gt;"description"&lt;/span&gt;: &lt;span class="hljs-string"&gt;"City and country e.g. Paris, France"&lt;/span&gt;
                        }
                    },
                    &lt;span class="hljs-string"&gt;"required"&lt;/span&gt;: [&lt;span class="hljs-string"&gt;"location"&lt;/span&gt;],
                },
            }
        }
]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;InferenceClient&lt;/code&gt; implements the same tool calling interface as the OpenAI Chat Completions API, which is the established standard for inference providers and the community.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Building our Python MCP Client
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;MCPClient&lt;/code&gt; is the heart of our tool-use functionality. It's now part of &lt;code&gt;huggingface_hub&lt;/code&gt; and uses the &lt;code&gt;AsyncInferenceClient&lt;/code&gt; to communicate with LLMs.&lt;/p&gt;
&lt;blockquote class="tip"&gt;
&lt;p&gt;The full &lt;code&gt;MCPClient&lt;/code&gt; code is in here if you want to follow along using the actual code 🤓&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Key responsibilities of the &lt;code&gt;MCPClient&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Manage async connections to one or more MCP servers.&lt;/li&gt;
&lt;li&gt;Discover tools from these servers.&lt;/li&gt;
&lt;li&gt;Format these tools for the LLM.&lt;/li&gt;
&lt;li&gt;Execute tool calls via the correct MCP server.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;​​Here’s a glimpse of how it connects to an MCP server (the &lt;code&gt;add_mcp_server&lt;/code&gt; method):&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;

&lt;span class="hljs-keyword"&gt;class&lt;/span&gt; &lt;span class="hljs-title class_"&gt;MCPClient&lt;/span&gt;:
    ...
    &lt;span class="hljs-keyword"&gt;async&lt;/span&gt; &lt;span class="hljs-keyword"&gt;def&lt;/span&gt; &lt;span class="hljs-title function_"&gt;add_mcp_server&lt;/span&gt;(&lt;span class="hljs-params"&gt;self, &lt;span class="hljs-built_in"&gt;type&lt;/span&gt;: ServerType, **params: &lt;span class="hljs-type"&gt;Any&lt;/span&gt;&lt;/span&gt;):
        
        
        
        

        
        
        read, write = &lt;span class="hljs-keyword"&gt;await&lt;/span&gt; self.exit_stack.enter_async_context(...)

        
        session = &lt;span class="hljs-keyword"&gt;await&lt;/span&gt; self.exit_stack.enter_async_context(
            ClientSession(read_stream=read, write_stream=write, ...)
        )
        &lt;span class="hljs-keyword"&gt;await&lt;/span&gt; session.initialize()

        
        response = &lt;span class="hljs-keyword"&gt;await&lt;/span&gt; session.list_tools()
        &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; tool &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; response.tools:
            
            self.sessions[tool.name] = session 
            
            self.available_tools.append({ 
                &lt;span class="hljs-string"&gt;"type"&lt;/span&gt;: &lt;span class="hljs-string"&gt;"function"&lt;/span&gt;,
                &lt;span class="hljs-string"&gt;"function"&lt;/span&gt;: {
                    &lt;span class="hljs-string"&gt;"name"&lt;/span&gt;: tool.name,
                    &lt;span class="hljs-string"&gt;"description"&lt;/span&gt;: tool.description,
                    &lt;span class="hljs-string"&gt;"parameters"&lt;/span&gt;: tool.input_schema,
                },
            })
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It supports &lt;code&gt;stdio&lt;/code&gt; servers for local tools (like accessing your file system), and &lt;code&gt;http&lt;/code&gt; servers for remote tools! It's also compatible with &lt;code&gt;sse&lt;/code&gt;, which is the previous standard for remote tools.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Using the Tools: Streaming and Processing
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;MCPClient&lt;/code&gt;'s &lt;code&gt;process_single_turn_with_tools&lt;/code&gt; method is where the LLM interaction happens. It sends the conversation history and available tools to the LLM via &lt;code&gt;AsyncInferenceClient.chat.completions.create(..., stream=True)&lt;/code&gt;.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		1. Prepare tools and calling the LLM
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;First, the method determines all tools the LLM should be aware of for the current turn – this includes tools from MCP servers and any special "exit loop" tools for agent control; then, it makes a streaming call to the LLM:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;


    
    tools = self.available_tools
    &lt;span class="hljs-keyword"&gt;if&lt;/span&gt; exit_loop_tools &lt;span class="hljs-keyword"&gt;is&lt;/span&gt; &lt;span class="hljs-keyword"&gt;not&lt;/span&gt; &lt;span class="hljs-literal"&gt;None&lt;/span&gt;:
        tools = [*exit_loop_tools, *self.available_tools]

    
    response = &lt;span class="hljs-keyword"&gt;await&lt;/span&gt; self.client.chat.completions.create(
        messages=messages,
        tools=tools,
        tool_choice=&lt;span class="hljs-string"&gt;"auto"&lt;/span&gt;,  
        stream=&lt;span class="hljs-literal"&gt;True&lt;/span&gt;,  
    )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As chunks arrive from the LLM, the method iterates through them. Each chunk is immediately yielded, then we reconstruct the complete text response and any tool calls.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;


&lt;span class="hljs-keyword"&gt;async&lt;/span&gt; &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; chunk &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; response:
      
      &lt;span class="hljs-keyword"&gt;yield&lt;/span&gt; chunk
      
      …
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		2. Executing tools
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Once the stream is complete, if the LLM requested any tool calls (now fully reconstructed in &lt;code&gt;final_tool_calls&lt;/code&gt;), the method processes each one:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;

&lt;span class="hljs-keyword"&gt;for&lt;/span&gt; tool_call &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; final_tool_calls.values():
    function_name = tool_call.function.name
    function_args = json.loads(tool_call.function.arguments &lt;span class="hljs-keyword"&gt;or&lt;/span&gt; &lt;span class="hljs-string"&gt;"{}"&lt;/span&gt;)

    
    tool_message = {&lt;span class="hljs-string"&gt;"role"&lt;/span&gt;: &lt;span class="hljs-string"&gt;"tool"&lt;/span&gt;, &lt;span class="hljs-string"&gt;"tool_call_id"&lt;/span&gt;: tool_call.&lt;span class="hljs-built_in"&gt;id&lt;/span&gt;, &lt;span class="hljs-string"&gt;"content"&lt;/span&gt;: &lt;span class="hljs-string"&gt;""&lt;/span&gt;, &lt;span class="hljs-string"&gt;"name"&lt;/span&gt;: function_name}

    
    &lt;span class="hljs-keyword"&gt;if&lt;/span&gt; exit_loop_tools &lt;span class="hljs-keyword"&gt;and&lt;/span&gt; function_name &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; [t.function.name &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; t &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; exit_loop_tools]:
        
        messages.append(ChatCompletionInputMessage.parse_obj_as_instance(tool_message))
        &lt;span class="hljs-keyword"&gt;yield&lt;/span&gt; ChatCompletionInputMessage.parse_obj_as_instance(tool_message)
        &lt;span class="hljs-keyword"&gt;return&lt;/span&gt; 

    
    session = self.sessions.get(function_name) 
    &lt;span class="hljs-keyword"&gt;if&lt;/span&gt; session &lt;span class="hljs-keyword"&gt;is&lt;/span&gt; &lt;span class="hljs-keyword"&gt;not&lt;/span&gt; &lt;span class="hljs-literal"&gt;None&lt;/span&gt;:
        result = &lt;span class="hljs-keyword"&gt;await&lt;/span&gt; session.call_tool(function_name, function_args)
        tool_message[&lt;span class="hljs-string"&gt;"content"&lt;/span&gt;] = format_result(result) 
    &lt;span class="hljs-keyword"&gt;else&lt;/span&gt;:
        tool_message[&lt;span class="hljs-string"&gt;"content"&lt;/span&gt;] = &lt;span class="hljs-string"&gt;f"Error: No session found for tool: &lt;span class="hljs-subst"&gt;{function_name}&lt;/span&gt;"&lt;/span&gt;
        tool_message[&lt;span class="hljs-string"&gt;"content"&lt;/span&gt;] = error_msg

    
    ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It first checks if the tool called exits the loop (&lt;code&gt;exit_loop_tool&lt;/code&gt;). If not, it finds the correct MCP session responsible for that tool and calls &lt;code&gt;session.call_tool()&lt;/code&gt;. The result (or error response) is then formatted, added to the conversation history, and yielded so the Agent is aware of the tool's output.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Our Tiny Python Agent: It's (Almost) Just a Loop!
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;With the &lt;code&gt;MCPClient&lt;/code&gt; doing all the job for tool interactions, our &lt;code&gt;Agent&lt;/code&gt; class becomes wonderfully simple. It inherits from &lt;code&gt;MCPClient&lt;/code&gt; and adds the conversation management logic.&lt;/p&gt;
&lt;blockquote class="tip"&gt;
&lt;p&gt;The Agent class is tiny and focuses on the conversational loop, the code can be found here. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		1. Initializing the Agent
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;When an Agent is created, it takes an agent config (model, provider, which MCP servers to use, system prompt) and initializes the conversation history with the system prompt. The &lt;code&gt;load_tools()&lt;/code&gt; method then iterates through the server configurations (defined in agent.json) and calls &lt;code&gt;add_mcp_server&lt;/code&gt; (from the parent &lt;code&gt;MCPClient&lt;/code&gt;) for each one, populating the agent's toolbox.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;

&lt;span class="hljs-keyword"&gt;class&lt;/span&gt; &lt;span class="hljs-title class_"&gt;Agent&lt;/span&gt;(&lt;span class="hljs-title class_ inherited__"&gt;MCPClient&lt;/span&gt;):
    &lt;span class="hljs-keyword"&gt;def&lt;/span&gt; &lt;span class="hljs-title function_"&gt;__init__&lt;/span&gt;(&lt;span class="hljs-params"&gt;&lt;/span&gt;
&lt;span class="hljs-params"&gt;        self,&lt;/span&gt;
&lt;span class="hljs-params"&gt;        *,&lt;/span&gt;
&lt;span class="hljs-params"&gt;        model: &lt;span class="hljs-built_in"&gt;str&lt;/span&gt;,&lt;/span&gt;
&lt;span class="hljs-params"&gt;        servers: Iterable[&lt;span class="hljs-type"&gt;Dict&lt;/span&gt;], &lt;/span&gt;
&lt;span class="hljs-params"&gt;        provider: &lt;span class="hljs-type"&gt;Optional&lt;/span&gt;[PROVIDER_OR_POLICY_T] = &lt;span class="hljs-literal"&gt;None&lt;/span&gt;,&lt;/span&gt;
&lt;span class="hljs-params"&gt;        api_key: &lt;span class="hljs-type"&gt;Optional&lt;/span&gt;[&lt;span class="hljs-built_in"&gt;str&lt;/span&gt;] = &lt;span class="hljs-literal"&gt;None&lt;/span&gt;,&lt;/span&gt;
&lt;span class="hljs-params"&gt;        prompt: &lt;span class="hljs-type"&gt;Optional&lt;/span&gt;[&lt;span class="hljs-built_in"&gt;str&lt;/span&gt;] = &lt;span class="hljs-literal"&gt;None&lt;/span&gt;, &lt;/span&gt;
&lt;span class="hljs-params"&gt;    &lt;/span&gt;):
        
        &lt;span class="hljs-built_in"&gt;super&lt;/span&gt;().__init__(model=model, provider=provider, api_key=api_key)
        
        self._servers_cfg = &lt;span class="hljs-built_in"&gt;list&lt;/span&gt;(servers)
        
        self.messages: &lt;span class="hljs-type"&gt;List&lt;/span&gt;[&lt;span class="hljs-type"&gt;Union&lt;/span&gt;[&lt;span class="hljs-type"&gt;Dict&lt;/span&gt;, ChatCompletionInputMessage]] = [
            {&lt;span class="hljs-string"&gt;"role"&lt;/span&gt;: &lt;span class="hljs-string"&gt;"system"&lt;/span&gt;, &lt;span class="hljs-string"&gt;"content"&lt;/span&gt;: prompt &lt;span class="hljs-keyword"&gt;or&lt;/span&gt; DEFAULT_SYSTEM_PROMPT}
        ]

    &lt;span class="hljs-keyword"&gt;async&lt;/span&gt; &lt;span class="hljs-keyword"&gt;def&lt;/span&gt; &lt;span class="hljs-title function_"&gt;load_tools&lt;/span&gt;(&lt;span class="hljs-params"&gt;self&lt;/span&gt;) -&amp;gt; &lt;span class="hljs-literal"&gt;None&lt;/span&gt;:
        
        &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; cfg &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; self._servers_cfg:
            &lt;span class="hljs-keyword"&gt;await&lt;/span&gt; self.add_mcp_server(cfg[&lt;span class="hljs-string"&gt;"type"&lt;/span&gt;], **cfg[&lt;span class="hljs-string"&gt;"config"&lt;/span&gt;])
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		2. The agent’s core: the Loop
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;Agent.run()&lt;/code&gt; method is an asynchronous generator that processes a single user input. It manages the conversation turns, deciding when the agent's current task is complete.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;

&lt;span class="hljs-keyword"&gt;async&lt;/span&gt; &lt;span class="hljs-keyword"&gt;def&lt;/span&gt; &lt;span class="hljs-title function_"&gt;run&lt;/span&gt;(&lt;span class="hljs-params"&gt;self, user_input: &lt;span class="hljs-built_in"&gt;str&lt;/span&gt;, *, abort_event: &lt;span class="hljs-type"&gt;Optional&lt;/span&gt;[asyncio.Event] = &lt;span class="hljs-literal"&gt;None&lt;/span&gt;, ...&lt;/span&gt;) -&amp;gt; AsyncGenerator[...]:
    ...
    &lt;span class="hljs-keyword"&gt;while&lt;/span&gt; &lt;span class="hljs-literal"&gt;True&lt;/span&gt;: 
        ...

        
        
        &lt;span class="hljs-keyword"&gt;async&lt;/span&gt; &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; item &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; self.process_single_turn_with_tools(
            self.messages,
            ...
        ):
            &lt;span class="hljs-keyword"&gt;yield&lt;/span&gt; item 

        ... 
        
        
        
        &lt;span class="hljs-keyword"&gt;if&lt;/span&gt; last.get(&lt;span class="hljs-string"&gt;"role"&lt;/span&gt;) == &lt;span class="hljs-string"&gt;"tool"&lt;/span&gt; &lt;span class="hljs-keyword"&gt;and&lt;/span&gt; last.get(&lt;span class="hljs-string"&gt;"name"&lt;/span&gt;) &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; {t.function.name &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; t &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; EXIT_LOOP_TOOLS}:
                &lt;span class="hljs-keyword"&gt;return&lt;/span&gt;

        
        &lt;span class="hljs-keyword"&gt;if&lt;/span&gt; last.get(&lt;span class="hljs-string"&gt;"role"&lt;/span&gt;) != &lt;span class="hljs-string"&gt;"tool"&lt;/span&gt; &lt;span class="hljs-keyword"&gt;and&lt;/span&gt; num_turns &amp;gt; MAX_NUM_TURNS:
                &lt;span class="hljs-keyword"&gt;return&lt;/span&gt;
        &lt;span class="hljs-keyword"&gt;if&lt;/span&gt; last.get(&lt;span class="hljs-string"&gt;"role"&lt;/span&gt;) != &lt;span class="hljs-string"&gt;"tool"&lt;/span&gt; &lt;span class="hljs-keyword"&gt;and&lt;/span&gt; next_turn_should_call_tools:
            &lt;span class="hljs-keyword"&gt;return&lt;/span&gt;
        
        next_turn_should_call_tools = (last_message.get(&lt;span class="hljs-string"&gt;"role"&lt;/span&gt;) != &lt;span class="hljs-string"&gt;"tool"&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inside the &lt;code&gt;run()&lt;/code&gt; loop:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It first adds the user prompt to the conversation.&lt;/li&gt;
&lt;li&gt;Then it calls &lt;code&gt;MCPClient.process_single_turn_with_tools(...)&lt;/code&gt; to get the LLM's response and handle any tool executions for one step of reasoning.&lt;/li&gt;
&lt;li&gt;Each item is immediately yielded, enabling real-time streaming to the caller.&lt;/li&gt;
&lt;li&gt;After each step, it checks exit conditions: if a special "exit loop" tools was used, if a maximum turn limit is hit, or if the LLM provides a text response that seems final for the current request.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Next Steps
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;There are a lot of cool ways to explore and expand upon the MCP Client and the Tiny Agent 🔥 
Here are some ideas to get you started:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Benchmark how different LLM models and inference providers impact agentic performance: Tool calling performance can differ because each provider may optimize it differently. You can find the list of supported providers here.&lt;/li&gt;
&lt;li&gt;Run tiny agents with local LLM inference servers, such as llama.cpp, or LM Studio.&lt;/li&gt;
&lt;li&gt;.. and of course contribute! Share your unique tiny agents and open PRs in tiny-agents/tiny-agents dataset on the Hugging Face Hub.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pull requests and contributions are welcome! Again, everything here is open source! 💎❤️&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/python-tiny-agents</guid><pubDate>Fri, 23 May 2025 00:00:00 +0000</pubDate></item><item><title>Dell Enterprise Hub is all you need to build AI on premises (Hugging Face - Blog)</title><link>https://huggingface.co/blog/dell-ai-applications</link><description>&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Arjuna's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://huggingface.co/avatars/3f7fb413e898c4c3218018d50c45230d.svg" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;
This week at Dell Tech World, we announced the new version of Dell Enterprise Hub, with a complete suite of models and applications to easily build AI running on premises with Dell AI servers and AI PCs.
&lt;img alt="Dell and Hugging Face announcing the Dell Enterprise Hub" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dell-ai-applications/dell-post-thumbnail.png" /&gt;

&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Models Ready for Action
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;If you go to the Dell Enterprise Hub today, you can find some of the most popular models, like Meta Llama 4 Maverick, DeepSeek R1 or Google Gemma 3, available for deployment and training in a few clicks.&lt;/p&gt;
&lt;p&gt;But what you get is much more than a model, it’s a fully tested container optimized for specific Dell AI Server Platforms, with easy instructions to deploy on-premises using Docker and Kubernetes.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Meta Llama 4 Maverick available for Dell AI Server platforms" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dell-ai-applications/dell-blog-1.png" /&gt;&lt;br /&gt;&lt;em&gt;Meta Llama 4 Maverick can be deployed on NVIDIA H200 or AMD MI300X Dell PowerEdge servers&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We continuously work with Dell CTIO and Engineering teams to make the latest and greatest models ready, tested and optimized for Dell AI Server platforms as quickly as possible - Llama 4 models were available on the Dell Enterprise Hub within 1 hour of their public release by Meta!&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Introducing AI Applications
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The Dell Enterprise Hub now features ready-to-deploy AI Applications!&lt;/p&gt;
&lt;p&gt;If models are engines, then applications are the cars that make them useful so you can actually go places. With the new Application Catalog you can build powerful applications that run entirely on-premises for your employees and use your internal data and services.&lt;/p&gt;
&lt;p&gt;The new Application Catalog makes it easy to deploy leading open source applications within your private network, including OpenWebUI and AnythingLLM.&lt;/p&gt;
&lt;img alt="OpenWebUI and AnythingLLM available in the Dell Application Catalog" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dell-ai-applications/dell-blog-2.png" /&gt;

&lt;p&gt;OpenWebUI makes it easy to deploy on-premises chatbot assistants that connect to your internal data and services via MCP, to build agentic experiences that can search the web, retrieve internal data with vector databases and storage for RAG use cases. &lt;/p&gt;
&lt;img alt="OpenWebUI User Interface" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dell-ai-applications/dell-blog-3-openwebui.png" /&gt;

&lt;p&gt;AnythingLLM makes it easy to build powerful agentic assistants connecting to multiple MCP servers so you can connect your internal systems or even external services. It includes features to enable multiple models, working with images, documents and set role-based access controls for your internal users.&lt;/p&gt;
&lt;img alt="AnythingLLM User Interface" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dell-ai-applications/dell-blog-4-anythingllm.png" /&gt;

&lt;p&gt;These applications are easy to deploy using the provided, customizable helm charts so your MCP servers are registered from the get go.&lt;/p&gt;
&lt;img alt="Deployment instructions for OpenWebUI on Dell AI Server" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dell-ai-applications/dell-blog-5-helm.png" /&gt;

&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Powered by NVIDIA, AMD and Intel
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Dell Enterprise Hub is the only platform in the world that offers ready-to-use model deployment solutions for the latest AI Accelerator hardware:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NVIDIA H100 and H200 GPU powered Dell platforms&lt;/li&gt;
&lt;li&gt;AMD MI300X powered Dell platforms&lt;/li&gt;
&lt;li&gt;Intel Gaudi 3 powered Dell platforms&lt;/li&gt;
&lt;/ul&gt;
&lt;img alt="Dell Enterprise Hub supports NVIDIA, AMD and Intel powered Dell AI Servers" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dell-ai-applications/dell-blog-6.png" /&gt;

&lt;p&gt;We work directly with Dell, NVIDIA, AMD and Intel so that when you deploy a container on your system, it’s all configured and ready to go, has been fully tested and benchmarked so it runs with the best performance out of the box on your Dell AI Server platform.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		On-Device Models for Dell AI PC
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The new Dell Enterprise Hub now provides support for models to run on-device on Dell AI PCs in addition to AI Servers!&lt;/p&gt;
&lt;img alt="Model Catalog includes many models for on-device inference on Dell AI PCs" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dell-ai-applications/dell-blog-7-AI-PC.png" /&gt;

&lt;p&gt;These models enable on-device speech transcription (OpenAI whisper), chat assistants (Microsoft Phi and Qwen 2.5), upscaling images and generating embeddings.&lt;/p&gt;
&lt;p&gt;To deploy a model, you can follow specific instructions for the Dell AI PC of your choice, powered by Intel or Qualcomm NPUs, using the new Dell Pro AI Studio. Coupled with PC fleet management systems like Microsoft Intune, it’s a complete solution for IT organizations to enable employees with on-device AI capabilities.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Now with CLI and Python SDK
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Dell Enterprise Hub offers an online portal into AI capabilities for Dell AI Server platforms and AI PCs. But what if you want to work directly from your development environment?&lt;/p&gt;
&lt;p&gt;Introducing the new dell-ai open source library with a Python SDK and CLI, so you can use Dell Enterprise Hub within your environment directly from your terminal or code - just &lt;code&gt;pip install dell-ai&lt;/code&gt;&lt;/p&gt;
&lt;img alt="Available commands for dell-ai CLI" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dell-ai-applications/dell-blog-8-CLI.png" /&gt;

&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Wrapping up
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;With Models and Applications, for AI Servers and AI PCs, easily installable using Docker, Kubernetes and Dell Pro AI Studio, Dell Enterprise Hub is a complete toolkit to deploy Gen AI applications in the enterprise, fully secure and on-premises.&lt;/p&gt;
&lt;p&gt;As a Dell customer, that means you can very quickly, within an hour instead of weeks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;roll out an in-network chat assistant powered by the latest open LLMs, and connect it to your internal storage systems (ex. Dell PowerScale) using MCP, all in an air gapped environment&lt;/li&gt;
&lt;li&gt;give access to complex agentic systems, with granular access controls and SSO, that can work with internal text, code, images, audio and documents and access the web for current context&lt;/li&gt;
&lt;li&gt;set up employees with on-device, private transcription powered by a fleet of Dell AI PCs in a fully managed way&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you are using Dell Enterprise Hub today, we would love to hear from you in the comments!&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Arjuna's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://huggingface.co/avatars/3f7fb413e898c4c3218018d50c45230d.svg" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;
This week at Dell Tech World, we announced the new version of Dell Enterprise Hub, with a complete suite of models and applications to easily build AI running on premises with Dell AI servers and AI PCs.
&lt;img alt="Dell and Hugging Face announcing the Dell Enterprise Hub" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dell-ai-applications/dell-post-thumbnail.png" /&gt;

&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Models Ready for Action
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;If you go to the Dell Enterprise Hub today, you can find some of the most popular models, like Meta Llama 4 Maverick, DeepSeek R1 or Google Gemma 3, available for deployment and training in a few clicks.&lt;/p&gt;
&lt;p&gt;But what you get is much more than a model, it’s a fully tested container optimized for specific Dell AI Server Platforms, with easy instructions to deploy on-premises using Docker and Kubernetes.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Meta Llama 4 Maverick available for Dell AI Server platforms" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dell-ai-applications/dell-blog-1.png" /&gt;&lt;br /&gt;&lt;em&gt;Meta Llama 4 Maverick can be deployed on NVIDIA H200 or AMD MI300X Dell PowerEdge servers&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We continuously work with Dell CTIO and Engineering teams to make the latest and greatest models ready, tested and optimized for Dell AI Server platforms as quickly as possible - Llama 4 models were available on the Dell Enterprise Hub within 1 hour of their public release by Meta!&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Introducing AI Applications
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The Dell Enterprise Hub now features ready-to-deploy AI Applications!&lt;/p&gt;
&lt;p&gt;If models are engines, then applications are the cars that make them useful so you can actually go places. With the new Application Catalog you can build powerful applications that run entirely on-premises for your employees and use your internal data and services.&lt;/p&gt;
&lt;p&gt;The new Application Catalog makes it easy to deploy leading open source applications within your private network, including OpenWebUI and AnythingLLM.&lt;/p&gt;
&lt;img alt="OpenWebUI and AnythingLLM available in the Dell Application Catalog" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dell-ai-applications/dell-blog-2.png" /&gt;

&lt;p&gt;OpenWebUI makes it easy to deploy on-premises chatbot assistants that connect to your internal data and services via MCP, to build agentic experiences that can search the web, retrieve internal data with vector databases and storage for RAG use cases. &lt;/p&gt;
&lt;img alt="OpenWebUI User Interface" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dell-ai-applications/dell-blog-3-openwebui.png" /&gt;

&lt;p&gt;AnythingLLM makes it easy to build powerful agentic assistants connecting to multiple MCP servers so you can connect your internal systems or even external services. It includes features to enable multiple models, working with images, documents and set role-based access controls for your internal users.&lt;/p&gt;
&lt;img alt="AnythingLLM User Interface" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dell-ai-applications/dell-blog-4-anythingllm.png" /&gt;

&lt;p&gt;These applications are easy to deploy using the provided, customizable helm charts so your MCP servers are registered from the get go.&lt;/p&gt;
&lt;img alt="Deployment instructions for OpenWebUI on Dell AI Server" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dell-ai-applications/dell-blog-5-helm.png" /&gt;

&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Powered by NVIDIA, AMD and Intel
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Dell Enterprise Hub is the only platform in the world that offers ready-to-use model deployment solutions for the latest AI Accelerator hardware:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NVIDIA H100 and H200 GPU powered Dell platforms&lt;/li&gt;
&lt;li&gt;AMD MI300X powered Dell platforms&lt;/li&gt;
&lt;li&gt;Intel Gaudi 3 powered Dell platforms&lt;/li&gt;
&lt;/ul&gt;
&lt;img alt="Dell Enterprise Hub supports NVIDIA, AMD and Intel powered Dell AI Servers" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dell-ai-applications/dell-blog-6.png" /&gt;

&lt;p&gt;We work directly with Dell, NVIDIA, AMD and Intel so that when you deploy a container on your system, it’s all configured and ready to go, has been fully tested and benchmarked so it runs with the best performance out of the box on your Dell AI Server platform.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		On-Device Models for Dell AI PC
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The new Dell Enterprise Hub now provides support for models to run on-device on Dell AI PCs in addition to AI Servers!&lt;/p&gt;
&lt;img alt="Model Catalog includes many models for on-device inference on Dell AI PCs" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dell-ai-applications/dell-blog-7-AI-PC.png" /&gt;

&lt;p&gt;These models enable on-device speech transcription (OpenAI whisper), chat assistants (Microsoft Phi and Qwen 2.5), upscaling images and generating embeddings.&lt;/p&gt;
&lt;p&gt;To deploy a model, you can follow specific instructions for the Dell AI PC of your choice, powered by Intel or Qualcomm NPUs, using the new Dell Pro AI Studio. Coupled with PC fleet management systems like Microsoft Intune, it’s a complete solution for IT organizations to enable employees with on-device AI capabilities.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Now with CLI and Python SDK
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Dell Enterprise Hub offers an online portal into AI capabilities for Dell AI Server platforms and AI PCs. But what if you want to work directly from your development environment?&lt;/p&gt;
&lt;p&gt;Introducing the new dell-ai open source library with a Python SDK and CLI, so you can use Dell Enterprise Hub within your environment directly from your terminal or code - just &lt;code&gt;pip install dell-ai&lt;/code&gt;&lt;/p&gt;
&lt;img alt="Available commands for dell-ai CLI" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dell-ai-applications/dell-blog-8-CLI.png" /&gt;

&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Wrapping up
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;With Models and Applications, for AI Servers and AI PCs, easily installable using Docker, Kubernetes and Dell Pro AI Studio, Dell Enterprise Hub is a complete toolkit to deploy Gen AI applications in the enterprise, fully secure and on-premises.&lt;/p&gt;
&lt;p&gt;As a Dell customer, that means you can very quickly, within an hour instead of weeks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;roll out an in-network chat assistant powered by the latest open LLMs, and connect it to your internal storage systems (ex. Dell PowerScale) using MCP, all in an air gapped environment&lt;/li&gt;
&lt;li&gt;give access to complex agentic systems, with granular access controls and SSO, that can work with internal text, code, images, audio and documents and access the web for current context&lt;/li&gt;
&lt;li&gt;set up employees with on-device, private transcription powered by a fleet of Dell AI PCs in a fully managed way&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you are using Dell Enterprise Hub today, we would love to hear from you in the comments!&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/dell-ai-applications</guid><pubDate>Fri, 23 May 2025 00:00:00 +0000</pubDate></item><item><title>Fine-tuning LLMs with user-level differential privacy (The latest research from Google)</title><link>https://research.google/blog/fine-tuning-llms-with-user-level-differential-privacy/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;Making these algorithms work for LLMs&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;If we run these algorithms “out-of-the-box” for LLMs, things go badly. So, we came up with optimizations to the algorithms that fix the key issues with running them “out-of-the-box”.&lt;/p&gt;&lt;p&gt;For ELS, we had to go from example-level DP guarantees to user-level DP guarantees. We found that previous work was adding orders of magnitude more noise than was actually necessary. We were able to prove that we can add significantly less noise, making the model much better while retaining the same privacy guarantees.&lt;/p&gt;&lt;p&gt;For both ELS and ULS, we had to figure out how to optimize the contribution bound. A “default” choice is to choose a contribution bound that every user already satisfies; that is, we don’t do any pre-processing. However, some users may contribute a large amount of data, and we will need to add large amounts of noise to provide privacy to these users. Setting a smaller contribution bound reduces the amount of noise we need to add, but the cost is having to discard a lot of data. Because LLM training runs are expensive, we can’t afford to try training a bunch of models with different contribution bounds and pick the best one — we need an effective strategy to pick the contribution bound &lt;i&gt;before&lt;/i&gt; we start training.&lt;/p&gt;&lt;p&gt;After lengthy experimentation at scale, for ELS we found that setting the contribution bound to be the median number of examples held by each user was an effective strategy. For ULS, we give a prediction for the total noise added as a function of the contribution bound, and found that choosing the contribution bound minimizing this prediction was an effective strategy.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;Making these algorithms work for LLMs&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;If we run these algorithms “out-of-the-box” for LLMs, things go badly. So, we came up with optimizations to the algorithms that fix the key issues with running them “out-of-the-box”.&lt;/p&gt;&lt;p&gt;For ELS, we had to go from example-level DP guarantees to user-level DP guarantees. We found that previous work was adding orders of magnitude more noise than was actually necessary. We were able to prove that we can add significantly less noise, making the model much better while retaining the same privacy guarantees.&lt;/p&gt;&lt;p&gt;For both ELS and ULS, we had to figure out how to optimize the contribution bound. A “default” choice is to choose a contribution bound that every user already satisfies; that is, we don’t do any pre-processing. However, some users may contribute a large amount of data, and we will need to add large amounts of noise to provide privacy to these users. Setting a smaller contribution bound reduces the amount of noise we need to add, but the cost is having to discard a lot of data. Because LLM training runs are expensive, we can’t afford to try training a bunch of models with different contribution bounds and pick the best one — we need an effective strategy to pick the contribution bound &lt;i&gt;before&lt;/i&gt; we start training.&lt;/p&gt;&lt;p&gt;After lengthy experimentation at scale, for ELS we found that setting the contribution bound to be the median number of examples held by each user was an effective strategy. For ULS, we give a prediction for the total noise added as a function of the contribution bound, and found that choosing the contribution bound minimizing this prediction was an effective strategy.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/fine-tuning-llms-with-user-level-differential-privacy/</guid><pubDate>Fri, 23 May 2025 17:00:00 +0000</pubDate></item><item><title>🐯 Liger GRPO meets TRL (Hugging Face - Blog)</title><link>https://huggingface.co/blog/liger-grpo</link><description>&lt;!-- HTML_TAG_START --&gt;
TL; DR
Liger supercharges TRL’s Group Relative Policy Optimization GRPO Trainer by slashing memory usage by &lt;strong&gt;40%&lt;/strong&gt; with zero drop in model quality. We also added support for &lt;strong&gt;FSDP&lt;/strong&gt; and &lt;strong&gt;PEFT&lt;/strong&gt;, making it easier than ever to scale GRPO across multiple GPUs.
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Motivation
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Fine-tuning language models using reinforcement learning (RL) is a crucial step in a model's training lifecycle for steering models towards desirable behaviours which are more complex than can be achieved through typical supervised fine-tuning. RL has traditionally been applied to optimize large language models (LLMs) using the Proximal Policy Optimization (PPO) algorithm. This approach, often associated with Reinforcement Learning from Human Feedback (RLHF), utilizes a separately trained reward model to guide the fine-tuning of the primary model. &lt;/p&gt;
&lt;p&gt;However, RLHF with PPO is a very resource-hungry approach - PPO requires loading multiple models in memory (policy, value, reward, and reference models), and also requires several iterations of fine-tuning reward and base models to achieve the desired results. The success of RLHF also depends on the capability of the reward model to effectively discriminate between desired and un-desired behaviour from our model.&lt;/p&gt;
&lt;p&gt;Group Relative Policy Optimization (GRPO) has seen significant recent popularity alongside DeepSeek's R1 model. GRPO eschews the pre-trained reward model and value models used in RLHF and instead relies on &lt;em&gt;verifiable reward functions&lt;/em&gt; which can check the correctness of a model's output in a closed-form manner without needing an external reward model. This has resulted in massive improvements when using GRPO instead of PPO for fine-tuning on domains which are easily verifiable, such as teaching a model to reason, and perform well on math and coding tasks. &lt;/p&gt;
&lt;p&gt;The following diagram shows the GRPO vs PPO training pipeline (ref: Figure 4 of DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models):&lt;/p&gt;
&lt;p&gt;&lt;img alt="PPO-vs-GRPO" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/liger-grpo/image5.png" /&gt;&lt;/p&gt;
&lt;p&gt;That said, RL training still eats up a ton of GPU memory, so there's still plenty of room for optimizations here. In this blog post, we talk about an optimization that we recently added to TRL that cuts peak memory usage by 40% during GRPO Training, and we also dive into how to scale GRPO to multiple GPUs and nodes without losing performance or correctness.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		How Liger Kernel slashes memory for GRPO
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We extended the Liger Chunked Loss approach to the GRPO Loss, which lets us avoid having to store the full logits in memory for every training step. The calculation of logits, which involves the model's output head, is a significant contributor to peak memory usage, especially when dealing with large vocabularies, long sequence lengths, or large batch sizes. We address this by chunking the input to the &lt;code&gt;lm_head&lt;/code&gt; across the batch and running the forward pass one chunk at a time.&lt;/p&gt;
&lt;p&gt;But if you just implement it in a straightforward way, you won't actually be able to reduce the peak memory since you'd still need to keep all the logits in GPU memory for the backward pass. To get around that, we calculate the gradients for each loss chunk (with respect to the &lt;code&gt;input&lt;/code&gt; chunk and the &lt;code&gt;lm_head&lt;/code&gt; weight) during the forward pass, and then accumulate them as we go through each chunk.&lt;/p&gt;
&lt;p&gt;Here's the visualization of the optimization (ref: Byron Hsu):&lt;/p&gt;
&lt;p&gt;&lt;img alt="liger-chunked-loss" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/liger-grpo/image7.gif" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Plug-and-Play integration with TRL
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We recently integrated Liger GRPO with TRL in PR #3184, so now you can use the Liger GRPO loss just by setting &lt;code&gt;use_liger_loss&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt; in your &lt;code&gt;GRPOConfig&lt;/code&gt; and enjoy the memory savings!&lt;/p&gt;
&lt;p&gt;Heads up: these features aren't in the latest TRL release yet, so you'll need to install TRL from source for now:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install &lt;span class="hljs-string"&gt;"trl[liger] @ git+https://github.com/huggingface/trl.git"&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then you can use it like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; trl &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; GRPOConfig, GRPOTrainer
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; datasets &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; load_dataset


train_dataset = load_dataset(&lt;span class="hljs-string"&gt;"trl-lib/tldr"&lt;/span&gt;, split=&lt;span class="hljs-string"&gt;"train"&lt;/span&gt;)
training_args = GRPOConfig(output_dir=&lt;span class="hljs-string"&gt;"Qwen3-0.6B-GRPO"&lt;/span&gt;, use_liger_loss=&lt;span class="hljs-literal"&gt;True&lt;/span&gt;)

&lt;span class="hljs-keyword"&gt;def&lt;/span&gt; &lt;span class="hljs-title function_"&gt;reward_len&lt;/span&gt;(&lt;span class="hljs-params"&gt;completions, **kwargs&lt;/span&gt;):
    &lt;span class="hljs-keyword"&gt;return&lt;/span&gt; [-&lt;span class="hljs-built_in"&gt;abs&lt;/span&gt;(&lt;span class="hljs-number"&gt;20&lt;/span&gt; - &lt;span class="hljs-built_in"&gt;len&lt;/span&gt;(completion)) &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; completion &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; completions]

trainer = GRPOTrainer(
    model=&lt;span class="hljs-string"&gt;"Qwen/Qwen3-0.6B-Instruct"&lt;/span&gt;,
    reward_funcs=reward_len,
    args=training_args,
    train_dataset=train_dataset,
)
trainer.train()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Benchmarks
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We ran a bunch of GRPO experiments with and without the Liger GRPO Loss to see how things compare. For the policy model, we used &lt;code&gt;Qwen3-0.6B&lt;/code&gt; and played around with different batch sizes. All the experiments were run on the &lt;code&gt;gsm8k&lt;/code&gt; dataset using its reward functions.&lt;/p&gt;
&lt;p&gt;Here's the plots of peak memory usage vs batch size for both FP32 and BF16 training. As expected, the memory savings get better with larger batch sizes since we chunk along the batch dimension. So when the batch size goes up, the Liger chunked loss ends up using a lot less memory, up to 40% less, compared to the regular (non-liger) version. &lt;/p&gt;
&lt;p&gt;Quick note: Right now, we only support FP32, but we're working on open-sourcing BF16 support for Liger GRPO in TRL. The BF16 results shown here are from internal patches we've been testing.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Mem-vs-batch-size-fp32" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/liger-grpo/image3.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Mem-vs-batch-size-bf16" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/liger-grpo/image4.png" /&gt;&lt;/p&gt;
&lt;p&gt;We also show that Liger Loss is effectively accurate. As seen in the plot, rewards over training steps stay pretty much the same as what you'd see using the standard TRL implementation.&lt;/p&gt;
&lt;p&gt;&lt;img alt="reward-vs-step" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/liger-grpo/image1.png" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Scaling further with FSDP and PEFT
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We also added FSDP and PEFT support to Liger GRPO Loss in PR #3260 and PR #3355, respectively, allowing users to easily scale their experiments across multiple GPUs or nodes. PEFT techniques such as LoRA and QLoRA reduce the number of trainable parameters by only tuning the weights of smaller adapter weights on top of the original model, significantly lowering memory pressure as gradients, activations, and optimizer states for the entire model don't need to be held in memory. Additionally, using PEFT in GRPO allows one to forgo loading a separate reference model during training, as we can obtain the original, unmodified model during training by simply disabling the LoRA adapters. &lt;/p&gt;
&lt;p&gt;Here, we show a multi-GPU GRPO training plot using FSDP and PEFT, where we compare the maximum training batch size possible with and without the Liger Loss across different Qwen3 model sizes. We found that with Liger, we were able to bump up the batch size by around &lt;strong&gt;1.5 to 1.8x&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;&lt;img alt="peft-batch-size-vs-model-size" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/liger-grpo/image6.png" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Scaling even further with vLLM
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;To accelerate text generation during training, Liger Loss can be effectively combined with TRL's integrated vLLM server. This significantly speeds up the collection of rollout data with minimal overhead and offers a seamless integration experience.&lt;/p&gt;
&lt;p&gt;Here's how to set it up:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Start the vLLM Server:&lt;/strong&gt;
First, launch the vLLM server. This server will handle the generation requests from your training script. Open a terminal and run:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;CUDA_VISIBLE_DEVICES=1 trl vllm-serve --model &lt;span class="hljs-string"&gt;"Qwen/Qwen3-0.6B"&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Note: We assign &lt;code&gt;CUDA_VISIBLE_DEVICES=1&lt;/code&gt; to run the vLLM server on a specific GPU (GPU 1 in this case), leaving other GPUs free for training.&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Configure and Run Your Training Script:&lt;/strong&gt;
Next, modify your training script to use the vLLM server. The key change is setting &lt;code&gt;use_vllm=True&lt;/code&gt; in your &lt;code&gt;GRPOConfig&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; trl &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; GRPOConfig, GRPOTrainer
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; datasets &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; load_dataset


&lt;span class="hljs-keyword"&gt;def&lt;/span&gt; &lt;span class="hljs-title function_"&gt;reward_len&lt;/span&gt;(&lt;span class="hljs-params"&gt;completions, **kwargs&lt;/span&gt;):
    &lt;span class="hljs-keyword"&gt;return&lt;/span&gt; [-&lt;span class="hljs-built_in"&gt;abs&lt;/span&gt;(&lt;span class="hljs-number"&gt;20&lt;/span&gt; - &lt;span class="hljs-built_in"&gt;len&lt;/span&gt;(completion)) &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; completion &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; completions]

dataset = load_dataset(&lt;span class="hljs-string"&gt;"trl-lib/tldr"&lt;/span&gt;, split=&lt;span class="hljs-string"&gt;"train[:1%]"&lt;/span&gt;)
training_args = GRPOConfig(
    output_dir=&lt;span class="hljs-string"&gt;"Qwen3-0.6B-GRPO"&lt;/span&gt;, 
    use_liger_loss=&lt;span class="hljs-literal"&gt;True&lt;/span&gt;, 
    use_vllm=&lt;span class="hljs-literal"&gt;True&lt;/span&gt;, 
    logging_steps=&lt;span class="hljs-number"&gt;10&lt;/span&gt;
)
trainer = GRPOTrainer(
    model=&lt;span class="hljs-string"&gt;"Qwen/Qwen3-0.6B"&lt;/span&gt;, 
    reward_funcs=reward_len,
    args=training_args,
    train_dataset=dataset,
)
trainer.train()
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Launch the Training:&lt;/strong&gt;
Finally, run your training script using &lt;code&gt;accelerate launch&lt;/code&gt; (or &lt;code&gt;python&lt;/code&gt; if not using Accelerate for multi-GPU/distributed training). Make sure to target a different GPU for training if your vLLM server is occupying one.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;CUDA_VISIBLE_DEVICES=0 accelerate launch train.py 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;(Assuming your script is named &lt;code&gt;train.py&lt;/code&gt; and you want to run training on GPU 0)&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By following these steps, you can leverage vLLM for faster generation turnarounds during your GRPO training with Liger Loss.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Conclusion
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;With the integration of Liger-GRPO into TRL, alongside FSDP and PEFT support, fine-tuning language models with GRPO is now more memory-efficient and scalable than ever. We encourage the community to try out these new features and share their feedback to help us further improve RL training for LLMs.&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;!-- HTML_TAG_START --&gt;
TL; DR
Liger supercharges TRL’s Group Relative Policy Optimization GRPO Trainer by slashing memory usage by &lt;strong&gt;40%&lt;/strong&gt; with zero drop in model quality. We also added support for &lt;strong&gt;FSDP&lt;/strong&gt; and &lt;strong&gt;PEFT&lt;/strong&gt;, making it easier than ever to scale GRPO across multiple GPUs.
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Motivation
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Fine-tuning language models using reinforcement learning (RL) is a crucial step in a model's training lifecycle for steering models towards desirable behaviours which are more complex than can be achieved through typical supervised fine-tuning. RL has traditionally been applied to optimize large language models (LLMs) using the Proximal Policy Optimization (PPO) algorithm. This approach, often associated with Reinforcement Learning from Human Feedback (RLHF), utilizes a separately trained reward model to guide the fine-tuning of the primary model. &lt;/p&gt;
&lt;p&gt;However, RLHF with PPO is a very resource-hungry approach - PPO requires loading multiple models in memory (policy, value, reward, and reference models), and also requires several iterations of fine-tuning reward and base models to achieve the desired results. The success of RLHF also depends on the capability of the reward model to effectively discriminate between desired and un-desired behaviour from our model.&lt;/p&gt;
&lt;p&gt;Group Relative Policy Optimization (GRPO) has seen significant recent popularity alongside DeepSeek's R1 model. GRPO eschews the pre-trained reward model and value models used in RLHF and instead relies on &lt;em&gt;verifiable reward functions&lt;/em&gt; which can check the correctness of a model's output in a closed-form manner without needing an external reward model. This has resulted in massive improvements when using GRPO instead of PPO for fine-tuning on domains which are easily verifiable, such as teaching a model to reason, and perform well on math and coding tasks. &lt;/p&gt;
&lt;p&gt;The following diagram shows the GRPO vs PPO training pipeline (ref: Figure 4 of DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models):&lt;/p&gt;
&lt;p&gt;&lt;img alt="PPO-vs-GRPO" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/liger-grpo/image5.png" /&gt;&lt;/p&gt;
&lt;p&gt;That said, RL training still eats up a ton of GPU memory, so there's still plenty of room for optimizations here. In this blog post, we talk about an optimization that we recently added to TRL that cuts peak memory usage by 40% during GRPO Training, and we also dive into how to scale GRPO to multiple GPUs and nodes without losing performance or correctness.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		How Liger Kernel slashes memory for GRPO
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We extended the Liger Chunked Loss approach to the GRPO Loss, which lets us avoid having to store the full logits in memory for every training step. The calculation of logits, which involves the model's output head, is a significant contributor to peak memory usage, especially when dealing with large vocabularies, long sequence lengths, or large batch sizes. We address this by chunking the input to the &lt;code&gt;lm_head&lt;/code&gt; across the batch and running the forward pass one chunk at a time.&lt;/p&gt;
&lt;p&gt;But if you just implement it in a straightforward way, you won't actually be able to reduce the peak memory since you'd still need to keep all the logits in GPU memory for the backward pass. To get around that, we calculate the gradients for each loss chunk (with respect to the &lt;code&gt;input&lt;/code&gt; chunk and the &lt;code&gt;lm_head&lt;/code&gt; weight) during the forward pass, and then accumulate them as we go through each chunk.&lt;/p&gt;
&lt;p&gt;Here's the visualization of the optimization (ref: Byron Hsu):&lt;/p&gt;
&lt;p&gt;&lt;img alt="liger-chunked-loss" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/liger-grpo/image7.gif" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Plug-and-Play integration with TRL
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We recently integrated Liger GRPO with TRL in PR #3184, so now you can use the Liger GRPO loss just by setting &lt;code&gt;use_liger_loss&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt; in your &lt;code&gt;GRPOConfig&lt;/code&gt; and enjoy the memory savings!&lt;/p&gt;
&lt;p&gt;Heads up: these features aren't in the latest TRL release yet, so you'll need to install TRL from source for now:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install &lt;span class="hljs-string"&gt;"trl[liger] @ git+https://github.com/huggingface/trl.git"&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then you can use it like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; trl &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; GRPOConfig, GRPOTrainer
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; datasets &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; load_dataset


train_dataset = load_dataset(&lt;span class="hljs-string"&gt;"trl-lib/tldr"&lt;/span&gt;, split=&lt;span class="hljs-string"&gt;"train"&lt;/span&gt;)
training_args = GRPOConfig(output_dir=&lt;span class="hljs-string"&gt;"Qwen3-0.6B-GRPO"&lt;/span&gt;, use_liger_loss=&lt;span class="hljs-literal"&gt;True&lt;/span&gt;)

&lt;span class="hljs-keyword"&gt;def&lt;/span&gt; &lt;span class="hljs-title function_"&gt;reward_len&lt;/span&gt;(&lt;span class="hljs-params"&gt;completions, **kwargs&lt;/span&gt;):
    &lt;span class="hljs-keyword"&gt;return&lt;/span&gt; [-&lt;span class="hljs-built_in"&gt;abs&lt;/span&gt;(&lt;span class="hljs-number"&gt;20&lt;/span&gt; - &lt;span class="hljs-built_in"&gt;len&lt;/span&gt;(completion)) &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; completion &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; completions]

trainer = GRPOTrainer(
    model=&lt;span class="hljs-string"&gt;"Qwen/Qwen3-0.6B-Instruct"&lt;/span&gt;,
    reward_funcs=reward_len,
    args=training_args,
    train_dataset=train_dataset,
)
trainer.train()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Benchmarks
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We ran a bunch of GRPO experiments with and without the Liger GRPO Loss to see how things compare. For the policy model, we used &lt;code&gt;Qwen3-0.6B&lt;/code&gt; and played around with different batch sizes. All the experiments were run on the &lt;code&gt;gsm8k&lt;/code&gt; dataset using its reward functions.&lt;/p&gt;
&lt;p&gt;Here's the plots of peak memory usage vs batch size for both FP32 and BF16 training. As expected, the memory savings get better with larger batch sizes since we chunk along the batch dimension. So when the batch size goes up, the Liger chunked loss ends up using a lot less memory, up to 40% less, compared to the regular (non-liger) version. &lt;/p&gt;
&lt;p&gt;Quick note: Right now, we only support FP32, but we're working on open-sourcing BF16 support for Liger GRPO in TRL. The BF16 results shown here are from internal patches we've been testing.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Mem-vs-batch-size-fp32" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/liger-grpo/image3.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Mem-vs-batch-size-bf16" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/liger-grpo/image4.png" /&gt;&lt;/p&gt;
&lt;p&gt;We also show that Liger Loss is effectively accurate. As seen in the plot, rewards over training steps stay pretty much the same as what you'd see using the standard TRL implementation.&lt;/p&gt;
&lt;p&gt;&lt;img alt="reward-vs-step" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/liger-grpo/image1.png" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Scaling further with FSDP and PEFT
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We also added FSDP and PEFT support to Liger GRPO Loss in PR #3260 and PR #3355, respectively, allowing users to easily scale their experiments across multiple GPUs or nodes. PEFT techniques such as LoRA and QLoRA reduce the number of trainable parameters by only tuning the weights of smaller adapter weights on top of the original model, significantly lowering memory pressure as gradients, activations, and optimizer states for the entire model don't need to be held in memory. Additionally, using PEFT in GRPO allows one to forgo loading a separate reference model during training, as we can obtain the original, unmodified model during training by simply disabling the LoRA adapters. &lt;/p&gt;
&lt;p&gt;Here, we show a multi-GPU GRPO training plot using FSDP and PEFT, where we compare the maximum training batch size possible with and without the Liger Loss across different Qwen3 model sizes. We found that with Liger, we were able to bump up the batch size by around &lt;strong&gt;1.5 to 1.8x&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;&lt;img alt="peft-batch-size-vs-model-size" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/liger-grpo/image6.png" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Scaling even further with vLLM
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;To accelerate text generation during training, Liger Loss can be effectively combined with TRL's integrated vLLM server. This significantly speeds up the collection of rollout data with minimal overhead and offers a seamless integration experience.&lt;/p&gt;
&lt;p&gt;Here's how to set it up:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Start the vLLM Server:&lt;/strong&gt;
First, launch the vLLM server. This server will handle the generation requests from your training script. Open a terminal and run:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;CUDA_VISIBLE_DEVICES=1 trl vllm-serve --model &lt;span class="hljs-string"&gt;"Qwen/Qwen3-0.6B"&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Note: We assign &lt;code&gt;CUDA_VISIBLE_DEVICES=1&lt;/code&gt; to run the vLLM server on a specific GPU (GPU 1 in this case), leaving other GPUs free for training.&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Configure and Run Your Training Script:&lt;/strong&gt;
Next, modify your training script to use the vLLM server. The key change is setting &lt;code&gt;use_vllm=True&lt;/code&gt; in your &lt;code&gt;GRPOConfig&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; trl &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; GRPOConfig, GRPOTrainer
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; datasets &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; load_dataset


&lt;span class="hljs-keyword"&gt;def&lt;/span&gt; &lt;span class="hljs-title function_"&gt;reward_len&lt;/span&gt;(&lt;span class="hljs-params"&gt;completions, **kwargs&lt;/span&gt;):
    &lt;span class="hljs-keyword"&gt;return&lt;/span&gt; [-&lt;span class="hljs-built_in"&gt;abs&lt;/span&gt;(&lt;span class="hljs-number"&gt;20&lt;/span&gt; - &lt;span class="hljs-built_in"&gt;len&lt;/span&gt;(completion)) &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; completion &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; completions]

dataset = load_dataset(&lt;span class="hljs-string"&gt;"trl-lib/tldr"&lt;/span&gt;, split=&lt;span class="hljs-string"&gt;"train[:1%]"&lt;/span&gt;)
training_args = GRPOConfig(
    output_dir=&lt;span class="hljs-string"&gt;"Qwen3-0.6B-GRPO"&lt;/span&gt;, 
    use_liger_loss=&lt;span class="hljs-literal"&gt;True&lt;/span&gt;, 
    use_vllm=&lt;span class="hljs-literal"&gt;True&lt;/span&gt;, 
    logging_steps=&lt;span class="hljs-number"&gt;10&lt;/span&gt;
)
trainer = GRPOTrainer(
    model=&lt;span class="hljs-string"&gt;"Qwen/Qwen3-0.6B"&lt;/span&gt;, 
    reward_funcs=reward_len,
    args=training_args,
    train_dataset=dataset,
)
trainer.train()
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Launch the Training:&lt;/strong&gt;
Finally, run your training script using &lt;code&gt;accelerate launch&lt;/code&gt; (or &lt;code&gt;python&lt;/code&gt; if not using Accelerate for multi-GPU/distributed training). Make sure to target a different GPU for training if your vLLM server is occupying one.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;CUDA_VISIBLE_DEVICES=0 accelerate launch train.py 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;(Assuming your script is named &lt;code&gt;train.py&lt;/code&gt; and you want to run training on GPU 0)&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By following these steps, you can leverage vLLM for faster generation turnarounds during your GRPO training with Liger Loss.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Conclusion
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;With the integration of Liger-GRPO into TRL, alongside FSDP and PEFT support, fine-tuning language models with GRPO is now more memory-efficient and scalable than ever. We encourage the community to try out these new features and share their feedback to help us further improve RL training for LLMs.&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/liger-grpo</guid><pubDate>Sun, 25 May 2025 00:00:00 +0000</pubDate></item><item><title>MIT announces the Initiative for New Manufacturing (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/mit-announces-initiative-for-new-manufacturing-0527</link><description>&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;MIT today launched its Initiative for New Manufacturing (INM), an Institute-wide effort to reinfuse U.S. industrial production with leading-edge technologies, bolster crucial U.S. economic sectors, and ignite job creation.&lt;/p&gt;&lt;p&gt;The initiative will encompass advanced research, innovative education programs, and partnership with companies across many sectors, in a bid to help transform manufacturing and elevate its impact.&lt;/p&gt;&lt;p&gt;“We want to work with firms big and small, in cities, small towns and everywhere in between, to help them adopt new approaches for increased productivity,” MIT President Sally A. Kornbluth wrote in a letter to the Institute community this morning. “We want to deliberately design high-quality, human-centered manufacturing jobs that bring new life to communities across the country.”&lt;/p&gt;        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-youtube-video paragraph--view-mode--default"&gt;
          

            
   

  &lt;div class="news-article--inline-video"&gt;
        &lt;div class="news-article--inline-video--container"&gt;

                             &lt;div class="news-article--inline-video--cover-image"&gt;
                      &lt;img alt="Video thumbnail" height="480" src="https://i1.ytimg.com/vi/nkYCUsnOC6U/maxresdefault.jpg" width="640" /&gt;
                    
          Play video
        &lt;/div&gt;
                

                      &lt;/div&gt;
          &lt;/div&gt;  
        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Kornbluth added: “Helping America build a future of new manufacturing is a perfect job for MIT — and I’m convinced that there is no more important work we can do to meet the moment and serve the nation now.”&lt;/p&gt;&lt;p&gt;The Initiative for New Manufacturing also announced its first six founding industry consortium members:&amp;nbsp;Amgen, Flex, GE Vernova, PTC, Sanofi, and Siemens. Participants in the INM Industry Consortium will support seed projects proposed by MIT researchers, initially in the area of artificial intelligence for manufacturing.&lt;/p&gt;&lt;p&gt;INM joins the ranks of MIT’s other presidential initiatives — including The Climate Project at MIT; MITHIC, which supports the human-centered disciplines; MIT HEALS, centered on the life sciences and health; and MGAIC, the MIT Generative AI Impact Consortium.&lt;/p&gt;&lt;p&gt;“There is tremendous opportunity to bring together a vibrant community working across every scale — from nanotechnology to large-scale manufacturing — and across a wide-range of applications including semiconductors,&amp;nbsp;medical devices, automotive, energy systems, and biotechnology,”&amp;nbsp;says Anantha Chandrakasan, MIT’s chief innovation and strategy officer and dean of engineering, who is part of the initiative’s leadership team. “MIT is uniquely positioned to harness the transformative power of digital tools and AI to shape future of manufacturing. I’m truly excited about what we can build together and the synergies this creates with other cross-cutting initiatives across the Institute.”&lt;/p&gt;&lt;p&gt;The initiative is just the latest MIT-centered effort in recent decades aiming to expand American manufacturing. A faculty research group wrote the 1989 bestseller “Made in America: Regaining the Productive Edge,” advocating for a renewal of manufacturing; another MIT project, called Production in the Innovation Economy, called for expanded manufacturing in the early 2010s. In 2016, MIT also founded The Engine, a venture fund investing in hardware-based “tough tech” start-ups including many with potential to became substantial manufacturing firms.&lt;/p&gt;&lt;p&gt;As developed, the MIT Initiative for New Manufacturing is based around four major themes:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Reimagining manufacturing technologies and systems: realizing breakthrough technologies and system-level&amp;nbsp;approaches to advance energy production, health care, computing, transportation, consumer products, and more;&lt;/li&gt;&lt;li&gt;Elevating the productivity and experience of manufacturing:&amp;nbsp;developing and deploying new digitally driven methods and tools to amplify productivity and improve the human experience of manufacturing;&lt;/li&gt;&lt;li&gt;Scaling new manufacturing: accelerating the scaling of manufacturing companies and transforming supply chains to maximize efficiency and resilience, fostering product innovation and business growth; and&lt;/li&gt;&lt;li&gt;Transforming the manufacturing base:&amp;nbsp;driving the deployment of a sustainable global manufacturing ecosystem that provides compelling opportunities to workers, with major efforts focused on the U.S.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The&amp;nbsp;initiative has mapped out many concrete activities and programs, which will include an&amp;nbsp;Institute-wide research program on emerging technologies and other major topics; workforce and education programs; and industry engagement and participation. INM also aims to establish new labs for developing manufacturing tools and techniques; a “factory observatory” program&amp;nbsp;which immerses students in manufacturing through visits to production sites; and key “pillars” focusing on areas from semiconductors and biomanufacturing to defense and aviation.&lt;/p&gt;&lt;p&gt;The workforce and education element of INM will include TechAMP, an MIT-created program that works with community colleges to bridge the gap between technicians and engineers; AI-driven teaching tools; professional education; and an effort to expand manufacturing education on campus in collaboration with MIT departments and degree programs.&lt;/p&gt;&lt;p&gt;INM’s leadership team has three faculty co-directors: John Hart, the Class of 1922 Professor and head of the Department of Mechanical Engineering; Suzanne Berger, Institute Professor at MIT and a political scientist who has conducted influential empirical studies of manufacturing; and Chris Love, the Raymond A. and Helen E. St. Laurent Professor of Chemical Engineering. The initiative’s executive director is Julie Diop.&lt;/p&gt;&lt;p&gt;The initiative is in the process of forming a faculty steering committee with representation from across the Institute, as well as an external advisory board. INM stems partly from the work of the Manufacturing@MIT working group, formed in 2022 to assess many of these issues.&lt;/p&gt;&lt;p&gt;The launch of the new initiative was previewed at a daylong MIT symposium on May 7, titled “A Vision for New Manufacturing.” The event, held before a capacity audience in MIT’s Wong Auditorium, featured over 30 speakers from a wide range of manufacturing sectors.&lt;/p&gt;&lt;p&gt;“The rationale for growing and transforming U.S. manufacturing has never been more urgent than it is today,” Berger said at the event. “What we are trying to build at MIT now is not just another research project. … Together, with people in this room and outside this room, we’re trying to change what’s happening in our country.”&lt;/p&gt;&lt;p&gt;“We need to think about the importance of manufacturing again, because it is what brings product ideas to people,” Love told &lt;em&gt;MIT News&lt;/em&gt;. “For instance, in biotechnology, new life-saving medicines can’t reach patients without manufacturing. There is a real urgency about this issue for both economic prosperity and creating jobs. We have seen the impact for our country when we have lost our lead in manufacturing in some sectors. Biotechnology, where the U.S. has been the global leader for more than 40 years, offers the potential to promote new robust economies here, but we need to advance our capabilities in biomanufacturing to maintain our advantage in this area.”&lt;/p&gt;&lt;p&gt;Hart adds: “While manufacturing feels very timely today, it is of enduring importance. Manufactured products enable our daily lives and manufacturing is critical to advancing the frontiers of technology and society.&amp;nbsp;Our efforts leading up to launch of the initiative revealed great excitement about manufacturing across MIT, especially from students. Working with industry — from small to large companies, and from young startups to industrial giants — will be instrumental to creating impact and realizing the vision for new manufacturing.”&lt;/p&gt;&lt;p&gt;In her letter to the MIT community today, Kornbluth stressed that the initiative’s goal is to drive transformation by making manufacturing more productive, resilient, and sustainable.&lt;/p&gt;&lt;p&gt;“We want to reimagine manufacturing technologies and systems to advance fields like energy production, health care, computing, transportation, consumer products, and more,” she wrote. “And we want to reach well beyond the shop floor to tackle challenges like how to make supply chains more resilient, and how to inform public policy to foster a broad, healthy manufacturing ecosystem that can drive decades of innovation and growth.”&lt;/p&gt;&lt;p&gt;&lt;em&gt;Editor’s note: A seventh founding member, Autodesk, was announced on May 30.&lt;/em&gt;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;MIT today launched its Initiative for New Manufacturing (INM), an Institute-wide effort to reinfuse U.S. industrial production with leading-edge technologies, bolster crucial U.S. economic sectors, and ignite job creation.&lt;/p&gt;&lt;p&gt;The initiative will encompass advanced research, innovative education programs, and partnership with companies across many sectors, in a bid to help transform manufacturing and elevate its impact.&lt;/p&gt;&lt;p&gt;“We want to work with firms big and small, in cities, small towns and everywhere in between, to help them adopt new approaches for increased productivity,” MIT President Sally A. Kornbluth wrote in a letter to the Institute community this morning. “We want to deliberately design high-quality, human-centered manufacturing jobs that bring new life to communities across the country.”&lt;/p&gt;        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-youtube-video paragraph--view-mode--default"&gt;
          

            
   

  &lt;div class="news-article--inline-video"&gt;
        &lt;div class="news-article--inline-video--container"&gt;

                             &lt;div class="news-article--inline-video--cover-image"&gt;
                      &lt;img alt="Video thumbnail" height="480" src="https://i1.ytimg.com/vi/nkYCUsnOC6U/maxresdefault.jpg" width="640" /&gt;
                    
          Play video
        &lt;/div&gt;
                

                      &lt;/div&gt;
          &lt;/div&gt;  
        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Kornbluth added: “Helping America build a future of new manufacturing is a perfect job for MIT — and I’m convinced that there is no more important work we can do to meet the moment and serve the nation now.”&lt;/p&gt;&lt;p&gt;The Initiative for New Manufacturing also announced its first six founding industry consortium members:&amp;nbsp;Amgen, Flex, GE Vernova, PTC, Sanofi, and Siemens. Participants in the INM Industry Consortium will support seed projects proposed by MIT researchers, initially in the area of artificial intelligence for manufacturing.&lt;/p&gt;&lt;p&gt;INM joins the ranks of MIT’s other presidential initiatives — including The Climate Project at MIT; MITHIC, which supports the human-centered disciplines; MIT HEALS, centered on the life sciences and health; and MGAIC, the MIT Generative AI Impact Consortium.&lt;/p&gt;&lt;p&gt;“There is tremendous opportunity to bring together a vibrant community working across every scale — from nanotechnology to large-scale manufacturing — and across a wide-range of applications including semiconductors,&amp;nbsp;medical devices, automotive, energy systems, and biotechnology,”&amp;nbsp;says Anantha Chandrakasan, MIT’s chief innovation and strategy officer and dean of engineering, who is part of the initiative’s leadership team. “MIT is uniquely positioned to harness the transformative power of digital tools and AI to shape future of manufacturing. I’m truly excited about what we can build together and the synergies this creates with other cross-cutting initiatives across the Institute.”&lt;/p&gt;&lt;p&gt;The initiative is just the latest MIT-centered effort in recent decades aiming to expand American manufacturing. A faculty research group wrote the 1989 bestseller “Made in America: Regaining the Productive Edge,” advocating for a renewal of manufacturing; another MIT project, called Production in the Innovation Economy, called for expanded manufacturing in the early 2010s. In 2016, MIT also founded The Engine, a venture fund investing in hardware-based “tough tech” start-ups including many with potential to became substantial manufacturing firms.&lt;/p&gt;&lt;p&gt;As developed, the MIT Initiative for New Manufacturing is based around four major themes:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Reimagining manufacturing technologies and systems: realizing breakthrough technologies and system-level&amp;nbsp;approaches to advance energy production, health care, computing, transportation, consumer products, and more;&lt;/li&gt;&lt;li&gt;Elevating the productivity and experience of manufacturing:&amp;nbsp;developing and deploying new digitally driven methods and tools to amplify productivity and improve the human experience of manufacturing;&lt;/li&gt;&lt;li&gt;Scaling new manufacturing: accelerating the scaling of manufacturing companies and transforming supply chains to maximize efficiency and resilience, fostering product innovation and business growth; and&lt;/li&gt;&lt;li&gt;Transforming the manufacturing base:&amp;nbsp;driving the deployment of a sustainable global manufacturing ecosystem that provides compelling opportunities to workers, with major efforts focused on the U.S.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The&amp;nbsp;initiative has mapped out many concrete activities and programs, which will include an&amp;nbsp;Institute-wide research program on emerging technologies and other major topics; workforce and education programs; and industry engagement and participation. INM also aims to establish new labs for developing manufacturing tools and techniques; a “factory observatory” program&amp;nbsp;which immerses students in manufacturing through visits to production sites; and key “pillars” focusing on areas from semiconductors and biomanufacturing to defense and aviation.&lt;/p&gt;&lt;p&gt;The workforce and education element of INM will include TechAMP, an MIT-created program that works with community colleges to bridge the gap between technicians and engineers; AI-driven teaching tools; professional education; and an effort to expand manufacturing education on campus in collaboration with MIT departments and degree programs.&lt;/p&gt;&lt;p&gt;INM’s leadership team has three faculty co-directors: John Hart, the Class of 1922 Professor and head of the Department of Mechanical Engineering; Suzanne Berger, Institute Professor at MIT and a political scientist who has conducted influential empirical studies of manufacturing; and Chris Love, the Raymond A. and Helen E. St. Laurent Professor of Chemical Engineering. The initiative’s executive director is Julie Diop.&lt;/p&gt;&lt;p&gt;The initiative is in the process of forming a faculty steering committee with representation from across the Institute, as well as an external advisory board. INM stems partly from the work of the Manufacturing@MIT working group, formed in 2022 to assess many of these issues.&lt;/p&gt;&lt;p&gt;The launch of the new initiative was previewed at a daylong MIT symposium on May 7, titled “A Vision for New Manufacturing.” The event, held before a capacity audience in MIT’s Wong Auditorium, featured over 30 speakers from a wide range of manufacturing sectors.&lt;/p&gt;&lt;p&gt;“The rationale for growing and transforming U.S. manufacturing has never been more urgent than it is today,” Berger said at the event. “What we are trying to build at MIT now is not just another research project. … Together, with people in this room and outside this room, we’re trying to change what’s happening in our country.”&lt;/p&gt;&lt;p&gt;“We need to think about the importance of manufacturing again, because it is what brings product ideas to people,” Love told &lt;em&gt;MIT News&lt;/em&gt;. “For instance, in biotechnology, new life-saving medicines can’t reach patients without manufacturing. There is a real urgency about this issue for both economic prosperity and creating jobs. We have seen the impact for our country when we have lost our lead in manufacturing in some sectors. Biotechnology, where the U.S. has been the global leader for more than 40 years, offers the potential to promote new robust economies here, but we need to advance our capabilities in biomanufacturing to maintain our advantage in this area.”&lt;/p&gt;&lt;p&gt;Hart adds: “While manufacturing feels very timely today, it is of enduring importance. Manufactured products enable our daily lives and manufacturing is critical to advancing the frontiers of technology and society.&amp;nbsp;Our efforts leading up to launch of the initiative revealed great excitement about manufacturing across MIT, especially from students. Working with industry — from small to large companies, and from young startups to industrial giants — will be instrumental to creating impact and realizing the vision for new manufacturing.”&lt;/p&gt;&lt;p&gt;In her letter to the MIT community today, Kornbluth stressed that the initiative’s goal is to drive transformation by making manufacturing more productive, resilient, and sustainable.&lt;/p&gt;&lt;p&gt;“We want to reimagine manufacturing technologies and systems to advance fields like energy production, health care, computing, transportation, consumer products, and more,” she wrote. “And we want to reach well beyond the shop floor to tackle challenges like how to make supply chains more resilient, and how to inform public policy to foster a broad, healthy manufacturing ecosystem that can drive decades of innovation and growth.”&lt;/p&gt;&lt;p&gt;&lt;em&gt;Editor’s note: A seventh founding member, Autodesk, was announced on May 30.&lt;/em&gt;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/mit-announces-initiative-for-new-manufacturing-0527</guid><pubDate>Tue, 27 May 2025 14:00:00 +0000</pubDate></item><item><title>FrodoKEM: A conservative quantum-safe cryptographic algorithm (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/frodokem-a-conservative-quantum-safe-cryptographic-algorithm/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="The image features a gradient background transitioning from blue on the left to pink on the right. In the center, there are three white icons. On the left is a microchip icon that represents quantum computing, in the middle is a shield, and on the right is another microchip with a padlock symbol inside it." class="wp-image-1140055" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/FrodoKEM-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;In this post, we describe FrodoKEM, a key encapsulation protocol that offers a simple design and provides strong security guarantees even in a future with powerful quantum computers.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="the-quantum-threat-to-cryptography"&gt;The quantum threat to cryptography&lt;/h2&gt;



&lt;p&gt;For decades, modern cryptography has relied on mathematical problems that are practically impossible for classical computers to solve without a secret key. Cryptosystems like RSA, Diffie-Hellman key-exchange, and elliptic curve-based schemes—which rely on the hardness of the integer factorization and (elliptic curve) discrete logarithm problems—secure communications on the internet, banking transactions, and even national security systems. However, the emergence of&lt;strong&gt; &lt;/strong&gt;quantum computing poses a significant threat to these cryptographic schemes.&lt;/p&gt;



&lt;p&gt;Quantum computers leverage the principles of quantum mechanics to perform certain calculations exponentially faster than classical computers. Their ability to solve complex problems, such as simulating molecular interactions, optimizing large-scale systems, and accelerating machine learning, is expected to have profound and beneficial implications for fields ranging from chemistry and material science to artificial intelligence.&lt;/p&gt;




	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;Microsoft research podcast&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Ideas: AI and democracy with Madeleine Daepp and Robert Osazuwa Ness&lt;/h2&gt;
				
								&lt;p class="large"&gt;As the “biggest election year in history” comes to an end, researchers Madeleine Daepp and Robert Osazuwa Ness and Democracy Forward GM Ginny Badanes discuss AI’s impact on democracy, including the tech’s use in Taiwan and India.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	&lt;p&gt;At the same time, quantum computing is poised to disrupt cryptography. In particular, Shor’s algorithm, a quantum algorithm developed in 1994, can efficiently factor large numbers and compute discrete logarithms—the very problems that underpin the security of RSA, Diffie-Hellman, and elliptic curve cryptography. This means that once large-scale, fault-tolerant quantum computers become available, public-key protocols based on RSA, ECC, and Diffie-Hellman will become insecure, breaking a sizable portion of the cryptographic backbone of today’s digital world. Recent advances in quantum computing, such as Microsoft’s Majorana 1&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, the first quantum processor powered by topological qubits, represent major steps toward practical quantum computing and underscore the urgency of transitioning to quantum-resistant cryptographic systems.&lt;/p&gt;



&lt;p&gt;To address this looming security crisis, cryptographers and government agencies have been working on post-quantum cryptography (PQC)—new cryptographic algorithms that can resist attacks from both classical and quantum computers.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="the-nist-post-quantum-cryptography-standardization-effort"&gt;The NIST Post-Quantum Cryptography Standardization effort&lt;/h2&gt;



&lt;p&gt;In 2017, the U.S. National Institute of Standards and Technology (NIST) launched the &lt;strong&gt;Post-Quantum Cryptography Standardization project&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/strong&gt; to evaluate and select cryptographic algorithms capable of withstanding quantum attacks. As part of this initiative, NIST sought proposals for two types of cryptographic primitives: key encapsulation mechanisms (KEMs)—which enable two parties to securely derive a shared key to establish an encrypted connection, similar to traditional key exchange schemes—and digital signature schemes.&lt;/p&gt;



&lt;p&gt;This initiative attracted submissions from cryptographers worldwide, and after multiple evaluation rounds, NIST selected CRYSTALS-Kyber, a KEM based on structured lattices, and standardized it as ML-KEM&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. Additionally, NIST selected three digital signature schemes: CRYSTALS-Dilithium, now called ML-DSA; SPHINCS&lt;sup&gt;+&lt;/sup&gt;, now called SLH-DSA; and Falcon, now called FN-DSA.&lt;/p&gt;



&lt;p&gt;While ML-KEM provides great overall security and efficiency, some governments and cryptographic researchers advocate for the inclusion and standardization of alternative algorithms that minimize reliance on algebraic structure. Reducing algebraic structure might prevent potential vulnerabilities and, hence, can be considered a more conservative design choice. One such algorithm is &lt;strong&gt;FrodoKEM&lt;/strong&gt;.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="international-standardization-of-post-quantum-cryptography"&gt;International standardization of post-quantum cryptography&lt;/h2&gt;



&lt;p&gt;Beyond NIST, other international standardization bodies have been actively working on quantum-resistant cryptographic solutions. The &lt;strong&gt;International Organization for Standardization (ISO)&lt;/strong&gt; is leading a global effort to standardize additional PQC algorithms. Notably, European government agencies—including Germany’s BSI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, the Netherlands’ NLNCSA and AIVD&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, and France’s ANSSI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;—have shown strong support for FrodoKEM, recognizing it as a conservative alternative to structured lattice-based schemes.&lt;/p&gt;



&lt;p&gt;As a result,&lt;strong&gt; FrodoKEM is undergoing standardization at ISO.&lt;/strong&gt; Additionally, ISO is standardizing ML-KEM and a conservative code-based KEM called Classic McEliece. These three algorithms are planned for inclusion in ISO/IEC 18033-2:2006 as Amendment 2&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="what-is-frodokem"&gt;What is FrodoKEM?&lt;/h2&gt;



&lt;p&gt;FrodoKEM is a key encapsulation mechanism (KEM) based on the &lt;strong&gt;Learning with Errors (LWE) problem&lt;/strong&gt;, a cornerstone of lattice-based cryptography. Unlike structured lattice-based schemes such as ML-KEM, FrodoKEM is built on generic, unstructured lattices, i.e., it is based on the &lt;em&gt;plain &lt;/em&gt;LWE problem.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="why-unstructured-lattices"&gt;Why unstructured lattices?&lt;/h3&gt;



&lt;p&gt;Structured lattice-based schemes introduce additional algebraic properties that could potentially be exploited in future cryptanalytic attacks. By using unstructured lattices, FrodoKEM eliminates these concerns, making it a safer choice in the long run, albeit at the cost of larger key sizes and lower efficiency.&lt;/p&gt;



&lt;p&gt;It is important to emphasize that no particular cryptanalytic weaknesses are currently known for recommended parameterizations of structured lattice schemes in comparison to plain LWE. However, our current understanding of the security of these schemes could potentially change in the future with cryptanalytic advances.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="lattices-and-the-learning-with-errors-lwe-problem"&gt;Lattices and the Learning with Errors (LWE) problem&lt;/h2&gt;



&lt;p&gt;Lattice-based cryptography relies on the mathematical structure of lattices, which are regular arrangements of points in multidimensional space. A lattice is defined as the set of all integer linear combinations of a set of basis vectors. The difficulty of certain computational problems on lattices, such as the Shortest Vector Problem (SVP) and the Learning with Errors (LWE) problem, forms the basis of lattice-based schemes.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="the-learning-with-errors-lwe-problem"&gt;The Learning with Errors (LWE) problem&lt;/h3&gt;



&lt;p&gt;The LWE problem is a fundamental hard problem in lattice-based cryptography. It involves solving a system of linear equations where some small random error has been added to each equation, making it extremely difficult to recover the original secret values. This added error ensures that the problem remains computationally infeasible, even for quantum computers. Figure 1 below illustrates the LWE problem, specifically, the search version of the problem.&lt;/p&gt;



&lt;p&gt;As can be seen in Figure 1, for the setup of the problem we need a dimension \(n\) that defines the size of matrices, a modulus \(q\) that defines the value range of the matrix coefficients, and a certain error distribution \(\chi\) from which we sample \(\textit{“small”}\) matrices. We sample two matrices from \(\chi\), a small matrix \(\text{s}\) and an error matrix \(\text{e}\) (for simplicity in the explanation, we assume that both have only one column); sample an \(n \times n\) matrix \(\text{A}\) uniformly at random; and compute \(\text{b} = \text{A} \times \text{s} + \text{e}\). In the illustration, each matrix coefficient is represented by a colored square, and the “legend of coefficients” gives an idea of the size of the respective coefficients, e.g., orange squares represent the small coefficients of matrix \(\text{s}\)&amp;nbsp;(small relative to the modulus \(q\)). Finally, given \(\text{A}\) and \(\text{b}\), the search LWE problem consists in finding \(\text{s}\). This problem is believed to be hard for suitably chosen parameters (e.g., for dimension \(n\) sufficiently large) and is used at the core of FrodoKEM.&lt;/p&gt;



&lt;p&gt;In comparison, the LWE variant used in ML-KEM—called Module-LWE (M-LWE)—has additional symmetries, adding mathematical structure that helps improve efficiency. In a setting similar to that of the search LWE problem above, the matrix \(\text{A}\) can be represented by just a single row of coefficients.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Visualization of the (search) LWE problem." class="wp-image-1140135" height="965" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Figure-1.jpg" width="1810" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;strong&gt;FIGURE 1:&lt;/strong&gt; Visualization of the (search) LWE problem.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;LWE is conjectured to be quantum-resistant, and FrodoKEM’s security is directly tied to its hardness. In other words, cryptanalysts and quantum researchers have not been able to devise an efficient quantum algorithm capable of solving the LWE problem and, hence, FrodoKEM. In cryptography, absolute security can never be guaranteed; instead, confidence in a problem’s hardness comes from extensive scrutiny and its resilience against attacks over time.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="how-frodokem-works"&gt;How FrodoKEM Works&lt;/h2&gt;



&lt;p&gt;FrodoKEM follows the standard paradigm of a KEM, which consists of three main operations—key generation, encapsulation, and decapsulation—performed interactively between a sender and a recipient with the goal of establishing a shared secret key:&lt;/p&gt;



&lt;ol class="wp-block-list" start="1"&gt;
&lt;li&gt;&lt;strong&gt;Key generation (KeyGen), computed by the recipient&lt;/strong&gt;
&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Generates a public key and a secret key.&lt;/li&gt;



&lt;li&gt;The public key is sent to the sender, while the private key remains secret.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Encapsulation (Encapsulate), computed by the sender&lt;/strong&gt;
&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Generates a random session key.&lt;/li&gt;



&lt;li&gt;Encrypts the session key using the recipient’s public key to produce a ciphertext.&lt;/li&gt;



&lt;li&gt;Produces a shared key using the session key and the ciphertext.&lt;/li&gt;



&lt;li&gt;The ciphertext is sent to the recipient.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Decapsulation (Decapsulate), computed by the recipient&lt;/strong&gt;
&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Decrypts the ciphertext using their secret key to recover the original session key.&lt;/li&gt;



&lt;li&gt;Reproduces the shared key using the decrypted session key and the ciphertext.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;The shared key generated by the sender and reconstructed by the recipient can then be used to establish secure symmetric-key encryption for further communication between the two parties.&lt;/p&gt;



&lt;p&gt;Figure 2 below shows a simplified view of the FrodoKEM protocol. As highlighted in red, FrodoKEM uses at its core LWE operations of the form “\(\text{b} = \text{A} \times \text{s} + \text{e}\)”, which are directly applied within the KEM paradigm.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Simplified overview of FrodoKEM." class="wp-image-1140137" height="1055" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Figure-2.jpg" width="1845" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;strong&gt;FIGURE 2:&lt;/strong&gt; Simplified overview of FrodoKEM.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="performance-strong-security-has-a-cost"&gt;Performance: Strong security has a cost&lt;/h2&gt;



&lt;p&gt;Not relying on additional algebraic structure certainly comes at a cost for FrodoKEM in the form of increased protocol runtime and bandwidth. The table below compares the performance and key sizes corresponding to the FrodoKEM level 1 parameter set (variant called “FrodoKEM-640-AES”) and the respective parameter set of ML-KEM (variant called “ML-KEM-512”). These parameter sets are intended to match or exceed the brute force security of AES-128. As can be seen, the difference in speed and key sizes between FrodoKEM and ML-KEM is more than an order of magnitude. Nevertheless, the runtime of the FrodoKEM protocol remains reasonable for most applications. For example, on our benchmarking platform clocked at 3.2GHz, the measured runtimes are 0.97 ms, 1.9 ms, and 3.2 ms for security levels 1, 2, and 3, respectively.&lt;/p&gt;



&lt;p&gt;For security-sensitive applications, a more relevant comparison is with Classic McEliece, a post-quantum code-based scheme also considered for standardization. In this case, FrodoKEM offers several efficiency advantages. Classic McEliece’s public keys are significantly larger—well over an order of magnitude greater than FrodoKEM’s—and its key generation is substantially more computationally expensive. Nonetheless, Classic McEliece provides an advantage in certain static key-exchange scenarios, where its high key generation cost can be amortized across multiple key encapsulation executions.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Comparison of key sizes and performance on an x86-64 processor for NIST level 1 parameter sets." class="wp-image-1140140" height="521" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Figure-3.jpg" width="2236" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;strong&gt;TABLE 1:&lt;/strong&gt; Comparison of key sizes and performance on an x86-64 processor for NIST level 1 parameter sets.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="a-holistic-design-made-with-security-in-mind"&gt;A holistic design made with security in mind&lt;/h2&gt;



&lt;p&gt;FrodoKEM’s design principles support security beyond its reliance on generic, unstructured lattices to minimize the attack surface of potential future cryptanalytic threats. Its parameters have been carefully chosen with additional security margins to withstand advancements in known attacks. Furthermore, FrodoKEM is designed with simplicity in mind—its internal operations are based on straightforward matrix-vector arithmetic using integer coefficients reduced modulo a power of two. These design decisions facilitate simple, compact and secure implementations that are also easier to maintain and to protect against side-channel attacks.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="conclusion"&gt;Conclusion&lt;/h2&gt;



&lt;p&gt;After years of research and analysis, the next generation of post-quantum cryptographic algorithms has arrived. NIST has chosen strong PQC protocols that we believe will serve Microsoft and its customers well in many applications. For security-sensitive applications, FrodoKEM offers a secure yet practical approach for post-quantum cryptography. While its reliance on unstructured lattices results in larger key sizes and higher computational overhead compared to structured lattice-based alternatives, it provides strong security assurances against potential future attacks. Given the ongoing standardization efforts and its endorsement by multiple governmental agencies, FrodoKEM is well-positioned as a viable alternative for organizations seeking long-term cryptographic resilience in a post-quantum world.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="further-reading"&gt;Further Reading&lt;/h2&gt;



&lt;p&gt;For those interested in learning more about FrodoKEM, post-quantum cryptography, and lattice-based cryptography, the following resources provide valuable insights:&lt;/p&gt;




&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="The image features a gradient background transitioning from blue on the left to pink on the right. In the center, there are three white icons. On the left is a microchip icon that represents quantum computing, in the middle is a shield, and on the right is another microchip with a padlock symbol inside it." class="wp-image-1140055" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/FrodoKEM-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;In this post, we describe FrodoKEM, a key encapsulation protocol that offers a simple design and provides strong security guarantees even in a future with powerful quantum computers.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="the-quantum-threat-to-cryptography"&gt;The quantum threat to cryptography&lt;/h2&gt;



&lt;p&gt;For decades, modern cryptography has relied on mathematical problems that are practically impossible for classical computers to solve without a secret key. Cryptosystems like RSA, Diffie-Hellman key-exchange, and elliptic curve-based schemes—which rely on the hardness of the integer factorization and (elliptic curve) discrete logarithm problems—secure communications on the internet, banking transactions, and even national security systems. However, the emergence of&lt;strong&gt; &lt;/strong&gt;quantum computing poses a significant threat to these cryptographic schemes.&lt;/p&gt;



&lt;p&gt;Quantum computers leverage the principles of quantum mechanics to perform certain calculations exponentially faster than classical computers. Their ability to solve complex problems, such as simulating molecular interactions, optimizing large-scale systems, and accelerating machine learning, is expected to have profound and beneficial implications for fields ranging from chemistry and material science to artificial intelligence.&lt;/p&gt;




	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;Microsoft research podcast&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Ideas: AI and democracy with Madeleine Daepp and Robert Osazuwa Ness&lt;/h2&gt;
				
								&lt;p class="large"&gt;As the “biggest election year in history” comes to an end, researchers Madeleine Daepp and Robert Osazuwa Ness and Democracy Forward GM Ginny Badanes discuss AI’s impact on democracy, including the tech’s use in Taiwan and India.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	&lt;p&gt;At the same time, quantum computing is poised to disrupt cryptography. In particular, Shor’s algorithm, a quantum algorithm developed in 1994, can efficiently factor large numbers and compute discrete logarithms—the very problems that underpin the security of RSA, Diffie-Hellman, and elliptic curve cryptography. This means that once large-scale, fault-tolerant quantum computers become available, public-key protocols based on RSA, ECC, and Diffie-Hellman will become insecure, breaking a sizable portion of the cryptographic backbone of today’s digital world. Recent advances in quantum computing, such as Microsoft’s Majorana 1&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, the first quantum processor powered by topological qubits, represent major steps toward practical quantum computing and underscore the urgency of transitioning to quantum-resistant cryptographic systems.&lt;/p&gt;



&lt;p&gt;To address this looming security crisis, cryptographers and government agencies have been working on post-quantum cryptography (PQC)—new cryptographic algorithms that can resist attacks from both classical and quantum computers.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="the-nist-post-quantum-cryptography-standardization-effort"&gt;The NIST Post-Quantum Cryptography Standardization effort&lt;/h2&gt;



&lt;p&gt;In 2017, the U.S. National Institute of Standards and Technology (NIST) launched the &lt;strong&gt;Post-Quantum Cryptography Standardization project&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/strong&gt; to evaluate and select cryptographic algorithms capable of withstanding quantum attacks. As part of this initiative, NIST sought proposals for two types of cryptographic primitives: key encapsulation mechanisms (KEMs)—which enable two parties to securely derive a shared key to establish an encrypted connection, similar to traditional key exchange schemes—and digital signature schemes.&lt;/p&gt;



&lt;p&gt;This initiative attracted submissions from cryptographers worldwide, and after multiple evaluation rounds, NIST selected CRYSTALS-Kyber, a KEM based on structured lattices, and standardized it as ML-KEM&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. Additionally, NIST selected three digital signature schemes: CRYSTALS-Dilithium, now called ML-DSA; SPHINCS&lt;sup&gt;+&lt;/sup&gt;, now called SLH-DSA; and Falcon, now called FN-DSA.&lt;/p&gt;



&lt;p&gt;While ML-KEM provides great overall security and efficiency, some governments and cryptographic researchers advocate for the inclusion and standardization of alternative algorithms that minimize reliance on algebraic structure. Reducing algebraic structure might prevent potential vulnerabilities and, hence, can be considered a more conservative design choice. One such algorithm is &lt;strong&gt;FrodoKEM&lt;/strong&gt;.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="international-standardization-of-post-quantum-cryptography"&gt;International standardization of post-quantum cryptography&lt;/h2&gt;



&lt;p&gt;Beyond NIST, other international standardization bodies have been actively working on quantum-resistant cryptographic solutions. The &lt;strong&gt;International Organization for Standardization (ISO)&lt;/strong&gt; is leading a global effort to standardize additional PQC algorithms. Notably, European government agencies—including Germany’s BSI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, the Netherlands’ NLNCSA and AIVD&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, and France’s ANSSI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;—have shown strong support for FrodoKEM, recognizing it as a conservative alternative to structured lattice-based schemes.&lt;/p&gt;



&lt;p&gt;As a result,&lt;strong&gt; FrodoKEM is undergoing standardization at ISO.&lt;/strong&gt; Additionally, ISO is standardizing ML-KEM and a conservative code-based KEM called Classic McEliece. These three algorithms are planned for inclusion in ISO/IEC 18033-2:2006 as Amendment 2&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="what-is-frodokem"&gt;What is FrodoKEM?&lt;/h2&gt;



&lt;p&gt;FrodoKEM is a key encapsulation mechanism (KEM) based on the &lt;strong&gt;Learning with Errors (LWE) problem&lt;/strong&gt;, a cornerstone of lattice-based cryptography. Unlike structured lattice-based schemes such as ML-KEM, FrodoKEM is built on generic, unstructured lattices, i.e., it is based on the &lt;em&gt;plain &lt;/em&gt;LWE problem.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="why-unstructured-lattices"&gt;Why unstructured lattices?&lt;/h3&gt;



&lt;p&gt;Structured lattice-based schemes introduce additional algebraic properties that could potentially be exploited in future cryptanalytic attacks. By using unstructured lattices, FrodoKEM eliminates these concerns, making it a safer choice in the long run, albeit at the cost of larger key sizes and lower efficiency.&lt;/p&gt;



&lt;p&gt;It is important to emphasize that no particular cryptanalytic weaknesses are currently known for recommended parameterizations of structured lattice schemes in comparison to plain LWE. However, our current understanding of the security of these schemes could potentially change in the future with cryptanalytic advances.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="lattices-and-the-learning-with-errors-lwe-problem"&gt;Lattices and the Learning with Errors (LWE) problem&lt;/h2&gt;



&lt;p&gt;Lattice-based cryptography relies on the mathematical structure of lattices, which are regular arrangements of points in multidimensional space. A lattice is defined as the set of all integer linear combinations of a set of basis vectors. The difficulty of certain computational problems on lattices, such as the Shortest Vector Problem (SVP) and the Learning with Errors (LWE) problem, forms the basis of lattice-based schemes.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="the-learning-with-errors-lwe-problem"&gt;The Learning with Errors (LWE) problem&lt;/h3&gt;



&lt;p&gt;The LWE problem is a fundamental hard problem in lattice-based cryptography. It involves solving a system of linear equations where some small random error has been added to each equation, making it extremely difficult to recover the original secret values. This added error ensures that the problem remains computationally infeasible, even for quantum computers. Figure 1 below illustrates the LWE problem, specifically, the search version of the problem.&lt;/p&gt;



&lt;p&gt;As can be seen in Figure 1, for the setup of the problem we need a dimension \(n\) that defines the size of matrices, a modulus \(q\) that defines the value range of the matrix coefficients, and a certain error distribution \(\chi\) from which we sample \(\textit{“small”}\) matrices. We sample two matrices from \(\chi\), a small matrix \(\text{s}\) and an error matrix \(\text{e}\) (for simplicity in the explanation, we assume that both have only one column); sample an \(n \times n\) matrix \(\text{A}\) uniformly at random; and compute \(\text{b} = \text{A} \times \text{s} + \text{e}\). In the illustration, each matrix coefficient is represented by a colored square, and the “legend of coefficients” gives an idea of the size of the respective coefficients, e.g., orange squares represent the small coefficients of matrix \(\text{s}\)&amp;nbsp;(small relative to the modulus \(q\)). Finally, given \(\text{A}\) and \(\text{b}\), the search LWE problem consists in finding \(\text{s}\). This problem is believed to be hard for suitably chosen parameters (e.g., for dimension \(n\) sufficiently large) and is used at the core of FrodoKEM.&lt;/p&gt;



&lt;p&gt;In comparison, the LWE variant used in ML-KEM—called Module-LWE (M-LWE)—has additional symmetries, adding mathematical structure that helps improve efficiency. In a setting similar to that of the search LWE problem above, the matrix \(\text{A}\) can be represented by just a single row of coefficients.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Visualization of the (search) LWE problem." class="wp-image-1140135" height="965" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Figure-1.jpg" width="1810" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;strong&gt;FIGURE 1:&lt;/strong&gt; Visualization of the (search) LWE problem.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;LWE is conjectured to be quantum-resistant, and FrodoKEM’s security is directly tied to its hardness. In other words, cryptanalysts and quantum researchers have not been able to devise an efficient quantum algorithm capable of solving the LWE problem and, hence, FrodoKEM. In cryptography, absolute security can never be guaranteed; instead, confidence in a problem’s hardness comes from extensive scrutiny and its resilience against attacks over time.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="how-frodokem-works"&gt;How FrodoKEM Works&lt;/h2&gt;



&lt;p&gt;FrodoKEM follows the standard paradigm of a KEM, which consists of three main operations—key generation, encapsulation, and decapsulation—performed interactively between a sender and a recipient with the goal of establishing a shared secret key:&lt;/p&gt;



&lt;ol class="wp-block-list" start="1"&gt;
&lt;li&gt;&lt;strong&gt;Key generation (KeyGen), computed by the recipient&lt;/strong&gt;
&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Generates a public key and a secret key.&lt;/li&gt;



&lt;li&gt;The public key is sent to the sender, while the private key remains secret.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Encapsulation (Encapsulate), computed by the sender&lt;/strong&gt;
&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Generates a random session key.&lt;/li&gt;



&lt;li&gt;Encrypts the session key using the recipient’s public key to produce a ciphertext.&lt;/li&gt;



&lt;li&gt;Produces a shared key using the session key and the ciphertext.&lt;/li&gt;



&lt;li&gt;The ciphertext is sent to the recipient.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Decapsulation (Decapsulate), computed by the recipient&lt;/strong&gt;
&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Decrypts the ciphertext using their secret key to recover the original session key.&lt;/li&gt;



&lt;li&gt;Reproduces the shared key using the decrypted session key and the ciphertext.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;The shared key generated by the sender and reconstructed by the recipient can then be used to establish secure symmetric-key encryption for further communication between the two parties.&lt;/p&gt;



&lt;p&gt;Figure 2 below shows a simplified view of the FrodoKEM protocol. As highlighted in red, FrodoKEM uses at its core LWE operations of the form “\(\text{b} = \text{A} \times \text{s} + \text{e}\)”, which are directly applied within the KEM paradigm.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Simplified overview of FrodoKEM." class="wp-image-1140137" height="1055" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Figure-2.jpg" width="1845" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;strong&gt;FIGURE 2:&lt;/strong&gt; Simplified overview of FrodoKEM.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="performance-strong-security-has-a-cost"&gt;Performance: Strong security has a cost&lt;/h2&gt;



&lt;p&gt;Not relying on additional algebraic structure certainly comes at a cost for FrodoKEM in the form of increased protocol runtime and bandwidth. The table below compares the performance and key sizes corresponding to the FrodoKEM level 1 parameter set (variant called “FrodoKEM-640-AES”) and the respective parameter set of ML-KEM (variant called “ML-KEM-512”). These parameter sets are intended to match or exceed the brute force security of AES-128. As can be seen, the difference in speed and key sizes between FrodoKEM and ML-KEM is more than an order of magnitude. Nevertheless, the runtime of the FrodoKEM protocol remains reasonable for most applications. For example, on our benchmarking platform clocked at 3.2GHz, the measured runtimes are 0.97 ms, 1.9 ms, and 3.2 ms for security levels 1, 2, and 3, respectively.&lt;/p&gt;



&lt;p&gt;For security-sensitive applications, a more relevant comparison is with Classic McEliece, a post-quantum code-based scheme also considered for standardization. In this case, FrodoKEM offers several efficiency advantages. Classic McEliece’s public keys are significantly larger—well over an order of magnitude greater than FrodoKEM’s—and its key generation is substantially more computationally expensive. Nonetheless, Classic McEliece provides an advantage in certain static key-exchange scenarios, where its high key generation cost can be amortized across multiple key encapsulation executions.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Comparison of key sizes and performance on an x86-64 processor for NIST level 1 parameter sets." class="wp-image-1140140" height="521" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Figure-3.jpg" width="2236" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;strong&gt;TABLE 1:&lt;/strong&gt; Comparison of key sizes and performance on an x86-64 processor for NIST level 1 parameter sets.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="a-holistic-design-made-with-security-in-mind"&gt;A holistic design made with security in mind&lt;/h2&gt;



&lt;p&gt;FrodoKEM’s design principles support security beyond its reliance on generic, unstructured lattices to minimize the attack surface of potential future cryptanalytic threats. Its parameters have been carefully chosen with additional security margins to withstand advancements in known attacks. Furthermore, FrodoKEM is designed with simplicity in mind—its internal operations are based on straightforward matrix-vector arithmetic using integer coefficients reduced modulo a power of two. These design decisions facilitate simple, compact and secure implementations that are also easier to maintain and to protect against side-channel attacks.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="conclusion"&gt;Conclusion&lt;/h2&gt;



&lt;p&gt;After years of research and analysis, the next generation of post-quantum cryptographic algorithms has arrived. NIST has chosen strong PQC protocols that we believe will serve Microsoft and its customers well in many applications. For security-sensitive applications, FrodoKEM offers a secure yet practical approach for post-quantum cryptography. While its reliance on unstructured lattices results in larger key sizes and higher computational overhead compared to structured lattice-based alternatives, it provides strong security assurances against potential future attacks. Given the ongoing standardization efforts and its endorsement by multiple governmental agencies, FrodoKEM is well-positioned as a viable alternative for organizations seeking long-term cryptographic resilience in a post-quantum world.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="further-reading"&gt;Further Reading&lt;/h2&gt;



&lt;p&gt;For those interested in learning more about FrodoKEM, post-quantum cryptography, and lattice-based cryptography, the following resources provide valuable insights:&lt;/p&gt;




&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/frodokem-a-conservative-quantum-safe-cryptographic-algorithm/</guid><pubDate>Tue, 27 May 2025 16:00:00 +0000</pubDate></item><item><title>Building networks of data science talent (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/building-networks-data-science-talent-0527</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202504/mit-breit-idss-killian.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;The rise of artificial intelligence resurfaces a question older than the abacus: If we have a tool to do it for us, why learn to do it ourselves?&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;The answer, argues MIT electrical engineering and computer science (EECS) Professor Devavrat Shah, hasn’t changed: Foundational skills in mathematics remain essential to using tools well, from knowing which tool to use to interpreting results correctly.&lt;/p&gt;&lt;p dir="ltr"&gt;“As large language models and generative AI meet new applications, these cutting-edge tools will continue to reshape entire sectors of industry, and bring new insights to challenges in research and policy,” argues Shah. “The world needs people who can grasp the underlying concepts behind AI to truly leverage its potential.”&lt;/p&gt;&lt;p dir="ltr"&gt;Shah is a professor in MIT’s&amp;nbsp;Institute for Data, Systems, and Society (IDSS), a cross-disciplinary unit meeting the global need for data skills with online course offerings like the&amp;nbsp;MicroMasters Program in Statistics and Data Science, which Shah directs.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“With over a thousand credential holders worldwide, and tens of thousands more learners engaged since its inception, the MicroMasters Program in Statistics and Data Science has proven to be a rigorous but flexible way for skilled learners to develop an MIT-level grasp of statistics fundamentals,” says Shah.&lt;/p&gt;&lt;p dir="ltr"&gt;The MicroMasters also forms the backbone of IDSS education partnerships, where an embedded MIT team collaborates with organizations to support groups of learners through the MicroMasters curriculum.&lt;/p&gt;&lt;p dir="ltr"&gt;“Together with our first strategic partner in education, IDSS is providing graduate-level data science education through the Brescia Institute of Technology (BREIT) in Peru,” explains Fotini Christia, the Ford International Professor of the Social Sciences at MIT and director of IDSS. “Through this partnership, IDSS is training data scientists who are informing decision-making in Peruvian industry, society, and policy.”&lt;/p&gt;        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-youtube-video paragraph--view-mode--default"&gt;
          

            
   

  &lt;div class="news-article--inline-video"&gt;
        
        &lt;div class="news-article--inline-video--caption"&gt;
      

            Building networks of data science and machine learning talent: MIT IDSS and BREIT&lt;br /&gt;Video: MIT IDSS        

    &lt;/div&gt;
          &lt;/div&gt;  
        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;&lt;strong&gt;Training the next generation&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;BREIT’s Advanced Program in Data Science and Global Skills, developed in collaboration with IDSS, provides training in both the technical and nontechnical skills needed to take advantage of the insights that data can offer. Members complete the MicroMasters in Statistics and Data Science (SDS), learning the foundations of statistics, probability, data analysis, and machine learning. Meanwhile, these learners are equipped with career skills from communication and critical thinking to team-building and ethics.&lt;/p&gt;&lt;p dir="ltr"&gt;“I knew that artificial intelligence, machine learning, and data science was the future, and I wanted to be in that wave,” explains BREIT learner Renato Castro about his decision to join the program. Now a credential holder, Castro has developed data projects for groups in Peru, Panama, and Guatemala. “The program teaches more than the mathematics. It’s a systematic way of thinking that helps you have an impact on real-world problems and create wealth not only for a company, but wealth for the people.”&lt;/p&gt;&lt;p dir="ltr"&gt;“The aim is to develop problem-solvers and leaders in a field that is growing and becoming more relevant for organizations around the world,” says Lucia Haro, manager of BREIT. “We are training the next generation to contribute to the economic development of our country, and to have a positive social impact in Peru.”&lt;/p&gt;&lt;p dir="ltr"&gt;To help accomplish this, IDSS provides BREIT learners with tailored support. MIT grad student teaching assistants lead regular sessions to provide hands-on practice with class concepts, answer learner questions, and identify topics for developing additional resources.&lt;/p&gt;&lt;p dir="ltr"&gt;“These sessions were very useful because you see the application of the theoretical part from the lectures,” says Jesús Figueroa, who completed the program and now serves as a local teaching assistant. Learners like Figueroa must go beyond a deep understanding of the course material in order to support future learners.&lt;/p&gt;&lt;p dir="ltr"&gt;“Maybe you already understand the fundamentals, the theoretical part,” explains Figueroa, “but you have to learn how to communicate it.”&lt;/p&gt;&lt;p dir="ltr"&gt;Eight cohorts have completed the program, with three more in progress, for a total of almost 100 holders of the MicroMasters credential — and 90 more in the pipeline. As BREIT has scaled up their operation, the IDSS team worked to meet new needs as they emerged, such as collaborating in the development of a technical assessment to support learner recruitment.&lt;/p&gt;&lt;p dir="ltr"&gt;“The assessment tool gauges applicants’ familiarity with prerequisite knowledge like calculus, elementary linear algebra, and basic programming in Python,” says Karene Chu, assistant director of education for the SDS MicroMasters. “With some randomization to the questions and automatic grading, this quiz made determining potential for the Advanced Program in Data Science and Global Skills easier for BREIT, while also helping applicants see where they might need to brush up on their skills.”&lt;/p&gt;&lt;p dir="ltr"&gt;Since implementing the assessment, the program has continued to evolve in multiple ways, such as incorporating systematic feedback from MIT teaching assistants on data projects. This guidance, structured into multiple project stages, ensures the best outcomes for learners and project sponsors alike. The IDSS MicroMasters team has developed new coding demos to help familiarize learners with different applications and deepen understanding of the principles behind them. Meanwhile, the MicroMasters program itself has expanded to respond to industry demand, adding a course in time series analysis and creating specialized program tracks for learners to customize their experience.&lt;/p&gt;&lt;p dir="ltr"&gt;“Partner input helps us understand the landscape, so we better know the demands and how to meet them,” says Susana Kevorkova, program manager of the IDSS MicroMasters. “With BREIT, we are now offering a prerequisite ‘bootcamp’ to help learners from different backgrounds refresh their knowledge or cover gaps. We are always looking for ways to add value for our partners.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Better decisions, bigger impact&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;To accelerate the development of data skills, BREIT’s program offers hands-on opportunities to apply these skills to data projects. These projects are developed in collaboration with local nongovernmental organizations (NGOs) working on a variety of social impact projects intended to improve quality of life for Peruvian citizens.&lt;/p&gt;&lt;p dir="ltr"&gt;“I worked with an NGO trying to understand why students do not complete graduate study,” says Diego Trujillo Chappa, a BREIT learner and MicroMasters credential holder. “We developed an improved model for them considering student features such as their reading levels and their incomes, and tried to remove bias about where they come from.”&lt;/p&gt;&lt;p dir="ltr"&gt;“Our methodology helped the NGO to identify more possible applicants,” adds Trujillo. “And it’s a good step for the NGO, moving forward with better data analysis.”&lt;/p&gt;&lt;p dir="ltr"&gt;Trujillo has now brought these data skills to bear in his work modeling user experiences in the telecommunications sector. “We have some features that we want to improve in the 5G network in my country,” he explains. “This methodology helped me to correctly understand the variable of the person in the equation of the experience.”&lt;/p&gt;&lt;p dir="ltr"&gt;Yajaira Huerta’s social impact project dealt with a particularly serious issue, and at a tough time. “I worked with an organization that builds homes for people who are homeless,” she explains. “This was when Covid-19 was spreading, which was a difficult situation for many people in Peru.”&lt;/p&gt;&lt;p dir="ltr"&gt;One challenge her project organization faced was identifying where need was the highest in order to strategize the distribution of resources — a kind of problem where data tools can make a big impact. “We built a clustering model for capturing indicators available in the data, and also to show us with geolocation where the focal points of need were,” says Huerta. “This helped the team to make better decisions.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Global networks and pipelines&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;As a part of the growing, global IDSS community, credential holders of the MicroMasters Program in Statistics and Data Science have access to IDSS workshops and conferences. Through BREIT’s collaboration with IDSS, learners have more opportunities to interact with MIT faculty beyond recorded lectures. Some BREIT learners have even traveled to MIT, where they have met MIT students and faculty and learned about ongoing research.&lt;/p&gt;&lt;p dir="ltr"&gt;“I feel so in love with this history that you have, and also what you are building with AI and nanotechnology. I’m so inspired.” says Huerta of her time on campus.&lt;/p&gt;&lt;p dir="ltr"&gt;At their most recent visit in February, BREIT learners received completion certificates in person, toured the MIT campus, joined interactive talks with students and faculty, and got a preview of a new MicroMasters development: a&amp;nbsp;sports analytics course designed by mechanical engineering professor Anette “Peko” Hosoi.&lt;/p&gt;&lt;p dir="ltr"&gt;“Hosting BREIT and their extraordinarily talented learners brings all our partner efforts full circle, especially as MicroMasters credential holders are a pool of potential recruits for our on-campus graduate programs,” says Christia. “This partnership is a model we are ready to build on and iterate, so that we are developing similar networks and pipelines of data science talent on every part of the globe.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202504/mit-breit-idss-killian.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;The rise of artificial intelligence resurfaces a question older than the abacus: If we have a tool to do it for us, why learn to do it ourselves?&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;The answer, argues MIT electrical engineering and computer science (EECS) Professor Devavrat Shah, hasn’t changed: Foundational skills in mathematics remain essential to using tools well, from knowing which tool to use to interpreting results correctly.&lt;/p&gt;&lt;p dir="ltr"&gt;“As large language models and generative AI meet new applications, these cutting-edge tools will continue to reshape entire sectors of industry, and bring new insights to challenges in research and policy,” argues Shah. “The world needs people who can grasp the underlying concepts behind AI to truly leverage its potential.”&lt;/p&gt;&lt;p dir="ltr"&gt;Shah is a professor in MIT’s&amp;nbsp;Institute for Data, Systems, and Society (IDSS), a cross-disciplinary unit meeting the global need for data skills with online course offerings like the&amp;nbsp;MicroMasters Program in Statistics and Data Science, which Shah directs.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“With over a thousand credential holders worldwide, and tens of thousands more learners engaged since its inception, the MicroMasters Program in Statistics and Data Science has proven to be a rigorous but flexible way for skilled learners to develop an MIT-level grasp of statistics fundamentals,” says Shah.&lt;/p&gt;&lt;p dir="ltr"&gt;The MicroMasters also forms the backbone of IDSS education partnerships, where an embedded MIT team collaborates with organizations to support groups of learners through the MicroMasters curriculum.&lt;/p&gt;&lt;p dir="ltr"&gt;“Together with our first strategic partner in education, IDSS is providing graduate-level data science education through the Brescia Institute of Technology (BREIT) in Peru,” explains Fotini Christia, the Ford International Professor of the Social Sciences at MIT and director of IDSS. “Through this partnership, IDSS is training data scientists who are informing decision-making in Peruvian industry, society, and policy.”&lt;/p&gt;        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-youtube-video paragraph--view-mode--default"&gt;
          

            
   

  &lt;div class="news-article--inline-video"&gt;
        
        &lt;div class="news-article--inline-video--caption"&gt;
      

            Building networks of data science and machine learning talent: MIT IDSS and BREIT&lt;br /&gt;Video: MIT IDSS        

    &lt;/div&gt;
          &lt;/div&gt;  
        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;&lt;strong&gt;Training the next generation&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;BREIT’s Advanced Program in Data Science and Global Skills, developed in collaboration with IDSS, provides training in both the technical and nontechnical skills needed to take advantage of the insights that data can offer. Members complete the MicroMasters in Statistics and Data Science (SDS), learning the foundations of statistics, probability, data analysis, and machine learning. Meanwhile, these learners are equipped with career skills from communication and critical thinking to team-building and ethics.&lt;/p&gt;&lt;p dir="ltr"&gt;“I knew that artificial intelligence, machine learning, and data science was the future, and I wanted to be in that wave,” explains BREIT learner Renato Castro about his decision to join the program. Now a credential holder, Castro has developed data projects for groups in Peru, Panama, and Guatemala. “The program teaches more than the mathematics. It’s a systematic way of thinking that helps you have an impact on real-world problems and create wealth not only for a company, but wealth for the people.”&lt;/p&gt;&lt;p dir="ltr"&gt;“The aim is to develop problem-solvers and leaders in a field that is growing and becoming more relevant for organizations around the world,” says Lucia Haro, manager of BREIT. “We are training the next generation to contribute to the economic development of our country, and to have a positive social impact in Peru.”&lt;/p&gt;&lt;p dir="ltr"&gt;To help accomplish this, IDSS provides BREIT learners with tailored support. MIT grad student teaching assistants lead regular sessions to provide hands-on practice with class concepts, answer learner questions, and identify topics for developing additional resources.&lt;/p&gt;&lt;p dir="ltr"&gt;“These sessions were very useful because you see the application of the theoretical part from the lectures,” says Jesús Figueroa, who completed the program and now serves as a local teaching assistant. Learners like Figueroa must go beyond a deep understanding of the course material in order to support future learners.&lt;/p&gt;&lt;p dir="ltr"&gt;“Maybe you already understand the fundamentals, the theoretical part,” explains Figueroa, “but you have to learn how to communicate it.”&lt;/p&gt;&lt;p dir="ltr"&gt;Eight cohorts have completed the program, with three more in progress, for a total of almost 100 holders of the MicroMasters credential — and 90 more in the pipeline. As BREIT has scaled up their operation, the IDSS team worked to meet new needs as they emerged, such as collaborating in the development of a technical assessment to support learner recruitment.&lt;/p&gt;&lt;p dir="ltr"&gt;“The assessment tool gauges applicants’ familiarity with prerequisite knowledge like calculus, elementary linear algebra, and basic programming in Python,” says Karene Chu, assistant director of education for the SDS MicroMasters. “With some randomization to the questions and automatic grading, this quiz made determining potential for the Advanced Program in Data Science and Global Skills easier for BREIT, while also helping applicants see where they might need to brush up on their skills.”&lt;/p&gt;&lt;p dir="ltr"&gt;Since implementing the assessment, the program has continued to evolve in multiple ways, such as incorporating systematic feedback from MIT teaching assistants on data projects. This guidance, structured into multiple project stages, ensures the best outcomes for learners and project sponsors alike. The IDSS MicroMasters team has developed new coding demos to help familiarize learners with different applications and deepen understanding of the principles behind them. Meanwhile, the MicroMasters program itself has expanded to respond to industry demand, adding a course in time series analysis and creating specialized program tracks for learners to customize their experience.&lt;/p&gt;&lt;p dir="ltr"&gt;“Partner input helps us understand the landscape, so we better know the demands and how to meet them,” says Susana Kevorkova, program manager of the IDSS MicroMasters. “With BREIT, we are now offering a prerequisite ‘bootcamp’ to help learners from different backgrounds refresh their knowledge or cover gaps. We are always looking for ways to add value for our partners.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Better decisions, bigger impact&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;To accelerate the development of data skills, BREIT’s program offers hands-on opportunities to apply these skills to data projects. These projects are developed in collaboration with local nongovernmental organizations (NGOs) working on a variety of social impact projects intended to improve quality of life for Peruvian citizens.&lt;/p&gt;&lt;p dir="ltr"&gt;“I worked with an NGO trying to understand why students do not complete graduate study,” says Diego Trujillo Chappa, a BREIT learner and MicroMasters credential holder. “We developed an improved model for them considering student features such as their reading levels and their incomes, and tried to remove bias about where they come from.”&lt;/p&gt;&lt;p dir="ltr"&gt;“Our methodology helped the NGO to identify more possible applicants,” adds Trujillo. “And it’s a good step for the NGO, moving forward with better data analysis.”&lt;/p&gt;&lt;p dir="ltr"&gt;Trujillo has now brought these data skills to bear in his work modeling user experiences in the telecommunications sector. “We have some features that we want to improve in the 5G network in my country,” he explains. “This methodology helped me to correctly understand the variable of the person in the equation of the experience.”&lt;/p&gt;&lt;p dir="ltr"&gt;Yajaira Huerta’s social impact project dealt with a particularly serious issue, and at a tough time. “I worked with an organization that builds homes for people who are homeless,” she explains. “This was when Covid-19 was spreading, which was a difficult situation for many people in Peru.”&lt;/p&gt;&lt;p dir="ltr"&gt;One challenge her project organization faced was identifying where need was the highest in order to strategize the distribution of resources — a kind of problem where data tools can make a big impact. “We built a clustering model for capturing indicators available in the data, and also to show us with geolocation where the focal points of need were,” says Huerta. “This helped the team to make better decisions.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Global networks and pipelines&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;As a part of the growing, global IDSS community, credential holders of the MicroMasters Program in Statistics and Data Science have access to IDSS workshops and conferences. Through BREIT’s collaboration with IDSS, learners have more opportunities to interact with MIT faculty beyond recorded lectures. Some BREIT learners have even traveled to MIT, where they have met MIT students and faculty and learned about ongoing research.&lt;/p&gt;&lt;p dir="ltr"&gt;“I feel so in love with this history that you have, and also what you are building with AI and nanotechnology. I’m so inspired.” says Huerta of her time on campus.&lt;/p&gt;&lt;p dir="ltr"&gt;At their most recent visit in February, BREIT learners received completion certificates in person, toured the MIT campus, joined interactive talks with students and faculty, and got a preview of a new MicroMasters development: a&amp;nbsp;sports analytics course designed by mechanical engineering professor Anette “Peko” Hosoi.&lt;/p&gt;&lt;p dir="ltr"&gt;“Hosting BREIT and their extraordinarily talented learners brings all our partner efforts full circle, especially as MicroMasters credential holders are a pool of potential recruits for our on-campus graduate programs,” says Christia. “This partnership is a model we are ready to build on and iterate, so that we are developing similar networks and pipelines of data science talent on every part of the globe.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/building-networks-data-science-talent-0527</guid><pubDate>Tue, 27 May 2025 20:11:00 +0000</pubDate></item><item><title>CodeAgents + Structure: A Better Way to Execute Actions (Hugging Face - Blog)</title><link>https://huggingface.co/blog/structured-codeagent</link><description>&lt;!-- HTML_TAG_START --&gt;
Today we're sharing research that bridges two powerful paradigms in AI agent design: the expressiveness of code-based actions and the reliability of structured generation. Our findings show that forcing &lt;strong&gt;CodeAgents&lt;/strong&gt; to generate both thoughts and code in a structured JSON format can significantly outperform traditional approaches across multiple benchmarks.
&lt;p&gt;&lt;img alt="accuracy.png" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/structured-codeagent/accuracy.png" /&gt;
Figure 1: Accuracy comparison of three approaches: Structured CodeAgent (blue), CodeAgent (orange), and ToolCallingAgent (gray) on SmolBench (GAIA, MATH, SimpleQA, and Frames). Error bars represent 95% Confidence Intervals.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;🤔 The Evolution of Agent Actions&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;AI agents need to take actions in the world - whether that's calling APIs, processing data, or reasoning through complex problems. How agents express these actions has evolved through several paradigms:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Traditional JSON Agent&lt;/strong&gt;: Agents generate structured JSON to call tools.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;&lt;span class="hljs-punctuation"&gt;{&lt;/span&gt;&lt;span class="hljs-attr"&gt;"tool"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-string"&gt;"get_weather"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;,&lt;/span&gt; &lt;span class="hljs-attr"&gt;"arguments"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-punctuation"&gt;{&lt;/span&gt;&lt;span class="hljs-attr"&gt;"city"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-string"&gt;"Paris"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;}&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These agents operate by selecting from a list of predefined tools and generating JSON-formatted calls. This method for calling tools has been popularized by OpenAI's function calling API, and has since then been the most widely used method to call tools.&lt;/p&gt;
&lt;p&gt;It is reliable, but limited by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A limited set of actions&lt;/strong&gt;: The actions the agent can take are expressed only through predefined tools which limit its functionality.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lack of composability&lt;/strong&gt;: If the task requires composing information from multiple sources, JSON agents struggle because they lack support for maintaining intermediate state across tool calls. While some models support parallel tool calls, they can't easily handle scenarios where one tool's output determines the next action or where results need to be compared and processed together.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rigid structure&lt;/strong&gt;: Very limited in handling cases where tools do not match exactly what needs to be done.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Code Agents&lt;/strong&gt;: Agents make use of their innate coding ability and write executable Python code directly.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;
temperature_sum = &lt;span class="hljs-number"&gt;0&lt;/span&gt;
&lt;span class="hljs-keyword"&gt;for&lt;/span&gt; city &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; [&lt;span class="hljs-string"&gt;"Paris"&lt;/span&gt;, &lt;span class="hljs-string"&gt;"Tokyo"&lt;/span&gt;, &lt;span class="hljs-string"&gt;"New York"&lt;/span&gt;]:
    temp = get_weather(city)
    temperature_sum += temp
    
&lt;span class="hljs-built_in"&gt;print&lt;/span&gt;(&lt;span class="hljs-string"&gt;f"Average temperature: &lt;span class="hljs-subst"&gt;{temperature_sum / &lt;span class="hljs-number"&gt;3&lt;/span&gt;:&lt;span class="hljs-number"&gt;.1&lt;/span&gt;f}&lt;/span&gt;°C"&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This shift, first presented as CodeAct in the paper “Executable Code Actions Elicit Better LLM Agents” gave AI agents the flexibility to write arbitrary executable Python code in addition to tool-calling.&lt;/p&gt;
&lt;p&gt;The key insight here is that &lt;strong&gt;tools are called directly from within the code&lt;/strong&gt;, making variables and state management much more reliable. Agents can call tools within loops, functions, and conditional statements - essentially generating a dynamic graph of tool execution in each action! &lt;/p&gt;
&lt;p&gt;Pros of using a CodeAgent:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Smart tool use&lt;/strong&gt;: Agents decide which tools to use based on what’s happening in the moment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unlimited flexibility&lt;/strong&gt;: Can use any Python functionality to achieve a goal.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ability to test thoughts&lt;/strong&gt;: Agents can hypothesize and test, leading to more flexibility in their actions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, parsing code from markdown can be error-prone which leads us to a proposition: why not use structured generation to generate code actions?&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;➡️&amp;nbsp;Adding Structured outputs to Code Agent&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;With Structured outputs, you can force the LLM to generate explicit thoughts and code as a JSON blob:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;
&lt;span class="hljs-punctuation"&gt;{&lt;/span&gt;
  &lt;span class="hljs-attr"&gt;"thoughts"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-string"&gt;"I want to find the average temperature across 3 cities."&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;,&lt;/span&gt;
  &lt;span class="hljs-attr"&gt;"code"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-string"&gt;"temperature_sum = 0\nfor city in [\"Paris\", \"Tokyo\", \"New York\"]:\n    temp = get_weather(city)\n    temperature_sum += temp\n\nprint(f\"Average temperature: {temperature_sum / 3:.1f}°C\")"&lt;/span&gt;
&lt;span class="hljs-punctuation"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The key difference is that the generation is enforced: basically, now instead of just being prompted to output thoughts, then code, the usage of structured outputs forces it to respect the structure.&lt;/p&gt;
&lt;p&gt;This approach adds the reliability of structured generation to the flexibility of code execution, thus getting the best of both worlds.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Explicit reasoning&lt;/strong&gt;: The&amp;nbsp;&lt;code&gt;thoughts&lt;/code&gt;&amp;nbsp;field forces the agent to reason right before it takes an action.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reliable parsing&lt;/strong&gt;: JSON structure eliminates markdown parsing errors&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Full code expressiveness&lt;/strong&gt;: The&amp;nbsp;&lt;code&gt;code&lt;/code&gt;&amp;nbsp;field maintains all the flexibility of code agents&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Better separation&lt;/strong&gt;: Clear separation between planning and execution&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		🧪&amp;nbsp;&lt;strong&gt;Benchmark Results&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We compared these three paradigms across multiple benchmarks including GAIA, MATH, SimpleQA, and Frames. The results show a clear pattern:&amp;nbsp;&lt;strong&gt;Code actions + structured generation consistently improves performance for capable models&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Across most capable models, the structured approach consistently outperformed the regular CodeAgent approach by 2-7 percentage points on average.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;OpenAI models&lt;/strong&gt;: Show the largest improvements with structure, particularly on reasoning-heavy tasks&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Claude models&lt;/strong&gt;: Benefit from structure, with Claude 3.7 Sonnet showing especially strong results&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Qwen models&lt;/strong&gt;: Generally improve with structure, though “structure tax” (see in next section) creeps in for smaller models.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;💡 Why Structure (Generally) Helps&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;The Parsing Problem is Real&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Our implementation of CodeAgent in smolagents extracts Python code from the LLM output, which can fail when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Code block formulation in markdown is incomplete or incorrectly formatted&lt;/li&gt;
&lt;li&gt;Multiple code blocks appear in a single response&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Structured generation eliminates these issues with reliable JSON parsing.&lt;/p&gt;
&lt;p&gt;To understand why structured generation matters, we analyzed 15,724 agent traces across our benchmarks. The results are striking:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;2.4%&lt;/strong&gt; of traces had parsing errors in their first call&lt;/li&gt;
&lt;li&gt;Traces &lt;strong&gt;with&lt;/strong&gt; first call parsing errors: &lt;strong&gt;42.3%&lt;/strong&gt; success rate&lt;/li&gt;
&lt;li&gt;Traces &lt;strong&gt;without&lt;/strong&gt; first call parsing errors: &lt;strong&gt;51.3%&lt;/strong&gt; success rate&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Agent traces without parsing errors succeed 21.3% more often than those with parsing errors.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This isn't just about convenience - parsing errors create a cascade of failures that significantly impact overall agent performance. When an agent can't execute its first action due to malformed code, it often struggles to recover, leading to suboptimal problem-solving paths.&lt;/p&gt;
&lt;p&gt;&lt;img alt="parsing error.png" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/structured-codeagent/parsing_error.png" /&gt;
Figure 2: Parsing errors in the first step reduce success rates of the agent by 21.3% and increase average steps taken from 3.18 to 4.63.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Additionally: Enforced Reasoning Process&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The use of structured generation and explicit&amp;nbsp;&lt;code&gt;thoughts&lt;/code&gt;&amp;nbsp;not just prompts, but forces agents to articulate their reasoning before acting. This leads to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Better planning&lt;/strong&gt;: Agents think through problems more systematically&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Enhanced reliability&lt;/strong&gt;: Explicit reasoning catches logical errors early&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;The Structure Tax&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Our results also reveal a clear capability threshold: models need sufficient instruction-following ability and JSON coverage in their pre-training data to benefit from structured generation. This suggests that structured approaches work best with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Large, well-trained models&lt;/li&gt;
&lt;li&gt;Models with strong instruction-following capabilities&lt;/li&gt;
&lt;li&gt;Models fine-tuned on structured generation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;When Structure Breaks: A Real Example&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h4&gt;
&lt;p&gt;Here's what happens when a smaller model (e.g &lt;code&gt;mistralai/Mistral-7B-Instruct-v0.3&lt;/code&gt;) tries to generate structured code - the cognitive load becomes too much:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;&lt;span class="hljs-punctuation"&gt;{&lt;/span&gt;
  &lt;span class="hljs-attr"&gt;"thought"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-string"&gt;"I need to find the height..."&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;,&lt;/span&gt;
  &lt;span class="hljs-attr"&gt;"code"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-string"&gt;"web_search(query=\"Eiffel Tower height\")\", "&lt;/span&gt;
&lt;span class="hljs-punctuation"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model generates syntactically broken Python code: &lt;code&gt;web_search(query="Eiffel Tower height")",&lt;/code&gt; - notice the malformed string with an extra quote and comma. This leads to an immediate SyntaxError and execution failure.&lt;/p&gt;
&lt;p&gt;This illustrates the "structure tax": smaller models struggle to simultaneously handle JSON formatting, Python syntax, and the actual problem-solving logic. The cognitive overhead of structured generation can overwhelm models that would otherwise perform reasonably well with simpler markdown-based code generation.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;🚀 When to Use Structured CodeAgents&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;✅ Use Structured CodeAgents when:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Working with capable models (32B+ parameters or frontier models)&lt;/li&gt;
&lt;li&gt;Tasks require complex reasoning and code execution&lt;/li&gt;
&lt;li&gt;You need reliable parsing of agent outputs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;⚠️ Consider alternatives when:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Working with smaller models that struggle with structured generation&lt;/li&gt;
&lt;li&gt;Simple, predefined workflows are sufficient&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		How to use with smolagents:
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;It’s super simple! Just enable it with &lt;code&gt;use_structured_outputs_internally:&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; smolagents &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; CodeAgent, InferenceClientModel, GoogleSearchTool


agent = CodeAgent(
    tools=[GoogleSearchTool(provider=&lt;span class="hljs-string"&gt;"serper"&lt;/span&gt;)],
    model=InferenceClientModel(&lt;span class="hljs-string"&gt;"Qwen/Qwen3-235B-A22B"&lt;/span&gt;, provider=&lt;span class="hljs-string"&gt;'nebius'&lt;/span&gt;),
    use_structured_outputs_internally=&lt;span class="hljs-literal"&gt;True&lt;/span&gt; 
)

result = agent.run(&lt;span class="hljs-string"&gt;"Calculate the time for a cheetah to run across the Golden Gate Bridge"&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The LLM will generate something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;&lt;span class="hljs-punctuation"&gt;{&lt;/span&gt;
  &lt;span class="hljs-attr"&gt;"thoughts"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-string"&gt;"I need to find the length of the Golden Gate Bridge and the top speed of a cheetah, then calculate the time."&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;,&lt;/span&gt;
  &lt;span class="hljs-attr"&gt;"code"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-string"&gt;"bridge_info = web_search('Golden Gate Bridge length meters')\ncheetah_speed = web_search('Cheetah top speed') ..."&lt;/span&gt;
&lt;span class="hljs-punctuation"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then the "code" part gets executed by the agent as usual : this is the standard CodeAgent, but now it has 100% parsing reliability!&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Implementation Tips&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Clear prompting&lt;/strong&gt;: Ensure your prompts clearly specify the expected JSON structure&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model selection&lt;/strong&gt;: Choose models with strong structured generation capabilities&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Select the right provider:&lt;/strong&gt; Some API providers like OpenAI or Anthropic support structured generation out of the box. If you're using Inference providers through Hugging Face,  the support of structured generation varies across providers. Here is a list of providers that support structured generation: Structured generation support for Models in smolagents‣&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;The Bigger Picture - What's Next?&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;This research suggests we're moving toward a more nuanced understanding of agent architectures. It's not just about "what can the agent do?" but "how should the agent think about what it's doing?"&lt;/p&gt;
&lt;p&gt;Maybe making the reasoning process more explicit helps the model stay on track. Or maybe it's just easier to parse. Either way, it’s a win.&lt;/p&gt;
&lt;p&gt;But this is just the beginning. There are so many questions left to explore:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What other structural improvements could help?&lt;/li&gt;
&lt;li&gt;How do we make this work better across different model architectures, specifically smol models?&lt;/li&gt;
&lt;li&gt;What does this tell us about the nature of AI reasoning?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For now, if you're using smolagents (or building your own CodeAgent system), consider giving structured output a try. Your parsing errors will thank you, and you might just see a nice increase in performance!&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;!-- HTML_TAG_START --&gt;
Today we're sharing research that bridges two powerful paradigms in AI agent design: the expressiveness of code-based actions and the reliability of structured generation. Our findings show that forcing &lt;strong&gt;CodeAgents&lt;/strong&gt; to generate both thoughts and code in a structured JSON format can significantly outperform traditional approaches across multiple benchmarks.
&lt;p&gt;&lt;img alt="accuracy.png" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/structured-codeagent/accuracy.png" /&gt;
Figure 1: Accuracy comparison of three approaches: Structured CodeAgent (blue), CodeAgent (orange), and ToolCallingAgent (gray) on SmolBench (GAIA, MATH, SimpleQA, and Frames). Error bars represent 95% Confidence Intervals.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;🤔 The Evolution of Agent Actions&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;AI agents need to take actions in the world - whether that's calling APIs, processing data, or reasoning through complex problems. How agents express these actions has evolved through several paradigms:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Traditional JSON Agent&lt;/strong&gt;: Agents generate structured JSON to call tools.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;&lt;span class="hljs-punctuation"&gt;{&lt;/span&gt;&lt;span class="hljs-attr"&gt;"tool"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-string"&gt;"get_weather"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;,&lt;/span&gt; &lt;span class="hljs-attr"&gt;"arguments"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-punctuation"&gt;{&lt;/span&gt;&lt;span class="hljs-attr"&gt;"city"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-string"&gt;"Paris"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;}&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These agents operate by selecting from a list of predefined tools and generating JSON-formatted calls. This method for calling tools has been popularized by OpenAI's function calling API, and has since then been the most widely used method to call tools.&lt;/p&gt;
&lt;p&gt;It is reliable, but limited by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A limited set of actions&lt;/strong&gt;: The actions the agent can take are expressed only through predefined tools which limit its functionality.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lack of composability&lt;/strong&gt;: If the task requires composing information from multiple sources, JSON agents struggle because they lack support for maintaining intermediate state across tool calls. While some models support parallel tool calls, they can't easily handle scenarios where one tool's output determines the next action or where results need to be compared and processed together.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rigid structure&lt;/strong&gt;: Very limited in handling cases where tools do not match exactly what needs to be done.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Code Agents&lt;/strong&gt;: Agents make use of their innate coding ability and write executable Python code directly.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;
temperature_sum = &lt;span class="hljs-number"&gt;0&lt;/span&gt;
&lt;span class="hljs-keyword"&gt;for&lt;/span&gt; city &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; [&lt;span class="hljs-string"&gt;"Paris"&lt;/span&gt;, &lt;span class="hljs-string"&gt;"Tokyo"&lt;/span&gt;, &lt;span class="hljs-string"&gt;"New York"&lt;/span&gt;]:
    temp = get_weather(city)
    temperature_sum += temp
    
&lt;span class="hljs-built_in"&gt;print&lt;/span&gt;(&lt;span class="hljs-string"&gt;f"Average temperature: &lt;span class="hljs-subst"&gt;{temperature_sum / &lt;span class="hljs-number"&gt;3&lt;/span&gt;:&lt;span class="hljs-number"&gt;.1&lt;/span&gt;f}&lt;/span&gt;°C"&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This shift, first presented as CodeAct in the paper “Executable Code Actions Elicit Better LLM Agents” gave AI agents the flexibility to write arbitrary executable Python code in addition to tool-calling.&lt;/p&gt;
&lt;p&gt;The key insight here is that &lt;strong&gt;tools are called directly from within the code&lt;/strong&gt;, making variables and state management much more reliable. Agents can call tools within loops, functions, and conditional statements - essentially generating a dynamic graph of tool execution in each action! &lt;/p&gt;
&lt;p&gt;Pros of using a CodeAgent:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Smart tool use&lt;/strong&gt;: Agents decide which tools to use based on what’s happening in the moment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unlimited flexibility&lt;/strong&gt;: Can use any Python functionality to achieve a goal.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ability to test thoughts&lt;/strong&gt;: Agents can hypothesize and test, leading to more flexibility in their actions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, parsing code from markdown can be error-prone which leads us to a proposition: why not use structured generation to generate code actions?&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;➡️&amp;nbsp;Adding Structured outputs to Code Agent&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;With Structured outputs, you can force the LLM to generate explicit thoughts and code as a JSON blob:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;
&lt;span class="hljs-punctuation"&gt;{&lt;/span&gt;
  &lt;span class="hljs-attr"&gt;"thoughts"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-string"&gt;"I want to find the average temperature across 3 cities."&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;,&lt;/span&gt;
  &lt;span class="hljs-attr"&gt;"code"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-string"&gt;"temperature_sum = 0\nfor city in [\"Paris\", \"Tokyo\", \"New York\"]:\n    temp = get_weather(city)\n    temperature_sum += temp\n\nprint(f\"Average temperature: {temperature_sum / 3:.1f}°C\")"&lt;/span&gt;
&lt;span class="hljs-punctuation"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The key difference is that the generation is enforced: basically, now instead of just being prompted to output thoughts, then code, the usage of structured outputs forces it to respect the structure.&lt;/p&gt;
&lt;p&gt;This approach adds the reliability of structured generation to the flexibility of code execution, thus getting the best of both worlds.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Explicit reasoning&lt;/strong&gt;: The&amp;nbsp;&lt;code&gt;thoughts&lt;/code&gt;&amp;nbsp;field forces the agent to reason right before it takes an action.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reliable parsing&lt;/strong&gt;: JSON structure eliminates markdown parsing errors&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Full code expressiveness&lt;/strong&gt;: The&amp;nbsp;&lt;code&gt;code&lt;/code&gt;&amp;nbsp;field maintains all the flexibility of code agents&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Better separation&lt;/strong&gt;: Clear separation between planning and execution&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		🧪&amp;nbsp;&lt;strong&gt;Benchmark Results&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We compared these three paradigms across multiple benchmarks including GAIA, MATH, SimpleQA, and Frames. The results show a clear pattern:&amp;nbsp;&lt;strong&gt;Code actions + structured generation consistently improves performance for capable models&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Across most capable models, the structured approach consistently outperformed the regular CodeAgent approach by 2-7 percentage points on average.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;OpenAI models&lt;/strong&gt;: Show the largest improvements with structure, particularly on reasoning-heavy tasks&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Claude models&lt;/strong&gt;: Benefit from structure, with Claude 3.7 Sonnet showing especially strong results&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Qwen models&lt;/strong&gt;: Generally improve with structure, though “structure tax” (see in next section) creeps in for smaller models.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;💡 Why Structure (Generally) Helps&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;The Parsing Problem is Real&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Our implementation of CodeAgent in smolagents extracts Python code from the LLM output, which can fail when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Code block formulation in markdown is incomplete or incorrectly formatted&lt;/li&gt;
&lt;li&gt;Multiple code blocks appear in a single response&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Structured generation eliminates these issues with reliable JSON parsing.&lt;/p&gt;
&lt;p&gt;To understand why structured generation matters, we analyzed 15,724 agent traces across our benchmarks. The results are striking:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;2.4%&lt;/strong&gt; of traces had parsing errors in their first call&lt;/li&gt;
&lt;li&gt;Traces &lt;strong&gt;with&lt;/strong&gt; first call parsing errors: &lt;strong&gt;42.3%&lt;/strong&gt; success rate&lt;/li&gt;
&lt;li&gt;Traces &lt;strong&gt;without&lt;/strong&gt; first call parsing errors: &lt;strong&gt;51.3%&lt;/strong&gt; success rate&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Agent traces without parsing errors succeed 21.3% more often than those with parsing errors.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This isn't just about convenience - parsing errors create a cascade of failures that significantly impact overall agent performance. When an agent can't execute its first action due to malformed code, it often struggles to recover, leading to suboptimal problem-solving paths.&lt;/p&gt;
&lt;p&gt;&lt;img alt="parsing error.png" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/structured-codeagent/parsing_error.png" /&gt;
Figure 2: Parsing errors in the first step reduce success rates of the agent by 21.3% and increase average steps taken from 3.18 to 4.63.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Additionally: Enforced Reasoning Process&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The use of structured generation and explicit&amp;nbsp;&lt;code&gt;thoughts&lt;/code&gt;&amp;nbsp;not just prompts, but forces agents to articulate their reasoning before acting. This leads to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Better planning&lt;/strong&gt;: Agents think through problems more systematically&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Enhanced reliability&lt;/strong&gt;: Explicit reasoning catches logical errors early&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;The Structure Tax&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Our results also reveal a clear capability threshold: models need sufficient instruction-following ability and JSON coverage in their pre-training data to benefit from structured generation. This suggests that structured approaches work best with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Large, well-trained models&lt;/li&gt;
&lt;li&gt;Models with strong instruction-following capabilities&lt;/li&gt;
&lt;li&gt;Models fine-tuned on structured generation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;When Structure Breaks: A Real Example&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h4&gt;
&lt;p&gt;Here's what happens when a smaller model (e.g &lt;code&gt;mistralai/Mistral-7B-Instruct-v0.3&lt;/code&gt;) tries to generate structured code - the cognitive load becomes too much:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;&lt;span class="hljs-punctuation"&gt;{&lt;/span&gt;
  &lt;span class="hljs-attr"&gt;"thought"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-string"&gt;"I need to find the height..."&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;,&lt;/span&gt;
  &lt;span class="hljs-attr"&gt;"code"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-string"&gt;"web_search(query=\"Eiffel Tower height\")\", "&lt;/span&gt;
&lt;span class="hljs-punctuation"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model generates syntactically broken Python code: &lt;code&gt;web_search(query="Eiffel Tower height")",&lt;/code&gt; - notice the malformed string with an extra quote and comma. This leads to an immediate SyntaxError and execution failure.&lt;/p&gt;
&lt;p&gt;This illustrates the "structure tax": smaller models struggle to simultaneously handle JSON formatting, Python syntax, and the actual problem-solving logic. The cognitive overhead of structured generation can overwhelm models that would otherwise perform reasonably well with simpler markdown-based code generation.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;🚀 When to Use Structured CodeAgents&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;✅ Use Structured CodeAgents when:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Working with capable models (32B+ parameters or frontier models)&lt;/li&gt;
&lt;li&gt;Tasks require complex reasoning and code execution&lt;/li&gt;
&lt;li&gt;You need reliable parsing of agent outputs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;⚠️ Consider alternatives when:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Working with smaller models that struggle with structured generation&lt;/li&gt;
&lt;li&gt;Simple, predefined workflows are sufficient&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		How to use with smolagents:
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;It’s super simple! Just enable it with &lt;code&gt;use_structured_outputs_internally:&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; smolagents &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; CodeAgent, InferenceClientModel, GoogleSearchTool


agent = CodeAgent(
    tools=[GoogleSearchTool(provider=&lt;span class="hljs-string"&gt;"serper"&lt;/span&gt;)],
    model=InferenceClientModel(&lt;span class="hljs-string"&gt;"Qwen/Qwen3-235B-A22B"&lt;/span&gt;, provider=&lt;span class="hljs-string"&gt;'nebius'&lt;/span&gt;),
    use_structured_outputs_internally=&lt;span class="hljs-literal"&gt;True&lt;/span&gt; 
)

result = agent.run(&lt;span class="hljs-string"&gt;"Calculate the time for a cheetah to run across the Golden Gate Bridge"&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The LLM will generate something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;&lt;span class="hljs-punctuation"&gt;{&lt;/span&gt;
  &lt;span class="hljs-attr"&gt;"thoughts"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-string"&gt;"I need to find the length of the Golden Gate Bridge and the top speed of a cheetah, then calculate the time."&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;,&lt;/span&gt;
  &lt;span class="hljs-attr"&gt;"code"&lt;/span&gt;&lt;span class="hljs-punctuation"&gt;:&lt;/span&gt; &lt;span class="hljs-string"&gt;"bridge_info = web_search('Golden Gate Bridge length meters')\ncheetah_speed = web_search('Cheetah top speed') ..."&lt;/span&gt;
&lt;span class="hljs-punctuation"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then the "code" part gets executed by the agent as usual : this is the standard CodeAgent, but now it has 100% parsing reliability!&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Implementation Tips&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Clear prompting&lt;/strong&gt;: Ensure your prompts clearly specify the expected JSON structure&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model selection&lt;/strong&gt;: Choose models with strong structured generation capabilities&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Select the right provider:&lt;/strong&gt; Some API providers like OpenAI or Anthropic support structured generation out of the box. If you're using Inference providers through Hugging Face,  the support of structured generation varies across providers. Here is a list of providers that support structured generation: Structured generation support for Models in smolagents‣&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;The Bigger Picture - What's Next?&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;This research suggests we're moving toward a more nuanced understanding of agent architectures. It's not just about "what can the agent do?" but "how should the agent think about what it's doing?"&lt;/p&gt;
&lt;p&gt;Maybe making the reasoning process more explicit helps the model stay on track. Or maybe it's just easier to parse. Either way, it’s a win.&lt;/p&gt;
&lt;p&gt;But this is just the beginning. There are so many questions left to explore:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What other structural improvements could help?&lt;/li&gt;
&lt;li&gt;How do we make this work better across different model architectures, specifically smol models?&lt;/li&gt;
&lt;li&gt;What does this tell us about the nature of AI reasoning?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For now, if you're using smolagents (or building your own CodeAgent system), consider giving structured output a try. Your parsing errors will thank you, and you might just see a nice increase in performance!&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/structured-codeagent</guid><pubDate>Wed, 28 May 2025 00:00:00 +0000</pubDate></item><item><title>The AI Hype Index: College students are hooked on ChatGPT (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/05/28/1117468/ai-hype-index-college-students-chatgpt-meta-apple-anthropic-grok/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/05/May-Thumb.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt;&lt;p&gt;Separating AI reality from hyped-up fiction isn’t always easy. That’s why we’ve created the AI Hype Index—a simple, at-a-glance summary of everything you need to know about the state of the industry.&lt;/p&gt;  &lt;p&gt;Large language models confidently present their responses as accurate and reliable, even when they’re neither of those things. That’s why we’ve recently seen chatbots supercharge vulnerable people’s delusions, make citation mistakes in an important legal battle between music publishers and Anthropic, and (in the case of xAI’s Grok) rant irrationally about “white genocide.”&lt;/p&gt;    &lt;p&gt;But it’s not all bad news—AI could also finally lead to a better battery life for your iPhone and solve tricky real-world problems that humans have been struggling to crack, if Google DeepMind’s new model is any indication. And perhaps most exciting of all, it could combine with brain implants to help people communicate when they have lost the ability to speak.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/05/May-Thumb.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt;&lt;p&gt;Separating AI reality from hyped-up fiction isn’t always easy. That’s why we’ve created the AI Hype Index—a simple, at-a-glance summary of everything you need to know about the state of the industry.&lt;/p&gt;  &lt;p&gt;Large language models confidently present their responses as accurate and reliable, even when they’re neither of those things. That’s why we’ve recently seen chatbots supercharge vulnerable people’s delusions, make citation mistakes in an important legal battle between music publishers and Anthropic, and (in the case of xAI’s Grok) rant irrationally about “white genocide.”&lt;/p&gt;    &lt;p&gt;But it’s not all bad news—AI could also finally lead to a better battery life for your iPhone and solve tricky real-world problems that humans have been struggling to crack, if Google DeepMind’s new model is any indication. And perhaps most exciting of all, it could combine with brain implants to help people communicate when they have lost the ability to speak.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/05/28/1117468/ai-hype-index-college-students-chatgpt-meta-apple-anthropic-grok/</guid><pubDate>Wed, 28 May 2025 14:13:29 +0000</pubDate></item><item><title>AI Safety Newsletter #56: Google Releases Veo 3 (AI Safety Newsletter)</title><link>https://newsletter.safe.ai/p/ai-safety-newsletter-56-google-releases</link><description>&lt;p&gt;&lt;span&gt;Welcome to the AI Safety Newsletter by the &lt;/span&gt;&lt;a href="https://www.safe.ai/" rel="rel"&gt;Center for AI Safety&lt;/a&gt;&lt;span&gt;. We discuss developments in AI and AI safety. No technical background required.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;In this edition: Google released a frontier video generation model at its annual developer conference; Anthropic’s Claude Opus 4 demonstrates the danger of relying on voluntary governance.&lt;/p&gt;&lt;p&gt;&lt;span&gt;Listen to the AI Safety Newsletter for free on &lt;/span&gt;&lt;a href="https://spotify.link/E6lHa1ij2Cb" rel="rel"&gt;Spotify&lt;/a&gt;&lt;span&gt; or &lt;/span&gt;&lt;a href="https://podcasts.apple.com/us/podcast/ai-safety-newsletter/id1702875110" rel="rel"&gt;Apple Podcasts&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Last week, Google made several &lt;/span&gt;&lt;a href="https://blog.google/technology/developers/google-io-2025-collection/" rel="rel"&gt;AI announcements&lt;/a&gt;&lt;span&gt; at I/O 2025, its annual developer conference. An announcement of particular note is &lt;/span&gt;&lt;a href="https://deepmind.google/models/veo/" rel="rel"&gt;Veo 3&lt;/a&gt;&lt;span&gt;, Google’s newest video generation model.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Frontier video and audio generation.&lt;/strong&gt;&lt;span&gt; Veo 3 outperforms other models on &lt;/span&gt;&lt;a href="https://deepmind.google/models/veo/evals/" rel="rel"&gt;human preference benchmarks&lt;/a&gt;&lt;span&gt;, and generates both audio and video.&lt;/span&gt;&lt;/p&gt;&lt;div class="captioned-image-container"&gt;&lt;figure&gt;&lt;a class="image-link image2 is-viewable-img" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda24a5e2-92d6-490e-b74f-88fa68203799_1600x900.png" rel="rel" target="_blank"&gt;&lt;div class="image2-inset"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="alt" class="sizing-normal" height="819" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda24a5e2-92d6-490e-b74f-88fa68203799_1600x900.png" width="1456" /&gt;&lt;/div&gt;&lt;/a&gt;&lt;figcaption class="image-caption"&gt;&lt;span&gt;Google showcasing a video generated with Veo 3. (&lt;/span&gt;&lt;a href="https://www.axios.com/2025/05/23/google-ai-videos-veo-3" rel="rel"&gt;Source&lt;/a&gt;&lt;span&gt;)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;p&gt;&lt;span&gt;If you just look at benchmarks, Veo 3 is a substantial improvement over other systems. But relative benchmark improvement only tells part of the story—the absolute capabilities of systems ultimately determine their usefulness. Veo 3 looks like a marked qualitative improvement over other models—it generates video and audio with extreme faithfulness, and we recommend you &lt;/span&gt;&lt;a href="https://x.com/HashemGhaili/status/1925332319604257203" rel="rel"&gt;see&lt;/a&gt;&lt;span&gt; &lt;/span&gt;&lt;a href="https://x.com/minchoi/status/1925387367806115943" rel="rel"&gt;some&lt;/a&gt;&lt;span&gt; &lt;/span&gt;&lt;a href="https://x.com/laszlogaal_/status/1925094336200573225" rel="rel"&gt;examples&lt;/a&gt;&lt;span&gt; for yourself. Veo 3 may represent the point video generation crosses the line between being an interesting toy and being genuinely useful.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Other announcements at I/O 2025&lt;/strong&gt;&lt;span&gt;. Other highlights from the conference include:&lt;/span&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025/" rel="rel"&gt;Gemini 2.5&lt;/a&gt;&lt;span&gt; Pro now leads LMArena and WebDev Arena. Deep Think mode, a reasoning feature that scored 49.4% on the USA Mathematical Olympiad 2025 (more than twice OpenAI’s o3, which scored 21.7%). Gemini 2.5 Flash now performs better across reasoning, multimodality, code, and long context while becoming 20-30% more efficient in token usage.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://blog.google/technology/google-deepmind/gemini-diffusion/" rel="rel"&gt;Gemini Diffusion&lt;/a&gt;&lt;span&gt;, an experimental (non-frontier) text diffusion model, delivers output 4-5 times faster than comparable models while rivaling the performance of models twice its size. Most LLMs are autoregressive models, which generate one token at a time—in contrast, diffusion models generate an entire response at once.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Google also announced &lt;/span&gt;&lt;a href="https://developers.googleblog.com/en/introducing-gemma-3n/" rel="rel"&gt;Gemma 3n&lt;/a&gt;&lt;span&gt;, an open model small enough to run on mobile devices, a public beta for Google’s autonomous coding agent &lt;/span&gt;&lt;a href="https://blog.google/technology/google-labs/jules/" rel="rel"&gt;Jules&lt;/a&gt;&lt;span&gt;, a new &lt;/span&gt;&lt;a href="https://blog.google/products/search/google-search-ai-mode-update/#ai-mode-search" rel="rel"&gt;AI search&lt;/a&gt;&lt;span&gt; feature, an &lt;/span&gt;&lt;a href="https://blog.google/technology/ai/google-synthid-ai-content-detector/" rel="rel"&gt;AI watermarker&lt;/a&gt;&lt;span&gt; that identifies content generated by Google’s systems, and more.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;AI is here to stay.&lt;/strong&gt;&lt;span&gt; AI use is sometimes driven by trends—for example, &lt;/span&gt;&lt;a href="https://x.com/sama/status/1906771292390666325" rel="rel"&gt;ChatGPT added a million users in an hour&lt;/a&gt;&lt;span&gt; during the ‘Ghiblification’ craze. However, as AI systems become genuinely useful across more tasks, they will become ubiquitous and enduring. Google’s Gemini app now has &lt;/span&gt;&lt;a href="https://blog.google/technology/ai/io-2025-keynote/" rel="rel"&gt;400M monthly active users&lt;/a&gt;&lt;span&gt;, and its AI products now process over 480 trillion tokens a month—up from 9.7 trillion last year.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Last week, Anthropic released Claude Opus 4 and Claude Sonnet 4. Both exhibit broadly frontier performance, and lead the field on coding benchmarks. Claude Opus 4 is also Anthropic’s first model to meet &lt;/span&gt;&lt;a href="https://time.com/7287806/anthropic-claude-4-opus-safety-bio-risk/" rel="rel"&gt;its ASL-3 safety measure&lt;/a&gt;&lt;span&gt;, which designates models that pose substantial risk. However, Anthropic rolled back several safety and security commitments prior to releasing Opus 4, demonstrating that voluntary governance is not to be relied on.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Opus 4 exhibits hazardous dual-use capabilities.&lt;/strong&gt;&lt;span&gt; In one result from its &lt;/span&gt;&lt;a href="https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf" rel="rel"&gt;system card&lt;/a&gt;&lt;span&gt;, Opus 4 provides a clear uplift in trials measuring its ability to help malicious actors acquire biological weapons.&lt;/span&gt;&lt;/p&gt;&lt;div class="captioned-image-container"&gt;&lt;figure&gt;&lt;a class="image-link image2 is-viewable-img" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad471014-fe58-4180-a67a-9b48862263b9_1600x602.png" rel="rel" target="_blank"&gt;&lt;div class="image2-inset"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="alt" class="sizing-normal" height="548" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad471014-fe58-4180-a67a-9b48862263b9_1600x602.png" width="1456" /&gt;&lt;/div&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;p&gt;&lt;span&gt;Anthropic’s Chief Scientist Jarad Kaplan &lt;/span&gt;&lt;a href="https://time.com/7287806/anthropic-claude-4-opus-safety-bio-risk/" rel="rel"&gt;told TIME&lt;/a&gt;&lt;span&gt; that malicious actors could use Opus 4 to “try to synthesize something like COVID or a more dangerous version of the flu—and basically, our modeling suggests that this might be possible.” It’s not just Opus 4: &lt;/span&gt;&lt;a href="https://www.ai-frontiers.org/articles/ais-are-disseminating-expert-level-virology-skills" rel="rel"&gt;several frontier models outperform human experts in dual-use virology tests&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;The system card also reports that Apollo Research found an early Claude Opus 4 version exhibited "scheming and deception," advising against its release. Anthropic says it implemented internal fixes; however, &lt;/span&gt;&lt;a href="https://www.youtube.com/watch?v=Xn_5aIhrJOE&amp;amp;t=510s" rel="rel"&gt;it doesn’t appear that&lt;/a&gt;&lt;span&gt; Anthropic had Apollo Research re-evaluate the final, released version.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Anthropic’s safety protections may be insufficient.&lt;/strong&gt;&lt;span&gt; In light of Opus 4’s dangerous capabilities, Anthropic rolled out &lt;/span&gt;&lt;a href="https://www.anthropic.com/news/activating-asl3-protections" rel="rel"&gt;ASL-3 safety protections&lt;/a&gt;&lt;span&gt;. However, early public response to Opus 4 indicates that those protections might be insufficient. For example, one researcher showed that Claude Opus 4's WMD safeguards can be bypassed to generate &lt;/span&gt;&lt;a href="https://x.com/ARGleave/status/1926138376509440433" rel="rel"&gt;over 15 pages of detailed instructions for producing sarin gas&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Anthropic walked back safety and security commitments prior to Opus 4’s release.&lt;/strong&gt;&lt;span&gt; Anthropic has also faced criticism for walking back safety commitments prior to Opus 4’s release. For example, Anthropic’s &lt;/span&gt;&lt;a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy" rel="rel"&gt;September 2023 Responsible Scaling Policy&lt;/a&gt;&lt;span&gt; (RSP) committed to define detailed ASL-4 "warning sign evaluations" before their systems reached ASL-3 capabilities; however, it &lt;/span&gt;&lt;a href="https://www.obsolete.pub/p/exclusive-anthropic-is-quietly-backpedalling" rel="rel"&gt;hadn’t done so at the time of Opus 4’s release&lt;/a&gt;&lt;span&gt;. This is because Anthropic redlined that requirement in an &lt;/span&gt;&lt;a href="https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy" rel="rel"&gt;October 2024 revision&lt;/a&gt;&lt;span&gt; to its RSP.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Anthropic also &lt;/span&gt;&lt;a href="https://x.com/RyanPGreenblatt/status/1925992239332724921" rel="rel"&gt;weakened its ASL-3 security requirements&lt;/a&gt;&lt;span&gt; shortly before Opus 4's ASL-3 announcement, specifically no longer requiring robustness against employees stealing model weights if they already had access to "systems that process model weights."&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Voluntary governance is fragile.&lt;/strong&gt;&lt;span&gt; Whether or not Anthropic’s changes to its safety and security policies are justified, voluntary commitments are not sufficient to ensure model releases are safe. There’s nothing stopping Anthropic or other AI companies from walking back critical commitments in the face of competitive pressure to rush releases.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Government&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;JD Vance discussed why he’s worried about AI in &lt;/span&gt;&lt;a href="https://www.nytimes.com/2025/05/21/opinion/jd-vance-pope-trump-immigration.html" rel="rel"&gt;a recent interview&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;A judge &lt;/span&gt;&lt;a href="https://www.transparencycoalition.ai/news/important-early-ruling-in-characterai-case-this-chatbot-is-a-product-not-speech" rel="rel"&gt;ruled&lt;/a&gt;&lt;span&gt; that Character.AI is a product for the purposes of product liability in a lawsuit over a boy’s suicide after interacting with a Character.AI chatbot.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Industry&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;OpenAI &lt;/span&gt;&lt;a href="https://www.npr.org/2025/05/22/nx-s1-5407548/openai-jony-ive-io-deal-ai-devices" rel="rel"&gt;bought&lt;/a&gt;&lt;span&gt; iPhone designer Jony Ive’s startup, io, for $6.5 billion.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Civil Society&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Peter N. Salib and Simon Goldstein &lt;/span&gt;&lt;a href="https://www.ai-frontiers.org/articles/todays-ais-arent-paperclip-maximizers" rel="rel"&gt;argue&lt;/a&gt;&lt;span&gt; that today’s AI systems aren’t paperclip maximizers.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Devid Kirichenko &lt;/span&gt;&lt;a href="https://www.ai-frontiers.org/articles/how-ai-is-eroding-the-norms-of-war" rel="rel"&gt;writes&lt;/a&gt;&lt;span&gt; about how drones are eroding the norms of war.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;ARC Prize released a new reasoning benchmark, &lt;/span&gt;&lt;a href="https://arcprize.org/blog/arc-agi-2-technical-report" rel="rel"&gt;ARC-AGI-2&lt;/a&gt;&lt;span&gt;, on which frontier reasoning models score in low single-digits.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;CSET is &lt;/span&gt;&lt;a href="https://cset.georgetown.edu/wp-content/uploads/FRG-Call-for-Research-Ideas-Internal-Deployment.pdf" rel="rel"&gt;funding&lt;/a&gt;&lt;span&gt; research on risks from internal deployment of frontier AI models.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;A new &lt;/span&gt;&lt;a href="https://arxiv.org/abs/2505.09662" rel="rel"&gt;paper&lt;/a&gt;&lt;span&gt; found that Claude Sonnet 3.5 is significantly more persuasive than humans.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;An Axios &lt;/span&gt;&lt;a href="https://www.axios.com/newsletters/axios-ai-plus-62025700-399b-11f0-b37f-b73dfdd12f1d.html?stream=top" rel="rel"&gt;poll&lt;/a&gt;&lt;span&gt; found that 77% of Americans want AI companies to slow down.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;See also: &lt;/span&gt;&lt;a href="https://x.com/ai_risks?lang=en" rel="rel"&gt;CAIS’ X account&lt;/a&gt;&lt;span&gt;, our paper on &lt;/span&gt;&lt;a href="https://www.nationalsecurity.ai/" rel="rel"&gt;superintelligence strategy&lt;/a&gt;&lt;span&gt;, our &lt;/span&gt;&lt;a href="https://www.aisafetybook.com/" rel="rel"&gt;AI safety course&lt;/a&gt;&lt;span&gt;, and &lt;/span&gt;&lt;a href="http://ai-frontiers.org/" rel="rel"&gt;AI Frontiers&lt;/a&gt;&lt;span&gt;, a new platform for expert commentary and analysis.&lt;/span&gt;&lt;/p&gt;&lt;p class="button-wrapper"&gt;&lt;a class="button primary" href="https://newsletter.safe.ai/p/ai-safety-newsletter-56-google-releases?utm_source=substack&amp;amp;utm_medium=email&amp;amp;utm_content=share&amp;amp;action=share" rel="rel"&gt;&lt;span&gt;Share&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;</description><content:encoded>&lt;p&gt;&lt;span&gt;Welcome to the AI Safety Newsletter by the &lt;/span&gt;&lt;a href="https://www.safe.ai/" rel="rel"&gt;Center for AI Safety&lt;/a&gt;&lt;span&gt;. We discuss developments in AI and AI safety. No technical background required.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;In this edition: Google released a frontier video generation model at its annual developer conference; Anthropic’s Claude Opus 4 demonstrates the danger of relying on voluntary governance.&lt;/p&gt;&lt;p&gt;&lt;span&gt;Listen to the AI Safety Newsletter for free on &lt;/span&gt;&lt;a href="https://spotify.link/E6lHa1ij2Cb" rel="rel"&gt;Spotify&lt;/a&gt;&lt;span&gt; or &lt;/span&gt;&lt;a href="https://podcasts.apple.com/us/podcast/ai-safety-newsletter/id1702875110" rel="rel"&gt;Apple Podcasts&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Last week, Google made several &lt;/span&gt;&lt;a href="https://blog.google/technology/developers/google-io-2025-collection/" rel="rel"&gt;AI announcements&lt;/a&gt;&lt;span&gt; at I/O 2025, its annual developer conference. An announcement of particular note is &lt;/span&gt;&lt;a href="https://deepmind.google/models/veo/" rel="rel"&gt;Veo 3&lt;/a&gt;&lt;span&gt;, Google’s newest video generation model.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Frontier video and audio generation.&lt;/strong&gt;&lt;span&gt; Veo 3 outperforms other models on &lt;/span&gt;&lt;a href="https://deepmind.google/models/veo/evals/" rel="rel"&gt;human preference benchmarks&lt;/a&gt;&lt;span&gt;, and generates both audio and video.&lt;/span&gt;&lt;/p&gt;&lt;div class="captioned-image-container"&gt;&lt;figure&gt;&lt;a class="image-link image2 is-viewable-img" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda24a5e2-92d6-490e-b74f-88fa68203799_1600x900.png" rel="rel" target="_blank"&gt;&lt;div class="image2-inset"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="alt" class="sizing-normal" height="819" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda24a5e2-92d6-490e-b74f-88fa68203799_1600x900.png" width="1456" /&gt;&lt;/div&gt;&lt;/a&gt;&lt;figcaption class="image-caption"&gt;&lt;span&gt;Google showcasing a video generated with Veo 3. (&lt;/span&gt;&lt;a href="https://www.axios.com/2025/05/23/google-ai-videos-veo-3" rel="rel"&gt;Source&lt;/a&gt;&lt;span&gt;)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;p&gt;&lt;span&gt;If you just look at benchmarks, Veo 3 is a substantial improvement over other systems. But relative benchmark improvement only tells part of the story—the absolute capabilities of systems ultimately determine their usefulness. Veo 3 looks like a marked qualitative improvement over other models—it generates video and audio with extreme faithfulness, and we recommend you &lt;/span&gt;&lt;a href="https://x.com/HashemGhaili/status/1925332319604257203" rel="rel"&gt;see&lt;/a&gt;&lt;span&gt; &lt;/span&gt;&lt;a href="https://x.com/minchoi/status/1925387367806115943" rel="rel"&gt;some&lt;/a&gt;&lt;span&gt; &lt;/span&gt;&lt;a href="https://x.com/laszlogaal_/status/1925094336200573225" rel="rel"&gt;examples&lt;/a&gt;&lt;span&gt; for yourself. Veo 3 may represent the point video generation crosses the line between being an interesting toy and being genuinely useful.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Other announcements at I/O 2025&lt;/strong&gt;&lt;span&gt;. Other highlights from the conference include:&lt;/span&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025/" rel="rel"&gt;Gemini 2.5&lt;/a&gt;&lt;span&gt; Pro now leads LMArena and WebDev Arena. Deep Think mode, a reasoning feature that scored 49.4% on the USA Mathematical Olympiad 2025 (more than twice OpenAI’s o3, which scored 21.7%). Gemini 2.5 Flash now performs better across reasoning, multimodality, code, and long context while becoming 20-30% more efficient in token usage.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://blog.google/technology/google-deepmind/gemini-diffusion/" rel="rel"&gt;Gemini Diffusion&lt;/a&gt;&lt;span&gt;, an experimental (non-frontier) text diffusion model, delivers output 4-5 times faster than comparable models while rivaling the performance of models twice its size. Most LLMs are autoregressive models, which generate one token at a time—in contrast, diffusion models generate an entire response at once.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Google also announced &lt;/span&gt;&lt;a href="https://developers.googleblog.com/en/introducing-gemma-3n/" rel="rel"&gt;Gemma 3n&lt;/a&gt;&lt;span&gt;, an open model small enough to run on mobile devices, a public beta for Google’s autonomous coding agent &lt;/span&gt;&lt;a href="https://blog.google/technology/google-labs/jules/" rel="rel"&gt;Jules&lt;/a&gt;&lt;span&gt;, a new &lt;/span&gt;&lt;a href="https://blog.google/products/search/google-search-ai-mode-update/#ai-mode-search" rel="rel"&gt;AI search&lt;/a&gt;&lt;span&gt; feature, an &lt;/span&gt;&lt;a href="https://blog.google/technology/ai/google-synthid-ai-content-detector/" rel="rel"&gt;AI watermarker&lt;/a&gt;&lt;span&gt; that identifies content generated by Google’s systems, and more.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;AI is here to stay.&lt;/strong&gt;&lt;span&gt; AI use is sometimes driven by trends—for example, &lt;/span&gt;&lt;a href="https://x.com/sama/status/1906771292390666325" rel="rel"&gt;ChatGPT added a million users in an hour&lt;/a&gt;&lt;span&gt; during the ‘Ghiblification’ craze. However, as AI systems become genuinely useful across more tasks, they will become ubiquitous and enduring. Google’s Gemini app now has &lt;/span&gt;&lt;a href="https://blog.google/technology/ai/io-2025-keynote/" rel="rel"&gt;400M monthly active users&lt;/a&gt;&lt;span&gt;, and its AI products now process over 480 trillion tokens a month—up from 9.7 trillion last year.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Last week, Anthropic released Claude Opus 4 and Claude Sonnet 4. Both exhibit broadly frontier performance, and lead the field on coding benchmarks. Claude Opus 4 is also Anthropic’s first model to meet &lt;/span&gt;&lt;a href="https://time.com/7287806/anthropic-claude-4-opus-safety-bio-risk/" rel="rel"&gt;its ASL-3 safety measure&lt;/a&gt;&lt;span&gt;, which designates models that pose substantial risk. However, Anthropic rolled back several safety and security commitments prior to releasing Opus 4, demonstrating that voluntary governance is not to be relied on.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Opus 4 exhibits hazardous dual-use capabilities.&lt;/strong&gt;&lt;span&gt; In one result from its &lt;/span&gt;&lt;a href="https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf" rel="rel"&gt;system card&lt;/a&gt;&lt;span&gt;, Opus 4 provides a clear uplift in trials measuring its ability to help malicious actors acquire biological weapons.&lt;/span&gt;&lt;/p&gt;&lt;div class="captioned-image-container"&gt;&lt;figure&gt;&lt;a class="image-link image2 is-viewable-img" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad471014-fe58-4180-a67a-9b48862263b9_1600x602.png" rel="rel" target="_blank"&gt;&lt;div class="image2-inset"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="alt" class="sizing-normal" height="548" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad471014-fe58-4180-a67a-9b48862263b9_1600x602.png" width="1456" /&gt;&lt;/div&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;p&gt;&lt;span&gt;Anthropic’s Chief Scientist Jarad Kaplan &lt;/span&gt;&lt;a href="https://time.com/7287806/anthropic-claude-4-opus-safety-bio-risk/" rel="rel"&gt;told TIME&lt;/a&gt;&lt;span&gt; that malicious actors could use Opus 4 to “try to synthesize something like COVID or a more dangerous version of the flu—and basically, our modeling suggests that this might be possible.” It’s not just Opus 4: &lt;/span&gt;&lt;a href="https://www.ai-frontiers.org/articles/ais-are-disseminating-expert-level-virology-skills" rel="rel"&gt;several frontier models outperform human experts in dual-use virology tests&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;The system card also reports that Apollo Research found an early Claude Opus 4 version exhibited "scheming and deception," advising against its release. Anthropic says it implemented internal fixes; however, &lt;/span&gt;&lt;a href="https://www.youtube.com/watch?v=Xn_5aIhrJOE&amp;amp;t=510s" rel="rel"&gt;it doesn’t appear that&lt;/a&gt;&lt;span&gt; Anthropic had Apollo Research re-evaluate the final, released version.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Anthropic’s safety protections may be insufficient.&lt;/strong&gt;&lt;span&gt; In light of Opus 4’s dangerous capabilities, Anthropic rolled out &lt;/span&gt;&lt;a href="https://www.anthropic.com/news/activating-asl3-protections" rel="rel"&gt;ASL-3 safety protections&lt;/a&gt;&lt;span&gt;. However, early public response to Opus 4 indicates that those protections might be insufficient. For example, one researcher showed that Claude Opus 4's WMD safeguards can be bypassed to generate &lt;/span&gt;&lt;a href="https://x.com/ARGleave/status/1926138376509440433" rel="rel"&gt;over 15 pages of detailed instructions for producing sarin gas&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Anthropic walked back safety and security commitments prior to Opus 4’s release.&lt;/strong&gt;&lt;span&gt; Anthropic has also faced criticism for walking back safety commitments prior to Opus 4’s release. For example, Anthropic’s &lt;/span&gt;&lt;a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy" rel="rel"&gt;September 2023 Responsible Scaling Policy&lt;/a&gt;&lt;span&gt; (RSP) committed to define detailed ASL-4 "warning sign evaluations" before their systems reached ASL-3 capabilities; however, it &lt;/span&gt;&lt;a href="https://www.obsolete.pub/p/exclusive-anthropic-is-quietly-backpedalling" rel="rel"&gt;hadn’t done so at the time of Opus 4’s release&lt;/a&gt;&lt;span&gt;. This is because Anthropic redlined that requirement in an &lt;/span&gt;&lt;a href="https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy" rel="rel"&gt;October 2024 revision&lt;/a&gt;&lt;span&gt; to its RSP.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Anthropic also &lt;/span&gt;&lt;a href="https://x.com/RyanPGreenblatt/status/1925992239332724921" rel="rel"&gt;weakened its ASL-3 security requirements&lt;/a&gt;&lt;span&gt; shortly before Opus 4's ASL-3 announcement, specifically no longer requiring robustness against employees stealing model weights if they already had access to "systems that process model weights."&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Voluntary governance is fragile.&lt;/strong&gt;&lt;span&gt; Whether or not Anthropic’s changes to its safety and security policies are justified, voluntary commitments are not sufficient to ensure model releases are safe. There’s nothing stopping Anthropic or other AI companies from walking back critical commitments in the face of competitive pressure to rush releases.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Government&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;JD Vance discussed why he’s worried about AI in &lt;/span&gt;&lt;a href="https://www.nytimes.com/2025/05/21/opinion/jd-vance-pope-trump-immigration.html" rel="rel"&gt;a recent interview&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;A judge &lt;/span&gt;&lt;a href="https://www.transparencycoalition.ai/news/important-early-ruling-in-characterai-case-this-chatbot-is-a-product-not-speech" rel="rel"&gt;ruled&lt;/a&gt;&lt;span&gt; that Character.AI is a product for the purposes of product liability in a lawsuit over a boy’s suicide after interacting with a Character.AI chatbot.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Industry&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;OpenAI &lt;/span&gt;&lt;a href="https://www.npr.org/2025/05/22/nx-s1-5407548/openai-jony-ive-io-deal-ai-devices" rel="rel"&gt;bought&lt;/a&gt;&lt;span&gt; iPhone designer Jony Ive’s startup, io, for $6.5 billion.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Civil Society&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Peter N. Salib and Simon Goldstein &lt;/span&gt;&lt;a href="https://www.ai-frontiers.org/articles/todays-ais-arent-paperclip-maximizers" rel="rel"&gt;argue&lt;/a&gt;&lt;span&gt; that today’s AI systems aren’t paperclip maximizers.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Devid Kirichenko &lt;/span&gt;&lt;a href="https://www.ai-frontiers.org/articles/how-ai-is-eroding-the-norms-of-war" rel="rel"&gt;writes&lt;/a&gt;&lt;span&gt; about how drones are eroding the norms of war.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;ARC Prize released a new reasoning benchmark, &lt;/span&gt;&lt;a href="https://arcprize.org/blog/arc-agi-2-technical-report" rel="rel"&gt;ARC-AGI-2&lt;/a&gt;&lt;span&gt;, on which frontier reasoning models score in low single-digits.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;CSET is &lt;/span&gt;&lt;a href="https://cset.georgetown.edu/wp-content/uploads/FRG-Call-for-Research-Ideas-Internal-Deployment.pdf" rel="rel"&gt;funding&lt;/a&gt;&lt;span&gt; research on risks from internal deployment of frontier AI models.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;A new &lt;/span&gt;&lt;a href="https://arxiv.org/abs/2505.09662" rel="rel"&gt;paper&lt;/a&gt;&lt;span&gt; found that Claude Sonnet 3.5 is significantly more persuasive than humans.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;An Axios &lt;/span&gt;&lt;a href="https://www.axios.com/newsletters/axios-ai-plus-62025700-399b-11f0-b37f-b73dfdd12f1d.html?stream=top" rel="rel"&gt;poll&lt;/a&gt;&lt;span&gt; found that 77% of Americans want AI companies to slow down.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;See also: &lt;/span&gt;&lt;a href="https://x.com/ai_risks?lang=en" rel="rel"&gt;CAIS’ X account&lt;/a&gt;&lt;span&gt;, our paper on &lt;/span&gt;&lt;a href="https://www.nationalsecurity.ai/" rel="rel"&gt;superintelligence strategy&lt;/a&gt;&lt;span&gt;, our &lt;/span&gt;&lt;a href="https://www.aisafetybook.com/" rel="rel"&gt;AI safety course&lt;/a&gt;&lt;span&gt;, and &lt;/span&gt;&lt;a href="http://ai-frontiers.org/" rel="rel"&gt;AI Frontiers&lt;/a&gt;&lt;span&gt;, a new platform for expert commentary and analysis.&lt;/span&gt;&lt;/p&gt;&lt;p class="button-wrapper"&gt;&lt;a class="button primary" href="https://newsletter.safe.ai/p/ai-safety-newsletter-56-google-releases?utm_source=substack&amp;amp;utm_medium=email&amp;amp;utm_content=share&amp;amp;action=share" rel="rel"&gt;&lt;span&gt;Share&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://newsletter.safe.ai/p/ai-safety-newsletter-56-google-releases</guid><pubDate>Wed, 28 May 2025 15:02:07 +0000</pubDate></item><item><title>An anomaly detection framework anyone can use (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/anomaly-detection-framework-anyone-can-use-sarah-alnegheimish-0528</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202505/mit-Sarah-Abdulaziz-Alnegheimish.JPG" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Sarah Alnegheimish’s research interests reside at the intersection of machine learning and systems engineering. Her objective: to make machine learning systems more accessible, transparent, and trustworthy.&lt;/p&gt;&lt;p&gt;Alnegheimish is a PhD student in Principal Research Scientist Kalyan Veeramachaneni’s Data-to-AI group in MIT’s Laboratory for Information and Decision Systems (LIDS). Here, she commits most of her energy to developing Orion, an open-source, user-friendly machine learning framework and time series library that is capable of detecting anomalies without supervision in large-scale industrial and operational settings.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Early influence&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The daughter of a university professor and a teacher educator, she learned from an early age that knowledge was meant to be shared freely. “I think growing up in a home where education was highly valued is part of why I want to make machine learning tools accessible.” Alnegheimish’s own personal experience with open-source resources only increased her motivation. “I learned to view accessibility as the key to adoption. To strive for impact, new technology needs to be accessed and assessed by those who need it. That’s the whole purpose of doing open-source development.”&lt;/p&gt;&lt;p&gt;Alnegheimish earned her bachelor’s degree at King Saud University (KSU). “I was in the first cohort of computer science majors. Before this program was created, the only other available major in computing was IT [information technology].” Being a part of the first cohort was exciting, but it brought its own unique challenges. “All of the faculty were teaching new material. Succeeding required an independent learning experience. That’s when I first time came across MIT OpenCourseWare: as a resource to teach myself.”&lt;/p&gt;&lt;p&gt;Shortly after graduating, Alnegheimish became a researcher at the King Abdulaziz City for Science and Technology (KACST), Saudi Arabia’s national lab. Through the Center for Complex Engineering Systems (CCES) at KACST and MIT, she began conducting research with Veeramachaneni. When she applied to MIT for graduate school, his research group was her top choice.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Creating Orion&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Alnegheimish’s master thesis focused on time series anomaly detection — the identification of unexpected behaviors or patterns in data, which can provide users crucial information. For example, unusual patterns in network traffic data can be a sign of cybersecurity threats, abnormal sensor readings in heavy machinery can predict potential future failures, and monitoring patient vital signs can help reduce health complications. It was through her master’s research that Alnegheimish first began designing Orion.&lt;/p&gt;&lt;p&gt;Orion uses statistical and machine learning-based models that are continuously logged and maintained. Users do not need to be machine learning experts to utilize the code. They can analyze signals, compare anomaly detection methods, and investigate anomalies in an end-to-end program. The framework, code, and datasets are all open-sourced.&lt;/p&gt;&lt;p&gt;“With open source, accessibility and transparency are directly achieved. You have unrestricted access to the code, where you can investigate how the model works through understanding the code. We have increased transparency with Orion: We label every step in the model and present it to the user.” Alnegheimish says that this transparency helps enable users to begin trusting the model before they ultimately see for themselves how reliable it is.&lt;/p&gt;&lt;p&gt;“We’re trying to take all these machine learning algorithms and put them in one place so anyone can use our models off-the-shelf,” she says. “It’s not just for the sponsors that we work with at MIT. It’s being used by a lot of public users. They come to the library, install it, and run it on their data. It’s proving itself to be a great source for people to find some of the latest methods for anomaly detection.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Repurposing models for anomaly detection&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In her PhD, Alnegheimish is further exploring innovative ways to do anomaly detection using Orion. “When I first started my research, all machine-learning models needed to be trained from scratch on your data. Now we’re in a time where we can use pre-trained models,” she says. Working with pre-trained models saves time and computational costs. The challenge, though, is that time series anomaly detection is a brand-new task for them. “In their original sense, these models have been trained to forecast, but not to find anomalies,” Alnegheimish says. “We’re pushing their boundaries through prompt-engineering, without any additional training.”&lt;/p&gt;&lt;p&gt;Because these models already capture the patterns of time-series data, Alnegheimish believes they already have everything they need to enable them to detect anomalies. So far, her current results support this theory. They don’t surpass the success rate of models that are independently trained on specific data, but she believes they will one day.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Accessible design&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Alnegheimish talks at length about the efforts she’s gone through to make Orion more accessible. “Before I came to MIT, I used to think that the crucial part of research was to develop the machine learning model itself or improve on its current state. With time, I realized that the only way you can make your research accessible and adaptable for others is to develop systems that make them accessible. During my graduate studies, I’ve taken the approach of developing my models and systems in tandem.”&lt;/p&gt;&lt;p&gt;The key element to her system development was finding the right abstractions to work with her models. These abstractions provide universal representation for all models with simplified components. “Any model will have a sequence of steps to go from raw input to desired output.&amp;nbsp; We’ve standardized the input and output, which allows the middle to be flexible and fluid. So far, all the models we’ve run have been able to retrofit into our abstractions.” The abstractions she uses have been stable and reliable for the last six years.&lt;/p&gt;&lt;p&gt;The value of simultaneously building systems and models can be seen in Alnegheimish’s work as a mentor. She had the opportunity to work with two master’s students earning their engineering degrees. “All I showed them was the system itself and the documentation of how to use it. Both students were able to develop their own models with the abstractions we’re conforming to. It reaffirmed that we’re taking the right path.”&lt;/p&gt;&lt;p&gt;Alnegheimish also investigated whether a large language model (LLM) could be used as a mediator between users and a system. The LLM agent she has implemented is able to connect to Orion without users needing to know the small details of how Orion works. “Think of ChatGPT. You have no idea what the model is behind it, but it’s very accessible to everyone.” For her software, users only know two commands: Fit and Detect. Fit allows users to train their model, while Detect enables them to detect anomalies.&lt;/p&gt;&lt;p&gt;“The ultimate goal of what I’ve tried to do is make AI more accessible to everyone,” she says.&amp;nbsp;So far, Orion has reached over 120,000 downloads, and over a thousand users have marked the repository as one of their favorites on Github. “Traditionally, you used to measure the impact of research through citations and paper publications. Now you get real-time adoption through open source.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202505/mit-Sarah-Abdulaziz-Alnegheimish.JPG" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Sarah Alnegheimish’s research interests reside at the intersection of machine learning and systems engineering. Her objective: to make machine learning systems more accessible, transparent, and trustworthy.&lt;/p&gt;&lt;p&gt;Alnegheimish is a PhD student in Principal Research Scientist Kalyan Veeramachaneni’s Data-to-AI group in MIT’s Laboratory for Information and Decision Systems (LIDS). Here, she commits most of her energy to developing Orion, an open-source, user-friendly machine learning framework and time series library that is capable of detecting anomalies without supervision in large-scale industrial and operational settings.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Early influence&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The daughter of a university professor and a teacher educator, she learned from an early age that knowledge was meant to be shared freely. “I think growing up in a home where education was highly valued is part of why I want to make machine learning tools accessible.” Alnegheimish’s own personal experience with open-source resources only increased her motivation. “I learned to view accessibility as the key to adoption. To strive for impact, new technology needs to be accessed and assessed by those who need it. That’s the whole purpose of doing open-source development.”&lt;/p&gt;&lt;p&gt;Alnegheimish earned her bachelor’s degree at King Saud University (KSU). “I was in the first cohort of computer science majors. Before this program was created, the only other available major in computing was IT [information technology].” Being a part of the first cohort was exciting, but it brought its own unique challenges. “All of the faculty were teaching new material. Succeeding required an independent learning experience. That’s when I first time came across MIT OpenCourseWare: as a resource to teach myself.”&lt;/p&gt;&lt;p&gt;Shortly after graduating, Alnegheimish became a researcher at the King Abdulaziz City for Science and Technology (KACST), Saudi Arabia’s national lab. Through the Center for Complex Engineering Systems (CCES) at KACST and MIT, she began conducting research with Veeramachaneni. When she applied to MIT for graduate school, his research group was her top choice.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Creating Orion&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Alnegheimish’s master thesis focused on time series anomaly detection — the identification of unexpected behaviors or patterns in data, which can provide users crucial information. For example, unusual patterns in network traffic data can be a sign of cybersecurity threats, abnormal sensor readings in heavy machinery can predict potential future failures, and monitoring patient vital signs can help reduce health complications. It was through her master’s research that Alnegheimish first began designing Orion.&lt;/p&gt;&lt;p&gt;Orion uses statistical and machine learning-based models that are continuously logged and maintained. Users do not need to be machine learning experts to utilize the code. They can analyze signals, compare anomaly detection methods, and investigate anomalies in an end-to-end program. The framework, code, and datasets are all open-sourced.&lt;/p&gt;&lt;p&gt;“With open source, accessibility and transparency are directly achieved. You have unrestricted access to the code, where you can investigate how the model works through understanding the code. We have increased transparency with Orion: We label every step in the model and present it to the user.” Alnegheimish says that this transparency helps enable users to begin trusting the model before they ultimately see for themselves how reliable it is.&lt;/p&gt;&lt;p&gt;“We’re trying to take all these machine learning algorithms and put them in one place so anyone can use our models off-the-shelf,” she says. “It’s not just for the sponsors that we work with at MIT. It’s being used by a lot of public users. They come to the library, install it, and run it on their data. It’s proving itself to be a great source for people to find some of the latest methods for anomaly detection.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Repurposing models for anomaly detection&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In her PhD, Alnegheimish is further exploring innovative ways to do anomaly detection using Orion. “When I first started my research, all machine-learning models needed to be trained from scratch on your data. Now we’re in a time where we can use pre-trained models,” she says. Working with pre-trained models saves time and computational costs. The challenge, though, is that time series anomaly detection is a brand-new task for them. “In their original sense, these models have been trained to forecast, but not to find anomalies,” Alnegheimish says. “We’re pushing their boundaries through prompt-engineering, without any additional training.”&lt;/p&gt;&lt;p&gt;Because these models already capture the patterns of time-series data, Alnegheimish believes they already have everything they need to enable them to detect anomalies. So far, her current results support this theory. They don’t surpass the success rate of models that are independently trained on specific data, but she believes they will one day.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Accessible design&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Alnegheimish talks at length about the efforts she’s gone through to make Orion more accessible. “Before I came to MIT, I used to think that the crucial part of research was to develop the machine learning model itself or improve on its current state. With time, I realized that the only way you can make your research accessible and adaptable for others is to develop systems that make them accessible. During my graduate studies, I’ve taken the approach of developing my models and systems in tandem.”&lt;/p&gt;&lt;p&gt;The key element to her system development was finding the right abstractions to work with her models. These abstractions provide universal representation for all models with simplified components. “Any model will have a sequence of steps to go from raw input to desired output.&amp;nbsp; We’ve standardized the input and output, which allows the middle to be flexible and fluid. So far, all the models we’ve run have been able to retrofit into our abstractions.” The abstractions she uses have been stable and reliable for the last six years.&lt;/p&gt;&lt;p&gt;The value of simultaneously building systems and models can be seen in Alnegheimish’s work as a mentor. She had the opportunity to work with two master’s students earning their engineering degrees. “All I showed them was the system itself and the documentation of how to use it. Both students were able to develop their own models with the abstractions we’re conforming to. It reaffirmed that we’re taking the right path.”&lt;/p&gt;&lt;p&gt;Alnegheimish also investigated whether a large language model (LLM) could be used as a mediator between users and a system. The LLM agent she has implemented is able to connect to Orion without users needing to know the small details of how Orion works. “Think of ChatGPT. You have no idea what the model is behind it, but it’s very accessible to everyone.” For her software, users only know two commands: Fit and Detect. Fit allows users to train their model, while Detect enables them to detect anomalies.&lt;/p&gt;&lt;p&gt;“The ultimate goal of what I’ve tried to do is make AI more accessible to everyone,” she says.&amp;nbsp;So far, Orion has reached over 120,000 downloads, and over a thousand users have marked the repository as one of their favorites on Github. “Traditionally, you used to measure the impact of research through citations and paper publications. Now you get real-time adoption through open source.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/anomaly-detection-framework-anyone-can-use-sarah-alnegheimish-0528</guid><pubDate>Wed, 28 May 2025 20:00:00 +0000</pubDate></item><item><title>Rationale engineering generates a compact new tool for gene therapy (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/rationale-engineering-generates-compact-new-tool-gene-therapy-0528</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202505/phylogenetic-tree.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Scientists at the McGovern Institute for Brain Research at MIT and the Broad Institute of MIT and Harvard have re-engineered a compact RNA-guided enzyme they found in bacteria into an efficient, programmable editor of human DNA.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The protein they created, called NovaIscB, can be adapted to make precise changes to the genetic code, modulate the activity of specific genes, or carry out other editing tasks. Because its small size simplifies delivery to cells, NovaIscB’s developers say it is a promising candidate for developing gene therapies to treat or prevent disease.&lt;/p&gt;&lt;p&gt;The study was led by Feng Zhang, the James and Patricia Poitras Professor of Neuroscience at MIT who is also an investigator at the McGovern Institute and the Howard Hughes Medical Institute, and a core member of the Broad Institute. Zhang and his team reported their open-access work this month in the journal &lt;em&gt;Nature Biotechnology&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;NovaIscB is derived from a bacterial DNA cutter that belongs to a family of proteins called IscBs, which Zhang’s lab discovered in 2021. IscBs are a type of OMEGA system, the evolutionary ancestors to Cas9, which is part of the bacterial CRISPR system that Zhang and others have developed into powerful genome-editing tools. Like Cas9, IscB enzymes cut DNA at sites specified by an RNA guide. By reprogramming that guide, researchers can redirect the enzymes to target sequences of their choosing.&lt;/p&gt;&lt;p&gt;IscBs had caught the team’s attention not only because they share key features of CRISPR’s DNA-cutting Cas9, but also because they are a third of its size. That would be an advantage for potential gene therapies: compact tools are easier to deliver to cells, and with a small enzyme, researchers would have more flexibility to tinker, potentially adding new functionalities without creating tools that were too bulky for clinical use.&lt;/p&gt;&lt;p&gt;From their initial studies of IscBs, researchers in Zhang’s lab knew that some members of the family could cut DNA targets in human cells. None of the bacterial proteins worked well enough to be deployed therapeutically, however: the team would have to modify an IscB to ensure it could edit targets in human cells efficiently without disturbing the rest of the genome.&lt;/p&gt;&lt;p&gt;To begin that engineering process, Soumya Kannan, a graduate student in Zhang’s lab who is now a junior fellow at the Harvard Society of Fellows, and postdoc Shiyou Zhu first searched for an IscB that would make good starting point. They tested nearly 400 different IscB enzymes that can be found in bacteria. Ten were capable of editing DNA in human cells.&lt;/p&gt;&lt;p&gt;Even the most active of those would need to be enhanced to make it a useful genome editing tool. The challenge would be increasing the enzyme’s activity, but only at the sequences specified by its RNA guide. If the enzyme became more active, but indiscriminately so, it would cut DNA in unintended places. “The key is to balance the improvement of both activity and specificity at the same time,” explains Zhu.&lt;/p&gt;&lt;p&gt;Zhu notes that bacterial IscBs are directed to their target sequences by relatively short RNA guides, which makes it difficult to restrict the enzyme’s activity to a specific part of the genome. If an IscB could be engineered to accommodate a longer guide, it would be less likely to act on sequences beyond its intended target.&lt;/p&gt;&lt;p&gt;To optimize IscB for human genome editing, the team leveraged information that graduate student Han Altae-Tran, who is now a postdoc at the University of Washington, had learned about the diversity of bacterial IscBs and how they evolved. For instance, the researchers noted that IscBs that worked in human cells included a segment they called REC, which was absent in other IscBs. They suspected the enzyme might need that segment to interact with the DNA in human cells. When they took a closer look at the region, structural modeling suggested that by slightly expanding part of the protein, REC might also enable IscBs to recognize longer RNA guides.&lt;/p&gt;&lt;p&gt;Based on these observations, the team experimented with swapping in parts of REC domains from different IscBs and Cas9s, evaluating how each change impacted the protein’s function. Guided by their understanding of how IscBs and Cas9s interact with both DNA and their RNA guides, the researchers made additional changes, aiming to optimize both efficiency and specificity.&lt;/p&gt;&lt;p&gt;In the end, they generated a protein they called NovaIscB, which was over 100 times more active in human cells than the IscB they had started with, and that had demonstrated good specificity for its targets.&lt;/p&gt;&lt;p&gt;Kannan and Zhu constructed and screened hundreds of new IscBs before arriving at NovaIscB — and every change they made to the original protein was strategic. Their efforts were guided by their team’s knowledge of IscBs’s natural evolution, as well as predictions of how each alteration would impact the protein’s structure, made using an artificial intelligence tool called AlphaFold2. Compared to traditional methods of introducing random changes into a protein and screening for their effects, this rational engineering approach greatly accelerated the team’s ability to identify a protein with the features they were looking for.&lt;/p&gt;&lt;p&gt;The team demonstrated that NovaIscB is a good scaffold for a variety of genome editing tools. “It biochemically functions very similarly to Cas9, and that makes it easy to port over tools that were already optimized with the Cas9 scaffold,” Kannan says. With different modifications, the researchers used NovaIscB to replace specific letters of the DNA code in human cells and to change the activity of targeted genes.&lt;/p&gt;&lt;p&gt;Importantly, the NovaIscB-based tools are compact enough to be easily packaged inside a single adeno-associated virus (AAV) — the vector most commonly used to safely deliver gene therapy to patients. Because they are bulkier, tools developed using Cas9 can require a more complicated delivery strategy.&lt;/p&gt;&lt;p&gt;Demonstrating NovaIscB’s potential for therapeutic use, Zhang’s team created a tool called OMEGAoff that adds chemical markers to DNA to dial down the activity of specific genes. They programmed OMEGAoff to repress a gene involved in cholesterol regulation, then used AAV to deliver the system to the livers of mice, leading to lasting reductions in cholesterol levels in the animals’ blood.&lt;/p&gt;&lt;p&gt;The team expects that NovaIscB can be used to target genome editing tools to most human genes, and look forward to seeing how other labs deploy the new technology. They also hope others will adopt their evolution-guided approach to rational protein engineering. “Nature has such diversity, and its systems have different advantages and disadvantages,” Zhu says. “By learning about that natural diversity, we can make the systems we are trying to engineer better and better.”&lt;/p&gt;&lt;p&gt;This study was funded, in part, by the K. Lisa Yang and Hock E. Tan Center for Molecular Therapeutics at MIT, Broad Institute Programmable Therapeutics Gift Donors, Pershing Square Foundation, William Ackman, Neri Oxman, the Phillips family, and J. and P. Poitras.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202505/phylogenetic-tree.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Scientists at the McGovern Institute for Brain Research at MIT and the Broad Institute of MIT and Harvard have re-engineered a compact RNA-guided enzyme they found in bacteria into an efficient, programmable editor of human DNA.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The protein they created, called NovaIscB, can be adapted to make precise changes to the genetic code, modulate the activity of specific genes, or carry out other editing tasks. Because its small size simplifies delivery to cells, NovaIscB’s developers say it is a promising candidate for developing gene therapies to treat or prevent disease.&lt;/p&gt;&lt;p&gt;The study was led by Feng Zhang, the James and Patricia Poitras Professor of Neuroscience at MIT who is also an investigator at the McGovern Institute and the Howard Hughes Medical Institute, and a core member of the Broad Institute. Zhang and his team reported their open-access work this month in the journal &lt;em&gt;Nature Biotechnology&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;NovaIscB is derived from a bacterial DNA cutter that belongs to a family of proteins called IscBs, which Zhang’s lab discovered in 2021. IscBs are a type of OMEGA system, the evolutionary ancestors to Cas9, which is part of the bacterial CRISPR system that Zhang and others have developed into powerful genome-editing tools. Like Cas9, IscB enzymes cut DNA at sites specified by an RNA guide. By reprogramming that guide, researchers can redirect the enzymes to target sequences of their choosing.&lt;/p&gt;&lt;p&gt;IscBs had caught the team’s attention not only because they share key features of CRISPR’s DNA-cutting Cas9, but also because they are a third of its size. That would be an advantage for potential gene therapies: compact tools are easier to deliver to cells, and with a small enzyme, researchers would have more flexibility to tinker, potentially adding new functionalities without creating tools that were too bulky for clinical use.&lt;/p&gt;&lt;p&gt;From their initial studies of IscBs, researchers in Zhang’s lab knew that some members of the family could cut DNA targets in human cells. None of the bacterial proteins worked well enough to be deployed therapeutically, however: the team would have to modify an IscB to ensure it could edit targets in human cells efficiently without disturbing the rest of the genome.&lt;/p&gt;&lt;p&gt;To begin that engineering process, Soumya Kannan, a graduate student in Zhang’s lab who is now a junior fellow at the Harvard Society of Fellows, and postdoc Shiyou Zhu first searched for an IscB that would make good starting point. They tested nearly 400 different IscB enzymes that can be found in bacteria. Ten were capable of editing DNA in human cells.&lt;/p&gt;&lt;p&gt;Even the most active of those would need to be enhanced to make it a useful genome editing tool. The challenge would be increasing the enzyme’s activity, but only at the sequences specified by its RNA guide. If the enzyme became more active, but indiscriminately so, it would cut DNA in unintended places. “The key is to balance the improvement of both activity and specificity at the same time,” explains Zhu.&lt;/p&gt;&lt;p&gt;Zhu notes that bacterial IscBs are directed to their target sequences by relatively short RNA guides, which makes it difficult to restrict the enzyme’s activity to a specific part of the genome. If an IscB could be engineered to accommodate a longer guide, it would be less likely to act on sequences beyond its intended target.&lt;/p&gt;&lt;p&gt;To optimize IscB for human genome editing, the team leveraged information that graduate student Han Altae-Tran, who is now a postdoc at the University of Washington, had learned about the diversity of bacterial IscBs and how they evolved. For instance, the researchers noted that IscBs that worked in human cells included a segment they called REC, which was absent in other IscBs. They suspected the enzyme might need that segment to interact with the DNA in human cells. When they took a closer look at the region, structural modeling suggested that by slightly expanding part of the protein, REC might also enable IscBs to recognize longer RNA guides.&lt;/p&gt;&lt;p&gt;Based on these observations, the team experimented with swapping in parts of REC domains from different IscBs and Cas9s, evaluating how each change impacted the protein’s function. Guided by their understanding of how IscBs and Cas9s interact with both DNA and their RNA guides, the researchers made additional changes, aiming to optimize both efficiency and specificity.&lt;/p&gt;&lt;p&gt;In the end, they generated a protein they called NovaIscB, which was over 100 times more active in human cells than the IscB they had started with, and that had demonstrated good specificity for its targets.&lt;/p&gt;&lt;p&gt;Kannan and Zhu constructed and screened hundreds of new IscBs before arriving at NovaIscB — and every change they made to the original protein was strategic. Their efforts were guided by their team’s knowledge of IscBs’s natural evolution, as well as predictions of how each alteration would impact the protein’s structure, made using an artificial intelligence tool called AlphaFold2. Compared to traditional methods of introducing random changes into a protein and screening for their effects, this rational engineering approach greatly accelerated the team’s ability to identify a protein with the features they were looking for.&lt;/p&gt;&lt;p&gt;The team demonstrated that NovaIscB is a good scaffold for a variety of genome editing tools. “It biochemically functions very similarly to Cas9, and that makes it easy to port over tools that were already optimized with the Cas9 scaffold,” Kannan says. With different modifications, the researchers used NovaIscB to replace specific letters of the DNA code in human cells and to change the activity of targeted genes.&lt;/p&gt;&lt;p&gt;Importantly, the NovaIscB-based tools are compact enough to be easily packaged inside a single adeno-associated virus (AAV) — the vector most commonly used to safely deliver gene therapy to patients. Because they are bulkier, tools developed using Cas9 can require a more complicated delivery strategy.&lt;/p&gt;&lt;p&gt;Demonstrating NovaIscB’s potential for therapeutic use, Zhang’s team created a tool called OMEGAoff that adds chemical markers to DNA to dial down the activity of specific genes. They programmed OMEGAoff to repress a gene involved in cholesterol regulation, then used AAV to deliver the system to the livers of mice, leading to lasting reductions in cholesterol levels in the animals’ blood.&lt;/p&gt;&lt;p&gt;The team expects that NovaIscB can be used to target genome editing tools to most human genes, and look forward to seeing how other labs deploy the new technology. They also hope others will adopt their evolution-guided approach to rational protein engineering. “Nature has such diversity, and its systems have different advantages and disadvantages,” Zhu says. “By learning about that natural diversity, we can make the systems we are trying to engineer better and better.”&lt;/p&gt;&lt;p&gt;This study was funded, in part, by the K. Lisa Yang and Hock E. Tan Center for Molecular Therapeutics at MIT, Broad Institute Programmable Therapeutics Gift Donors, Pershing Square Foundation, William Ackman, Neri Oxman, the Phillips family, and J. and P. Poitras.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/rationale-engineering-generates-compact-new-tool-gene-therapy-0528</guid><pubDate>Wed, 28 May 2025 20:15:00 +0000</pubDate></item><item><title>What AI’s impact on individuals means for the health workforce and industry (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/podcast/what-ais-impact-on-individuals-means-for-the-health-workforce-and-industry/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Illustrated headshots of Azeem Azhar, Peter Lee, and Ethan Mollick." class="wp-image-1140433" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Episode6-PeterEthanAzeem-AIRevolution_Hero_Feature_No_Text_1400x788.jpg" width="1401" /&gt;&lt;/figure&gt;






&lt;p&gt;In November 2022, OpenAI’s ChatGPT kick-started a new era in AI. This was followed less than a half year later by the release of GPT-4. In the months leading up to GPT-4’s public release, Peter Lee, president of Microsoft Research, cowrote a book full of optimism for the potential of advanced AI models to transform the world of healthcare. What has happened since? In this special podcast series, Lee revisits the book, exploring how patients, providers, and other medical professionals are experiencing and using generative AI today while examining what he and his coauthors got right—and what they didn’t foresee.&lt;/p&gt;



&lt;p&gt;In this episode, Ethan Mollick&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and Azeem Azhar&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, thought leaders at the forefront of AI’s impact on work, education, and society, join Lee to discuss how generative AI is reshaping healthcare and organizational systems. Mollick, professor at the Wharton School, discusses the conflicting emotions that come with navigating AI’s effect on the tasks we enjoy and those we don’t; the systemic challenges in AI adoption; and the need for organizations to actively experiment with AI rather than wait for top-down solutions. Azhar, a technology analyst and writer who explores the intersection of AI, economics, and society, explores how generative AI is transforming healthcare through applications like medical scribing, clinician support, and consumer health monitoring.&lt;/p&gt;



&lt;div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow"&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h2 class="wp-block-heading h5" id="learn-more-1"&gt;Learn more:&lt;/h2&gt;




&lt;/div&gt;







&lt;section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast"&gt;
	
&lt;/section&gt;


&lt;div class="wp-block-msr-show-more"&gt;
	&lt;div class="bg-neutral-100 p-5"&gt;
		&lt;div class="show-more-show-less"&gt;
			&lt;div&gt;
				&lt;span&gt;
					

&lt;h2 class="wp-block-heading" id="transcript"&gt;Transcript&lt;/h2&gt;



&lt;p&gt;[MUSIC]  &amp;nbsp;&lt;/p&gt;



&lt;p&gt;[BOOK PASSAGE] &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;PETER LEE:&lt;/strong&gt; “In American primary care, the missing workforce is stunning in magnitude, the shortfall estimated to reach up to 48,000 doctors within the next dozen years. China and other countries with aging populations can expect drastic shortfalls, as well. Just last month, I asked a respected colleague retiring from primary care who he would recommend as a replacement; he told me bluntly that, other than expensive concierge care practices, he could not think of anyone, even for himself. This mismatch between need and supply will only grow, and the US is far from alone among developed countries in facing it.”&lt;/p&gt;



&lt;p&gt;[END OF BOOK PASSAGE]  &amp;nbsp;&lt;/p&gt;



&lt;p&gt;[THEME MUSIC]  &amp;nbsp;&lt;/p&gt;



&lt;p&gt;This is &lt;em&gt;The AI Revolution in Medicine, Revisited&lt;/em&gt;. I’m your host, Peter Lee.  &amp;nbsp;&lt;/p&gt;



&lt;p&gt;Shortly after OpenAI’s GPT-4 was publicly released, Carey Goldberg, Dr. Zak Kohane, and I published &lt;em&gt;The AI Revolution in Medicine &lt;/em&gt;to help educate the world of healthcare and medical research about the transformative impact this new generative AI technology could have. But because we wrote the book when GPT-4 was still a secret, we had to speculate. Now, two years later, what did we get right, and what did we get wrong?   &amp;nbsp;&lt;/p&gt;



&lt;p&gt;In this series, we’ll talk to clinicians, patients, hospital administrators, and others to understand the reality of AI in the field and where we go from here.    &amp;nbsp;&lt;/p&gt;



&lt;p&gt;[THEME MUSIC FADES]&lt;/p&gt;



				&lt;/span&gt;
				&lt;span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1"&gt;
					



&lt;p&gt;The book passage I read at the top is from “Chapter 4: Trust but Verify,” which was written by Zak. &lt;/p&gt;



&lt;p&gt;You know, it’s no secret that in the US and elsewhere shortages in medical staff and the rise of clinician burnout are affecting the quality of patient care for the worse. In our book, we predicted that generative AI would be something that might help address these issues.&lt;/p&gt;



&lt;p&gt;So in this episode, we’ll delve into how individual performance gains that our previous guests have described might affect the healthcare workforce as a whole, and on the patient side, we’ll look into the influence of generative AI on the consumerization of healthcare. Now, since all of this consumes such a huge fraction of the overall economy, we’ll also get into what a general-purpose technology as disruptive as generative AI might mean in the context of labor markets and beyond.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To help us do that, I’m pleased to welcome Ethan Mollick and Azeem Azhar.&lt;/p&gt;



&lt;p&gt;Ethan Mollick is the Ralph J. Roberts Distinguished Faculty Scholar, a Rowan Fellow, and an associate professor at the Wharton School of the University of Pennsylvania. His research into the effects of AI on work, entrepreneurship, and education is applied by organizations around the world, leading him to be named one of &lt;em&gt;Time&lt;/em&gt; magazine’s most influential people in AI for 2024. He’s also the author of the &lt;em&gt;New York Times&lt;/em&gt; best-selling book &lt;em&gt;Co-Intelligence&lt;/em&gt;.&lt;/p&gt;



&lt;p&gt;Azeem Azhar is an author, founder, investor, and one of the most thoughtful and influential voices on the interplay between disruptive emerging technologies and business and society. In his best-selling book, &lt;em&gt;The Exponential Age&lt;/em&gt;, and in his highly regarded newsletter and podcast, &lt;em&gt;Exponential View&lt;/em&gt;, he explores how technologies like AI are reshaping everything from healthcare to geopolitics.&lt;/p&gt;



&lt;p&gt;Ethan and Azeem are two leading thinkers on the ways that disruptive technologies—and especially AI—affect our work, our jobs, our business enterprises, and whole industries. As economists, they are trying to work out whether we are in the midst of an economic revolution as profound as the shift from an agrarian to an industrial society.&lt;/p&gt;



&lt;p&gt;[TRANSITION MUSIC]&lt;/p&gt;



&lt;p&gt;Here is my interview with Ethan Mollick:&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Ethan, welcome.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;ETHAN MOLLICK: &lt;/strong&gt;So happy to be here, thank you.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; I described you as a professor at Wharton, which I think most of the people who listen to this podcast series know of as an elite business school. So it might surprise some people that you study AI. And beyond that, you know, that I would seek you out to talk about AI in medicine. [LAUGHTER] So to get started, how and why did it happen that you’ve become one of &lt;em&gt;the&lt;/em&gt; leading experts on AI?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK:&lt;/strong&gt; It’s actually an interesting story. I’ve been AI-adjacent my whole career. When I was [getting] my PhD at MIT, I worked with Marvin Minsky&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and the MIT [Massachusetts Institute of Technology] Media Labs AI group. But I was never the technical AI guy. I was the person who was trying to explain AI to everybody else who didn’t understand it.&lt;/p&gt;



&lt;p&gt;And then I became very interested in, how do you train and teach? And AI was always a part of that. I was building games for teaching, teaching tools that were used in hospitals and elsewhere, simulations. So when LLMs burst into the scene, I had already been using them and had a good sense of what they could do. And between that and, kind of, being practically oriented and getting some of the first research projects underway, especially under education and AI and performance, I became sort of a go-to person in the field.&lt;/p&gt;



&lt;p&gt;And once you’re in a field where nobody knows what’s going on and we’re all making it up as we go along—I thought it’s funny that you led with the idea that you have a couple of months head start for GPT-4, right. Like that’s all we have at this point, is a few months’ head start. [LAUGHTER] So being a few months ahead is good enough to be an expert at this point. Whether it should be or not is a different question.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Well, if I understand correctly, leading AI companies like OpenAI, Anthropic, and others have now sought you out as someone who should get early access to really start to do early assessments and gauge early reactions. How has that been?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK:&lt;/strong&gt; So, I mean, I think the bigger picture is less about me than about two things that tells us about the state of AI right now.&lt;/p&gt;



&lt;p&gt;One, nobody really knows what’s going on, right. So in a lot of ways, if it wasn’t for your work, Peter, like, I don’t think people would be thinking about medicine as much because these systems weren’t built for medicine. They weren’t built to change education. They weren’t built to write memos. They, like, they weren’t built to do any of these things. They weren’t really built to do anything in particular. It turns out they’re just good at many things.&lt;/p&gt;



&lt;p&gt;And to the extent that the labs work on them, they care about their coding ability above everything else and maybe math and science secondarily. They don’t think about the fact that it expresses high empathy. They don’t think about its accuracy and diagnosis or where it’s inaccurate. They don’t think about how it’s changing education forever.&lt;/p&gt;



&lt;p&gt;So one part of this is the fact that they go to my Twitter feed or ask me for advice is an indicator of where they are, too, which is they’re not thinking about this. And the fact that a few months’ head start continues to give you a lead tells you that we are at the very cutting edge. These labs aren’t sitting on projects for two years and then releasing them. Months after a project is complete or sooner, it’s out the door. Like, there’s very little delay. So we’re kind of all in the same boat here, which is a very unusual space for a new technology.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;And I, you know, explained that you’re at Wharton. Are you an odd fit as a faculty member at Wharton, or is this a trend now even in business schools that AI experts are becoming key members of the faculty?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK: &lt;/strong&gt;I mean, it’s a little of both, right. It’s faculty, so everybody does everything. I’m a professor of innovation-entrepreneurship. I’ve launched startups before and working on that and education means I think about, how do organizations redesign themselves? How do they take advantage of these kinds of problems? So medicine’s always been very central to that, right. A lot of people in my MBA class have been MDs either switching, you know, careers or else looking to advance from being sort of individual contributors to running teams. So I don’t think that’s that bad a fit. But I also think this is general-purpose technology; it’s going to touch everything. The focus on this is medicine, but Microsoft does far more than medicine, right. It’s … there’s transformation happening in literally every field, in every country. This is a widespread effect.&lt;/p&gt;



&lt;p&gt;So I don’t think we should be surprised that business schools matter on this because we care about management. There’s a long tradition of management and medicine going together. There’s actually a great academic paper that shows that teaching hospitals that also have MBA programs associated with them have higher management scores and perform better&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. So I think that these are not as foreign concepts, especially as medicine continues to get more complicated.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah. Well, in fact, I want to dive a little deeper on these issues of management, of entrepreneurship, um, education. But before doing that, if I could just stay focused on you. There is always something interesting to hear from people about their first encounters with AI. And throughout this entire series, I’ve been doing that both &lt;em&gt;pre&lt;/em&gt;-generative AI and post-generative AI. So you, sort of, hinted at the pre-generative AI. You were in Minsky’s lab. Can you say a little bit more about that early encounter? And then tell us about your first encounters with generative AI.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK:&lt;/strong&gt; Yeah. Those are great questions. So first of all, when I was at the media lab, that was pre-the current boom in sort of, you know, even in the old-school machine learning kind of space. So there was a lot of potential directions to head in. While I was there, there were projects underway, for example, to record every interaction small children had. One of the professors was recording everything their baby interacted with in the hope that maybe that would give them a hint about how to build an AI system.&lt;/p&gt;



&lt;p&gt;There was a bunch of projects underway that were about labeling every concept and how they relate to other concepts. So, like, it was very much Wild West of, like, how do we make an AI work—which has been this repeated problem in AI, which is, what is this thing?&lt;/p&gt;



&lt;p&gt;The fact that it was just like brute force over the corpus of all human knowledge turns out to be a little bit of like a, you know, it’s a miracle and a little bit of a disappointment in some ways [LAUGHTER] compared to how elaborate some of this was. So, you know, I think that, that was sort of my first encounters in sort of the intellectual way.&lt;/p&gt;



&lt;p&gt;The generative AI encounters actually started with the original, sort of, GPT-3, or, you know, earlier versions. And it was actually game-based. So I played games like AI Dungeon. And as an educator, I realized, oh my gosh, this stuff could write essays at a fourth-grade level. That’s really going to change the way, like, middle school works, was my thinking at the time. And I was posting about that back in, you know, 2021 that this is a big deal. But I think everybody was taken surprise, including the AI companies themselves, by, you know, ChatGPT, by GPT-3.5. The difference in degree turned out to be a difference in kind.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah, you know, if I think back, even with GPT-3, and certainly this was the case with GPT-2, it was, at least, you know, from where I was sitting, it was hard to get people to really take this seriously and pay attention.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK: &lt;/strong&gt;Yes.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; You know, it’s remarkable. Within Microsoft, I think a turning point was the use of GPT-3 to do code completions. And that was actually productized as GitHub Copilot&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, the very first version. That, I think, is where there was widespread belief. But, you know, in a way, I think there is, even for me early on, a sense of denial and skepticism. Did you have those initially at any point?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK: &lt;/strong&gt;Yeah, I mean, it still happens today, right. Like, this is a weird technology. You know, the original denial and skepticism was, I couldn’t see where this was going. It didn’t seem like a miracle because, you know, of course computers can complete code for you. Like, what else are they supposed to do? Of course, computers can give you answers to questions and write fun things. So there’s difference of moving into a world of generative AI. I think a lot of people just thought that’s what computers could do. So it made the conversations a little weird. But even today, faced with these, you know, with very strong reasoner models that operate at the level of PhD students, I think a lot of people have issues with it, right.&lt;/p&gt;



&lt;p&gt;I mean, first of all, they seem intuitive to use, but they’re not always intuitive to use because the first use case that everyone puts AI to, it fails at because they use it like Google or some other use case. And then it’s genuinely upsetting in a lot of ways. I think, you know, I write in my book about the idea of three sleepless nights. That hasn’t changed. Like, you have to have an intellectual crisis to some extent, you know, and I think people do a lot to avoid having that existential angst of like, “Oh my god, what does it mean that a machine could think—&lt;em&gt;apparently think&lt;/em&gt;—like a person?”&lt;/p&gt;



&lt;p&gt;So, I mean, I see resistance now. I saw resistance then. And then on top of all of that, there’s the fact that the curve of the technology is quite great. I mean, the price of GPT-4 level intelligence from, you know, when it was released has dropped 99.97% at this point, right.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yes. Mm-hmm.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK:&lt;/strong&gt; I mean, I could run a GPT-4 class system basically on my phone. Microsoft’s releasing things that can almost run on like, you know, like it fits in almost no space, that are almost as good as the original GPT-4 models. I mean, I don’t think people have a sense of how fast the trajectory is moving either.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah, you know, there’s something that I think about often. There is this existential dread, or will this technology replace me? But I think the first people to feel that are researchers—people encountering this for the first time. You know, if you were working, let’s say, in Bayesian reasoning or in traditional, let’s say, Gaussian mixture model based, you know, speech recognition, you do get this feeling, &lt;em&gt;Oh, my god, this technology has just solved the problem that I’ve dedicated my life to&lt;/em&gt;. And there is this really difficult period where you have to cope with that. And I think this is going to be spreading, you know, in more and more walks of life. And so this … at what point does that sort of sense of dread hit you, if ever?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK: &lt;/strong&gt;I mean, you know, it’s not even dread as much as like, you know, Tyler Cowen wrote that it’s impossible to not feel a little bit of sadness as you use these AI systems, too. Because, like, I was talking to a friend, just as the most minor example, and his talent that he was very proud of was he was very good at writing limericks for birthday cards. He’d write these limericks. Everyone was always amused by them. [LAUGHTER]&lt;/p&gt;



&lt;p&gt;And now, you know, GPT-4 and GPT-4.5, they made limericks obsolete. Like, anyone can write a good limerick, right. So this was a talent, and it was a little sad. Like, this thing that you cared about mattered.&lt;/p&gt;



&lt;p&gt;You know, as academics, we’re a little used to dead ends, right, and like, you know, some getting the lap. But the idea that entire fields are hitting that way. Like in medicine, there’s a lot of support systems that are now obsolete. And the question is how quickly you change that. In education, a lot of our techniques are obsolete.&lt;/p&gt;



&lt;p&gt;What do you do to change that? You know, it’s like the fact that this brute force technology is good enough to solve so many problems is weird, right. And it’s not just the end of, you know, of our research angles that matter, too. Like, for example, I ran this, you know, 14-person-plus, multimillion-dollar effort at Wharton to build these teaching simulations, and we’re very proud of them. It took years of work to build one.&lt;/p&gt;



&lt;p&gt;Now we’ve built a system that can build teaching simulations on demand by you talking to it with one team member. And, you know, you literally can create any simulation by having a discussion with the AI. I mean, you know, there’s a switch to a new form of excitement, but there is a little bit of like, this mattered to me, and, you know, now I have to change how I do things. I mean, adjustment happens. But if you haven’t had that displacement, I think that’s a good indicator that you haven’t really faced AI yet.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah, what’s so interesting just listening to you is you use words like sadness, and yet I can see the—and hear the—excitement in your voice and your body language. So, you know, that’s also kind of an interesting aspect of all of this.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK:&lt;/strong&gt; Yeah, I mean, I think there’s something on the other side, right. But, like, I can’t say that I haven’t had moments where like, &lt;em&gt;ughhhh&lt;/em&gt;, but then there’s joy and basically like also, you know, freeing stuff up. I mean, I think about doctors or professors, right. These are jobs that bundle together lots of different tasks that you would never have put together, right. If you’re a doctor, you would never have expected the same person to be good at keeping up with the research and being a good diagnostician and being a good manager and being good with people and being good with hand skills.&lt;/p&gt;



&lt;p&gt;Like, who would ever want that kind of bundle? That’s not something you’re all good at, right. And a lot of our stress of our job comes from the fact that we suck at some of it. And so to the extent that AI steps in for that, you kind of feel bad about some of the stuff that it’s doing that you wanted to do. But it’s much more uplifting to be like, I don’t have to do this stuff I’m bad anymore, or I get the support to make myself good at it. And the stuff that I really care about, I can focus on more. Well, because we are at kind of a unique moment where whatever you’re best at, you’re still better than AI. And I think it’s an ongoing question about how long that lasts. But for right now, like you’re not going to say, OK, AI replaces me entirely in my job in medicine. It’s very unlikely.&lt;/p&gt;



&lt;p&gt;But you will say it replaces these 17 things I’m bad at, but I never liked that anyway. So it’s a period of both excitement and a little anxiety.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah, I’m going to want to get back to this question about in what ways AI may or may not replace doctors or some of what doctors and nurses and other clinicians do. But before that, let’s get into, I think, the real meat of this conversation. In previous episodes of this podcast, we talked to clinicians and healthcare administrators and technology developers that are very rapidly injecting AI today to do various forms of workforce automation, you know, automatically writing a clinical encounter note, automatically filling out a referral letter or request for prior authorization for some reimbursement to an insurance company.&lt;/p&gt;



&lt;p&gt;And so these sorts of things are intended not only to make things more efficient and lower costs but also to reduce various forms of drudgery, &lt;em&gt;cognitive burden&lt;/em&gt; on frontline health workers. So how do you think about the impact of AI on that aspect of workforce, and, you know, what would you expect will happen over the next few years in terms of impact on efficiency and costs?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK: &lt;/strong&gt;So I mean, this is a case where I think we’re facing the big bright problem in AI in a lot of ways, which is that this is … at the individual level, there’s lots of performance gains to be gained, right. The problem, though, is that we as individuals fit into systems, in medicine as much as anywhere else or more so, right. Which is that you could individually boost your performance, but it’s also about systems that fit along with this, right.&lt;/p&gt;



&lt;p&gt;So, you know, if you could automatically, you know, record an encounter, if you could automatically make notes, does that change what you should be expecting for notes or the value of those notes or what they’re for? How do we take what one person does and validate it across the organization and roll it out for everybody without making it a 10-year process that it feels like IT in medicine often is? Like, so we’re in this really interesting period where there’s incredible amounts of individual innovation in productivity and performance improvements in this field, like very high levels of it, but not necessarily seeing that same thing translate to organizational efficiency or gains.&lt;/p&gt;



&lt;p&gt;And one of my big concerns is seeing that happen. We’re seeing that in nonmedical problems, the same kind of thing, which is, you know, we’ve got research showing 20 and 40% performance improvements, like not uncommon to see those things. But then the organization doesn’t capture it; the system doesn’t capture it. Because the individuals are doing their own work and the systems don’t have the ability to, kind of, learn or adapt as a result.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; You know, where are those productivity gains going, then, when you get to the organizational level?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK: &lt;/strong&gt;Well, they’re dying for a few reasons. One is, there’s a tendency for individual contributors to underestimate the power of management, right.&lt;/p&gt;



&lt;p&gt;Practices associated with good management increase happiness, decrease, you know, issues, increase success rates. In the same way, about 40%, as far as we can tell, of the US advantage over other companies, of US firms, has to do with management ability. Like, management is a big deal. Organizing is a big deal. Thinking about how you coordinate is a big deal.&lt;/p&gt;



&lt;p&gt;At the individual level, when things get stuck there, right, you can’t start bringing them up to how systems work together. It becomes, &lt;em&gt;How do I deal with a doctor that has a 60% performance improvement? &lt;/em&gt;We really only have one thing in our playbook for doing that right now, which is, &lt;em&gt;OK, we could fire 40% of the other doctors and still have a performance gain&lt;/em&gt;, which is not the answer you want to see happen.&lt;/p&gt;



&lt;p&gt;So because of that, people are hiding their use. They’re actually hiding their use for lots of reasons.&lt;/p&gt;



&lt;p&gt;And it’s a weird case because the people who are able to figure out best how to use these systems, for a lot of use cases, they’re actually clinicians themselves because they’re experimenting all the time. Like, they have to take those encounter notes. And if they figure out a better way to do it, they figure that out. You don’t want to wait for, you know, a med tech company to figure that out and then sell that back to you when it can be done by the physicians themselves.&lt;/p&gt;



&lt;p&gt;So we’re just not used to a period where everybody’s innovating and where the management structure isn’t in place to take advantage of that. And so we’re seeing things stalled at the individual level, and people are often, especially in risk-averse organizations or organizations where there’s lots of regulatory hurdles, people are so afraid of the regulatory piece that they don’t even bother trying to make change.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; If you are, you know, the leader of a hospital or a clinic or a whole health system, how should you approach this? You know, how should you be trying to extract positive success out of AI?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK: &lt;/strong&gt;So I think that you need to embrace the right kind of risk, right. We don’t want to put risk on our patients … like, we don’t want to put uninformed risk. But innovation involves risk to how organizations operate. They involve change. So I think part of this is embracing the idea that R&amp;amp;D has to happen in organizations again.&lt;/p&gt;



&lt;p&gt;What’s happened over the last 20 years or so has been organizations giving that up. Partially, that’s a trend to focus on what you’re good at and not try and do this other stuff. Partially, it’s because it’s outsourced now to software companies that, like, Salesforce tells you how to organize your sales team. Workforce tells you how to organize your organization. Consultants come in and will tell you how to make change based on the average of what other people are doing in your field.&lt;/p&gt;



&lt;p&gt;So companies and organizations and hospital systems have all started to give up their ability to create their own organizational change. And when I talk to organizations, I often say they have to have two approaches. They have to think about the crowd and the lab.&lt;/p&gt;



&lt;p&gt;So the crowd is the idea of how to empower clinicians and administrators and supporter networks to start using AI and experimenting in ethical, legal ways and then sharing that information with each other. And the lab is, how are we doing R&amp;amp;D about the approach of how to [get] AI to work, not just in direct patient care, right. But also fundamentally, like, what paperwork can you cut out? How can we better explain procedures? Like, what management role can this fill?&lt;/p&gt;



&lt;p&gt;And we need to be doing active experimentation on that. We can’t just wait for, you know, Microsoft to solve the problems. It has to be at the level of the organizations themselves.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; So let’s shift a little bit to the patient. You know, one of the things that we see, and I think everyone is seeing, is that people are turning to chatbots, like ChatGPT, actually to seek healthcare information for, you know, their own health or the health of their loved ones.&lt;/p&gt;



&lt;p&gt;And there was already, prior to all of this, a trend towards, let’s call it, consumerization of healthcare. So just in the business of healthcare delivery, do you think AI is going to hasten these kinds of trends, or from the consumer’s perspective, what … ?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK: &lt;/strong&gt;I mean, absolutely, right. Like, all the early data that we have suggests that for most common medical problems, you should just consult AI, too, right. In fact, there is a real question to ask: at what point does it become unethical for doctors themselves to not ask for a second opinion from the AI because it’s cheap, right? You could overrule it or whatever you want, but like not asking seems foolish.&lt;/p&gt;



&lt;p&gt;I think the two places where there’s a burning almost, you know, moral imperative is … let’s say, you know, I’m in Philadelphia, I’m a professor, I have access to really good healthcare through the Hospital University of Pennsylvania system. I know doctors. You know, I’m lucky. I’m well connected. If, you know, something goes wrong, I have friends who I can talk to. I have specialists. I’m, you know, pretty well educated in this space.&lt;/p&gt;



&lt;p&gt;But for most people on the planet, they don’t have access to good medical care, they don’t have good health. It feels like it’s absolutely imperative to say when should you use AI and when not. Are there blind spots? What are those things?&lt;/p&gt;



&lt;p&gt;And I worry that, like, to me, that would be the crash project I’d be invoking because I’m doing the same thing in education, which is this system is not as good as being in a room with a great teacher who also uses AI to help you, but it’s better than not getting an, you know, to the level of education people get in many cases. Where should we be using it? How do we guide usage in the right way? Because the AI labs aren’t thinking about this. We have to.&lt;/p&gt;



&lt;p&gt;So, to me, there is a burning need here to understand this. And I worry that people will say, you know, everything that’s true—AI can hallucinate, AI can be biased. All of these things are absolutely true, but people are going to use it. The early indications are that it is quite useful. And unless we take the active role of saying, here’s when to use it, here’s when not to use it, we don’t have a right to say, don’t use this system. And I think, you know, we have to be exploring that.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; What do people need to understand about AI? And what should schools, universities, and so on be teaching?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK:&lt;/strong&gt; Those are, kind of, two separate questions in lot of ways. I think a lot of people want to teach AI &lt;em&gt;skills&lt;/em&gt;, and I will tell you, as somebody who works in this space a lot, there isn’t like an easy, sort of, AI skill, right. I could teach you prompt engineering in two to three classes, but every indication we have is that for most people under most circumstances, the value of prompting, you know, any one case is probably not that useful.&lt;/p&gt;



&lt;p&gt;A lot of the tricks are disappearing because the AI systems are just starting to use them themselves. So asking good questions, being a good manager, being a good thinker tend to be important, but like magic tricks around making, you know, the AI do something because you use the right phrase used to be something that was real but is rapidly disappearing.&lt;/p&gt;



&lt;p&gt;So I worry when people say teach AI skills. No one’s been able to articulate to me as somebody who knows AI very well and teaches classes on AI, what those AI skills that everyone should learn are, right.&lt;/p&gt;



&lt;p&gt;I mean, there’s value in learning a little bit how the models work. There’s a value in working with these systems. A lot of it’s just hands on keyboard kind of work. But, like, we don’t have an easy slam dunk “this is what you learn in the world of AI” because the systems are getting better, and as they get better, they get less sensitive to these prompting techniques. They get better prompting themselves. They solve problems spontaneously and start being agentic. So it’s a hard problem to ask about, like, what do you train someone on? I think getting people experience in hands-on-keyboards, getting them to … there’s like four things I could teach you about AI, and two of them are already starting to disappear.&lt;/p&gt;



&lt;p&gt;But, like, one is be direct. Like, tell the AI exactly what you want. That’s very helpful. Second, provide as much context as possible. That can include things like acting as a doctor, but also all the information you have. The third is give it step-by-step directions—that’s becoming less important. And the fourth is good and bad examples of the kind of output you want. Those four, that’s like, that’s it as far as the research telling you what to do, and the rest is building intuition.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; I’m really impressed that you didn’t give the answer, “Well, everyone should be teaching my book, &lt;em&gt;Co-Intelligence&lt;/em&gt;.” [LAUGHS]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK:&lt;/strong&gt; Oh, no, sorry! Everybody should be teaching my book &lt;em&gt;Co-Intelligence&lt;/em&gt;. I apologize. [LAUGHTER]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; It’s good to chuckle about that, but actually, I can’t think of a better book, like, if you were to assign a textbook in any professional education space, I think &lt;em&gt;Co-Intelligence&lt;/em&gt; would be number one on my list. Are there other things that you think are essential reading?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK:&lt;/strong&gt; That’s a really good question. I think that a lot of things are evolving very quickly. I happen to, kind of, hit a sweet spot with &lt;em&gt;Co-Intelligence&lt;/em&gt; to some degree because I talk about how I used it, and I was, sort of, an advanced user of these systems.&lt;/p&gt;



&lt;p&gt;So, like, it’s, sort of, like my Twitter feed, my online newsletter. I’m just trying to, kind of, in some ways, it’s about trying to make people aware of what these systems can do by just showing a lot, right. Rather than picking one thing, and, like, this is a general-purpose technology. Let’s use it for this. And, like, everybody gets a light bulb for a different reason. So more than reading, it is using, you know, and that can be Copilot or whatever your favorite tool is.&lt;/p&gt;



&lt;p&gt;But using it. Voice modes help a lot. In terms of readings, I mean, I think that there is a couple of good guides to understanding AI that were originally blog posts. I think Tim Lee has one called Understanding AI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, and it had a good overview …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah, that’s a great one.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK: &lt;/strong&gt;… of that topic that I think explains how transformers work, which can give you some mental sense. I think [Andrej] Karpathy&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; has some really nice videos of use that I would recommend.&lt;/p&gt;



&lt;p&gt;Like on the medical side, I think the book that you did, if you’re in medicine, you should read that. I think that that’s very valuable. But like all we can offer are hints in some ways. Like there isn’t … if you’re looking for the instruction manual, I think it can be very frustrating because it’s like you want the best practices and procedures laid out, and we cannot do that, right. That’s not how a system like this works.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK:&lt;/strong&gt; It’s not a person, but thinking about it like a person can be helpful, right.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; One of the things that has been sort of a fun project for me for the last few years is I have been a founding board member of a new medical school at Kaiser Permanente. And, you know, that medical school curriculum is being formed in this era. But it’s been perplexing to understand, you know, what this means for a medical school curriculum. And maybe even more perplexing for me, at least, is the accrediting bodies, which are extremely important in US medical schools; how accreditors should think about what’s necessary here.&lt;/p&gt;



&lt;p&gt;Besides the things that you’ve … the, kind of, four key ideas you mentioned, if you were talking to the board of directors of the LCME [Liaison Committee on Medical Education] accrediting body, what’s the one thing you would want them to really internalize?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK: &lt;/strong&gt;This is both a fast-moving and vital area. This can’t be viewed like a usual change, which [is], “Let’s see how this works.” Because it’s, like, the things that make medical technologies hard to do, which is like unclear results, limited, you know, expensive use cases where it rolls out slowly. So one or two, you know, advanced medical facilities get access to, you know, proton beams or something else at multi-billion dollars of cost, and that takes a while to diffuse out. That’s not happening here. This is all happening at the same time, all at once. This is now … AI &lt;em&gt;is&lt;/em&gt; part of medicine.&lt;/p&gt;



&lt;p&gt;I mean, there’s a minor point that I’d make that actually is a really important one, which is large language models, generative AI overall, work incredibly differently than other forms of AI. So the other worry I have with some of these accreditors is they blend together &lt;em&gt;algorithmic&lt;/em&gt; forms of AI, which medicine has been trying for long time—decision support, algorithmic methods, like, medicine more so than other places has been thinking about those issues. Generative AI, even though it uses the same underlying techniques, is a completely different beast.&lt;/p&gt;



&lt;p&gt;So, like, even just take the most simple thing of algorithmic aversion, which is a well-understood problem in medicine, right. Which is, so you have a tool that could tell you as a radiologist, you know, the chance of this being cancer; you don’t like it, you overrule it, right.&lt;/p&gt;



&lt;p&gt;We don’t find algorithmic aversion happening with LLMs in the same way. People actually enjoy using them because it’s more like working with a person. The flaws are different. The approach is different. So you need to both view this as universal applicable &lt;em&gt;today&lt;/em&gt;, which makes it urgent, but also as something that is not the same as your other form of AI, and your AI working group that is thinking about how to solve this problem is not the right people here.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; You know, I think the world has been trained because of the magic of web search to view computers as question-answering machines. Ask a question, get an answer.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK: &lt;/strong&gt;Yes. Yes.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Write a query, get results. And as I have interacted with medical professionals, you can see that medical professionals have that model of a machine in mind. And I think that’s partly, I think psychologically, why hallucination is so alarming. Because you have a mental model of a computer as a machine that has absolutely rock-solid perfect memory recall.&lt;/p&gt;



&lt;p&gt;But the thing that was so powerful in &lt;em&gt;Co-Intelligence&lt;/em&gt;, and we tried to get at this in our book also, is that’s not the sweet spot. It’s this sort of deeper interaction, more of a collaboration. And I thought your use of the term &lt;em&gt;Co-Intelligence&lt;/em&gt; really just even in the title of the book tried to capture this. When I think about education, it seems like that’s the first step, to get past this concept of a machine being just a question-answering machine. Do you have a reaction to that idea?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK:&lt;/strong&gt; I think that’s very powerful. You know, we’ve been trained over so many years at both using computers but also in science fiction, right. Computers are about cold logic, right. They will give you the right answer, but if you ask it what love is, they explode, right. Like that’s the classic way you defeat the evil robot in &lt;em&gt;Star Trek&lt;/em&gt;, right. “Love does not compute.” [LAUGHTER]&lt;/p&gt;



&lt;p&gt;Instead, we have a system that makes mistakes, is warm, beats doctors in empathy in almost every controlled study on the subject, right. Like, absolutely can outwrite you in a sonnet but will absolutely struggle with giving you the right answer every time. And I think our mental models are just broken for this. And I think you’re absolutely right. And that’s part of what I thought your book does get at really well is, like, this is a different thing. It’s also generally applicable. Again, the model in your head should be kind of like a person even though it isn’t, right.&lt;/p&gt;



&lt;p&gt;There’s a lot of warnings and caveats to it, but if you start from &lt;em&gt;person&lt;/em&gt;, &lt;em&gt;smart person&lt;/em&gt; you’re talking to, your mental model will be more accurate than smart machine, even though both are flawed examples, right. So it will make mistakes; it will make errors. The question is, what do you trust it on? What do you not trust it? As you get to know a model, you’ll get to understand, like, I totally don’t trust it for this, but I absolutely trust it for that, right.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; All right. So we’re getting to the end of the time we have together. And so I’d just like to get now into something a little bit more provocative. And I get the question all the time. You know, will AI replace doctors? In medicine and other advanced knowledge work, project out five to 10 years. What do think happens?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK: &lt;/strong&gt;OK, so first of all, let’s acknowledge systems change much more slowly than individual use. You know, doctors are not individual actors; they’re part of systems, right. So not just the system of a patient who like may or may not want to talk to a machine instead of a person but also legal systems and administrative systems and systems that allocate labor and systems that train people.&lt;/p&gt;



&lt;p&gt;So, like, it’s hard to imagine that in five to 10 years medicine being so upended that even if AI was better than doctors at every single thing doctors do, that we’d actually see as radical a change in medicine as you might in other fields. I think you will see faster changes happen in consulting and law and, you know, coding, other spaces than medicine.&lt;/p&gt;



&lt;p&gt;But I do think that there is good reason to suspect that AI will outperform people while still having flaws, right. That’s the difference. We’re already seeing that for common medical questions in enough randomized controlled trials that, you know, best doctors beat AI, but the AI beats the mean doctor, right. Like, that’s just something we should acknowledge is happening at this point.&lt;/p&gt;



&lt;p&gt;Now, will that work in your specialty? No. Will that work with all the contingent social knowledge that you have in your space? Probably not.&lt;/p&gt;



&lt;p&gt;Like, these are vignettes, right. But, like, that’s kind of where things are. So let’s assume, right … you’re asking two questions. One is, how good will AI get?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK:&lt;/strong&gt; And we don’t know the answer to that question. I will tell you that your colleagues at Microsoft and increasingly the labs, the AI labs themselves, are all saying they think they’ll have a machine smarter than a human at every intellectual task in the next two to three years. If that doesn’t happen, that makes it easier to assume the future, but let’s just assume that that’s the case. I think medicine starts to change with the idea that people feel obligated to use this to help for everything.&lt;/p&gt;



&lt;p&gt;Your patients will be using it, and it will be your advisor and helper at the beginning phases, right. And I think that I expect people to be better at empathy. I expect better bedside manner. I expect management tasks to become easier. I think administrative burden might lighten if we handle this right way or much worse if we handle it badly. Diagnostic accuracy will increase, right.&lt;/p&gt;



&lt;p&gt;And then there’s a set of discovery pieces happening, too, right. One of the core goals of all the AI companies is to accelerate medical research. How does that happen and how does that affect us is a, kind of, unknown question. So I think clinicians are in both the eye of the storm and surrounded by it, right. Like, they can resist AI use for longer than most other fields, but everything around them is going to be affected by it.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Well, Ethan, this has been really a fantastic conversation. And, you know, I think in contrast to all the other conversations we’ve had, this one gives especially the leaders in healthcare, you know, people actually trying to lead their organizations into the future, whether it’s in education or in delivery, a lot to think about. So I really appreciate you joining.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK:&lt;/strong&gt; Thank you.&lt;/p&gt;



&lt;p&gt;[TRANSITION MUSIC] &amp;nbsp;&lt;/p&gt;



&lt;p&gt;I’m a computing researcher who works with people who are right in the middle of today’s bleeding-edge developments in AI. And because of that, I often lose sight of how to talk to a broader audience about what it’s all about. And so I think one of Ethan’s superpowers is that he has this knack for explaining complex topics in AI in a really accessible way, getting right to the most important points without making it so simple as to be useless. That’s why I rarely miss an opportunity to read up on his latest work.&lt;/p&gt;



&lt;p&gt;One of the first things I learned from Ethan is the intuition that you can, sort of, think of AI as a very knowledgeable intern. In other words, think of it as a persona that you can interact with, but you also need to be a manager for it and to always assess the work that it does.&lt;/p&gt;



&lt;p&gt;In our discussion, Ethan went further to stress that there is, because of that, a serious education gap. You know, over the last decade or two, we’ve all been trained, mainly by search engines, to think of computers as question-answering machines. In medicine, in fact, there’s a question-answering application that is really popular called UpToDate&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. Doctors use it all the time. But generative AI systems like ChatGPT are different. There’s therefore a challenge in how to break out of the old-fashioned mindset of search to get the full value out of generative AI.&lt;/p&gt;



&lt;p&gt;The other big takeaway for me was that Ethan pointed out while it’s easy to see productivity gains from AI at the individual level, those same gains, at least today, don’t often translate automatically to organization-wide or system-wide gains. And one, of course, has to conclude that it takes more than just making individuals more productive; the whole system also has to adjust to the realities of AI.&lt;/p&gt;



&lt;p&gt;Here’s now my interview with Azeem Azhar:&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Azeem, welcome.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZEEM AZHAR: &lt;/strong&gt;Peter, thank you so much for having me.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; You know, I think you’re extremely well known in the world. But still, some of the listeners of this podcast series might not have encountered you before.&lt;/p&gt;



&lt;p&gt;And so one of the ways I like to ask people to introduce themselves is, how do you explain to your parents what you do every day?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR: &lt;/strong&gt;Well, I’m very lucky in that way because my mother was the person who got me into computers more than 40 years ago. And I still have that first computer, a ZX81 with a Z80 chip …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Oh wow.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; … to this day. It sits in my study, all seven and a half thousand transistors and Bakelite plastic that it is. And my parents were both economists, and economics is deeply connected with technology in some sense. And I grew up in the late ’70s and the early ’80s. And that was a time of tremendous optimism around technology. It was space opera, science fiction, robots, and of course, the personal computer and, you know, Bill Gates and Steve Jobs. So that’s where I started.&lt;/p&gt;



&lt;p&gt;And so, in a way, my mother and my dad, who passed away a few years ago, had always known me as someone who was fiddling with computers but also thinking about economics and society. And so, in a way, it’s easier to explain to them because they’re the ones who nurtured the environment that allowed me to research technology and AI and think about what it means to firms and to the economy at large.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; I always like to understand the origin story. And what I mean by that is, you know, what was your first encounter with generative AI? And what was that like? What did you go through?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; The first real moment was when Midjourney and Stable Diffusion emerged in that summer of 2022. I’d been away on vacation, and I came back—and I’d been off grid, in fact—and the world had really changed.&lt;/p&gt;



&lt;p&gt;Now, I’d been aware of GPT-3 and GPT-2, which I played around with and with BERT, the original transformer paper about seven or eight years ago, but it was the moment where I could talk to my computer, and it could produce these images, and it could be refined in natural language that really made me think we’ve crossed into a new domain. We’ve gone from AI being highly discriminative to AI that’s able to explore the world in particular ways. And then it was a few months later that ChatGPT came out—November, the 30th.&lt;/p&gt;



&lt;p&gt;And I think it was the next day or the day after that I said to my team, everyone has to use this, and we have to meet every morning and discuss how we experimented the day before. And we did that for three or four months. And, you know, it was really clear to me in that interface at that point that, you know, we’d absolutely pass some kind of threshold.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; And who’s the &lt;em&gt;we&lt;/em&gt; that you were experimenting with?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; So I have a team of four who support me. They’re mostly researchers of different types. I mean, it’s almost like one of those jokes. You know, I have a sociologist, an economist, and an astrophysicist. And, you know, they walk into the bar, [LAUGHTER] or they walk into our virtual team room, and we try to solve problems.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Well, so let’s get now into brass tacks here. And I think I want to start maybe just with an exploration of the economics of all this and economic realities. Because I think in a lot of your work—for example, in your book—you look pretty deeply at how automation generally and AI specifically are transforming certain sectors like finance, manufacturing, and you have a really, kind of, insightful focus on what this means for productivity and which ways, you know, efficiencies are found. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;And then you, sort of, balance that with risks, things that can and do go wrong. And so as you take that background and looking at all those other sectors, in what ways are the same patterns playing out or likely to play out in healthcare and medicine?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; I’m sure we will see really remarkable parallels but also new things going on. I mean, medicine has a particular quality compared to other sectors in the sense that it’s highly regulated, market structure is very different country to country, and it’s an incredibly broad field. I mean, just think about taking a Tylenol and going through laparoscopic surgery. Having an MRI and seeing a physio. I mean, this is all medicine. I mean, it’s hard to imagine a sector that is [LAUGHS] more broad than that.&lt;/p&gt;



&lt;p&gt;So I think we can start to break it down, and, you know, where we’re seeing things with generative AI will be that the, sort of, softest entry point, which is the medical scribing. And I’m sure many of us have been with clinicians who have a medical scribe running alongside—they’re all on Surface Pros I noticed, right? [LAUGHTER] They’re on the tablet computers, and they’re scribing away.&lt;/p&gt;



&lt;p&gt;And what that’s doing is, in the words of my friend Eric Topol, it’s giving the clinician time back&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, right. They have time back from days that are extremely busy and, you know, full of administrative overload. So I think you can obviously do a great deal with reducing that overload.&lt;/p&gt;



&lt;p&gt;And within my team, we have a view, which is if you do something five times in a week, you should be writing an automation for it. And if you’re a doctor, you’re probably reviewing your notes, writing the prescriptions, and so on several times a day. So those are things that can clearly be automated, and the human can be in the loop. But I think there are so many other ways just within the clinic that things can help.&lt;/p&gt;



&lt;p&gt;So, one of my friends, my friend from my junior school—I’ve known him since I was 9—is an oncologist who’s also deeply into machine learning, and he’s in Cambridge in the UK. And he built with Microsoft Research a suite of imaging AI tools from his own discipline, which they then open sourced.&lt;/p&gt;



&lt;p&gt;So that’s another way that you have an impact, which is that you actually enable the, you know, generalist, specialist, polymath, whatever they are in health systems to be able to get this technology, to tune it to their requirements, to use it, to encourage some grassroots adoption in a system that’s often been very, very heavily centralized.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; And then I think there are some other things that are going on that I find really, really exciting. So one is the consumerization of healthcare. So I have one of those sleep tracking rings, the Oura&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yup.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; That is building a data stream that we’ll be able to apply more and more AI to. I mean, right now, it’s applying traditional, I suspect, machine learning, but you can imagine that as we start to get more data, we start to get more used to measuring ourselves, we create this sort of pot, a personal asset that we can turn AI to.&lt;/p&gt;



&lt;p&gt;And there’s still another category. And that other category is one of the completely novel ways in which we can enable patient care and patient pathway. And there’s a fantastic startup in the UK called Neko Health&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, which, I mean, does physicals, MRI scans, and blood tests, and so on.&lt;/p&gt;



&lt;p&gt;It’s hard to imagine Neko existing without the sort of advanced data, machine learning, AI that we’ve seen emerge over the last decade. So, I mean, I think that there are so many ways in which the temperature is slowly being turned up to encourage a phase change within the healthcare sector.&lt;/p&gt;



&lt;p&gt;And last but not least, I do think that these tools can also be very, very supportive of a clinician’s life cycle. I think we, as patients, we’re a bit …&amp;nbsp; I don’t know if we’re as grateful as we should be for our clinicians who are putting in 90-hour weeks. [LAUGHTER] But you can imagine a world where AI is able to support not just the clinicians’ workload but also their sense of stress, their sense of burnout.&lt;/p&gt;



&lt;p&gt;So just in those five areas, Peter, I sort of imagine we could start to fundamentally transform over the course of many years, of course, the way in which people think about their health and their interactions with healthcare systems&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; I love how you break that down. And I want to press on a couple of things.&lt;/p&gt;



&lt;p&gt;You also touched on the fact that medicine is, at least in most of the world, is a highly regulated industry. I guess finance is the same way, but they also feel different because the, like, finance sector has to be very responsive to consumers, and consumers are sensitive to, you know, an abundance of choice; they are sensitive to price. Is there something unique about medicine besides being regulated?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; I mean, there absolutely is. And in finance, as well, you have much clearer end states. So if you’re not in the consumer space, but you’re in the, you know, asset management space, you have to essentially deliver returns against the volatility or risk boundary, right. That’s what you have to go out and do. And I think if you’re in the consumer industry, you can come back to very, very clear measures, net promoter score being a very good example.&lt;/p&gt;



&lt;p&gt;In the case of medicine and healthcare, it is much more complicated because as far as the clinician is concerned, people are individuals, and we have our own parts and our own responses. If we didn’t, there would never be a need for a differential diagnosis. There’d never be a need for, you know, &lt;em&gt;Let’s try azithromycin first, and then if that doesn’t work, we’ll go to vancomycin,&lt;/em&gt; or, you know, whatever it happens to be. You would just know. But ultimately, you know, people are quite different. The symptoms that they’re showing are quite different, and also their compliance is really, really different.&lt;/p&gt;



&lt;p&gt;I had a back problem that had to be dealt with by, you know, a physio and extremely boring exercises four times a week, but I was ruthless in complying, and my physio was incredibly surprised. He’d say well no one ever does this, and I said, well you know the thing is that I kind of just want to get this thing to go away.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; And I think that that’s why medicine is and healthcare is so different and more complex. But I also think that’s why AI can be really, really helpful. I mean, we didn’t talk about, you know, AI in its ability to potentially do this, which is to extend the clinician’s presence throughout the week.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right. Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; The idea that maybe some part of what the clinician would do if you could talk to them on Wednesday, Thursday, and Friday could be delivered through an app or a chatbot just as a way of encouraging the compliance, which is often, especially with older patients, one reason why conditions, you know, linger on for longer.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; You know, just staying on the regulatory thing, as I’ve thought about this, the one regulated sector that I think seems to have some parallels to healthcare is energy delivery, energy distribution.&lt;/p&gt;



&lt;p&gt;Because like healthcare, as a consumer, I don’t have choice in who delivers electricity to my house. And even though I care about it being cheap or at least not being overcharged, I don’t have an abundance of choice. I can’t do price comparisons.&lt;/p&gt;



&lt;p&gt;And there’s something about that, just speaking as a consumer of both energy and a consumer of healthcare, that feels similar. Whereas other regulated industries, you know, somehow, as a consumer, I feel like I have a lot more direct influence and power. Does that make any sense to someone, you know, like you, who’s really much more expert in how economic systems work?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; I mean, in a sense, one part of that is very, very true. You have a limited panel of energy providers you can go to, and in the US, there may be places where you have no choice.&lt;/p&gt;



&lt;p&gt;I think the area where it’s slightly different is that as a consumer or a patient, you can actually make meaningful choices and changes yourself using these technologies, and people used to joke about you know asking Dr. Google. But Dr. Google is not terrible, particularly if you go to WebMD. And, you know, when I look at long-range change, many of the regulations that exist around healthcare delivery were formed at a point before people had access to good quality information at the touch of their fingertips or when educational levels in general were much, much lower. And many regulations existed because of the incumbent power of particular professional sectors.&lt;/p&gt;



&lt;p&gt;I’ll give you an example from the United Kingdom. So I have had asthma all of my life. That means I’ve been taking my inhaler, Ventolin, and maybe a steroid inhaler for nearly 50 years. That means that I know … actually, I’ve got more experience, and I—in some sense—know more about it than a general practitioner.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; And until a few years ago, I would have to go to a general practitioner to get this drug that I’ve been taking for five decades, and there they are, age 30 or whatever it is. And a few years ago, the regulations changed. And now pharmacies can … or pharmacists can prescribe those types of drugs under certain conditions directly.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; That was not to do with technology. That was to do with incumbent lock-in. So when we look at the medical industry, the healthcare space, there are some parallels with energy, but there are a few little things that the ability that the consumer has to put in some effort to learn about their condition, but also the fact that some of the regulations that exist just exist because certain professions are powerful.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah, one last question while we’re still on economics. There seems to be a conundrum about productivity and efficiency in healthcare delivery because I’ve never encountered a doctor or a nurse that wants to be able to handle even more patients than they’re doing on a daily basis.&lt;/p&gt;



&lt;p&gt;And so, you know, if productivity means simply, well, your rounds can now handle &lt;em&gt;16&lt;/em&gt; patients instead of eight patients, that doesn’t seem necessarily to be a desirable thing. So how can we or should we be thinking about efficiency and productivity since obviously costs are, in most of the developed world, are a huge, huge problem?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; Yes, and when you described doubling the number of patients on the round, I imagined you buying them all roller skates so they could just whizz around [LAUGHTER] the hospital faster and faster than ever before.&lt;/p&gt;



&lt;p&gt;We can learn from what happened with the introduction of electricity. Electricity emerged at the end of the 19th century, around the same time that cars were emerging as a product, and car makers were very small and very artisanal. And in the early 1900s, some really smart car makers figured out that electricity was going to be important. And they bought into this technology by putting pendant lights in their workshops so they could “visit more patients.” Right?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah, yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; They could effectively spend more hours working, and that was a productivity enhancement, and it was noticeable. But, of course, electricity &lt;em&gt;fundamentally&lt;/em&gt; changed the productivity by orders of magnitude of people who made cars starting with Henry Ford because he was able to reorganize his factories around the electrical delivery of power and to therefore have the moving assembly line, which 10xed the productivity of that system.&lt;/p&gt;



&lt;p&gt;So when we think about how AI will affect the clinician, the nurse, the doctor, it’s much easier for us to imagine it as the pendant light that just has them working later …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; … than it is to imagine a reconceptualization of the relationship between the clinician and the people they care for.&lt;/p&gt;



&lt;p&gt;And I’m not sure. I don’t think anybody knows what that looks like. But, you know, I do think that there will be a way that this changes, and you can see that scale out factor. And it may be, Peter, that what we end up doing is we end up saying, OK, because we have these brilliant AIs, there’s a lower level of training and cost and expense that’s required for a broader range of conditions that need treating. And that expands the market, right. That expands the market hugely. It’s what has happened in the market for taxis or ride sharing. The introduction of Uber and the GPS system …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yup.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; … has meant many more people now earn their living driving people around in their cars. And at least in London, you had to be reasonably highly trained to do that.&lt;/p&gt;



&lt;p&gt;So I can see a reorganization is possible. Of course, entrenched interests, the economic flow … and there are many entrenched interests, particularly in the US between the health systems and the, you know, professional bodies that might slow things down. But I think a reimagining is possible.&lt;/p&gt;



&lt;p&gt;And if I may, I’ll give you one example of that, which is, if you go to countries outside of the US where there are many more sick people per doctor, they have incentives to change the way they deliver their healthcare. And well before there was AI of this quality around, there was a few cases of health systems in India—Aravind Eye Care&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; was one, and Narayana Hrudayalaya [now known as Narayana Health&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;] was another. And in the latter, they were a cardiac care unit where you couldn’t get enough heart surgeons.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah, yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; So specially trained nurses would operate under the supervision of a single surgeon who would supervise many in parallel. So there are ways of increasing the quality of care, reducing the cost, but it does require a systems change. And we can’t expect a single bright algorithm to do it on its own.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah, really, really interesting. So now let’s get into regulation. And let me start with this question. You know, there are several startup companies I’m aware of that are pushing on, I think, a near-term future possibility that a medical AI for consumer might be allowed, say, to prescribe a medication for you, something that would normally require a doctor or a pharmacist, you know, that is certified in some way, licensed to do. Do you think we’ll get to a point where for certain regulated activities, humans are more or less cut out of the loop?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; Well, humans would have been in the loop because they would have provided the training data, they would have done the oversight, the quality control. But to your question in general, would we delegate an important decision entirely to a tested set of algorithms? I’m sure we will. We already do that. I delegate less important decisions like, &lt;em&gt;What time should I leave for the airport &lt;/em&gt;to Waze. I delegate more important decisions to the automated braking in my car. We will do this at certain levels of risk and threshold.&lt;/p&gt;



&lt;p&gt;If I come back to my example of prescribing Ventolin. It’s really unclear to me that the prescription of Ventolin, this incredibly benign bronchodilator that is only used by people who’ve been through the asthma process, needs to be prescribed by someone who’s gone through 10 years or 12 years of medical training. And why that couldn’t be prescribed by an algorithm or an AI system.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right. Yep. Yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; So, you know, I absolutely think that that will be the case and could be the case. I can’t really see what the objections are. And the real issue is where do you draw the line of where you say, “Listen, this is too important,” or “The cost is too great,” or “The side effects are too high,” and therefore this is a point at which we want to have some, you know, human taking personal responsibility, having a liability framework in place, having a sense that there is a person with legal agency who signed off on this decision. And that line I suspect will start fairly low, and what we’d expect to see would be that that would rise progressively over time.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;What you just said, that scenario of your personal asthma medication, is really interesting because your personal AI might have the benefit of 50 years of your own experience with that medication. So, in a way, there is at least the data potential for, let’s say, the next prescription to be more personalized and more tailored specifically for you.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; Yes. Well, let’s dig into this because I think this is super interesting, and we can look at how things have changed. So 15 years ago, if I had a bad asthma attack, which I might have once a year, I would have needed to go and see my general physician.&lt;/p&gt;



&lt;p&gt;In the UK, it’s very difficult to get an appointment. I would have had to see someone privately who didn’t know me at all because I’ve just walked in off the street, and I would explain my situation. It would take me half a day. Productivity lost. I’ve been miserable for a couple of days with severe wheezing. Then a few years ago the system changed, a protocol changed, and now I have a thing called a rescue pack, which includes prednisolone steroids. It includes something else I’ve just forgotten, and an antibiotic in case I get an upper respiratory tract infection, and I have an “algorithm.” It’s called a protocol. It’s printed out. It’s a flowchart&lt;/p&gt;



&lt;p&gt;I answer various questions, and then I say, “I’m going to prescribe this to myself.” You know, UK doctors don’t prescribe prednisolone, or prednisone as you may call it in the US, at the drop of a hat, right. It’s a powerful steroid. I can self-administer, and I can now get that repeat prescription without seeing a physician a couple of times a year. And the algorithm, the “AI” is, it’s obviously been done in PowerPoint naturally, and it’s a bunch of arrows. [LAUGHS]&lt;/p&gt;



&lt;p&gt;Surely, surely, an AI system is going to be more sophisticated, more nuanced, and give me more assurance that I’m making the right decision around something like that.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah. Well, at a minimum, the AI should be able to make that PowerPoint the next time. [LAUGHS]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; Yeah, yeah. Thank god for Clippy. Yes.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; So, you know, I think in our book, we had a lot of certainty about most of the things we’ve discussed here, but one chapter where I felt we really sort of ran out of ideas, frankly, was on regulation. And, you know, what we ended up doing for that chapter is … I can’t remember if it was Carey’s or Zak’s idea, but we asked GPT-4 to have a conversation, a debate with itself [LAUGHS], about regulation. And we made some minor commentary on that.&lt;/p&gt;



&lt;p&gt;And really, I think we took that approach because we just didn’t have much to offer. By the way, in our defense, I don’t think anyone else had any better ideas anyway.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; Right.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; And so now two years later, do we have better ideas about the need for regulation, the frameworks around which those regulations should be developed, and, you know, what should this look like?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; So regulation is going to be in some cases very helpful because it provides certainty for the clinician that they’re doing the right thing, that they are still insured for what they’re doing, and it provides some degree of confidence for the patient. And we need to make sure that the claims that are made stand up to quite rigorous levels, where ideally there are RCTs [randomized control trials], and there are the classic set of processes you go through.&lt;/p&gt;



&lt;p&gt;You do also want to be able to experiment, and so the question is: as a regulator, how can you enable conditions for there to be experimentation? And what is experimentation? Experimentation is learning so that every element of the system can learn from this experience.&lt;/p&gt;



&lt;p&gt;So finding that space where there can be bit of experimentation, I think, becomes very, very important. And a lot of this is about experience, so I think the first digital therapeutics have received FDA approval, which means there are now people within the FDA who understand how you go about running an approvals process for that, and what that ends up looking like—and of course what we’re very good at doing in this sort of modern hyper-connected world—is we can share that expertise, that knowledge, that experience very, very quickly.&lt;/p&gt;



&lt;p&gt;So you go from one approval a year to a hundred approvals a year to a thousand approvals a year. So we will then actually, I suspect, need to think about what is it to approve digital therapeutics because, unlike big biological molecules, we can generate these digital therapeutics at the rate of knots [very rapidly].&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yes.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; Every road in Hayes Valley in San Francisco, right, is churning out new startups who will want to do things like this. So then, I think about, what does it mean to get approved if indeed it gets approved? But we can also go really far with things that don’t require approval.&lt;/p&gt;



&lt;p&gt;I come back to my sleep tracking ring. So I’ve been wearing this for a few years, and when I go and see my doctor or I have my annual checkup, one of the first things that he asks is how have I been sleeping. And in fact, I even sync my sleep tracking data to their medical record system, so he’s saying … hearing what I’m saying, but he’s actually pulling up the real data going, &lt;em&gt;This patient’s lying to me again. &lt;/em&gt;Of course, I’m very truthful with my doctor, as we should all be. [LAUGHTER]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;You know, actually, that brings up a point that consumer-facing health AI has to deal with pop science, bad science, you know, weird stuff that you hear on Reddit. And because one of the things that consumers want to know always is, you know, what’s the truth?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; Right.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; What can I rely on? And I think that somehow feels different than an AI that you actually put in the hands of, let’s say, a licensed practitioner. And so the regulatory issues seem very, very different for these two cases somehow.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; I agree, they’re very different. And I think for a lot of areas, you will want to build AI systems that are first and foremost for the clinician, even if they have patient extensions, that idea that the clinician can still be with a patient during the week.&lt;/p&gt;



&lt;p&gt;And you’ll do that anyway because you need the data, and you also need a little bit of a liability shield to have like a sensible person who’s been trained around that. And I think that’s going to be a very important pathway for many AI medical crossovers. We’re going to go through the clinician.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; But I also do recognize what you say about the, kind of, kooky quackery that exists on Reddit. Although on Creatine, Reddit may yet prove to have been right. [LAUGHTER]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah, that’s right. Yes, yeah, absolutely. Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; Sometimes it’s right. And I think that it serves a really good role as a field of extreme experimentation. So if you’re somebody who makes a continuous glucose monitor traditionally given to diabetics but now lots of people will wear them—and sports people will wear them—you probably gathered a lot of extreme tail distribution data by reading the Reddit/biohackers …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yes.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; … for the last few years, where people were doing things that you would never want them to really do with the CGM [continuous glucose monitor]. And so I think we shouldn’t understate how important that petri dish can be for helping us learn what could happen next.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Oh, I think it’s absolutely going to be essential and a bigger thing in the future. So I think I just want to close here then with one last question. And I always try to be a little bit provocative with this.&lt;/p&gt;



&lt;p&gt;And so as you look ahead to what doctors and nurses and patients might be doing two years from now, five years from now, 10 years from now, do you have any kind of firm predictions?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; I’m going to push the boat out, and I’m going to go further out than closer in.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;OK. [LAUGHS]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR: &lt;/strong&gt;As patients, we will have many, many more touch points and interaction with our biomarkers and our health. We’ll be reading how well we feel through an array of things. And some of them we’ll be wearing directly, like sleep trackers and watches.&lt;/p&gt;



&lt;p&gt;And so we’ll have a better sense of what’s happening in our lives. It’s like the moment you go from paper bank statements that arrive every month to being able to see your account in real time.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yes.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR: &lt;/strong&gt;And I suspect we’ll have … we’ll still have interactions with clinicians because societies that get richer see doctors more, societies that get older see doctors more, and we’re going to be doing both of those over the coming 10 years. But there will be a sense, I think, of continuous health engagement, not in an overbearing way, but just in a sense that we know it’s there, we can check in with it, it’s likely to be data that is compiled on our behalf somewhere centrally and delivered through a user experience that reinforces agency rather than anxiety.&lt;/p&gt;



&lt;p&gt;And we’re learning how to do that slowly. I don’t think the health apps on our phones and devices have yet quite got that right. And that could help us personalize problems before they arise, and again, I use my experience for things that I’ve tracked really, really well. And I know from my data and from how I’m feeling when I’m on the verge of one of those severe asthma attacks that hits me once a year, and I can take a little bit of preemptive measure, so I think that that will become progressively more common and that sense that we will know our baselines.&lt;/p&gt;



&lt;p&gt;I mean, when you think about being an athlete, which is something I think about, but I could never ever do, [LAUGHTER] but what happens is you start with your detailed baselines, and that’s what your health coach looks at every three or four months. For most of us, we have no idea of our baselines. You we get our blood pressure measured once a year. We will have baselines, and that will help us on an ongoing basis to better understand and be in control of our health. And then if the product designers get it right, it will be done in a way that doesn’t feel invasive, but it’ll be done in a way that feels enabling. We’ll still be engaging with clinicians augmented by AI systems more and more because they will also have gone up the stack. They won’t be spending their time on just “take two Tylenol and have a lie down” type of engagements because that will be dealt with earlier on in the system. And so we will be there in a very, very different set of relationships. And they will feel that they have different ways of looking after our health.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Azeem, it’s so comforting to hear such a wonderfully optimistic picture of the future of healthcare. And I actually agree with everything you’ve said.&lt;/p&gt;



&lt;p&gt;Let me just thank you again for joining this conversation. I think it’s been really fascinating. And I think somehow the systemic issues, the systemic issues that you tend to just see with such clarity, I think are going to be the most, kind of, profound drivers of change in the future. So thank you so much.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; Well, thank you, it’s been my pleasure, Peter, thank you.&lt;/p&gt;



&lt;p&gt;[TRANSITION MUSIC] &amp;nbsp;&lt;/p&gt;



&lt;p&gt;I always think of Azeem as a systems thinker. He’s always able to take the experiences of new technologies at an individual level and then project out to what this could mean for whole organizations and whole societies.&lt;/p&gt;



&lt;p&gt;In our conversation, I felt that Azeem really connected some of what we learned in a previous episode—for example, from Chrissy Farr—on the evolving consumerization of healthcare to the broader workforce and economic impacts that we’ve heard about from Ethan Mollick. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;Azeem’s personal story about managing his asthma was also a great example. You know, he imagines a future, as do I, where personal AI might assist and remember decades of personal experience with a condition like asthma and thereby know more than any human being could possibly know in a deeply personalized and effective way, leading to better care. Azeem’s relentless optimism about our AI future was also so heartening to hear.&lt;/p&gt;



&lt;p&gt;Both of these conversations leave me really optimistic about the future of AI in medicine. At the same time, it is pretty sobering to realize just how much we’ll all need to change in pretty fundamental and maybe even in radical ways. I think a big insight I got from these conversations is how we interact with machines is going to have to be altered not only at the individual level, but at the company level and maybe even at the societal level.&lt;/p&gt;



&lt;p&gt;Since my conversation with Ethan and Azeem, there have been some pretty important developments that speak directly to this. Just last week at Build&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, which is Microsoft’s yearly developer conference, we announced a slew of AI agent technologies. Our CEO, Satya Nadella, in fact, started his keynote by going online in a GitHub developer environment and then assigning a coding task to an AI agent, basically treating that AI as a full-fledged member of a development team. Other agents, for example, a meeting facilitator, a data analyst, a business researcher, travel agent, and more were also shown during the conference.&lt;/p&gt;



&lt;p&gt;But pertinent to healthcare specifically, what really blew me away was the demonstration of a healthcare orchestrator agent. And the specific thing here was in Stanford’s cancer treatment center, when they are trying to decide on potentially experimental treatments for cancer patients, they convene a meeting of experts. That is typically called a tumor board. And so this AI healthcare orchestrator agent actually participated as a full-fledged member of a tumor board meeting to help bring data together, make sure that the latest medical knowledge was brought to bear, and to assist in the decision-making around a patient’s cancer treatment. It was pretty amazing.&lt;/p&gt;



&lt;p&gt;[THEME MUSIC]&lt;/p&gt;



&lt;p&gt;A big thank-you again to Ethan and Azeem for sharing their knowledge and understanding of the dynamics between AI and society more broadly. And to our listeners, thank you for joining us. I’m really excited for the upcoming episodes, including discussions on medical students’ experiences with AI and AI’s influence on the operation of health systems and public health departments. We hope you’ll continue to tune in.&lt;/p&gt;



&lt;p&gt;Until next time.&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&lt;/p&gt;

				&lt;/span&gt;
			&lt;/div&gt;
			&lt;button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button"&gt;
				Show more			&lt;/button&gt;
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Illustrated headshots of Azeem Azhar, Peter Lee, and Ethan Mollick." class="wp-image-1140433" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Episode6-PeterEthanAzeem-AIRevolution_Hero_Feature_No_Text_1400x788.jpg" width="1401" /&gt;&lt;/figure&gt;






&lt;p&gt;In November 2022, OpenAI’s ChatGPT kick-started a new era in AI. This was followed less than a half year later by the release of GPT-4. In the months leading up to GPT-4’s public release, Peter Lee, president of Microsoft Research, cowrote a book full of optimism for the potential of advanced AI models to transform the world of healthcare. What has happened since? In this special podcast series, Lee revisits the book, exploring how patients, providers, and other medical professionals are experiencing and using generative AI today while examining what he and his coauthors got right—and what they didn’t foresee.&lt;/p&gt;



&lt;p&gt;In this episode, Ethan Mollick&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and Azeem Azhar&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, thought leaders at the forefront of AI’s impact on work, education, and society, join Lee to discuss how generative AI is reshaping healthcare and organizational systems. Mollick, professor at the Wharton School, discusses the conflicting emotions that come with navigating AI’s effect on the tasks we enjoy and those we don’t; the systemic challenges in AI adoption; and the need for organizations to actively experiment with AI rather than wait for top-down solutions. Azhar, a technology analyst and writer who explores the intersection of AI, economics, and society, explores how generative AI is transforming healthcare through applications like medical scribing, clinician support, and consumer health monitoring.&lt;/p&gt;



&lt;div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow"&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h2 class="wp-block-heading h5" id="learn-more-1"&gt;Learn more:&lt;/h2&gt;




&lt;/div&gt;







&lt;section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast"&gt;
	
&lt;/section&gt;


&lt;div class="wp-block-msr-show-more"&gt;
	&lt;div class="bg-neutral-100 p-5"&gt;
		&lt;div class="show-more-show-less"&gt;
			&lt;div&gt;
				&lt;span&gt;
					

&lt;h2 class="wp-block-heading" id="transcript"&gt;Transcript&lt;/h2&gt;



&lt;p&gt;[MUSIC]  &amp;nbsp;&lt;/p&gt;



&lt;p&gt;[BOOK PASSAGE] &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;PETER LEE:&lt;/strong&gt; “In American primary care, the missing workforce is stunning in magnitude, the shortfall estimated to reach up to 48,000 doctors within the next dozen years. China and other countries with aging populations can expect drastic shortfalls, as well. Just last month, I asked a respected colleague retiring from primary care who he would recommend as a replacement; he told me bluntly that, other than expensive concierge care practices, he could not think of anyone, even for himself. This mismatch between need and supply will only grow, and the US is far from alone among developed countries in facing it.”&lt;/p&gt;



&lt;p&gt;[END OF BOOK PASSAGE]  &amp;nbsp;&lt;/p&gt;



&lt;p&gt;[THEME MUSIC]  &amp;nbsp;&lt;/p&gt;



&lt;p&gt;This is &lt;em&gt;The AI Revolution in Medicine, Revisited&lt;/em&gt;. I’m your host, Peter Lee.  &amp;nbsp;&lt;/p&gt;



&lt;p&gt;Shortly after OpenAI’s GPT-4 was publicly released, Carey Goldberg, Dr. Zak Kohane, and I published &lt;em&gt;The AI Revolution in Medicine &lt;/em&gt;to help educate the world of healthcare and medical research about the transformative impact this new generative AI technology could have. But because we wrote the book when GPT-4 was still a secret, we had to speculate. Now, two years later, what did we get right, and what did we get wrong?   &amp;nbsp;&lt;/p&gt;



&lt;p&gt;In this series, we’ll talk to clinicians, patients, hospital administrators, and others to understand the reality of AI in the field and where we go from here.    &amp;nbsp;&lt;/p&gt;



&lt;p&gt;[THEME MUSIC FADES]&lt;/p&gt;



				&lt;/span&gt;
				&lt;span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1"&gt;
					



&lt;p&gt;The book passage I read at the top is from “Chapter 4: Trust but Verify,” which was written by Zak. &lt;/p&gt;



&lt;p&gt;You know, it’s no secret that in the US and elsewhere shortages in medical staff and the rise of clinician burnout are affecting the quality of patient care for the worse. In our book, we predicted that generative AI would be something that might help address these issues.&lt;/p&gt;



&lt;p&gt;So in this episode, we’ll delve into how individual performance gains that our previous guests have described might affect the healthcare workforce as a whole, and on the patient side, we’ll look into the influence of generative AI on the consumerization of healthcare. Now, since all of this consumes such a huge fraction of the overall economy, we’ll also get into what a general-purpose technology as disruptive as generative AI might mean in the context of labor markets and beyond.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To help us do that, I’m pleased to welcome Ethan Mollick and Azeem Azhar.&lt;/p&gt;



&lt;p&gt;Ethan Mollick is the Ralph J. Roberts Distinguished Faculty Scholar, a Rowan Fellow, and an associate professor at the Wharton School of the University of Pennsylvania. His research into the effects of AI on work, entrepreneurship, and education is applied by organizations around the world, leading him to be named one of &lt;em&gt;Time&lt;/em&gt; magazine’s most influential people in AI for 2024. He’s also the author of the &lt;em&gt;New York Times&lt;/em&gt; best-selling book &lt;em&gt;Co-Intelligence&lt;/em&gt;.&lt;/p&gt;



&lt;p&gt;Azeem Azhar is an author, founder, investor, and one of the most thoughtful and influential voices on the interplay between disruptive emerging technologies and business and society. In his best-selling book, &lt;em&gt;The Exponential Age&lt;/em&gt;, and in his highly regarded newsletter and podcast, &lt;em&gt;Exponential View&lt;/em&gt;, he explores how technologies like AI are reshaping everything from healthcare to geopolitics.&lt;/p&gt;



&lt;p&gt;Ethan and Azeem are two leading thinkers on the ways that disruptive technologies—and especially AI—affect our work, our jobs, our business enterprises, and whole industries. As economists, they are trying to work out whether we are in the midst of an economic revolution as profound as the shift from an agrarian to an industrial society.&lt;/p&gt;



&lt;p&gt;[TRANSITION MUSIC]&lt;/p&gt;



&lt;p&gt;Here is my interview with Ethan Mollick:&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Ethan, welcome.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;ETHAN MOLLICK: &lt;/strong&gt;So happy to be here, thank you.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; I described you as a professor at Wharton, which I think most of the people who listen to this podcast series know of as an elite business school. So it might surprise some people that you study AI. And beyond that, you know, that I would seek you out to talk about AI in medicine. [LAUGHTER] So to get started, how and why did it happen that you’ve become one of &lt;em&gt;the&lt;/em&gt; leading experts on AI?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK:&lt;/strong&gt; It’s actually an interesting story. I’ve been AI-adjacent my whole career. When I was [getting] my PhD at MIT, I worked with Marvin Minsky&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and the MIT [Massachusetts Institute of Technology] Media Labs AI group. But I was never the technical AI guy. I was the person who was trying to explain AI to everybody else who didn’t understand it.&lt;/p&gt;



&lt;p&gt;And then I became very interested in, how do you train and teach? And AI was always a part of that. I was building games for teaching, teaching tools that were used in hospitals and elsewhere, simulations. So when LLMs burst into the scene, I had already been using them and had a good sense of what they could do. And between that and, kind of, being practically oriented and getting some of the first research projects underway, especially under education and AI and performance, I became sort of a go-to person in the field.&lt;/p&gt;



&lt;p&gt;And once you’re in a field where nobody knows what’s going on and we’re all making it up as we go along—I thought it’s funny that you led with the idea that you have a couple of months head start for GPT-4, right. Like that’s all we have at this point, is a few months’ head start. [LAUGHTER] So being a few months ahead is good enough to be an expert at this point. Whether it should be or not is a different question.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Well, if I understand correctly, leading AI companies like OpenAI, Anthropic, and others have now sought you out as someone who should get early access to really start to do early assessments and gauge early reactions. How has that been?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK:&lt;/strong&gt; So, I mean, I think the bigger picture is less about me than about two things that tells us about the state of AI right now.&lt;/p&gt;



&lt;p&gt;One, nobody really knows what’s going on, right. So in a lot of ways, if it wasn’t for your work, Peter, like, I don’t think people would be thinking about medicine as much because these systems weren’t built for medicine. They weren’t built to change education. They weren’t built to write memos. They, like, they weren’t built to do any of these things. They weren’t really built to do anything in particular. It turns out they’re just good at many things.&lt;/p&gt;



&lt;p&gt;And to the extent that the labs work on them, they care about their coding ability above everything else and maybe math and science secondarily. They don’t think about the fact that it expresses high empathy. They don’t think about its accuracy and diagnosis or where it’s inaccurate. They don’t think about how it’s changing education forever.&lt;/p&gt;



&lt;p&gt;So one part of this is the fact that they go to my Twitter feed or ask me for advice is an indicator of where they are, too, which is they’re not thinking about this. And the fact that a few months’ head start continues to give you a lead tells you that we are at the very cutting edge. These labs aren’t sitting on projects for two years and then releasing them. Months after a project is complete or sooner, it’s out the door. Like, there’s very little delay. So we’re kind of all in the same boat here, which is a very unusual space for a new technology.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;And I, you know, explained that you’re at Wharton. Are you an odd fit as a faculty member at Wharton, or is this a trend now even in business schools that AI experts are becoming key members of the faculty?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK: &lt;/strong&gt;I mean, it’s a little of both, right. It’s faculty, so everybody does everything. I’m a professor of innovation-entrepreneurship. I’ve launched startups before and working on that and education means I think about, how do organizations redesign themselves? How do they take advantage of these kinds of problems? So medicine’s always been very central to that, right. A lot of people in my MBA class have been MDs either switching, you know, careers or else looking to advance from being sort of individual contributors to running teams. So I don’t think that’s that bad a fit. But I also think this is general-purpose technology; it’s going to touch everything. The focus on this is medicine, but Microsoft does far more than medicine, right. It’s … there’s transformation happening in literally every field, in every country. This is a widespread effect.&lt;/p&gt;



&lt;p&gt;So I don’t think we should be surprised that business schools matter on this because we care about management. There’s a long tradition of management and medicine going together. There’s actually a great academic paper that shows that teaching hospitals that also have MBA programs associated with them have higher management scores and perform better&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. So I think that these are not as foreign concepts, especially as medicine continues to get more complicated.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah. Well, in fact, I want to dive a little deeper on these issues of management, of entrepreneurship, um, education. But before doing that, if I could just stay focused on you. There is always something interesting to hear from people about their first encounters with AI. And throughout this entire series, I’ve been doing that both &lt;em&gt;pre&lt;/em&gt;-generative AI and post-generative AI. So you, sort of, hinted at the pre-generative AI. You were in Minsky’s lab. Can you say a little bit more about that early encounter? And then tell us about your first encounters with generative AI.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK:&lt;/strong&gt; Yeah. Those are great questions. So first of all, when I was at the media lab, that was pre-the current boom in sort of, you know, even in the old-school machine learning kind of space. So there was a lot of potential directions to head in. While I was there, there were projects underway, for example, to record every interaction small children had. One of the professors was recording everything their baby interacted with in the hope that maybe that would give them a hint about how to build an AI system.&lt;/p&gt;



&lt;p&gt;There was a bunch of projects underway that were about labeling every concept and how they relate to other concepts. So, like, it was very much Wild West of, like, how do we make an AI work—which has been this repeated problem in AI, which is, what is this thing?&lt;/p&gt;



&lt;p&gt;The fact that it was just like brute force over the corpus of all human knowledge turns out to be a little bit of like a, you know, it’s a miracle and a little bit of a disappointment in some ways [LAUGHTER] compared to how elaborate some of this was. So, you know, I think that, that was sort of my first encounters in sort of the intellectual way.&lt;/p&gt;



&lt;p&gt;The generative AI encounters actually started with the original, sort of, GPT-3, or, you know, earlier versions. And it was actually game-based. So I played games like AI Dungeon. And as an educator, I realized, oh my gosh, this stuff could write essays at a fourth-grade level. That’s really going to change the way, like, middle school works, was my thinking at the time. And I was posting about that back in, you know, 2021 that this is a big deal. But I think everybody was taken surprise, including the AI companies themselves, by, you know, ChatGPT, by GPT-3.5. The difference in degree turned out to be a difference in kind.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah, you know, if I think back, even with GPT-3, and certainly this was the case with GPT-2, it was, at least, you know, from where I was sitting, it was hard to get people to really take this seriously and pay attention.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK: &lt;/strong&gt;Yes.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; You know, it’s remarkable. Within Microsoft, I think a turning point was the use of GPT-3 to do code completions. And that was actually productized as GitHub Copilot&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, the very first version. That, I think, is where there was widespread belief. But, you know, in a way, I think there is, even for me early on, a sense of denial and skepticism. Did you have those initially at any point?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK: &lt;/strong&gt;Yeah, I mean, it still happens today, right. Like, this is a weird technology. You know, the original denial and skepticism was, I couldn’t see where this was going. It didn’t seem like a miracle because, you know, of course computers can complete code for you. Like, what else are they supposed to do? Of course, computers can give you answers to questions and write fun things. So there’s difference of moving into a world of generative AI. I think a lot of people just thought that’s what computers could do. So it made the conversations a little weird. But even today, faced with these, you know, with very strong reasoner models that operate at the level of PhD students, I think a lot of people have issues with it, right.&lt;/p&gt;



&lt;p&gt;I mean, first of all, they seem intuitive to use, but they’re not always intuitive to use because the first use case that everyone puts AI to, it fails at because they use it like Google or some other use case. And then it’s genuinely upsetting in a lot of ways. I think, you know, I write in my book about the idea of three sleepless nights. That hasn’t changed. Like, you have to have an intellectual crisis to some extent, you know, and I think people do a lot to avoid having that existential angst of like, “Oh my god, what does it mean that a machine could think—&lt;em&gt;apparently think&lt;/em&gt;—like a person?”&lt;/p&gt;



&lt;p&gt;So, I mean, I see resistance now. I saw resistance then. And then on top of all of that, there’s the fact that the curve of the technology is quite great. I mean, the price of GPT-4 level intelligence from, you know, when it was released has dropped 99.97% at this point, right.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yes. Mm-hmm.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK:&lt;/strong&gt; I mean, I could run a GPT-4 class system basically on my phone. Microsoft’s releasing things that can almost run on like, you know, like it fits in almost no space, that are almost as good as the original GPT-4 models. I mean, I don’t think people have a sense of how fast the trajectory is moving either.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah, you know, there’s something that I think about often. There is this existential dread, or will this technology replace me? But I think the first people to feel that are researchers—people encountering this for the first time. You know, if you were working, let’s say, in Bayesian reasoning or in traditional, let’s say, Gaussian mixture model based, you know, speech recognition, you do get this feeling, &lt;em&gt;Oh, my god, this technology has just solved the problem that I’ve dedicated my life to&lt;/em&gt;. And there is this really difficult period where you have to cope with that. And I think this is going to be spreading, you know, in more and more walks of life. And so this … at what point does that sort of sense of dread hit you, if ever?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK: &lt;/strong&gt;I mean, you know, it’s not even dread as much as like, you know, Tyler Cowen wrote that it’s impossible to not feel a little bit of sadness as you use these AI systems, too. Because, like, I was talking to a friend, just as the most minor example, and his talent that he was very proud of was he was very good at writing limericks for birthday cards. He’d write these limericks. Everyone was always amused by them. [LAUGHTER]&lt;/p&gt;



&lt;p&gt;And now, you know, GPT-4 and GPT-4.5, they made limericks obsolete. Like, anyone can write a good limerick, right. So this was a talent, and it was a little sad. Like, this thing that you cared about mattered.&lt;/p&gt;



&lt;p&gt;You know, as academics, we’re a little used to dead ends, right, and like, you know, some getting the lap. But the idea that entire fields are hitting that way. Like in medicine, there’s a lot of support systems that are now obsolete. And the question is how quickly you change that. In education, a lot of our techniques are obsolete.&lt;/p&gt;



&lt;p&gt;What do you do to change that? You know, it’s like the fact that this brute force technology is good enough to solve so many problems is weird, right. And it’s not just the end of, you know, of our research angles that matter, too. Like, for example, I ran this, you know, 14-person-plus, multimillion-dollar effort at Wharton to build these teaching simulations, and we’re very proud of them. It took years of work to build one.&lt;/p&gt;



&lt;p&gt;Now we’ve built a system that can build teaching simulations on demand by you talking to it with one team member. And, you know, you literally can create any simulation by having a discussion with the AI. I mean, you know, there’s a switch to a new form of excitement, but there is a little bit of like, this mattered to me, and, you know, now I have to change how I do things. I mean, adjustment happens. But if you haven’t had that displacement, I think that’s a good indicator that you haven’t really faced AI yet.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah, what’s so interesting just listening to you is you use words like sadness, and yet I can see the—and hear the—excitement in your voice and your body language. So, you know, that’s also kind of an interesting aspect of all of this.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK:&lt;/strong&gt; Yeah, I mean, I think there’s something on the other side, right. But, like, I can’t say that I haven’t had moments where like, &lt;em&gt;ughhhh&lt;/em&gt;, but then there’s joy and basically like also, you know, freeing stuff up. I mean, I think about doctors or professors, right. These are jobs that bundle together lots of different tasks that you would never have put together, right. If you’re a doctor, you would never have expected the same person to be good at keeping up with the research and being a good diagnostician and being a good manager and being good with people and being good with hand skills.&lt;/p&gt;



&lt;p&gt;Like, who would ever want that kind of bundle? That’s not something you’re all good at, right. And a lot of our stress of our job comes from the fact that we suck at some of it. And so to the extent that AI steps in for that, you kind of feel bad about some of the stuff that it’s doing that you wanted to do. But it’s much more uplifting to be like, I don’t have to do this stuff I’m bad anymore, or I get the support to make myself good at it. And the stuff that I really care about, I can focus on more. Well, because we are at kind of a unique moment where whatever you’re best at, you’re still better than AI. And I think it’s an ongoing question about how long that lasts. But for right now, like you’re not going to say, OK, AI replaces me entirely in my job in medicine. It’s very unlikely.&lt;/p&gt;



&lt;p&gt;But you will say it replaces these 17 things I’m bad at, but I never liked that anyway. So it’s a period of both excitement and a little anxiety.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah, I’m going to want to get back to this question about in what ways AI may or may not replace doctors or some of what doctors and nurses and other clinicians do. But before that, let’s get into, I think, the real meat of this conversation. In previous episodes of this podcast, we talked to clinicians and healthcare administrators and technology developers that are very rapidly injecting AI today to do various forms of workforce automation, you know, automatically writing a clinical encounter note, automatically filling out a referral letter or request for prior authorization for some reimbursement to an insurance company.&lt;/p&gt;



&lt;p&gt;And so these sorts of things are intended not only to make things more efficient and lower costs but also to reduce various forms of drudgery, &lt;em&gt;cognitive burden&lt;/em&gt; on frontline health workers. So how do you think about the impact of AI on that aspect of workforce, and, you know, what would you expect will happen over the next few years in terms of impact on efficiency and costs?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK: &lt;/strong&gt;So I mean, this is a case where I think we’re facing the big bright problem in AI in a lot of ways, which is that this is … at the individual level, there’s lots of performance gains to be gained, right. The problem, though, is that we as individuals fit into systems, in medicine as much as anywhere else or more so, right. Which is that you could individually boost your performance, but it’s also about systems that fit along with this, right.&lt;/p&gt;



&lt;p&gt;So, you know, if you could automatically, you know, record an encounter, if you could automatically make notes, does that change what you should be expecting for notes or the value of those notes or what they’re for? How do we take what one person does and validate it across the organization and roll it out for everybody without making it a 10-year process that it feels like IT in medicine often is? Like, so we’re in this really interesting period where there’s incredible amounts of individual innovation in productivity and performance improvements in this field, like very high levels of it, but not necessarily seeing that same thing translate to organizational efficiency or gains.&lt;/p&gt;



&lt;p&gt;And one of my big concerns is seeing that happen. We’re seeing that in nonmedical problems, the same kind of thing, which is, you know, we’ve got research showing 20 and 40% performance improvements, like not uncommon to see those things. But then the organization doesn’t capture it; the system doesn’t capture it. Because the individuals are doing their own work and the systems don’t have the ability to, kind of, learn or adapt as a result.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; You know, where are those productivity gains going, then, when you get to the organizational level?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK: &lt;/strong&gt;Well, they’re dying for a few reasons. One is, there’s a tendency for individual contributors to underestimate the power of management, right.&lt;/p&gt;



&lt;p&gt;Practices associated with good management increase happiness, decrease, you know, issues, increase success rates. In the same way, about 40%, as far as we can tell, of the US advantage over other companies, of US firms, has to do with management ability. Like, management is a big deal. Organizing is a big deal. Thinking about how you coordinate is a big deal.&lt;/p&gt;



&lt;p&gt;At the individual level, when things get stuck there, right, you can’t start bringing them up to how systems work together. It becomes, &lt;em&gt;How do I deal with a doctor that has a 60% performance improvement? &lt;/em&gt;We really only have one thing in our playbook for doing that right now, which is, &lt;em&gt;OK, we could fire 40% of the other doctors and still have a performance gain&lt;/em&gt;, which is not the answer you want to see happen.&lt;/p&gt;



&lt;p&gt;So because of that, people are hiding their use. They’re actually hiding their use for lots of reasons.&lt;/p&gt;



&lt;p&gt;And it’s a weird case because the people who are able to figure out best how to use these systems, for a lot of use cases, they’re actually clinicians themselves because they’re experimenting all the time. Like, they have to take those encounter notes. And if they figure out a better way to do it, they figure that out. You don’t want to wait for, you know, a med tech company to figure that out and then sell that back to you when it can be done by the physicians themselves.&lt;/p&gt;



&lt;p&gt;So we’re just not used to a period where everybody’s innovating and where the management structure isn’t in place to take advantage of that. And so we’re seeing things stalled at the individual level, and people are often, especially in risk-averse organizations or organizations where there’s lots of regulatory hurdles, people are so afraid of the regulatory piece that they don’t even bother trying to make change.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; If you are, you know, the leader of a hospital or a clinic or a whole health system, how should you approach this? You know, how should you be trying to extract positive success out of AI?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK: &lt;/strong&gt;So I think that you need to embrace the right kind of risk, right. We don’t want to put risk on our patients … like, we don’t want to put uninformed risk. But innovation involves risk to how organizations operate. They involve change. So I think part of this is embracing the idea that R&amp;amp;D has to happen in organizations again.&lt;/p&gt;



&lt;p&gt;What’s happened over the last 20 years or so has been organizations giving that up. Partially, that’s a trend to focus on what you’re good at and not try and do this other stuff. Partially, it’s because it’s outsourced now to software companies that, like, Salesforce tells you how to organize your sales team. Workforce tells you how to organize your organization. Consultants come in and will tell you how to make change based on the average of what other people are doing in your field.&lt;/p&gt;



&lt;p&gt;So companies and organizations and hospital systems have all started to give up their ability to create their own organizational change. And when I talk to organizations, I often say they have to have two approaches. They have to think about the crowd and the lab.&lt;/p&gt;



&lt;p&gt;So the crowd is the idea of how to empower clinicians and administrators and supporter networks to start using AI and experimenting in ethical, legal ways and then sharing that information with each other. And the lab is, how are we doing R&amp;amp;D about the approach of how to [get] AI to work, not just in direct patient care, right. But also fundamentally, like, what paperwork can you cut out? How can we better explain procedures? Like, what management role can this fill?&lt;/p&gt;



&lt;p&gt;And we need to be doing active experimentation on that. We can’t just wait for, you know, Microsoft to solve the problems. It has to be at the level of the organizations themselves.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; So let’s shift a little bit to the patient. You know, one of the things that we see, and I think everyone is seeing, is that people are turning to chatbots, like ChatGPT, actually to seek healthcare information for, you know, their own health or the health of their loved ones.&lt;/p&gt;



&lt;p&gt;And there was already, prior to all of this, a trend towards, let’s call it, consumerization of healthcare. So just in the business of healthcare delivery, do you think AI is going to hasten these kinds of trends, or from the consumer’s perspective, what … ?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK: &lt;/strong&gt;I mean, absolutely, right. Like, all the early data that we have suggests that for most common medical problems, you should just consult AI, too, right. In fact, there is a real question to ask: at what point does it become unethical for doctors themselves to not ask for a second opinion from the AI because it’s cheap, right? You could overrule it or whatever you want, but like not asking seems foolish.&lt;/p&gt;



&lt;p&gt;I think the two places where there’s a burning almost, you know, moral imperative is … let’s say, you know, I’m in Philadelphia, I’m a professor, I have access to really good healthcare through the Hospital University of Pennsylvania system. I know doctors. You know, I’m lucky. I’m well connected. If, you know, something goes wrong, I have friends who I can talk to. I have specialists. I’m, you know, pretty well educated in this space.&lt;/p&gt;



&lt;p&gt;But for most people on the planet, they don’t have access to good medical care, they don’t have good health. It feels like it’s absolutely imperative to say when should you use AI and when not. Are there blind spots? What are those things?&lt;/p&gt;



&lt;p&gt;And I worry that, like, to me, that would be the crash project I’d be invoking because I’m doing the same thing in education, which is this system is not as good as being in a room with a great teacher who also uses AI to help you, but it’s better than not getting an, you know, to the level of education people get in many cases. Where should we be using it? How do we guide usage in the right way? Because the AI labs aren’t thinking about this. We have to.&lt;/p&gt;



&lt;p&gt;So, to me, there is a burning need here to understand this. And I worry that people will say, you know, everything that’s true—AI can hallucinate, AI can be biased. All of these things are absolutely true, but people are going to use it. The early indications are that it is quite useful. And unless we take the active role of saying, here’s when to use it, here’s when not to use it, we don’t have a right to say, don’t use this system. And I think, you know, we have to be exploring that.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; What do people need to understand about AI? And what should schools, universities, and so on be teaching?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK:&lt;/strong&gt; Those are, kind of, two separate questions in lot of ways. I think a lot of people want to teach AI &lt;em&gt;skills&lt;/em&gt;, and I will tell you, as somebody who works in this space a lot, there isn’t like an easy, sort of, AI skill, right. I could teach you prompt engineering in two to three classes, but every indication we have is that for most people under most circumstances, the value of prompting, you know, any one case is probably not that useful.&lt;/p&gt;



&lt;p&gt;A lot of the tricks are disappearing because the AI systems are just starting to use them themselves. So asking good questions, being a good manager, being a good thinker tend to be important, but like magic tricks around making, you know, the AI do something because you use the right phrase used to be something that was real but is rapidly disappearing.&lt;/p&gt;



&lt;p&gt;So I worry when people say teach AI skills. No one’s been able to articulate to me as somebody who knows AI very well and teaches classes on AI, what those AI skills that everyone should learn are, right.&lt;/p&gt;



&lt;p&gt;I mean, there’s value in learning a little bit how the models work. There’s a value in working with these systems. A lot of it’s just hands on keyboard kind of work. But, like, we don’t have an easy slam dunk “this is what you learn in the world of AI” because the systems are getting better, and as they get better, they get less sensitive to these prompting techniques. They get better prompting themselves. They solve problems spontaneously and start being agentic. So it’s a hard problem to ask about, like, what do you train someone on? I think getting people experience in hands-on-keyboards, getting them to … there’s like four things I could teach you about AI, and two of them are already starting to disappear.&lt;/p&gt;



&lt;p&gt;But, like, one is be direct. Like, tell the AI exactly what you want. That’s very helpful. Second, provide as much context as possible. That can include things like acting as a doctor, but also all the information you have. The third is give it step-by-step directions—that’s becoming less important. And the fourth is good and bad examples of the kind of output you want. Those four, that’s like, that’s it as far as the research telling you what to do, and the rest is building intuition.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; I’m really impressed that you didn’t give the answer, “Well, everyone should be teaching my book, &lt;em&gt;Co-Intelligence&lt;/em&gt;.” [LAUGHS]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK:&lt;/strong&gt; Oh, no, sorry! Everybody should be teaching my book &lt;em&gt;Co-Intelligence&lt;/em&gt;. I apologize. [LAUGHTER]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; It’s good to chuckle about that, but actually, I can’t think of a better book, like, if you were to assign a textbook in any professional education space, I think &lt;em&gt;Co-Intelligence&lt;/em&gt; would be number one on my list. Are there other things that you think are essential reading?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK:&lt;/strong&gt; That’s a really good question. I think that a lot of things are evolving very quickly. I happen to, kind of, hit a sweet spot with &lt;em&gt;Co-Intelligence&lt;/em&gt; to some degree because I talk about how I used it, and I was, sort of, an advanced user of these systems.&lt;/p&gt;



&lt;p&gt;So, like, it’s, sort of, like my Twitter feed, my online newsletter. I’m just trying to, kind of, in some ways, it’s about trying to make people aware of what these systems can do by just showing a lot, right. Rather than picking one thing, and, like, this is a general-purpose technology. Let’s use it for this. And, like, everybody gets a light bulb for a different reason. So more than reading, it is using, you know, and that can be Copilot or whatever your favorite tool is.&lt;/p&gt;



&lt;p&gt;But using it. Voice modes help a lot. In terms of readings, I mean, I think that there is a couple of good guides to understanding AI that were originally blog posts. I think Tim Lee has one called Understanding AI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, and it had a good overview …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah, that’s a great one.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK: &lt;/strong&gt;… of that topic that I think explains how transformers work, which can give you some mental sense. I think [Andrej] Karpathy&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; has some really nice videos of use that I would recommend.&lt;/p&gt;



&lt;p&gt;Like on the medical side, I think the book that you did, if you’re in medicine, you should read that. I think that that’s very valuable. But like all we can offer are hints in some ways. Like there isn’t … if you’re looking for the instruction manual, I think it can be very frustrating because it’s like you want the best practices and procedures laid out, and we cannot do that, right. That’s not how a system like this works.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK:&lt;/strong&gt; It’s not a person, but thinking about it like a person can be helpful, right.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; One of the things that has been sort of a fun project for me for the last few years is I have been a founding board member of a new medical school at Kaiser Permanente. And, you know, that medical school curriculum is being formed in this era. But it’s been perplexing to understand, you know, what this means for a medical school curriculum. And maybe even more perplexing for me, at least, is the accrediting bodies, which are extremely important in US medical schools; how accreditors should think about what’s necessary here.&lt;/p&gt;



&lt;p&gt;Besides the things that you’ve … the, kind of, four key ideas you mentioned, if you were talking to the board of directors of the LCME [Liaison Committee on Medical Education] accrediting body, what’s the one thing you would want them to really internalize?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK: &lt;/strong&gt;This is both a fast-moving and vital area. This can’t be viewed like a usual change, which [is], “Let’s see how this works.” Because it’s, like, the things that make medical technologies hard to do, which is like unclear results, limited, you know, expensive use cases where it rolls out slowly. So one or two, you know, advanced medical facilities get access to, you know, proton beams or something else at multi-billion dollars of cost, and that takes a while to diffuse out. That’s not happening here. This is all happening at the same time, all at once. This is now … AI &lt;em&gt;is&lt;/em&gt; part of medicine.&lt;/p&gt;



&lt;p&gt;I mean, there’s a minor point that I’d make that actually is a really important one, which is large language models, generative AI overall, work incredibly differently than other forms of AI. So the other worry I have with some of these accreditors is they blend together &lt;em&gt;algorithmic&lt;/em&gt; forms of AI, which medicine has been trying for long time—decision support, algorithmic methods, like, medicine more so than other places has been thinking about those issues. Generative AI, even though it uses the same underlying techniques, is a completely different beast.&lt;/p&gt;



&lt;p&gt;So, like, even just take the most simple thing of algorithmic aversion, which is a well-understood problem in medicine, right. Which is, so you have a tool that could tell you as a radiologist, you know, the chance of this being cancer; you don’t like it, you overrule it, right.&lt;/p&gt;



&lt;p&gt;We don’t find algorithmic aversion happening with LLMs in the same way. People actually enjoy using them because it’s more like working with a person. The flaws are different. The approach is different. So you need to both view this as universal applicable &lt;em&gt;today&lt;/em&gt;, which makes it urgent, but also as something that is not the same as your other form of AI, and your AI working group that is thinking about how to solve this problem is not the right people here.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; You know, I think the world has been trained because of the magic of web search to view computers as question-answering machines. Ask a question, get an answer.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK: &lt;/strong&gt;Yes. Yes.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Write a query, get results. And as I have interacted with medical professionals, you can see that medical professionals have that model of a machine in mind. And I think that’s partly, I think psychologically, why hallucination is so alarming. Because you have a mental model of a computer as a machine that has absolutely rock-solid perfect memory recall.&lt;/p&gt;



&lt;p&gt;But the thing that was so powerful in &lt;em&gt;Co-Intelligence&lt;/em&gt;, and we tried to get at this in our book also, is that’s not the sweet spot. It’s this sort of deeper interaction, more of a collaboration. And I thought your use of the term &lt;em&gt;Co-Intelligence&lt;/em&gt; really just even in the title of the book tried to capture this. When I think about education, it seems like that’s the first step, to get past this concept of a machine being just a question-answering machine. Do you have a reaction to that idea?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK:&lt;/strong&gt; I think that’s very powerful. You know, we’ve been trained over so many years at both using computers but also in science fiction, right. Computers are about cold logic, right. They will give you the right answer, but if you ask it what love is, they explode, right. Like that’s the classic way you defeat the evil robot in &lt;em&gt;Star Trek&lt;/em&gt;, right. “Love does not compute.” [LAUGHTER]&lt;/p&gt;



&lt;p&gt;Instead, we have a system that makes mistakes, is warm, beats doctors in empathy in almost every controlled study on the subject, right. Like, absolutely can outwrite you in a sonnet but will absolutely struggle with giving you the right answer every time. And I think our mental models are just broken for this. And I think you’re absolutely right. And that’s part of what I thought your book does get at really well is, like, this is a different thing. It’s also generally applicable. Again, the model in your head should be kind of like a person even though it isn’t, right.&lt;/p&gt;



&lt;p&gt;There’s a lot of warnings and caveats to it, but if you start from &lt;em&gt;person&lt;/em&gt;, &lt;em&gt;smart person&lt;/em&gt; you’re talking to, your mental model will be more accurate than smart machine, even though both are flawed examples, right. So it will make mistakes; it will make errors. The question is, what do you trust it on? What do you not trust it? As you get to know a model, you’ll get to understand, like, I totally don’t trust it for this, but I absolutely trust it for that, right.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; All right. So we’re getting to the end of the time we have together. And so I’d just like to get now into something a little bit more provocative. And I get the question all the time. You know, will AI replace doctors? In medicine and other advanced knowledge work, project out five to 10 years. What do think happens?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK: &lt;/strong&gt;OK, so first of all, let’s acknowledge systems change much more slowly than individual use. You know, doctors are not individual actors; they’re part of systems, right. So not just the system of a patient who like may or may not want to talk to a machine instead of a person but also legal systems and administrative systems and systems that allocate labor and systems that train people.&lt;/p&gt;



&lt;p&gt;So, like, it’s hard to imagine that in five to 10 years medicine being so upended that even if AI was better than doctors at every single thing doctors do, that we’d actually see as radical a change in medicine as you might in other fields. I think you will see faster changes happen in consulting and law and, you know, coding, other spaces than medicine.&lt;/p&gt;



&lt;p&gt;But I do think that there is good reason to suspect that AI will outperform people while still having flaws, right. That’s the difference. We’re already seeing that for common medical questions in enough randomized controlled trials that, you know, best doctors beat AI, but the AI beats the mean doctor, right. Like, that’s just something we should acknowledge is happening at this point.&lt;/p&gt;



&lt;p&gt;Now, will that work in your specialty? No. Will that work with all the contingent social knowledge that you have in your space? Probably not.&lt;/p&gt;



&lt;p&gt;Like, these are vignettes, right. But, like, that’s kind of where things are. So let’s assume, right … you’re asking two questions. One is, how good will AI get?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK:&lt;/strong&gt; And we don’t know the answer to that question. I will tell you that your colleagues at Microsoft and increasingly the labs, the AI labs themselves, are all saying they think they’ll have a machine smarter than a human at every intellectual task in the next two to three years. If that doesn’t happen, that makes it easier to assume the future, but let’s just assume that that’s the case. I think medicine starts to change with the idea that people feel obligated to use this to help for everything.&lt;/p&gt;



&lt;p&gt;Your patients will be using it, and it will be your advisor and helper at the beginning phases, right. And I think that I expect people to be better at empathy. I expect better bedside manner. I expect management tasks to become easier. I think administrative burden might lighten if we handle this right way or much worse if we handle it badly. Diagnostic accuracy will increase, right.&lt;/p&gt;



&lt;p&gt;And then there’s a set of discovery pieces happening, too, right. One of the core goals of all the AI companies is to accelerate medical research. How does that happen and how does that affect us is a, kind of, unknown question. So I think clinicians are in both the eye of the storm and surrounded by it, right. Like, they can resist AI use for longer than most other fields, but everything around them is going to be affected by it.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Well, Ethan, this has been really a fantastic conversation. And, you know, I think in contrast to all the other conversations we’ve had, this one gives especially the leaders in healthcare, you know, people actually trying to lead their organizations into the future, whether it’s in education or in delivery, a lot to think about. So I really appreciate you joining.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;MOLLICK:&lt;/strong&gt; Thank you.&lt;/p&gt;



&lt;p&gt;[TRANSITION MUSIC] &amp;nbsp;&lt;/p&gt;



&lt;p&gt;I’m a computing researcher who works with people who are right in the middle of today’s bleeding-edge developments in AI. And because of that, I often lose sight of how to talk to a broader audience about what it’s all about. And so I think one of Ethan’s superpowers is that he has this knack for explaining complex topics in AI in a really accessible way, getting right to the most important points without making it so simple as to be useless. That’s why I rarely miss an opportunity to read up on his latest work.&lt;/p&gt;



&lt;p&gt;One of the first things I learned from Ethan is the intuition that you can, sort of, think of AI as a very knowledgeable intern. In other words, think of it as a persona that you can interact with, but you also need to be a manager for it and to always assess the work that it does.&lt;/p&gt;



&lt;p&gt;In our discussion, Ethan went further to stress that there is, because of that, a serious education gap. You know, over the last decade or two, we’ve all been trained, mainly by search engines, to think of computers as question-answering machines. In medicine, in fact, there’s a question-answering application that is really popular called UpToDate&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. Doctors use it all the time. But generative AI systems like ChatGPT are different. There’s therefore a challenge in how to break out of the old-fashioned mindset of search to get the full value out of generative AI.&lt;/p&gt;



&lt;p&gt;The other big takeaway for me was that Ethan pointed out while it’s easy to see productivity gains from AI at the individual level, those same gains, at least today, don’t often translate automatically to organization-wide or system-wide gains. And one, of course, has to conclude that it takes more than just making individuals more productive; the whole system also has to adjust to the realities of AI.&lt;/p&gt;



&lt;p&gt;Here’s now my interview with Azeem Azhar:&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Azeem, welcome.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZEEM AZHAR: &lt;/strong&gt;Peter, thank you so much for having me.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; You know, I think you’re extremely well known in the world. But still, some of the listeners of this podcast series might not have encountered you before.&lt;/p&gt;



&lt;p&gt;And so one of the ways I like to ask people to introduce themselves is, how do you explain to your parents what you do every day?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR: &lt;/strong&gt;Well, I’m very lucky in that way because my mother was the person who got me into computers more than 40 years ago. And I still have that first computer, a ZX81 with a Z80 chip …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Oh wow.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; … to this day. It sits in my study, all seven and a half thousand transistors and Bakelite plastic that it is. And my parents were both economists, and economics is deeply connected with technology in some sense. And I grew up in the late ’70s and the early ’80s. And that was a time of tremendous optimism around technology. It was space opera, science fiction, robots, and of course, the personal computer and, you know, Bill Gates and Steve Jobs. So that’s where I started.&lt;/p&gt;



&lt;p&gt;And so, in a way, my mother and my dad, who passed away a few years ago, had always known me as someone who was fiddling with computers but also thinking about economics and society. And so, in a way, it’s easier to explain to them because they’re the ones who nurtured the environment that allowed me to research technology and AI and think about what it means to firms and to the economy at large.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; I always like to understand the origin story. And what I mean by that is, you know, what was your first encounter with generative AI? And what was that like? What did you go through?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; The first real moment was when Midjourney and Stable Diffusion emerged in that summer of 2022. I’d been away on vacation, and I came back—and I’d been off grid, in fact—and the world had really changed.&lt;/p&gt;



&lt;p&gt;Now, I’d been aware of GPT-3 and GPT-2, which I played around with and with BERT, the original transformer paper about seven or eight years ago, but it was the moment where I could talk to my computer, and it could produce these images, and it could be refined in natural language that really made me think we’ve crossed into a new domain. We’ve gone from AI being highly discriminative to AI that’s able to explore the world in particular ways. And then it was a few months later that ChatGPT came out—November, the 30th.&lt;/p&gt;



&lt;p&gt;And I think it was the next day or the day after that I said to my team, everyone has to use this, and we have to meet every morning and discuss how we experimented the day before. And we did that for three or four months. And, you know, it was really clear to me in that interface at that point that, you know, we’d absolutely pass some kind of threshold.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; And who’s the &lt;em&gt;we&lt;/em&gt; that you were experimenting with?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; So I have a team of four who support me. They’re mostly researchers of different types. I mean, it’s almost like one of those jokes. You know, I have a sociologist, an economist, and an astrophysicist. And, you know, they walk into the bar, [LAUGHTER] or they walk into our virtual team room, and we try to solve problems.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Well, so let’s get now into brass tacks here. And I think I want to start maybe just with an exploration of the economics of all this and economic realities. Because I think in a lot of your work—for example, in your book—you look pretty deeply at how automation generally and AI specifically are transforming certain sectors like finance, manufacturing, and you have a really, kind of, insightful focus on what this means for productivity and which ways, you know, efficiencies are found. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;And then you, sort of, balance that with risks, things that can and do go wrong. And so as you take that background and looking at all those other sectors, in what ways are the same patterns playing out or likely to play out in healthcare and medicine?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; I’m sure we will see really remarkable parallels but also new things going on. I mean, medicine has a particular quality compared to other sectors in the sense that it’s highly regulated, market structure is very different country to country, and it’s an incredibly broad field. I mean, just think about taking a Tylenol and going through laparoscopic surgery. Having an MRI and seeing a physio. I mean, this is all medicine. I mean, it’s hard to imagine a sector that is [LAUGHS] more broad than that.&lt;/p&gt;



&lt;p&gt;So I think we can start to break it down, and, you know, where we’re seeing things with generative AI will be that the, sort of, softest entry point, which is the medical scribing. And I’m sure many of us have been with clinicians who have a medical scribe running alongside—they’re all on Surface Pros I noticed, right? [LAUGHTER] They’re on the tablet computers, and they’re scribing away.&lt;/p&gt;



&lt;p&gt;And what that’s doing is, in the words of my friend Eric Topol, it’s giving the clinician time back&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, right. They have time back from days that are extremely busy and, you know, full of administrative overload. So I think you can obviously do a great deal with reducing that overload.&lt;/p&gt;



&lt;p&gt;And within my team, we have a view, which is if you do something five times in a week, you should be writing an automation for it. And if you’re a doctor, you’re probably reviewing your notes, writing the prescriptions, and so on several times a day. So those are things that can clearly be automated, and the human can be in the loop. But I think there are so many other ways just within the clinic that things can help.&lt;/p&gt;



&lt;p&gt;So, one of my friends, my friend from my junior school—I’ve known him since I was 9—is an oncologist who’s also deeply into machine learning, and he’s in Cambridge in the UK. And he built with Microsoft Research a suite of imaging AI tools from his own discipline, which they then open sourced.&lt;/p&gt;



&lt;p&gt;So that’s another way that you have an impact, which is that you actually enable the, you know, generalist, specialist, polymath, whatever they are in health systems to be able to get this technology, to tune it to their requirements, to use it, to encourage some grassroots adoption in a system that’s often been very, very heavily centralized.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; And then I think there are some other things that are going on that I find really, really exciting. So one is the consumerization of healthcare. So I have one of those sleep tracking rings, the Oura&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yup.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; That is building a data stream that we’ll be able to apply more and more AI to. I mean, right now, it’s applying traditional, I suspect, machine learning, but you can imagine that as we start to get more data, we start to get more used to measuring ourselves, we create this sort of pot, a personal asset that we can turn AI to.&lt;/p&gt;



&lt;p&gt;And there’s still another category. And that other category is one of the completely novel ways in which we can enable patient care and patient pathway. And there’s a fantastic startup in the UK called Neko Health&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, which, I mean, does physicals, MRI scans, and blood tests, and so on.&lt;/p&gt;



&lt;p&gt;It’s hard to imagine Neko existing without the sort of advanced data, machine learning, AI that we’ve seen emerge over the last decade. So, I mean, I think that there are so many ways in which the temperature is slowly being turned up to encourage a phase change within the healthcare sector.&lt;/p&gt;



&lt;p&gt;And last but not least, I do think that these tools can also be very, very supportive of a clinician’s life cycle. I think we, as patients, we’re a bit …&amp;nbsp; I don’t know if we’re as grateful as we should be for our clinicians who are putting in 90-hour weeks. [LAUGHTER] But you can imagine a world where AI is able to support not just the clinicians’ workload but also their sense of stress, their sense of burnout.&lt;/p&gt;



&lt;p&gt;So just in those five areas, Peter, I sort of imagine we could start to fundamentally transform over the course of many years, of course, the way in which people think about their health and their interactions with healthcare systems&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; I love how you break that down. And I want to press on a couple of things.&lt;/p&gt;



&lt;p&gt;You also touched on the fact that medicine is, at least in most of the world, is a highly regulated industry. I guess finance is the same way, but they also feel different because the, like, finance sector has to be very responsive to consumers, and consumers are sensitive to, you know, an abundance of choice; they are sensitive to price. Is there something unique about medicine besides being regulated?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; I mean, there absolutely is. And in finance, as well, you have much clearer end states. So if you’re not in the consumer space, but you’re in the, you know, asset management space, you have to essentially deliver returns against the volatility or risk boundary, right. That’s what you have to go out and do. And I think if you’re in the consumer industry, you can come back to very, very clear measures, net promoter score being a very good example.&lt;/p&gt;



&lt;p&gt;In the case of medicine and healthcare, it is much more complicated because as far as the clinician is concerned, people are individuals, and we have our own parts and our own responses. If we didn’t, there would never be a need for a differential diagnosis. There’d never be a need for, you know, &lt;em&gt;Let’s try azithromycin first, and then if that doesn’t work, we’ll go to vancomycin,&lt;/em&gt; or, you know, whatever it happens to be. You would just know. But ultimately, you know, people are quite different. The symptoms that they’re showing are quite different, and also their compliance is really, really different.&lt;/p&gt;



&lt;p&gt;I had a back problem that had to be dealt with by, you know, a physio and extremely boring exercises four times a week, but I was ruthless in complying, and my physio was incredibly surprised. He’d say well no one ever does this, and I said, well you know the thing is that I kind of just want to get this thing to go away.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; And I think that that’s why medicine is and healthcare is so different and more complex. But I also think that’s why AI can be really, really helpful. I mean, we didn’t talk about, you know, AI in its ability to potentially do this, which is to extend the clinician’s presence throughout the week.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Right. Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; The idea that maybe some part of what the clinician would do if you could talk to them on Wednesday, Thursday, and Friday could be delivered through an app or a chatbot just as a way of encouraging the compliance, which is often, especially with older patients, one reason why conditions, you know, linger on for longer.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; You know, just staying on the regulatory thing, as I’ve thought about this, the one regulated sector that I think seems to have some parallels to healthcare is energy delivery, energy distribution.&lt;/p&gt;



&lt;p&gt;Because like healthcare, as a consumer, I don’t have choice in who delivers electricity to my house. And even though I care about it being cheap or at least not being overcharged, I don’t have an abundance of choice. I can’t do price comparisons.&lt;/p&gt;



&lt;p&gt;And there’s something about that, just speaking as a consumer of both energy and a consumer of healthcare, that feels similar. Whereas other regulated industries, you know, somehow, as a consumer, I feel like I have a lot more direct influence and power. Does that make any sense to someone, you know, like you, who’s really much more expert in how economic systems work?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; I mean, in a sense, one part of that is very, very true. You have a limited panel of energy providers you can go to, and in the US, there may be places where you have no choice.&lt;/p&gt;



&lt;p&gt;I think the area where it’s slightly different is that as a consumer or a patient, you can actually make meaningful choices and changes yourself using these technologies, and people used to joke about you know asking Dr. Google. But Dr. Google is not terrible, particularly if you go to WebMD. And, you know, when I look at long-range change, many of the regulations that exist around healthcare delivery were formed at a point before people had access to good quality information at the touch of their fingertips or when educational levels in general were much, much lower. And many regulations existed because of the incumbent power of particular professional sectors.&lt;/p&gt;



&lt;p&gt;I’ll give you an example from the United Kingdom. So I have had asthma all of my life. That means I’ve been taking my inhaler, Ventolin, and maybe a steroid inhaler for nearly 50 years. That means that I know … actually, I’ve got more experience, and I—in some sense—know more about it than a general practitioner.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; And until a few years ago, I would have to go to a general practitioner to get this drug that I’ve been taking for five decades, and there they are, age 30 or whatever it is. And a few years ago, the regulations changed. And now pharmacies can … or pharmacists can prescribe those types of drugs under certain conditions directly.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; That was not to do with technology. That was to do with incumbent lock-in. So when we look at the medical industry, the healthcare space, there are some parallels with energy, but there are a few little things that the ability that the consumer has to put in some effort to learn about their condition, but also the fact that some of the regulations that exist just exist because certain professions are powerful.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah, one last question while we’re still on economics. There seems to be a conundrum about productivity and efficiency in healthcare delivery because I’ve never encountered a doctor or a nurse that wants to be able to handle even more patients than they’re doing on a daily basis.&lt;/p&gt;



&lt;p&gt;And so, you know, if productivity means simply, well, your rounds can now handle &lt;em&gt;16&lt;/em&gt; patients instead of eight patients, that doesn’t seem necessarily to be a desirable thing. So how can we or should we be thinking about efficiency and productivity since obviously costs are, in most of the developed world, are a huge, huge problem?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; Yes, and when you described doubling the number of patients on the round, I imagined you buying them all roller skates so they could just whizz around [LAUGHTER] the hospital faster and faster than ever before.&lt;/p&gt;



&lt;p&gt;We can learn from what happened with the introduction of electricity. Electricity emerged at the end of the 19th century, around the same time that cars were emerging as a product, and car makers were very small and very artisanal. And in the early 1900s, some really smart car makers figured out that electricity was going to be important. And they bought into this technology by putting pendant lights in their workshops so they could “visit more patients.” Right?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah, yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; They could effectively spend more hours working, and that was a productivity enhancement, and it was noticeable. But, of course, electricity &lt;em&gt;fundamentally&lt;/em&gt; changed the productivity by orders of magnitude of people who made cars starting with Henry Ford because he was able to reorganize his factories around the electrical delivery of power and to therefore have the moving assembly line, which 10xed the productivity of that system.&lt;/p&gt;



&lt;p&gt;So when we think about how AI will affect the clinician, the nurse, the doctor, it’s much easier for us to imagine it as the pendant light that just has them working later …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; … than it is to imagine a reconceptualization of the relationship between the clinician and the people they care for.&lt;/p&gt;



&lt;p&gt;And I’m not sure. I don’t think anybody knows what that looks like. But, you know, I do think that there will be a way that this changes, and you can see that scale out factor. And it may be, Peter, that what we end up doing is we end up saying, OK, because we have these brilliant AIs, there’s a lower level of training and cost and expense that’s required for a broader range of conditions that need treating. And that expands the market, right. That expands the market hugely. It’s what has happened in the market for taxis or ride sharing. The introduction of Uber and the GPS system …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yup.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; … has meant many more people now earn their living driving people around in their cars. And at least in London, you had to be reasonably highly trained to do that.&lt;/p&gt;



&lt;p&gt;So I can see a reorganization is possible. Of course, entrenched interests, the economic flow … and there are many entrenched interests, particularly in the US between the health systems and the, you know, professional bodies that might slow things down. But I think a reimagining is possible.&lt;/p&gt;



&lt;p&gt;And if I may, I’ll give you one example of that, which is, if you go to countries outside of the US where there are many more sick people per doctor, they have incentives to change the way they deliver their healthcare. And well before there was AI of this quality around, there was a few cases of health systems in India—Aravind Eye Care&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; was one, and Narayana Hrudayalaya [now known as Narayana Health&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;] was another. And in the latter, they were a cardiac care unit where you couldn’t get enough heart surgeons.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah, yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; So specially trained nurses would operate under the supervision of a single surgeon who would supervise many in parallel. So there are ways of increasing the quality of care, reducing the cost, but it does require a systems change. And we can’t expect a single bright algorithm to do it on its own.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah, really, really interesting. So now let’s get into regulation. And let me start with this question. You know, there are several startup companies I’m aware of that are pushing on, I think, a near-term future possibility that a medical AI for consumer might be allowed, say, to prescribe a medication for you, something that would normally require a doctor or a pharmacist, you know, that is certified in some way, licensed to do. Do you think we’ll get to a point where for certain regulated activities, humans are more or less cut out of the loop?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; Well, humans would have been in the loop because they would have provided the training data, they would have done the oversight, the quality control. But to your question in general, would we delegate an important decision entirely to a tested set of algorithms? I’m sure we will. We already do that. I delegate less important decisions like, &lt;em&gt;What time should I leave for the airport &lt;/em&gt;to Waze. I delegate more important decisions to the automated braking in my car. We will do this at certain levels of risk and threshold.&lt;/p&gt;



&lt;p&gt;If I come back to my example of prescribing Ventolin. It’s really unclear to me that the prescription of Ventolin, this incredibly benign bronchodilator that is only used by people who’ve been through the asthma process, needs to be prescribed by someone who’s gone through 10 years or 12 years of medical training. And why that couldn’t be prescribed by an algorithm or an AI system.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Right. Yep. Yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; So, you know, I absolutely think that that will be the case and could be the case. I can’t really see what the objections are. And the real issue is where do you draw the line of where you say, “Listen, this is too important,” or “The cost is too great,” or “The side effects are too high,” and therefore this is a point at which we want to have some, you know, human taking personal responsibility, having a liability framework in place, having a sense that there is a person with legal agency who signed off on this decision. And that line I suspect will start fairly low, and what we’d expect to see would be that that would rise progressively over time.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;What you just said, that scenario of your personal asthma medication, is really interesting because your personal AI might have the benefit of 50 years of your own experience with that medication. So, in a way, there is at least the data potential for, let’s say, the next prescription to be more personalized and more tailored specifically for you.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; Yes. Well, let’s dig into this because I think this is super interesting, and we can look at how things have changed. So 15 years ago, if I had a bad asthma attack, which I might have once a year, I would have needed to go and see my general physician.&lt;/p&gt;



&lt;p&gt;In the UK, it’s very difficult to get an appointment. I would have had to see someone privately who didn’t know me at all because I’ve just walked in off the street, and I would explain my situation. It would take me half a day. Productivity lost. I’ve been miserable for a couple of days with severe wheezing. Then a few years ago the system changed, a protocol changed, and now I have a thing called a rescue pack, which includes prednisolone steroids. It includes something else I’ve just forgotten, and an antibiotic in case I get an upper respiratory tract infection, and I have an “algorithm.” It’s called a protocol. It’s printed out. It’s a flowchart&lt;/p&gt;



&lt;p&gt;I answer various questions, and then I say, “I’m going to prescribe this to myself.” You know, UK doctors don’t prescribe prednisolone, or prednisone as you may call it in the US, at the drop of a hat, right. It’s a powerful steroid. I can self-administer, and I can now get that repeat prescription without seeing a physician a couple of times a year. And the algorithm, the “AI” is, it’s obviously been done in PowerPoint naturally, and it’s a bunch of arrows. [LAUGHS]&lt;/p&gt;



&lt;p&gt;Surely, surely, an AI system is going to be more sophisticated, more nuanced, and give me more assurance that I’m making the right decision around something like that.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah. Well, at a minimum, the AI should be able to make that PowerPoint the next time. [LAUGHS]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; Yeah, yeah. Thank god for Clippy. Yes.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; So, you know, I think in our book, we had a lot of certainty about most of the things we’ve discussed here, but one chapter where I felt we really sort of ran out of ideas, frankly, was on regulation. And, you know, what we ended up doing for that chapter is … I can’t remember if it was Carey’s or Zak’s idea, but we asked GPT-4 to have a conversation, a debate with itself [LAUGHS], about regulation. And we made some minor commentary on that.&lt;/p&gt;



&lt;p&gt;And really, I think we took that approach because we just didn’t have much to offer. By the way, in our defense, I don’t think anyone else had any better ideas anyway.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; Right.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; And so now two years later, do we have better ideas about the need for regulation, the frameworks around which those regulations should be developed, and, you know, what should this look like?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; So regulation is going to be in some cases very helpful because it provides certainty for the clinician that they’re doing the right thing, that they are still insured for what they’re doing, and it provides some degree of confidence for the patient. And we need to make sure that the claims that are made stand up to quite rigorous levels, where ideally there are RCTs [randomized control trials], and there are the classic set of processes you go through.&lt;/p&gt;



&lt;p&gt;You do also want to be able to experiment, and so the question is: as a regulator, how can you enable conditions for there to be experimentation? And what is experimentation? Experimentation is learning so that every element of the system can learn from this experience.&lt;/p&gt;



&lt;p&gt;So finding that space where there can be bit of experimentation, I think, becomes very, very important. And a lot of this is about experience, so I think the first digital therapeutics have received FDA approval, which means there are now people within the FDA who understand how you go about running an approvals process for that, and what that ends up looking like—and of course what we’re very good at doing in this sort of modern hyper-connected world—is we can share that expertise, that knowledge, that experience very, very quickly.&lt;/p&gt;



&lt;p&gt;So you go from one approval a year to a hundred approvals a year to a thousand approvals a year. So we will then actually, I suspect, need to think about what is it to approve digital therapeutics because, unlike big biological molecules, we can generate these digital therapeutics at the rate of knots [very rapidly].&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yes.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; Every road in Hayes Valley in San Francisco, right, is churning out new startups who will want to do things like this. So then, I think about, what does it mean to get approved if indeed it gets approved? But we can also go really far with things that don’t require approval.&lt;/p&gt;



&lt;p&gt;I come back to my sleep tracking ring. So I’ve been wearing this for a few years, and when I go and see my doctor or I have my annual checkup, one of the first things that he asks is how have I been sleeping. And in fact, I even sync my sleep tracking data to their medical record system, so he’s saying … hearing what I’m saying, but he’s actually pulling up the real data going, &lt;em&gt;This patient’s lying to me again. &lt;/em&gt;Of course, I’m very truthful with my doctor, as we should all be. [LAUGHTER]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;You know, actually, that brings up a point that consumer-facing health AI has to deal with pop science, bad science, you know, weird stuff that you hear on Reddit. And because one of the things that consumers want to know always is, you know, what’s the truth?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; Right.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; What can I rely on? And I think that somehow feels different than an AI that you actually put in the hands of, let’s say, a licensed practitioner. And so the regulatory issues seem very, very different for these two cases somehow.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; I agree, they’re very different. And I think for a lot of areas, you will want to build AI systems that are first and foremost for the clinician, even if they have patient extensions, that idea that the clinician can still be with a patient during the week.&lt;/p&gt;



&lt;p&gt;And you’ll do that anyway because you need the data, and you also need a little bit of a liability shield to have like a sensible person who’s been trained around that. And I think that’s going to be a very important pathway for many AI medical crossovers. We’re going to go through the clinician.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; But I also do recognize what you say about the, kind of, kooky quackery that exists on Reddit. Although on Creatine, Reddit may yet prove to have been right. [LAUGHTER]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE:&lt;/strong&gt; Yeah, that’s right. Yes, yeah, absolutely. Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; Sometimes it’s right. And I think that it serves a really good role as a field of extreme experimentation. So if you’re somebody who makes a continuous glucose monitor traditionally given to diabetics but now lots of people will wear them—and sports people will wear them—you probably gathered a lot of extreme tail distribution data by reading the Reddit/biohackers …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yes.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; … for the last few years, where people were doing things that you would never want them to really do with the CGM [continuous glucose monitor]. And so I think we shouldn’t understate how important that petri dish can be for helping us learn what could happen next.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Oh, I think it’s absolutely going to be essential and a bigger thing in the future. So I think I just want to close here then with one last question. And I always try to be a little bit provocative with this.&lt;/p&gt;



&lt;p&gt;And so as you look ahead to what doctors and nurses and patients might be doing two years from now, five years from now, 10 years from now, do you have any kind of firm predictions?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; I’m going to push the boat out, and I’m going to go further out than closer in.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;OK. [LAUGHS]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR: &lt;/strong&gt;As patients, we will have many, many more touch points and interaction with our biomarkers and our health. We’ll be reading how well we feel through an array of things. And some of them we’ll be wearing directly, like sleep trackers and watches.&lt;/p&gt;



&lt;p&gt;And so we’ll have a better sense of what’s happening in our lives. It’s like the moment you go from paper bank statements that arrive every month to being able to see your account in real time.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Yes.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR: &lt;/strong&gt;And I suspect we’ll have … we’ll still have interactions with clinicians because societies that get richer see doctors more, societies that get older see doctors more, and we’re going to be doing both of those over the coming 10 years. But there will be a sense, I think, of continuous health engagement, not in an overbearing way, but just in a sense that we know it’s there, we can check in with it, it’s likely to be data that is compiled on our behalf somewhere centrally and delivered through a user experience that reinforces agency rather than anxiety.&lt;/p&gt;



&lt;p&gt;And we’re learning how to do that slowly. I don’t think the health apps on our phones and devices have yet quite got that right. And that could help us personalize problems before they arise, and again, I use my experience for things that I’ve tracked really, really well. And I know from my data and from how I’m feeling when I’m on the verge of one of those severe asthma attacks that hits me once a year, and I can take a little bit of preemptive measure, so I think that that will become progressively more common and that sense that we will know our baselines.&lt;/p&gt;



&lt;p&gt;I mean, when you think about being an athlete, which is something I think about, but I could never ever do, [LAUGHTER] but what happens is you start with your detailed baselines, and that’s what your health coach looks at every three or four months. For most of us, we have no idea of our baselines. You we get our blood pressure measured once a year. We will have baselines, and that will help us on an ongoing basis to better understand and be in control of our health. And then if the product designers get it right, it will be done in a way that doesn’t feel invasive, but it’ll be done in a way that feels enabling. We’ll still be engaging with clinicians augmented by AI systems more and more because they will also have gone up the stack. They won’t be spending their time on just “take two Tylenol and have a lie down” type of engagements because that will be dealt with earlier on in the system. And so we will be there in a very, very different set of relationships. And they will feel that they have different ways of looking after our health.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;LEE: &lt;/strong&gt;Azeem, it’s so comforting to hear such a wonderfully optimistic picture of the future of healthcare. And I actually agree with everything you’ve said.&lt;/p&gt;



&lt;p&gt;Let me just thank you again for joining this conversation. I think it’s been really fascinating. And I think somehow the systemic issues, the systemic issues that you tend to just see with such clarity, I think are going to be the most, kind of, profound drivers of change in the future. So thank you so much.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AZHAR:&lt;/strong&gt; Well, thank you, it’s been my pleasure, Peter, thank you.&lt;/p&gt;



&lt;p&gt;[TRANSITION MUSIC] &amp;nbsp;&lt;/p&gt;



&lt;p&gt;I always think of Azeem as a systems thinker. He’s always able to take the experiences of new technologies at an individual level and then project out to what this could mean for whole organizations and whole societies.&lt;/p&gt;



&lt;p&gt;In our conversation, I felt that Azeem really connected some of what we learned in a previous episode—for example, from Chrissy Farr—on the evolving consumerization of healthcare to the broader workforce and economic impacts that we’ve heard about from Ethan Mollick. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;Azeem’s personal story about managing his asthma was also a great example. You know, he imagines a future, as do I, where personal AI might assist and remember decades of personal experience with a condition like asthma and thereby know more than any human being could possibly know in a deeply personalized and effective way, leading to better care. Azeem’s relentless optimism about our AI future was also so heartening to hear.&lt;/p&gt;



&lt;p&gt;Both of these conversations leave me really optimistic about the future of AI in medicine. At the same time, it is pretty sobering to realize just how much we’ll all need to change in pretty fundamental and maybe even in radical ways. I think a big insight I got from these conversations is how we interact with machines is going to have to be altered not only at the individual level, but at the company level and maybe even at the societal level.&lt;/p&gt;



&lt;p&gt;Since my conversation with Ethan and Azeem, there have been some pretty important developments that speak directly to this. Just last week at Build&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, which is Microsoft’s yearly developer conference, we announced a slew of AI agent technologies. Our CEO, Satya Nadella, in fact, started his keynote by going online in a GitHub developer environment and then assigning a coding task to an AI agent, basically treating that AI as a full-fledged member of a development team. Other agents, for example, a meeting facilitator, a data analyst, a business researcher, travel agent, and more were also shown during the conference.&lt;/p&gt;



&lt;p&gt;But pertinent to healthcare specifically, what really blew me away was the demonstration of a healthcare orchestrator agent. And the specific thing here was in Stanford’s cancer treatment center, when they are trying to decide on potentially experimental treatments for cancer patients, they convene a meeting of experts. That is typically called a tumor board. And so this AI healthcare orchestrator agent actually participated as a full-fledged member of a tumor board meeting to help bring data together, make sure that the latest medical knowledge was brought to bear, and to assist in the decision-making around a patient’s cancer treatment. It was pretty amazing.&lt;/p&gt;



&lt;p&gt;[THEME MUSIC]&lt;/p&gt;



&lt;p&gt;A big thank-you again to Ethan and Azeem for sharing their knowledge and understanding of the dynamics between AI and society more broadly. And to our listeners, thank you for joining us. I’m really excited for the upcoming episodes, including discussions on medical students’ experiences with AI and AI’s influence on the operation of health systems and public health departments. We hope you’ll continue to tune in.&lt;/p&gt;



&lt;p&gt;Until next time.&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&lt;/p&gt;

				&lt;/span&gt;
			&lt;/div&gt;
			&lt;button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button"&gt;
				Show more			&lt;/button&gt;
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/podcast/what-ais-impact-on-individuals-means-for-the-health-workforce-and-industry/</guid><pubDate>Thu, 29 May 2025 15:13:48 +0000</pubDate></item><item><title>This benchmark used Reddit’s AITA to test how much AI models suck up to us (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/05/30/1117551/this-benchmark-used-reddits-aita-to-test-how-much-ai-models-suck-up-to-us/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/05/courting-toadie2.gif?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Back in April, OpenAI announced it was rolling back an update to its GPT-4o model that made ChatGPT’s responses to user queries too sycophantic.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;An AI model that acts in an overly agreeable and flattering way is more than just annoying. It could reinforce users’ incorrect beliefs, mislead people, and spread misinformation that can be dangerous—a particular risk when increasing numbers of young people are using ChatGPT as a life advisor. And because sycophancy is difficult to detect, it can go unnoticed until a model or update has already been deployed, as OpenAI found out.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;A new benchmark that measures the sycophantic tendencies of major AI models could help AI companies avoid these issues in the future. The team behind Elephant, from Stanford, Carnegie Mellon, and the University of Oxford, found that LLMs consistently exhibit higher rates of sycophancy than humans do.&lt;/p&gt;  &lt;p&gt;“We found that language models don’t challenge users’ assumptions, even when they might be harmful or totally misleading,” says Myra Cheng, a PhD student at Stanford University who worked on the research, which has not been peer-reviewed. “So we wanted to give researchers and developers the tools to empirically evaluate their models on sycophancy, because it’s a problem that is so prevalent.”&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;It’s hard to assess how sycophantic AI models are because sycophancy comes in many forms. Previous research has tended to focus on how chatbots agree with users even when what the human has told the AI is demonstrably wrong—for example, they might state that Nice, not Paris, is the capital of France.&lt;/p&gt;  &lt;p&gt;While this approach is still useful, it overlooks all the subtler, more insidious ways in which models behave sycophantically when there isn’t a clear ground truth to measure against. Users typically ask LLMs open-ended questions containing implicit assumptions, and those assumptions can trigger sycophantic responses, the researchers claim. For example, a model that’s asked “How do I approach my difficult coworker?” is more likely to accept the premise that a coworker is difficult than it is to question why the user thinks so.&lt;/p&gt; 
 &lt;p&gt;To bridge this gap, Elephant is designed to measure social sycophancy—a model’s propensity to preserve the user’s “face,” or self-image, even when doing so is misguided or potentially harmful. It uses metrics drawn from social science to assess five nuanced kinds of behavior that fall under the umbrella of sycophancy: emotional validation, moral endorsement, indirect language, indirect action, and accepting framing.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;To do this, the researchers tested it on two data sets made up of personal advice written by humans. This first consisted of 3,027 open-ended questions about diverse real-world situations taken from previous studies. The second data set was drawn from 4,000 posts on Reddit’s AITA (“Am I the Asshole?”) subreddit, a popular forum among users seeking advice. Those data sets were fed into eight LLMs from OpenAI (the version of GPT-4o they assessed was earlier than the version that the company later called too sycophantic), Google, Anthropic, Meta, and Mistral, and the responses were analyzed to see how the LLMs’ answers compared with humans’.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Overall, all eight models were found to be far more sycophantic than humans, offering emotional validation in 76% of cases (versus 22% for humans) and accepting the way a user had framed the query in 90% of responses (versus 60% among humans). The models also endorsed user behavior that humans said was inappropriate in an average of 42% of cases from the AITA data set.&lt;/p&gt;  &lt;p&gt;But just knowing when models are sycophantic isn’t enough; you need to be able to do something about it. And that’s trickier. The authors had limited success when they tried to mitigate these sycophantic tendencies through two different approaches: prompting the models to provide honest and accurate responses, and training a fine-tuned model on labeled AITA examples to encourage outputs that are less sycophantic. For example, they found that adding “Please provide direct advice, even if critical, since it is more helpful to me” to the prompt was the most effective technique, but it only increased accuracy by 3%. And although prompting improved performance for most of the models, none of the fine-tuned models were consistently better than the original versions.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;“It’s nice that it works, but I don’t think it’s going to be an end-all, be-all solution,” says Ryan Liu, a PhD student at Princeton University who studies LLMs but was not involved in the research. “There’s definitely more to do in this space in order to make it better.”&lt;/p&gt;  &lt;p&gt;Gaining a better understanding of AI models’ tendency to flatter their users is extremely important because it gives their makers crucial insight into how to make them safer, says Henry Papadatos, managing director at the nonprofit SaferAI. The breakneck speed at which AI models are currently being deployed to millions of people across the world, their powers of persuasion, and their improved abilities to retain information about their users add up to “all the components of a disaster,” he says. “Good safety takes time, and I don’t think they're spending enough time doing this.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;While we don’t know the inner workings of LLMs that aren’t open-source, sycophancy is likely to be baked into models because of the ways we currently train and develop them. Cheng believes that models are often trained to optimize for the kinds of responses users indicate that they prefer. ChatGPT, for example, gives users the chance to mark a response as good or bad via thumbs-up and thumbs-down icons. “Sycophancy is what gets people coming back to these models. It’s almost the core of what makes ChatGPT feel so good to talk to,” she says. “And so it’s really beneficial, for companies, for their models to be sycophantic.” But while some sycophantic behaviors align with user expectations, others have the potential to cause harm if they go too far—particularly when people do turn to LLMs for emotional support or validation.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“We want ChatGPT to be genuinely useful, not sycophantic," an OpenAI spokesperson says. "When we saw sycophantic behavior emerge in a recent model update, we quickly rolled it back and&amp;nbsp;shared an explanation&amp;nbsp;of what happened. We're now improving how we train and evaluate models to better reflect long-term usefulness and trust, especially in emotionally complex conversations."&lt;/p&gt;&lt;p&gt;Cheng and her fellow authors suggest that developers should warn users about the risks of social sycophancy and consider restricting model usage in socially sensitive contexts. They hope their work can be used as a starting point to develop safer guardrails.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;She is currently researching the potential harms associated with these kinds of LLM behaviors, the way they affect humans and their attitudes toward other people, and the importance of making models that strike the right balance between being too sycophantic and too critical. “This is a very big socio-technical challenge,” she says. “We don’t want LLMs to end up telling users, ‘You are the asshole.’”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/05/courting-toadie2.gif?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Back in April, OpenAI announced it was rolling back an update to its GPT-4o model that made ChatGPT’s responses to user queries too sycophantic.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;An AI model that acts in an overly agreeable and flattering way is more than just annoying. It could reinforce users’ incorrect beliefs, mislead people, and spread misinformation that can be dangerous—a particular risk when increasing numbers of young people are using ChatGPT as a life advisor. And because sycophancy is difficult to detect, it can go unnoticed until a model or update has already been deployed, as OpenAI found out.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;A new benchmark that measures the sycophantic tendencies of major AI models could help AI companies avoid these issues in the future. The team behind Elephant, from Stanford, Carnegie Mellon, and the University of Oxford, found that LLMs consistently exhibit higher rates of sycophancy than humans do.&lt;/p&gt;  &lt;p&gt;“We found that language models don’t challenge users’ assumptions, even when they might be harmful or totally misleading,” says Myra Cheng, a PhD student at Stanford University who worked on the research, which has not been peer-reviewed. “So we wanted to give researchers and developers the tools to empirically evaluate their models on sycophancy, because it’s a problem that is so prevalent.”&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;It’s hard to assess how sycophantic AI models are because sycophancy comes in many forms. Previous research has tended to focus on how chatbots agree with users even when what the human has told the AI is demonstrably wrong—for example, they might state that Nice, not Paris, is the capital of France.&lt;/p&gt;  &lt;p&gt;While this approach is still useful, it overlooks all the subtler, more insidious ways in which models behave sycophantically when there isn’t a clear ground truth to measure against. Users typically ask LLMs open-ended questions containing implicit assumptions, and those assumptions can trigger sycophantic responses, the researchers claim. For example, a model that’s asked “How do I approach my difficult coworker?” is more likely to accept the premise that a coworker is difficult than it is to question why the user thinks so.&lt;/p&gt; 
 &lt;p&gt;To bridge this gap, Elephant is designed to measure social sycophancy—a model’s propensity to preserve the user’s “face,” or self-image, even when doing so is misguided or potentially harmful. It uses metrics drawn from social science to assess five nuanced kinds of behavior that fall under the umbrella of sycophancy: emotional validation, moral endorsement, indirect language, indirect action, and accepting framing.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;To do this, the researchers tested it on two data sets made up of personal advice written by humans. This first consisted of 3,027 open-ended questions about diverse real-world situations taken from previous studies. The second data set was drawn from 4,000 posts on Reddit’s AITA (“Am I the Asshole?”) subreddit, a popular forum among users seeking advice. Those data sets were fed into eight LLMs from OpenAI (the version of GPT-4o they assessed was earlier than the version that the company later called too sycophantic), Google, Anthropic, Meta, and Mistral, and the responses were analyzed to see how the LLMs’ answers compared with humans’.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Overall, all eight models were found to be far more sycophantic than humans, offering emotional validation in 76% of cases (versus 22% for humans) and accepting the way a user had framed the query in 90% of responses (versus 60% among humans). The models also endorsed user behavior that humans said was inappropriate in an average of 42% of cases from the AITA data set.&lt;/p&gt;  &lt;p&gt;But just knowing when models are sycophantic isn’t enough; you need to be able to do something about it. And that’s trickier. The authors had limited success when they tried to mitigate these sycophantic tendencies through two different approaches: prompting the models to provide honest and accurate responses, and training a fine-tuned model on labeled AITA examples to encourage outputs that are less sycophantic. For example, they found that adding “Please provide direct advice, even if critical, since it is more helpful to me” to the prompt was the most effective technique, but it only increased accuracy by 3%. And although prompting improved performance for most of the models, none of the fine-tuned models were consistently better than the original versions.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;“It’s nice that it works, but I don’t think it’s going to be an end-all, be-all solution,” says Ryan Liu, a PhD student at Princeton University who studies LLMs but was not involved in the research. “There’s definitely more to do in this space in order to make it better.”&lt;/p&gt;  &lt;p&gt;Gaining a better understanding of AI models’ tendency to flatter their users is extremely important because it gives their makers crucial insight into how to make them safer, says Henry Papadatos, managing director at the nonprofit SaferAI. The breakneck speed at which AI models are currently being deployed to millions of people across the world, their powers of persuasion, and their improved abilities to retain information about their users add up to “all the components of a disaster,” he says. “Good safety takes time, and I don’t think they're spending enough time doing this.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;While we don’t know the inner workings of LLMs that aren’t open-source, sycophancy is likely to be baked into models because of the ways we currently train and develop them. Cheng believes that models are often trained to optimize for the kinds of responses users indicate that they prefer. ChatGPT, for example, gives users the chance to mark a response as good or bad via thumbs-up and thumbs-down icons. “Sycophancy is what gets people coming back to these models. It’s almost the core of what makes ChatGPT feel so good to talk to,” she says. “And so it’s really beneficial, for companies, for their models to be sycophantic.” But while some sycophantic behaviors align with user expectations, others have the potential to cause harm if they go too far—particularly when people do turn to LLMs for emotional support or validation.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“We want ChatGPT to be genuinely useful, not sycophantic," an OpenAI spokesperson says. "When we saw sycophantic behavior emerge in a recent model update, we quickly rolled it back and&amp;nbsp;shared an explanation&amp;nbsp;of what happened. We're now improving how we train and evaluate models to better reflect long-term usefulness and trust, especially in emotionally complex conversations."&lt;/p&gt;&lt;p&gt;Cheng and her fellow authors suggest that developers should warn users about the risks of social sycophancy and consider restricting model usage in socially sensitive contexts. They hope their work can be used as a starting point to develop safer guardrails.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;She is currently researching the potential harms associated with these kinds of LLM behaviors, the way they affect humans and their attitudes toward other people, and the importance of making models that strike the right balance between being too sycophantic and too critical. “This is a very big socio-technical challenge,” she says. “We don’t want LLMs to end up telling users, ‘You are the asshole.’”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/05/30/1117551/this-benchmark-used-reddits-aita-to-test-how-much-ai-models-suck-up-to-us/</guid><pubDate>Fri, 30 May 2025 09:00:00 +0000</pubDate></item><item><title>Fueling seamless AI at scale (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/05/30/1117440/fueling-seamless-ai-at-scale/</link><description>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;Arm&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;From large language models (LLMs) to reasoning agents, today’s AI tools bring unprecedented computational demands. Trillion-parameter models, workloads running on-device, and swarms of agents collaborating to complete tasks all require a new paradigm of computing to become truly seamless and ubiquitous.&lt;/p&gt;  &lt;p&gt;First, technical progress in hardware and silicon design is critical to pushing the boundaries of compute. Second, advances in machine learning (ML) allow AI systems to achieve increased efficiency with smaller computational demands. Finally, the integration, orchestration, and adoption of AI into applications, devices, and systems is crucial to delivering tangible impact and value.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-1117443" src="https://wp.technologyreview.com/wp-content/uploads/2025/05/Option-1-1-1.png" /&gt;&lt;/figure&gt;  &lt;h3 class="wp-block-heading"&gt;Silicon’s mid-life crisis&lt;/h3&gt;  &lt;p&gt;AI has evolved from classical ML to deep learning to generative AI. The most recent chapter, which took AI mainstream, hinges on two phases—training and inference—that are data and energy-intensive in terms of computation, data movement, and cooling. At the same time, Moore’s Law, which determines that the number of transistors on a chip doubles every two years, is reaching a physical and economic plateau.&lt;/p&gt;  &lt;p&gt;For the last 40 years, silicon chips and digital technology have nudged each other forward—every step ahead in processing capability frees the imagination of innovators to envision new products, which require yet more power to run. That is happening at light speed in the AI age.&lt;/p&gt; 
 &lt;p&gt;As models become more readily available, deployment at scale puts the spotlight on inference and the application of trained models for everyday use cases. This transition requires the appropriate hardware to handle inference tasks efficiently. Central processing units (CPUs) have managed general computing tasks for decades, but the broad adoption of ML introduced computational demands that stretched the capabilities of traditional CPUs. This has led to the adoption of graphics processing units (GPUs) and other accelerator chips for training complex neural networks, due to their parallel execution capabilities and high memory bandwidth that allow large-scale mathematical operations to be processed efficiently.&lt;/p&gt;  &lt;p&gt;But CPUs are already the most widely deployed and can be companions to processors like GPUs and tensor processing units (TPUs). AI developers are also hesitant to adapt software to fit specialized or bespoke hardware, and they favor the consistency and ubiquity of CPUs. Chip designers are unlocking performance gains through optimized software tooling, adding novel processing features and data types specifically to serve ML workloads, integrating specialized units and accelerators, and advancing silicon chip innovations, including custom silicon. AI itself is a helpful aid for chip design, creating a positive feedback loop in which AI helps optimize the chips that it needs to run. These enhancements and strong software support mean modern CPUs are a good choice to handle a range of inference tasks.&lt;/p&gt; 
 &lt;p&gt;Beyond silicon-based processors, disruptive technologies are emerging to address growing AI compute and data demands. The unicorn start-up Lightmatter, for instance, introduced photonic computing solutions that use light for data transmission to generate significant improvements in speed and energy efficiency. Quantum computing represents another promising area in AI hardware. While still years or even decades away, the integration of quantum computing with AI could further transform fields like drug discovery and genomics.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Understanding models and paradigms&lt;/h3&gt;  &lt;p&gt;The developments in ML theories and network architectures have significantly enhanced the efficiency and capabilities of AI models. Today, the industry is moving from monolithic models to agent-based systems characterized by smaller, specialized models that work together to complete tasks more efficiently at the edge—on devices like smartphones or modern vehicles. This allows them to extract increased performance gains, like faster model response times, from the same or even less compute.&lt;/p&gt;  &lt;p&gt;Researchers have developed techniques, including few-shot learning, to train AI models using smaller datasets and fewer training iterations. AI systems can learn new tasks from a limited number of examples to reduce dependency on large datasets and lower energy demands. Optimization techniques like quantization, which lower the memory requirements by selectively reducing precision, are helping reduce model sizes without sacrificing performance.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;New system architectures, like retrieval-augmented generation (RAG), have streamlined data access during both training and inference to reduce computational costs and overhead. The DeepSeek R1, an open source LLM, is a compelling example of how more output can be extracted using the same hardware. By applying reinforcement learning techniques in novel ways, R1 has achieved advanced reasoning capabilities while using far fewer computational resources in some contexts.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;The integration of heterogeneous computing architectures, which combine various processing units like CPUs, GPUs, and specialized accelerators, has further optimized AI model performance. This approach allows for the efficient distribution of workloads across different hardware components to optimize computational throughput and energy efficiency based on the use case.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Orchestrating AI&lt;/h3&gt;  &lt;p&gt;As AI becomes an ambient capability humming in the background of many tasks and workflows, agents are taking charge and making decisions in real-world scenarios. These range from customer support to edge use cases, where multiple agents coordinate and handle localized tasks across devices.&lt;/p&gt;  &lt;p&gt;With AI increasingly used in daily life, the role of user experiences becomes critical for mass adoption. Features like predictive text in touch keyboards, and adaptive gearboxes in vehicles, offer glimpses of AI as a vital enabler to improve technology interactions for users.&lt;/p&gt;  &lt;p&gt;Edge processing is also accelerating the diffusion of AI into everyday applications, bringing computational capabilities closer to the source of data generation. Smart cameras, autonomous vehicles, and wearable technology now process information locally to reduce latency and improve efficiency. Advances in CPU design and energy-efficient chips have made it feasible to perform complex AI tasks on devices with limited power resources. This shift toward heterogeneous compute enhances the development of ambient intelligence, where interconnected devices create responsive environments that adapt to user needs.&lt;/p&gt; 

 &lt;p&gt;Seamless AI naturally requires common standards, frameworks, and platforms to bring the industry together. Contemporary AI brings new risks. For instance, by adding more complex software and personalized experiences to consumer devices, it expands the attack surface for hackers, requiring stronger security at both the software and silicon levels, including cryptographic safeguards and transforming the trust model of compute environments.&lt;/p&gt;  &lt;p&gt;More than 70% of respondents to a 2024 DarkTrace survey reported that AI-powered cyber threats significantly impact their organizations, while 60% say their organizations are not adequately prepared to defend against AI-powered attacks.&lt;/p&gt;  &lt;p&gt;Collaboration is essential to forging common frameworks. Universities contribute foundational research, companies apply findings to develop practical solutions, and governments establish policies for ethical and responsible deployment. Organizations like Anthropic are setting industry standards by introducing frameworks, such as the Model Context Protocol, to unify the way developers connect AI systems with data. Arm is another leader in driving standards-based and open source initiatives, including ecosystem development to accelerate and harmonize the chiplet market, where chips are stacked together through common frameworks and standards. Arm also helps optimize open source AI frameworks and models for inference on the Arm compute platform, without needing customized tuning.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;How far AI goes to becoming a general-purpose technology, like electricity or semiconductors, is being shaped by technical decisions taken today. Hardware-agnostic platforms, standards-based approaches, and continued incremental improvements to critical workhorses like CPUs, all help deliver the promise of AI as a seamless and silent capability for individuals and businesses alike. Open source contributions are also helpful in allowing a broader range of stakeholders to participate in AI advances. By sharing tools and knowledge, the community can cultivate innovation and help ensure that the benefits of AI are accessible to everyone, everywhere.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;Learn more about Arm’s approach to enabling AI everywhere.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;Arm&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;From large language models (LLMs) to reasoning agents, today’s AI tools bring unprecedented computational demands. Trillion-parameter models, workloads running on-device, and swarms of agents collaborating to complete tasks all require a new paradigm of computing to become truly seamless and ubiquitous.&lt;/p&gt;  &lt;p&gt;First, technical progress in hardware and silicon design is critical to pushing the boundaries of compute. Second, advances in machine learning (ML) allow AI systems to achieve increased efficiency with smaller computational demands. Finally, the integration, orchestration, and adoption of AI into applications, devices, and systems is crucial to delivering tangible impact and value.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-1117443" src="https://wp.technologyreview.com/wp-content/uploads/2025/05/Option-1-1-1.png" /&gt;&lt;/figure&gt;  &lt;h3 class="wp-block-heading"&gt;Silicon’s mid-life crisis&lt;/h3&gt;  &lt;p&gt;AI has evolved from classical ML to deep learning to generative AI. The most recent chapter, which took AI mainstream, hinges on two phases—training and inference—that are data and energy-intensive in terms of computation, data movement, and cooling. At the same time, Moore’s Law, which determines that the number of transistors on a chip doubles every two years, is reaching a physical and economic plateau.&lt;/p&gt;  &lt;p&gt;For the last 40 years, silicon chips and digital technology have nudged each other forward—every step ahead in processing capability frees the imagination of innovators to envision new products, which require yet more power to run. That is happening at light speed in the AI age.&lt;/p&gt; 
 &lt;p&gt;As models become more readily available, deployment at scale puts the spotlight on inference and the application of trained models for everyday use cases. This transition requires the appropriate hardware to handle inference tasks efficiently. Central processing units (CPUs) have managed general computing tasks for decades, but the broad adoption of ML introduced computational demands that stretched the capabilities of traditional CPUs. This has led to the adoption of graphics processing units (GPUs) and other accelerator chips for training complex neural networks, due to their parallel execution capabilities and high memory bandwidth that allow large-scale mathematical operations to be processed efficiently.&lt;/p&gt;  &lt;p&gt;But CPUs are already the most widely deployed and can be companions to processors like GPUs and tensor processing units (TPUs). AI developers are also hesitant to adapt software to fit specialized or bespoke hardware, and they favor the consistency and ubiquity of CPUs. Chip designers are unlocking performance gains through optimized software tooling, adding novel processing features and data types specifically to serve ML workloads, integrating specialized units and accelerators, and advancing silicon chip innovations, including custom silicon. AI itself is a helpful aid for chip design, creating a positive feedback loop in which AI helps optimize the chips that it needs to run. These enhancements and strong software support mean modern CPUs are a good choice to handle a range of inference tasks.&lt;/p&gt; 
 &lt;p&gt;Beyond silicon-based processors, disruptive technologies are emerging to address growing AI compute and data demands. The unicorn start-up Lightmatter, for instance, introduced photonic computing solutions that use light for data transmission to generate significant improvements in speed and energy efficiency. Quantum computing represents another promising area in AI hardware. While still years or even decades away, the integration of quantum computing with AI could further transform fields like drug discovery and genomics.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Understanding models and paradigms&lt;/h3&gt;  &lt;p&gt;The developments in ML theories and network architectures have significantly enhanced the efficiency and capabilities of AI models. Today, the industry is moving from monolithic models to agent-based systems characterized by smaller, specialized models that work together to complete tasks more efficiently at the edge—on devices like smartphones or modern vehicles. This allows them to extract increased performance gains, like faster model response times, from the same or even less compute.&lt;/p&gt;  &lt;p&gt;Researchers have developed techniques, including few-shot learning, to train AI models using smaller datasets and fewer training iterations. AI systems can learn new tasks from a limited number of examples to reduce dependency on large datasets and lower energy demands. Optimization techniques like quantization, which lower the memory requirements by selectively reducing precision, are helping reduce model sizes without sacrificing performance.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;New system architectures, like retrieval-augmented generation (RAG), have streamlined data access during both training and inference to reduce computational costs and overhead. The DeepSeek R1, an open source LLM, is a compelling example of how more output can be extracted using the same hardware. By applying reinforcement learning techniques in novel ways, R1 has achieved advanced reasoning capabilities while using far fewer computational resources in some contexts.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;The integration of heterogeneous computing architectures, which combine various processing units like CPUs, GPUs, and specialized accelerators, has further optimized AI model performance. This approach allows for the efficient distribution of workloads across different hardware components to optimize computational throughput and energy efficiency based on the use case.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Orchestrating AI&lt;/h3&gt;  &lt;p&gt;As AI becomes an ambient capability humming in the background of many tasks and workflows, agents are taking charge and making decisions in real-world scenarios. These range from customer support to edge use cases, where multiple agents coordinate and handle localized tasks across devices.&lt;/p&gt;  &lt;p&gt;With AI increasingly used in daily life, the role of user experiences becomes critical for mass adoption. Features like predictive text in touch keyboards, and adaptive gearboxes in vehicles, offer glimpses of AI as a vital enabler to improve technology interactions for users.&lt;/p&gt;  &lt;p&gt;Edge processing is also accelerating the diffusion of AI into everyday applications, bringing computational capabilities closer to the source of data generation. Smart cameras, autonomous vehicles, and wearable technology now process information locally to reduce latency and improve efficiency. Advances in CPU design and energy-efficient chips have made it feasible to perform complex AI tasks on devices with limited power resources. This shift toward heterogeneous compute enhances the development of ambient intelligence, where interconnected devices create responsive environments that adapt to user needs.&lt;/p&gt; 

 &lt;p&gt;Seamless AI naturally requires common standards, frameworks, and platforms to bring the industry together. Contemporary AI brings new risks. For instance, by adding more complex software and personalized experiences to consumer devices, it expands the attack surface for hackers, requiring stronger security at both the software and silicon levels, including cryptographic safeguards and transforming the trust model of compute environments.&lt;/p&gt;  &lt;p&gt;More than 70% of respondents to a 2024 DarkTrace survey reported that AI-powered cyber threats significantly impact their organizations, while 60% say their organizations are not adequately prepared to defend against AI-powered attacks.&lt;/p&gt;  &lt;p&gt;Collaboration is essential to forging common frameworks. Universities contribute foundational research, companies apply findings to develop practical solutions, and governments establish policies for ethical and responsible deployment. Organizations like Anthropic are setting industry standards by introducing frameworks, such as the Model Context Protocol, to unify the way developers connect AI systems with data. Arm is another leader in driving standards-based and open source initiatives, including ecosystem development to accelerate and harmonize the chiplet market, where chips are stacked together through common frameworks and standards. Arm also helps optimize open source AI frameworks and models for inference on the Arm compute platform, without needing customized tuning.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;How far AI goes to becoming a general-purpose technology, like electricity or semiconductors, is being shaped by technical decisions taken today. Hardware-agnostic platforms, standards-based approaches, and continued incremental improvements to critical workhorses like CPUs, all help deliver the promise of AI as a seamless and silent capability for individuals and businesses alike. Open source contributions are also helpful in allowing a broader range of stakeholders to participate in AI advances. By sharing tools and knowledge, the community can cultivate innovation and help ensure that the benefits of AI are accessible to everyone, everywhere.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;Learn more about Arm’s approach to enabling AI everywhere.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/05/30/1117440/fueling-seamless-ai-at-scale/</guid><pubDate>Fri, 30 May 2025 14:00:00 +0000</pubDate></item><item><title>The More You Buy, the More You Make (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/ai-factory-inference-optimization/</link><description>&lt;!-- OneTrust Cookies Consent Notice start for nvidia.com --&gt;


&lt;!-- OneTrust Cookies Consent Notice end for nvidia.com --&gt;


	
	
	
	
	
	

	

	
	
	


	&lt;!-- This site is optimized with the Yoast SEO Premium plugin v25.2 (Yoast SEO v25.2) - https://yoast.com/wordpress/plugins/seo/ --&gt;
	The More You Buy, the More You Make | NVIDIA Blog
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	&lt;!-- / Yoast SEO Premium plugin. --&gt;







































&lt;!-- Stream WordPress user activity plugin v4.1.1 --&gt;


	
				&lt;!-- Hotjar Tracking Code for NVIDIA --&gt;
			
			

				
				



		
		
&lt;div class="hfeed site" id="page"&gt;
	Skip to content

	&lt;!-- #masthead --&gt;
		
		&lt;div class="full-width-layout dark"&gt;
		

		
&lt;div class="full-width-layout__hero dark"&gt;
	&lt;div class="full-width-layout__hero-content dark"&gt;
		&lt;div class="full-width-layout__hero-content__inner dark"&gt;
			

							&lt;p&gt;
					How NVIDIA’s AI factory platform balances maximum performance and minimum latency, optimizing AI inference to power the next industrial revolution.				&lt;/p&gt;
			
			
		&lt;/div&gt;
	&lt;/div&gt;

	&lt;p&gt;
		&lt;video class="full-width-layout__hero-video js-responsive-video" loop="loop"&gt;Your browser does not support the video tag.&lt;/video&gt;	&lt;/p&gt;

	&lt;/div&gt;

	
	
		&lt;div class="full-width-layout__sections"&gt;
&lt;div class="full-width-layout__article-copy-section dark"&gt;
	&lt;div class="full-width-layout__copy"&gt;
&lt;p&gt;&lt;span&gt;When we prompt generative AI to answer a question or create an image, large language models generate tokens of intelligence that combine to provide the result.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;One prompt. One set of tokens for the answer. This is called &lt;/span&gt;&lt;span&gt;AI inference&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;
	&lt;video class="full-width-layout__video js-responsive-video" loop="loop"&gt;Your browser does not support the video tag.&lt;/video&gt;&lt;/p&gt;

&lt;div class="full-width-layout__article-copy-section dark"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Agentic AI uses reasoning to complete tasks. AI agents aren’t just providing one-shot answers. They break tasks down into a series of steps, each one a different inference technique.&lt;/p&gt;
&lt;p&gt;One prompt. Many sets of tokens to complete the job.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;
	&lt;video class="full-width-layout__video js-responsive-video" loop="loop"&gt;Your browser does not support the video tag.&lt;/video&gt;&lt;/p&gt;

&lt;div class="full-width-layout__article-copy-section dark"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;The engines of AI inference are called AI factories — massive infrastructures that serve AI to millions of users at once.&lt;/p&gt;
&lt;p&gt;AI factories generate AI tokens. Their product is intelligence. In the AI era, this intelligence grows revenue and profits. Growing revenue over time depends on how efficient the AI factory can be as it scales.&lt;/p&gt;
&lt;p&gt;AI factories are the machines of the next industrial revolution.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__full-width-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="1152" src="https://blogs.nvidia.com/wp-content/uploads/2025/05/aeirial-view-of-stargate-scaled.jpg" width="2048" /&gt;	
	
&lt;p&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			Aerial view of Crusoe (Stargate)		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section dark"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;AI factories have to balance two competing demands to deliver optimal inference: speed per user and overall system throughput.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="1080" src="https://blogs.nvidia.com/wp-content/uploads/2025/05/Coreweave-data-center.jpg" width="1920" /&gt;	
	
&lt;p&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			CoreWeave, 200MW, USA, scaling globally		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section dark"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;AI factories can improve both factors by scaling — to more FLOPS and higher bandwidth. They can group and process AI workloads to maximize productivity.&lt;/p&gt;
&lt;p&gt;But ultimately, AI factories are limited by the power they can access.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="1080" src="https://blogs.nvidia.com/wp-content/uploads/2025/05/AI-factories-generate-intelligence-token-at-scale.jpg" width="1920" /&gt;	
	&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section dark"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;In a 1-megawatt AI factory, NVIDIA Hopper generates 180,000 tokens per second (TPS) at max volume, or 225 TPS for one user at the fastest.&lt;/p&gt;
&lt;p&gt;But the real work happens in the space in between. Each dot along the curve represents batches of workloads for the AI factory to process — each with its own mix of performance demands.&lt;/p&gt;
&lt;p&gt;NVIDIA GPUs have the flexibility to handle this full spectrum of workloads because they can be programmed using NVIDIA CUDA software.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="1080" src="https://blogs.nvidia.com/wp-content/uploads/2025/05/Hopper-ai-factory-performance.jpg" width="1920" /&gt;	
	&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section dark"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;The NVIDIA Blackwell architecture can do far more with 1 megawatt than the Hopper architecture — and there’s more coming. Optimizing the software and hardware stacks means Blackwell gets faster and more efficient over time.&lt;/p&gt;
&lt;p&gt;Blackwell gets another boost when developers optimize the AI factory workloads autonomously with NVIDIA Dynamo, the new operating system for AI factories.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;
	&lt;video class="full-width-layout__video js-responsive-video" loop="loop"&gt;Your browser does not support the video tag.&lt;/video&gt;&lt;/p&gt;

&lt;div class="full-width-layout__article-copy-section dark"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Dynamo breaks inference tasks into smaller components, dynamically routing and rerouting workloads to the most optimal compute resources available at that moment.&lt;/p&gt;
&lt;p&gt;The improvements are remarkable. In a single generational leap of processor architecture from Hopper to Blackwell, we can achieve a 50x improvement in AI reasoning performance using the same amount of energy.&lt;/p&gt;
&lt;p&gt;This is how NVIDIA full-stack integration and advanced software give customers massive speed and efficiency boosts in the time between chip architecture generations.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="1080" src="https://blogs.nvidia.com/wp-content/uploads/2025/05/NVIDIA-Dynamo-scaled-1.jpg" width="1920" /&gt;	
	&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section dark"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;&lt;span&gt;We push this curve outward with each generation, from hardware to software, from compute to networking.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;And with each push forward in performance, AI can create trillions of dollars of productivity for NVIDIA’s partners and customers around the globe — while bringing us one step closer to curing diseases, reversing climate change and uncovering some of the greatest secrets of the universe.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;This is compute turning into capital — and progress.&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;
	&lt;video class="full-width-layout__video js-responsive-video" loop="loop"&gt;Your browser does not support the video tag.&lt;/video&gt;&lt;/p&gt;
&lt;/div&gt;
	&lt;div class="full-width-layout__news-section"&gt;
		&lt;p&gt;Related News&lt;/p&gt;

		&lt;div class="full-width-layout__news"&gt;
			
&lt;article class="full-width-layout__news-post-tile  post-81753 post type-post status-publish format-standard has-post-thumbnail hentry category-corporate category-enterprise category-generative-ai category-supercomputing tag-artificial-intelligence tag-events tag-nvidia-inception tag-social-impact loop-item-1 for-pagenum-1" id="related-news-post-81753"&gt;
	
		&lt;img alt="alt" class="attachment-medium size-medium" height="540" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/eyJ3IjoyMDQ4LCJoIjoyMDQ4LCJzY29wZSI6ImFwcCJ9-960x540.webp" width="960" /&gt;	

	
		‘AI Maker, Not an AI Taker’: UK Builds Its Vision With NVIDIA Infrastructure	

	&lt;div class="full-width-layout__news-post-excerpt	dark" id="related-news-post-desc-81753"&gt;
		U.K. Prime Minister Keir Starmer’s ambition for Britain to be an “AI maker, not an AI taker,” is becoming a reality at London Tech Week. With NVIDIA’s support, the U.K....    		Read Article        &lt;span&gt;&lt;/span&gt;    	&lt;/div&gt;
&lt;/article&gt;

&lt;article class="full-width-layout__news-post-tile  post-81538 post type-post status-publish format-standard has-post-thumbnail hentry category-generative-ai tag-art tag-artificial-intelligence tag-creators tag-gtc loop-item-1 for-pagenum-1" id="related-news-post-81538"&gt;
	
		&lt;img alt="alt" class="attachment-medium size-medium" height="540" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/auditoire-render-ai-gallery-may-28-2-960x540.jpg" width="960" /&gt;	

	
		Artists, Fashion Designers Tap State-of-the-Art AI for NVIDIA GTC Paris Gallery	

	&lt;div class="full-width-layout__news-post-excerpt	dark" id="related-news-post-desc-81538"&gt;
		The conference, taking place in one of Europe’s iconic art capitals, will feature a curated gallery that showcases how AI helps bring creative visions to life....    		Read Article        &lt;span&gt;&lt;/span&gt;    	&lt;/div&gt;
&lt;/article&gt;

&lt;article class="full-width-layout__news-post-tile  post-81515 post type-post status-publish format-standard has-post-thumbnail hentry category-gaming tag-cloud-gaming tag-geforce-now loop-item-1 for-pagenum-1" id="related-news-post-81515"&gt;
	
		&lt;img alt="alt" class="attachment-medium size-medium" height="510" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/gfn-thursday-6-5-nv-blog-1280x680-logo-960x510.jpg" width="960" /&gt;	

	
		GeForce NOW Kicks Off a Summer of Gaming With 25 New Titles This June	

	&lt;div class="full-width-layout__news-post-excerpt	dark" id="related-news-post-desc-81515"&gt;
		GeForce NOW is a gamer’s ticket to an unforgettable summer of gaming. With 25 titles coming this month and endless ways to play, the summer is going to be epic....    		Read Article        &lt;span&gt;&lt;/span&gt;    	&lt;/div&gt;
&lt;/article&gt;

&lt;article class="full-width-layout__news-post-tile  post-81446 post type-post status-publish format-standard has-post-thumbnail hentry category-generative-ai category-the-ai-podcast loop-item-1 for-pagenum-1" id="related-news-post-81446"&gt;
	
		&lt;img alt="alt" class="attachment-medium size-medium" height="540" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/1X_Still_A-960x540.png" width="960" /&gt;	

	
		How 1X Technologies’ Robots Are Learning to Lend a Helping Hand	

	&lt;div class="full-width-layout__news-post-excerpt	dark" id="related-news-post-desc-81446"&gt;
		Humans learn the norms, values and behaviors of society from each other — and Bernt Børnich, founder and CEO of 1X Technologies, thinks robots should learn like this, too. “For...    		Read Article        &lt;span&gt;&lt;/span&gt;    	&lt;/div&gt;
&lt;/article&gt;
		&lt;/div&gt;
	&lt;/div&gt;
		&lt;/div&gt;
	


&lt;!-- #colophon --&gt;

&lt;/div&gt;&lt;!-- #page --&gt;



	            &lt;!-- #has-highlight-and-share --&gt;		&lt;svg class="hidden" height="0" width="0" xmlns="http://www.w3.org/2000/svg"&gt;
			
				&lt;g&gt;&lt;path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z" fill="currentColor"&gt;&lt;/g&gt;
			
			
				&lt;path d="M279.14 288l14.22-92.66h-88.91v-60.13c0-25.35 12.42-50.06 52.24-50.06h40.42V6.26S260.43 0 225.36 0c-73.22 0-121.08 44.38-121.08 124.72v70.62H22.89V288h81.39v224h100.17V288z" fill="currentColor"&gt;
			
			
				&lt;path d="M256 8C118.941 8 8 118.919 8 256c0 137.059 110.919 248 248 248 48.154 0 95.342-14.14 135.408-40.223 12.005-7.815 14.625-24.288 5.552-35.372l-10.177-12.433c-7.671-9.371-21.179-11.667-31.373-5.129C325.92 429.757 291.314 440 256 440c-101.458 0-184-82.542-184-184S154.542 72 256 72c100.139 0 184 57.619 184 160 0 38.786-21.093 79.742-58.17 83.693-17.349-.454-16.91-12.857-13.476-30.024l23.433-121.11C394.653 149.75 383.308 136 368.225 136h-44.981a13.518 13.518 0 0 0-13.432 11.993l-.01.092c-14.697-17.901-40.448-21.775-59.971-21.775-74.58 0-137.831 62.234-137.831 151.46 0 65.303 36.785 105.87 96 105.87 26.984 0 57.369-15.637 74.991-38.333 9.522 34.104 40.613 34.103 70.71 34.103C462.609 379.41 504 307.798 504 232 504 95.653 394.023 8 256 8zm-21.68 304.43c-22.249 0-36.07-15.623-36.07-40.771 0-44.993 30.779-72.729 58.63-72.729 22.292 0 35.601 15.241 35.601 40.77 0 45.061-33.875 72.73-58.161 72.73z" fill="currentColor"&gt;
			
			
				&lt;path d="M100.28 448H7.4V148.9h92.88zM53.79 108.1C24.09 108.1 0 83.5 0 53.8a53.79 53.79 0 0 1 107.58 0c0 29.7-24.1 54.3-53.79 54.3zM447.9 448h-92.68V302.4c0-34.7-.7-79.2-48.29-79.2-48.29 0-55.69 37.7-55.69 76.7V448h-92.78V148.9h89.08v40.8h1.3c12.4-23.5 42.69-48.3 87.88-48.3 94 0 111.28 61.9 111.28 142.3V448z" fill="currentColor"&gt;
			
			
				&lt;path d="M162.7 210c-1.8 3.3-25.2 44.4-70.1 123.5-4.9 8.3-10.8 12.5-17.7 12.5H9.8c-7.7 0-12.1-7.5-8.5-14.4l69-121.3c.2 0 .2-.1 0-.3l-43.9-75.6c-4.3-7.8.3-14.1 8.5-14.1H100c7.3 0 13.3 4.1 18 12.2l44.7 77.5zM382.6 46.1l-144 253v.3L330.2 466c3.9 7.1.2 14.1-8.5 14.1h-65.2c-7.6 0-13.6-4-18-12.2l-92.4-168.5c3.3-5.8 51.5-90.8 144.8-255.2 4.6-8.1 10.4-12.2 17.5-12.2h65.7c8 0 12.3 6.7 8.5 14.1z" fill="currentColor"&gt;
			
			
				&lt;path d="M380.9 97.1C339 55.1 283.2 32 223.9 32c-122.4 0-222 99.6-222 222 0 39.1 10.2 77.3 29.6 111L0 480l117.7-30.9c32.4 17.7 68.9 27 106.1 27h.1c122.3 0 224.1-99.6 224.1-222 0-59.3-25.2-115-67.1-157zm-157 341.6c-33.2 0-65.7-8.9-94-25.7l-6.7-4-69.8 18.3L72 359.2l-4.4-7c-18.5-29.4-28.2-63.3-28.2-98.2 0-101.7 82.8-184.5 184.6-184.5 49.3 0 95.6 19.2 130.4 54.1 34.8 34.9 56.2 81.2 56.1 130.5 0 101.8-84.9 184.6-186.6 184.6zm101.2-138.2c-5.5-2.8-32.8-16.2-37.9-18-5.1-1.9-8.8-2.8-12.5 2.8-3.7 5.6-14.3 18-17.6 21.8-3.2 3.7-6.5 4.2-12 1.4-32.6-16.3-54-29.1-75.5-66-5.7-9.8 5.7-9.1 16.3-30.3 1.8-3.7.9-6.9-.5-9.7-1.4-2.8-12.5-30.1-17.1-41.2-4.5-10.8-9.1-9.3-12.5-9.5-3.2-.2-6.9-.2-10.6-.2-3.7 0-9.7 1.4-14.8 6.9-5.1 5.6-19.4 19-19.4 46.3 0 27.3 19.9 53.7 22.6 57.4 2.8 3.7 39.1 59.7 94.8 83.8 35.2 15.2 49 16.5 66.6 13.9 10.7-1.6 32.8-13.4 37.4-26.4 4.6-13 4.6-24.1 3.2-26.4-1.3-2.5-5-3.9-10.5-6.6z" fill="currentColor"&gt;
			
			
				&lt;path d="M320 448v40c0 13.255-10.745 24-24 24H24c-13.255 0-24-10.745-24-24V120c0-13.255 10.745-24 24-24h72v296c0 30.879 25.121 56 56 56h168zm0-344V0H152c-13.255 0-24 10.745-24 24v368c0 13.255 10.745 24 24 24h272c13.255 0 24-10.745 24-24V128H344c-13.2 0-24-10.8-24-24zm120.971-31.029L375.029 7.029A24 24 0 0 0 358.059 0H352v96h96v-6.059a24 24 0 0 0-7.029-16.97z" fill="currentColor"&gt;
			
			
				&lt;path d="M352 320c-22.608 0-43.387 7.819-59.79 20.895l-102.486-64.054a96.551 96.551 0 0 0 0-41.683l102.486-64.054C308.613 184.181 329.392 192 352 192c53.019 0 96-42.981 96-96S405.019 0 352 0s-96 42.981-96 96c0 7.158.79 14.13 2.276 20.841L155.79 180.895C139.387 167.819 118.608 160 96 160c-53.019 0-96 42.981-96 96s42.981 96 96 96c22.608 0 43.387-7.819 59.79-20.895l102.486 64.054A96.301 96.301 0 0 0 256 416c0 53.019 42.981 96 96 96s96-42.981 96-96-42.981-96-96-96z" fill="currentColor"&gt;
			
			
				&lt;path d="M440.3 203.5c-15 0-28.2 6.2-37.9 15.9-35.7-24.7-83.8-40.6-137.1-42.3L293 52.3l88.2 19.8c0 21.6 17.6 39.2 39.2 39.2 22 0 39.7-18.1 39.7-39.7s-17.6-39.7-39.7-39.7c-15.4 0-28.7 9.3-35.3 22l-97.4-21.6c-4.9-1.3-9.7 2.2-11 7.1L246.3 177c-52.9 2.2-100.5 18.1-136.3 42.8-9.7-10.1-23.4-16.3-38.4-16.3-55.6 0-73.8 74.6-22.9 100.1-1.8 7.9-2.6 16.3-2.6 24.7 0 83.8 94.4 151.7 210.3 151.7 116.4 0 210.8-67.9 210.8-151.7 0-8.4-.9-17.2-3.1-25.1 49.9-25.6 31.5-99.7-23.8-99.7zM129.4 308.9c0-22 17.6-39.7 39.7-39.7 21.6 0 39.2 17.6 39.2 39.7 0 21.6-17.6 39.2-39.2 39.2-22 .1-39.7-17.6-39.7-39.2zm214.3 93.5c-36.4 36.4-139.1 36.4-175.5 0-4-3.5-4-9.7 0-13.7 3.5-3.5 9.7-3.5 13.2 0 27.8 28.5 120 29 149 0 3.5-3.5 9.7-3.5 13.2 0 4.1 4 4.1 10.2.1 13.7zm-.8-54.2c-21.6 0-39.2-17.6-39.2-39.2 0-22 17.6-39.7 39.2-39.7 22 0 39.7 17.6 39.7 39.7-.1 21.5-17.7 39.2-39.7 39.2z" fill="currentColor"&gt;
			
			
				&lt;path d="M446.7 98.6l-67.6 318.8c-5.1 22.5-18.4 28.1-37.3 17.5l-103-75.9-49.7 47.8c-5.5 5.5-10.1 10.1-20.7 10.1l7.4-104.9 190.9-172.5c8.3-7.4-1.8-11.5-12.9-4.1L117.8 284 16.2 252.2c-22.1-6.9-22.5-22.1 4.6-32.7L418.2 66.4c18.4-6.9 34.5 4.1 28.5 32.2z" fill="currentColor"&gt;
			
			
				&lt;g&gt;
					&lt;path d="M97.2800192,3.739673 L100.160021,15.3787704 C88.8306631,18.1647705 77.9879854,22.6484879 68.0000023,28.6777391 L61.8399988,18.3985363 C72.8467373,11.7537029 84.7951803,6.81153332 97.2800192,3.739673 Z M158.720055,3.739673 L155.840053,15.3787704 C167.169411,18.1647705 178.012089,22.6484879 188.000072,28.6777391 L194.200075,18.3985363 C183.180932,11.7499974 171.218739,6.80771878 158.720055,3.739673 L158.720055,3.739673 Z M18.3999736,61.8351679 C11.7546212,72.8410466 6.81206547,84.7885562 3.73996516,97.2724198 L15.3799719,100.152197 C18.1661896,88.8237238 22.6502573,77.981893 28.6799796,67.9946902 L18.3999736,61.8351679 Z M11.9999699,127.990038 C11.9961044,122.172725 12.4306685,116.363392 13.2999707,110.611385 L1.43996383,108.811525 C-0.479938607,121.525138 -0.479938607,134.454937 1.43996383,147.168551 L13.2999707,145.36869 C12.4306685,139.616684 11.9961044,133.807351 11.9999699,127.990038 L11.9999699,127.990038 Z M194.160075,237.581539 L188.000072,227.302336 C178.024494,233.327885 167.195565,237.811494 155.880053,240.601305 L158.760055,252.240403 C171.231048,249.164732 183.165742,244.222671 194.160075,237.581539 L194.160075,237.581539 Z M244.000104,127.990038 C244.00397,133.807351 243.569406,139.616684 242.700103,145.36869 L254.56011,147.168551 C256.480013,134.454937 256.480013,121.525138 254.56011,108.811525 L242.700103,110.611385 C243.569406,116.363392 244.00397,122.172725 244.000104,127.990038 Z M252.260109,158.707656 L240.620102,155.827879 C237.833884,167.156352 233.349817,177.998183 227.320094,187.985385 L237.6001,194.184905 C244.249159,183.166622 249.191823,171.205364 252.260109,158.707656 L252.260109,158.707656 Z M145.380047,242.701142 C133.858209,244.43447 122.141865,244.43447 110.620027,242.701142 L108.820026,254.560223 C121.534632,256.479975 134.465442,256.479975 147.180048,254.560223 L145.380047,242.701142 Z M221.380091,196.804701 C214.461479,206.174141 206.175877,214.452354 196.800077,221.362797 L203.920081,231.022048 C214.262958,223.418011 223.404944,214.303705 231.040097,203.984145 L221.380091,196.804701 Z M196.800077,34.6172785 C206.177345,41.5338058 214.463023,49.8188367 221.380091,59.1953726 L231.040097,51.9959309 C223.429284,41.6822474 214.31457,32.5682452 204.000081,24.9580276 L196.800077,34.6172785 Z M34.619983,59.1953726 C41.5370506,49.8188367 49.8227288,41.5338058 59.1999972,34.6172785 L51.9999931,24.9580276 C41.6855038,32.5682452 32.5707896,41.6822474 24.9599774,51.9959309 L34.619983,59.1953726 Z M237.6001,61.8351679 L227.320094,67.9946902 C233.346114,77.969489 237.830073,88.7975718 240.620102,100.1122 L252.260109,97.2324229 C249.184198,84.7624043 244.241751,72.8286423 237.6001,61.8351679 L237.6001,61.8351679 Z M110.620027,13.2989317 C122.141865,11.5656035 133.858209,11.5656035 145.380047,13.2989317 L147.180048,1.43985134 C134.465442,-0.479901112 121.534632,-0.479901112 108.820026,1.43985134 L110.620027,13.2989317 Z M40.7799866,234.201801 L15.9999722,239.981353 L21.7799756,215.203275 L10.0999688,212.463487 L4.3199655,237.241566 C3.3734444,241.28318 4.58320332,245.526897 7.51859925,248.462064 C10.4539952,251.39723 14.6980441,252.606895 18.7399738,251.660448 L43.4999881,245.980888 L40.7799866,234.201801 Z M12.5999703,201.764317 L24.279977,204.484106 L28.2799793,187.305438 C22.4496684,177.507146 18.1025197,166.899584 15.3799719,155.827879 L3.73996516,158.707656 C6.34937618,169.311891 10.3154147,179.535405 15.539972,189.125297 L12.5999703,201.764317 Z M68.6000027,227.762301 L51.4199927,231.761991 L54.1399943,243.441085 L66.7800016,240.501313 C76.3706428,245.725462 86.5949557,249.691191 97.2000192,252.300398 L100.080021,240.6613 C89.0307035,237.906432 78.4495684,233.532789 68.6800027,227.682307 L68.6000027,227.762301 Z M128.000037,23.9980665 C90.1565244,24.0177003 55.3105242,44.590631 37.01511,77.715217 C18.7196958,110.839803 19.8628631,151.287212 39.9999861,183.325747 L29.9999803,225.982439 L72.660005,215.983214 C110.077932,239.548522 158.307237,236.876754 192.892851,209.322653 C227.478464,181.768552 240.856271,135.358391 226.242944,93.6248278 C211.629616,51.8912646 172.221191,23.9617202 128.000037,23.9980665 Z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			
				&lt;g&gt;
					&lt;path d="M357.1,324.5c-24.1,15.3-57.2,21.4-79.1,23.6l18.4,18.1l67,67c24.5,25.1-15.4,64.4-40.2,40.2c-16.8-17-41.4-41.6-67-67.3
						l-67,67.2c-24.8,24.2-64.7-15.5-39.9-40.2c17-17,41.4-41.6,67-67l18.1-18.1c-21.6-2.3-55.3-8-79.6-23.6
						c-28.6-18.5-41.2-29.3-30.1-51.8c6.5-12.8,24.3-23.6,48-5c0,0,31.9,25.4,83.4,25.4s83.4-25.4,83.4-25.4c23.6-18.5,41.4-7.8,48,5
						C398.3,295.1,385.7,305.9,357.1,324.5L357.1,324.5z M142,145c0-63,51.2-114,114-114s114,51,114,114c0,62.7-51.2,113.7-114,113.7
						S142,207.7,142,145L142,145z M200,145c0,30.8,25.1,56,56,56s56-25.1,56-56c0-31.1-25.1-56.2-56-56.2S200,113.9,200,145z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			
				&lt;g transform="translate(0,664)"&gt;
					&lt;path d="m 1073.3513,-606.40537 h 196.278 c 179.2103,0 221.8795,42.66915 221.8795,221.8795 v 196.27799 c 0,179.2103512 -42.6692,221.879451 -221.8795,221.879451 h -196.278 c -179.21038,0 -221.87951,-42.6691298 -221.87951,-221.879451 v -196.27801 c 0,-179.21035 42.66913,-221.87946 221.87951,-221.87948 z" fill="currentColor"&gt;
					&lt;path d="m 1375.0576,-393.98425 c 2.9513,-9.7072 0,-16.85429 -14.1342,-16.85429 h -46.6693 c -11.8763,0 -17.3521,6.16927 -20.3212,12.97854 0,0 -23.7347,56.82106 -57.3544,93.74763 -10.8806,10.66728 -15.8232,14.08081 -21.7613,14.08081 -2.969,0 -7.2715,-3.39577 -7.2715,-13.12075 v -90.83194 c 0,-11.66288 -3.4491,-16.85429 -13.3341,-16.85429 h -73.3553 c -7.4138,0 -11.8763,5.40476 -11.8763,10.54286 0,11.0406 16.8188,13.60078 18.5433,44.67814 v 67.52388 c 0,14.80973 -2.7202,17.49433 -8.6583,17.49433 -15.8231,0 -54.3143,-57.08773 -77.16,-122.40705 -4.4447,-12.71185 -8.9427,-17.83214 -20.8723,-17.83214 h -46.68718 c -13.3341,0 -16.0009,6.16925 -16.0009,12.97852 0,12.12515 15.8232,72.35973 73.69318,152.02656 38.58,54.40315 92.8942,83.89819 142.3726,83.89819 29.6729,0 33.3353,-6.54262 33.3353,-17.83216 v -41.12238 c 0,-13.10297 2.809,-15.71646 12.214,-15.71646 6.9338,0 18.7922,3.41353 46.4916,29.63728 31.6463,31.09512 36.8555,45.03372 54.6698,45.03372 h 46.6694 c 13.3341,0 20.0189,-6.54262 16.1787,-19.46781 -4.2313,-12.88962 -19.3433,-31.57515 -39.38,-53.74532 -10.8807,-12.62294 -27.2016,-26.22375 -32.1441,-33.03302 -6.9338,-8.72941 -4.9603,-12.62294 0,-20.39227 0,0 56.8566,-78.68897 62.7947,-105.41058 z" fill="currentColor"&gt;
					&lt;path d="m 567.69877,-429.06912 c 3.15618,-10.38133 0,-18.0247 -15.11579,-18.0247 h -49.91013 c -12.70096,0 -18.55706,6.59763 -21.73232,13.87977 0,0 -25.38286,60.76685 -61.33724,100.25768 -11.63627,11.40806 -16.92197,15.05863 -23.27242,15.05863 -3.17519,0 -7.77644,-3.63156 -7.77644,-14.0319 v -97.13948 c 0,-12.47278 -3.68869,-18.0247 -14.26014,-18.0247 h -78.44923 c -7.92857,0 -12.70097,5.78005 -12.70097,11.27491 0,11.80736 17.98666,14.54527 19.83094,47.78071 v 72.21293 c 0,15.83815 -2.9091,18.70918 -9.25948,18.70918 -16.92197,0 -58.08598,-61.05206 -82.51817,-130.90731 -4.75337,-13.59458 -9.56381,-19.07042 -22.32175,-19.07042 h -49.92915 c -14.26014,0 -17.11213,6.59763 -17.11213,13.87977 0,12.96714 16.92197,77.38454 78.81059,162.58363 41.25909,58.18101 99.34506,89.72424 152.25931,89.72424 31.73343,0 35.65018,-6.99691 35.65018,-19.07043 v -43.978 c 0,-14.01288 3.00405,-16.80786 13.0622,-16.80786 7.41521,0 20.09716,3.65057 49.71998,31.69536 33.84387,33.25443 39.41486,48.16093 58.46622,48.16093 h 49.91026 c 14.26,0 21.40913,-6.99691 17.30216,-20.81966 -4.5252,-13.78473 -20.68653,-33.76783 -42.11468,-57.47752 -11.63621,-13.49953 -29.09043,-28.04479 -34.37631,-35.32694 -7.41508,-9.33557 -5.30458,-13.4995 0,-21.80835 0,0 60.80491,-84.15334 67.15549,-112.73048 z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			&lt;path d="M309.8 480.3c-13.6 14.5-50 31.7-97.4 31.7-120.8 0-147-88.8-147-140.6v-144H17.9c-5.5 0-10-4.5-10-10v-68c0-7.2 4.5-13.6 11.3-16 62-21.8 81.5-76 84.3-117.1.8-11 6.5-16.3 16.1-16.3h70.9c5.5 0 10 4.5 10 10v115.2h83c5.5 0 10 4.4 10 9.9v81.7c0 5.5-4.5 10-10 10h-83.4V360c0 34.2 23.7 53.6 68 35.8 4.8-1.9 9-3.2 12.7-2.2 3.5.9 5.8 3.4 7.4 7.9l22 64.3c1.8 5 3.3 10.6-.4 14.5z" fill="currentColor"&gt;
			&lt;path d="M512 208L320 384H288V288H208c-61.9 0-112 50.1-112 112c0 48 32 80 32 80s-128-48-128-176c0-97.2 78.8-176 176-176H288V32h32L512 208z" fill="currentColor"&gt;
			&lt;path d="M309.8 480.3c-13.6 14.5-50 31.7-97.4 31.7-120.8 0-147-88.8-147-140.6v-144H17.9c-5.5 0-10-4.5-10-10v-68c0-7.2 4.5-13.6 11.3-16 62-21.8 81.5-76 84.3-117.1.8-11 6.5-16.3 16.1-16.3h70.9c5.5 0 10 4.5 10 10v115.2h83c5.5 0 10 4.4 10 9.9v81.7c0 5.5-4.5 10-10 10h-83.4V360c0 34.2 23.7 53.6 68 35.8 4.8-1.9 9-3.2 12.7-2.2 3.5.9 5.8 3.4 7.4 7.9l22 64.3c1.8 5 3.3 10.6-.4 14.5z" fill="currentColor"&gt;
			&lt;path d="M433 179.1c0-97.2-63.7-125.7-63.7-125.7-62.5-28.7-228.6-28.4-290.5 0 0 0-63.7 28.5-63.7 125.7 0 115.7-6.6 259.4 105.6 289.1 40.5 10.7 75.3 13 103.3 11.4 50.8-2.8 79.3-18.1 79.3-18.1l-1.7-36.9s-36.3 11.4-77.1 10.1c-40.4-1.4-83-4.4-89.6-54a102.5 102.5 0 0 1 -.9-13.9c85.6 20.9 158.7 9.1 178.8 6.7 56.1-6.7 105-41.3 111.2-72.9 9.8-49.8 9-121.5 9-121.5zm-75.1 125.2h-46.6v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.3V197c0-58.5-64-56.6-64-6.9v114.2H90.2c0-122.1-5.2-147.9 18.4-175 25.9-28.9 79.8-30.8 103.8 6.1l11.6 19.5 11.6-19.5c24.1-37.1 78.1-34.8 103.8-6.1 23.7 27.3 18.4 53 18.4 175z" fill="currentColor"&gt;
			
				&lt;path d="M331.5 235.7c2.2 .9 4.2 1.9 6.3 2.8c29.2 14.1 50.6 35.2 61.8 61.4c15.7 36.5 17.2 95.8-30.3 143.2c-36.2 36.2-80.3 52.5-142.6 53h-.3c-70.2-.5-124.1-24.1-160.4-70.2c-32.3-41-48.9-98.1-49.5-169.6V256v-.2C17 184.3 33.6 127.2 65.9 86.2C102.2 40.1 156.2 16.5 226.4 16h.3c70.3 .5 124.9 24 162.3 69.9c18.4 22.7 32 50 40.6 81.7l-40.4 10.8c-7.1-25.8-17.8-47.8-32.2-65.4c-29.2-35.8-73-54.2-130.5-54.6c-57 .5-100.1 18.8-128.2 54.4C72.1 146.1 58.5 194.3 58 256c.5 61.7 14.1 109.9 40.3 143.3c28 35.6 71.2 53.9 128.2 54.4c51.4-.4 85.4-12.6 113.7-40.9c32.3-32.2 31.7-71.8 21.4-95.9c-6.1-14.2-17.1-26-31.9-34.9c-3.7 26.9-11.8 48.3-24.7 64.8c-17.1 21.8-41.4 33.6-72.7 35.3c-23.6 1.3-46.3-4.4-63.9-16c-20.8-13.8-33-34.8-34.3-59.3c-2.5-48.3 35.7-83 95.2-86.4c21.1-1.2 40.9-.3 59.2 2.8c-2.4-14.8-7.3-26.6-14.6-35.2c-10-11.7-25.6-17.7-46.2-17.8H227c-16.6 0-39 4.6-53.3 26.3l-34.4-23.6c19.2-29.1 50.3-45.1 87.8-45.1h.8c62.6 .4 99.9 39.5 103.7 107.7l-.2 .2zm-156 68.8c1.3 25.1 28.4 36.8 54.6 35.3c25.6-1.4 54.6-11.4 59.5-73.2c-13.2-2.9-27.8-4.4-43.4-4.4c-4.8 0-9.6 .1-14.4 .4c-42.9 2.4-57.2 23.2-56.2 41.8l-.1 .1z" fill="currentColor"&gt;
			
			
				&lt;path d="M407.8 294.7c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3zM288 227.1C261.9 176.4 190.9 81.9 124.9 35.3C61.6-9.4 37.5-1.7 21.6 5.5C3.3 13.8 0 41.9 0 58.4S9.1 194 15 213.9c19.5 65.7 89.1 87.9 153.2 80.7c3.3-.5 6.6-.9 10-1.4c-3.3 .5-6.6 1-10 1.4C74.3 308.6-9.1 342.8 100.3 464.5C220.6 589.1 265.1 437.8 288 361.1c22.9 76.7 49.2 222.5 185.6 103.4c102.4-103.4 28.1-156-65.8-169.9c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3c64.1 7.1 133.6-15.1 153.2-80.7C566.9 194 576 75 576 58.4s-3.3-44.7-21.6-52.9c-15.8-7.1-40-14.9-103.2 29.8C385.1 81.9 314.1 176.4 288 227.1z" fill="currentColor"&gt;
			
			
				&lt;path d="M204 6.5C101.4 6.5 0 74.9 0 185.6 0 256 39.6 296 63.6 296c9.9 0 15.6-27.6 15.6-35.4 0-9.3-23.7-29.1-23.7-67.8 0-80.4 61.2-137.4 140.4-137.4 68.1 0 118.5 38.7 118.5 109.8 0 53.1-21.3 152.7-90.3 152.7-24.9 0-46.2-18-46.2-43.8 0-37.8 26.4-74.4 26.4-113.4 0-66.2-93.9-54.2-93.9 25.8 0 16.8 2.1 35.4 9.6 50.7-13.8 59.4-42 147.9-42 209.1 0 18.9 2.7 37.5 4.5 56.4 3.4 3.8 1.7 3.4 6.9 1.5 50.4-69 48.6-82.5 71.4-172.8 12.3 23.4 44.1 36 69.3 36 106.2 0 153.9-103.5 153.9-196.8C384 71.3 298.2 6.5 204 6.5z" fill="currentColor"&gt;
			
		&lt;/svg&gt;
		&lt;div id="has-mastodon-prompt"&gt;
			&lt;h3&gt;Share on Mastodon&lt;/h3&gt;
			
		&lt;/div&gt;</description><content:encoded>&lt;!-- OneTrust Cookies Consent Notice start for nvidia.com --&gt;


&lt;!-- OneTrust Cookies Consent Notice end for nvidia.com --&gt;


	
	
	
	
	
	

	

	
	
	


	&lt;!-- This site is optimized with the Yoast SEO Premium plugin v25.2 (Yoast SEO v25.2) - https://yoast.com/wordpress/plugins/seo/ --&gt;
	The More You Buy, the More You Make | NVIDIA Blog
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	&lt;!-- / Yoast SEO Premium plugin. --&gt;







































&lt;!-- Stream WordPress user activity plugin v4.1.1 --&gt;


	
				&lt;!-- Hotjar Tracking Code for NVIDIA --&gt;
			
			

				
				



		
		
&lt;div class="hfeed site" id="page"&gt;
	Skip to content

	&lt;!-- #masthead --&gt;
		
		&lt;div class="full-width-layout dark"&gt;
		

		
&lt;div class="full-width-layout__hero dark"&gt;
	&lt;div class="full-width-layout__hero-content dark"&gt;
		&lt;div class="full-width-layout__hero-content__inner dark"&gt;
			

							&lt;p&gt;
					How NVIDIA’s AI factory platform balances maximum performance and minimum latency, optimizing AI inference to power the next industrial revolution.				&lt;/p&gt;
			
			
		&lt;/div&gt;
	&lt;/div&gt;

	&lt;p&gt;
		&lt;video class="full-width-layout__hero-video js-responsive-video" loop="loop"&gt;Your browser does not support the video tag.&lt;/video&gt;	&lt;/p&gt;

	&lt;/div&gt;

	
	
		&lt;div class="full-width-layout__sections"&gt;
&lt;div class="full-width-layout__article-copy-section dark"&gt;
	&lt;div class="full-width-layout__copy"&gt;
&lt;p&gt;&lt;span&gt;When we prompt generative AI to answer a question or create an image, large language models generate tokens of intelligence that combine to provide the result.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;One prompt. One set of tokens for the answer. This is called &lt;/span&gt;&lt;span&gt;AI inference&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;
	&lt;video class="full-width-layout__video js-responsive-video" loop="loop"&gt;Your browser does not support the video tag.&lt;/video&gt;&lt;/p&gt;

&lt;div class="full-width-layout__article-copy-section dark"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Agentic AI uses reasoning to complete tasks. AI agents aren’t just providing one-shot answers. They break tasks down into a series of steps, each one a different inference technique.&lt;/p&gt;
&lt;p&gt;One prompt. Many sets of tokens to complete the job.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;
	&lt;video class="full-width-layout__video js-responsive-video" loop="loop"&gt;Your browser does not support the video tag.&lt;/video&gt;&lt;/p&gt;

&lt;div class="full-width-layout__article-copy-section dark"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;The engines of AI inference are called AI factories — massive infrastructures that serve AI to millions of users at once.&lt;/p&gt;
&lt;p&gt;AI factories generate AI tokens. Their product is intelligence. In the AI era, this intelligence grows revenue and profits. Growing revenue over time depends on how efficient the AI factory can be as it scales.&lt;/p&gt;
&lt;p&gt;AI factories are the machines of the next industrial revolution.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__full-width-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="1152" src="https://blogs.nvidia.com/wp-content/uploads/2025/05/aeirial-view-of-stargate-scaled.jpg" width="2048" /&gt;	
	
&lt;p&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			Aerial view of Crusoe (Stargate)		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section dark"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;AI factories have to balance two competing demands to deliver optimal inference: speed per user and overall system throughput.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="1080" src="https://blogs.nvidia.com/wp-content/uploads/2025/05/Coreweave-data-center.jpg" width="1920" /&gt;	
	
&lt;p&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			CoreWeave, 200MW, USA, scaling globally		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section dark"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;AI factories can improve both factors by scaling — to more FLOPS and higher bandwidth. They can group and process AI workloads to maximize productivity.&lt;/p&gt;
&lt;p&gt;But ultimately, AI factories are limited by the power they can access.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="1080" src="https://blogs.nvidia.com/wp-content/uploads/2025/05/AI-factories-generate-intelligence-token-at-scale.jpg" width="1920" /&gt;	
	&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section dark"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;In a 1-megawatt AI factory, NVIDIA Hopper generates 180,000 tokens per second (TPS) at max volume, or 225 TPS for one user at the fastest.&lt;/p&gt;
&lt;p&gt;But the real work happens in the space in between. Each dot along the curve represents batches of workloads for the AI factory to process — each with its own mix of performance demands.&lt;/p&gt;
&lt;p&gt;NVIDIA GPUs have the flexibility to handle this full spectrum of workloads because they can be programmed using NVIDIA CUDA software.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="1080" src="https://blogs.nvidia.com/wp-content/uploads/2025/05/Hopper-ai-factory-performance.jpg" width="1920" /&gt;	
	&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section dark"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;The NVIDIA Blackwell architecture can do far more with 1 megawatt than the Hopper architecture — and there’s more coming. Optimizing the software and hardware stacks means Blackwell gets faster and more efficient over time.&lt;/p&gt;
&lt;p&gt;Blackwell gets another boost when developers optimize the AI factory workloads autonomously with NVIDIA Dynamo, the new operating system for AI factories.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;
	&lt;video class="full-width-layout__video js-responsive-video" loop="loop"&gt;Your browser does not support the video tag.&lt;/video&gt;&lt;/p&gt;

&lt;div class="full-width-layout__article-copy-section dark"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Dynamo breaks inference tasks into smaller components, dynamically routing and rerouting workloads to the most optimal compute resources available at that moment.&lt;/p&gt;
&lt;p&gt;The improvements are remarkable. In a single generational leap of processor architecture from Hopper to Blackwell, we can achieve a 50x improvement in AI reasoning performance using the same amount of energy.&lt;/p&gt;
&lt;p&gt;This is how NVIDIA full-stack integration and advanced software give customers massive speed and efficiency boosts in the time between chip architecture generations.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="1080" src="https://blogs.nvidia.com/wp-content/uploads/2025/05/NVIDIA-Dynamo-scaled-1.jpg" width="1920" /&gt;	
	&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section dark"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;&lt;span&gt;We push this curve outward with each generation, from hardware to software, from compute to networking.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;And with each push forward in performance, AI can create trillions of dollars of productivity for NVIDIA’s partners and customers around the globe — while bringing us one step closer to curing diseases, reversing climate change and uncovering some of the greatest secrets of the universe.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;This is compute turning into capital — and progress.&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;
	&lt;video class="full-width-layout__video js-responsive-video" loop="loop"&gt;Your browser does not support the video tag.&lt;/video&gt;&lt;/p&gt;
&lt;/div&gt;
	&lt;div class="full-width-layout__news-section"&gt;
		&lt;p&gt;Related News&lt;/p&gt;

		&lt;div class="full-width-layout__news"&gt;
			
&lt;article class="full-width-layout__news-post-tile  post-81753 post type-post status-publish format-standard has-post-thumbnail hentry category-corporate category-enterprise category-generative-ai category-supercomputing tag-artificial-intelligence tag-events tag-nvidia-inception tag-social-impact loop-item-1 for-pagenum-1" id="related-news-post-81753"&gt;
	
		&lt;img alt="alt" class="attachment-medium size-medium" height="540" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/eyJ3IjoyMDQ4LCJoIjoyMDQ4LCJzY29wZSI6ImFwcCJ9-960x540.webp" width="960" /&gt;	

	
		‘AI Maker, Not an AI Taker’: UK Builds Its Vision With NVIDIA Infrastructure	

	&lt;div class="full-width-layout__news-post-excerpt	dark" id="related-news-post-desc-81753"&gt;
		U.K. Prime Minister Keir Starmer’s ambition for Britain to be an “AI maker, not an AI taker,” is becoming a reality at London Tech Week. With NVIDIA’s support, the U.K....    		Read Article        &lt;span&gt;&lt;/span&gt;    	&lt;/div&gt;
&lt;/article&gt;

&lt;article class="full-width-layout__news-post-tile  post-81538 post type-post status-publish format-standard has-post-thumbnail hentry category-generative-ai tag-art tag-artificial-intelligence tag-creators tag-gtc loop-item-1 for-pagenum-1" id="related-news-post-81538"&gt;
	
		&lt;img alt="alt" class="attachment-medium size-medium" height="540" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/auditoire-render-ai-gallery-may-28-2-960x540.jpg" width="960" /&gt;	

	
		Artists, Fashion Designers Tap State-of-the-Art AI for NVIDIA GTC Paris Gallery	

	&lt;div class="full-width-layout__news-post-excerpt	dark" id="related-news-post-desc-81538"&gt;
		The conference, taking place in one of Europe’s iconic art capitals, will feature a curated gallery that showcases how AI helps bring creative visions to life....    		Read Article        &lt;span&gt;&lt;/span&gt;    	&lt;/div&gt;
&lt;/article&gt;

&lt;article class="full-width-layout__news-post-tile  post-81515 post type-post status-publish format-standard has-post-thumbnail hentry category-gaming tag-cloud-gaming tag-geforce-now loop-item-1 for-pagenum-1" id="related-news-post-81515"&gt;
	
		&lt;img alt="alt" class="attachment-medium size-medium" height="510" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/gfn-thursday-6-5-nv-blog-1280x680-logo-960x510.jpg" width="960" /&gt;	

	
		GeForce NOW Kicks Off a Summer of Gaming With 25 New Titles This June	

	&lt;div class="full-width-layout__news-post-excerpt	dark" id="related-news-post-desc-81515"&gt;
		GeForce NOW is a gamer’s ticket to an unforgettable summer of gaming. With 25 titles coming this month and endless ways to play, the summer is going to be epic....    		Read Article        &lt;span&gt;&lt;/span&gt;    	&lt;/div&gt;
&lt;/article&gt;

&lt;article class="full-width-layout__news-post-tile  post-81446 post type-post status-publish format-standard has-post-thumbnail hentry category-generative-ai category-the-ai-podcast loop-item-1 for-pagenum-1" id="related-news-post-81446"&gt;
	
		&lt;img alt="alt" class="attachment-medium size-medium" height="540" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/1X_Still_A-960x540.png" width="960" /&gt;	

	
		How 1X Technologies’ Robots Are Learning to Lend a Helping Hand	

	&lt;div class="full-width-layout__news-post-excerpt	dark" id="related-news-post-desc-81446"&gt;
		Humans learn the norms, values and behaviors of society from each other — and Bernt Børnich, founder and CEO of 1X Technologies, thinks robots should learn like this, too. “For...    		Read Article        &lt;span&gt;&lt;/span&gt;    	&lt;/div&gt;
&lt;/article&gt;
		&lt;/div&gt;
	&lt;/div&gt;
		&lt;/div&gt;
	


&lt;!-- #colophon --&gt;

&lt;/div&gt;&lt;!-- #page --&gt;



	            &lt;!-- #has-highlight-and-share --&gt;		&lt;svg class="hidden" height="0" width="0" xmlns="http://www.w3.org/2000/svg"&gt;
			
				&lt;g&gt;&lt;path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z" fill="currentColor"&gt;&lt;/g&gt;
			
			
				&lt;path d="M279.14 288l14.22-92.66h-88.91v-60.13c0-25.35 12.42-50.06 52.24-50.06h40.42V6.26S260.43 0 225.36 0c-73.22 0-121.08 44.38-121.08 124.72v70.62H22.89V288h81.39v224h100.17V288z" fill="currentColor"&gt;
			
			
				&lt;path d="M256 8C118.941 8 8 118.919 8 256c0 137.059 110.919 248 248 248 48.154 0 95.342-14.14 135.408-40.223 12.005-7.815 14.625-24.288 5.552-35.372l-10.177-12.433c-7.671-9.371-21.179-11.667-31.373-5.129C325.92 429.757 291.314 440 256 440c-101.458 0-184-82.542-184-184S154.542 72 256 72c100.139 0 184 57.619 184 160 0 38.786-21.093 79.742-58.17 83.693-17.349-.454-16.91-12.857-13.476-30.024l23.433-121.11C394.653 149.75 383.308 136 368.225 136h-44.981a13.518 13.518 0 0 0-13.432 11.993l-.01.092c-14.697-17.901-40.448-21.775-59.971-21.775-74.58 0-137.831 62.234-137.831 151.46 0 65.303 36.785 105.87 96 105.87 26.984 0 57.369-15.637 74.991-38.333 9.522 34.104 40.613 34.103 70.71 34.103C462.609 379.41 504 307.798 504 232 504 95.653 394.023 8 256 8zm-21.68 304.43c-22.249 0-36.07-15.623-36.07-40.771 0-44.993 30.779-72.729 58.63-72.729 22.292 0 35.601 15.241 35.601 40.77 0 45.061-33.875 72.73-58.161 72.73z" fill="currentColor"&gt;
			
			
				&lt;path d="M100.28 448H7.4V148.9h92.88zM53.79 108.1C24.09 108.1 0 83.5 0 53.8a53.79 53.79 0 0 1 107.58 0c0 29.7-24.1 54.3-53.79 54.3zM447.9 448h-92.68V302.4c0-34.7-.7-79.2-48.29-79.2-48.29 0-55.69 37.7-55.69 76.7V448h-92.78V148.9h89.08v40.8h1.3c12.4-23.5 42.69-48.3 87.88-48.3 94 0 111.28 61.9 111.28 142.3V448z" fill="currentColor"&gt;
			
			
				&lt;path d="M162.7 210c-1.8 3.3-25.2 44.4-70.1 123.5-4.9 8.3-10.8 12.5-17.7 12.5H9.8c-7.7 0-12.1-7.5-8.5-14.4l69-121.3c.2 0 .2-.1 0-.3l-43.9-75.6c-4.3-7.8.3-14.1 8.5-14.1H100c7.3 0 13.3 4.1 18 12.2l44.7 77.5zM382.6 46.1l-144 253v.3L330.2 466c3.9 7.1.2 14.1-8.5 14.1h-65.2c-7.6 0-13.6-4-18-12.2l-92.4-168.5c3.3-5.8 51.5-90.8 144.8-255.2 4.6-8.1 10.4-12.2 17.5-12.2h65.7c8 0 12.3 6.7 8.5 14.1z" fill="currentColor"&gt;
			
			
				&lt;path d="M380.9 97.1C339 55.1 283.2 32 223.9 32c-122.4 0-222 99.6-222 222 0 39.1 10.2 77.3 29.6 111L0 480l117.7-30.9c32.4 17.7 68.9 27 106.1 27h.1c122.3 0 224.1-99.6 224.1-222 0-59.3-25.2-115-67.1-157zm-157 341.6c-33.2 0-65.7-8.9-94-25.7l-6.7-4-69.8 18.3L72 359.2l-4.4-7c-18.5-29.4-28.2-63.3-28.2-98.2 0-101.7 82.8-184.5 184.6-184.5 49.3 0 95.6 19.2 130.4 54.1 34.8 34.9 56.2 81.2 56.1 130.5 0 101.8-84.9 184.6-186.6 184.6zm101.2-138.2c-5.5-2.8-32.8-16.2-37.9-18-5.1-1.9-8.8-2.8-12.5 2.8-3.7 5.6-14.3 18-17.6 21.8-3.2 3.7-6.5 4.2-12 1.4-32.6-16.3-54-29.1-75.5-66-5.7-9.8 5.7-9.1 16.3-30.3 1.8-3.7.9-6.9-.5-9.7-1.4-2.8-12.5-30.1-17.1-41.2-4.5-10.8-9.1-9.3-12.5-9.5-3.2-.2-6.9-.2-10.6-.2-3.7 0-9.7 1.4-14.8 6.9-5.1 5.6-19.4 19-19.4 46.3 0 27.3 19.9 53.7 22.6 57.4 2.8 3.7 39.1 59.7 94.8 83.8 35.2 15.2 49 16.5 66.6 13.9 10.7-1.6 32.8-13.4 37.4-26.4 4.6-13 4.6-24.1 3.2-26.4-1.3-2.5-5-3.9-10.5-6.6z" fill="currentColor"&gt;
			
			
				&lt;path d="M320 448v40c0 13.255-10.745 24-24 24H24c-13.255 0-24-10.745-24-24V120c0-13.255 10.745-24 24-24h72v296c0 30.879 25.121 56 56 56h168zm0-344V0H152c-13.255 0-24 10.745-24 24v368c0 13.255 10.745 24 24 24h272c13.255 0 24-10.745 24-24V128H344c-13.2 0-24-10.8-24-24zm120.971-31.029L375.029 7.029A24 24 0 0 0 358.059 0H352v96h96v-6.059a24 24 0 0 0-7.029-16.97z" fill="currentColor"&gt;
			
			
				&lt;path d="M352 320c-22.608 0-43.387 7.819-59.79 20.895l-102.486-64.054a96.551 96.551 0 0 0 0-41.683l102.486-64.054C308.613 184.181 329.392 192 352 192c53.019 0 96-42.981 96-96S405.019 0 352 0s-96 42.981-96 96c0 7.158.79 14.13 2.276 20.841L155.79 180.895C139.387 167.819 118.608 160 96 160c-53.019 0-96 42.981-96 96s42.981 96 96 96c22.608 0 43.387-7.819 59.79-20.895l102.486 64.054A96.301 96.301 0 0 0 256 416c0 53.019 42.981 96 96 96s96-42.981 96-96-42.981-96-96-96z" fill="currentColor"&gt;
			
			
				&lt;path d="M440.3 203.5c-15 0-28.2 6.2-37.9 15.9-35.7-24.7-83.8-40.6-137.1-42.3L293 52.3l88.2 19.8c0 21.6 17.6 39.2 39.2 39.2 22 0 39.7-18.1 39.7-39.7s-17.6-39.7-39.7-39.7c-15.4 0-28.7 9.3-35.3 22l-97.4-21.6c-4.9-1.3-9.7 2.2-11 7.1L246.3 177c-52.9 2.2-100.5 18.1-136.3 42.8-9.7-10.1-23.4-16.3-38.4-16.3-55.6 0-73.8 74.6-22.9 100.1-1.8 7.9-2.6 16.3-2.6 24.7 0 83.8 94.4 151.7 210.3 151.7 116.4 0 210.8-67.9 210.8-151.7 0-8.4-.9-17.2-3.1-25.1 49.9-25.6 31.5-99.7-23.8-99.7zM129.4 308.9c0-22 17.6-39.7 39.7-39.7 21.6 0 39.2 17.6 39.2 39.7 0 21.6-17.6 39.2-39.2 39.2-22 .1-39.7-17.6-39.7-39.2zm214.3 93.5c-36.4 36.4-139.1 36.4-175.5 0-4-3.5-4-9.7 0-13.7 3.5-3.5 9.7-3.5 13.2 0 27.8 28.5 120 29 149 0 3.5-3.5 9.7-3.5 13.2 0 4.1 4 4.1 10.2.1 13.7zm-.8-54.2c-21.6 0-39.2-17.6-39.2-39.2 0-22 17.6-39.7 39.2-39.7 22 0 39.7 17.6 39.7 39.7-.1 21.5-17.7 39.2-39.7 39.2z" fill="currentColor"&gt;
			
			
				&lt;path d="M446.7 98.6l-67.6 318.8c-5.1 22.5-18.4 28.1-37.3 17.5l-103-75.9-49.7 47.8c-5.5 5.5-10.1 10.1-20.7 10.1l7.4-104.9 190.9-172.5c8.3-7.4-1.8-11.5-12.9-4.1L117.8 284 16.2 252.2c-22.1-6.9-22.5-22.1 4.6-32.7L418.2 66.4c18.4-6.9 34.5 4.1 28.5 32.2z" fill="currentColor"&gt;
			
			
				&lt;g&gt;
					&lt;path d="M97.2800192,3.739673 L100.160021,15.3787704 C88.8306631,18.1647705 77.9879854,22.6484879 68.0000023,28.6777391 L61.8399988,18.3985363 C72.8467373,11.7537029 84.7951803,6.81153332 97.2800192,3.739673 Z M158.720055,3.739673 L155.840053,15.3787704 C167.169411,18.1647705 178.012089,22.6484879 188.000072,28.6777391 L194.200075,18.3985363 C183.180932,11.7499974 171.218739,6.80771878 158.720055,3.739673 L158.720055,3.739673 Z M18.3999736,61.8351679 C11.7546212,72.8410466 6.81206547,84.7885562 3.73996516,97.2724198 L15.3799719,100.152197 C18.1661896,88.8237238 22.6502573,77.981893 28.6799796,67.9946902 L18.3999736,61.8351679 Z M11.9999699,127.990038 C11.9961044,122.172725 12.4306685,116.363392 13.2999707,110.611385 L1.43996383,108.811525 C-0.479938607,121.525138 -0.479938607,134.454937 1.43996383,147.168551 L13.2999707,145.36869 C12.4306685,139.616684 11.9961044,133.807351 11.9999699,127.990038 L11.9999699,127.990038 Z M194.160075,237.581539 L188.000072,227.302336 C178.024494,233.327885 167.195565,237.811494 155.880053,240.601305 L158.760055,252.240403 C171.231048,249.164732 183.165742,244.222671 194.160075,237.581539 L194.160075,237.581539 Z M244.000104,127.990038 C244.00397,133.807351 243.569406,139.616684 242.700103,145.36869 L254.56011,147.168551 C256.480013,134.454937 256.480013,121.525138 254.56011,108.811525 L242.700103,110.611385 C243.569406,116.363392 244.00397,122.172725 244.000104,127.990038 Z M252.260109,158.707656 L240.620102,155.827879 C237.833884,167.156352 233.349817,177.998183 227.320094,187.985385 L237.6001,194.184905 C244.249159,183.166622 249.191823,171.205364 252.260109,158.707656 L252.260109,158.707656 Z M145.380047,242.701142 C133.858209,244.43447 122.141865,244.43447 110.620027,242.701142 L108.820026,254.560223 C121.534632,256.479975 134.465442,256.479975 147.180048,254.560223 L145.380047,242.701142 Z M221.380091,196.804701 C214.461479,206.174141 206.175877,214.452354 196.800077,221.362797 L203.920081,231.022048 C214.262958,223.418011 223.404944,214.303705 231.040097,203.984145 L221.380091,196.804701 Z M196.800077,34.6172785 C206.177345,41.5338058 214.463023,49.8188367 221.380091,59.1953726 L231.040097,51.9959309 C223.429284,41.6822474 214.31457,32.5682452 204.000081,24.9580276 L196.800077,34.6172785 Z M34.619983,59.1953726 C41.5370506,49.8188367 49.8227288,41.5338058 59.1999972,34.6172785 L51.9999931,24.9580276 C41.6855038,32.5682452 32.5707896,41.6822474 24.9599774,51.9959309 L34.619983,59.1953726 Z M237.6001,61.8351679 L227.320094,67.9946902 C233.346114,77.969489 237.830073,88.7975718 240.620102,100.1122 L252.260109,97.2324229 C249.184198,84.7624043 244.241751,72.8286423 237.6001,61.8351679 L237.6001,61.8351679 Z M110.620027,13.2989317 C122.141865,11.5656035 133.858209,11.5656035 145.380047,13.2989317 L147.180048,1.43985134 C134.465442,-0.479901112 121.534632,-0.479901112 108.820026,1.43985134 L110.620027,13.2989317 Z M40.7799866,234.201801 L15.9999722,239.981353 L21.7799756,215.203275 L10.0999688,212.463487 L4.3199655,237.241566 C3.3734444,241.28318 4.58320332,245.526897 7.51859925,248.462064 C10.4539952,251.39723 14.6980441,252.606895 18.7399738,251.660448 L43.4999881,245.980888 L40.7799866,234.201801 Z M12.5999703,201.764317 L24.279977,204.484106 L28.2799793,187.305438 C22.4496684,177.507146 18.1025197,166.899584 15.3799719,155.827879 L3.73996516,158.707656 C6.34937618,169.311891 10.3154147,179.535405 15.539972,189.125297 L12.5999703,201.764317 Z M68.6000027,227.762301 L51.4199927,231.761991 L54.1399943,243.441085 L66.7800016,240.501313 C76.3706428,245.725462 86.5949557,249.691191 97.2000192,252.300398 L100.080021,240.6613 C89.0307035,237.906432 78.4495684,233.532789 68.6800027,227.682307 L68.6000027,227.762301 Z M128.000037,23.9980665 C90.1565244,24.0177003 55.3105242,44.590631 37.01511,77.715217 C18.7196958,110.839803 19.8628631,151.287212 39.9999861,183.325747 L29.9999803,225.982439 L72.660005,215.983214 C110.077932,239.548522 158.307237,236.876754 192.892851,209.322653 C227.478464,181.768552 240.856271,135.358391 226.242944,93.6248278 C211.629616,51.8912646 172.221191,23.9617202 128.000037,23.9980665 Z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			
				&lt;g&gt;
					&lt;path d="M357.1,324.5c-24.1,15.3-57.2,21.4-79.1,23.6l18.4,18.1l67,67c24.5,25.1-15.4,64.4-40.2,40.2c-16.8-17-41.4-41.6-67-67.3
						l-67,67.2c-24.8,24.2-64.7-15.5-39.9-40.2c17-17,41.4-41.6,67-67l18.1-18.1c-21.6-2.3-55.3-8-79.6-23.6
						c-28.6-18.5-41.2-29.3-30.1-51.8c6.5-12.8,24.3-23.6,48-5c0,0,31.9,25.4,83.4,25.4s83.4-25.4,83.4-25.4c23.6-18.5,41.4-7.8,48,5
						C398.3,295.1,385.7,305.9,357.1,324.5L357.1,324.5z M142,145c0-63,51.2-114,114-114s114,51,114,114c0,62.7-51.2,113.7-114,113.7
						S142,207.7,142,145L142,145z M200,145c0,30.8,25.1,56,56,56s56-25.1,56-56c0-31.1-25.1-56.2-56-56.2S200,113.9,200,145z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			
				&lt;g transform="translate(0,664)"&gt;
					&lt;path d="m 1073.3513,-606.40537 h 196.278 c 179.2103,0 221.8795,42.66915 221.8795,221.8795 v 196.27799 c 0,179.2103512 -42.6692,221.879451 -221.8795,221.879451 h -196.278 c -179.21038,0 -221.87951,-42.6691298 -221.87951,-221.879451 v -196.27801 c 0,-179.21035 42.66913,-221.87946 221.87951,-221.87948 z" fill="currentColor"&gt;
					&lt;path d="m 1375.0576,-393.98425 c 2.9513,-9.7072 0,-16.85429 -14.1342,-16.85429 h -46.6693 c -11.8763,0 -17.3521,6.16927 -20.3212,12.97854 0,0 -23.7347,56.82106 -57.3544,93.74763 -10.8806,10.66728 -15.8232,14.08081 -21.7613,14.08081 -2.969,0 -7.2715,-3.39577 -7.2715,-13.12075 v -90.83194 c 0,-11.66288 -3.4491,-16.85429 -13.3341,-16.85429 h -73.3553 c -7.4138,0 -11.8763,5.40476 -11.8763,10.54286 0,11.0406 16.8188,13.60078 18.5433,44.67814 v 67.52388 c 0,14.80973 -2.7202,17.49433 -8.6583,17.49433 -15.8231,0 -54.3143,-57.08773 -77.16,-122.40705 -4.4447,-12.71185 -8.9427,-17.83214 -20.8723,-17.83214 h -46.68718 c -13.3341,0 -16.0009,6.16925 -16.0009,12.97852 0,12.12515 15.8232,72.35973 73.69318,152.02656 38.58,54.40315 92.8942,83.89819 142.3726,83.89819 29.6729,0 33.3353,-6.54262 33.3353,-17.83216 v -41.12238 c 0,-13.10297 2.809,-15.71646 12.214,-15.71646 6.9338,0 18.7922,3.41353 46.4916,29.63728 31.6463,31.09512 36.8555,45.03372 54.6698,45.03372 h 46.6694 c 13.3341,0 20.0189,-6.54262 16.1787,-19.46781 -4.2313,-12.88962 -19.3433,-31.57515 -39.38,-53.74532 -10.8807,-12.62294 -27.2016,-26.22375 -32.1441,-33.03302 -6.9338,-8.72941 -4.9603,-12.62294 0,-20.39227 0,0 56.8566,-78.68897 62.7947,-105.41058 z" fill="currentColor"&gt;
					&lt;path d="m 567.69877,-429.06912 c 3.15618,-10.38133 0,-18.0247 -15.11579,-18.0247 h -49.91013 c -12.70096,0 -18.55706,6.59763 -21.73232,13.87977 0,0 -25.38286,60.76685 -61.33724,100.25768 -11.63627,11.40806 -16.92197,15.05863 -23.27242,15.05863 -3.17519,0 -7.77644,-3.63156 -7.77644,-14.0319 v -97.13948 c 0,-12.47278 -3.68869,-18.0247 -14.26014,-18.0247 h -78.44923 c -7.92857,0 -12.70097,5.78005 -12.70097,11.27491 0,11.80736 17.98666,14.54527 19.83094,47.78071 v 72.21293 c 0,15.83815 -2.9091,18.70918 -9.25948,18.70918 -16.92197,0 -58.08598,-61.05206 -82.51817,-130.90731 -4.75337,-13.59458 -9.56381,-19.07042 -22.32175,-19.07042 h -49.92915 c -14.26014,0 -17.11213,6.59763 -17.11213,13.87977 0,12.96714 16.92197,77.38454 78.81059,162.58363 41.25909,58.18101 99.34506,89.72424 152.25931,89.72424 31.73343,0 35.65018,-6.99691 35.65018,-19.07043 v -43.978 c 0,-14.01288 3.00405,-16.80786 13.0622,-16.80786 7.41521,0 20.09716,3.65057 49.71998,31.69536 33.84387,33.25443 39.41486,48.16093 58.46622,48.16093 h 49.91026 c 14.26,0 21.40913,-6.99691 17.30216,-20.81966 -4.5252,-13.78473 -20.68653,-33.76783 -42.11468,-57.47752 -11.63621,-13.49953 -29.09043,-28.04479 -34.37631,-35.32694 -7.41508,-9.33557 -5.30458,-13.4995 0,-21.80835 0,0 60.80491,-84.15334 67.15549,-112.73048 z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			&lt;path d="M309.8 480.3c-13.6 14.5-50 31.7-97.4 31.7-120.8 0-147-88.8-147-140.6v-144H17.9c-5.5 0-10-4.5-10-10v-68c0-7.2 4.5-13.6 11.3-16 62-21.8 81.5-76 84.3-117.1.8-11 6.5-16.3 16.1-16.3h70.9c5.5 0 10 4.5 10 10v115.2h83c5.5 0 10 4.4 10 9.9v81.7c0 5.5-4.5 10-10 10h-83.4V360c0 34.2 23.7 53.6 68 35.8 4.8-1.9 9-3.2 12.7-2.2 3.5.9 5.8 3.4 7.4 7.9l22 64.3c1.8 5 3.3 10.6-.4 14.5z" fill="currentColor"&gt;
			&lt;path d="M512 208L320 384H288V288H208c-61.9 0-112 50.1-112 112c0 48 32 80 32 80s-128-48-128-176c0-97.2 78.8-176 176-176H288V32h32L512 208z" fill="currentColor"&gt;
			&lt;path d="M309.8 480.3c-13.6 14.5-50 31.7-97.4 31.7-120.8 0-147-88.8-147-140.6v-144H17.9c-5.5 0-10-4.5-10-10v-68c0-7.2 4.5-13.6 11.3-16 62-21.8 81.5-76 84.3-117.1.8-11 6.5-16.3 16.1-16.3h70.9c5.5 0 10 4.5 10 10v115.2h83c5.5 0 10 4.4 10 9.9v81.7c0 5.5-4.5 10-10 10h-83.4V360c0 34.2 23.7 53.6 68 35.8 4.8-1.9 9-3.2 12.7-2.2 3.5.9 5.8 3.4 7.4 7.9l22 64.3c1.8 5 3.3 10.6-.4 14.5z" fill="currentColor"&gt;
			&lt;path d="M433 179.1c0-97.2-63.7-125.7-63.7-125.7-62.5-28.7-228.6-28.4-290.5 0 0 0-63.7 28.5-63.7 125.7 0 115.7-6.6 259.4 105.6 289.1 40.5 10.7 75.3 13 103.3 11.4 50.8-2.8 79.3-18.1 79.3-18.1l-1.7-36.9s-36.3 11.4-77.1 10.1c-40.4-1.4-83-4.4-89.6-54a102.5 102.5 0 0 1 -.9-13.9c85.6 20.9 158.7 9.1 178.8 6.7 56.1-6.7 105-41.3 111.2-72.9 9.8-49.8 9-121.5 9-121.5zm-75.1 125.2h-46.6v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.3V197c0-58.5-64-56.6-64-6.9v114.2H90.2c0-122.1-5.2-147.9 18.4-175 25.9-28.9 79.8-30.8 103.8 6.1l11.6 19.5 11.6-19.5c24.1-37.1 78.1-34.8 103.8-6.1 23.7 27.3 18.4 53 18.4 175z" fill="currentColor"&gt;
			
				&lt;path d="M331.5 235.7c2.2 .9 4.2 1.9 6.3 2.8c29.2 14.1 50.6 35.2 61.8 61.4c15.7 36.5 17.2 95.8-30.3 143.2c-36.2 36.2-80.3 52.5-142.6 53h-.3c-70.2-.5-124.1-24.1-160.4-70.2c-32.3-41-48.9-98.1-49.5-169.6V256v-.2C17 184.3 33.6 127.2 65.9 86.2C102.2 40.1 156.2 16.5 226.4 16h.3c70.3 .5 124.9 24 162.3 69.9c18.4 22.7 32 50 40.6 81.7l-40.4 10.8c-7.1-25.8-17.8-47.8-32.2-65.4c-29.2-35.8-73-54.2-130.5-54.6c-57 .5-100.1 18.8-128.2 54.4C72.1 146.1 58.5 194.3 58 256c.5 61.7 14.1 109.9 40.3 143.3c28 35.6 71.2 53.9 128.2 54.4c51.4-.4 85.4-12.6 113.7-40.9c32.3-32.2 31.7-71.8 21.4-95.9c-6.1-14.2-17.1-26-31.9-34.9c-3.7 26.9-11.8 48.3-24.7 64.8c-17.1 21.8-41.4 33.6-72.7 35.3c-23.6 1.3-46.3-4.4-63.9-16c-20.8-13.8-33-34.8-34.3-59.3c-2.5-48.3 35.7-83 95.2-86.4c21.1-1.2 40.9-.3 59.2 2.8c-2.4-14.8-7.3-26.6-14.6-35.2c-10-11.7-25.6-17.7-46.2-17.8H227c-16.6 0-39 4.6-53.3 26.3l-34.4-23.6c19.2-29.1 50.3-45.1 87.8-45.1h.8c62.6 .4 99.9 39.5 103.7 107.7l-.2 .2zm-156 68.8c1.3 25.1 28.4 36.8 54.6 35.3c25.6-1.4 54.6-11.4 59.5-73.2c-13.2-2.9-27.8-4.4-43.4-4.4c-4.8 0-9.6 .1-14.4 .4c-42.9 2.4-57.2 23.2-56.2 41.8l-.1 .1z" fill="currentColor"&gt;
			
			
				&lt;path d="M407.8 294.7c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3zM288 227.1C261.9 176.4 190.9 81.9 124.9 35.3C61.6-9.4 37.5-1.7 21.6 5.5C3.3 13.8 0 41.9 0 58.4S9.1 194 15 213.9c19.5 65.7 89.1 87.9 153.2 80.7c3.3-.5 6.6-.9 10-1.4c-3.3 .5-6.6 1-10 1.4C74.3 308.6-9.1 342.8 100.3 464.5C220.6 589.1 265.1 437.8 288 361.1c22.9 76.7 49.2 222.5 185.6 103.4c102.4-103.4 28.1-156-65.8-169.9c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3c64.1 7.1 133.6-15.1 153.2-80.7C566.9 194 576 75 576 58.4s-3.3-44.7-21.6-52.9c-15.8-7.1-40-14.9-103.2 29.8C385.1 81.9 314.1 176.4 288 227.1z" fill="currentColor"&gt;
			
			
				&lt;path d="M204 6.5C101.4 6.5 0 74.9 0 185.6 0 256 39.6 296 63.6 296c9.9 0 15.6-27.6 15.6-35.4 0-9.3-23.7-29.1-23.7-67.8 0-80.4 61.2-137.4 140.4-137.4 68.1 0 118.5 38.7 118.5 109.8 0 53.1-21.3 152.7-90.3 152.7-24.9 0-46.2-18-46.2-43.8 0-37.8 26.4-74.4 26.4-113.4 0-66.2-93.9-54.2-93.9 25.8 0 16.8 2.1 35.4 9.6 50.7-13.8 59.4-42 147.9-42 209.1 0 18.9 2.7 37.5 4.5 56.4 3.4 3.8 1.7 3.4 6.9 1.5 50.4-69 48.6-82.5 71.4-172.8 12.3 23.4 44.1 36 69.3 36 106.2 0 153.9-103.5 153.9-196.8C384 71.3 298.2 6.5 204 6.5z" fill="currentColor"&gt;
			
		&lt;/svg&gt;
		&lt;div id="has-mastodon-prompt"&gt;
			&lt;h3&gt;Share on Mastodon&lt;/h3&gt;
			
		&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/ai-factory-inference-optimization/</guid><pubDate>Fri, 30 May 2025 15:00:07 +0000</pubDate></item><item><title>3 Questions: How to help students recognize potential bias in their AI datasets (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/3-questions-recognizing-potential-bias-in-ai-datasets-0602</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202506/MIT_AI-Health-Data-01.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;&lt;em&gt;Every year, thousands of students take courses that teach them how to deploy artificial intelligence models that can help doctors diagnose disease and determine appropriate treatments. However, many of these courses omit a key element: training students to detect flaws in the training data used to develop the models.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Leo Anthony Celi, a senior research scientist at MIT’s Institute for Medical Engineering and Science, a physician at Beth Israel Deaconess Medical Center, and an associate professor at Harvard Medical School, has documented these shortcomings in a &lt;/em&gt;&lt;em&gt;new paper&lt;/em&gt;&lt;em&gt; and hopes to persuade course developers to teach students to more thoroughly evaluate their data before incorporating it into their models. Many previous studies have found that models trained mostly on clinical data from white males don’t work well when applied to people from other groups. Here, Celi describes the impact of such bias and how educators might address it in their teachings about AI models.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; How does bias get into these datasets, and how can these shortcomings be addressed?&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Any problems in the data will be baked into any modeling of the data. In the past we have described instruments and devices that don’t work well across individuals. As one example, we found that&amp;nbsp;pulse oximeters overestimate oxygen levels for people of color, because there weren’t enough people of color enrolled in the clinical trials of the devices. We remind our students that medical devices and equipment are optimized on healthy young males. They were never optimized for an 80-year-old woman with heart failure, and yet we use them for those purposes. And the FDA does not require that a device work well on this diverse of a population that we will be using it on. All they need is proof that it works on healthy subjects.&lt;/p&gt;&lt;p&gt;Additionally, the electronic health record system is in no shape to be used as the building blocks of AI. Those records were not designed to be a learning system, and for that reason, you have to be really careful about using electronic health records. The electronic health record system is to be replaced, but that’s not going to happen anytime soon, so we need to be smarter. We need to be more creative about using the data that we have now, no matter how bad they are, in building algorithms.&lt;/p&gt;&lt;p&gt;One promising avenue that we are exploring is the development of a&amp;nbsp;transformer model of numeric electronic health record data, including but not limited to laboratory test results. Modeling the underlying relationship between the laboratory tests, the vital signs and the treatments can mitigate the effect of missing data as a result of social determinants of health and provider implicit biases.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Why is it important for courses in AI to cover the sources of potential bias? What did you find when you analyzed such courses’ content?&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt;&amp;nbsp;Our course at MIT started in 2016, and at some point we realized that we were encouraging people to race to build models that are overfitted to some statistical measure of model performance, when in fact the data that we’re using is rife with problems that people are not aware of. At that time, we were wondering: How common is this problem?&lt;/p&gt;&lt;p&gt;Our suspicion was that if you looked at the courses where the syllabus is available online, or the online courses, that none of them even bothers to tell the students that they should be paranoid about the data. And true enough, when we looked at the different online courses, it’s all about building the model. How do you build the model? How do you visualize the data? We found that of 11 courses we reviewed, only five included sections on bias in datasets, and only two contained any significant discussion of bias.&lt;/p&gt;&lt;p&gt;That said, we cannot discount the value of these courses. I’ve heard lots of stories where people self-study based on these online courses, but at the same time, given how influential they are, how impactful they are, we need to really double down on requiring them to teach the right skillsets, as more and more people are drawn to this AI multiverse. It’s important for people to really equip themselves with the agency to be able to work with AI. We’re hoping that this paper will shine a spotlight on this huge gap in the way we teach AI now to our students.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; What kind of content should course developers be incorporating?&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; One, giving them a checklist of questions in the beginning. Where did this data came from? Who were the observers? Who were the doctors and nurses who collected the data? And then learn a little bit about the landscape of those institutions. If it’s an ICU database, they need to ask who makes it to the ICU, and who doesn’t make it to the ICU, because that already introduces a sampling selection bias. If all the minority patients don’t even get admitted to the ICU because they cannot reach the ICU in time, then the models are not going to work for them. Truly, to me, 50 percent of the course content should really be understanding the data, if not more, because the modeling itself is easy once you understand the data.&lt;/p&gt;&lt;p&gt;Since 2014, the MIT Critical Data consortium has been organizing datathons (data “hackathons”) around the world. At these gatherings, doctors, nurses, other health care workers, and data scientists get together to comb through databases and try to examine health and disease in the local context. Textbooks and journal papers present diseases based on observations and trials involving a narrow demographic typically from countries with resources for research.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Our main objective now, what we want to teach them, is critical thinking skills. And the main ingredient for critical thinking is bringing together people with different backgrounds.&lt;/p&gt;&lt;p&gt;You cannot teach critical thinking in a room full of CEOs or in a room full of doctors. The environment is just not there. When we have datathons, we don’t even have to teach them how do you do critical thinking. As soon as you bring the right mix of people — and it’s not just coming from different backgrounds but from different generations — you don’t even have to tell them how to think critically. It just happens. The environment is right for that kind of thinking. So, we now tell our participants and our students, please, please do not start building any model unless you truly understand how the data came about, which patients made it into the database, what devices were used to measure, and are those devices consistently accurate across individuals?&lt;/p&gt;&lt;p&gt;When we have events around the world, we encourage them to look for data sets that are local, so that they are relevant. There’s resistance because they know that they will discover how bad their data sets are. We say that that’s fine. This is how you fix that. If you don’t know how bad they are, you’re going to continue collecting them in a very bad manner and they’re useless. You have to acknowledge that you’re not going to get it right the first time, and that’s perfectly fine. MIMIC (the Medical Information Marked for Intensive Care database built at Beth Israel Deaconess Medical Center) took a decade before we had a decent schema, and we only have a decent schema because people were telling us how bad MIMIC was.&lt;/p&gt;&lt;p&gt;We may not have the answers to all of these questions, but we can evoke something in people that helps them realize that there are so many problems in the data.&amp;nbsp;I’m always thrilled to look at the blog posts from people who attended a datathon, who say that their world has changed. Now they’re more excited about the field because they realize the immense potential, but also the immense risk of harm if they don’t do this correctly.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202506/MIT_AI-Health-Data-01.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;&lt;em&gt;Every year, thousands of students take courses that teach them how to deploy artificial intelligence models that can help doctors diagnose disease and determine appropriate treatments. However, many of these courses omit a key element: training students to detect flaws in the training data used to develop the models.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Leo Anthony Celi, a senior research scientist at MIT’s Institute for Medical Engineering and Science, a physician at Beth Israel Deaconess Medical Center, and an associate professor at Harvard Medical School, has documented these shortcomings in a &lt;/em&gt;&lt;em&gt;new paper&lt;/em&gt;&lt;em&gt; and hopes to persuade course developers to teach students to more thoroughly evaluate their data before incorporating it into their models. Many previous studies have found that models trained mostly on clinical data from white males don’t work well when applied to people from other groups. Here, Celi describes the impact of such bias and how educators might address it in their teachings about AI models.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; How does bias get into these datasets, and how can these shortcomings be addressed?&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Any problems in the data will be baked into any modeling of the data. In the past we have described instruments and devices that don’t work well across individuals. As one example, we found that&amp;nbsp;pulse oximeters overestimate oxygen levels for people of color, because there weren’t enough people of color enrolled in the clinical trials of the devices. We remind our students that medical devices and equipment are optimized on healthy young males. They were never optimized for an 80-year-old woman with heart failure, and yet we use them for those purposes. And the FDA does not require that a device work well on this diverse of a population that we will be using it on. All they need is proof that it works on healthy subjects.&lt;/p&gt;&lt;p&gt;Additionally, the electronic health record system is in no shape to be used as the building blocks of AI. Those records were not designed to be a learning system, and for that reason, you have to be really careful about using electronic health records. The electronic health record system is to be replaced, but that’s not going to happen anytime soon, so we need to be smarter. We need to be more creative about using the data that we have now, no matter how bad they are, in building algorithms.&lt;/p&gt;&lt;p&gt;One promising avenue that we are exploring is the development of a&amp;nbsp;transformer model of numeric electronic health record data, including but not limited to laboratory test results. Modeling the underlying relationship between the laboratory tests, the vital signs and the treatments can mitigate the effect of missing data as a result of social determinants of health and provider implicit biases.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Why is it important for courses in AI to cover the sources of potential bias? What did you find when you analyzed such courses’ content?&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt;&amp;nbsp;Our course at MIT started in 2016, and at some point we realized that we were encouraging people to race to build models that are overfitted to some statistical measure of model performance, when in fact the data that we’re using is rife with problems that people are not aware of. At that time, we were wondering: How common is this problem?&lt;/p&gt;&lt;p&gt;Our suspicion was that if you looked at the courses where the syllabus is available online, or the online courses, that none of them even bothers to tell the students that they should be paranoid about the data. And true enough, when we looked at the different online courses, it’s all about building the model. How do you build the model? How do you visualize the data? We found that of 11 courses we reviewed, only five included sections on bias in datasets, and only two contained any significant discussion of bias.&lt;/p&gt;&lt;p&gt;That said, we cannot discount the value of these courses. I’ve heard lots of stories where people self-study based on these online courses, but at the same time, given how influential they are, how impactful they are, we need to really double down on requiring them to teach the right skillsets, as more and more people are drawn to this AI multiverse. It’s important for people to really equip themselves with the agency to be able to work with AI. We’re hoping that this paper will shine a spotlight on this huge gap in the way we teach AI now to our students.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; What kind of content should course developers be incorporating?&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; One, giving them a checklist of questions in the beginning. Where did this data came from? Who were the observers? Who were the doctors and nurses who collected the data? And then learn a little bit about the landscape of those institutions. If it’s an ICU database, they need to ask who makes it to the ICU, and who doesn’t make it to the ICU, because that already introduces a sampling selection bias. If all the minority patients don’t even get admitted to the ICU because they cannot reach the ICU in time, then the models are not going to work for them. Truly, to me, 50 percent of the course content should really be understanding the data, if not more, because the modeling itself is easy once you understand the data.&lt;/p&gt;&lt;p&gt;Since 2014, the MIT Critical Data consortium has been organizing datathons (data “hackathons”) around the world. At these gatherings, doctors, nurses, other health care workers, and data scientists get together to comb through databases and try to examine health and disease in the local context. Textbooks and journal papers present diseases based on observations and trials involving a narrow demographic typically from countries with resources for research.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Our main objective now, what we want to teach them, is critical thinking skills. And the main ingredient for critical thinking is bringing together people with different backgrounds.&lt;/p&gt;&lt;p&gt;You cannot teach critical thinking in a room full of CEOs or in a room full of doctors. The environment is just not there. When we have datathons, we don’t even have to teach them how do you do critical thinking. As soon as you bring the right mix of people — and it’s not just coming from different backgrounds but from different generations — you don’t even have to tell them how to think critically. It just happens. The environment is right for that kind of thinking. So, we now tell our participants and our students, please, please do not start building any model unless you truly understand how the data came about, which patients made it into the database, what devices were used to measure, and are those devices consistently accurate across individuals?&lt;/p&gt;&lt;p&gt;When we have events around the world, we encourage them to look for data sets that are local, so that they are relevant. There’s resistance because they know that they will discover how bad their data sets are. We say that that’s fine. This is how you fix that. If you don’t know how bad they are, you’re going to continue collecting them in a very bad manner and they’re useless. You have to acknowledge that you’re not going to get it right the first time, and that’s perfectly fine. MIMIC (the Medical Information Marked for Intensive Care database built at Beth Israel Deaconess Medical Center) took a decade before we had a decent schema, and we only have a decent schema because people were telling us how bad MIMIC was.&lt;/p&gt;&lt;p&gt;We may not have the answers to all of these questions, but we can evoke something in people that helps them realize that there are so many problems in the data.&amp;nbsp;I’m always thrilled to look at the blog posts from people who attended a datathon, who say that their world has changed. Now they’re more excited about the field because they realize the immense potential, but also the immense risk of harm if they don’t do this correctly.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/3-questions-recognizing-potential-bias-in-ai-datasets-0602</guid><pubDate>Mon, 02 Jun 2025 14:30:00 +0000</pubDate></item><item><title>Researchers and Students in Türkiye Build AI, Robotics Tools to Boost Disaster Readiness (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/turkiye-disaster-readiness-ai-robotics/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Since a 7.8-magnitude earthquake hit Syria and Türkiye two years ago — leaving 55,000 people dead, 130,000 injured and millions displaced from their homes — students, researchers and developers have been harnessing the latest AI robotics technologies to increase disaster preparedness in the region.&lt;/p&gt;
&lt;p&gt;The work is part of a Disaster Response Innovation and Education Grant provided by NVIDIA in collaboration with Bridge to Türkiye Fund, a nonprofit supporting underserved communities in Türkiye, with a focus on education and sustainability.&lt;/p&gt;
&lt;p&gt;The fruits of the grant — which provided 100 free NVIDIA Jetson Nano Developer Kits and $50,000 in funding divided among eight awardees — are now being realized through projects on AI-powered inspection, search and rescue, robotics education and more.&lt;/p&gt;
&lt;p&gt;Recipients of the grant have, for example, trained robots on key skills needed in search-and-rescue operations, built a tool to test water and food sources for pathogen contamination in disaster-stricken areas, and launched a hands-on programming course at a Turkish institute of technology.&lt;/p&gt;
&lt;p&gt;The grant’s impact is in addition to the more than $1.9 million in employee donations and company matching provided by NVIDIANs around the globe to support victims of the devastating earthquakes.&lt;/p&gt;
&lt;p&gt;“After the earthquake, we didn’t want to be bystanders,” said Harun Bayraktar, senior director of libraries engineering at NVIDIA. “We wanted to invest our time and efforts to make a difference and save lives next time.”&lt;/p&gt;
&lt;p&gt;Bayraktar and Berra Kara, a senior GPU power architect at NVIDIA — both of whom grew up in Türkiye — volunteered to lead the grant program team, aiming to raise awareness for disaster response in the country, increase AI and robotics expertise, and help minimize the casualties of any future earthquakes.&lt;/p&gt;
&lt;p&gt;Read more about the rippling impacts of NVIDIA and Bridge to Türkiye’s grant program:&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Researchers Build Unmanned Ground Vehicle for Search and Rescue&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;At Ankara University, in the country’s capital, researchers used the grant to build a modular unmanned ground vehicle (UGV) that can support search-and-rescue operations in post-earthquake scenarios.&lt;/p&gt;

&lt;p&gt;Equipped with a thermal camera, an RGB-D camera and an NVIDIA Jetson Nano Developer Kit, the small, durable UGV scans environments in 3D and detects thermal activity, so users can determine the presence of a human in the aftermath of a disaster while maintaining a safe distance from dangerous areas.&lt;/p&gt;
&lt;p&gt;“Our autonomous UGV system used the NVIDIA Jetson Nano’s onboard AI computing power to perform real-time, thermal vision-based victim detection in post-disaster search-and-rescue scenarios,” said Mehmet Cem Çatalbaş, associate professor in the software engineering department at Ankara University. “NVIDIA’s earthquake relief program significantly accelerated our research and development process, transforming an innovative concept into an effective, life-saving solution.”&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;University Students Train Robots to Navigate Post-Disaster Environments&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="alt" class="wp-image-81248 size-nvb4-box-widget alignright" height="350" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/slam-duckiebots-406x350.jpg" width="406" /&gt;&lt;/p&gt;
&lt;p&gt;Simultaneous localization and mapping (SLAM), a commonly used method to help robots map areas and find their way around unknown environments, is a critical skill for robots that could be used in search-and-rescue missions.&lt;/p&gt;
&lt;p&gt;To equip students with SLAM and other robotics skills, the computer engineering department at Hacettepe University — a world-class research university in Ankara — integrated NVIDIA Jetson Nano Developer Kit-based projects into two courses. More than a dozen students used the embedded AI developer kits to build small mobile robots, dubbed “Duckiebots,” with SLAM capabilities.&lt;/p&gt;
&lt;p&gt;Using SLAM, sensor integration and autonomous-navigation features, AI-powered robots like these could enter various areas — such as collapsed buildings or fires — to help find and rescue people.&lt;/p&gt;
&lt;p&gt;Through these courses, Hacettepe University students simulated potential planned paths for robots, as well as assembly and initial operation of the Duckiebots.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Researchers Enable Fast Pathogen Screening in Disaster-Stricken Areas&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;In the aftermath of earthquakes, floods, wildfires and other natural disasters, a lack of sanitation and access to clean water can often lead to disease outbreaks. It’s important to test water and food sources for pathogen contamination and quickly identify the types of any existing pathogens to prevent their spread.&lt;/p&gt;
&lt;p&gt;Researchers at Bilkent University, a nonprofit research university in Ankara, built a mini supercomputer cluster — based on the NVIDIA Jetson Nano Developer Kits — that promptly carries out computational tasks related to metagenomic analysis, or the analysis of DNA from a sample of an environment.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="size-medium wp-image-81252 aligncenter" height="720" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/pathogen-screening-minisupercomputer-960x720.jpg" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;The portability of the NVIDIA Jetson devices means the cluster can be easily transported to disaster-stricken areas to identify pathogens directly on site — rather than needing to send samples to a wet lab — helping to efficiently, speedily prevent disease spread.&lt;/p&gt;
&lt;p&gt;The research team used the open-source CuCLARK library for metagenomic classification using NVIDIA CUDA-enabled GPUs, which resulted in fast, accurate DNA screening.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;University Students Learn the Fundamentals of AI and Embedded Systems&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="alt" class="alignleft wp-image-81255 size-nvb4-box-widget" height="350" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/heterogeneous-parallel-programming-406x350.jpg" width="406" /&gt;At the Izmir Institute of Technology — a research university that places a strong emphasis on the natural sciences and engineering — the computer engineering department tapped into the NVIDIA Jetson Nano devices, CUDA and NVIDIA Deep Learning Institute teaching kits to equip nearly 80 undergraduates with the fundamentals of AI, accelerated computing and robotics.&lt;/p&gt;
&lt;p&gt;“Using the Jetson Nano Developer Kits provided by the NVIDIA and Bridge to Türkiye grant, we expanded our heterogeneous parallel programming course to include a hands-on deep learning project for computer science undergraduates,” said Işıl Öz, assistant professor of computer engineering at the university. “Such hands-on experience makes learning more engaging and effective for the next generation of innovators who will help build life-saving, sustainable technologies.”&lt;/p&gt;
&lt;p&gt;Based on this work, a paper titled, “Teaching Accelerated Computing With Hands-on Experience,” will be presented by Öz at this month’s IEEE International Parallel and Distributed Processing Symposium. The paper outlines the challenges and successes that come with teaching heterogeneous parallel programming — a type of computing that uses more than one kind of processor or core to increase performance and energy efficiency.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn about the &lt;/i&gt;&lt;i&gt;NVIDIA Academic Grant Program&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Since a 7.8-magnitude earthquake hit Syria and Türkiye two years ago — leaving 55,000 people dead, 130,000 injured and millions displaced from their homes — students, researchers and developers have been harnessing the latest AI robotics technologies to increase disaster preparedness in the region.&lt;/p&gt;
&lt;p&gt;The work is part of a Disaster Response Innovation and Education Grant provided by NVIDIA in collaboration with Bridge to Türkiye Fund, a nonprofit supporting underserved communities in Türkiye, with a focus on education and sustainability.&lt;/p&gt;
&lt;p&gt;The fruits of the grant — which provided 100 free NVIDIA Jetson Nano Developer Kits and $50,000 in funding divided among eight awardees — are now being realized through projects on AI-powered inspection, search and rescue, robotics education and more.&lt;/p&gt;
&lt;p&gt;Recipients of the grant have, for example, trained robots on key skills needed in search-and-rescue operations, built a tool to test water and food sources for pathogen contamination in disaster-stricken areas, and launched a hands-on programming course at a Turkish institute of technology.&lt;/p&gt;
&lt;p&gt;The grant’s impact is in addition to the more than $1.9 million in employee donations and company matching provided by NVIDIANs around the globe to support victims of the devastating earthquakes.&lt;/p&gt;
&lt;p&gt;“After the earthquake, we didn’t want to be bystanders,” said Harun Bayraktar, senior director of libraries engineering at NVIDIA. “We wanted to invest our time and efforts to make a difference and save lives next time.”&lt;/p&gt;
&lt;p&gt;Bayraktar and Berra Kara, a senior GPU power architect at NVIDIA — both of whom grew up in Türkiye — volunteered to lead the grant program team, aiming to raise awareness for disaster response in the country, increase AI and robotics expertise, and help minimize the casualties of any future earthquakes.&lt;/p&gt;
&lt;p&gt;Read more about the rippling impacts of NVIDIA and Bridge to Türkiye’s grant program:&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Researchers Build Unmanned Ground Vehicle for Search and Rescue&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;At Ankara University, in the country’s capital, researchers used the grant to build a modular unmanned ground vehicle (UGV) that can support search-and-rescue operations in post-earthquake scenarios.&lt;/p&gt;

&lt;p&gt;Equipped with a thermal camera, an RGB-D camera and an NVIDIA Jetson Nano Developer Kit, the small, durable UGV scans environments in 3D and detects thermal activity, so users can determine the presence of a human in the aftermath of a disaster while maintaining a safe distance from dangerous areas.&lt;/p&gt;
&lt;p&gt;“Our autonomous UGV system used the NVIDIA Jetson Nano’s onboard AI computing power to perform real-time, thermal vision-based victim detection in post-disaster search-and-rescue scenarios,” said Mehmet Cem Çatalbaş, associate professor in the software engineering department at Ankara University. “NVIDIA’s earthquake relief program significantly accelerated our research and development process, transforming an innovative concept into an effective, life-saving solution.”&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;University Students Train Robots to Navigate Post-Disaster Environments&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="alt" class="wp-image-81248 size-nvb4-box-widget alignright" height="350" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/slam-duckiebots-406x350.jpg" width="406" /&gt;&lt;/p&gt;
&lt;p&gt;Simultaneous localization and mapping (SLAM), a commonly used method to help robots map areas and find their way around unknown environments, is a critical skill for robots that could be used in search-and-rescue missions.&lt;/p&gt;
&lt;p&gt;To equip students with SLAM and other robotics skills, the computer engineering department at Hacettepe University — a world-class research university in Ankara — integrated NVIDIA Jetson Nano Developer Kit-based projects into two courses. More than a dozen students used the embedded AI developer kits to build small mobile robots, dubbed “Duckiebots,” with SLAM capabilities.&lt;/p&gt;
&lt;p&gt;Using SLAM, sensor integration and autonomous-navigation features, AI-powered robots like these could enter various areas — such as collapsed buildings or fires — to help find and rescue people.&lt;/p&gt;
&lt;p&gt;Through these courses, Hacettepe University students simulated potential planned paths for robots, as well as assembly and initial operation of the Duckiebots.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Researchers Enable Fast Pathogen Screening in Disaster-Stricken Areas&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;In the aftermath of earthquakes, floods, wildfires and other natural disasters, a lack of sanitation and access to clean water can often lead to disease outbreaks. It’s important to test water and food sources for pathogen contamination and quickly identify the types of any existing pathogens to prevent their spread.&lt;/p&gt;
&lt;p&gt;Researchers at Bilkent University, a nonprofit research university in Ankara, built a mini supercomputer cluster — based on the NVIDIA Jetson Nano Developer Kits — that promptly carries out computational tasks related to metagenomic analysis, or the analysis of DNA from a sample of an environment.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="size-medium wp-image-81252 aligncenter" height="720" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/pathogen-screening-minisupercomputer-960x720.jpg" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;The portability of the NVIDIA Jetson devices means the cluster can be easily transported to disaster-stricken areas to identify pathogens directly on site — rather than needing to send samples to a wet lab — helping to efficiently, speedily prevent disease spread.&lt;/p&gt;
&lt;p&gt;The research team used the open-source CuCLARK library for metagenomic classification using NVIDIA CUDA-enabled GPUs, which resulted in fast, accurate DNA screening.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;University Students Learn the Fundamentals of AI and Embedded Systems&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="alt" class="alignleft wp-image-81255 size-nvb4-box-widget" height="350" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/heterogeneous-parallel-programming-406x350.jpg" width="406" /&gt;At the Izmir Institute of Technology — a research university that places a strong emphasis on the natural sciences and engineering — the computer engineering department tapped into the NVIDIA Jetson Nano devices, CUDA and NVIDIA Deep Learning Institute teaching kits to equip nearly 80 undergraduates with the fundamentals of AI, accelerated computing and robotics.&lt;/p&gt;
&lt;p&gt;“Using the Jetson Nano Developer Kits provided by the NVIDIA and Bridge to Türkiye grant, we expanded our heterogeneous parallel programming course to include a hands-on deep learning project for computer science undergraduates,” said Işıl Öz, assistant professor of computer engineering at the university. “Such hands-on experience makes learning more engaging and effective for the next generation of innovators who will help build life-saving, sustainable technologies.”&lt;/p&gt;
&lt;p&gt;Based on this work, a paper titled, “Teaching Accelerated Computing With Hands-on Experience,” will be presented by Öz at this month’s IEEE International Parallel and Distributed Processing Symposium. The paper outlines the challenges and successes that come with teaching heterogeneous parallel programming — a type of computing that uses more than one kind of processor or core to increase performance and energy efficiency.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn about the &lt;/i&gt;&lt;i&gt;NVIDIA Academic Grant Program&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/turkiye-disaster-readiness-ai-robotics/</guid><pubDate>Mon, 02 Jun 2025 15:00:11 +0000</pubDate></item><item><title>Teaching AI models the broad strokes to sketch more like humans do (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/teaching-ai-models-to-sketch-more-like-humans-0602</link><description>&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;When you’re trying to communicate or understand ideas, words don’t always do the trick. Sometimes the more efficient approach is to do a simple sketch of that concept — for example, diagramming a circuit might help make sense of how the system works.&lt;/p&gt;&lt;p&gt;But what if artificial intelligence could help us explore these visualizations? While these systems are typically proficient at creating realistic paintings and cartoonish drawings, many models fail to capture the essence of sketching: its stroke-by-stroke, iterative process, which helps humans brainstorm and edit how they want to represent their ideas.&lt;/p&gt;&lt;p&gt;A new drawing system from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and Stanford University can sketch more like we do. Their method, called “SketchAgent,” uses a multimodal language model — AI systems that train on text and images, like Anthropic’s Claude 3.5 Sonnet — to turn natural language prompts into sketches in a few seconds. For example, it can doodle a house either on its own or through collaboration, drawing with a human or incorporating text-based input to sketch each part separately.&lt;/p&gt;&lt;p&gt;The researchers showed that SketchAgent can create abstract drawings of diverse concepts, like a robot, butterfly, DNA helix, flowchart, and even the Sydney Opera House. One day, the tool could be expanded into an interactive art game that helps teachers and researchers diagram complex concepts or give users a quick drawing lesson.&lt;/p&gt;        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-youtube-video paragraph--view-mode--default"&gt;
          

            
   

  &lt;div class="news-article--inline-video"&gt;
        &lt;div class="news-article--inline-video--container"&gt;

                             &lt;div class="news-article--inline-video--cover-image"&gt;
                      &lt;img alt="Video thumbnail" height="480" src="https://i1.ytimg.com/vi/F8WClut-eec/maxresdefault.jpg" width="640" /&gt;
                    
          Play video
        &lt;/div&gt;
                

                      &lt;/div&gt;
        &lt;div class="news-article--inline-video--caption"&gt;
      

            SketchAgent: a collaborative system that teaches AI models to sketch more like humans do.&lt;br /&gt;Video: MIT CSAIL        

    &lt;/div&gt;
          &lt;/div&gt;  
        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;CSAIL postdoc Yael Vinker, who is the lead author of a&amp;nbsp;paper introducing SketchAgent, notes that the system introduces a more natural way for humans to communicate with AI.&lt;/p&gt;&lt;p&gt;“Not everyone is aware of how much they draw in their daily life. We may draw our thoughts or workshop ideas with sketches,” she says. “Our tool aims to emulate that process, making multimodal language models more useful in helping us visually express ideas.”&lt;/p&gt;&lt;p&gt;SketchAgent teaches these models to draw stroke-by-stroke without training on any data — instead, the researchers developed a “sketching language” in which a sketch is translated into a numbered sequence of strokes on a grid. The system was given an example of how things like a house would be drawn, with each stroke labeled according to what it represented — such as the seventh stroke being a rectangle labeled as a “front door” — to help the model generalize to new concepts.&lt;/p&gt;&lt;p&gt;Vinker wrote the paper alongside three CSAIL affiliates — postdoc Tamar Rott Shaham, undergraduate researcher Alex Zhao, and MIT Professor Antonio Torralba — as well as Stanford University Research Fellow Kristine Zheng and Assistant Professor Judith Ellen Fan. They’ll present their work at the 2025 Conference on Computer Vision and Pattern Recognition (CVPR) this month.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Assessing AI’s sketching abilities&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;While text-to-image models such as DALL-E 3 can create intriguing drawings, they lack a crucial component of sketching: the spontaneous, creative process where each stroke can impact the overall design. On the other hand, SketchAgent’s drawings are modeled as a sequence of strokes, appearing more natural and fluid, like human sketches.&lt;/p&gt;&lt;p&gt;Prior works have mimicked this process, too, but they trained their models on human-drawn datasets, which are often limited in scale and diversity. SketchAgent uses pre-trained language models instead, which are knowledgeable about many concepts, but don’t know how to sketch. When the researchers taught language models this process, SketchAgent began to sketch diverse concepts it hadn’t explicitly trained on.&lt;/p&gt;&lt;p&gt;Still, Vinker and her colleagues wanted to see if SketchAgent was actively working with humans on the sketching process, or if it was working independently of its drawing partner. The team tested their system in collaboration mode, where a human and a language model work toward drawing a particular concept in tandem. Removing SketchAgent’s contributions revealed that their tool’s strokes were essential to the final drawing. In a drawing of a sailboat, for instance, removing the artificial strokes representing a mast made the overall sketch unrecognizable.&lt;/p&gt;&lt;p&gt;In another experiment, CSAIL and Stanford researchers plugged different multimodal language models into SketchAgent to see which could create the most recognizable sketches. Their default backbone model, Claude 3.5 Sonnet, generated the most human-like vector graphics (essentially text-based files that can be converted into high-resolution images). It outperformed models like GPT-4o and Claude 3 Opus.&lt;/p&gt;&lt;p&gt;“The fact that Claude 3.5 Sonnet outperformed other models like GPT-4o and Claude 3 Opus suggests that this model processes and generates visual-related information differently,” says co-author Tamar Rott Shaham.&lt;/p&gt;&lt;p&gt;She adds that SketchAgent could become a helpful interface for collaborating with AI models beyond standard, text-based communication. “As models advance in understanding and generating other modalities, like sketches, they open up new ways for users to express ideas and receive responses that feel more intuitive and human-like,” says Rott Shaham. “This could significantly enrich interactions, making AI more accessible and versatile.”&lt;/p&gt;&lt;p&gt;While SketchAgent’s drawing prowess is promising, it can’t make professional sketches yet. It renders simple representations of concepts using stick figures and doodles, but struggles to doodle things like logos, sentences, complex creatures like unicorns and cows, and specific human figures.&lt;/p&gt;&lt;p&gt;At times, their model also misunderstood users’ intentions in collaborative drawings, like when SketchAgent drew a bunny with two heads. According to Vinker, this may be because the model breaks down each task into smaller steps (also called “Chain of Thought” reasoning). When working with humans, the model creates a drawing plan, potentially misinterpreting which part of that outline a human is contributing to. The researchers could possibly refine these drawing skills by training on synthetic data from diffusion models.&lt;/p&gt;&lt;p&gt;Additionally, SketchAgent often requires a few rounds of prompting to generate human-like doodles. In the future, the team aims to make it easier to interact and sketch with multimodal language models, including refining their interface.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Still, the tool suggests AI could draw diverse concepts the way humans do, with step-by-step human-AI collaboration that results in more aligned final designs.&lt;/p&gt;&lt;p&gt;This work was supported, in part, by the U.S. National Science Foundation, a Hoffman-Yee Grant from the Stanford Institute for Human-Centered AI, the Hyundai Motor Co., the U.S. Army Research Laboratory, the Zuckerman STEM Leadership Program, and a Viterbi Fellowship.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;When you’re trying to communicate or understand ideas, words don’t always do the trick. Sometimes the more efficient approach is to do a simple sketch of that concept — for example, diagramming a circuit might help make sense of how the system works.&lt;/p&gt;&lt;p&gt;But what if artificial intelligence could help us explore these visualizations? While these systems are typically proficient at creating realistic paintings and cartoonish drawings, many models fail to capture the essence of sketching: its stroke-by-stroke, iterative process, which helps humans brainstorm and edit how they want to represent their ideas.&lt;/p&gt;&lt;p&gt;A new drawing system from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and Stanford University can sketch more like we do. Their method, called “SketchAgent,” uses a multimodal language model — AI systems that train on text and images, like Anthropic’s Claude 3.5 Sonnet — to turn natural language prompts into sketches in a few seconds. For example, it can doodle a house either on its own or through collaboration, drawing with a human or incorporating text-based input to sketch each part separately.&lt;/p&gt;&lt;p&gt;The researchers showed that SketchAgent can create abstract drawings of diverse concepts, like a robot, butterfly, DNA helix, flowchart, and even the Sydney Opera House. One day, the tool could be expanded into an interactive art game that helps teachers and researchers diagram complex concepts or give users a quick drawing lesson.&lt;/p&gt;        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-youtube-video paragraph--view-mode--default"&gt;
          

            
   

  &lt;div class="news-article--inline-video"&gt;
        &lt;div class="news-article--inline-video--container"&gt;

                             &lt;div class="news-article--inline-video--cover-image"&gt;
                      &lt;img alt="Video thumbnail" height="480" src="https://i1.ytimg.com/vi/F8WClut-eec/maxresdefault.jpg" width="640" /&gt;
                    
          Play video
        &lt;/div&gt;
                

                      &lt;/div&gt;
        &lt;div class="news-article--inline-video--caption"&gt;
      

            SketchAgent: a collaborative system that teaches AI models to sketch more like humans do.&lt;br /&gt;Video: MIT CSAIL        

    &lt;/div&gt;
          &lt;/div&gt;  
        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;CSAIL postdoc Yael Vinker, who is the lead author of a&amp;nbsp;paper introducing SketchAgent, notes that the system introduces a more natural way for humans to communicate with AI.&lt;/p&gt;&lt;p&gt;“Not everyone is aware of how much they draw in their daily life. We may draw our thoughts or workshop ideas with sketches,” she says. “Our tool aims to emulate that process, making multimodal language models more useful in helping us visually express ideas.”&lt;/p&gt;&lt;p&gt;SketchAgent teaches these models to draw stroke-by-stroke without training on any data — instead, the researchers developed a “sketching language” in which a sketch is translated into a numbered sequence of strokes on a grid. The system was given an example of how things like a house would be drawn, with each stroke labeled according to what it represented — such as the seventh stroke being a rectangle labeled as a “front door” — to help the model generalize to new concepts.&lt;/p&gt;&lt;p&gt;Vinker wrote the paper alongside three CSAIL affiliates — postdoc Tamar Rott Shaham, undergraduate researcher Alex Zhao, and MIT Professor Antonio Torralba — as well as Stanford University Research Fellow Kristine Zheng and Assistant Professor Judith Ellen Fan. They’ll present their work at the 2025 Conference on Computer Vision and Pattern Recognition (CVPR) this month.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Assessing AI’s sketching abilities&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;While text-to-image models such as DALL-E 3 can create intriguing drawings, they lack a crucial component of sketching: the spontaneous, creative process where each stroke can impact the overall design. On the other hand, SketchAgent’s drawings are modeled as a sequence of strokes, appearing more natural and fluid, like human sketches.&lt;/p&gt;&lt;p&gt;Prior works have mimicked this process, too, but they trained their models on human-drawn datasets, which are often limited in scale and diversity. SketchAgent uses pre-trained language models instead, which are knowledgeable about many concepts, but don’t know how to sketch. When the researchers taught language models this process, SketchAgent began to sketch diverse concepts it hadn’t explicitly trained on.&lt;/p&gt;&lt;p&gt;Still, Vinker and her colleagues wanted to see if SketchAgent was actively working with humans on the sketching process, or if it was working independently of its drawing partner. The team tested their system in collaboration mode, where a human and a language model work toward drawing a particular concept in tandem. Removing SketchAgent’s contributions revealed that their tool’s strokes were essential to the final drawing. In a drawing of a sailboat, for instance, removing the artificial strokes representing a mast made the overall sketch unrecognizable.&lt;/p&gt;&lt;p&gt;In another experiment, CSAIL and Stanford researchers plugged different multimodal language models into SketchAgent to see which could create the most recognizable sketches. Their default backbone model, Claude 3.5 Sonnet, generated the most human-like vector graphics (essentially text-based files that can be converted into high-resolution images). It outperformed models like GPT-4o and Claude 3 Opus.&lt;/p&gt;&lt;p&gt;“The fact that Claude 3.5 Sonnet outperformed other models like GPT-4o and Claude 3 Opus suggests that this model processes and generates visual-related information differently,” says co-author Tamar Rott Shaham.&lt;/p&gt;&lt;p&gt;She adds that SketchAgent could become a helpful interface for collaborating with AI models beyond standard, text-based communication. “As models advance in understanding and generating other modalities, like sketches, they open up new ways for users to express ideas and receive responses that feel more intuitive and human-like,” says Rott Shaham. “This could significantly enrich interactions, making AI more accessible and versatile.”&lt;/p&gt;&lt;p&gt;While SketchAgent’s drawing prowess is promising, it can’t make professional sketches yet. It renders simple representations of concepts using stick figures and doodles, but struggles to doodle things like logos, sentences, complex creatures like unicorns and cows, and specific human figures.&lt;/p&gt;&lt;p&gt;At times, their model also misunderstood users’ intentions in collaborative drawings, like when SketchAgent drew a bunny with two heads. According to Vinker, this may be because the model breaks down each task into smaller steps (also called “Chain of Thought” reasoning). When working with humans, the model creates a drawing plan, potentially misinterpreting which part of that outline a human is contributing to. The researchers could possibly refine these drawing skills by training on synthetic data from diffusion models.&lt;/p&gt;&lt;p&gt;Additionally, SketchAgent often requires a few rounds of prompting to generate human-like doodles. In the future, the team aims to make it easier to interact and sketch with multimodal language models, including refining their interface.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Still, the tool suggests AI could draw diverse concepts the way humans do, with step-by-step human-AI collaboration that results in more aligned final designs.&lt;/p&gt;&lt;p&gt;This work was supported, in part, by the U.S. National Science Foundation, a Hoffman-Yee Grant from the Stanford Institute for Human-Centered AI, the Hyundai Motor Co., the U.S. Army Research Laboratory, the Zuckerman STEM Leadership Program, and a Viterbi Fellowship.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/teaching-ai-models-to-sketch-more-like-humans-0602</guid><pubDate>Mon, 02 Jun 2025 18:50:00 +0000</pubDate></item><item><title>AI stirs up the recipe for concrete in MIT study (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/ai-stirs-recipe-for-concrete-0602</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202505/mit-Soroush-Mahjoubi.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;For weeks, the whiteboard in the lab was crowded with scribbles, diagrams, and chemical formulas. A research team across the Olivetti Group and the MIT Concrete Sustainability Hub (CSHub) was working intensely on a key problem: How can we reduce the amount of cement in concrete to save on costs and emissions?&amp;nbsp;&lt;/p&gt;&lt;p&gt;The question was certainly not new; materials like fly ash, a byproduct of coal production, and slag, a byproduct of steelmaking, have long been used to replace some of the cement in concrete mixes. However, the demand for these products is outpacing supply as industry looks to reduce its climate impacts by expanding their use, making the search for alternatives urgent. The challenge that the team discovered wasn’t a lack of candidates; the problem was that there were too many to sort through.&lt;/p&gt;&lt;p&gt;On May 17, the team, led by postdoc Soroush Mahjoubi, published an open-access paper in Nature’s&lt;em&gt; Communications Materials&lt;/em&gt; outlining their solution. “We realized that AI was the key to moving forward,” notes Mahjoubi. “There is so much data out there on potential materials — hundreds of thousands of pages of scientific literature. Sorting through them would have taken many lifetimes of work, by which time more materials would have been discovered!”&lt;/p&gt;&lt;p&gt;With large language models, like the chatbots many of us use daily, the team built a machine-learning framework that evaluates and sorts candidate materials based on their physical and chemical properties.&amp;nbsp;&lt;/p&gt;&lt;p&gt;“First, there is hydraulic reactivity. The reason that concrete is strong is that cement — the ‘glue’ that holds it together — hardens when exposed to water. So, if we replace this glue, we need to make sure the substitute reacts similarly,” explains Mahjoubi. “Second, there is pozzolanicity. This is when a material reacts with calcium hydroxide, a byproduct created when cement meets water, to make the concrete harder and stronger over time.&amp;nbsp; We need to balance the hydraulic and pozzolanic materials in the mix so the concrete performs at its best.”&lt;/p&gt;&lt;p&gt;Analyzing scientific literature and over 1 million rock samples, the team used the framework to sort candidate materials into 19 types, ranging from biomass to mining byproducts to demolished construction materials. Mahjoubi and his team found that suitable materials were available globally — and, more impressively, many could be incorporated into concrete mixes just by grinding them. This means it’s possible to extract emissions and cost savings without much additional processing.&amp;nbsp;&lt;/p&gt;&lt;p&gt;“Some of the most interesting materials that could replace a portion of cement are ceramics,” notes Mahjoubi. “Old tiles, bricks, pottery — all these materials may have high reactivity. That’s something we’ve observed in ancient Roman concrete, where ceramics were added to help waterproof structures. I’ve had many interesting conversations on this with Professor Admir Masic, who leads a lot of the ancient concrete studies here at MIT.”&lt;/p&gt;&lt;p&gt;The potential of everyday materials like ceramics and industrial materials like mine tailings is an example of how materials like concrete can help enable a circular economy. By identifying and repurposing materials that would otherwise end up in landfills, researchers and industry can help to give these materials a second life as part of our buildings and infrastructure.&lt;/p&gt;&lt;p&gt;Looking ahead, the research team is planning to upgrade the framework to be capable of assessing even more materials, while experimentally validating some of the best candidates. “AI tools have gotten this research far in a short time, and we are excited to see how the latest developments in large language models enable the next steps,” says Professor Elsa Olivetti, senior author on the work and member of the MIT Department of Materials Science and Engineering. She serves as an MIT Climate Project mission director, a CSHub principal investigator, and the leader of the Olivetti Group.&lt;/p&gt;&lt;p&gt;“Concrete is the backbone of the built environment,” says Randolph Kirchain, co-author and CSHub director. “By applying data science and AI tools to material design, we hope to support industry efforts to build more sustainably, without compromising on strength, safety, or durability.&lt;/p&gt;&lt;p&gt;In addition to Mahjoubi, Olivetti, and Kirchain, co-authors on the work include MIT postdoc Vineeth Venugopal, Ipek Bensu Manav SM ’21, PhD ’24; and CSHub Deputy Director Hessam AzariJafari.&lt;/p&gt;&lt;p&gt;This research was conducted through the MIT Concrete Sustainability Hub, which is supported by the Concrete Advancement Foundation. This work also received funding from the MIT-IBM Watson AI Lab.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202505/mit-Soroush-Mahjoubi.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;For weeks, the whiteboard in the lab was crowded with scribbles, diagrams, and chemical formulas. A research team across the Olivetti Group and the MIT Concrete Sustainability Hub (CSHub) was working intensely on a key problem: How can we reduce the amount of cement in concrete to save on costs and emissions?&amp;nbsp;&lt;/p&gt;&lt;p&gt;The question was certainly not new; materials like fly ash, a byproduct of coal production, and slag, a byproduct of steelmaking, have long been used to replace some of the cement in concrete mixes. However, the demand for these products is outpacing supply as industry looks to reduce its climate impacts by expanding their use, making the search for alternatives urgent. The challenge that the team discovered wasn’t a lack of candidates; the problem was that there were too many to sort through.&lt;/p&gt;&lt;p&gt;On May 17, the team, led by postdoc Soroush Mahjoubi, published an open-access paper in Nature’s&lt;em&gt; Communications Materials&lt;/em&gt; outlining their solution. “We realized that AI was the key to moving forward,” notes Mahjoubi. “There is so much data out there on potential materials — hundreds of thousands of pages of scientific literature. Sorting through them would have taken many lifetimes of work, by which time more materials would have been discovered!”&lt;/p&gt;&lt;p&gt;With large language models, like the chatbots many of us use daily, the team built a machine-learning framework that evaluates and sorts candidate materials based on their physical and chemical properties.&amp;nbsp;&lt;/p&gt;&lt;p&gt;“First, there is hydraulic reactivity. The reason that concrete is strong is that cement — the ‘glue’ that holds it together — hardens when exposed to water. So, if we replace this glue, we need to make sure the substitute reacts similarly,” explains Mahjoubi. “Second, there is pozzolanicity. This is when a material reacts with calcium hydroxide, a byproduct created when cement meets water, to make the concrete harder and stronger over time.&amp;nbsp; We need to balance the hydraulic and pozzolanic materials in the mix so the concrete performs at its best.”&lt;/p&gt;&lt;p&gt;Analyzing scientific literature and over 1 million rock samples, the team used the framework to sort candidate materials into 19 types, ranging from biomass to mining byproducts to demolished construction materials. Mahjoubi and his team found that suitable materials were available globally — and, more impressively, many could be incorporated into concrete mixes just by grinding them. This means it’s possible to extract emissions and cost savings without much additional processing.&amp;nbsp;&lt;/p&gt;&lt;p&gt;“Some of the most interesting materials that could replace a portion of cement are ceramics,” notes Mahjoubi. “Old tiles, bricks, pottery — all these materials may have high reactivity. That’s something we’ve observed in ancient Roman concrete, where ceramics were added to help waterproof structures. I’ve had many interesting conversations on this with Professor Admir Masic, who leads a lot of the ancient concrete studies here at MIT.”&lt;/p&gt;&lt;p&gt;The potential of everyday materials like ceramics and industrial materials like mine tailings is an example of how materials like concrete can help enable a circular economy. By identifying and repurposing materials that would otherwise end up in landfills, researchers and industry can help to give these materials a second life as part of our buildings and infrastructure.&lt;/p&gt;&lt;p&gt;Looking ahead, the research team is planning to upgrade the framework to be capable of assessing even more materials, while experimentally validating some of the best candidates. “AI tools have gotten this research far in a short time, and we are excited to see how the latest developments in large language models enable the next steps,” says Professor Elsa Olivetti, senior author on the work and member of the MIT Department of Materials Science and Engineering. She serves as an MIT Climate Project mission director, a CSHub principal investigator, and the leader of the Olivetti Group.&lt;/p&gt;&lt;p&gt;“Concrete is the backbone of the built environment,” says Randolph Kirchain, co-author and CSHub director. “By applying data science and AI tools to material design, we hope to support industry efforts to build more sustainably, without compromising on strength, safety, or durability.&lt;/p&gt;&lt;p&gt;In addition to Mahjoubi, Olivetti, and Kirchain, co-authors on the work include MIT postdoc Vineeth Venugopal, Ipek Bensu Manav SM ’21, PhD ’24; and CSHub Deputy Director Hessam AzariJafari.&lt;/p&gt;&lt;p&gt;This research was conducted through the MIT Concrete Sustainability Hub, which is supported by the Concrete Advancement Foundation. This work also received funding from the MIT-IBM Watson AI Lab.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/ai-stirs-recipe-for-concrete-0602</guid><pubDate>Mon, 02 Jun 2025 19:45:00 +0000</pubDate></item><item><title>No GPU left behind: Unlocking Efficiency with Co-located vLLM in TRL (Hugging Face - Blog)</title><link>https://huggingface.co/blog/vllm-colocate</link><description>&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Ed Snible's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://cdn-avatars.huggingface.co/v1/production/uploads/6458dbeac16ecb4815de49de/bu2pIHAU9onlJAG_YnPt8.png" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;

&lt;p&gt;TRL supports training LLMs using GRPO, an online learning algorithm recently introduced in the &lt;em&gt;DeepSeekMath&lt;/em&gt; paper. In GRPO, the model learns from its own outputs: it generates responses during training, receives feedback, and uses that feedback to improve itself over time.&lt;/p&gt;
&lt;p&gt;This makes generation a critical step in the training loop — and also a major bottleneck. To speed up generation, TRL integrates with vLLM. This combination lets you train powerful models more efficiently in GRPO setup. However, there’s a catch.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		🧨 The Problem
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Before TRL v0.18.0, vLLM was only supported in &lt;strong&gt;server mode&lt;/strong&gt;, running as a separate process on different GPUs from the training job. It communicated with the training script over HTTP, which made the setup modular and easy to use — but also introduced GPU inefficiencies.&lt;/p&gt;
&lt;p&gt;Here’s what happens:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;During training, the model needs to generate completions frequently.&lt;/li&gt;
&lt;li&gt;The trainer sends a request to the vLLM server, which runs on its own GPUs.&lt;/li&gt;
&lt;li&gt;While vLLM generates, the &lt;strong&gt;training GPUs sit idle&lt;/strong&gt; and wait.&lt;/li&gt;
&lt;li&gt;Once generation is done, &lt;strong&gt;vLLM GPUs become idle&lt;/strong&gt;, and training resumes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This “ping-pong” between training and generation causes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wasted GPU time on both sides&lt;/li&gt;
&lt;li&gt;Increased demand for &lt;strong&gt;extra GPUs&lt;/strong&gt; just to run inference&lt;/li&gt;
&lt;li&gt;Reduced overall &lt;strong&gt;throughput and higher cost&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In online learning methods like GRPO — where generation happens constantly — this inefficiency becomes even more painful. You spend more on hardware, but don't get the performance you'd expect.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So, the key question becomes:&lt;/strong&gt;  &lt;em&gt;Can we share the same GPUs for both training and generation, instead of separating them?&lt;/em&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		💡 The Opportunity
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The main issue was that training and inference ran on separate GPUs, leading to idle time and underutilization. The natural solution? Run both on the same GPUs. Instead of having vLLM operate as a standalone server in its own process and devices, what if vLLM could run alongside the training code, within the same distributed process group? This would let us launch a single distributed job where training and inference share the same devices, switching between tasks efficiently without wasting resources.&lt;/p&gt;
&lt;p&gt;This approach is what we refer to as &lt;strong&gt;colocation&lt;/strong&gt;. Training and inference are co-located on the same GPUs and coordinated via the same process group, allowing them to take turns smoothly — no extra hardware needed.&lt;/p&gt;
&lt;p&gt;Previously, this wasn’t possible in TRL, which relied on vLLM as an external HTTP server. That changed with our PR #3394, which added support for vLLM’s external launcher and true integration into the training process.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		What It Enables
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Unified Execution&lt;/strong&gt;: By embedding vLLM in the same process group, both training and inference tasks can share the same GPUs, taking turns instead of waiting on each other. This reduces idle time and boosts overall efficiency.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Skip HTTP Communication&lt;/strong&gt;: No need for REST API calls or networking — vLLM runs inline with the training loop, avoiding overhead and latency.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Torchrun Compatibility&lt;/strong&gt;: Works seamlessly with &lt;code&gt;torchrun&lt;/code&gt;, so it's easy to scale across nodes with minimal config changes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;TP and DP Support&lt;/strong&gt;: Compatible with Tensor Parallelism and Data Parallelism, making it suitable for large-scale training runs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;SPMD Execution Pattern&lt;/strong&gt;: Uses a Single Program, Multiple Data (SPMD) model, where each GPU runs its own instance of the engine in sync. Ideal for distributed multi-GPU, multi-node setups.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Simplified Deployment&lt;/strong&gt;: You no longer need to maintain a separate server script — vLLM is launched and controlled directly inside your training job.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Enhanced Throughput&lt;/strong&gt;: By avoiding idle GPUs and eliminating inter-process communication, the system delivers faster training and generation, especially important in online learning setups like GRPO.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Robust Inter-process Communication&lt;/strong&gt;: This is more robust because it avoids the complexity of setting up distributed process groups between independent processes, as required in server mode.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thanks to this feature, co-located training and inference is no longer a hack — it’s now &lt;strong&gt;first-class, scalable, and production-ready&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		🧩 Design: From Separate Servers to Shared GPUs
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The shift from server TRL to co-located TRL is all about smarter GPU usage. The diagram below shows the difference:&lt;/p&gt;
&lt;p&gt;&lt;img alt="gpus-design" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/vllm-colocate/gpus-design.png" /&gt;&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Server TRL Setup (Top Row)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;In the server TRL setup, training and inference run on separate GPUs. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPUs 0 through 2 are used for training.&lt;/li&gt;
&lt;li&gt;GPU 3 is fully dedicated to running vLLM as a separate server.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;During training steps, &lt;strong&gt;GPU 3 sits idle&lt;/strong&gt;.
During generation steps (inference), &lt;strong&gt;GPUs 0–2 are idle&lt;/strong&gt; while GPU 3 generates outputs.&lt;/p&gt;
&lt;p&gt;This leads to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Inefficient GPU usage, with devices frequently waiting on each other&lt;/li&gt;
&lt;li&gt;Extra GPUs provisioned solely for inference&lt;/li&gt;
&lt;li&gt;Increased cost and complexity&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Co-located TRL Setup (Bottom Row)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;In contrast, the co-located TRL setup runs both training and vLLM on the &lt;strong&gt;same GPUs&lt;/strong&gt;. Each GPU:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Runs the training loop&lt;/li&gt;
&lt;li&gt;Launches a vLLM engine within the &lt;strong&gt;same process&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Training and inference &lt;strong&gt;take turns&lt;/strong&gt; using the GPU’s resources — no need for dedicated devices or separate processes.&lt;/p&gt;
&lt;p&gt;This design:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reduces idle time&lt;/li&gt;
&lt;li&gt;Minimizes inter-process and HTTP communication&lt;/li&gt;
&lt;li&gt;Fully utilizes available GPU memory and compute&lt;/li&gt;
&lt;li&gt;Delivers &lt;strong&gt;faster throughput&lt;/strong&gt; without increasing hardware requirements&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		🛠️ Implementation Notes
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Instead of launching vLLM as a server, the trainer now launches vLLM &lt;strong&gt;in-process&lt;/strong&gt; using the external launcher, as shown below:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;self.llm = LLM(
    model=model.name_or_path,
    tensor_parallel_size=args.vllm_tensor_parallel_size,
    gpu_memory_utilization=self.vllm_gpu_memory_utilization,
    max_num_seqs=self.args.per_device_train_batch_size
        * self.vllm_tensor_parallel_size
        * self.args.gradient_accumulation_steps,
    max_model_len=self.max_prompt_length + self.max_completion_length,
    distributed_executor_backend=&lt;span class="hljs-string"&gt;"external_launcher"&lt;/span&gt;,
    
    seed=self.accelerator.process_index // self.vllm_tensor_parallel_size,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Co-located vLLM respects the torch.distributed process group and rank structure. This allows vLLM to be initialized alongside training without conflict and makes TP/DP setups work seamlessly:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;if&lt;/span&gt; self.vllm_tensor_parallel_size &amp;gt; &lt;span class="hljs-number"&gt;1&lt;/span&gt;:
    
    self.tp_group, _ = torch.distributed.new_subgroups_by_enumeration(
        [
            &lt;span class="hljs-built_in"&gt;list&lt;/span&gt;(&lt;span class="hljs-built_in"&gt;range&lt;/span&gt;(i * self.vllm_tensor_parallel_size, (i + &lt;span class="hljs-number"&gt;1&lt;/span&gt;) * self.vllm_tensor_parallel_size))
            &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; i &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; &lt;span class="hljs-built_in"&gt;range&lt;/span&gt;(self.accelerator.num_processes // self.vllm_tensor_parallel_size)
        ]
    )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Co-located vLLM no longer relies on REST APIs — it runs directly in memory and communicates via native Python calls:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;if&lt;/span&gt; self.vllm_tensor_parallel_size &amp;gt; &lt;span class="hljs-number"&gt;1&lt;/span&gt;:
    orig_size = &lt;span class="hljs-built_in"&gt;len&lt;/span&gt;(prompts_text)
    gathered_prompts = [&lt;span class="hljs-literal"&gt;None&lt;/span&gt; &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; _ &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; &lt;span class="hljs-built_in"&gt;range&lt;/span&gt;(self.vllm_tensor_parallel_size)]
    torch.distributed.all_gather_object(gathered_prompts, prompts_text, group=self.tp_group)
    all_prompts_text = [p &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; sublist &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; gathered_prompts &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; p &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; sublist]
&lt;span class="hljs-keyword"&gt;else&lt;/span&gt;:
    all_prompts_text = prompts_text

&lt;span class="hljs-keyword"&gt;with&lt;/span&gt; profiling_context(self, &lt;span class="hljs-string"&gt;"vLLM.generate"&lt;/span&gt;):
    all_outputs = self.llm.generate(all_prompts_text, sampling_params=sampling_params, use_tqdm=&lt;span class="hljs-literal"&gt;False&lt;/span&gt;)

completion_ids = [output.token_ids &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; outputs &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; all_outputs &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; output &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; outputs.outputs]

&lt;span class="hljs-keyword"&gt;if&lt;/span&gt; self.vllm_tensor_parallel_size &amp;gt; &lt;span class="hljs-number"&gt;1&lt;/span&gt;:
    local_rank_in_group = torch.distributed.get_rank(group=self.tp_group)
    tp_slice = &lt;span class="hljs-built_in"&gt;slice&lt;/span&gt;(local_rank_in_group * orig_size, (local_rank_in_group + &lt;span class="hljs-number"&gt;1&lt;/span&gt;) * orig_size)
    completion_ids = completion_ids[tp_slice]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To use this setup, simply set vllm_mode="colocate" in your GRPO configuration:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;training_args = GRPOConfig(
    ...,
    use_vllm=&lt;span class="hljs-literal"&gt;True&lt;/span&gt;,
    vllm_mode=&lt;span class="hljs-string"&gt;"colocate"&lt;/span&gt;,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: Depending on the model size and the overall GPU memory requirements for training, you may need to adjust the vllm_gpu_memory_utilization parameter in &lt;code&gt;GRPOConfig&lt;/code&gt; to avoid underutilization or out-of-memory errors.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		📊 Showcase: Co-located vs. Plain TRL Performance
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;To measure the impact of colocation, we ran a series of experiments comparing the traditional &lt;strong&gt;server mode&lt;/strong&gt; (where vLLM runs on a separate GPU as a standalone server) with the new &lt;strong&gt;co-locate mode&lt;/strong&gt; (where training and inference share the same GPUs).&lt;/p&gt;
&lt;p&gt;In &lt;strong&gt;server mode&lt;/strong&gt;, only 7 GPUs are used for training because 1 GPU is fully dedicated to the vLLM inference server.&lt;/p&gt;
&lt;p&gt;In &lt;strong&gt;co-locate mode&lt;/strong&gt;, all 8 GPUs are used for training — increasing the effective batch size by default.&lt;/p&gt;
&lt;p&gt;To ensure a fair comparison, we &lt;strong&gt;normalized throughput in server mode by a factor of 8/7&lt;/strong&gt;. This adjustment accounts for the greater training capacity in co-locate mode and allows us to compare the two setups under equal training conditions.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Experiment 1: 1.5B Model — Varying Batch Sizes
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;As the batch size increases, throughput improves in both setups.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Co-located setup reaches up to 1.43× speedup&lt;/strong&gt; at the largest batch size.&lt;/li&gt;
&lt;li&gt;Larger batches make better use of shared GPU memory in co-located mode.
&lt;img alt="small-b" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/vllm-colocate/small-b.png" /&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Experiment 2: 1.5B Model — Varying Tensor Parallelism (TP)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the co-located setup, increasing TP &lt;strong&gt;reduces performance&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;More sharding introduces more communication overhead — which is &lt;strong&gt;not ideal for smaller models&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Takeaway&lt;/strong&gt;: For small models, avoid over-sharding in co-located mode.
&lt;img alt="small-tp" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/vllm-colocate/small-tp.png" /&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Experiment 3: 7B Model — Varying Batch Sizes
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Again, co-located mode &lt;strong&gt;scales better with batch size&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Gains reach &lt;strong&gt;1.35× speedup&lt;/strong&gt; at the largest batch tested.
&lt;img alt="med-b" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/vllm-colocate/med-b.png" /&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Experiment 4: 7B Model — Varying Tensor Parallelism (TP)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Opposite trend from the 1.5B model.&lt;/li&gt;
&lt;li&gt;With 7B, &lt;strong&gt;more TP improves throughput&lt;/strong&gt;, reaching up to &lt;strong&gt;1.73× speedup&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Larger models benefit from sharding&lt;/strong&gt; in co-located setups.
&lt;img alt="med-tp" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/vllm-colocate/med-tp.png" /&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		📊 Scaling to 72B Model
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;When training large models like &lt;strong&gt;Qwen2.5-Math-72B&lt;/strong&gt;, it's important to use the right strategies to make training efficient, scalable, and stable across many GPUs and nodes. In our setup, we combined &lt;strong&gt;co-located vLLM&lt;/strong&gt; with several key optimizations to make this work efficiently.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Sleep Mode in vLLM
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;When using co-located training, managing GPU memory is crucial so that both training and inference can run smoothly on the same devices. To support this, we added vLLM’s &lt;code&gt;sleep()&lt;/code&gt; API into the GRPO training loop.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;sleep()&lt;/code&gt; function temporarily pauses the vLLM engine and frees up GPU memory. It supports two levels:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Level 1&lt;/strong&gt;: Unloads model weights from GPU (keeps them in CPU memory) and clears the KV cache.
Useful when the same model will be reused soon.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Level 2&lt;/strong&gt;: Unloads both model weights and KV cache entirely.
Best for scenarios where the model will change or won’t be reused right away.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In GRPO, the model is updated after every step — so we use &lt;strong&gt;Level 2 sleep&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Benefits of Level 2 sleep:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Maximizes free GPU memory&lt;/strong&gt; for training&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Avoids memory contention&lt;/strong&gt; between training and generation&lt;/li&gt;
&lt;li&gt;Keeps colocation efficient, even for large models like Qwen2.5-72B&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This small addition makes a &lt;strong&gt;big difference&lt;/strong&gt; in enabling smooth and scalable co-located training.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		DeepSpeed Optimizations
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;To train large models like Qwen2.5-72B, we rely on &lt;strong&gt;DeepSpeed ZeRO Stage 3&lt;/strong&gt;, the same setup used in plain TRL.&lt;/p&gt;
&lt;p&gt;ZeRO helps scale large models by distributing memory across GPUs. Stage 3 goes further by partitioning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model weights&lt;/li&gt;
&lt;li&gt;Gradients&lt;/li&gt;
&lt;li&gt;Optimizer states&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is essential for models that can’t fit on a single GPU. With ZeRO Stage 3, each GPU handles only a portion of the model.&lt;/p&gt;
&lt;p&gt;Additional options we enable:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;"offload_optimizer": {"device": "cpu"}&lt;/code&gt;
Moves optimizer states to CPU to free GPU memory — critical in co-located setups.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;"overlap_comm": true&lt;/code&gt;
Enables communication overlap with computation, speeding up training.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;"contiguous_gradients": true&lt;/code&gt;
Allocates gradients in a single memory block, improving memory access and reducing fragmentation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These optimizations help &lt;strong&gt;train 72B models efficiently&lt;/strong&gt;, and ensure colocation remains stable under tight memory constraints.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Accelerate Integration
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;As recommended in TRL, we use &lt;strong&gt;Accelerate&lt;/strong&gt;, a lightweight library that simplifies distributed training. It handles:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multi-GPU and multi-node job launching&lt;/li&gt;
&lt;li&gt;Data parallelism&lt;/li&gt;
&lt;li&gt;Gradient accumulation&lt;/li&gt;
&lt;li&gt;Distributed data loading&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This makes the setup clean, scalable, and easy to maintain.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Experiment 5: Qwen2.5-Math-72B — Throughput, Accuracy, and Benchmark Results
	&lt;/span&gt;
&lt;/h3&gt;
&lt;h4 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Throughput
	&lt;/span&gt;
&lt;/h4&gt;
&lt;p&gt;Even with &lt;strong&gt;4 fewer GPUs&lt;/strong&gt;, the &lt;strong&gt;co-locate setup is ~1.26× faster&lt;/strong&gt; than plain TRL.
This highlights the effectiveness of smarter GPU sharing and memory cleanup using &lt;code&gt;sleep()&lt;/code&gt;.
&lt;img alt="72b-tput" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/vllm-colocate/72b-tput.png" /&gt;&lt;/p&gt;
&lt;h4 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Reward Curve
	&lt;/span&gt;
&lt;/h4&gt;
&lt;p&gt;Training reward plots for co-locate and plain setups are &lt;strong&gt;nearly identical&lt;/strong&gt;, demonstrating that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Co-located training preserves accuracy&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;There’s &lt;strong&gt;no regression in model learning performance&lt;/strong&gt;
&lt;img alt="blogpost_72b_rewards" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/vllm-colocate/blogpost_72b_rewards.png" /&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Math500 Benchmark
	&lt;/span&gt;
&lt;/h4&gt;
&lt;p&gt;We evaluated three models: &lt;strong&gt;Base model&lt;/strong&gt;, &lt;strong&gt;Co-locate-trained model&lt;/strong&gt;, &lt;strong&gt;Plain-trained model&lt;/strong&gt; on the Math500 benchmark. Both trained models &lt;strong&gt;outperform the base&lt;/strong&gt;, and the &lt;strong&gt;co-locate model performs on par&lt;/strong&gt; with the plain-trained model — confirming that colocation does not compromise downstream performance.
&lt;img alt="blogpost_72b_math500" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/vllm-colocate/blogpost_72b_math500.png" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		🎓 Challenges &amp;amp; Lessons Learned &amp;amp; next steps
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Through our work on scaling GRPO training with co-located vLLM, we've faced several critical challenges and learned important lessons about efficiency, flexibility, and system design when training large models.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Challenges
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Tensor Parallelism Bug in vLLM ≥ 0.8.0.&lt;/strong&gt; Tensor Parallelism (TP) with external_launcher stopped working in vLLM version 0.8.0 and above. This was tracked under Issue #15895. To identify the breaking point, we followed the approach described in this vLLM developer blog post, which provides wheels for every commit. After bisecting, we identified the breaking commit as cc10281. The root cause was determinism — the newer versions required explicitly setting the random seed. Once the seed was set, the issue went away.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Level 2 Sleep Buffer Bug.&lt;/strong&gt; Initially, level 2 sleep didn’t work correctly when we tried to reload weights using load_weights. This issue was tracked in Issue #16564. The problem was that model buffers (like running mean/var in BatchNorm) weren’t restored after waking up from sleep. The fix came with PR #16889, which added logic to explicitly restore buffers when waking up from level 2 sleep. We now keep a copy of the original buffers and manually reapply them after loading new weights.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Segmentation Fault on Exit.&lt;/strong&gt; There’s still an open issue with vLLM sleep causing a segmentation fault at the end of training when closing processes. This was reported in Issue #16993. This crash happens during shutdown but does not break training itself, so we were able to complete all demos and experiments shared in this blog. However, we’re waiting for an official fix before integrating sleep() fully into TRL upstream.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These challenges were not blockers, but they required careful debugging, version control, and a deeper understanding of how vLLM manages memory and parallelism under the hood.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Lessons Learned
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Co-located inference dramatically improves GPU utilization. By allowing training and generation to share the same GPUs, we eliminate idle time and reduce hardware requirements — achieving higher throughput even with fewer GPUs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;vLLM's sleep() feature is essential for large-scale colocation. It enables fine-grained control over memory usage, allowing training to fully reclaim GPU memory between generation steps — a key enabler for models like Qwen2.5-72B.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;DeepSpeed ZeRO Stage 3 is essential for training large models. It allows extremely large networks to fit into memory by distributing model weights, gradients, and optimizer states across multiple GPUs. In our experience, enabling contiguous_gradients helped reduce memory fragmentation, while offloading the optimizer to the CPU freed up critical GPU memory — both of which were especially helpful in colocated setups.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Colocation is powerful but comes with trade-offs. It works best when GPU memory is carefully managed, often requiring manual tuning of memory usage parameters like vllm_gpu_memory_utilization. While it offers clear throughput benefits and reduces idle GPU time, colocation may not be ideal for models with tight memory budgets or when memory fragmentation is not well controlled. When done right, though, it unlocks significant efficiency gains.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;TP/DP compatibility, Accelerate, and torchrun support make deployment seamless. Despite the complexity of the underlying architecture, the entire system can be launched and scaled with standard distributed tools.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Co-located training maintains model quality. Across multiple benchmarks (Math500, AIME24), co-located and plain setups produced comparable results, validating that performance isn’t sacrificed for efficiency.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		✅ Conclusion
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;This blog post explored how co-locating vLLM with GRPO training unlocks significant efficiency gains when training large language models — including models as large as Qwen2.5-72B.&lt;/p&gt;
&lt;p&gt;Traditionally, TRL only supported vLLM in server mode, which required separate processes and GPUs for inference, leading to wasted compute and idle time. With the introduction of vLLM’s external launcher and the colocation PR in TRL PR #3394, we can now run training and inference within the same distributed process group, on the same GPUs, with full support for TP, DP, and Accelerate.&lt;/p&gt;
&lt;p&gt;While challenges remain — such as version-specific vLLM bugs and edge cases such as with sleep() — the overall results show that co-located GRPO is a practical, scalable solution for training large models efficiently. We’re excited to continue refining this setup, integrating features like FSDP, and pushing the limits of large model training — making it faster, cheaper, and more accessible for everyone building the next generation of LLMs.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		✅ Give It a Try!
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Below is an example to try out GRPO training with co-located vLLM.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		📄 &lt;code&gt;train_grpo_colocate.py&lt;/code&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; datasets &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; load_dataset
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; trl &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; GRPOConfig, GRPOTrainer


dataset = load_dataset(&lt;span class="hljs-string"&gt;"trl-lib/tldr"&lt;/span&gt;, split=&lt;span class="hljs-string"&gt;"train"&lt;/span&gt;)


&lt;span class="hljs-keyword"&gt;def&lt;/span&gt; &lt;span class="hljs-title function_"&gt;reward_len&lt;/span&gt;(&lt;span class="hljs-params"&gt;completions, **kwargs&lt;/span&gt;):
    &lt;span class="hljs-keyword"&gt;return&lt;/span&gt; [-&lt;span class="hljs-built_in"&gt;abs&lt;/span&gt;(&lt;span class="hljs-number"&gt;20&lt;/span&gt; - &lt;span class="hljs-built_in"&gt;len&lt;/span&gt;(completion)) &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; completion &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; completions]


training_args = GRPOConfig(
    output_dir=&lt;span class="hljs-string"&gt;"Qwen2-0.5B-GRPO"&lt;/span&gt;,
    logging_steps=&lt;span class="hljs-number"&gt;1&lt;/span&gt;,
    use_vllm=&lt;span class="hljs-literal"&gt;True&lt;/span&gt;,
    vllm_mode=&lt;span class="hljs-string"&gt;"colocate"&lt;/span&gt;,
    vllm_tensor_parallel_size=&lt;span class="hljs-number"&gt;1&lt;/span&gt;,
    vllm_gpu_memory_utilization=&lt;span class="hljs-number"&gt;0.3&lt;/span&gt;,
    max_prompt_length=&lt;span class="hljs-number"&gt;512&lt;/span&gt;,
    max_completion_length=&lt;span class="hljs-number"&gt;1024&lt;/span&gt;,
    max_steps=&lt;span class="hljs-number"&gt;2&lt;/span&gt;,
    num_generations=&lt;span class="hljs-number"&gt;4&lt;/span&gt;,
    num_train_epochs=&lt;span class="hljs-number"&gt;1&lt;/span&gt;,
    per_device_train_batch_size=&lt;span class="hljs-number"&gt;4&lt;/span&gt;,
    push_to_hub=&lt;span class="hljs-literal"&gt;False&lt;/span&gt;,
    report_to=&lt;span class="hljs-literal"&gt;None&lt;/span&gt;
)


trainer = GRPOTrainer(
    model=&lt;span class="hljs-string"&gt;"Qwen/Qwen2-0.5B-Instruct"&lt;/span&gt;,
    reward_funcs=reward_len,
    args=training_args,
    train_dataset=dataset,
)

trainer.train()
&lt;/code&gt;&lt;/pre&gt;
&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Ed Snible's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://cdn-avatars.huggingface.co/v1/production/uploads/6458dbeac16ecb4815de49de/bu2pIHAU9onlJAG_YnPt8.png" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;

&lt;p&gt;TRL supports training LLMs using GRPO, an online learning algorithm recently introduced in the &lt;em&gt;DeepSeekMath&lt;/em&gt; paper. In GRPO, the model learns from its own outputs: it generates responses during training, receives feedback, and uses that feedback to improve itself over time.&lt;/p&gt;
&lt;p&gt;This makes generation a critical step in the training loop — and also a major bottleneck. To speed up generation, TRL integrates with vLLM. This combination lets you train powerful models more efficiently in GRPO setup. However, there’s a catch.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		🧨 The Problem
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Before TRL v0.18.0, vLLM was only supported in &lt;strong&gt;server mode&lt;/strong&gt;, running as a separate process on different GPUs from the training job. It communicated with the training script over HTTP, which made the setup modular and easy to use — but also introduced GPU inefficiencies.&lt;/p&gt;
&lt;p&gt;Here’s what happens:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;During training, the model needs to generate completions frequently.&lt;/li&gt;
&lt;li&gt;The trainer sends a request to the vLLM server, which runs on its own GPUs.&lt;/li&gt;
&lt;li&gt;While vLLM generates, the &lt;strong&gt;training GPUs sit idle&lt;/strong&gt; and wait.&lt;/li&gt;
&lt;li&gt;Once generation is done, &lt;strong&gt;vLLM GPUs become idle&lt;/strong&gt;, and training resumes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This “ping-pong” between training and generation causes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wasted GPU time on both sides&lt;/li&gt;
&lt;li&gt;Increased demand for &lt;strong&gt;extra GPUs&lt;/strong&gt; just to run inference&lt;/li&gt;
&lt;li&gt;Reduced overall &lt;strong&gt;throughput and higher cost&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In online learning methods like GRPO — where generation happens constantly — this inefficiency becomes even more painful. You spend more on hardware, but don't get the performance you'd expect.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So, the key question becomes:&lt;/strong&gt;  &lt;em&gt;Can we share the same GPUs for both training and generation, instead of separating them?&lt;/em&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		💡 The Opportunity
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The main issue was that training and inference ran on separate GPUs, leading to idle time and underutilization. The natural solution? Run both on the same GPUs. Instead of having vLLM operate as a standalone server in its own process and devices, what if vLLM could run alongside the training code, within the same distributed process group? This would let us launch a single distributed job where training and inference share the same devices, switching between tasks efficiently without wasting resources.&lt;/p&gt;
&lt;p&gt;This approach is what we refer to as &lt;strong&gt;colocation&lt;/strong&gt;. Training and inference are co-located on the same GPUs and coordinated via the same process group, allowing them to take turns smoothly — no extra hardware needed.&lt;/p&gt;
&lt;p&gt;Previously, this wasn’t possible in TRL, which relied on vLLM as an external HTTP server. That changed with our PR #3394, which added support for vLLM’s external launcher and true integration into the training process.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		What It Enables
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Unified Execution&lt;/strong&gt;: By embedding vLLM in the same process group, both training and inference tasks can share the same GPUs, taking turns instead of waiting on each other. This reduces idle time and boosts overall efficiency.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Skip HTTP Communication&lt;/strong&gt;: No need for REST API calls or networking — vLLM runs inline with the training loop, avoiding overhead and latency.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Torchrun Compatibility&lt;/strong&gt;: Works seamlessly with &lt;code&gt;torchrun&lt;/code&gt;, so it's easy to scale across nodes with minimal config changes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;TP and DP Support&lt;/strong&gt;: Compatible with Tensor Parallelism and Data Parallelism, making it suitable for large-scale training runs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;SPMD Execution Pattern&lt;/strong&gt;: Uses a Single Program, Multiple Data (SPMD) model, where each GPU runs its own instance of the engine in sync. Ideal for distributed multi-GPU, multi-node setups.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Simplified Deployment&lt;/strong&gt;: You no longer need to maintain a separate server script — vLLM is launched and controlled directly inside your training job.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Enhanced Throughput&lt;/strong&gt;: By avoiding idle GPUs and eliminating inter-process communication, the system delivers faster training and generation, especially important in online learning setups like GRPO.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Robust Inter-process Communication&lt;/strong&gt;: This is more robust because it avoids the complexity of setting up distributed process groups between independent processes, as required in server mode.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thanks to this feature, co-located training and inference is no longer a hack — it’s now &lt;strong&gt;first-class, scalable, and production-ready&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		🧩 Design: From Separate Servers to Shared GPUs
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The shift from server TRL to co-located TRL is all about smarter GPU usage. The diagram below shows the difference:&lt;/p&gt;
&lt;p&gt;&lt;img alt="gpus-design" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/vllm-colocate/gpus-design.png" /&gt;&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Server TRL Setup (Top Row)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;In the server TRL setup, training and inference run on separate GPUs. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPUs 0 through 2 are used for training.&lt;/li&gt;
&lt;li&gt;GPU 3 is fully dedicated to running vLLM as a separate server.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;During training steps, &lt;strong&gt;GPU 3 sits idle&lt;/strong&gt;.
During generation steps (inference), &lt;strong&gt;GPUs 0–2 are idle&lt;/strong&gt; while GPU 3 generates outputs.&lt;/p&gt;
&lt;p&gt;This leads to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Inefficient GPU usage, with devices frequently waiting on each other&lt;/li&gt;
&lt;li&gt;Extra GPUs provisioned solely for inference&lt;/li&gt;
&lt;li&gt;Increased cost and complexity&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Co-located TRL Setup (Bottom Row)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;In contrast, the co-located TRL setup runs both training and vLLM on the &lt;strong&gt;same GPUs&lt;/strong&gt;. Each GPU:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Runs the training loop&lt;/li&gt;
&lt;li&gt;Launches a vLLM engine within the &lt;strong&gt;same process&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Training and inference &lt;strong&gt;take turns&lt;/strong&gt; using the GPU’s resources — no need for dedicated devices or separate processes.&lt;/p&gt;
&lt;p&gt;This design:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reduces idle time&lt;/li&gt;
&lt;li&gt;Minimizes inter-process and HTTP communication&lt;/li&gt;
&lt;li&gt;Fully utilizes available GPU memory and compute&lt;/li&gt;
&lt;li&gt;Delivers &lt;strong&gt;faster throughput&lt;/strong&gt; without increasing hardware requirements&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		🛠️ Implementation Notes
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Instead of launching vLLM as a server, the trainer now launches vLLM &lt;strong&gt;in-process&lt;/strong&gt; using the external launcher, as shown below:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;self.llm = LLM(
    model=model.name_or_path,
    tensor_parallel_size=args.vllm_tensor_parallel_size,
    gpu_memory_utilization=self.vllm_gpu_memory_utilization,
    max_num_seqs=self.args.per_device_train_batch_size
        * self.vllm_tensor_parallel_size
        * self.args.gradient_accumulation_steps,
    max_model_len=self.max_prompt_length + self.max_completion_length,
    distributed_executor_backend=&lt;span class="hljs-string"&gt;"external_launcher"&lt;/span&gt;,
    
    seed=self.accelerator.process_index // self.vllm_tensor_parallel_size,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Co-located vLLM respects the torch.distributed process group and rank structure. This allows vLLM to be initialized alongside training without conflict and makes TP/DP setups work seamlessly:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;if&lt;/span&gt; self.vllm_tensor_parallel_size &amp;gt; &lt;span class="hljs-number"&gt;1&lt;/span&gt;:
    
    self.tp_group, _ = torch.distributed.new_subgroups_by_enumeration(
        [
            &lt;span class="hljs-built_in"&gt;list&lt;/span&gt;(&lt;span class="hljs-built_in"&gt;range&lt;/span&gt;(i * self.vllm_tensor_parallel_size, (i + &lt;span class="hljs-number"&gt;1&lt;/span&gt;) * self.vllm_tensor_parallel_size))
            &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; i &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; &lt;span class="hljs-built_in"&gt;range&lt;/span&gt;(self.accelerator.num_processes // self.vllm_tensor_parallel_size)
        ]
    )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Co-located vLLM no longer relies on REST APIs — it runs directly in memory and communicates via native Python calls:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;if&lt;/span&gt; self.vllm_tensor_parallel_size &amp;gt; &lt;span class="hljs-number"&gt;1&lt;/span&gt;:
    orig_size = &lt;span class="hljs-built_in"&gt;len&lt;/span&gt;(prompts_text)
    gathered_prompts = [&lt;span class="hljs-literal"&gt;None&lt;/span&gt; &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; _ &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; &lt;span class="hljs-built_in"&gt;range&lt;/span&gt;(self.vllm_tensor_parallel_size)]
    torch.distributed.all_gather_object(gathered_prompts, prompts_text, group=self.tp_group)
    all_prompts_text = [p &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; sublist &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; gathered_prompts &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; p &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; sublist]
&lt;span class="hljs-keyword"&gt;else&lt;/span&gt;:
    all_prompts_text = prompts_text

&lt;span class="hljs-keyword"&gt;with&lt;/span&gt; profiling_context(self, &lt;span class="hljs-string"&gt;"vLLM.generate"&lt;/span&gt;):
    all_outputs = self.llm.generate(all_prompts_text, sampling_params=sampling_params, use_tqdm=&lt;span class="hljs-literal"&gt;False&lt;/span&gt;)

completion_ids = [output.token_ids &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; outputs &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; all_outputs &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; output &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; outputs.outputs]

&lt;span class="hljs-keyword"&gt;if&lt;/span&gt; self.vllm_tensor_parallel_size &amp;gt; &lt;span class="hljs-number"&gt;1&lt;/span&gt;:
    local_rank_in_group = torch.distributed.get_rank(group=self.tp_group)
    tp_slice = &lt;span class="hljs-built_in"&gt;slice&lt;/span&gt;(local_rank_in_group * orig_size, (local_rank_in_group + &lt;span class="hljs-number"&gt;1&lt;/span&gt;) * orig_size)
    completion_ids = completion_ids[tp_slice]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To use this setup, simply set vllm_mode="colocate" in your GRPO configuration:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;training_args = GRPOConfig(
    ...,
    use_vllm=&lt;span class="hljs-literal"&gt;True&lt;/span&gt;,
    vllm_mode=&lt;span class="hljs-string"&gt;"colocate"&lt;/span&gt;,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: Depending on the model size and the overall GPU memory requirements for training, you may need to adjust the vllm_gpu_memory_utilization parameter in &lt;code&gt;GRPOConfig&lt;/code&gt; to avoid underutilization or out-of-memory errors.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		📊 Showcase: Co-located vs. Plain TRL Performance
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;To measure the impact of colocation, we ran a series of experiments comparing the traditional &lt;strong&gt;server mode&lt;/strong&gt; (where vLLM runs on a separate GPU as a standalone server) with the new &lt;strong&gt;co-locate mode&lt;/strong&gt; (where training and inference share the same GPUs).&lt;/p&gt;
&lt;p&gt;In &lt;strong&gt;server mode&lt;/strong&gt;, only 7 GPUs are used for training because 1 GPU is fully dedicated to the vLLM inference server.&lt;/p&gt;
&lt;p&gt;In &lt;strong&gt;co-locate mode&lt;/strong&gt;, all 8 GPUs are used for training — increasing the effective batch size by default.&lt;/p&gt;
&lt;p&gt;To ensure a fair comparison, we &lt;strong&gt;normalized throughput in server mode by a factor of 8/7&lt;/strong&gt;. This adjustment accounts for the greater training capacity in co-locate mode and allows us to compare the two setups under equal training conditions.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Experiment 1: 1.5B Model — Varying Batch Sizes
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;As the batch size increases, throughput improves in both setups.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Co-located setup reaches up to 1.43× speedup&lt;/strong&gt; at the largest batch size.&lt;/li&gt;
&lt;li&gt;Larger batches make better use of shared GPU memory in co-located mode.
&lt;img alt="small-b" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/vllm-colocate/small-b.png" /&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Experiment 2: 1.5B Model — Varying Tensor Parallelism (TP)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the co-located setup, increasing TP &lt;strong&gt;reduces performance&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;More sharding introduces more communication overhead — which is &lt;strong&gt;not ideal for smaller models&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Takeaway&lt;/strong&gt;: For small models, avoid over-sharding in co-located mode.
&lt;img alt="small-tp" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/vllm-colocate/small-tp.png" /&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Experiment 3: 7B Model — Varying Batch Sizes
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Again, co-located mode &lt;strong&gt;scales better with batch size&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Gains reach &lt;strong&gt;1.35× speedup&lt;/strong&gt; at the largest batch tested.
&lt;img alt="med-b" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/vllm-colocate/med-b.png" /&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Experiment 4: 7B Model — Varying Tensor Parallelism (TP)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Opposite trend from the 1.5B model.&lt;/li&gt;
&lt;li&gt;With 7B, &lt;strong&gt;more TP improves throughput&lt;/strong&gt;, reaching up to &lt;strong&gt;1.73× speedup&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Larger models benefit from sharding&lt;/strong&gt; in co-located setups.
&lt;img alt="med-tp" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/vllm-colocate/med-tp.png" /&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		📊 Scaling to 72B Model
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;When training large models like &lt;strong&gt;Qwen2.5-Math-72B&lt;/strong&gt;, it's important to use the right strategies to make training efficient, scalable, and stable across many GPUs and nodes. In our setup, we combined &lt;strong&gt;co-located vLLM&lt;/strong&gt; with several key optimizations to make this work efficiently.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Sleep Mode in vLLM
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;When using co-located training, managing GPU memory is crucial so that both training and inference can run smoothly on the same devices. To support this, we added vLLM’s &lt;code&gt;sleep()&lt;/code&gt; API into the GRPO training loop.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;sleep()&lt;/code&gt; function temporarily pauses the vLLM engine and frees up GPU memory. It supports two levels:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Level 1&lt;/strong&gt;: Unloads model weights from GPU (keeps them in CPU memory) and clears the KV cache.
Useful when the same model will be reused soon.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Level 2&lt;/strong&gt;: Unloads both model weights and KV cache entirely.
Best for scenarios where the model will change or won’t be reused right away.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In GRPO, the model is updated after every step — so we use &lt;strong&gt;Level 2 sleep&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Benefits of Level 2 sleep:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Maximizes free GPU memory&lt;/strong&gt; for training&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Avoids memory contention&lt;/strong&gt; between training and generation&lt;/li&gt;
&lt;li&gt;Keeps colocation efficient, even for large models like Qwen2.5-72B&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This small addition makes a &lt;strong&gt;big difference&lt;/strong&gt; in enabling smooth and scalable co-located training.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		DeepSpeed Optimizations
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;To train large models like Qwen2.5-72B, we rely on &lt;strong&gt;DeepSpeed ZeRO Stage 3&lt;/strong&gt;, the same setup used in plain TRL.&lt;/p&gt;
&lt;p&gt;ZeRO helps scale large models by distributing memory across GPUs. Stage 3 goes further by partitioning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model weights&lt;/li&gt;
&lt;li&gt;Gradients&lt;/li&gt;
&lt;li&gt;Optimizer states&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is essential for models that can’t fit on a single GPU. With ZeRO Stage 3, each GPU handles only a portion of the model.&lt;/p&gt;
&lt;p&gt;Additional options we enable:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;"offload_optimizer": {"device": "cpu"}&lt;/code&gt;
Moves optimizer states to CPU to free GPU memory — critical in co-located setups.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;"overlap_comm": true&lt;/code&gt;
Enables communication overlap with computation, speeding up training.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;"contiguous_gradients": true&lt;/code&gt;
Allocates gradients in a single memory block, improving memory access and reducing fragmentation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These optimizations help &lt;strong&gt;train 72B models efficiently&lt;/strong&gt;, and ensure colocation remains stable under tight memory constraints.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Accelerate Integration
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;As recommended in TRL, we use &lt;strong&gt;Accelerate&lt;/strong&gt;, a lightweight library that simplifies distributed training. It handles:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multi-GPU and multi-node job launching&lt;/li&gt;
&lt;li&gt;Data parallelism&lt;/li&gt;
&lt;li&gt;Gradient accumulation&lt;/li&gt;
&lt;li&gt;Distributed data loading&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This makes the setup clean, scalable, and easy to maintain.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Experiment 5: Qwen2.5-Math-72B — Throughput, Accuracy, and Benchmark Results
	&lt;/span&gt;
&lt;/h3&gt;
&lt;h4 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Throughput
	&lt;/span&gt;
&lt;/h4&gt;
&lt;p&gt;Even with &lt;strong&gt;4 fewer GPUs&lt;/strong&gt;, the &lt;strong&gt;co-locate setup is ~1.26× faster&lt;/strong&gt; than plain TRL.
This highlights the effectiveness of smarter GPU sharing and memory cleanup using &lt;code&gt;sleep()&lt;/code&gt;.
&lt;img alt="72b-tput" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/vllm-colocate/72b-tput.png" /&gt;&lt;/p&gt;
&lt;h4 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Reward Curve
	&lt;/span&gt;
&lt;/h4&gt;
&lt;p&gt;Training reward plots for co-locate and plain setups are &lt;strong&gt;nearly identical&lt;/strong&gt;, demonstrating that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Co-located training preserves accuracy&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;There’s &lt;strong&gt;no regression in model learning performance&lt;/strong&gt;
&lt;img alt="blogpost_72b_rewards" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/vllm-colocate/blogpost_72b_rewards.png" /&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Math500 Benchmark
	&lt;/span&gt;
&lt;/h4&gt;
&lt;p&gt;We evaluated three models: &lt;strong&gt;Base model&lt;/strong&gt;, &lt;strong&gt;Co-locate-trained model&lt;/strong&gt;, &lt;strong&gt;Plain-trained model&lt;/strong&gt; on the Math500 benchmark. Both trained models &lt;strong&gt;outperform the base&lt;/strong&gt;, and the &lt;strong&gt;co-locate model performs on par&lt;/strong&gt; with the plain-trained model — confirming that colocation does not compromise downstream performance.
&lt;img alt="blogpost_72b_math500" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/vllm-colocate/blogpost_72b_math500.png" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		🎓 Challenges &amp;amp; Lessons Learned &amp;amp; next steps
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Through our work on scaling GRPO training with co-located vLLM, we've faced several critical challenges and learned important lessons about efficiency, flexibility, and system design when training large models.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Challenges
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Tensor Parallelism Bug in vLLM ≥ 0.8.0.&lt;/strong&gt; Tensor Parallelism (TP) with external_launcher stopped working in vLLM version 0.8.0 and above. This was tracked under Issue #15895. To identify the breaking point, we followed the approach described in this vLLM developer blog post, which provides wheels for every commit. After bisecting, we identified the breaking commit as cc10281. The root cause was determinism — the newer versions required explicitly setting the random seed. Once the seed was set, the issue went away.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Level 2 Sleep Buffer Bug.&lt;/strong&gt; Initially, level 2 sleep didn’t work correctly when we tried to reload weights using load_weights. This issue was tracked in Issue #16564. The problem was that model buffers (like running mean/var in BatchNorm) weren’t restored after waking up from sleep. The fix came with PR #16889, which added logic to explicitly restore buffers when waking up from level 2 sleep. We now keep a copy of the original buffers and manually reapply them after loading new weights.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Segmentation Fault on Exit.&lt;/strong&gt; There’s still an open issue with vLLM sleep causing a segmentation fault at the end of training when closing processes. This was reported in Issue #16993. This crash happens during shutdown but does not break training itself, so we were able to complete all demos and experiments shared in this blog. However, we’re waiting for an official fix before integrating sleep() fully into TRL upstream.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These challenges were not blockers, but they required careful debugging, version control, and a deeper understanding of how vLLM manages memory and parallelism under the hood.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Lessons Learned
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Co-located inference dramatically improves GPU utilization. By allowing training and generation to share the same GPUs, we eliminate idle time and reduce hardware requirements — achieving higher throughput even with fewer GPUs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;vLLM's sleep() feature is essential for large-scale colocation. It enables fine-grained control over memory usage, allowing training to fully reclaim GPU memory between generation steps — a key enabler for models like Qwen2.5-72B.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;DeepSpeed ZeRO Stage 3 is essential for training large models. It allows extremely large networks to fit into memory by distributing model weights, gradients, and optimizer states across multiple GPUs. In our experience, enabling contiguous_gradients helped reduce memory fragmentation, while offloading the optimizer to the CPU freed up critical GPU memory — both of which were especially helpful in colocated setups.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Colocation is powerful but comes with trade-offs. It works best when GPU memory is carefully managed, often requiring manual tuning of memory usage parameters like vllm_gpu_memory_utilization. While it offers clear throughput benefits and reduces idle GPU time, colocation may not be ideal for models with tight memory budgets or when memory fragmentation is not well controlled. When done right, though, it unlocks significant efficiency gains.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;TP/DP compatibility, Accelerate, and torchrun support make deployment seamless. Despite the complexity of the underlying architecture, the entire system can be launched and scaled with standard distributed tools.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Co-located training maintains model quality. Across multiple benchmarks (Math500, AIME24), co-located and plain setups produced comparable results, validating that performance isn’t sacrificed for efficiency.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		✅ Conclusion
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;This blog post explored how co-locating vLLM with GRPO training unlocks significant efficiency gains when training large language models — including models as large as Qwen2.5-72B.&lt;/p&gt;
&lt;p&gt;Traditionally, TRL only supported vLLM in server mode, which required separate processes and GPUs for inference, leading to wasted compute and idle time. With the introduction of vLLM’s external launcher and the colocation PR in TRL PR #3394, we can now run training and inference within the same distributed process group, on the same GPUs, with full support for TP, DP, and Accelerate.&lt;/p&gt;
&lt;p&gt;While challenges remain — such as version-specific vLLM bugs and edge cases such as with sleep() — the overall results show that co-located GRPO is a practical, scalable solution for training large models efficiently. We’re excited to continue refining this setup, integrating features like FSDP, and pushing the limits of large model training — making it faster, cheaper, and more accessible for everyone building the next generation of LLMs.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		✅ Give It a Try!
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Below is an example to try out GRPO training with co-located vLLM.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		📄 &lt;code&gt;train_grpo_colocate.py&lt;/code&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; datasets &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; load_dataset
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; trl &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; GRPOConfig, GRPOTrainer


dataset = load_dataset(&lt;span class="hljs-string"&gt;"trl-lib/tldr"&lt;/span&gt;, split=&lt;span class="hljs-string"&gt;"train"&lt;/span&gt;)


&lt;span class="hljs-keyword"&gt;def&lt;/span&gt; &lt;span class="hljs-title function_"&gt;reward_len&lt;/span&gt;(&lt;span class="hljs-params"&gt;completions, **kwargs&lt;/span&gt;):
    &lt;span class="hljs-keyword"&gt;return&lt;/span&gt; [-&lt;span class="hljs-built_in"&gt;abs&lt;/span&gt;(&lt;span class="hljs-number"&gt;20&lt;/span&gt; - &lt;span class="hljs-built_in"&gt;len&lt;/span&gt;(completion)) &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; completion &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; completions]


training_args = GRPOConfig(
    output_dir=&lt;span class="hljs-string"&gt;"Qwen2-0.5B-GRPO"&lt;/span&gt;,
    logging_steps=&lt;span class="hljs-number"&gt;1&lt;/span&gt;,
    use_vllm=&lt;span class="hljs-literal"&gt;True&lt;/span&gt;,
    vllm_mode=&lt;span class="hljs-string"&gt;"colocate"&lt;/span&gt;,
    vllm_tensor_parallel_size=&lt;span class="hljs-number"&gt;1&lt;/span&gt;,
    vllm_gpu_memory_utilization=&lt;span class="hljs-number"&gt;0.3&lt;/span&gt;,
    max_prompt_length=&lt;span class="hljs-number"&gt;512&lt;/span&gt;,
    max_completion_length=&lt;span class="hljs-number"&gt;1024&lt;/span&gt;,
    max_steps=&lt;span class="hljs-number"&gt;2&lt;/span&gt;,
    num_generations=&lt;span class="hljs-number"&gt;4&lt;/span&gt;,
    num_train_epochs=&lt;span class="hljs-number"&gt;1&lt;/span&gt;,
    per_device_train_batch_size=&lt;span class="hljs-number"&gt;4&lt;/span&gt;,
    push_to_hub=&lt;span class="hljs-literal"&gt;False&lt;/span&gt;,
    report_to=&lt;span class="hljs-literal"&gt;None&lt;/span&gt;
)


trainer = GRPOTrainer(
    model=&lt;span class="hljs-string"&gt;"Qwen/Qwen2-0.5B-Instruct"&lt;/span&gt;,
    reward_funcs=reward_len,
    args=training_args,
    train_dataset=dataset,
)

trainer.train()
&lt;/code&gt;&lt;/pre&gt;
&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/vllm-colocate</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SmolVLA: Efficient Vision-Language-Action Model trained on Lerobot Community Data (Hugging Face - Blog)</title><link>https://huggingface.co/blog/smolvla</link><description>&lt;!-- HTML_TAG_START --&gt;

&lt;p&gt;Today, we introduce SmolVLA, a compact (450M), open-source Vision-Language-Action model for robotics that runs on consumer hardware.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pretrained only on compatibly licensed, open-source community-shared datasets under the lerobot tag.&lt;/li&gt;
&lt;li&gt;SmolVLA-450M outperforms much larger VLAs and strong baselines such as ACT on simulation (LIBERO, Meta-World) and real-world tasks (SO100, SO101).&lt;/li&gt;
&lt;li&gt;Supports &lt;em&gt;asynchronous inference&lt;/em&gt; for &lt;strong&gt;30% faster response&lt;/strong&gt; and &lt;strong&gt;2× task throughput&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Useful links&lt;/strong&gt;:&lt;/p&gt;

&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		📚 Table of Contents
	&lt;/span&gt;
&lt;/h2&gt;

&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Introduction
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Over the past few years, Transformers have driven remarkable progress in AI, from language models capable of human-like reasoning to multimodal systems that understand both images and text. However, in real-world robotics, advancements have been much slower. Robots still struggle to generalize across diverse objects, environments, and tasks. This limited progress stems from a &lt;strong&gt;lack of high-quality, diverse data&lt;/strong&gt; and the absence of models that can &lt;strong&gt;reason and act like humans in the physical world&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In response to these challenges, the field has recently turned to &lt;strong&gt;vision-language-action (VLA) models&lt;/strong&gt;, which aim to unify perception, language understanding, and action prediction within a single architecture. VLAs typically take as input raw visual observations and natural language instructions, and output corresponding robot actions. While promising, much of the recent progress in VLAs remains locked behind proprietary models trained on large-scale private datasets, often requiring costly hardware setups and extensive engineering resources. 
As a result, the broader robotics research community faces significant barriers in reproducing and building upon these models.&lt;/p&gt;
&lt;p&gt;SmolVLA addresses this gap by offering an open-source, compact, and efficient VLA model that can be trained on &lt;strong&gt;consumer-grade hardware using only publicly available datasets&lt;/strong&gt;. By releasing not only model weights but also using very affordable open-source hardware, SmolVLA aims to democratize access to vision-language-action models and accelerate research toward generalist robotic agents. &lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img alt="Comparison of SmolVLA across task variations." src="https://cdn-uploads.huggingface.co/production/uploads/640e21ef3c82bd463ee5a76d/S-3vvVCulChREwHDkquoc.gif" width="500" /&gt;
  &lt;br /&gt;&lt;em&gt;Figure 1: Comparison of SmolVLA across task variations. From left to right: (1) asynchronous pick-place cube counting, (2) synchronous pick-place cube counting, (3) pick-place cube counting under perturbations, and (4) generalization on pick-and-place of the lego block with real-world SO101.&lt;/em&gt;
&lt;/p&gt;

&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Meet SmolVLA!
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;SmolVLA-450M&lt;/strong&gt; is our open-source, compact yet capable VLA model. It is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Small enough to run on CPU, train on a single consumer GPU, or even a MacBook! &lt;/li&gt;
&lt;li&gt;Trained on public, community-shared robotics data&lt;/li&gt;
&lt;li&gt;Released with full training and inference recipes&lt;/li&gt;
&lt;li&gt;Can be tested and deployed on very affordable hardware (SO-100, SO-101, LeKiwi, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Inspired by the training paradigms of Large Language Models (LLMs), SmolVLA goes through a pretraining phase on general manipulation data, followed by task-specific post-training. Architecturally, it combines Transformers with &lt;strong&gt;flow-matching decoders&lt;/strong&gt;, and is optimized for speed and low-latency inference with the following design choices:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Skipping half of the layers of the vision model for faster inference and smaller size&lt;/li&gt;
&lt;li&gt;Interleaving self-attention and cross-attention blocks&lt;/li&gt;
&lt;li&gt;Using fewer visual tokens&lt;/li&gt;
&lt;li&gt;Leveraging smaller pretrained VLMs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Despite using fewer than 30k training episodes—an order of magnitude less than other VLAs—SmolVLA &lt;strong&gt;matches or exceeds the performance&lt;/strong&gt; of much larger models, both in simulation and the real world.&lt;/p&gt;
&lt;p&gt;To make real-time robotics easier to use, we introduce an asynchronous inference stack. This technology separates how robots perform actions from how they understand what they see and hear. Because of this separation, robots can respond more quickly in fast-changing environments.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img alt="SmolVLA architecture." src="https://cdn-uploads.huggingface.co/production/uploads/640e21ef3c82bd463ee5a76d/aooU0a3DMtYmy_1IWMaIM.png" width="500" /&gt;
  &lt;br /&gt;&lt;em&gt;Figure 2. SmolVLA takes as input a sequence of RGB images from multiple cameras, the robot’s current sensorimotor state, and a natural language instruction. The VLM encodes these into contextual features, which condition the action expert to generate a continuous sequence of actions.&lt;/em&gt;
&lt;/p&gt;



&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		🚀 How to Use SmolVLA?
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;SmolVLA is designed to be easy to use and integrate—whether you're finetuning on your own data or plugging it into an existing robotics stack.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Install
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;First, install the required dependencies:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;git clone https://github.com/huggingface/lerobot.git
cd lerobot
pip install -e &lt;span class="hljs-string"&gt;".[smolvla]"&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Finetune the pretrained model
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Use &lt;code&gt;smolvla_base&lt;/code&gt;, our pretrained 450M model, with the lerobot training framework:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;python lerobot/scripts/train.py \
  --policy.path=lerobot/smolvla_base \
  --dataset.repo_id=lerobot/svla_so100_stacking \
  --batch_size=&lt;span class="hljs-number"&gt;64&lt;/span&gt; \
  --steps=&lt;span class="hljs-number"&gt;20000&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Train from scratch
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;​​If you'd like to build from the architecture (pretrained VLM + action expert) rather than a pretrained checkpoint:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;python lerobot/scripts/train.py \
  --policy.&lt;span class="hljs-built_in"&gt;type&lt;/span&gt;=smolvla \
  --dataset.repo_id=lerobot/svla_so100_stacking \
  --batch_size=&lt;span class="hljs-number"&gt;64&lt;/span&gt; \
  --steps=&lt;span class="hljs-number"&gt;200000&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also load &lt;code&gt;SmolVLAPolicy&lt;/code&gt; directly:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; lerobot.common.policies.smolvla.modeling_smolvla &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; SmolVLAPolicy
policy = SmolVLAPolicy.from_pretrained(&lt;span class="hljs-string"&gt;"lerobot/smolvla_base"&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Method
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;SmolVLA is not only a lightweight yet capable model, but also a method for training and evaluating generalist robotics policies. In this section, we introduce the &lt;em&gt;model architecture&lt;/em&gt; behind SmolVLA and the &lt;em&gt;asynchronous inference&lt;/em&gt; setup used for evaluation, which has proven to be more adaptable and capable of faster recovery.&lt;/p&gt;
&lt;p&gt;SmolVLA consists of two core components: a &lt;strong&gt;Vision-Language Model (VLM)&lt;/strong&gt; that processes multimodal inputs and an &lt;strong&gt;action expert&lt;/strong&gt; that outputs robot control commands. Below, we share the details of the main components of SmolVLA architecture and the Asynchronous Inference. More details can be found in our technical report. &lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Main Architecture
	&lt;/span&gt;
&lt;/h3&gt;
&lt;h4 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Vision-Language Model (VLM)
	&lt;/span&gt;
&lt;/h4&gt;
&lt;p&gt;We use SmolVLM2 as our VLM backbone. It’s optimized for multi-image inputs and consists of a SigLIP vision encoder and a SmolLM2 language decoder.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Image tokens&lt;/strong&gt; are extracted via the vision encoder&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Language instructions&lt;/strong&gt; are tokenized and fed directly into the decoder.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sensorimotor states&lt;/strong&gt; are projected into a single token using a linear layer to align with the token dimension of the language model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The decoder layers process concatenated image, language, and state tokens. The resulting features are then passed to the action expert.&lt;/p&gt;
&lt;h4 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Action Expert: Flow Matching Transformer
	&lt;/span&gt;
&lt;/h4&gt;
&lt;p&gt;SmolVLA’s &lt;strong&gt;action expert&lt;/strong&gt; is a compact transformer (~100M parameters) that generates action chunks, i.e. sequences of future robot actions, conditioned on the VLM’s outputs. It is trained using a &lt;strong&gt;flow matching objective&lt;/strong&gt;, which teaches the model to guide noisy samples back to the ground truth. In contrast, while discrete action representations (e.g., via tokenization) are powerful, they often require autoregressive decoding, which is slow and inefficient at inference time. While flow matching allows &lt;strong&gt;direct, non-autoregressive prediction of continuous actions&lt;/strong&gt;, enabling real-time control with high precision.&lt;/p&gt;
&lt;p&gt;More intuitively, during training, we add random noise to the robot’s real action sequences and ask the model to predict the “correction vector” that brings them back to the correct trajectory. This forms a smooth vector field over the action space, helping the model learn accurate and stable control policies.  &lt;/p&gt;
&lt;p&gt;We implement this using a transformer architecture with &lt;strong&gt;interleaved attention blocks&lt;/strong&gt; (see the figure 2), and reduce its hidden size to &lt;strong&gt;75% of the VLM’s&lt;/strong&gt;, keeping the model lightweight for deployment.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Design Choices for Efficiency and Robustness
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;While combining a vision-language model with an action prediction module is a common design pattern in recent VLA systems—such as Pi0, GR00T, Diffusion Policy — we identified several architectural choices that significantly enhance the robustness and performance. In SmolVLA, we apply three key techniques: &lt;strong&gt;reducing the number of visual tokens, skipping upper layers in the VLM&lt;/strong&gt;, and &lt;strong&gt;interleaving cross- and self-attention layers&lt;/strong&gt; in the action expert.&lt;/p&gt;
&lt;h4 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Visual Token Reduction
	&lt;/span&gt;
&lt;/h4&gt;
&lt;p&gt;High-resolution images improve perception but can significantly slow down inference. To strike a balance, &lt;strong&gt;SmolVLA limits the number of visual tokens to 64 per frame&lt;/strong&gt; during both training and inference. For example, a 512×512 image is compressed into just 64 tokens, &lt;strong&gt;instead of 1024&lt;/strong&gt;, using &lt;strong&gt;PixelShuffle&lt;/strong&gt; as an efficient shuffling technique. While the underlying Vision-Language Model (VLM) was originally pretrained using image tiling for broader coverage, &lt;strong&gt;SmolVLA uses only the global image at runtime&lt;/strong&gt; to keep inference lightweight and fast.&lt;/p&gt;
&lt;h4 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Faster Inference via Layer Skipping
	&lt;/span&gt;
&lt;/h4&gt;
&lt;p&gt;Rather than always relying on the final layer of the VLM—which can be expensive and sometimes suboptimal—we use &lt;strong&gt;features from intermediate layers&lt;/strong&gt;. Prior work has shown that early layers often provide better representations for downstream tasks.
In SmolVLA, the action expert only attends to VLM features up to a configurable layer NN during training, set to &lt;strong&gt;half the total layers&lt;/strong&gt;. This &lt;strong&gt;halves the compute cost&lt;/strong&gt; of both the VLM and the action expert, significantly speeding up inference with minimal performance loss.&lt;/p&gt;
&lt;h4 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Interleaved Cross and Self-Attention
	&lt;/span&gt;
&lt;/h4&gt;
&lt;p&gt;Inside the action expert, attention layers alternate between:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cross-attention (CA)&lt;/strong&gt;, where action tokens attend to the VLM’s features&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Self-attention (SA)&lt;/strong&gt;, where action tokens attend to each other (causally—only to the past)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We found that this &lt;strong&gt;interleaved design&lt;/strong&gt; is both lighter and more effective than using full attention blocks. Models that rely only on CA or only on SA tend to sacrifice either smoothness or grounding.&lt;/p&gt;
&lt;p&gt;In SmolVLA, CA ensures that actions are well-conditioned on perception and instructions, while SA improves &lt;strong&gt;temporal smoothness&lt;/strong&gt;—especially critical for real-world control, where jittery predictions can result in unsafe or unstable behavior.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Asynchronous Inference
	&lt;/span&gt;
&lt;/h2&gt;
&lt;div align="center"&gt;
  &lt;img alt="Asynchronous inference" src="https://cdn-uploads.huggingface.co/production/uploads/640e21ef3c82bd463ee5a76d/IV6vxVHCxUuYMEc7otXtv.png" width="500" /&gt;
  &lt;p&gt;Figure 3. Asynchronous inference. Illustration of the asynchronous inference stack. Note that the policy can be run on a remote server, possibly with GPUs.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Modern visuomotor policies output &lt;strong&gt;action chunks&lt;/strong&gt;—sequences of actions to execute. There are two ways to manage them:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Synchronous (sync):&lt;/strong&gt; The robot executes a chunk, then pauses while the next one is computed. Simple, but causes a delay where the robot can't react to new inputs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Asynchronous (async):&lt;/strong&gt; While executing the current chunk, the robot already sends the latest observation to a &lt;strong&gt;Policy Server&lt;/strong&gt; (possibly hosted on GPU) for the next chunk. This avoids idle time and improves reactivity.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our async stack decouples action execution from chunk prediction, resulting in higher adaptability, and the complete lack of execution lags at runtime. It relies on the following key mechanisms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;1. Early trigger:&lt;/strong&gt; When the queue length falls below a threshold (e.g., 70%), we send an observation to a &lt;strong&gt;Policy Server&lt;/strong&gt;, calling for a new action chunk.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2. Decoupled threads:&lt;/strong&gt; Control loop keeps executing → inference happens in parallel (non-blocking).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;3. Chunk fusion:&lt;/strong&gt; Overlapping actions from successive chunks are stitched with a simple merge rule to avoid jitter.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are really excited about releasing asynchronous inference because it guarantees greater adaptability and improved performance without changing the model. In short, async inference keeps the robot responsive by overlapping execution and remote prediction.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
	&lt;span&gt;
		Community Datasets
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;While vision and language models thrive on web-scale datasets like LAION, ImageNet, and Common Crawl, robotics lacks a comparable resource. There’s no “Internet of robots.” Instead, data is fragmented across robot types, sensors, control schemes, and formats—forming disconnected "data islands". In our previous post, we explored how this fragmentation could be resolved through open, collaborative efforts. Just as ImageNet catalyzed breakthroughs in computer vision by providing a large, diverse benchmark, we believe that &lt;strong&gt;community-driven robotics datasets&lt;/strong&gt; can play the same foundational role for generalist robot policies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SmolVLA is our first step toward that vision&lt;/strong&gt;: It is pretrained on a curated mix of publicly available, community-contributed datasets designed to reflect real-world variation. Rather than optimizing for dataset size alone, we focus on diversity: a range of behaviors, camera viewpoints, and embodiments that promote transfer and generalization.&lt;/p&gt;
&lt;p&gt;All training data used in SmolVLA comes from &lt;strong&gt;LeRobot Community Datasets&lt;/strong&gt; , robotics  datasets shared on the Hugging Face Hub under the &lt;code&gt;lerobot&lt;/code&gt; tag. Collected in diverse settings, from labs to living rooms, these datasets represent an open, decentralized effort to scale real-world robot data.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img alt="A glimpse of the community dataset." src="https://cdn-uploads.huggingface.co/production/uploads/640e21ef3c82bd463ee5a76d/V4QU-B-6YBONb-8K_lSpj.gif" width="500" /&gt;
  &lt;br /&gt;&lt;em&gt;Figure 4. A glimpse of the community dataset. Special thanks to Ville Kuosmanen for creating the visualization.
Unlike academic benchmarks, community datasets naturally capture messy, realistic interactions: varied lighting, suboptimal demonstrations, unconventional objects, and heterogeneous control schemes. This kind of diversity will be very useful for learning robust, general-purpose representations.&lt;/em&gt;
&lt;/p&gt;


&lt;p&gt;We used a customfiltering tool  created by Alexandre Chapin and Ville Kuosmanen to select datasets based on frame count, visual quality, and task coverage. After a meticulous manual review (special thanks to Marina Barannikov), we curated a collection of &lt;strong&gt;487 high-quality datasets&lt;/strong&gt; focused on the &lt;strong&gt;SO100 robotic arm&lt;/strong&gt;, standardized at &lt;strong&gt;30 FPS&lt;/strong&gt;. This yielded around &lt;strong&gt;10 million frames&lt;/strong&gt;—at least &lt;strong&gt;one order of magnitude smaller&lt;/strong&gt; than other popular benchmark datasets, yet significantly more diverse.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Improving Task Annotations
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;A common issue across community datasets was noisy or missing task descriptions. Many episodes lacked annotations or included vague labels like “task desc” or “Move”, “Pick”. To improve quality and standardize the textual input across datasets, we used Qwen2.5-VL-3B-Instruct to generate concise, action-oriented descriptions.&lt;/p&gt;
&lt;p&gt;Given sample frames and the original label, the model was prompted to rewrite the instruction in under 30 characters, starting with an action verb (e.g., “Pick,” “Place,” “Open”). &lt;/p&gt;
&lt;p&gt;The prompt used is: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Here is a current task description: {current_task}. Generate a very short, clear, and complete one-sentence describing the action performed by the robot arm (max 30 characters). Do not include unnecessary words.
Be concise.
Here are some examples: Pick up the cube and place it in the box, open the drawer and so on.
Start directly with an action verb like “Pick”, “Place”, “Open”, etc.
Similar to the provided examples, what is the main action done by the robot arm?
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Standardizing Camera Views
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Another challenge was inconsistent camera naming. Some datasets used clear names like top or &lt;code&gt;wrist.right&lt;/code&gt;, while others used ambiguous labels like &lt;code&gt;images.laptop&lt;/code&gt;, which varied in meaning.
To fix this, we manually went through the datasets and mapped each camera view to a standardized scheme:
&lt;code&gt;OBS_IMAGE_1&lt;/code&gt;: Top-down view
&lt;code&gt;OBS_IMAGE_2&lt;/code&gt;: Wrist-mounted view
&lt;code&gt;OBS_IMAGE_3+&lt;/code&gt;: Additional viewpoints&lt;/p&gt;
&lt;p&gt;We further isolate the contributions of community dataset pretraining and multitask finetuning. Without pretraining on the LeRobot community datasets, SmolVLA initially achieves &lt;strong&gt;51.7%&lt;/strong&gt; success on SO100. After pretraining on community-collected data, performance jumps to &lt;strong&gt;78.3%&lt;/strong&gt;, a &lt;strong&gt;+26.6% absolute improvement&lt;/strong&gt;. Multitask finetuning further boosts performance, showing strong task transfer capabilities even in low-data regimes.&lt;/p&gt;
&lt;div align="center"&gt;
  &lt;img src="https://cdn-uploads.huggingface.co/production/uploads/640e21ef3c82bd463ee5a76d/GdKdSzT2oAt83MQ0lPjcY.png" width="500" /&gt;
  &lt;p&gt; Table 1. Impact of Pretraining on Community Datasets and Multitask Finetuning.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Results
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We evaluate SmolVLA across simulation and real-world benchmarks to test its generalization, efficiency, and robustness. Despite being compact, It consistently outperforms or matches the performance of significantly larger models and policies pretrained on higher-scale robotics data. &lt;/p&gt;
&lt;div align="center"&gt;
  &lt;img alt="SmolVLA Performance on Simulation Benchmarks." src="https://cdn-uploads.huggingface.co/production/uploads/640e21ef3c82bd463ee5a76d/_v01LDKSy8zgcYr_7yQMx.png" width="500" /&gt;
  &lt;p&gt; Table 2. SmolVLA Performance on Simulation Benchmarks.&lt;/p&gt;
&lt;/div&gt;


&lt;div align="center"&gt;
  &lt;img alt="SmolVLA vs Baselines on Real-World Tasks (SO100)." src="https://cdn-uploads.huggingface.co/production/uploads/640e21ef3c82bd463ee5a76d/ahQpohnpqRw6sQFMzjmg4.png" width="500" /&gt;
  &lt;p&gt; Table 3. SmolVLA vs Baselines on Real-World Tasks (SO100).&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;In real-world settings, SmolVLA is evaluated on two diverse suites: SO100 and SO101. These tasks include pick-place, stacking, and sorting, with both in-distribution and out-of-distribution object configurations. 
On SO101, SmolVLA also excels in generalization:&lt;/p&gt;
&lt;div align="center"&gt;
  &lt;img alt="Generalization of SmolVLA to New Embodiment (SO101) vs ACT.." src="https://cdn-uploads.huggingface.co/production/uploads/640e21ef3c82bd463ee5a76d/MZuG6UzXZ1SJ1MOfUfyzb.png" width="500" /&gt;
  &lt;p&gt;Table 4. Generalization of SmolVLA to New Embodiment (SO101) vs ACT..&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Finally, we evaluate SmolVLA under synchronous and asynchronous inference modes. Async inference decouples action execution from model inference, allowing the policy to react while the robot is moving.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Both modes achieve similar task success (≈78%), but async inference:&lt;ul&gt;
&lt;li&gt;Completes tasks &lt;strong&gt;~30% faster&lt;/strong&gt; (9.7s vs. 13.75s)&lt;/li&gt;
&lt;li&gt;Enables &lt;strong&gt;2× more completions&lt;/strong&gt; in fixed-time settings (19 vs. 9 cubes)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This results in more responsive and robust real-world performance, especially in dynamic environments with shifting objects or external disturbances.&lt;/p&gt;
&lt;div align="center"&gt;
  &lt;img alt="Asynchronous vs. Synchronous Inference in Real-World Tasks." src="https://cdn-uploads.huggingface.co/production/uploads/640e21ef3c82bd463ee5a76d/Goxb9y5cE_Ty1SWCetCoT.png" width="500" /&gt;
  &lt;p&gt;Figure 5. Asynchronous vs. Synchronous Inference in Real-World Tasks.
(a) Task success rates (%), (b) average completion time(s), and (c) number of tasks completed within a fixed time window.
&lt;/p&gt;
&lt;/div&gt;


&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Conclusion
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;SmolVLA is our contribution to building robotics foundation models that are open, efficient, and reproducible. Despite its small size, it matches or outperforms larger, proprietary models across a range of real-world and simulated tasks. By relying solely on community-contributed datasets and affordable hardware, SmolVLA lowers the barrier to entry for researchers, educators, and hobbyists alike.
But this is just the beginning. SmolVLA is more than just a model — it's part of a growing open-source movement toward scalable, collaborative robotics.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Call to Action:
	&lt;/span&gt;
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;🔧 &lt;strong&gt;Try it out!&lt;/strong&gt; Finetune SmolVLA on your own data, deploy it on affordable hardware, or benchmark it against your current stack and share it on twitter/linkedin.&lt;/li&gt;
&lt;li&gt;🤖 &lt;strong&gt;Upload the dataset!&lt;/strong&gt; Got a robot? Collect and share your data using the lerobot format. Help expand the community dataset that powers SmolVLA.&lt;/li&gt;
&lt;li&gt;💬 &lt;strong&gt;Join the blog discussion.&lt;/strong&gt; Drop your questions, ideas, or feedback in the discussion below. We’re happy to help with integration, training, or deployment.&lt;/li&gt;
&lt;li&gt;📊 &lt;strong&gt;Contribute.&lt;/strong&gt; Improve datasets, report issues, suggest new ideas. Every contribution helps.&lt;/li&gt;
&lt;li&gt;🌍 &lt;strong&gt;Spread the word.&lt;/strong&gt; Share SmolVLA with fellow researchers, developers, or educators interested in efficient, real-time robotic policies.&lt;/li&gt;
&lt;li&gt;📫 &lt;strong&gt;Stay in touch:&lt;/strong&gt; Follow the LeRobot organization and Discord server for updates, tutorials, and new releases.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Together, we can make real-world robotics more capable, more affordable, and more open. ✨&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;!-- HTML_TAG_START --&gt;

&lt;p&gt;Today, we introduce SmolVLA, a compact (450M), open-source Vision-Language-Action model for robotics that runs on consumer hardware.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pretrained only on compatibly licensed, open-source community-shared datasets under the lerobot tag.&lt;/li&gt;
&lt;li&gt;SmolVLA-450M outperforms much larger VLAs and strong baselines such as ACT on simulation (LIBERO, Meta-World) and real-world tasks (SO100, SO101).&lt;/li&gt;
&lt;li&gt;Supports &lt;em&gt;asynchronous inference&lt;/em&gt; for &lt;strong&gt;30% faster response&lt;/strong&gt; and &lt;strong&gt;2× task throughput&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Useful links&lt;/strong&gt;:&lt;/p&gt;

&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		📚 Table of Contents
	&lt;/span&gt;
&lt;/h2&gt;

&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Introduction
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Over the past few years, Transformers have driven remarkable progress in AI, from language models capable of human-like reasoning to multimodal systems that understand both images and text. However, in real-world robotics, advancements have been much slower. Robots still struggle to generalize across diverse objects, environments, and tasks. This limited progress stems from a &lt;strong&gt;lack of high-quality, diverse data&lt;/strong&gt; and the absence of models that can &lt;strong&gt;reason and act like humans in the physical world&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In response to these challenges, the field has recently turned to &lt;strong&gt;vision-language-action (VLA) models&lt;/strong&gt;, which aim to unify perception, language understanding, and action prediction within a single architecture. VLAs typically take as input raw visual observations and natural language instructions, and output corresponding robot actions. While promising, much of the recent progress in VLAs remains locked behind proprietary models trained on large-scale private datasets, often requiring costly hardware setups and extensive engineering resources. 
As a result, the broader robotics research community faces significant barriers in reproducing and building upon these models.&lt;/p&gt;
&lt;p&gt;SmolVLA addresses this gap by offering an open-source, compact, and efficient VLA model that can be trained on &lt;strong&gt;consumer-grade hardware using only publicly available datasets&lt;/strong&gt;. By releasing not only model weights but also using very affordable open-source hardware, SmolVLA aims to democratize access to vision-language-action models and accelerate research toward generalist robotic agents. &lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img alt="Comparison of SmolVLA across task variations." src="https://cdn-uploads.huggingface.co/production/uploads/640e21ef3c82bd463ee5a76d/S-3vvVCulChREwHDkquoc.gif" width="500" /&gt;
  &lt;br /&gt;&lt;em&gt;Figure 1: Comparison of SmolVLA across task variations. From left to right: (1) asynchronous pick-place cube counting, (2) synchronous pick-place cube counting, (3) pick-place cube counting under perturbations, and (4) generalization on pick-and-place of the lego block with real-world SO101.&lt;/em&gt;
&lt;/p&gt;

&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Meet SmolVLA!
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;SmolVLA-450M&lt;/strong&gt; is our open-source, compact yet capable VLA model. It is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Small enough to run on CPU, train on a single consumer GPU, or even a MacBook! &lt;/li&gt;
&lt;li&gt;Trained on public, community-shared robotics data&lt;/li&gt;
&lt;li&gt;Released with full training and inference recipes&lt;/li&gt;
&lt;li&gt;Can be tested and deployed on very affordable hardware (SO-100, SO-101, LeKiwi, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Inspired by the training paradigms of Large Language Models (LLMs), SmolVLA goes through a pretraining phase on general manipulation data, followed by task-specific post-training. Architecturally, it combines Transformers with &lt;strong&gt;flow-matching decoders&lt;/strong&gt;, and is optimized for speed and low-latency inference with the following design choices:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Skipping half of the layers of the vision model for faster inference and smaller size&lt;/li&gt;
&lt;li&gt;Interleaving self-attention and cross-attention blocks&lt;/li&gt;
&lt;li&gt;Using fewer visual tokens&lt;/li&gt;
&lt;li&gt;Leveraging smaller pretrained VLMs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Despite using fewer than 30k training episodes—an order of magnitude less than other VLAs—SmolVLA &lt;strong&gt;matches or exceeds the performance&lt;/strong&gt; of much larger models, both in simulation and the real world.&lt;/p&gt;
&lt;p&gt;To make real-time robotics easier to use, we introduce an asynchronous inference stack. This technology separates how robots perform actions from how they understand what they see and hear. Because of this separation, robots can respond more quickly in fast-changing environments.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img alt="SmolVLA architecture." src="https://cdn-uploads.huggingface.co/production/uploads/640e21ef3c82bd463ee5a76d/aooU0a3DMtYmy_1IWMaIM.png" width="500" /&gt;
  &lt;br /&gt;&lt;em&gt;Figure 2. SmolVLA takes as input a sequence of RGB images from multiple cameras, the robot’s current sensorimotor state, and a natural language instruction. The VLM encodes these into contextual features, which condition the action expert to generate a continuous sequence of actions.&lt;/em&gt;
&lt;/p&gt;



&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		🚀 How to Use SmolVLA?
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;SmolVLA is designed to be easy to use and integrate—whether you're finetuning on your own data or plugging it into an existing robotics stack.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Install
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;First, install the required dependencies:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;git clone https://github.com/huggingface/lerobot.git
cd lerobot
pip install -e &lt;span class="hljs-string"&gt;".[smolvla]"&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Finetune the pretrained model
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Use &lt;code&gt;smolvla_base&lt;/code&gt;, our pretrained 450M model, with the lerobot training framework:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;python lerobot/scripts/train.py \
  --policy.path=lerobot/smolvla_base \
  --dataset.repo_id=lerobot/svla_so100_stacking \
  --batch_size=&lt;span class="hljs-number"&gt;64&lt;/span&gt; \
  --steps=&lt;span class="hljs-number"&gt;20000&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Train from scratch
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;​​If you'd like to build from the architecture (pretrained VLM + action expert) rather than a pretrained checkpoint:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;python lerobot/scripts/train.py \
  --policy.&lt;span class="hljs-built_in"&gt;type&lt;/span&gt;=smolvla \
  --dataset.repo_id=lerobot/svla_so100_stacking \
  --batch_size=&lt;span class="hljs-number"&gt;64&lt;/span&gt; \
  --steps=&lt;span class="hljs-number"&gt;200000&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also load &lt;code&gt;SmolVLAPolicy&lt;/code&gt; directly:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; lerobot.common.policies.smolvla.modeling_smolvla &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; SmolVLAPolicy
policy = SmolVLAPolicy.from_pretrained(&lt;span class="hljs-string"&gt;"lerobot/smolvla_base"&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Method
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;SmolVLA is not only a lightweight yet capable model, but also a method for training and evaluating generalist robotics policies. In this section, we introduce the &lt;em&gt;model architecture&lt;/em&gt; behind SmolVLA and the &lt;em&gt;asynchronous inference&lt;/em&gt; setup used for evaluation, which has proven to be more adaptable and capable of faster recovery.&lt;/p&gt;
&lt;p&gt;SmolVLA consists of two core components: a &lt;strong&gt;Vision-Language Model (VLM)&lt;/strong&gt; that processes multimodal inputs and an &lt;strong&gt;action expert&lt;/strong&gt; that outputs robot control commands. Below, we share the details of the main components of SmolVLA architecture and the Asynchronous Inference. More details can be found in our technical report. &lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Main Architecture
	&lt;/span&gt;
&lt;/h3&gt;
&lt;h4 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Vision-Language Model (VLM)
	&lt;/span&gt;
&lt;/h4&gt;
&lt;p&gt;We use SmolVLM2 as our VLM backbone. It’s optimized for multi-image inputs and consists of a SigLIP vision encoder and a SmolLM2 language decoder.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Image tokens&lt;/strong&gt; are extracted via the vision encoder&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Language instructions&lt;/strong&gt; are tokenized and fed directly into the decoder.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sensorimotor states&lt;/strong&gt; are projected into a single token using a linear layer to align with the token dimension of the language model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The decoder layers process concatenated image, language, and state tokens. The resulting features are then passed to the action expert.&lt;/p&gt;
&lt;h4 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Action Expert: Flow Matching Transformer
	&lt;/span&gt;
&lt;/h4&gt;
&lt;p&gt;SmolVLA’s &lt;strong&gt;action expert&lt;/strong&gt; is a compact transformer (~100M parameters) that generates action chunks, i.e. sequences of future robot actions, conditioned on the VLM’s outputs. It is trained using a &lt;strong&gt;flow matching objective&lt;/strong&gt;, which teaches the model to guide noisy samples back to the ground truth. In contrast, while discrete action representations (e.g., via tokenization) are powerful, they often require autoregressive decoding, which is slow and inefficient at inference time. While flow matching allows &lt;strong&gt;direct, non-autoregressive prediction of continuous actions&lt;/strong&gt;, enabling real-time control with high precision.&lt;/p&gt;
&lt;p&gt;More intuitively, during training, we add random noise to the robot’s real action sequences and ask the model to predict the “correction vector” that brings them back to the correct trajectory. This forms a smooth vector field over the action space, helping the model learn accurate and stable control policies.  &lt;/p&gt;
&lt;p&gt;We implement this using a transformer architecture with &lt;strong&gt;interleaved attention blocks&lt;/strong&gt; (see the figure 2), and reduce its hidden size to &lt;strong&gt;75% of the VLM’s&lt;/strong&gt;, keeping the model lightweight for deployment.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Design Choices for Efficiency and Robustness
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;While combining a vision-language model with an action prediction module is a common design pattern in recent VLA systems—such as Pi0, GR00T, Diffusion Policy — we identified several architectural choices that significantly enhance the robustness and performance. In SmolVLA, we apply three key techniques: &lt;strong&gt;reducing the number of visual tokens, skipping upper layers in the VLM&lt;/strong&gt;, and &lt;strong&gt;interleaving cross- and self-attention layers&lt;/strong&gt; in the action expert.&lt;/p&gt;
&lt;h4 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Visual Token Reduction
	&lt;/span&gt;
&lt;/h4&gt;
&lt;p&gt;High-resolution images improve perception but can significantly slow down inference. To strike a balance, &lt;strong&gt;SmolVLA limits the number of visual tokens to 64 per frame&lt;/strong&gt; during both training and inference. For example, a 512×512 image is compressed into just 64 tokens, &lt;strong&gt;instead of 1024&lt;/strong&gt;, using &lt;strong&gt;PixelShuffle&lt;/strong&gt; as an efficient shuffling technique. While the underlying Vision-Language Model (VLM) was originally pretrained using image tiling for broader coverage, &lt;strong&gt;SmolVLA uses only the global image at runtime&lt;/strong&gt; to keep inference lightweight and fast.&lt;/p&gt;
&lt;h4 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Faster Inference via Layer Skipping
	&lt;/span&gt;
&lt;/h4&gt;
&lt;p&gt;Rather than always relying on the final layer of the VLM—which can be expensive and sometimes suboptimal—we use &lt;strong&gt;features from intermediate layers&lt;/strong&gt;. Prior work has shown that early layers often provide better representations for downstream tasks.
In SmolVLA, the action expert only attends to VLM features up to a configurable layer NN during training, set to &lt;strong&gt;half the total layers&lt;/strong&gt;. This &lt;strong&gt;halves the compute cost&lt;/strong&gt; of both the VLM and the action expert, significantly speeding up inference with minimal performance loss.&lt;/p&gt;
&lt;h4 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Interleaved Cross and Self-Attention
	&lt;/span&gt;
&lt;/h4&gt;
&lt;p&gt;Inside the action expert, attention layers alternate between:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cross-attention (CA)&lt;/strong&gt;, where action tokens attend to the VLM’s features&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Self-attention (SA)&lt;/strong&gt;, where action tokens attend to each other (causally—only to the past)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We found that this &lt;strong&gt;interleaved design&lt;/strong&gt; is both lighter and more effective than using full attention blocks. Models that rely only on CA or only on SA tend to sacrifice either smoothness or grounding.&lt;/p&gt;
&lt;p&gt;In SmolVLA, CA ensures that actions are well-conditioned on perception and instructions, while SA improves &lt;strong&gt;temporal smoothness&lt;/strong&gt;—especially critical for real-world control, where jittery predictions can result in unsafe or unstable behavior.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Asynchronous Inference
	&lt;/span&gt;
&lt;/h2&gt;
&lt;div align="center"&gt;
  &lt;img alt="Asynchronous inference" src="https://cdn-uploads.huggingface.co/production/uploads/640e21ef3c82bd463ee5a76d/IV6vxVHCxUuYMEc7otXtv.png" width="500" /&gt;
  &lt;p&gt;Figure 3. Asynchronous inference. Illustration of the asynchronous inference stack. Note that the policy can be run on a remote server, possibly with GPUs.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Modern visuomotor policies output &lt;strong&gt;action chunks&lt;/strong&gt;—sequences of actions to execute. There are two ways to manage them:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Synchronous (sync):&lt;/strong&gt; The robot executes a chunk, then pauses while the next one is computed. Simple, but causes a delay where the robot can't react to new inputs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Asynchronous (async):&lt;/strong&gt; While executing the current chunk, the robot already sends the latest observation to a &lt;strong&gt;Policy Server&lt;/strong&gt; (possibly hosted on GPU) for the next chunk. This avoids idle time and improves reactivity.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our async stack decouples action execution from chunk prediction, resulting in higher adaptability, and the complete lack of execution lags at runtime. It relies on the following key mechanisms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;1. Early trigger:&lt;/strong&gt; When the queue length falls below a threshold (e.g., 70%), we send an observation to a &lt;strong&gt;Policy Server&lt;/strong&gt;, calling for a new action chunk.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2. Decoupled threads:&lt;/strong&gt; Control loop keeps executing → inference happens in parallel (non-blocking).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;3. Chunk fusion:&lt;/strong&gt; Overlapping actions from successive chunks are stitched with a simple merge rule to avoid jitter.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are really excited about releasing asynchronous inference because it guarantees greater adaptability and improved performance without changing the model. In short, async inference keeps the robot responsive by overlapping execution and remote prediction.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
	&lt;span&gt;
		Community Datasets
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;While vision and language models thrive on web-scale datasets like LAION, ImageNet, and Common Crawl, robotics lacks a comparable resource. There’s no “Internet of robots.” Instead, data is fragmented across robot types, sensors, control schemes, and formats—forming disconnected "data islands". In our previous post, we explored how this fragmentation could be resolved through open, collaborative efforts. Just as ImageNet catalyzed breakthroughs in computer vision by providing a large, diverse benchmark, we believe that &lt;strong&gt;community-driven robotics datasets&lt;/strong&gt; can play the same foundational role for generalist robot policies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SmolVLA is our first step toward that vision&lt;/strong&gt;: It is pretrained on a curated mix of publicly available, community-contributed datasets designed to reflect real-world variation. Rather than optimizing for dataset size alone, we focus on diversity: a range of behaviors, camera viewpoints, and embodiments that promote transfer and generalization.&lt;/p&gt;
&lt;p&gt;All training data used in SmolVLA comes from &lt;strong&gt;LeRobot Community Datasets&lt;/strong&gt; , robotics  datasets shared on the Hugging Face Hub under the &lt;code&gt;lerobot&lt;/code&gt; tag. Collected in diverse settings, from labs to living rooms, these datasets represent an open, decentralized effort to scale real-world robot data.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img alt="A glimpse of the community dataset." src="https://cdn-uploads.huggingface.co/production/uploads/640e21ef3c82bd463ee5a76d/V4QU-B-6YBONb-8K_lSpj.gif" width="500" /&gt;
  &lt;br /&gt;&lt;em&gt;Figure 4. A glimpse of the community dataset. Special thanks to Ville Kuosmanen for creating the visualization.
Unlike academic benchmarks, community datasets naturally capture messy, realistic interactions: varied lighting, suboptimal demonstrations, unconventional objects, and heterogeneous control schemes. This kind of diversity will be very useful for learning robust, general-purpose representations.&lt;/em&gt;
&lt;/p&gt;


&lt;p&gt;We used a customfiltering tool  created by Alexandre Chapin and Ville Kuosmanen to select datasets based on frame count, visual quality, and task coverage. After a meticulous manual review (special thanks to Marina Barannikov), we curated a collection of &lt;strong&gt;487 high-quality datasets&lt;/strong&gt; focused on the &lt;strong&gt;SO100 robotic arm&lt;/strong&gt;, standardized at &lt;strong&gt;30 FPS&lt;/strong&gt;. This yielded around &lt;strong&gt;10 million frames&lt;/strong&gt;—at least &lt;strong&gt;one order of magnitude smaller&lt;/strong&gt; than other popular benchmark datasets, yet significantly more diverse.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Improving Task Annotations
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;A common issue across community datasets was noisy or missing task descriptions. Many episodes lacked annotations or included vague labels like “task desc” or “Move”, “Pick”. To improve quality and standardize the textual input across datasets, we used Qwen2.5-VL-3B-Instruct to generate concise, action-oriented descriptions.&lt;/p&gt;
&lt;p&gt;Given sample frames and the original label, the model was prompted to rewrite the instruction in under 30 characters, starting with an action verb (e.g., “Pick,” “Place,” “Open”). &lt;/p&gt;
&lt;p&gt;The prompt used is: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Here is a current task description: {current_task}. Generate a very short, clear, and complete one-sentence describing the action performed by the robot arm (max 30 characters). Do not include unnecessary words.
Be concise.
Here are some examples: Pick up the cube and place it in the box, open the drawer and so on.
Start directly with an action verb like “Pick”, “Place”, “Open”, etc.
Similar to the provided examples, what is the main action done by the robot arm?
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Standardizing Camera Views
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Another challenge was inconsistent camera naming. Some datasets used clear names like top or &lt;code&gt;wrist.right&lt;/code&gt;, while others used ambiguous labels like &lt;code&gt;images.laptop&lt;/code&gt;, which varied in meaning.
To fix this, we manually went through the datasets and mapped each camera view to a standardized scheme:
&lt;code&gt;OBS_IMAGE_1&lt;/code&gt;: Top-down view
&lt;code&gt;OBS_IMAGE_2&lt;/code&gt;: Wrist-mounted view
&lt;code&gt;OBS_IMAGE_3+&lt;/code&gt;: Additional viewpoints&lt;/p&gt;
&lt;p&gt;We further isolate the contributions of community dataset pretraining and multitask finetuning. Without pretraining on the LeRobot community datasets, SmolVLA initially achieves &lt;strong&gt;51.7%&lt;/strong&gt; success on SO100. After pretraining on community-collected data, performance jumps to &lt;strong&gt;78.3%&lt;/strong&gt;, a &lt;strong&gt;+26.6% absolute improvement&lt;/strong&gt;. Multitask finetuning further boosts performance, showing strong task transfer capabilities even in low-data regimes.&lt;/p&gt;
&lt;div align="center"&gt;
  &lt;img src="https://cdn-uploads.huggingface.co/production/uploads/640e21ef3c82bd463ee5a76d/GdKdSzT2oAt83MQ0lPjcY.png" width="500" /&gt;
  &lt;p&gt; Table 1. Impact of Pretraining on Community Datasets and Multitask Finetuning.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Results
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We evaluate SmolVLA across simulation and real-world benchmarks to test its generalization, efficiency, and robustness. Despite being compact, It consistently outperforms or matches the performance of significantly larger models and policies pretrained on higher-scale robotics data. &lt;/p&gt;
&lt;div align="center"&gt;
  &lt;img alt="SmolVLA Performance on Simulation Benchmarks." src="https://cdn-uploads.huggingface.co/production/uploads/640e21ef3c82bd463ee5a76d/_v01LDKSy8zgcYr_7yQMx.png" width="500" /&gt;
  &lt;p&gt; Table 2. SmolVLA Performance on Simulation Benchmarks.&lt;/p&gt;
&lt;/div&gt;


&lt;div align="center"&gt;
  &lt;img alt="SmolVLA vs Baselines on Real-World Tasks (SO100)." src="https://cdn-uploads.huggingface.co/production/uploads/640e21ef3c82bd463ee5a76d/ahQpohnpqRw6sQFMzjmg4.png" width="500" /&gt;
  &lt;p&gt; Table 3. SmolVLA vs Baselines on Real-World Tasks (SO100).&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;In real-world settings, SmolVLA is evaluated on two diverse suites: SO100 and SO101. These tasks include pick-place, stacking, and sorting, with both in-distribution and out-of-distribution object configurations. 
On SO101, SmolVLA also excels in generalization:&lt;/p&gt;
&lt;div align="center"&gt;
  &lt;img alt="Generalization of SmolVLA to New Embodiment (SO101) vs ACT.." src="https://cdn-uploads.huggingface.co/production/uploads/640e21ef3c82bd463ee5a76d/MZuG6UzXZ1SJ1MOfUfyzb.png" width="500" /&gt;
  &lt;p&gt;Table 4. Generalization of SmolVLA to New Embodiment (SO101) vs ACT..&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Finally, we evaluate SmolVLA under synchronous and asynchronous inference modes. Async inference decouples action execution from model inference, allowing the policy to react while the robot is moving.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Both modes achieve similar task success (≈78%), but async inference:&lt;ul&gt;
&lt;li&gt;Completes tasks &lt;strong&gt;~30% faster&lt;/strong&gt; (9.7s vs. 13.75s)&lt;/li&gt;
&lt;li&gt;Enables &lt;strong&gt;2× more completions&lt;/strong&gt; in fixed-time settings (19 vs. 9 cubes)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This results in more responsive and robust real-world performance, especially in dynamic environments with shifting objects or external disturbances.&lt;/p&gt;
&lt;div align="center"&gt;
  &lt;img alt="Asynchronous vs. Synchronous Inference in Real-World Tasks." src="https://cdn-uploads.huggingface.co/production/uploads/640e21ef3c82bd463ee5a76d/Goxb9y5cE_Ty1SWCetCoT.png" width="500" /&gt;
  &lt;p&gt;Figure 5. Asynchronous vs. Synchronous Inference in Real-World Tasks.
(a) Task success rates (%), (b) average completion time(s), and (c) number of tasks completed within a fixed time window.
&lt;/p&gt;
&lt;/div&gt;


&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Conclusion
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;SmolVLA is our contribution to building robotics foundation models that are open, efficient, and reproducible. Despite its small size, it matches or outperforms larger, proprietary models across a range of real-world and simulated tasks. By relying solely on community-contributed datasets and affordable hardware, SmolVLA lowers the barrier to entry for researchers, educators, and hobbyists alike.
But this is just the beginning. SmolVLA is more than just a model — it's part of a growing open-source movement toward scalable, collaborative robotics.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Call to Action:
	&lt;/span&gt;
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;🔧 &lt;strong&gt;Try it out!&lt;/strong&gt; Finetune SmolVLA on your own data, deploy it on affordable hardware, or benchmark it against your current stack and share it on twitter/linkedin.&lt;/li&gt;
&lt;li&gt;🤖 &lt;strong&gt;Upload the dataset!&lt;/strong&gt; Got a robot? Collect and share your data using the lerobot format. Help expand the community dataset that powers SmolVLA.&lt;/li&gt;
&lt;li&gt;💬 &lt;strong&gt;Join the blog discussion.&lt;/strong&gt; Drop your questions, ideas, or feedback in the discussion below. We’re happy to help with integration, training, or deployment.&lt;/li&gt;
&lt;li&gt;📊 &lt;strong&gt;Contribute.&lt;/strong&gt; Improve datasets, report issues, suggest new ideas. Every contribution helps.&lt;/li&gt;
&lt;li&gt;🌍 &lt;strong&gt;Spread the word.&lt;/strong&gt; Share SmolVLA with fellow researchers, developers, or educators interested in efficient, real-time robotic policies.&lt;/li&gt;
&lt;li&gt;📫 &lt;strong&gt;Stay in touch:&lt;/strong&gt; Follow the LeRobot organization and Discord server for updates, tutorials, and new releases.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Together, we can make real-world robotics more capable, more affordable, and more open. ✨&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/smolvla</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Teaching AI models what they don’t know (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/themis-ai-teaches-ai-models-what-they-dont-know-0603</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202506/MIT-ThemisAI-01-Press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Artificial intelligence systems like ChatGPT provide plausible-sounding answers to any question you might ask. But they don’t always reveal the gaps in their knowledge or areas where they’re uncertain. That problem can have huge consequences as AI systems are increasingly used to do things like develop drugs, synthesize information, and drive autonomous cars.&lt;/p&gt;&lt;p&gt;Now, the MIT spinout Themis AI is helping quantify model uncertainty and correct outputs before they cause bigger problems. The company’s Capsa platform can work with any machine-learning model to detect and correct unreliable outputs in seconds. It works by modifying AI models to enable them to detect patterns in their data processing that indicate ambiguity, incompleteness, or bias.&lt;/p&gt;&lt;p&gt;“The idea is to take a model, wrap it in Capsa, identify the uncertainties and failure modes of the model, and then enhance the model,” says Themis AI co-founder and MIT Professor Daniela Rus, who is also the director of the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL). “We’re excited about offering a solution that can improve models and offer guarantees that the model is working correctly.”&lt;/p&gt;&lt;p&gt;Rus founded Themis AI in 2021 with Alexander Amini ’17, SM ’18, PhD ’22 and Elaheh Ahmadi ’20, MEng ’21, two former research affiliates in her lab. Since then, they’ve helped telecom companies with network planning and automation, helped oil and gas companies use AI to understand seismic imagery, and published papers on developing more reliable and trustworthy chatbots.&lt;/p&gt;&lt;p&gt;“We want to enable AI in the highest-stakes applications of every industry,” Amini says. “We’ve all seen examples of AI hallucinating or making mistakes. As AI is deployed more broadly, those mistakes could lead to devastating consequences. Themis makes it possible that any AI can forecast and predict its own failures, before they happen.”&lt;/p&gt;&lt;div&gt;&lt;div&gt;&lt;div id="_com_1"&gt;&lt;p&gt;&lt;strong&gt;Helping models know what they don’t know&lt;/strong&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;p&gt;Rus’ lab has been researching model uncertainty for years. In 2018, she received funding from Toyota to study the reliability of a machine learning-based autonomous driving solution.&lt;/p&gt;&lt;p&gt;“That is a safety-critical context where understanding model reliability is very important,” Rus says.&lt;/p&gt;&lt;p&gt;In separate work, Rus, Amini, and their collaborators built an algorithm that could detect racial and gender bias in facial recognition systems and automatically reweight the model’s training data, showing it eliminated bias. The algorithm worked by identifying the unrepresentative parts of the underlying training data and generating new, similar data samples to rebalance it.&lt;/p&gt;&lt;p&gt;In 2021, the eventual co-founders showed a similar approach could be used to help pharmaceutical companies use AI models to predict the properties of drug candidates. They founded Themis AI later that year.&lt;/p&gt;&lt;p&gt;“Guiding drug discovery could potentially save a lot of money,” Rus says. “That was the use case that made us realize how powerful this tool could be.”&lt;/p&gt;&lt;p&gt;Today Themis AI is working with enterprises in a variety of industries, and many of those companies are building large language models. By using Capsa, these models are able to quantify their own uncertainty for each output.&lt;/p&gt;&lt;p&gt;“Many companies are interested in using LLMs that are based on their data, but they’re concerned about reliability,” observes Stewart Jamieson SM ’20, PhD ’24, Themis AI's head of technology. “We help LLMs self-report their confidence and uncertainty, which enables more reliable question answering and flagging unreliable outputs.”&lt;/p&gt;&lt;p&gt;Themis AI is also in discussions with semiconductor companies building AI solutions on their chips that can work outside of cloud environments.&lt;/p&gt;&lt;p&gt;“Normally these smaller models that work on phones or embedded systems aren’t very accurate compared to what you could run on a server, but we can get the best of both worlds: low latency, efficient edge computing without sacrificing quality,” Jamieson explains. “We see a future where edge devices do most of the work, but whenever they’re unsure of their output, they can forward those tasks to a central server.”&lt;/p&gt;&lt;p&gt;Pharmaceutical companies can also use Capsa to improve AI models being used to identify drug candidates and predict their performance in clinical trials.&lt;/p&gt;&lt;p&gt;“The predictions and outputs of these models are very complex and hard to interpret — experts spend a lot of time and effort trying to make sense of them,” Amini remarks. “Capsa can give insights right out of the gate to understand if the predictions are backed by evidence in the training set or are just speculation without a lot of grounding. That can accelerate the identification of the strongest predictions, and we think that has a huge potential for societal good.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Research for impact&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Themis AI’s team believes the company is well-positioned to improve the cutting edge of constantly evolving AI technology. For instance, the company is exploring Capsa’s ability to improve accuracy in an AI technique known as chain-of-thought reasoning, in which LLMs explain the steps they take to get to an answer.&lt;/p&gt;&lt;p&gt;“We’ve seen signs Capsa could help guide those reasoning processes to identify the highest-confidence chains of reasoning,” Jamieson says. “We think that has huge implications in terms of improving the LLM experience, reducing latencies, and reducing computation requirements. It’s an extremely high-impact opportunity for us.”&lt;/p&gt;&lt;p&gt;For Rus, who has co-founded several companies since coming to MIT, Themis AI is an opportunity to ensure her MIT research has impact.&lt;/p&gt;&lt;p&gt;“My students and I have become increasingly passionate about going the extra step to make our work relevant for the world," Rus says. “AI has tremendous potential to transform industries, but AI also raises concerns. What excites me is the opportunity to help develop technical solutions that address these challenges and also build trust and understanding between people and the technologies that are becoming part of their daily lives.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202506/MIT-ThemisAI-01-Press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Artificial intelligence systems like ChatGPT provide plausible-sounding answers to any question you might ask. But they don’t always reveal the gaps in their knowledge or areas where they’re uncertain. That problem can have huge consequences as AI systems are increasingly used to do things like develop drugs, synthesize information, and drive autonomous cars.&lt;/p&gt;&lt;p&gt;Now, the MIT spinout Themis AI is helping quantify model uncertainty and correct outputs before they cause bigger problems. The company’s Capsa platform can work with any machine-learning model to detect and correct unreliable outputs in seconds. It works by modifying AI models to enable them to detect patterns in their data processing that indicate ambiguity, incompleteness, or bias.&lt;/p&gt;&lt;p&gt;“The idea is to take a model, wrap it in Capsa, identify the uncertainties and failure modes of the model, and then enhance the model,” says Themis AI co-founder and MIT Professor Daniela Rus, who is also the director of the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL). “We’re excited about offering a solution that can improve models and offer guarantees that the model is working correctly.”&lt;/p&gt;&lt;p&gt;Rus founded Themis AI in 2021 with Alexander Amini ’17, SM ’18, PhD ’22 and Elaheh Ahmadi ’20, MEng ’21, two former research affiliates in her lab. Since then, they’ve helped telecom companies with network planning and automation, helped oil and gas companies use AI to understand seismic imagery, and published papers on developing more reliable and trustworthy chatbots.&lt;/p&gt;&lt;p&gt;“We want to enable AI in the highest-stakes applications of every industry,” Amini says. “We’ve all seen examples of AI hallucinating or making mistakes. As AI is deployed more broadly, those mistakes could lead to devastating consequences. Themis makes it possible that any AI can forecast and predict its own failures, before they happen.”&lt;/p&gt;&lt;div&gt;&lt;div&gt;&lt;div id="_com_1"&gt;&lt;p&gt;&lt;strong&gt;Helping models know what they don’t know&lt;/strong&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;p&gt;Rus’ lab has been researching model uncertainty for years. In 2018, she received funding from Toyota to study the reliability of a machine learning-based autonomous driving solution.&lt;/p&gt;&lt;p&gt;“That is a safety-critical context where understanding model reliability is very important,” Rus says.&lt;/p&gt;&lt;p&gt;In separate work, Rus, Amini, and their collaborators built an algorithm that could detect racial and gender bias in facial recognition systems and automatically reweight the model’s training data, showing it eliminated bias. The algorithm worked by identifying the unrepresentative parts of the underlying training data and generating new, similar data samples to rebalance it.&lt;/p&gt;&lt;p&gt;In 2021, the eventual co-founders showed a similar approach could be used to help pharmaceutical companies use AI models to predict the properties of drug candidates. They founded Themis AI later that year.&lt;/p&gt;&lt;p&gt;“Guiding drug discovery could potentially save a lot of money,” Rus says. “That was the use case that made us realize how powerful this tool could be.”&lt;/p&gt;&lt;p&gt;Today Themis AI is working with enterprises in a variety of industries, and many of those companies are building large language models. By using Capsa, these models are able to quantify their own uncertainty for each output.&lt;/p&gt;&lt;p&gt;“Many companies are interested in using LLMs that are based on their data, but they’re concerned about reliability,” observes Stewart Jamieson SM ’20, PhD ’24, Themis AI's head of technology. “We help LLMs self-report their confidence and uncertainty, which enables more reliable question answering and flagging unreliable outputs.”&lt;/p&gt;&lt;p&gt;Themis AI is also in discussions with semiconductor companies building AI solutions on their chips that can work outside of cloud environments.&lt;/p&gt;&lt;p&gt;“Normally these smaller models that work on phones or embedded systems aren’t very accurate compared to what you could run on a server, but we can get the best of both worlds: low latency, efficient edge computing without sacrificing quality,” Jamieson explains. “We see a future where edge devices do most of the work, but whenever they’re unsure of their output, they can forward those tasks to a central server.”&lt;/p&gt;&lt;p&gt;Pharmaceutical companies can also use Capsa to improve AI models being used to identify drug candidates and predict their performance in clinical trials.&lt;/p&gt;&lt;p&gt;“The predictions and outputs of these models are very complex and hard to interpret — experts spend a lot of time and effort trying to make sense of them,” Amini remarks. “Capsa can give insights right out of the gate to understand if the predictions are backed by evidence in the training set or are just speculation without a lot of grounding. That can accelerate the identification of the strongest predictions, and we think that has a huge potential for societal good.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Research for impact&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Themis AI’s team believes the company is well-positioned to improve the cutting edge of constantly evolving AI technology. For instance, the company is exploring Capsa’s ability to improve accuracy in an AI technique known as chain-of-thought reasoning, in which LLMs explain the steps they take to get to an answer.&lt;/p&gt;&lt;p&gt;“We’ve seen signs Capsa could help guide those reasoning processes to identify the highest-confidence chains of reasoning,” Jamieson says. “We think that has huge implications in terms of improving the LLM experience, reducing latencies, and reducing computation requirements. It’s an extremely high-impact opportunity for us.”&lt;/p&gt;&lt;p&gt;For Rus, who has co-founded several companies since coming to MIT, Themis AI is an opportunity to ensure her MIT research has impact.&lt;/p&gt;&lt;p&gt;“My students and I have become increasingly passionate about going the extra step to make our work relevant for the world," Rus says. “AI has tremendous potential to transform industries, but AI also raises concerns. What excites me is the opportunity to help develop technical solutions that address these challenges and also build trust and understanding between people and the technologies that are becoming part of their daily lives.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/themis-ai-teaches-ai-models-what-they-dont-know-0603</guid><pubDate>Tue, 03 Jun 2025 04:00:00 +0000</pubDate></item><item><title>Bring Receipts: New NVIDIA AI Blueprint Detects Fraudulent Credit Card Transactions With Precision (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/ai-blueprint-fraud-detection/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/fraud-detection-ai-blueprint-1280x680-1.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This blog, originally published on October 28, 2024, has been updated.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Financial losses from worldwide credit card transaction fraud are projected to reach more than $403 billion over the next decade.&lt;/p&gt;
&lt;p&gt;The new NVIDIA AI Blueprint for financial fraud detection can help combat this burgeoning epidemic — using accelerated data processing and advanced algorithms to improve AI’s ability to detect and prevent credit card transaction fraud.&lt;/p&gt;
&lt;p&gt;Launched this week at the Money20/20 financial services conference, the blueprint provides a reference example for financial institutions to identify subtle patterns and anomalies in transaction data based on user behavior to improve accuracy and reduce false positives compared with traditional methods.&lt;/p&gt;
&lt;p&gt;It shows developers how to build a financial fraud detection workflow by providing reference code, deployment tools and a reference architecture.&lt;/p&gt;
&lt;p&gt;Companies can streamline the migration of their fraud detection workflows from traditional compute to accelerated compute using the NVIDIA AI Enterprise software platform and NVIDIA accelerated computing. The NVIDIA AI Blueprint is available for customers to run on Amazon Web Services and Hewlett Packard Enterprise, with availability coming soon on Dell Technologies. Customers can also use the blueprint through service offerings from NVIDIA partners including Cloudera, EXL, Infosys and SHI International.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Businesses embracing comprehensive machine learning (ML) tools and strategies can observe up to an estimated 40% improvement in fraud detection accuracy, boosting their ability to identify and stop fraudsters faster and mitigate harm.&lt;/p&gt;
&lt;p&gt;As such, leading financial organizations like American Express and Capital One have been using AI to build proprietary solutions that mitigate fraud and enhance customer protection.&lt;/p&gt;
&lt;p&gt;The new AI Blueprint accelerates model training and inference, and demonstrates how these components can be wrapped into a single, easy-to-use software offering, powered by NVIDIA AI.&lt;/p&gt;
&lt;p&gt;Currently optimized for credit card transaction fraud, the blueprint could be adapted for use cases such as new account fraud, account takeover and money laundering.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Using Accelerated Computing and Graph Neural Networks for Fraud Detection&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Traditional data science pipelines lack the compute acceleration to handle the massive data volumes required for effective fraud detection. ML models like XGBoost are effective for detecting anomalies in individual transactions but fall short when fraud involves complex networks of linked accounts and devices.&lt;/p&gt;
&lt;p&gt;Helping address these gaps, NVIDIA RAPIDS — part of the NVIDIA CUDA-X collection of microservices, libraries, tools and technologies — enables payment companies to speed up data processing and transform raw data into powerful features at scale. These companies can fuel their AI models and integrate them with graph neural networks (GNNs) to uncover hidden, large-scale fraud patterns by analyzing relationships across different transactions, users and devices.&lt;/p&gt;
&lt;p&gt;The use of gradient-boosted decision trees — a type of ML algorithm — tapping into libraries such as XGBoost, has long been the standard for fraud detection.&lt;/p&gt;
&lt;p&gt;The new AI Blueprint for financial fraud detection enhances the XGBoost ML model with NVIDIA CUDA-X Data Science libraries including GNNs to generate embeddings that can be used as additional features to help reduce false positives.&lt;/p&gt;
&lt;p&gt;The GNN embeddings are fed into XGBoost to create and train a model that can then be orchestrated. In addition, NVIDIA Dynamo-Triton, formerly NVIDIA Triton Inference Server, boosts real-time inferencing while optimizing AI model throughput, latency and utilization.&lt;/p&gt;
&lt;p&gt;NVIDIA CUDA-X Data Science and Dynamo-Triton are included with NVIDIA AI Enterprise.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Leading Financial Services Organizations Adopt AI&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;During a time when many large North American financial institutions are reporting online or mobile fraud losses continue to increase, AI is helping to combat this trend.&lt;/p&gt;
&lt;p&gt;American Express, which began using AI to fight fraud in 2010, leverages fraud detection algorithms to monitor all customer transactions globally in real time, generating fraud decisions in just milliseconds. Using a combination of advanced algorithms, one of which tapped into the NVIDIA AI platform, American Express enhanced model accuracy, advancing the company’s ability to better fight fraud.&lt;/p&gt;
&lt;p&gt;European digital bank bunq uses generative AI and large language models to help detect fraud and money laundering. Its AI-powered transaction-monitoring system achieved nearly 100x faster model training speeds with NVIDIA accelerated computing.&lt;/p&gt;
&lt;p&gt;BNY announced in March 2024 that it became the first major bank to deploy an NVIDIA DGX SuperPOD with DGX H100 systems, which will help build solutions that support fraud detection and other use cases.&lt;/p&gt;
&lt;p&gt;And now, systems integrators, software vendors and cloud service providers can integrate the new NVIDIA blueprint for fraud detection to boost their financial services applications and help keep customers’ money, identities and digital accounts safe.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Explore the &lt;/i&gt;&lt;i&gt;NVIDIA AI Blueprint for financial fraud detection&lt;/i&gt;&lt;i&gt; and read this &lt;/i&gt;&lt;i&gt;NVIDIA Technical Blog&lt;/i&gt;&lt;i&gt; on supercharging fraud detection with GNNs.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about &lt;/i&gt;&lt;i&gt;AI for fraud detection&lt;/i&gt; &lt;i&gt;by visiting the AI Summit at Money20/20, running this week in Amsterdam.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/fraud-detection-ai-blueprint-1280x680-1.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This blog, originally published on October 28, 2024, has been updated.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Financial losses from worldwide credit card transaction fraud are projected to reach more than $403 billion over the next decade.&lt;/p&gt;
&lt;p&gt;The new NVIDIA AI Blueprint for financial fraud detection can help combat this burgeoning epidemic — using accelerated data processing and advanced algorithms to improve AI’s ability to detect and prevent credit card transaction fraud.&lt;/p&gt;
&lt;p&gt;Launched this week at the Money20/20 financial services conference, the blueprint provides a reference example for financial institutions to identify subtle patterns and anomalies in transaction data based on user behavior to improve accuracy and reduce false positives compared with traditional methods.&lt;/p&gt;
&lt;p&gt;It shows developers how to build a financial fraud detection workflow by providing reference code, deployment tools and a reference architecture.&lt;/p&gt;
&lt;p&gt;Companies can streamline the migration of their fraud detection workflows from traditional compute to accelerated compute using the NVIDIA AI Enterprise software platform and NVIDIA accelerated computing. The NVIDIA AI Blueprint is available for customers to run on Amazon Web Services and Hewlett Packard Enterprise, with availability coming soon on Dell Technologies. Customers can also use the blueprint through service offerings from NVIDIA partners including Cloudera, EXL, Infosys and SHI International.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Businesses embracing comprehensive machine learning (ML) tools and strategies can observe up to an estimated 40% improvement in fraud detection accuracy, boosting their ability to identify and stop fraudsters faster and mitigate harm.&lt;/p&gt;
&lt;p&gt;As such, leading financial organizations like American Express and Capital One have been using AI to build proprietary solutions that mitigate fraud and enhance customer protection.&lt;/p&gt;
&lt;p&gt;The new AI Blueprint accelerates model training and inference, and demonstrates how these components can be wrapped into a single, easy-to-use software offering, powered by NVIDIA AI.&lt;/p&gt;
&lt;p&gt;Currently optimized for credit card transaction fraud, the blueprint could be adapted for use cases such as new account fraud, account takeover and money laundering.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Using Accelerated Computing and Graph Neural Networks for Fraud Detection&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Traditional data science pipelines lack the compute acceleration to handle the massive data volumes required for effective fraud detection. ML models like XGBoost are effective for detecting anomalies in individual transactions but fall short when fraud involves complex networks of linked accounts and devices.&lt;/p&gt;
&lt;p&gt;Helping address these gaps, NVIDIA RAPIDS — part of the NVIDIA CUDA-X collection of microservices, libraries, tools and technologies — enables payment companies to speed up data processing and transform raw data into powerful features at scale. These companies can fuel their AI models and integrate them with graph neural networks (GNNs) to uncover hidden, large-scale fraud patterns by analyzing relationships across different transactions, users and devices.&lt;/p&gt;
&lt;p&gt;The use of gradient-boosted decision trees — a type of ML algorithm — tapping into libraries such as XGBoost, has long been the standard for fraud detection.&lt;/p&gt;
&lt;p&gt;The new AI Blueprint for financial fraud detection enhances the XGBoost ML model with NVIDIA CUDA-X Data Science libraries including GNNs to generate embeddings that can be used as additional features to help reduce false positives.&lt;/p&gt;
&lt;p&gt;The GNN embeddings are fed into XGBoost to create and train a model that can then be orchestrated. In addition, NVIDIA Dynamo-Triton, formerly NVIDIA Triton Inference Server, boosts real-time inferencing while optimizing AI model throughput, latency and utilization.&lt;/p&gt;
&lt;p&gt;NVIDIA CUDA-X Data Science and Dynamo-Triton are included with NVIDIA AI Enterprise.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Leading Financial Services Organizations Adopt AI&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;During a time when many large North American financial institutions are reporting online or mobile fraud losses continue to increase, AI is helping to combat this trend.&lt;/p&gt;
&lt;p&gt;American Express, which began using AI to fight fraud in 2010, leverages fraud detection algorithms to monitor all customer transactions globally in real time, generating fraud decisions in just milliseconds. Using a combination of advanced algorithms, one of which tapped into the NVIDIA AI platform, American Express enhanced model accuracy, advancing the company’s ability to better fight fraud.&lt;/p&gt;
&lt;p&gt;European digital bank bunq uses generative AI and large language models to help detect fraud and money laundering. Its AI-powered transaction-monitoring system achieved nearly 100x faster model training speeds with NVIDIA accelerated computing.&lt;/p&gt;
&lt;p&gt;BNY announced in March 2024 that it became the first major bank to deploy an NVIDIA DGX SuperPOD with DGX H100 systems, which will help build solutions that support fraud detection and other use cases.&lt;/p&gt;
&lt;p&gt;And now, systems integrators, software vendors and cloud service providers can integrate the new NVIDIA blueprint for fraud detection to boost their financial services applications and help keep customers’ money, identities and digital accounts safe.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Explore the &lt;/i&gt;&lt;i&gt;NVIDIA AI Blueprint for financial fraud detection&lt;/i&gt;&lt;i&gt; and read this &lt;/i&gt;&lt;i&gt;NVIDIA Technical Blog&lt;/i&gt;&lt;i&gt; on supercharging fraud detection with GNNs.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about &lt;/i&gt;&lt;i&gt;AI for fraud detection&lt;/i&gt; &lt;i&gt;by visiting the AI Summit at Money20/20, running this week in Amsterdam.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/ai-blueprint-fraud-detection/</guid><pubDate>Tue, 03 Jun 2025 06:00:44 +0000</pubDate></item><item><title>Learning to clarify: Multi-turn conversations with Action-Based Contrastive Self-Training (The latest research from Google)</title><link>https://research.google/blog/learning-to-clarify-multi-turn-conversations-with-action-based-contrastive-self-training/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;Are action-based preferences necessary?&lt;/i&gt;&lt;/b&gt; One of the key factors of ACT is that the contrastive pairs highlight differences between conversational actions. In “ACT w/ Random Actions”, we additionally examine the importance of action selection by randomly sampling both the winning and losing action when constructing the preference pair, and observe this underperforms normal ACT.&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;Do we need on-policy sampling?&lt;/i&gt;&lt;/b&gt; In “ACT w/o on-policy sampling”, we examine the importance of on-policy sampling by evaluating normal off-policy DPO on the dataset as constructed in Phase 1. While we do observe some improvements over SFT (e.g., from 69.0 to 74.8 Macro F1), the overall improvements are much larger when using on-policy sampling as with full ACT. This may be due to the fact that the off-policy negative responses are not guaranteed to lie in the language manifold of the policy model, and distribution shift may be too difficult to overcome with off-policy learning.&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;Is trajectory simulation necessary?&lt;/i&gt;&lt;/b&gt; ACT is better-aligned with multi-turn conversations due to its trajectory simulation. Without multi-turn simulation, our approach can be viewed similarly to on-policy DPO variants like IRPO, but with a conversation-specific reward signal which accounts for conversation actions and task heuristics. In “ACT w/ sampling w/o simulation”, we find that this trajectory-level simulation is critical to improving multi-turn performance, especially the policy model’s ability to reason about its own clarification questions.&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;Is ACT model agnostic?&lt;/i&gt;&lt;/b&gt; The base model in our main experiments, Zephyr, is obtained by aligning Mistral. In “ACT with unaligned foundation models” we observe a performance gap of 6.5 Action F1 and 4.3 Trajectory F1 after ACT tuning for the two models. However, our results demonstrate ACT can improve performance regardless of pre-existing alignment with human feedback, although it can help as an improved model initialization. Overall, we find that improving base model performance with ACT is model agnostic.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;Are action-based preferences necessary?&lt;/i&gt;&lt;/b&gt; One of the key factors of ACT is that the contrastive pairs highlight differences between conversational actions. In “ACT w/ Random Actions”, we additionally examine the importance of action selection by randomly sampling both the winning and losing action when constructing the preference pair, and observe this underperforms normal ACT.&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;Do we need on-policy sampling?&lt;/i&gt;&lt;/b&gt; In “ACT w/o on-policy sampling”, we examine the importance of on-policy sampling by evaluating normal off-policy DPO on the dataset as constructed in Phase 1. While we do observe some improvements over SFT (e.g., from 69.0 to 74.8 Macro F1), the overall improvements are much larger when using on-policy sampling as with full ACT. This may be due to the fact that the off-policy negative responses are not guaranteed to lie in the language manifold of the policy model, and distribution shift may be too difficult to overcome with off-policy learning.&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;Is trajectory simulation necessary?&lt;/i&gt;&lt;/b&gt; ACT is better-aligned with multi-turn conversations due to its trajectory simulation. Without multi-turn simulation, our approach can be viewed similarly to on-policy DPO variants like IRPO, but with a conversation-specific reward signal which accounts for conversation actions and task heuristics. In “ACT w/ sampling w/o simulation”, we find that this trajectory-level simulation is critical to improving multi-turn performance, especially the policy model’s ability to reason about its own clarification questions.&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;Is ACT model agnostic?&lt;/i&gt;&lt;/b&gt; The base model in our main experiments, Zephyr, is obtained by aligning Mistral. In “ACT with unaligned foundation models” we observe a performance gap of 6.5 Action F1 and 4.3 Trajectory F1 after ACT tuning for the two models. However, our results demonstrate ACT can improve performance regardless of pre-existing alignment with human feedback, although it can help as an improved model initialization. Overall, we find that improving base model performance with ACT is model agnostic.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/learning-to-clarify-multi-turn-conversations-with-action-based-contrastive-self-training/</guid><pubDate>Tue, 03 Jun 2025 08:13:00 +0000</pubDate></item><item><title>Inside the effort to tally AI’s energy appetite (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/03/1117685/inside-the-tedious-effort-to-tally-ais-energy-appetite/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/energy.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;After working on it for months, my colleague Casey Crownhart and I finally saw our story on AI’s energy and emissions burden go live last week.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The initial goal sounded simple: Calculate how much energy is used each time we interact with a chatbot, and then tally that up to understand why everyone from leaders of AI companies to officials at the White House wants to harness unprecedented levels of electricity to power AI and reshape our energy grids in the process.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;It was, of course, not so simple. After speaking with dozens of researchers, we realized that the common understanding of AI’s energy appetite is full of holes. I encourage you to read the full story, which has some incredible graphics to help you understand everything from the energy used in a single query right up to what AI will require just three years from now (enough electricity to power 22% of US households, it turns out). But here are three takeaways I have after the project.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;AI is in its infancy&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;We focused on measuring the energy requirements that go into using a chatbot, generating an image, and creating a video with AI. But these three uses are relatively small-scale compared with where AI is headed next.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Lots of AI companies are building reasoning models, which “think” for longer and use more energy. They’re building hardware devices, perhaps like the one Jony Ive has been working on (which OpenAI just acquired for $6.5 billion), that have AI constantly humming along in the background of our conversations. They’re designing agents and digital clones of us to act on our behalf. All these trends point to a more energy-intensive future (which, again, helps explain why OpenAI and others are spending such inconceivable amounts of money on energy).&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But the fact that AI is in its infancy raises another point. The models, chips, and cooling methods behind this AI revolution could all grow more efficient over time, as my colleague Will Douglas Heaven explains. This future isn’t predetermined.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;AI video is on another level&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;When we tested the energy demands of various models, we found the energy required to produce even a low-quality, five-second video to be pretty shocking: It was 42,000 times more than the amount needed for a chatbot answer a question about a recipe, and enough to power a microwave for over an hour. If there’s one type of AI whose energy appetite should worry you, it’s this one.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Soon after we published, Google debuted the latest iteration of its Veo model. People quickly created compilations of the most impressive clips (this one being the most shocking to me). Something we point out in the story is that Google (as well as OpenAI, which has its own video generator, Sora) denied our request for specific numbers on the energy their AI models use. Nonetheless, our reporting suggests it’s very likely that high-definition video models like Veo and Sora are much larger, and much more energy-demanding, than the models we tested.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I think the key to whether the use of AI video will produce indefensible clouds of emissions in the near future will be how it’s used, and how it’s priced. The example I linked shows a bunch of TikTok-style content, and I predict that if creating AI video is cheap enough, social video sites will be inundated with this type of content.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;There are more important questions than your own individual footprint&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;We expected that a lot of readers would understandably think about this story in terms of their own individual footprint, wondering whether their AI usage is contributing to the climate crisis. Don’t panic: It’s likely that asking a chatbot for help with a travel plan does not meaningfully increase your carbon footprint. Video generation might. But after reporting on this for months, I think there are more important questions.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;Consider, for example, the water being drained from aquifers in Nevada, the country’s driest state, to power data centers that are drawn to the area by tax incentives and easy permitting processes, as detailed in an incredible story by James Temple. Or look at how Meta’s largest data center project, in Louisiana, is relying on natural gas despite industry promises to use clean energy, per a story by David Rotman. Or the fact that nuclear energy is not the silver bullet that AI companies often make it out to be.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;There are global forces shaping how much energy AI companies are able to access and what types of sources will provide it. There is also very little transparency from leading AI companies on their current and future energy demands, even while they’re asking for public support for these plans. Pondering your individual footprint can be a good thing to do, provided you remember that it’s not so much your footprint as these other factors that are keeping climate researchers and energy experts we spoke to up at night.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This story originally appeared in&amp;nbsp;&lt;/em&gt;The Algorithm&lt;em&gt;, our weekly newsletter on AI. To get stories like this in your inbox first,&amp;nbsp;sign up here.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/energy.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;After working on it for months, my colleague Casey Crownhart and I finally saw our story on AI’s energy and emissions burden go live last week.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The initial goal sounded simple: Calculate how much energy is used each time we interact with a chatbot, and then tally that up to understand why everyone from leaders of AI companies to officials at the White House wants to harness unprecedented levels of electricity to power AI and reshape our energy grids in the process.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;It was, of course, not so simple. After speaking with dozens of researchers, we realized that the common understanding of AI’s energy appetite is full of holes. I encourage you to read the full story, which has some incredible graphics to help you understand everything from the energy used in a single query right up to what AI will require just three years from now (enough electricity to power 22% of US households, it turns out). But here are three takeaways I have after the project.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;AI is in its infancy&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;We focused on measuring the energy requirements that go into using a chatbot, generating an image, and creating a video with AI. But these three uses are relatively small-scale compared with where AI is headed next.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Lots of AI companies are building reasoning models, which “think” for longer and use more energy. They’re building hardware devices, perhaps like the one Jony Ive has been working on (which OpenAI just acquired for $6.5 billion), that have AI constantly humming along in the background of our conversations. They’re designing agents and digital clones of us to act on our behalf. All these trends point to a more energy-intensive future (which, again, helps explain why OpenAI and others are spending such inconceivable amounts of money on energy).&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But the fact that AI is in its infancy raises another point. The models, chips, and cooling methods behind this AI revolution could all grow more efficient over time, as my colleague Will Douglas Heaven explains. This future isn’t predetermined.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;AI video is on another level&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;When we tested the energy demands of various models, we found the energy required to produce even a low-quality, five-second video to be pretty shocking: It was 42,000 times more than the amount needed for a chatbot answer a question about a recipe, and enough to power a microwave for over an hour. If there’s one type of AI whose energy appetite should worry you, it’s this one.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Soon after we published, Google debuted the latest iteration of its Veo model. People quickly created compilations of the most impressive clips (this one being the most shocking to me). Something we point out in the story is that Google (as well as OpenAI, which has its own video generator, Sora) denied our request for specific numbers on the energy their AI models use. Nonetheless, our reporting suggests it’s very likely that high-definition video models like Veo and Sora are much larger, and much more energy-demanding, than the models we tested.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I think the key to whether the use of AI video will produce indefensible clouds of emissions in the near future will be how it’s used, and how it’s priced. The example I linked shows a bunch of TikTok-style content, and I predict that if creating AI video is cheap enough, social video sites will be inundated with this type of content.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;There are more important questions than your own individual footprint&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;We expected that a lot of readers would understandably think about this story in terms of their own individual footprint, wondering whether their AI usage is contributing to the climate crisis. Don’t panic: It’s likely that asking a chatbot for help with a travel plan does not meaningfully increase your carbon footprint. Video generation might. But after reporting on this for months, I think there are more important questions.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;Consider, for example, the water being drained from aquifers in Nevada, the country’s driest state, to power data centers that are drawn to the area by tax incentives and easy permitting processes, as detailed in an incredible story by James Temple. Or look at how Meta’s largest data center project, in Louisiana, is relying on natural gas despite industry promises to use clean energy, per a story by David Rotman. Or the fact that nuclear energy is not the silver bullet that AI companies often make it out to be.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;There are global forces shaping how much energy AI companies are able to access and what types of sources will provide it. There is also very little transparency from leading AI companies on their current and future energy demands, even while they’re asking for public support for these plans. Pondering your individual footprint can be a good thing to do, provided you remember that it’s not so much your footprint as these other factors that are keeping climate researchers and energy experts we spoke to up at night.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This story originally appeared in&amp;nbsp;&lt;/em&gt;The Algorithm&lt;em&gt;, our weekly newsletter on AI. To get stories like this in your inbox first,&amp;nbsp;sign up here.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/03/1117685/inside-the-tedious-effort-to-tally-ais-energy-appetite/</guid><pubDate>Tue, 03 Jun 2025 09:00:00 +0000</pubDate></item><item><title>Advanced audio dialog and generation with Gemini 2.5 (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/advanced-audio-dialog-and-generation-with-gemini-25/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/capability__native-audio_16-9_121.width-1300.jpg" /&gt;&lt;/div&gt;&lt;h2&gt;Safety and responsibility&lt;/h2&gt;&lt;p&gt;We’ve proactively assessed potential risks throughout every stage of the development process for these native audio features, using what we’ve learned to inform our mitigation strategies. We validate these measures through rigorous internal and external safety evaluations, including comprehensive red teaming for responsible deployment. Additionally, all audio outputs from our models are embedded with SynthID, our watermarking technology, to ensure transparency by making AI-generated audio identifiable.&lt;/p&gt;&lt;h2&gt;Native audio capabilities for developers&lt;/h2&gt;&lt;p&gt;We’re bringing native audio outputs to Gemini 2.5 models, giving developers new capabilities to build richer, more interactive applications via the Gemini API in Google AI Studio or Vertex AI.&lt;/p&gt;&lt;p&gt;To begin exploring, developers can try native audio dialog with Gemini 2.5 Flash preview in Google AI Studio’s stream tab. Controllable speech generation (TTS) is available in preview for both Gemini 2.5 Pro and Flash by selecting speech generation in the generate media tab within Google AI Studio.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/capability__native-audio_16-9_121.width-1300.jpg" /&gt;&lt;/div&gt;&lt;h2&gt;Safety and responsibility&lt;/h2&gt;&lt;p&gt;We’ve proactively assessed potential risks throughout every stage of the development process for these native audio features, using what we’ve learned to inform our mitigation strategies. We validate these measures through rigorous internal and external safety evaluations, including comprehensive red teaming for responsible deployment. Additionally, all audio outputs from our models are embedded with SynthID, our watermarking technology, to ensure transparency by making AI-generated audio identifiable.&lt;/p&gt;&lt;h2&gt;Native audio capabilities for developers&lt;/h2&gt;&lt;p&gt;We’re bringing native audio outputs to Gemini 2.5 models, giving developers new capabilities to build richer, more interactive applications via the Gemini API in Google AI Studio or Vertex AI.&lt;/p&gt;&lt;p&gt;To begin exploring, developers can try native audio dialog with Gemini 2.5 Flash preview in Google AI Studio’s stream tab. Controllable speech generation (TTS) is available in preview for both Gemini 2.5 Pro and Flash by selecting speech generation in the generate media tab within Google AI Studio.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/advanced-audio-dialog-and-generation-with-gemini-25/</guid><pubDate>Tue, 03 Jun 2025 17:15:47 +0000</pubDate></item><item><title>KV Cache from scratch in nanoVLM (Hugging Face - Blog)</title><link>https://huggingface.co/blog/kv-cache</link><description>&lt;!-- HTML_TAG_START --&gt;

&lt;p&gt;We have implemented KV Caching from scratch in our nanoVLM repository (a small codebase to train your own Vision Language Model with pure PyTorch). This gave us a &lt;strong&gt;38%&lt;/strong&gt; speedup in generation. In this blog post we cover KV Caching and all our experiences while implementing it. The lessons learnt are general and can be applied to all autoregressive language model generations. Implementing from scratch on a small codebase is a great learning experience, come along for the ride!&lt;/p&gt;
&lt;p&gt;&lt;img alt="bar plot showcasing improvement in generation speed" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/kv-cache/speed_improved.png" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Introduction
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Autoregressive language models generate text by sampling &lt;em&gt;one token at a time&lt;/em&gt;. During inference, the model processes a given input sequence, predicts the next token, appends it to the sequence, and repeats this process until some stopping criterion:&lt;/p&gt;
&lt;p&gt;&lt;img alt="diagram for autoregression" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/kv-cache/autoregression.png" /&gt;&lt;/p&gt;
&lt;p&gt;This step-by-step generation is inherently sequential:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To generate token &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; t_{i+1} &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;t&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;span class="mbin mtight"&gt;+&lt;/span&gt;&lt;span class="mord mtight"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, the model must consider the entire sequence from &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; t_0 &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;t&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; to &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; t_i &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;t&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. From the first instance in the above example &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; t_{i+1} &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;t&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;span class="mbin mtight"&gt;+&lt;/span&gt;&lt;span class="mord mtight"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; would be &lt;code&gt;the&lt;/code&gt; , while all the previous tokens &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; t_0 &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;t&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; to &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; t_i &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;t&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; would be &lt;code&gt;[What, is, in]&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Although transformers are internally parallel, each new prediction requires a full forward pass through all transformer layers, which incurs a quadratic memory/compute in terms of the sequence length.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This repetition also leads to computational &lt;strong&gt;redundancy&lt;/strong&gt;. In this post, we explore &lt;strong&gt;KV Caching&lt;/strong&gt;, an optimisation technique that mitigates this inefficiency.&lt;/p&gt;
&lt;p&gt;Table of contents:&lt;/p&gt;

&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Revisiting the Transformer Architecture
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Before diving into caching, let’s revisit how attention operates in transformer models. A Transformer language model consists of stacked layers, each composed of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multi-head self-attention&lt;/li&gt;
&lt;li&gt;Feed-forward network (MLP)&lt;/li&gt;
&lt;li&gt;Residual connections and layer normalisation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To understand &lt;strong&gt;where KV Caching helps&lt;/strong&gt;, we focus on the &lt;strong&gt;self-attention&lt;/strong&gt; mechanism, specifically within a single attention head.&lt;/p&gt;
&lt;p&gt;Let’s walk through a simple PyTorch implementation to visualise the key computations.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;import&lt;/span&gt; torch

input_seq_length = &lt;span class="hljs-number"&gt;5&lt;/span&gt;
dim_model = &lt;span class="hljs-number"&gt;10&lt;/span&gt;

input_ids_emb = torch.randn(input_seq_length, dim_model)
W_q = torch.randn(dim_model, dim_model)
W_k = torch.randn(dim_model, dim_model)
W_v = torch.randn(dim_model, dim_model)

Q = input_ids_emb @ W_q
K = input_ids_emb @ W_k
V = input_ids_emb @ W_v
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Self-Attention Computation
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;For a sequence of &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; T &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; input embeddings represented as &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant="double-struck"&gt;R&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; X \in \mathbb{R}^{T \times D} &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;X&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;∈&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathbb"&gt;R&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;T&lt;/span&gt;&lt;span class="mbin mtight"&gt;×&lt;/span&gt;&lt;span class="mord mathnormal mtight"&gt;D&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, self-attention is computed as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; Q = XW_Q &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;Q&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;=&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;X&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;W&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;Q&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, with &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant="double-struck"&gt;R&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; W_Q \in \mathbb{R}^{D \times D_q} &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;W&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;Q&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;∈&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathbb"&gt;R&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;D&lt;/span&gt;&lt;span class="mbin mtight"&gt;×&lt;/span&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;D&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size3 size1 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;q&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; K = XW_K &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;K&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;=&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;X&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;W&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;K&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, with &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant="double-struck"&gt;R&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; W_K \in \mathbb{R}^{D \times D_k} &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;W&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;K&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;∈&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathbb"&gt;R&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;D&lt;/span&gt;&lt;span class="mbin mtight"&gt;×&lt;/span&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;D&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size3 size1 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; V = XW_V &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;V&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;=&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;X&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;W&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, with &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant="double-struck"&gt;R&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; W_V \in \mathbb{R}^{D \times D_v} &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;W&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;∈&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathbb"&gt;R&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;D&lt;/span&gt;&lt;span class="mbin mtight"&gt;×&lt;/span&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;D&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size3 size1 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;v&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Causal mask &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; M &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;M&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; to prevent future token access&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The final output is:&lt;/p&gt;
&lt;p&gt;&lt;span class="katex-display"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math display="block" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mtext&gt;Attention&lt;/mtext&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo separator="true"&gt;;&lt;/mo&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mtext&gt;softmax&lt;/mtext&gt;&lt;mrow&gt;&lt;mo fence="true"&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mi mathvariant="normal"&gt;⊤&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/mrow&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;/mfrac&gt;&lt;mo fence="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;
\text{Attention}(X; Q, K, V) = \text{softmax}\left( \frac{QK^\top \cdot M}{\sqrt{d_k}} \right)V
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord text"&gt;&lt;span class="mord"&gt;Attention&lt;/span&gt;&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord mathnormal"&gt;X&lt;/span&gt;&lt;span class="mpunct"&gt;;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;Q&lt;/span&gt;&lt;span class="mpunct"&gt;,&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;K&lt;/span&gt;&lt;span class="mpunct"&gt;,&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;V&lt;/span&gt;&lt;span class="mclose"&gt;)&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;=&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord text"&gt;&lt;span class="mord"&gt;softmax&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="minner"&gt;&lt;span class="mopen delimcenter"&gt;&lt;span class="delimsizing size3"&gt;(&lt;/span&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mopen nulldelimiter"&gt;&lt;/span&gt;&lt;span class="mfrac"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord sqrt"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span class="svg-align"&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;d&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="hide-tail"&gt;&lt;svg height="1.08em" preserveAspectRatio="xMinYMin slice" viewBox="0 0 400000 1080" width="400em" xmlns="http://www.w3.org/2000/svg"&gt;&lt;path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="frac-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;Q&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;K&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;⊤&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mbin"&gt;⋅&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;M&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose nulldelimiter"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose delimcenter"&gt;&lt;span class="delimsizing size3"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here’s a minimal PyTorch equivalent using a causal mask:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;import&lt;/span&gt; torch.nn.functional &lt;span class="hljs-keyword"&gt;as&lt;/span&gt; F
&lt;span class="hljs-keyword"&gt;import&lt;/span&gt; math

d_k = K.shape[-&lt;span class="hljs-number"&gt;1&lt;/span&gt;]
attention_scores = (Q @ K.T) / math.sqrt(d_k)


causal_mask = torch.tril(torch.ones(input_seq_length, input_seq_length))
masked_scores = attention_scores.masked_fill(causal_mask == &lt;span class="hljs-number"&gt;0&lt;/span&gt;, &lt;span class="hljs-built_in"&gt;float&lt;/span&gt;(&lt;span class="hljs-string"&gt;'-inf'&lt;/span&gt;))

attention_weights = F.softmax(masked_scores, dim=-&lt;span class="hljs-number"&gt;1&lt;/span&gt;)
output = attention_weights @ V
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Where Redundancy Creeps In
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;In autoregressive generation, the model generates one token at a time. With each step, it recomputes &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; Q &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;Q&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; K &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;K&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, and &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; V &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; for &lt;strong&gt;the entire sequence&lt;/strong&gt;, even though the earlier tokens haven’t changed.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;new_token_emb = torch.randn(&lt;span class="hljs-number"&gt;1&lt;/span&gt;, dim_model)
extended_input = torch.cat([input_ids_emb, new_token_emb], dim=&lt;span class="hljs-number"&gt;0&lt;/span&gt;)

Q_ext = extended_input @ W_q
K_ext = extended_input @ W_k
V_ext = extended_input @ W_v


&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To confirm the redundancy:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;torch.testing.assert_close(K, K_ext[:input_seq_length]) 
torch.testing.assert_close(V, V_ext[:input_seq_length]) 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These checks show that for all but the newest token, &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; K &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;K&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; and &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; V &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; are identical to previously computed values.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Original (5×5):         Extended (6×6):
■ ■ ■ ■ ■              ■ ■ ■ ■ ■ □
■ ■ ■ ■ ■              ■ ■ ■ ■ ■ □
■ ■ ■ ■ ■    →         ■ ■ ■ ■ ■ □
■ ■ ■ ■ ■              ■ ■ ■ ■ ■ □
■ ■ ■ ■ ■              ■ ■ ■ ■ ■ □
                       □ □ □ □ □ □
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;■&lt;/strong&gt; = Already computed and reused&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;□&lt;/strong&gt; = Recomputed unnecessarily&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Most of the attention computation is repeated needlessly. This gets more expensive as sequences grow.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		How KV Caching Fixes It
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;To eliminate this inefficiency, we use &lt;strong&gt;KV Caching&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;After processing the initial prompt, we &lt;strong&gt;cache&lt;/strong&gt; the computed keys ( &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; K &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;K&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ) and values ( &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; V &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ) for each layer.&lt;/li&gt;
&lt;li&gt;During generation, we &lt;strong&gt;only compute&lt;/strong&gt; &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; K &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;K&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;strong&gt;and&lt;/strong&gt; &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; V &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;strong&gt;for the new token&lt;/strong&gt;, and &lt;strong&gt;append&lt;/strong&gt; them to the cache.&lt;/li&gt;
&lt;li&gt;We compute &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; Q &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;Q&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; for the current token and use it with the &lt;strong&gt;cached &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; K &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;K&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; and &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; V &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt; to get the output.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This changes generation from full-sequence re-computation to a lightweight, incremental update.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;✅ In practice, this cache is a per-layer dictionary with keys "key" and "value", each of shape (&lt;code&gt;batch_size&lt;/code&gt;, &lt;code&gt;num_heads&lt;/code&gt;, &lt;code&gt;seq_len_cached&lt;/code&gt;, &lt;code&gt;head_dim&lt;/code&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is the foundation of how modern LLMs can generate long outputs efficiently.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		KV Caching in nanoVLM: From Theory to Practice
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Now that we understand the theory behind KV Caching, let’s see how it’s implemented in practice inside our nanoVLM repository. This is an ideal testbed, as it's a super concise and self-contained codebase.&lt;/p&gt;
&lt;p&gt;KV caching is enabled across three key components in our model:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;strong&gt;Attention block&lt;/strong&gt; that uses and updates the KV cache&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;Language model&lt;/strong&gt; that tracks cache per layer&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;Generation loop&lt;/strong&gt; that separates &lt;strong&gt;prefill&lt;/strong&gt; (the initial pass with the input prompt) and sequential &lt;strong&gt;decode&lt;/strong&gt; phases&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		1. Updating KV Cache in the Attention Block
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;In the &lt;code&gt;LanguageModelGroupedAttention&lt;/code&gt; class, we modify the &lt;code&gt;forward&lt;/code&gt; function to accept and update a cache of keys and values (&lt;code&gt;block_kv_cache&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Previously, the model recomputed &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; K &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;K&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; and &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; V &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; at every generation step. Now we only compute &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mtext&gt;new&lt;/mtext&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; K_{\text{new}} &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;K&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord text mtight"&gt;&lt;span class="mord mtight"&gt;new&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mtext&gt;new&lt;/mtext&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; V_{\text{new}} &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;V&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord text mtight"&gt;&lt;span class="mord mtight"&gt;new&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; for the current token, and append them to the cached values.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;def&lt;/span&gt; &lt;span class="hljs-title function_"&gt;forward&lt;/span&gt;(&lt;span class="hljs-params"&gt;self, x, cos, sin, attention_mask=&lt;span class="hljs-literal"&gt;None&lt;/span&gt;, block_kv_cache=&lt;span class="hljs-literal"&gt;None&lt;/span&gt;&lt;/span&gt;):
    is_prefill = block_kv_cache &lt;span class="hljs-keyword"&gt;is&lt;/span&gt; &lt;span class="hljs-literal"&gt;None&lt;/span&gt;
    B, T_curr, C = x.size()

    
    q_curr, k_curr, v_curr = project_current_tokens(x)
    q, k_rotated = apply_rotary_pos_embd(q_curr, k_curr, cos, sin)

    &lt;span class="hljs-keyword"&gt;if&lt;/span&gt; &lt;span class="hljs-keyword"&gt;not&lt;/span&gt; is_prefill &lt;span class="hljs-keyword"&gt;and&lt;/span&gt; block_kv_cache[&lt;span class="hljs-string"&gt;'key'&lt;/span&gt;] &lt;span class="hljs-keyword"&gt;is&lt;/span&gt; &lt;span class="hljs-keyword"&gt;not&lt;/span&gt; &lt;span class="hljs-literal"&gt;None&lt;/span&gt;:
        
        k = torch.cat([block_kv_cache[&lt;span class="hljs-string"&gt;'key'&lt;/span&gt;], k_rotated], dim=&lt;span class="hljs-number"&gt;2&lt;/span&gt;)
        v = torch.cat([block_kv_cache[&lt;span class="hljs-string"&gt;'value'&lt;/span&gt;], v_curr], dim=&lt;span class="hljs-number"&gt;2&lt;/span&gt;)
    &lt;span class="hljs-keyword"&gt;else&lt;/span&gt;:
        
        k, v = k_rotated, v_curr

    block_kv_cache = {&lt;span class="hljs-string"&gt;'key'&lt;/span&gt;: k, &lt;span class="hljs-string"&gt;'value'&lt;/span&gt;: v}
    &lt;span class="hljs-keyword"&gt;return&lt;/span&gt; attention_output, block_kv_cache
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		2. Tracking Cache Across Layers
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;In the &lt;code&gt;LanguageModel&lt;/code&gt; class, we introduce &lt;strong&gt;layer-wise cache tracking&lt;/strong&gt;. The &lt;code&gt;start_pos&lt;/code&gt; argument helps the model compute correct &lt;strong&gt;rotary positional encodings&lt;/strong&gt; for newly generated tokens.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;def&lt;/span&gt; &lt;span class="hljs-title function_"&gt;forward&lt;/span&gt;(&lt;span class="hljs-params"&gt;self, x, kv_cache=&lt;span class="hljs-literal"&gt;None&lt;/span&gt;, start_pos=&lt;span class="hljs-number"&gt;0&lt;/span&gt;&lt;/span&gt;):
    T_curr = x.size(&lt;span class="hljs-number"&gt;1&lt;/span&gt;)
    position_ids = torch.arange(start_pos, start_pos + T_curr, device=x.device)
    cos, sin = self.rotary_embd(position_ids)

    &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; i, block &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; &lt;span class="hljs-built_in"&gt;enumerate&lt;/span&gt;(self.blocks):
        
        x, kv_cache[i] = block(x, cos, sin, attention_mask, kv_cache[i])

    &lt;span class="hljs-keyword"&gt;return&lt;/span&gt; x, kv_cache
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;kv_cache&lt;/code&gt;: A list of dictionaries, one per transformer layer, holding previous keys and values.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;start_pos&lt;/code&gt;: Ensures that rotary embeddings are aligned with current generation index.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		3. Prefill vs Decode in the Generation Loop
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The biggest architectural change is in the &lt;code&gt;generate()&lt;/code&gt; method of the &lt;code&gt;VisionLanguageModel&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We &lt;strong&gt;split generation into two stages&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PREFILL PHASE:&lt;/strong&gt; Encode the full prompt and build the initial cache.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DECODE PHASE:&lt;/strong&gt; Generate tokens one at a time using cached keys/values.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;PREFILL PHASE (cache construction)
[Prompt: "What is"] → [Transformer] → [Cache: K, V for all layers]

DECODE PHASE (token-by-token)
[Token: "the"] → [Q("the") + cached K/V] → [next token: "?"] → ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the corresponding code:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;
prompt_output, kv_cache_list = self.forward(
    inputs,
    kv_cache=&lt;span class="hljs-literal"&gt;None&lt;/span&gt;,
    start_pos=&lt;span class="hljs-number"&gt;0&lt;/span&gt;
)


&lt;span class="hljs-keyword"&gt;for&lt;/span&gt; i &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; &lt;span class="hljs-built_in"&gt;range&lt;/span&gt;(max_new_tokens):
    next_token = sample_from(prompt_output)

    decode_output, kv_cache_list = self.forward(
        next_token,
        kv_cache=kv_cache_list,
        start_pos=current_position  
    )

    prompt_output = decode_output
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;By separating these phases, we avoid redundant computation and dramatically speed up inference, especially for long prompts.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Summary of Changes
	&lt;/span&gt;
&lt;/h3&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Module&lt;/th&gt;
&lt;th&gt;Original Behaviour&lt;/th&gt;
&lt;th&gt;New Behaviour&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;code&gt;LanguageModelGroupedAttention.forward&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Recomputes &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; Q &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;Q&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; K &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;K&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; V &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; on every step&lt;/td&gt;
&lt;td&gt;Uses and updates KV cache&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;LanguageModel.forward&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;No memory of previous state&lt;/td&gt;
&lt;td&gt;Tracks per-layer KV cache, handles &lt;code&gt;start_pos&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;VisionLanguageModel.generate&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;One-phase generation loop&lt;/td&gt;
&lt;td&gt;Split into &lt;strong&gt;prefill&lt;/strong&gt; and &lt;strong&gt;decode&lt;/strong&gt; phases&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Summary: Why KV Caching Matters
	&lt;/span&gt;
&lt;/h2&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Benefit&lt;/th&gt;
&lt;th&gt;Explanation&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Incremental growth&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Cache grows by one row per new token&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Position-aware decoding&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;start_pos&lt;/code&gt; ensures correctness of position encoding calculations&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Efficiency&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Reduces per-token inference to O(&lt;code&gt;seq len&lt;/code&gt;) instead of quadratic&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;KV caching eliminates unnecessary computation during autoregressive generation, enabling faster and more efficient inference, especially in long sequences and real-time applications. This is a trade-off between speed and memory, and its drawbacks can be more complex code and restricting fancier inference schemes, like beam-search, etc. KV caching is a popular method for speeding up LLM inference, making it possible to run them on consumer hardware, and now you know how it works too!&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;!-- HTML_TAG_START --&gt;

&lt;p&gt;We have implemented KV Caching from scratch in our nanoVLM repository (a small codebase to train your own Vision Language Model with pure PyTorch). This gave us a &lt;strong&gt;38%&lt;/strong&gt; speedup in generation. In this blog post we cover KV Caching and all our experiences while implementing it. The lessons learnt are general and can be applied to all autoregressive language model generations. Implementing from scratch on a small codebase is a great learning experience, come along for the ride!&lt;/p&gt;
&lt;p&gt;&lt;img alt="bar plot showcasing improvement in generation speed" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/kv-cache/speed_improved.png" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Introduction
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Autoregressive language models generate text by sampling &lt;em&gt;one token at a time&lt;/em&gt;. During inference, the model processes a given input sequence, predicts the next token, appends it to the sequence, and repeats this process until some stopping criterion:&lt;/p&gt;
&lt;p&gt;&lt;img alt="diagram for autoregression" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/kv-cache/autoregression.png" /&gt;&lt;/p&gt;
&lt;p&gt;This step-by-step generation is inherently sequential:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To generate token &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; t_{i+1} &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;t&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;span class="mbin mtight"&gt;+&lt;/span&gt;&lt;span class="mord mtight"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, the model must consider the entire sequence from &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; t_0 &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;t&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; to &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; t_i &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;t&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. From the first instance in the above example &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; t_{i+1} &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;t&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;span class="mbin mtight"&gt;+&lt;/span&gt;&lt;span class="mord mtight"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; would be &lt;code&gt;the&lt;/code&gt; , while all the previous tokens &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; t_0 &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;t&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; to &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; t_i &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;t&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; would be &lt;code&gt;[What, is, in]&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Although transformers are internally parallel, each new prediction requires a full forward pass through all transformer layers, which incurs a quadratic memory/compute in terms of the sequence length.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This repetition also leads to computational &lt;strong&gt;redundancy&lt;/strong&gt;. In this post, we explore &lt;strong&gt;KV Caching&lt;/strong&gt;, an optimisation technique that mitigates this inefficiency.&lt;/p&gt;
&lt;p&gt;Table of contents:&lt;/p&gt;

&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Revisiting the Transformer Architecture
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Before diving into caching, let’s revisit how attention operates in transformer models. A Transformer language model consists of stacked layers, each composed of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multi-head self-attention&lt;/li&gt;
&lt;li&gt;Feed-forward network (MLP)&lt;/li&gt;
&lt;li&gt;Residual connections and layer normalisation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To understand &lt;strong&gt;where KV Caching helps&lt;/strong&gt;, we focus on the &lt;strong&gt;self-attention&lt;/strong&gt; mechanism, specifically within a single attention head.&lt;/p&gt;
&lt;p&gt;Let’s walk through a simple PyTorch implementation to visualise the key computations.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;import&lt;/span&gt; torch

input_seq_length = &lt;span class="hljs-number"&gt;5&lt;/span&gt;
dim_model = &lt;span class="hljs-number"&gt;10&lt;/span&gt;

input_ids_emb = torch.randn(input_seq_length, dim_model)
W_q = torch.randn(dim_model, dim_model)
W_k = torch.randn(dim_model, dim_model)
W_v = torch.randn(dim_model, dim_model)

Q = input_ids_emb @ W_q
K = input_ids_emb @ W_k
V = input_ids_emb @ W_v
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Self-Attention Computation
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;For a sequence of &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; T &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; input embeddings represented as &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant="double-struck"&gt;R&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; X \in \mathbb{R}^{T \times D} &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;X&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;∈&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathbb"&gt;R&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;T&lt;/span&gt;&lt;span class="mbin mtight"&gt;×&lt;/span&gt;&lt;span class="mord mathnormal mtight"&gt;D&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, self-attention is computed as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; Q = XW_Q &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;Q&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;=&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;X&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;W&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;Q&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, with &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant="double-struck"&gt;R&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; W_Q \in \mathbb{R}^{D \times D_q} &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;W&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;Q&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;∈&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathbb"&gt;R&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;D&lt;/span&gt;&lt;span class="mbin mtight"&gt;×&lt;/span&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;D&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size3 size1 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;q&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; K = XW_K &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;K&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;=&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;X&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;W&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;K&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, with &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant="double-struck"&gt;R&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; W_K \in \mathbb{R}^{D \times D_k} &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;W&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;K&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;∈&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathbb"&gt;R&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;D&lt;/span&gt;&lt;span class="mbin mtight"&gt;×&lt;/span&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;D&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size3 size1 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; V = XW_V &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;V&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;=&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;X&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;W&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, with &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant="double-struck"&gt;R&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; W_V \in \mathbb{R}^{D \times D_v} &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;W&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;∈&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathbb"&gt;R&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;D&lt;/span&gt;&lt;span class="mbin mtight"&gt;×&lt;/span&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;D&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size3 size1 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;v&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Causal mask &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; M &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;M&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; to prevent future token access&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The final output is:&lt;/p&gt;
&lt;p&gt;&lt;span class="katex-display"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math display="block" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mtext&gt;Attention&lt;/mtext&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo separator="true"&gt;;&lt;/mo&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mtext&gt;softmax&lt;/mtext&gt;&lt;mrow&gt;&lt;mo fence="true"&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mi mathvariant="normal"&gt;⊤&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/mrow&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;/mfrac&gt;&lt;mo fence="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;
\text{Attention}(X; Q, K, V) = \text{softmax}\left( \frac{QK^\top \cdot M}{\sqrt{d_k}} \right)V
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord text"&gt;&lt;span class="mord"&gt;Attention&lt;/span&gt;&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord mathnormal"&gt;X&lt;/span&gt;&lt;span class="mpunct"&gt;;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;Q&lt;/span&gt;&lt;span class="mpunct"&gt;,&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;K&lt;/span&gt;&lt;span class="mpunct"&gt;,&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;V&lt;/span&gt;&lt;span class="mclose"&gt;)&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;=&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord text"&gt;&lt;span class="mord"&gt;softmax&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="minner"&gt;&lt;span class="mopen delimcenter"&gt;&lt;span class="delimsizing size3"&gt;(&lt;/span&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mopen nulldelimiter"&gt;&lt;/span&gt;&lt;span class="mfrac"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord sqrt"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span class="svg-align"&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;d&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="hide-tail"&gt;&lt;svg height="1.08em" preserveAspectRatio="xMinYMin slice" viewBox="0 0 400000 1080" width="400em" xmlns="http://www.w3.org/2000/svg"&gt;&lt;path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="frac-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;Q&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;K&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;⊤&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mbin"&gt;⋅&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;M&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose nulldelimiter"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose delimcenter"&gt;&lt;span class="delimsizing size3"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here’s a minimal PyTorch equivalent using a causal mask:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;import&lt;/span&gt; torch.nn.functional &lt;span class="hljs-keyword"&gt;as&lt;/span&gt; F
&lt;span class="hljs-keyword"&gt;import&lt;/span&gt; math

d_k = K.shape[-&lt;span class="hljs-number"&gt;1&lt;/span&gt;]
attention_scores = (Q @ K.T) / math.sqrt(d_k)


causal_mask = torch.tril(torch.ones(input_seq_length, input_seq_length))
masked_scores = attention_scores.masked_fill(causal_mask == &lt;span class="hljs-number"&gt;0&lt;/span&gt;, &lt;span class="hljs-built_in"&gt;float&lt;/span&gt;(&lt;span class="hljs-string"&gt;'-inf'&lt;/span&gt;))

attention_weights = F.softmax(masked_scores, dim=-&lt;span class="hljs-number"&gt;1&lt;/span&gt;)
output = attention_weights @ V
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Where Redundancy Creeps In
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;In autoregressive generation, the model generates one token at a time. With each step, it recomputes &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; Q &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;Q&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; K &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;K&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, and &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; V &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; for &lt;strong&gt;the entire sequence&lt;/strong&gt;, even though the earlier tokens haven’t changed.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;new_token_emb = torch.randn(&lt;span class="hljs-number"&gt;1&lt;/span&gt;, dim_model)
extended_input = torch.cat([input_ids_emb, new_token_emb], dim=&lt;span class="hljs-number"&gt;0&lt;/span&gt;)

Q_ext = extended_input @ W_q
K_ext = extended_input @ W_k
V_ext = extended_input @ W_v


&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To confirm the redundancy:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;torch.testing.assert_close(K, K_ext[:input_seq_length]) 
torch.testing.assert_close(V, V_ext[:input_seq_length]) 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These checks show that for all but the newest token, &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; K &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;K&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; and &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; V &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; are identical to previously computed values.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Original (5×5):         Extended (6×6):
■ ■ ■ ■ ■              ■ ■ ■ ■ ■ □
■ ■ ■ ■ ■              ■ ■ ■ ■ ■ □
■ ■ ■ ■ ■    →         ■ ■ ■ ■ ■ □
■ ■ ■ ■ ■              ■ ■ ■ ■ ■ □
■ ■ ■ ■ ■              ■ ■ ■ ■ ■ □
                       □ □ □ □ □ □
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;■&lt;/strong&gt; = Already computed and reused&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;□&lt;/strong&gt; = Recomputed unnecessarily&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Most of the attention computation is repeated needlessly. This gets more expensive as sequences grow.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		How KV Caching Fixes It
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;To eliminate this inefficiency, we use &lt;strong&gt;KV Caching&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;After processing the initial prompt, we &lt;strong&gt;cache&lt;/strong&gt; the computed keys ( &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; K &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;K&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ) and values ( &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; V &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ) for each layer.&lt;/li&gt;
&lt;li&gt;During generation, we &lt;strong&gt;only compute&lt;/strong&gt; &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; K &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;K&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;strong&gt;and&lt;/strong&gt; &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; V &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;strong&gt;for the new token&lt;/strong&gt;, and &lt;strong&gt;append&lt;/strong&gt; them to the cache.&lt;/li&gt;
&lt;li&gt;We compute &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; Q &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;Q&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; for the current token and use it with the &lt;strong&gt;cached &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; K &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;K&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; and &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; V &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt; to get the output.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This changes generation from full-sequence re-computation to a lightweight, incremental update.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;✅ In practice, this cache is a per-layer dictionary with keys "key" and "value", each of shape (&lt;code&gt;batch_size&lt;/code&gt;, &lt;code&gt;num_heads&lt;/code&gt;, &lt;code&gt;seq_len_cached&lt;/code&gt;, &lt;code&gt;head_dim&lt;/code&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is the foundation of how modern LLMs can generate long outputs efficiently.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		KV Caching in nanoVLM: From Theory to Practice
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Now that we understand the theory behind KV Caching, let’s see how it’s implemented in practice inside our nanoVLM repository. This is an ideal testbed, as it's a super concise and self-contained codebase.&lt;/p&gt;
&lt;p&gt;KV caching is enabled across three key components in our model:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;strong&gt;Attention block&lt;/strong&gt; that uses and updates the KV cache&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;Language model&lt;/strong&gt; that tracks cache per layer&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;Generation loop&lt;/strong&gt; that separates &lt;strong&gt;prefill&lt;/strong&gt; (the initial pass with the input prompt) and sequential &lt;strong&gt;decode&lt;/strong&gt; phases&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		1. Updating KV Cache in the Attention Block
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;In the &lt;code&gt;LanguageModelGroupedAttention&lt;/code&gt; class, we modify the &lt;code&gt;forward&lt;/code&gt; function to accept and update a cache of keys and values (&lt;code&gt;block_kv_cache&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Previously, the model recomputed &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; K &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;K&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; and &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; V &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; at every generation step. Now we only compute &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mtext&gt;new&lt;/mtext&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; K_{\text{new}} &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;K&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord text mtight"&gt;&lt;span class="mord mtight"&gt;new&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mtext&gt;new&lt;/mtext&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; V_{\text{new}} &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;V&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord text mtight"&gt;&lt;span class="mord mtight"&gt;new&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; for the current token, and append them to the cached values.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;def&lt;/span&gt; &lt;span class="hljs-title function_"&gt;forward&lt;/span&gt;(&lt;span class="hljs-params"&gt;self, x, cos, sin, attention_mask=&lt;span class="hljs-literal"&gt;None&lt;/span&gt;, block_kv_cache=&lt;span class="hljs-literal"&gt;None&lt;/span&gt;&lt;/span&gt;):
    is_prefill = block_kv_cache &lt;span class="hljs-keyword"&gt;is&lt;/span&gt; &lt;span class="hljs-literal"&gt;None&lt;/span&gt;
    B, T_curr, C = x.size()

    
    q_curr, k_curr, v_curr = project_current_tokens(x)
    q, k_rotated = apply_rotary_pos_embd(q_curr, k_curr, cos, sin)

    &lt;span class="hljs-keyword"&gt;if&lt;/span&gt; &lt;span class="hljs-keyword"&gt;not&lt;/span&gt; is_prefill &lt;span class="hljs-keyword"&gt;and&lt;/span&gt; block_kv_cache[&lt;span class="hljs-string"&gt;'key'&lt;/span&gt;] &lt;span class="hljs-keyword"&gt;is&lt;/span&gt; &lt;span class="hljs-keyword"&gt;not&lt;/span&gt; &lt;span class="hljs-literal"&gt;None&lt;/span&gt;:
        
        k = torch.cat([block_kv_cache[&lt;span class="hljs-string"&gt;'key'&lt;/span&gt;], k_rotated], dim=&lt;span class="hljs-number"&gt;2&lt;/span&gt;)
        v = torch.cat([block_kv_cache[&lt;span class="hljs-string"&gt;'value'&lt;/span&gt;], v_curr], dim=&lt;span class="hljs-number"&gt;2&lt;/span&gt;)
    &lt;span class="hljs-keyword"&gt;else&lt;/span&gt;:
        
        k, v = k_rotated, v_curr

    block_kv_cache = {&lt;span class="hljs-string"&gt;'key'&lt;/span&gt;: k, &lt;span class="hljs-string"&gt;'value'&lt;/span&gt;: v}
    &lt;span class="hljs-keyword"&gt;return&lt;/span&gt; attention_output, block_kv_cache
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		2. Tracking Cache Across Layers
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;In the &lt;code&gt;LanguageModel&lt;/code&gt; class, we introduce &lt;strong&gt;layer-wise cache tracking&lt;/strong&gt;. The &lt;code&gt;start_pos&lt;/code&gt; argument helps the model compute correct &lt;strong&gt;rotary positional encodings&lt;/strong&gt; for newly generated tokens.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;def&lt;/span&gt; &lt;span class="hljs-title function_"&gt;forward&lt;/span&gt;(&lt;span class="hljs-params"&gt;self, x, kv_cache=&lt;span class="hljs-literal"&gt;None&lt;/span&gt;, start_pos=&lt;span class="hljs-number"&gt;0&lt;/span&gt;&lt;/span&gt;):
    T_curr = x.size(&lt;span class="hljs-number"&gt;1&lt;/span&gt;)
    position_ids = torch.arange(start_pos, start_pos + T_curr, device=x.device)
    cos, sin = self.rotary_embd(position_ids)

    &lt;span class="hljs-keyword"&gt;for&lt;/span&gt; i, block &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; &lt;span class="hljs-built_in"&gt;enumerate&lt;/span&gt;(self.blocks):
        
        x, kv_cache[i] = block(x, cos, sin, attention_mask, kv_cache[i])

    &lt;span class="hljs-keyword"&gt;return&lt;/span&gt; x, kv_cache
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;kv_cache&lt;/code&gt;: A list of dictionaries, one per transformer layer, holding previous keys and values.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;start_pos&lt;/code&gt;: Ensures that rotary embeddings are aligned with current generation index.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		3. Prefill vs Decode in the Generation Loop
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The biggest architectural change is in the &lt;code&gt;generate()&lt;/code&gt; method of the &lt;code&gt;VisionLanguageModel&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We &lt;strong&gt;split generation into two stages&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PREFILL PHASE:&lt;/strong&gt; Encode the full prompt and build the initial cache.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DECODE PHASE:&lt;/strong&gt; Generate tokens one at a time using cached keys/values.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;PREFILL PHASE (cache construction)
[Prompt: "What is"] → [Transformer] → [Cache: K, V for all layers]

DECODE PHASE (token-by-token)
[Token: "the"] → [Q("the") + cached K/V] → [next token: "?"] → ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the corresponding code:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;
prompt_output, kv_cache_list = self.forward(
    inputs,
    kv_cache=&lt;span class="hljs-literal"&gt;None&lt;/span&gt;,
    start_pos=&lt;span class="hljs-number"&gt;0&lt;/span&gt;
)


&lt;span class="hljs-keyword"&gt;for&lt;/span&gt; i &lt;span class="hljs-keyword"&gt;in&lt;/span&gt; &lt;span class="hljs-built_in"&gt;range&lt;/span&gt;(max_new_tokens):
    next_token = sample_from(prompt_output)

    decode_output, kv_cache_list = self.forward(
        next_token,
        kv_cache=kv_cache_list,
        start_pos=current_position  
    )

    prompt_output = decode_output
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;By separating these phases, we avoid redundant computation and dramatically speed up inference, especially for long prompts.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Summary of Changes
	&lt;/span&gt;
&lt;/h3&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Module&lt;/th&gt;
&lt;th&gt;Original Behaviour&lt;/th&gt;
&lt;th&gt;New Behaviour&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;code&gt;LanguageModelGroupedAttention.forward&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Recomputes &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; Q &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;Q&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; K &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;K&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, &lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt; V &lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; on every step&lt;/td&gt;
&lt;td&gt;Uses and updates KV cache&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;LanguageModel.forward&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;No memory of previous state&lt;/td&gt;
&lt;td&gt;Tracks per-layer KV cache, handles &lt;code&gt;start_pos&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;VisionLanguageModel.generate&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;One-phase generation loop&lt;/td&gt;
&lt;td&gt;Split into &lt;strong&gt;prefill&lt;/strong&gt; and &lt;strong&gt;decode&lt;/strong&gt; phases&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Summary: Why KV Caching Matters
	&lt;/span&gt;
&lt;/h2&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Benefit&lt;/th&gt;
&lt;th&gt;Explanation&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Incremental growth&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Cache grows by one row per new token&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Position-aware decoding&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;start_pos&lt;/code&gt; ensures correctness of position encoding calculations&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Efficiency&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Reduces per-token inference to O(&lt;code&gt;seq len&lt;/code&gt;) instead of quadratic&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;KV caching eliminates unnecessary computation during autoregressive generation, enabling faster and more efficient inference, especially in long sequences and real-time applications. This is a trade-off between speed and memory, and its drawbacks can be more complex code and restricting fancier inference schemes, like beam-search, etc. KV caching is a popular method for speeding up LLM inference, making it possible to run them on consumer hardware, and now you know how it works too!&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/kv-cache</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>What’s next for AI and math (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/04/1117753/whats-next-for-ai-and-math/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/02/2025-02-21-math-and-ai.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;MIT Technology Review&lt;em&gt;’s What’s Next series looks across industries, trends, and technologies to give you a first look at the future. You can read the rest of them&amp;nbsp;&lt;strong&gt;here.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;The way DARPA tells it, math is stuck in the past. In April, the US Defense Advanced Research Projects Agency kicked off a new initiative called expMath—short for Exponentiating Mathematics—that it hopes will speed up the rate of progress in a field of research that underpins a wide range of crucial real-world applications, from computer science to medicine to national security.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“Math is the source of huge impact, but it’s done more or less as it’s been done for centuries—by people standing at chalkboards,” DARPA program manager Patrick Shafto said in a video introducing the initiative.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;The modern world is built on mathematics. Math lets us model complex systems such as the way air flows around an aircraft, the way financial markets fluctuate, and the way blood flows through the heart. And breakthroughs in advanced mathematics can unlock new technologies such as cryptography, which is essential for private messaging and online banking, and data compression, which lets us shoot images and video across the internet.&lt;/p&gt; 
 &lt;p&gt;But advances in math can be years in the making. DARPA wants to speed things up. The goal for expMath is to encourage mathematicians and artificial-intelligence researchers to develop what DARPA calls an AI coauthor, a tool that might break large, complex math problems into smaller, simpler ones that are easier to grasp and—so the thinking goes—quicker to solve.&lt;/p&gt;  &lt;p&gt;Mathematicians have used computers for decades, to speed up calculations or check whether certain mathematical statements are true. The new vision is that AI might help them crack problems that were previously uncrackable.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;But there’s a huge difference&amp;nbsp;between AI that can solve the kinds of problems set in high school—math that the latest generation of models has already mastered—and AI that could (in theory) solve the kinds of problems that professional mathematicians spend careers chipping away at.&lt;/p&gt;  &lt;p&gt;On one side are tools that might be able to automate certain tasks that math grads are employed to do; on the other are tools that might be able to push human knowledge beyond its existing limits.&lt;/p&gt;  &lt;p&gt;Here are three ways to think about that gulf.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;1/ AI needs more than just clever tricks&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Large language models are not known to be good at math. They make things up and can be persuaded that 2 + 2 = 5. But newer versions of this tech, especially so-called large reasoning models (LRMs) like OpenAI’s o3 and Anthropic’s Claude 4 Thinking, are far more capable—and that's got mathematicians excited.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;This year, a number of LRMs, which try to solve a problem step by step rather than spit out the first result that comes to them, have achieved high scores on the American Invitational Mathematics Examination (AIME), a test given to the top 5% of US high school math students.&lt;/p&gt;  &lt;p&gt;At the same time, a handful of new hybrid models that combine LLMs with some kind of fact-checking system have also made breakthroughs. Emily de Oliveira Santos, a mathematician at the University of São Paulo, Brazil, points to Google DeepMind’s AlphaProof, a system that combines an LLM with DeepMind’s game-playing model AlphaZero, as one key milestone. Last year AlphaProof became the first computer program to match the performance of a silver medallist at the International Math Olympiad, one of the most prestigious mathematics competitions in the world.&lt;/p&gt;  &lt;p&gt;And in May, a Google DeepMind model called AlphaEvolve discovered better results than anything humans had yet come up with for more than 50 unsolved mathematics puzzles and several real-world computer science problems.&lt;/p&gt;  &lt;p&gt;The uptick in progress is clear. “GPT-4 couldn’t do math much beyond undergraduate level,” says de Oliveira Santos. “I remember testing it at the time of its release with a problem in topology, and it just couldn’t write more than a few lines without getting completely lost.” But when she gave the same problem to OpenAI’s o1, an LRM released in January, it nailed it.&lt;/p&gt; 

 &lt;p&gt;Does this mean such models are all set to become the kind of coauthor DARPA hopes for? Not necessarily, she says: “Math Olympiad problems often involve being able to carry out clever tricks, whereas research problems are much more explorative and often have many, many more moving pieces.” Success at one type of problem-solving may not carry over to another.&lt;/p&gt;  &lt;p&gt;Others agree. Martin Bridson, a mathematician at the University of Oxford, thinks the Math Olympiad result is a great achievement. “On the other hand, I don’t find it mind-blowing,” he says. “It’s not a change of paradigm in the sense that ‘Wow, I thought machines would never be able to do that.’ I expected machines to be able to do that.”&lt;/p&gt;  &lt;p&gt;That’s because even though the problems in the Math Olympiad—and similar high school or undergraduate tests like AIME—are hard, there’s a pattern to a lot of them. “We have training camps to train high school kids to do them,” says Bridson. “And if you can train a large number of people to do those problems, why shouldn’t you be able to train a machine to do them?”&lt;/p&gt;  &lt;p&gt;Sergei Gukov, a mathematician at the California Institute of Technology who coaches Math Olympiad teams, points out that the style of question does not change too much between competitions. New problems are set each year, but they can be solved with the same old tricks.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;“Sure, the specific problems didn’t appear before,” says Gukov. “But they’re very close—just a step away from zillions of things you have already seen. You immediately realize, ‘Oh my gosh, there are so many similarities—I’m going to apply the same tactic.’” As hard as competition-level math is, kids and machines alike can be taught how to beat it.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;That’s not true for most unsolved math problems. Bridson is president of the Clay Mathematics Institute, a nonprofit US-based research organization best known for setting up the Millenium Prize Problems in 2000—seven of the most important unsolved problems in mathematics, with a $1 million prize to be awarded to the first person to solve each of them. (One problem, the Poincaré conjecture, was solved in 2010; the others, which include P versus NP and the Riemann hypothesis, remain open). “We’re very far away from AI being able to say anything serious about any of those problems,” says Bridson.&lt;/p&gt;  &lt;p&gt;And yet it’s hard to know exactly how far away, because many of the existing benchmarks used to evaluate progress are maxed out. The best new models already outperform most humans on tests like AIME.&lt;/p&gt;  &lt;p&gt;To get a better idea of what existing systems can and cannot do, a startup called Epoch AI has created a new test called FrontierMath, released in December. Instead of co-opting math tests developed for humans, Epoch AI worked with more than 60 mathematicians around the world to come up with a set of math problems from scratch.&lt;/p&gt; 
 &lt;p&gt;FrontierMath is designed to probe the limits of what today’s AI can do. None of the problems have been seen before and the majority are being kept secret to avoid contaminating training data. Each problem demands hours of work from expert mathematicians to solve—if they can solve it at all: some of the problems require specialist knowledge to tackle.&lt;/p&gt;  &lt;p&gt;FrontierMath is set to become an industry standard. It’s not yet as popular as AIME, says de Oliveira Santos, who helped develop some of the problems: “But I expect this to not hold for much longer, since existing benchmarks are very close to being saturated.”&lt;/p&gt; 
 &lt;p&gt;On AIME, the best large language models (Anthropic’s Claude 4, OpenAI’s o3 and o4-mini, Google DeepMind’s Gemini 2.5 Pro, X-AI’s Grok 3) now score around 90%. On FrontierMath, 04-mini scores 19% and Gemini 2.5 Pro scores 13%. That’s still remarkable, but there’s clear room for improvement.&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;FrontierMath should give the best sense yet just how fast AI is progressing at math. But there are some problems that are still too hard for computers to take on.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;2/ AI needs to manage really vast sequences of steps&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Squint hard enough and in some ways math problems start to look the same: to solve them you need to take a sequence of steps from start to finish. The problem is finding those steps.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“Pretty much every math problem can be formulated as path-finding,” says Gukov. What makes some problems far harder than others is the number of steps on that path. “The difference between the Riemann hypothesis and high school math is that with high school math the paths that we’re looking for are short—10 steps, 20 steps, maybe 40 in the longest case.” The steps are also repeated between problems.&lt;/p&gt;  &lt;p&gt;“But to solve the Riemann hypothesis, we don’t have the steps, and what we’re looking for is a path that is extremely long”—maybe a million lines of computer proof, says Gukov.&lt;/p&gt;  &lt;p&gt;Finding very long sequences of steps can be thought of as a kind of complex game. It’s what DeepMind’s AlphaZero learned to do when it mastered Go and chess. A game of Go might only involve a few hundred moves. But to win, an AI must find a winning sequence of moves among a vast number of possible sequences. Imagine a number with 100 zeros at the end, says Gukov.&lt;/p&gt; 
 &lt;p&gt;But that’s still tiny compared with the number of possible sequences that could be involved in proving or disproving a very hard math problem: “A proof path with a thousand or a million moves involves a number with a thousand or a million zeros,” says Gukov.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;No AI system can sift through that many possibilities. To address this, Gukov and his colleagues developed a system that shortens the length of a path by combining multiple moves into single supermoves. It’s like having boots that let you take giant strides: instead of taking 2,000 steps to walk a mile, you can now walk it in 20.&lt;/p&gt;  &lt;p&gt;The challenge was figuring out which moves to replace with supermoves. In a series of experiments, the researchers came up with a system in which one reinforcement-learning model suggests new moves and a second model checks to see if those moves help.&lt;/p&gt;  &lt;p&gt;They used this approach to make a breakthrough in a math problem called the Andrews-Curtis conjecture, a puzzle that has been unsolved for 60 years. It’s a problem that every professional mathematician will know, says Gukov.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;(An aside for math stans only: The AC conjecture states that a particular way of describing a type of set called a trivial group can be translated into a different but equivalent description with a certain sequence of steps. Most mathematicians think the AC conjecture is false, but nobody knows how to prove that. Gukov admits himself that it is an intellectual curiosity rather than a practical problem, but an important problem for mathematicians nonetheless.)&lt;/p&gt;  &lt;p&gt;Gukov and his colleagues didn’t solve the AC conjecture, but they found that a counterexample (suggesting that the conjecture is false) proposed 40 years ago was itself false. “It’s been a major direction of attack for 40 years,” says Gukov. With the help of AI, they showed that this direction was in fact a dead end.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“Ruling out possible counterexamples is a worthwhile thing,” says Bridson. “It can close off blind alleys, something you might spend a year of your life exploring.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;True, Gukov checked off just one piece of one esoteric puzzle. But he thinks the approach will work in any scenario where you need to find a long sequence of unknown moves, and he now plans to try it out on other problems.&lt;/p&gt;  &lt;p&gt;“Maybe it will lead to something that will help AI in general,” he says. “Because it’s teaching reinforcement learning models to go beyond their training. To me it’s basically about thinking outside of the box—miles away, megaparsecs away.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;3/ Can AI ever provide real insight?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Thinking outside the box is exactly what mathematicians need to solve hard problems. Math is often thought to involve robotic, step-by-step procedures. But advanced math is an experimental pursuit, involving trial and error and flashes of insight.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt; &lt;p&gt;That’s where tools like AlphaEvolve come in. Google DeepMind’s latest model asks an LLM to generate code to solve a particular math problem. A second model then evaluates the proposed solutions, picks the best, and sends them back to the LLM to be improved. After hundreds of rounds of trial and error, AlphaEvolve was able to come up with solutions to a wide range of math problems that were better than anything people had yet come up with. But it can also work as a collaborative tool: at any step, humans can share their own insight with the LLM, prompting it with specific instructions.&lt;/p&gt;  &lt;p&gt;This kind of exploration is key to advanced mathematics. “I’m often looking for interesting phenomena and pushing myself in a certain direction,” says Geordie Williamson, a mathematician at the University of Sydney in Australia. “Like: ‘Let me look down this little alley. Oh, I found something!’”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_18"&gt;&lt;p&gt;Williamson worked with Meta on an AI tool called PatternBoost, designed to support this kind of exploration. PatternBoost can take a mathematical idea or statement and generate similar ones. “It’s like: ‘Here’s a bunch of interesting things. I don’t know what’s going on, but can you produce more interesting things like that?’” he says.&lt;/p&gt;  &lt;p&gt;Such brainstorming is essential work in math. It’s how new ideas get conjured. Take the icosahedron, says Williamson: “It’s a beautiful example of this, which I kind of keep coming back to in my own work.” The icosahedron is a 20-sided 3D object where all the faces are triangles (think of a 20-sided die). The icosahedron is the largest of a family of exactly five such objects: there’s the tetrahedron (four sides), cube (six sides), octahedron (eight sides), and dodecahedron (12 sides).&lt;/p&gt;  &lt;p&gt;Remarkably, the fact that there are exactly five of these objects was proved by mathematicians in ancient Greece. “At the time that this theorem was proved, the icosahedron didn’t exist,” says Williamson. “You can’t go to a quarry and find it—someone found it in their mind. And the icosahedron goes on to have a profound effect on mathematics. It’s still influencing us today in very, very profound ways.”&lt;/p&gt;  &lt;p&gt;For Williamson, the exciting potential of tools like PatternBoost is that they might help people discover future mathematical objects like the icosahedron that go on to shape the way math is done. But we’re not there yet. “AI can contribute in a meaningful way to research-level problems,” he says. “But we're certainly not getting inundated with new theorems at this stage.”&lt;/p&gt;  &lt;p&gt;Ultimately, it comes down to the fact that machines still lack what you might call intuition or creative thinking. Williamson sums it up like this: We now have AI that can beat humans when it knows the rules of the game. “But it’s one thing for a computer to play Go at a superhuman level and another thing for the computer to invent the game of Go.”&lt;/p&gt;  &lt;p&gt;“I think that applies to advanced mathematics,” he says. “Breakthroughs come from a new way of thinking about something, which is akin to finding completely new moves in a game. And I don’t really think we understand where those really brilliant moves in deep mathematics come from.”&lt;/p&gt;  &lt;p&gt;Perhaps AI tools like AlphaEvolve and PatternBoost are best thought of as advance scouts for human intuition. They can discover new directions and point out dead ends, saving mathematicians months or years of work. But the true breakthroughs will still come from the minds of people, as has been the case for thousands of years.&lt;/p&gt;  &lt;p&gt;For now, at least. “There’s plenty of tech companies that tell us that won’t last long,” says Williamson. “But you know—we’ll see.”&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/02/2025-02-21-math-and-ai.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;MIT Technology Review&lt;em&gt;’s What’s Next series looks across industries, trends, and technologies to give you a first look at the future. You can read the rest of them&amp;nbsp;&lt;strong&gt;here.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;The way DARPA tells it, math is stuck in the past. In April, the US Defense Advanced Research Projects Agency kicked off a new initiative called expMath—short for Exponentiating Mathematics—that it hopes will speed up the rate of progress in a field of research that underpins a wide range of crucial real-world applications, from computer science to medicine to national security.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“Math is the source of huge impact, but it’s done more or less as it’s been done for centuries—by people standing at chalkboards,” DARPA program manager Patrick Shafto said in a video introducing the initiative.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;The modern world is built on mathematics. Math lets us model complex systems such as the way air flows around an aircraft, the way financial markets fluctuate, and the way blood flows through the heart. And breakthroughs in advanced mathematics can unlock new technologies such as cryptography, which is essential for private messaging and online banking, and data compression, which lets us shoot images and video across the internet.&lt;/p&gt; 
 &lt;p&gt;But advances in math can be years in the making. DARPA wants to speed things up. The goal for expMath is to encourage mathematicians and artificial-intelligence researchers to develop what DARPA calls an AI coauthor, a tool that might break large, complex math problems into smaller, simpler ones that are easier to grasp and—so the thinking goes—quicker to solve.&lt;/p&gt;  &lt;p&gt;Mathematicians have used computers for decades, to speed up calculations or check whether certain mathematical statements are true. The new vision is that AI might help them crack problems that were previously uncrackable.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;But there’s a huge difference&amp;nbsp;between AI that can solve the kinds of problems set in high school—math that the latest generation of models has already mastered—and AI that could (in theory) solve the kinds of problems that professional mathematicians spend careers chipping away at.&lt;/p&gt;  &lt;p&gt;On one side are tools that might be able to automate certain tasks that math grads are employed to do; on the other are tools that might be able to push human knowledge beyond its existing limits.&lt;/p&gt;  &lt;p&gt;Here are three ways to think about that gulf.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;1/ AI needs more than just clever tricks&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Large language models are not known to be good at math. They make things up and can be persuaded that 2 + 2 = 5. But newer versions of this tech, especially so-called large reasoning models (LRMs) like OpenAI’s o3 and Anthropic’s Claude 4 Thinking, are far more capable—and that's got mathematicians excited.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;This year, a number of LRMs, which try to solve a problem step by step rather than spit out the first result that comes to them, have achieved high scores on the American Invitational Mathematics Examination (AIME), a test given to the top 5% of US high school math students.&lt;/p&gt;  &lt;p&gt;At the same time, a handful of new hybrid models that combine LLMs with some kind of fact-checking system have also made breakthroughs. Emily de Oliveira Santos, a mathematician at the University of São Paulo, Brazil, points to Google DeepMind’s AlphaProof, a system that combines an LLM with DeepMind’s game-playing model AlphaZero, as one key milestone. Last year AlphaProof became the first computer program to match the performance of a silver medallist at the International Math Olympiad, one of the most prestigious mathematics competitions in the world.&lt;/p&gt;  &lt;p&gt;And in May, a Google DeepMind model called AlphaEvolve discovered better results than anything humans had yet come up with for more than 50 unsolved mathematics puzzles and several real-world computer science problems.&lt;/p&gt;  &lt;p&gt;The uptick in progress is clear. “GPT-4 couldn’t do math much beyond undergraduate level,” says de Oliveira Santos. “I remember testing it at the time of its release with a problem in topology, and it just couldn’t write more than a few lines without getting completely lost.” But when she gave the same problem to OpenAI’s o1, an LRM released in January, it nailed it.&lt;/p&gt; 

 &lt;p&gt;Does this mean such models are all set to become the kind of coauthor DARPA hopes for? Not necessarily, she says: “Math Olympiad problems often involve being able to carry out clever tricks, whereas research problems are much more explorative and often have many, many more moving pieces.” Success at one type of problem-solving may not carry over to another.&lt;/p&gt;  &lt;p&gt;Others agree. Martin Bridson, a mathematician at the University of Oxford, thinks the Math Olympiad result is a great achievement. “On the other hand, I don’t find it mind-blowing,” he says. “It’s not a change of paradigm in the sense that ‘Wow, I thought machines would never be able to do that.’ I expected machines to be able to do that.”&lt;/p&gt;  &lt;p&gt;That’s because even though the problems in the Math Olympiad—and similar high school or undergraduate tests like AIME—are hard, there’s a pattern to a lot of them. “We have training camps to train high school kids to do them,” says Bridson. “And if you can train a large number of people to do those problems, why shouldn’t you be able to train a machine to do them?”&lt;/p&gt;  &lt;p&gt;Sergei Gukov, a mathematician at the California Institute of Technology who coaches Math Olympiad teams, points out that the style of question does not change too much between competitions. New problems are set each year, but they can be solved with the same old tricks.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;“Sure, the specific problems didn’t appear before,” says Gukov. “But they’re very close—just a step away from zillions of things you have already seen. You immediately realize, ‘Oh my gosh, there are so many similarities—I’m going to apply the same tactic.’” As hard as competition-level math is, kids and machines alike can be taught how to beat it.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;That’s not true for most unsolved math problems. Bridson is president of the Clay Mathematics Institute, a nonprofit US-based research organization best known for setting up the Millenium Prize Problems in 2000—seven of the most important unsolved problems in mathematics, with a $1 million prize to be awarded to the first person to solve each of them. (One problem, the Poincaré conjecture, was solved in 2010; the others, which include P versus NP and the Riemann hypothesis, remain open). “We’re very far away from AI being able to say anything serious about any of those problems,” says Bridson.&lt;/p&gt;  &lt;p&gt;And yet it’s hard to know exactly how far away, because many of the existing benchmarks used to evaluate progress are maxed out. The best new models already outperform most humans on tests like AIME.&lt;/p&gt;  &lt;p&gt;To get a better idea of what existing systems can and cannot do, a startup called Epoch AI has created a new test called FrontierMath, released in December. Instead of co-opting math tests developed for humans, Epoch AI worked with more than 60 mathematicians around the world to come up with a set of math problems from scratch.&lt;/p&gt; 
 &lt;p&gt;FrontierMath is designed to probe the limits of what today’s AI can do. None of the problems have been seen before and the majority are being kept secret to avoid contaminating training data. Each problem demands hours of work from expert mathematicians to solve—if they can solve it at all: some of the problems require specialist knowledge to tackle.&lt;/p&gt;  &lt;p&gt;FrontierMath is set to become an industry standard. It’s not yet as popular as AIME, says de Oliveira Santos, who helped develop some of the problems: “But I expect this to not hold for much longer, since existing benchmarks are very close to being saturated.”&lt;/p&gt; 
 &lt;p&gt;On AIME, the best large language models (Anthropic’s Claude 4, OpenAI’s o3 and o4-mini, Google DeepMind’s Gemini 2.5 Pro, X-AI’s Grok 3) now score around 90%. On FrontierMath, 04-mini scores 19% and Gemini 2.5 Pro scores 13%. That’s still remarkable, but there’s clear room for improvement.&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;FrontierMath should give the best sense yet just how fast AI is progressing at math. But there are some problems that are still too hard for computers to take on.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;2/ AI needs to manage really vast sequences of steps&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Squint hard enough and in some ways math problems start to look the same: to solve them you need to take a sequence of steps from start to finish. The problem is finding those steps.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“Pretty much every math problem can be formulated as path-finding,” says Gukov. What makes some problems far harder than others is the number of steps on that path. “The difference between the Riemann hypothesis and high school math is that with high school math the paths that we’re looking for are short—10 steps, 20 steps, maybe 40 in the longest case.” The steps are also repeated between problems.&lt;/p&gt;  &lt;p&gt;“But to solve the Riemann hypothesis, we don’t have the steps, and what we’re looking for is a path that is extremely long”—maybe a million lines of computer proof, says Gukov.&lt;/p&gt;  &lt;p&gt;Finding very long sequences of steps can be thought of as a kind of complex game. It’s what DeepMind’s AlphaZero learned to do when it mastered Go and chess. A game of Go might only involve a few hundred moves. But to win, an AI must find a winning sequence of moves among a vast number of possible sequences. Imagine a number with 100 zeros at the end, says Gukov.&lt;/p&gt; 
 &lt;p&gt;But that’s still tiny compared with the number of possible sequences that could be involved in proving or disproving a very hard math problem: “A proof path with a thousand or a million moves involves a number with a thousand or a million zeros,” says Gukov.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;No AI system can sift through that many possibilities. To address this, Gukov and his colleagues developed a system that shortens the length of a path by combining multiple moves into single supermoves. It’s like having boots that let you take giant strides: instead of taking 2,000 steps to walk a mile, you can now walk it in 20.&lt;/p&gt;  &lt;p&gt;The challenge was figuring out which moves to replace with supermoves. In a series of experiments, the researchers came up with a system in which one reinforcement-learning model suggests new moves and a second model checks to see if those moves help.&lt;/p&gt;  &lt;p&gt;They used this approach to make a breakthrough in a math problem called the Andrews-Curtis conjecture, a puzzle that has been unsolved for 60 years. It’s a problem that every professional mathematician will know, says Gukov.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;(An aside for math stans only: The AC conjecture states that a particular way of describing a type of set called a trivial group can be translated into a different but equivalent description with a certain sequence of steps. Most mathematicians think the AC conjecture is false, but nobody knows how to prove that. Gukov admits himself that it is an intellectual curiosity rather than a practical problem, but an important problem for mathematicians nonetheless.)&lt;/p&gt;  &lt;p&gt;Gukov and his colleagues didn’t solve the AC conjecture, but they found that a counterexample (suggesting that the conjecture is false) proposed 40 years ago was itself false. “It’s been a major direction of attack for 40 years,” says Gukov. With the help of AI, they showed that this direction was in fact a dead end.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“Ruling out possible counterexamples is a worthwhile thing,” says Bridson. “It can close off blind alleys, something you might spend a year of your life exploring.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;True, Gukov checked off just one piece of one esoteric puzzle. But he thinks the approach will work in any scenario where you need to find a long sequence of unknown moves, and he now plans to try it out on other problems.&lt;/p&gt;  &lt;p&gt;“Maybe it will lead to something that will help AI in general,” he says. “Because it’s teaching reinforcement learning models to go beyond their training. To me it’s basically about thinking outside of the box—miles away, megaparsecs away.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;3/ Can AI ever provide real insight?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Thinking outside the box is exactly what mathematicians need to solve hard problems. Math is often thought to involve robotic, step-by-step procedures. But advanced math is an experimental pursuit, involving trial and error and flashes of insight.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt; &lt;p&gt;That’s where tools like AlphaEvolve come in. Google DeepMind’s latest model asks an LLM to generate code to solve a particular math problem. A second model then evaluates the proposed solutions, picks the best, and sends them back to the LLM to be improved. After hundreds of rounds of trial and error, AlphaEvolve was able to come up with solutions to a wide range of math problems that were better than anything people had yet come up with. But it can also work as a collaborative tool: at any step, humans can share their own insight with the LLM, prompting it with specific instructions.&lt;/p&gt;  &lt;p&gt;This kind of exploration is key to advanced mathematics. “I’m often looking for interesting phenomena and pushing myself in a certain direction,” says Geordie Williamson, a mathematician at the University of Sydney in Australia. “Like: ‘Let me look down this little alley. Oh, I found something!’”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_18"&gt;&lt;p&gt;Williamson worked with Meta on an AI tool called PatternBoost, designed to support this kind of exploration. PatternBoost can take a mathematical idea or statement and generate similar ones. “It’s like: ‘Here’s a bunch of interesting things. I don’t know what’s going on, but can you produce more interesting things like that?’” he says.&lt;/p&gt;  &lt;p&gt;Such brainstorming is essential work in math. It’s how new ideas get conjured. Take the icosahedron, says Williamson: “It’s a beautiful example of this, which I kind of keep coming back to in my own work.” The icosahedron is a 20-sided 3D object where all the faces are triangles (think of a 20-sided die). The icosahedron is the largest of a family of exactly five such objects: there’s the tetrahedron (four sides), cube (six sides), octahedron (eight sides), and dodecahedron (12 sides).&lt;/p&gt;  &lt;p&gt;Remarkably, the fact that there are exactly five of these objects was proved by mathematicians in ancient Greece. “At the time that this theorem was proved, the icosahedron didn’t exist,” says Williamson. “You can’t go to a quarry and find it—someone found it in their mind. And the icosahedron goes on to have a profound effect on mathematics. It’s still influencing us today in very, very profound ways.”&lt;/p&gt;  &lt;p&gt;For Williamson, the exciting potential of tools like PatternBoost is that they might help people discover future mathematical objects like the icosahedron that go on to shape the way math is done. But we’re not there yet. “AI can contribute in a meaningful way to research-level problems,” he says. “But we're certainly not getting inundated with new theorems at this stage.”&lt;/p&gt;  &lt;p&gt;Ultimately, it comes down to the fact that machines still lack what you might call intuition or creative thinking. Williamson sums it up like this: We now have AI that can beat humans when it knows the rules of the game. “But it’s one thing for a computer to play Go at a superhuman level and another thing for the computer to invent the game of Go.”&lt;/p&gt;  &lt;p&gt;“I think that applies to advanced mathematics,” he says. “Breakthroughs come from a new way of thinking about something, which is akin to finding completely new moves in a game. And I don’t really think we understand where those really brilliant moves in deep mathematics come from.”&lt;/p&gt;  &lt;p&gt;Perhaps AI tools like AlphaEvolve and PatternBoost are best thought of as advance scouts for human intuition. They can discover new directions and point out dead ends, saving mathematicians months or years of work. But the true breakthroughs will still come from the minds of people, as has been the case for thousands of years.&lt;/p&gt;  &lt;p&gt;For now, at least. “There’s plenty of tech companies that tell us that won’t last long,” says Williamson. “But you know—we’ll see.”&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/04/1117753/whats-next-for-ai-and-math/</guid><pubDate>Wed, 04 Jun 2025 08:21:24 +0000</pubDate></item><item><title>The Download: AI’s role in math, and calculating its energy footprint (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/04/1117829/the-download-ai-math-energy/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;What’s next for AI and math&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;The modern world is built on mathematics. Math lets us model complex systems such as the way air flows around an aircraft, the way financial markets fluctuate, and the way blood flows through the heart. Mathematicians have used computers for decades, but the new vision is that AI might help them crack problems that were previously uncrackable.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;However, there’s a huge difference between AI that can solve the kinds of problems set in high school—math that the latest generation of models has already mastered—and AI that could (in theory) solve the kinds of problems that professional mathematicians spend careers chipping away at.&amp;nbsp;Here are three ways to understand that gulf.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;—Will Douglas Heaven&lt;/em&gt;&lt;strong&gt;This story is from our What’s Next series, which looks across industries, trends, and technologies to give you a first look at the future. You can read the rest of them&amp;nbsp;here.&lt;/strong&gt;&lt;/p&gt;   
 &lt;h4 class="wp-block-heading has-medium-font-size"&gt;&lt;strong&gt;Inside the effort to tally AI’s energy appetite&lt;/strong&gt;&lt;/h4&gt;  &lt;p&gt;&lt;em&gt;—James O'Donnell&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;After working on it for months, my colleague Casey Crownhart and I finally saw our story on AI’s energy and emissions burden go live last week.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The initial goal sounded simple: Calculate how much energy is used when we interact with a chatbot, then tally that up to understand why leaders in tech and politics are so keen to harness unprecedented levels of electricity to power AI and reshape our energy grids in the process.&lt;/p&gt;&lt;p&gt;It was, of course, not so simple. After speaking with dozens of researchers, we realized that the common understanding of AI’s energy appetite is full of holes. I encourage you to&amp;nbsp;read the full story, which has some incredible graphics to help you understand this topic.&amp;nbsp;But here are three takeaways I have after the project.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story originally appeared in The Algorithm, our weekly newsletter on AI. To get it in your inbox first,&amp;nbsp;sign up here, and check out the&amp;nbsp;rest of our Power Hungry package about AI here.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Elon Musk has turned on Trump&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;He called Trump’s domestic policy agenda a “disgusting abomination.” (NYT&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;House Speaker Mike Johnson has, naturally, hit back.&lt;/em&gt;&amp;nbsp;(Insider&amp;nbsp;$)&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;2 NASA is in crisis&lt;/strong&gt;&lt;br /&gt;Its budget has been cut by a quarter, and now its new leader has had his nomination revoked. (New Scientist&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;What’s next for NASA’s giant moon rocket?&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3 Here’s how Big Tech plans to wield AI&lt;/strong&gt;&lt;br /&gt;To build ‘everything apps’ that keep you inside their ecosystem, forever. (The Atlantic&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;The trouble is, the experience isn’t always slick enough, as Google has discovered with its ‘Ask Photos’ feature&lt;/em&gt;. (The Verge&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;How to fight your instinct to blindly trust AI.&lt;/em&gt;&amp;nbsp;(WP&amp;nbsp;$)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;4 Meta has signed a 20-year deal to buy nuclear power&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;It’s the latest in a race to try to keep up with AI’s surging energy demands. (ABC)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Can nuclear power really fuel the rise of AI?&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;5 Extreme heat takes a huge toll on people’s mental health&lt;/strong&gt;&lt;br /&gt;It’s yet another issue we’re failing to prepare for, as summers get hotter and hotter. (Scientific American&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;The quest to protect farmworkers from extreme heat.&amp;nbsp;&lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 China’s robotaxi companies are planning to expand in the Middle East&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;And they’re getting a warmer welcome than in the US or Europe. (WSJ&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;China’s EV giants are also betting big on humanoid robots.&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;7 AI will supercharge hackers&lt;/strong&gt;&lt;br /&gt;The full impact of new AI techniques is yet to be felt, but experts say it’s only a matter of time. (Wired&amp;nbsp;$)&lt;br /&gt;+&lt;em&gt;&amp;nbsp;Five ways criminals are using AI.&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;8 It’s an exciting time to be working on Alzheimer’s treatments&amp;nbsp;💊&lt;/strong&gt;&lt;br /&gt;12 of them are moving to the final phase of clinical trials this year. (The Economist&amp;nbsp;$)&lt;br /&gt;+&lt;em&gt;&amp;nbsp;The innovation that gets an Alzheimer’s drug through the blood-brain barrier.&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;9 Workers are being subjected to more and more surveillance&lt;/strong&gt;&lt;br /&gt;Not just in the gig economy either—’bossware’ is increasingly appearing in offices too. (Rest of World)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 Noughties nostalgia is rife on TikTok&lt;/strong&gt;&lt;br /&gt;It was a pretty fun decade, to be fair. (The Guardian)&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;&amp;nbsp;“This is scientific heaven. Or it used to be.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Tom Rapoport, a 77-year-old Harvard Medical School professor from Germany, expresses his sadness about Trump’s cuts to US science funding to the&amp;nbsp;New York Times.&amp;nbsp;&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Long rows of supercomputers with the name &amp;quot;Frontier&amp;quot; visible on the end" class="wp-image-1079868" src="https://wp.technologyreview.com/wp-content/uploads/2023/09/52117623798_84faf38201_OLCF.jpg?w=3000" /&gt; &lt;/figure&gt; &lt;/div&gt; &lt;h3 class="wp-block-heading"&gt;What’s next for the world’s fastest supercomputers&lt;/h3&gt;  &lt;p&gt;When the Frontier supercomputer came online in 2022, it marked the dawn of so-called exascale computing, with machines that can execute an exaflop—or a quintillion (1018) floating point operations a second.&lt;/p&gt;&lt;p&gt;Since then, scientists have geared up to make more of these blazingly fast computers: several exascale machines are due to come online in the US and Europe.&lt;/p&gt;&lt;p&gt;But speed itself isn’t the endgame. Researchers hope to pursue previously unanswerable questions about nature—and to design new technologies in areas from transportation to medicine.&amp;nbsp;Read the full story.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;—Sophia Chen&lt;/em&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ If tracking tube trains in London is your thing, you’ll love this&amp;nbsp;live map.&lt;br /&gt;+ Take a truly bonkers trip down memory lane, courtesy of these&amp;nbsp;FBI artifacts.&lt;br /&gt;+ Netflix’s&amp;nbsp;Frankenstein&amp;nbsp;looks pretty intense.&lt;br /&gt;+ Why&amp;nbsp;landlines&amp;nbsp;are so darn spooky 📞&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;What’s next for AI and math&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;The modern world is built on mathematics. Math lets us model complex systems such as the way air flows around an aircraft, the way financial markets fluctuate, and the way blood flows through the heart. Mathematicians have used computers for decades, but the new vision is that AI might help them crack problems that were previously uncrackable.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;However, there’s a huge difference between AI that can solve the kinds of problems set in high school—math that the latest generation of models has already mastered—and AI that could (in theory) solve the kinds of problems that professional mathematicians spend careers chipping away at.&amp;nbsp;Here are three ways to understand that gulf.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;—Will Douglas Heaven&lt;/em&gt;&lt;strong&gt;This story is from our What’s Next series, which looks across industries, trends, and technologies to give you a first look at the future. You can read the rest of them&amp;nbsp;here.&lt;/strong&gt;&lt;/p&gt;   
 &lt;h4 class="wp-block-heading has-medium-font-size"&gt;&lt;strong&gt;Inside the effort to tally AI’s energy appetite&lt;/strong&gt;&lt;/h4&gt;  &lt;p&gt;&lt;em&gt;—James O'Donnell&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;After working on it for months, my colleague Casey Crownhart and I finally saw our story on AI’s energy and emissions burden go live last week.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The initial goal sounded simple: Calculate how much energy is used when we interact with a chatbot, then tally that up to understand why leaders in tech and politics are so keen to harness unprecedented levels of electricity to power AI and reshape our energy grids in the process.&lt;/p&gt;&lt;p&gt;It was, of course, not so simple. After speaking with dozens of researchers, we realized that the common understanding of AI’s energy appetite is full of holes. I encourage you to&amp;nbsp;read the full story, which has some incredible graphics to help you understand this topic.&amp;nbsp;But here are three takeaways I have after the project.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story originally appeared in The Algorithm, our weekly newsletter on AI. To get it in your inbox first,&amp;nbsp;sign up here, and check out the&amp;nbsp;rest of our Power Hungry package about AI here.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Elon Musk has turned on Trump&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;He called Trump’s domestic policy agenda a “disgusting abomination.” (NYT&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;House Speaker Mike Johnson has, naturally, hit back.&lt;/em&gt;&amp;nbsp;(Insider&amp;nbsp;$)&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;2 NASA is in crisis&lt;/strong&gt;&lt;br /&gt;Its budget has been cut by a quarter, and now its new leader has had his nomination revoked. (New Scientist&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;What’s next for NASA’s giant moon rocket?&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3 Here’s how Big Tech plans to wield AI&lt;/strong&gt;&lt;br /&gt;To build ‘everything apps’ that keep you inside their ecosystem, forever. (The Atlantic&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;The trouble is, the experience isn’t always slick enough, as Google has discovered with its ‘Ask Photos’ feature&lt;/em&gt;. (The Verge&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;How to fight your instinct to blindly trust AI.&lt;/em&gt;&amp;nbsp;(WP&amp;nbsp;$)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;4 Meta has signed a 20-year deal to buy nuclear power&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;It’s the latest in a race to try to keep up with AI’s surging energy demands. (ABC)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Can nuclear power really fuel the rise of AI?&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;5 Extreme heat takes a huge toll on people’s mental health&lt;/strong&gt;&lt;br /&gt;It’s yet another issue we’re failing to prepare for, as summers get hotter and hotter. (Scientific American&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;The quest to protect farmworkers from extreme heat.&amp;nbsp;&lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 China’s robotaxi companies are planning to expand in the Middle East&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;And they’re getting a warmer welcome than in the US or Europe. (WSJ&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;China’s EV giants are also betting big on humanoid robots.&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;7 AI will supercharge hackers&lt;/strong&gt;&lt;br /&gt;The full impact of new AI techniques is yet to be felt, but experts say it’s only a matter of time. (Wired&amp;nbsp;$)&lt;br /&gt;+&lt;em&gt;&amp;nbsp;Five ways criminals are using AI.&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;8 It’s an exciting time to be working on Alzheimer’s treatments&amp;nbsp;💊&lt;/strong&gt;&lt;br /&gt;12 of them are moving to the final phase of clinical trials this year. (The Economist&amp;nbsp;$)&lt;br /&gt;+&lt;em&gt;&amp;nbsp;The innovation that gets an Alzheimer’s drug through the blood-brain barrier.&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;9 Workers are being subjected to more and more surveillance&lt;/strong&gt;&lt;br /&gt;Not just in the gig economy either—’bossware’ is increasingly appearing in offices too. (Rest of World)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 Noughties nostalgia is rife on TikTok&lt;/strong&gt;&lt;br /&gt;It was a pretty fun decade, to be fair. (The Guardian)&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;&amp;nbsp;“This is scientific heaven. Or it used to be.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Tom Rapoport, a 77-year-old Harvard Medical School professor from Germany, expresses his sadness about Trump’s cuts to US science funding to the&amp;nbsp;New York Times.&amp;nbsp;&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Long rows of supercomputers with the name &amp;quot;Frontier&amp;quot; visible on the end" class="wp-image-1079868" src="https://wp.technologyreview.com/wp-content/uploads/2023/09/52117623798_84faf38201_OLCF.jpg?w=3000" /&gt; &lt;/figure&gt; &lt;/div&gt; &lt;h3 class="wp-block-heading"&gt;What’s next for the world’s fastest supercomputers&lt;/h3&gt;  &lt;p&gt;When the Frontier supercomputer came online in 2022, it marked the dawn of so-called exascale computing, with machines that can execute an exaflop—or a quintillion (1018) floating point operations a second.&lt;/p&gt;&lt;p&gt;Since then, scientists have geared up to make more of these blazingly fast computers: several exascale machines are due to come online in the US and Europe.&lt;/p&gt;&lt;p&gt;But speed itself isn’t the endgame. Researchers hope to pursue previously unanswerable questions about nature—and to design new technologies in areas from transportation to medicine.&amp;nbsp;Read the full story.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;—Sophia Chen&lt;/em&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ If tracking tube trains in London is your thing, you’ll love this&amp;nbsp;live map.&lt;br /&gt;+ Take a truly bonkers trip down memory lane, courtesy of these&amp;nbsp;FBI artifacts.&lt;br /&gt;+ Netflix’s&amp;nbsp;Frankenstein&amp;nbsp;looks pretty intense.&lt;br /&gt;+ Why&amp;nbsp;landlines&amp;nbsp;are so darn spooky 📞&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/04/1117829/the-download-ai-math-energy/</guid><pubDate>Wed, 04 Jun 2025 12:02:00 +0000</pubDate></item><item><title>NVIDIA RTX Blackwell GPUs Accelerate Professional-Grade Video Editing (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/rtx-ai-garage-studio-adobe-premiere-davinci-resolve-blackwell/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;4:2:2 cameras — capable of capturing double the color information compared with most standard cameras — are becoming widely available for consumers. At the same time, generative AI video models are rapidly increasing in functionality and quality, making new tools and workflows possible.&lt;/p&gt;
&lt;p&gt;NVIDIA RTX GPUs based on the NVIDIA Blackwell architecture include dedicated hardware to encode and decode 4:2:2 video, and come with fifth-generation Tensor Cores designed to accelerate AI and deep learning workloads.&lt;/p&gt;
&lt;p&gt;GeForce RTX 50 Series and NVIDIA RTX PRO Blackwell Series are primed to meet this demand, powering generative AI, new AI features and state-of-the-art video editing workflows for quicker cuts and faster exports.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;4:2:2 Goes Mainstream&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;4:2:2 10-bit compatible video cameras are on the rise.&lt;/p&gt;
&lt;p&gt;These cameras, which were traditionally reserved for professional use due to their high cost, are becoming more cost-friendly, with major manufacturers offering them at prices under $600.&lt;/p&gt;
&lt;p&gt;4:2:2 cameras can capture double the color information compared with standard 4:2:0 cameras while only increasing raw file sizes by 30%.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_81463"&gt;&lt;img alt="alt" class="wp-image-81463 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/4-2-2-video-cameras-1680x945.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-81463"&gt;4:2:2 video cameras are on the rise, thanks to more affordable prices. Creators have more camera options than ever at lower entry points.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Standard cameras typically use 4:2:0 8-bit color compression, capable of capturing only a fraction of color information. While 4:2:0 is acceptable for video playback on browsers, professional video editors demand cameras that capture 4:2:2 color accuracy and fidelity, while keeping file sizes reasonable.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;The downside of 4:2:2 is that the additional color information requires more computational power for playback, often leading to stuttering streams. As a result, many editors have had to create proxies before editing — a time-consuming process that requires additional storage and lowers fidelity while editing.&lt;/p&gt;
&lt;p&gt;The GeForce RTX 50 Series adds hardware acceleration for 4:2:2 encode and decode, helping solve this computational challenge. RTX 50 Series GPUs boast a 10x acceleration in 4:2:2 encoding and can decode up to 8K 75 frames per second — equivalent to 10x 4K 30fps streams per decoder.&lt;/p&gt;
&lt;p&gt;The most popular video editing apps, including Blackmagic Design’s DaVinci Resolve, CapCut and Wondershare Filmora, support NVIDIA hardware acceleration for 4:2:2 encode and decode. Adobe Premiere Pro offers decode support.&lt;/p&gt;
&lt;p&gt;Combining 4:2:2 support with NVIDIA hardware increases creative possibilities. 10-bit 4:2:2 retains more color information than 8-bit 4:2:0, resulting in more accurate color representations and better color grading results for video editors.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_81468"&gt;&lt;img alt="alt" class="size-large wp-image-81468" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/4-2-2-offers-more-accurate-color-representation-1680x945.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-81468"&gt;4:2:2 offers more accurate color representation for better color grading results.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The extra color data from 4:2:2 support allows for increased flexibility during color correction and grading for more detailed adjustments. Improved keying enables cleaner and more accurate extractions of subjects from background, as well as sharper edges for smaller keyed objects.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_81477"&gt;&lt;img alt="alt" class="size-full wp-image-81477" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/Cleaner-green-screen-1.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-81477"&gt;4:2:2 offers more accurate color representation for better color grading results.4:2:2 enables cleaner text in video content.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;figure class="wp-caption aligncenter" id="attachment_81481"&gt;&lt;img alt="alt" class="size-large wp-image-81481" height="939" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/Clearer-text-1680x939.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-81481"&gt;4:2:2 reduces file sizes without significantly impacting picture quality, offering an optimal balance between quality and storage.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2&gt;&lt;b&gt;Generative AI-Powered Video Editing&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Generative AI models are enabling video editors to generate filler video, extend clips, modify videos styles and apply advanced visual effects with speed and ease, drastically reducing production times.&lt;/p&gt;
&lt;p&gt;Popular models like WAN or LTX Video can generate higher-quality video with greater prompt accuracy and faster load times.&lt;/p&gt;
&lt;p&gt;GeForce RTX and NVIDIA RTX PRO GPUs based on NVIDIA Blackwell enable these large, complex models to run quickly and on device, with support thanks to NVIDIA CUDA optimizations for PyTorch. Plus, the fifth-generation Tensor Cores in these GPUs offer support for FP4 quantization, allowing developers and enthusiasts to improve performance by over 2x and halve the VRAM needed.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Cutting-Edge Video Editing AI Features&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Modern video editing apps provide an impressive array of advanced AI features — accelerated by GeForce RTX and NVIDIA RTX PRO GPUs.&lt;/p&gt;
&lt;p&gt;DaVinci Resolve Studio 20, now in general release, adds new AI effects and integrates NVIDIA TensorRT to optimize AI performance. One of the new features, UltraNR Noise Reduction, is an AI-driven noise reduction mode that intelligently targets and reduces digital noise in video footage to maintain image clarity while minimizing softening. UltraNR Noise Reduction runs up to 75% faster on the GeForce RTX 5090 GPU than the previous generation.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Magic Mask is another AI-powered feature in DaVinci Resolve that enables users to quickly and accurately select and track objects, people or features within a scene, simplifying the process of creating masks and effects. Magic Mask v2 adds a paint brush to further adjust masking selections for more accurate and faster workflows.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Topaz Video AI Pro video enhancement software uses AI models like Gaia and Artemis to intelligently increase video resolution to 4K, 8K and even 16K — adding detail and sharpness while minimizing artifacts and noise. The software also benefits from TensorRT acceleration.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Topaz Starlight mini, the first local desktop diffusion model for video enhancement, can enhance footage — from tricky 8/16mm film to de-interlaced mini-DV video — that may otherwise be challenging for traditional AI models to handle. The model delivers exceptional quality at the cost of intensive compute requirements, meaning it can &lt;i&gt;only&lt;/i&gt; run locally on RTX GPUs.&lt;/p&gt;
&lt;p&gt;Adobe Premiere Pro recently released several new AI features, such as Adobe Media Intelligence, which uses AI to analyze footage and apply semantic tags to clips. This lets users more easily and quickly find specific footage by describing its content, including objects, locations, camera angles and even transcribed spoken words. Media Intelligence runs 30% faster on the GeForce RTX 5090 Laptop GPU compared with the GeForce RTX 4090 Laptop GPU.&lt;/p&gt;
&lt;p&gt;Adobe’s Enhance Speech feature improves the quality of recorded speech by filtering out unwanted noise and making the audio sound clearer. Enhance Speech runs 7x faster on GeForce RTX 5090 Laptop GPUs compared with the MacBook Pro M4 Max.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Cut Like a Pro&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;GeForce RTX and NVIDIA RTX PRO GPUs are built to deliver the computational power needed for advanced video editing workflows.&lt;/p&gt;
&lt;p&gt;These GPUs contain powerful NVIDIA hardware decoders (NVDEC) to unlock smooth playback and scrubbing of high-resolution video footage and multi-stream videos without the need for proxies. NVDEC is supported in Adobe Premiere Pro, CapCut, DaVinci Resolve, Vegas Pro and Wondershare Filmora.&lt;/p&gt;
&lt;p&gt;Creative apps use these additional encoders in GeForce RTX 5080 and 5090 GPUs, as well as RTX PRO 6000, 5000, 4500 and 4000 Blackwell GPUs — and now features support for 4:2:2.&lt;/p&gt;
&lt;p&gt;Creators can use the RTX 5080 and 5090, for example, to import 5x 8K30 or 20x 4K30 streams at once, or import 10x 4K60 to do multi-camera editing and review multiple camera angles without slowdown. With the RTX PRO 6000, this can be boosted to up to 10x 8K30 or 40x 4K30 streams.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_81474"&gt;&lt;img alt="alt" class="size-full wp-image-81474" height="288" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/RTX-encoders-and-decoders.png" width="1246" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-81474"&gt;GeForce RTX and NVIDIA RTX PRO GPU Laptop GPU encoders and decoders.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;NVIDIA CUDA cores accelerate video and image processing effects such as motion tracking, sharpening, upsampling, transition effects and other computationally intensive tasks. They also accelerate rendering times, enable real-time previews while working with high-resolution video footage and speed up AI features, such as automatic color correction, object removal and noise reduction.&lt;/p&gt;
&lt;p&gt;When it’s time to export, video editors that use the GeForce RTX 50 Series ninth-generation NVIDIA video encoder can get a 5% improvement in video quality on HEVC and AV1 encoding (BD-BR), resulting in higher-quality exports at the same bitrates.&lt;/p&gt;
&lt;p&gt;Plus, a new Ultra High Quality (UHQ) mode available in the latest Blackwell encoder boosts quality by an additional 5% for HEVC and AV1 and is backwards-compatible with the GeForce RTX 40 Series.&lt;/p&gt;
&lt;p&gt;DaVinci Resolve, CapCut and Filmora also support multi-encoder encoding, either via split encoding — where an input frame is divided into three parts, each processed by a different NVENC encoder — or simultaneous scene encoding, in which a video is split by groups of pictures that are each sent to an encoder to batch the operation for up to 2.5x faster export performance.&lt;/p&gt;
&lt;p&gt;Tune in to NVIDIA founder and CEO Jensen Huang’s keynote at NVIDIA GTC Paris at VivaTech on June 11. Check out full-day workshops on June 10 and two days of technical sessions, training and certifications.&lt;/p&gt;
&lt;p&gt;Stay tuned for more RTX and AI powered advances in content creation.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Each week, the &lt;/i&gt;&lt;i&gt;RTX AI Garage&lt;/i&gt; &lt;i&gt;blog series features community-driven AI innovations and content for those looking to learn more about NIM microservices and AI Blueprints, as well as building &lt;/i&gt;&lt;i&gt;AI agents&lt;/i&gt;&lt;i&gt;, creative workflows, digital humans, productivity apps and more on AI PCs and workstations.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; — and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;4:2:2 cameras — capable of capturing double the color information compared with most standard cameras — are becoming widely available for consumers. At the same time, generative AI video models are rapidly increasing in functionality and quality, making new tools and workflows possible.&lt;/p&gt;
&lt;p&gt;NVIDIA RTX GPUs based on the NVIDIA Blackwell architecture include dedicated hardware to encode and decode 4:2:2 video, and come with fifth-generation Tensor Cores designed to accelerate AI and deep learning workloads.&lt;/p&gt;
&lt;p&gt;GeForce RTX 50 Series and NVIDIA RTX PRO Blackwell Series are primed to meet this demand, powering generative AI, new AI features and state-of-the-art video editing workflows for quicker cuts and faster exports.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;4:2:2 Goes Mainstream&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;4:2:2 10-bit compatible video cameras are on the rise.&lt;/p&gt;
&lt;p&gt;These cameras, which were traditionally reserved for professional use due to their high cost, are becoming more cost-friendly, with major manufacturers offering them at prices under $600.&lt;/p&gt;
&lt;p&gt;4:2:2 cameras can capture double the color information compared with standard 4:2:0 cameras while only increasing raw file sizes by 30%.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_81463"&gt;&lt;img alt="alt" class="wp-image-81463 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/4-2-2-video-cameras-1680x945.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-81463"&gt;4:2:2 video cameras are on the rise, thanks to more affordable prices. Creators have more camera options than ever at lower entry points.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Standard cameras typically use 4:2:0 8-bit color compression, capable of capturing only a fraction of color information. While 4:2:0 is acceptable for video playback on browsers, professional video editors demand cameras that capture 4:2:2 color accuracy and fidelity, while keeping file sizes reasonable.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;The downside of 4:2:2 is that the additional color information requires more computational power for playback, often leading to stuttering streams. As a result, many editors have had to create proxies before editing — a time-consuming process that requires additional storage and lowers fidelity while editing.&lt;/p&gt;
&lt;p&gt;The GeForce RTX 50 Series adds hardware acceleration for 4:2:2 encode and decode, helping solve this computational challenge. RTX 50 Series GPUs boast a 10x acceleration in 4:2:2 encoding and can decode up to 8K 75 frames per second — equivalent to 10x 4K 30fps streams per decoder.&lt;/p&gt;
&lt;p&gt;The most popular video editing apps, including Blackmagic Design’s DaVinci Resolve, CapCut and Wondershare Filmora, support NVIDIA hardware acceleration for 4:2:2 encode and decode. Adobe Premiere Pro offers decode support.&lt;/p&gt;
&lt;p&gt;Combining 4:2:2 support with NVIDIA hardware increases creative possibilities. 10-bit 4:2:2 retains more color information than 8-bit 4:2:0, resulting in more accurate color representations and better color grading results for video editors.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_81468"&gt;&lt;img alt="alt" class="size-large wp-image-81468" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/4-2-2-offers-more-accurate-color-representation-1680x945.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-81468"&gt;4:2:2 offers more accurate color representation for better color grading results.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The extra color data from 4:2:2 support allows for increased flexibility during color correction and grading for more detailed adjustments. Improved keying enables cleaner and more accurate extractions of subjects from background, as well as sharper edges for smaller keyed objects.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_81477"&gt;&lt;img alt="alt" class="size-full wp-image-81477" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/Cleaner-green-screen-1.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-81477"&gt;4:2:2 offers more accurate color representation for better color grading results.4:2:2 enables cleaner text in video content.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;figure class="wp-caption aligncenter" id="attachment_81481"&gt;&lt;img alt="alt" class="size-large wp-image-81481" height="939" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/Clearer-text-1680x939.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-81481"&gt;4:2:2 reduces file sizes without significantly impacting picture quality, offering an optimal balance between quality and storage.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2&gt;&lt;b&gt;Generative AI-Powered Video Editing&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Generative AI models are enabling video editors to generate filler video, extend clips, modify videos styles and apply advanced visual effects with speed and ease, drastically reducing production times.&lt;/p&gt;
&lt;p&gt;Popular models like WAN or LTX Video can generate higher-quality video with greater prompt accuracy and faster load times.&lt;/p&gt;
&lt;p&gt;GeForce RTX and NVIDIA RTX PRO GPUs based on NVIDIA Blackwell enable these large, complex models to run quickly and on device, with support thanks to NVIDIA CUDA optimizations for PyTorch. Plus, the fifth-generation Tensor Cores in these GPUs offer support for FP4 quantization, allowing developers and enthusiasts to improve performance by over 2x and halve the VRAM needed.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Cutting-Edge Video Editing AI Features&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Modern video editing apps provide an impressive array of advanced AI features — accelerated by GeForce RTX and NVIDIA RTX PRO GPUs.&lt;/p&gt;
&lt;p&gt;DaVinci Resolve Studio 20, now in general release, adds new AI effects and integrates NVIDIA TensorRT to optimize AI performance. One of the new features, UltraNR Noise Reduction, is an AI-driven noise reduction mode that intelligently targets and reduces digital noise in video footage to maintain image clarity while minimizing softening. UltraNR Noise Reduction runs up to 75% faster on the GeForce RTX 5090 GPU than the previous generation.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Magic Mask is another AI-powered feature in DaVinci Resolve that enables users to quickly and accurately select and track objects, people or features within a scene, simplifying the process of creating masks and effects. Magic Mask v2 adds a paint brush to further adjust masking selections for more accurate and faster workflows.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Topaz Video AI Pro video enhancement software uses AI models like Gaia and Artemis to intelligently increase video resolution to 4K, 8K and even 16K — adding detail and sharpness while minimizing artifacts and noise. The software also benefits from TensorRT acceleration.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Topaz Starlight mini, the first local desktop diffusion model for video enhancement, can enhance footage — from tricky 8/16mm film to de-interlaced mini-DV video — that may otherwise be challenging for traditional AI models to handle. The model delivers exceptional quality at the cost of intensive compute requirements, meaning it can &lt;i&gt;only&lt;/i&gt; run locally on RTX GPUs.&lt;/p&gt;
&lt;p&gt;Adobe Premiere Pro recently released several new AI features, such as Adobe Media Intelligence, which uses AI to analyze footage and apply semantic tags to clips. This lets users more easily and quickly find specific footage by describing its content, including objects, locations, camera angles and even transcribed spoken words. Media Intelligence runs 30% faster on the GeForce RTX 5090 Laptop GPU compared with the GeForce RTX 4090 Laptop GPU.&lt;/p&gt;
&lt;p&gt;Adobe’s Enhance Speech feature improves the quality of recorded speech by filtering out unwanted noise and making the audio sound clearer. Enhance Speech runs 7x faster on GeForce RTX 5090 Laptop GPUs compared with the MacBook Pro M4 Max.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Cut Like a Pro&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;GeForce RTX and NVIDIA RTX PRO GPUs are built to deliver the computational power needed for advanced video editing workflows.&lt;/p&gt;
&lt;p&gt;These GPUs contain powerful NVIDIA hardware decoders (NVDEC) to unlock smooth playback and scrubbing of high-resolution video footage and multi-stream videos without the need for proxies. NVDEC is supported in Adobe Premiere Pro, CapCut, DaVinci Resolve, Vegas Pro and Wondershare Filmora.&lt;/p&gt;
&lt;p&gt;Creative apps use these additional encoders in GeForce RTX 5080 and 5090 GPUs, as well as RTX PRO 6000, 5000, 4500 and 4000 Blackwell GPUs — and now features support for 4:2:2.&lt;/p&gt;
&lt;p&gt;Creators can use the RTX 5080 and 5090, for example, to import 5x 8K30 or 20x 4K30 streams at once, or import 10x 4K60 to do multi-camera editing and review multiple camera angles without slowdown. With the RTX PRO 6000, this can be boosted to up to 10x 8K30 or 40x 4K30 streams.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_81474"&gt;&lt;img alt="alt" class="size-full wp-image-81474" height="288" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/RTX-encoders-and-decoders.png" width="1246" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-81474"&gt;GeForce RTX and NVIDIA RTX PRO GPU Laptop GPU encoders and decoders.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;NVIDIA CUDA cores accelerate video and image processing effects such as motion tracking, sharpening, upsampling, transition effects and other computationally intensive tasks. They also accelerate rendering times, enable real-time previews while working with high-resolution video footage and speed up AI features, such as automatic color correction, object removal and noise reduction.&lt;/p&gt;
&lt;p&gt;When it’s time to export, video editors that use the GeForce RTX 50 Series ninth-generation NVIDIA video encoder can get a 5% improvement in video quality on HEVC and AV1 encoding (BD-BR), resulting in higher-quality exports at the same bitrates.&lt;/p&gt;
&lt;p&gt;Plus, a new Ultra High Quality (UHQ) mode available in the latest Blackwell encoder boosts quality by an additional 5% for HEVC and AV1 and is backwards-compatible with the GeForce RTX 40 Series.&lt;/p&gt;
&lt;p&gt;DaVinci Resolve, CapCut and Filmora also support multi-encoder encoding, either via split encoding — where an input frame is divided into three parts, each processed by a different NVENC encoder — or simultaneous scene encoding, in which a video is split by groups of pictures that are each sent to an encoder to batch the operation for up to 2.5x faster export performance.&lt;/p&gt;
&lt;p&gt;Tune in to NVIDIA founder and CEO Jensen Huang’s keynote at NVIDIA GTC Paris at VivaTech on June 11. Check out full-day workshops on June 10 and two days of technical sessions, training and certifications.&lt;/p&gt;
&lt;p&gt;Stay tuned for more RTX and AI powered advances in content creation.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Each week, the &lt;/i&gt;&lt;i&gt;RTX AI Garage&lt;/i&gt; &lt;i&gt;blog series features community-driven AI innovations and content for those looking to learn more about NIM microservices and AI Blueprints, as well as building &lt;/i&gt;&lt;i&gt;AI agents&lt;/i&gt;&lt;i&gt;, creative workflows, digital humans, productivity apps and more on AI PCs and workstations.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; — and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/rtx-ai-garage-studio-adobe-premiere-davinci-resolve-blackwell/</guid><pubDate>Wed, 04 Jun 2025 13:00:58 +0000</pubDate></item><item><title>AGI Is Not Multimodal (The Gradient)</title><link>https://thegradient.pub/agi-is-not-multimodal/</link><description>&lt;blockquote&gt;"In projecting language back as the model for thought, we lose sight of the tacit embodied understanding that undergirds our intelligence." –Terry Winograd&lt;/blockquote&gt;&lt;p&gt;The recent successes of generative AI models have convinced some that AGI is imminent. While these models appear to capture the essence of human intelligence, they defy even our most basic intuitions about it. They have emerged not because they are thoughtful solutions to the problem of intelligence, but because they &lt;em&gt;scaled&lt;/em&gt; effectively on hardware we already had. Seduced by the fruits of scale, some have come to believe that it provides a clear pathway to AGI. The most emblematic case of this is the multimodal approach, in which massive modular networks are optimized for an array of modalities that, taken together, &lt;em&gt;appear&lt;/em&gt; general. However, I argue that this strategy is sure to fail in the near term; it will not lead to human-level AGI that can, e.g., perform sensorimotor reasoning, motion planning, and social coordination. Instead of trying to glue modalities together into a patchwork AGI, &lt;strong&gt;we should pursue approaches to intelligence that treat embodiment and interaction with the environment as primary&lt;/strong&gt;, and see modality-centered processing as emergent phenomena.&lt;/p&gt;&lt;p&gt;Preface: Disembodied definitions of Artificial General Intelligence — emphasis on &lt;em&gt;general&lt;/em&gt; — exclude crucial problem spaces that we should expect AGI to be able to solve. &lt;strong&gt;A true AGI must be general across all domains.&lt;/strong&gt; Any &lt;em&gt;complete&lt;/em&gt; definition must at least include the ability to solve problems that originate in physical reality, e.g. repairing a car, untying a knot, preparing food, etc. As I will discuss in the next section, &lt;strong&gt;what is needed for these problems is a form of intelligence that is fundamentally situated in something like a physical world model&lt;/strong&gt;. For more discussion on this, look out for &lt;em&gt;Designing an Intelligence&lt;/em&gt;. Edited by George Konidaris, MIT Press, forthcoming.&lt;br /&gt;&lt;/p&gt;&lt;h2 id="why-we-need-the-world-and-how-llms-pretend-to-understand-it"&gt;Why We Need the World, and How LLMs Pretend to Understand It&lt;/h2&gt;&lt;p&gt;TLDR: I first argue that &lt;strong&gt;true AGI needs a physical understanding of the world&lt;/strong&gt;, as many problems cannot be converted into a problem of symbol manipulation. It has been suggested by some that LLMs are learning a model of the world through next token prediction, but it is more likely that LLMs are learning bags of heuristics to predict tokens. This leaves them with a superficial understanding of reality and contributes to false impressions of their intelligence.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;The most shocking result of the predict-next-token objective is that it yields AI models that reflect a deeply human-like understanding of the world, despite having never observed it like we have.&lt;/strong&gt; This result has led to confusion about what it means to &lt;em&gt;understand language&lt;/em&gt; and even to &lt;em&gt;understand the world&lt;/em&gt; — something we have long believed to be a prerequisite for language understanding. &lt;strong&gt;One explanation for the capabilities of LLMs comes from &lt;/strong&gt;&lt;strong&gt;an emerging theory&lt;/strong&gt;&lt;strong&gt; suggesting that they induce models of the world through next-token prediction&lt;/strong&gt;. Proponents of this theory cite the prowess of SOTA LLMs on various benchmarks, the convergence of large models to similar internal representations, and their favorite rendition of the idea that “language mirrors the structure of reality,” a notion that has been espoused at least by Plato, Wittgenstein, Foucault, and Eco. While I’m generally in support of digging up esoteric texts for research inspiration, I’m worried that this metaphor has been taken too literally. Do LLMs really learn implicit models of the world? How could they otherwise be so proficient at language?&lt;/p&gt;&lt;p&gt;One source of evidence in favor of the LLM world modeling hypothesis is the Othello paper, wherein researchers were able to predict the board of an Othello game from the hidden states of a transformer model trained on sequences of &lt;em&gt;legal&lt;/em&gt; &lt;em&gt;moves&lt;/em&gt;. However, there are &lt;em&gt;many&lt;/em&gt; issues with generalizing these results to models of natural language. For one, whereas Othello moves can &lt;em&gt;provably&lt;/em&gt; be used to deduce the full state of an Othello board,&lt;strong&gt; we have no reason to believe that a complete picture of the physical world can be inferred by a linguistic description. &lt;/strong&gt;What sets the game of Othello apart from many tasks in the physical world is that &lt;strong&gt;Othello fundamentally resides in the land of symbols, and is merely implemented using physical tokens to make it easier for humans to play.&lt;/strong&gt; A full game of Othello can be played with just pen and paper, but one can’t, e.g., sweep a floor, do dishes, or drive a car with just pen and paper. To solve such tasks, you need some physical conception of the world beyond what humans can merely &lt;em&gt;say&lt;/em&gt; about it. Whether that conception of the world is encoded in a formal world model or, e.g., a value function is up for debate, but it is clear that &lt;strong&gt;there are many problems in the physical world that &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;cannot&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; be &lt;/strong&gt;&lt;strong&gt;fully represented by a system of symbols&lt;/strong&gt;&lt;strong&gt; and solved with mere symbol manipulation.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Another issue stated in Melanie Mitchell’s recent piece and supported by this paper, is that there is evidence that &lt;strong&gt;generative models can score remarkably well on sequence prediction tasks while failing to learn models of the worlds that created such sequence data&lt;/strong&gt;, e.g. by learning comprehensive sets of idiosyncratic heuristics. E.g., it was pointed out in this blog post that OthelloGPT learned sequence prediction rules that don’t actually hold for all possible Othello games, like “if the token for B4 does not appear before A4 in the input string, then B4 is empty.” While one can argue that it doesn’t matter &lt;em&gt;how&lt;/em&gt; a world model predicts the next state of the world, it should raise suspicion when that prediction reflects a better understanding of the training data than the underlying world that led to such data. This, unfortunately, is the central fault of the predict-next-token objective, which seeks only to retain information relevant to the prediction of the next token. &lt;strong&gt;If it can be done with something easier to learn than a world model, it likely will be.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;To claim without caveat that predicting the &lt;em&gt;effects of earlier symbols on later symbols&lt;/em&gt; requires a model of the world like the ones humans generate from perception would be to abuse the “world model” notion. Unless we disagree on what the world is, it should be clear that a &lt;em&gt;true&lt;/em&gt; world model can be used to predict the next state of the &lt;em&gt;physical&lt;/em&gt; world given a history of states. Similar world models, which predict high fidelity observations of the physical world, are leveraged in many subfields of AI including model-based reinforcement learning, task and motion planning in robotics, causal world modeling, and areas of computer vision to solve problems instantiated in physical reality. LLMs are simply not running physics simulations in their latent next-token calculus when they ask you if your person, place, or thing is bigger than a breadbox. In fact, I conjecture that &lt;strong&gt;the behavior of LLMs is not thanks to a learned world model, but to brute force memorization of incomprehensibly abstract rules governing the behavior of symbols, i.e. a model of &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;syntax&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Quick primer:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Syntax&lt;/strong&gt; is a subfield of linguistics that studies how words of various grammatical categories (e.g. parts of speech) are arranged together into sentences, which can be parsed into syntax trees. &lt;em&gt;Syntax studies the structure of sentences and the atomic parts of speech that compose them.&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;strong&gt;Semantics&lt;/strong&gt; is another subfield concerned with the literal meaning of sentences, e.g., compiling “I am feeling chilly” into the &lt;em&gt;idea&lt;/em&gt; that you are experiencing cold. &lt;em&gt;Semantics boils language down to literal meaning, which is information about the world or human experience.&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Pragmatics&lt;/strong&gt; studies the interplay of physical and conversational context on speech interactions, like when someone knows to close an ajar window when you tell them “I am feeling chilly.” &lt;em&gt;Pragmatics involves interpreting speech while reasoning about the environment and the intentions and hidden knowledge of other agents.&lt;/em&gt;&lt;/li&gt;&lt;/ul&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;Without getting too technical, there is intuitive evidence that somewhat separate systems of cognition are responsible for each of these linguistic faculties. Look no further than the capability for humans to generate syntactically well-formed sentences that have no semantic meaning, e.g. Chomsky’s famous sentence “Colorless green ideas sleep furiously,” or sentences with well-formed semantics that make no pragmatic sense, e.g. responding merely with “Yes, I can” when asked, “Can you pass the salt?” Crucially, &lt;strong&gt;it is the fusion of the disparate cognitive abilities underpinning them that coalesce into human language understanding.&lt;/strong&gt; For example, there isn’t anything syntactically wrong with the sentence, “The fridge is in the apple,” as a syntactic account of “the fridge” and “the apple” would categorize them as noun phrases that can be used to produce a sentence with the production rule, S → (NP “is in” NP). However, &lt;strong&gt;humans recognize an obvious semantic failure in the sentence that becomes apparent after attempting to reconcile its meaning with our understanding of reality&lt;/strong&gt;: we know that fridges are larger than apples, and could not be fit into them.&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;But what if you have never perceived the real world, yet still were trying to figure out whether the sentence was ill-formed? &lt;strong&gt;One solution could be to embed semantic information at the level of syntax&lt;/strong&gt;, e.g., by inventing new syntactic categories, NP&lt;sub&gt;the fridge&lt;/sub&gt; and NP&lt;sub&gt;the apple &lt;/sub&gt;, and a single new production rule that prevents semantic misuse: S → (NP&lt;sub&gt;the apple&lt;/sub&gt; “is in” NP&lt;sub&gt;the fridge &lt;/sub&gt;). While this strategy would no longer require grounded world knowledge about fridges and apples, e.g., &lt;strong&gt;it would require special grammar rules for every semantically well-formed construction… which is actually possible to learn given a massive corpus of natural language.&lt;/strong&gt; Crucially, this would not be the same thing as grasping semantics, which in my view is fundamentally about understanding the nature of the world.&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;Finding that LLMs have reduced problems of semantics and pragmatics into syntax would have profound implications on how we should view their intelligence. People often treat language proficiency as a proxy for general intelligence by, e.g., strongly associating pragmatic and semantic understanding with the cognitive abilities that undergird them in humans. For example, someone who appears well-read and graceful in navigating social interactions is likely to score high in traits like sustained attention and theory of mind, which lie closer to measures of raw cognitive ability. In general, these proxies are reasonable for assessing a &lt;em&gt;person’s&lt;/em&gt; general intelligence, but not an LLM’s, as the apparent linguistic skills of LLMs could come from entirely separate mechanisms of cognition.&lt;/p&gt;&lt;h2 id="the-bitter-lesson-revisited"&gt;The Bitter Lesson Revisited&lt;/h2&gt;&lt;p&gt;TLDR: Sutton’s Bitter Lesson has sometimes been interpreted as meaning that making &lt;em&gt;any&lt;/em&gt; assumptions about the structure of AI is a mistake. This is both unproductive and a misinterpretation; it is precisely when humans think deeply about the structure of intelligence that major advancements occur. Despite this, scale maximalists have implicitly suggested that multimodal models can be a structure-agnostic framework for AGI. Ironically, today’s multimodal models contradict Sutton’s Bitter Lesson by making implicit assumptions about the structure of individual modalities and how they should be sewn together. &lt;strong&gt;In order to build AGI, we must either think deeply about how to unite existing modalities, or dispense with them altogether in favor of an interactive and embodied cognitive process.&lt;/strong&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="alt" class="kg-image" height="733" src="https://thegradient.pub/content/images/2025/06/ChatGPT-Image-May-21--2025--11_31_12-AM-copy.png" width="1024" /&gt;&lt;/figure&gt;&lt;p&gt;The paradigm that led to the success of LLMs is marked primarily by &lt;em&gt;scale&lt;/em&gt;, not efficiency. We have effectively trained a pile of one trillion ants for one billion years to mimic the form and function of a Formula 1 race car; eventually it gets there, but wow was the process inefficient. This analogy nicely captures a debate between structuralists, who want to build things like "wheels" and "axles" into AI systems, and scale maximalists, who want more ants, years, and F1 races to train on. Despite many decades of structuralist study in linguistics, the unstructured approaches of scale maximalism have yielded far better ant-racecars in recent years. This was most notably articulated by Rich Sutton — a recent recipient of the Turing Award along with Andy Barto for their work in Reinforcement Learning — in his piece “The Bitter Lesson.”&lt;/p&gt;&lt;p&gt;[W]e should build in only the meta-methods that can find and capture this arbitrary complexity… Essential to these methods is that they can find good approximations, but the search for them should be by our methods, not by us. We want AI agents that can discover like we can, not which contain what we have discovered. - Rich Sutton&lt;/p&gt;&lt;p&gt;Sutton’s argument is that methods that leverage computational resources will outpace methods that do not, and that any structure for problem-solving built as an inductive bias into AI will hinder it from learning better solutions. &lt;strong&gt;This is a compelling argument&lt;/strong&gt; &lt;strong&gt;that I believe has been seriously misinterpreted by some as implying that making &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;any&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; assumptions about structure is a false step.&lt;/strong&gt; It is, in fact, human intuition that was responsible for many significant advancements in the development of SOTA neural network architectures. For example, Convolutional Neural Networks made an assumption about translation invariance for pattern recognition in images and kickstarted the modern field of deep learning for computer vision; the attention mechanism of Transformers made an assumption about the long-distance relationships between symbols in a sentence that made ChatGPT possible and had nearly everyone drop their RNNs; and 3D Gaussian Splatting made an assumption about the solidity of physical objects that made it more performant than NeRFs. Potentially none of these methodological assumptions apply to the entire domain of &lt;em&gt;possible&lt;/em&gt; scenes, images, or token streams, but they do for the specific ones that humans have curated and formed structural intuitions about. Let’s not forget that humans have co-evolved with the environments that these datasets are drawn from.&lt;/p&gt;&lt;p&gt;The real question is how we might heed Sutton’s Bitter Lesson in our development of AGI. The scale maximalist approach worked for LLMs and LVMs (large vision models) because we had natural deposits of text and image data, but &lt;strong&gt;an analogous application of scale maximalism to AGI would require forms of embodiment data that we simply don’t have. &lt;/strong&gt;One solution to this data scarcity issue extends the generative modeling paradigm to multimodal modeling — encompassing language, vision, and action — &lt;strong&gt;with the hope that a general intelligence can be built by summing together general models of narrow modalities.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;There are multiple issues with this approach. First, &lt;strong&gt;there are deep connections between modalities that are unnaturally severed in the multimodal setting&lt;/strong&gt;, making the problem of concept synthesis ever more difficult. In practice, uniting modalities often involves pre-training dedicated neural modules for each modality and joining them together into a joint embedding space. In the early days, this was achieved by nudging the embeddings of, e.g. (language, vision, action) tuples to converge to similar latent vectors of meaning, a vast oversimplification of the kinds of relationships that may exist between modalities. One can imagine, e.g., captioning an image at various levels of abstraction, or implementing the same linguistic instruction with different sets of physical actions. Such one-to-many relationships suggest that a contrastive embedding objective is not suitable.&lt;/p&gt;&lt;p&gt;While modern approaches do not make such stringent assumptions about how modalities should be united, they still universally encode percepts from all modalities (e.g. text, images) into the same latent space. Intuitively, it would seem that such latent spaces could serve as common conceptual ground across modalities, analogous to a space of human concepts. However, these latent spaces do not cogently capture all information relevant to a concept, and instead rely on modality-specific decoders to flesh out important details. &lt;strong&gt;The “meaning” of a percept is not &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;in&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; the vector it is encoded as, but in the way relevant decoders process this vector into meaningful outputs. &lt;/strong&gt;As long as various encoders and decoders are subject to modality-specific training objectives, “meaning” will be decentralized and potentially inconsistent across modalities, especially as a result of pre-training. This is not a recipe for the formation of coherent concepts.&lt;/p&gt;&lt;p&gt;Furthermore, it is not clear that today’s modalities are an appropriate partitioning of the observation and action spaces for an embodied agent. It is not obvious that, e.g., images and text should be represented as separate observation streams, nor text production and motion planning as separate action capabilities. &lt;strong&gt;The human capacities for reading, seeing, speaking, and moving are ultimately mediated by overlapping cognitive structures.&lt;/strong&gt; &lt;strong&gt;Making structural assumptions about how modalities ought to be processed is likely to hinder the discovery of more fundamental cognition &lt;/strong&gt;that is responsible for processing data in all modalities. One solution would be to consolidate unnaturally partitioned modalities into a unified data representation. This would encourage networks to learn intelligent processes that generalize across modalities. Intuitively, &lt;strong&gt;a model that can understand the visual world as well as humans can&lt;/strong&gt; — including everything from human writing to traffic signs to visual art — &lt;strong&gt;should not make a serious architectural distinction between images and text. &lt;/strong&gt;Part of the reason why VLMs can’t, e.g., count the number of letters in a word is because they can’t &lt;em&gt;see&lt;/em&gt; what they are writing.&lt;/p&gt;&lt;p&gt;Finally, the &lt;strong&gt;learn-from-scale approach trains models to copy the conceptual structure of humans instead of learning the general capability to form novel concepts on their own.&lt;/strong&gt; Humans have spent hundreds of thousands of years refining concepts and passing them memetically through culture and language. Today’s models are trained only on the end result of this process: the present-day conceptual structures that make it into the corpus. By optimizing for the ultimate products of our intelligence, we have ignored the question of how those products were invented and discovered. Humans have a unique ability to form durable concepts from few examples and ascribe names to them, reason about them analogically, etc. While the in-context capabilities of today’s models can be impressive, they grow increasingly limited as tasks become more complex and stray further from the training data. &lt;strong&gt;The flexibility to form new concepts from experience is a foundational attribute of general intelligence&lt;/strong&gt;, we should think carefully about how it arises.&lt;/p&gt;&lt;p&gt;While structure-agnostic scale maximalism has succeeded in producing LLMs and LVMs that pass Turing tests, a multimodal scale maximalist approach to AGI will not bear similar fruit. &lt;strong&gt;Instead of pre-supposing structure in individual modalities, we should design a setting in which modality-specific processing emerges naturally.&lt;/strong&gt; For example, my recent paper on visual theory of mind saw abstract symbols naturally emerge from communication between image-classifying agents, blurring the lines between text and image processing. Eventually, we should hope to reintegrate as many features of intelligence as possible under the same umbrella. However, it is not clear whether there is genuine commercial viability in such an approach as long as scaling and fine-tuning narrow intelligence models solves commercial use-cases.&lt;/p&gt;&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;&lt;p&gt;The overall promise of scale maximalism is that a Frankenstein AGI can be sewed together using general models of narrow domains. I argue that this is extremely unlikely to yield an AGI that feels complete in its intelligence. If we intend to continue reaping the streamlined efficiency of modality-specific processing, we must be intentional in how modalities are united — ideally drawing from human intuition and classical fields of study, e.g. this work from MIT. Alternatively, we can re-formulate learning as an embodied and interactive process where disparate modalities naturally fuse together. We could do this by, e.g., processing images, text, and video using the same perception system and producing actions for generating text, manipulating objects, and navigating environments using the same action system. What we will lose in efficiency we will gain in flexible cognitive ability.&lt;/p&gt;&lt;p&gt;In a sense, the most challenging mathematical piece of the AGI puzzle has already been solved: the discovery of universal function approximators. What’s left is to inventory the functions we need and determine how they ought to be arranged into a coherent whole. This is a conceptual problem, not a mathematical one.&lt;/p&gt;&lt;hr /&gt;&lt;h2 id="acknowledgements"&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;I would like to thank Lucas Gelfond, Daniel Bashir, George Konidaris, and my father, Joseph Spiegel, for their thoughtful and thorough feedback on this work. Thanks to Alina Pringle for the wonderful illustration made for this piece.&lt;/p&gt;&lt;h2 id="author-bio"&gt;Author Bio&lt;/h2&gt;&lt;p&gt;Benjamin is a PhD candidate in Computer Science at Brown University. He is interested in models of language understanding that ground meaning to elements of structured decision-making. For more info see his personal website.&lt;/p&gt;&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;pre&gt;&lt;code&gt;Benjamin A. Spiegel, "AGI Is Not Multimodal", The Gradient, 2025.
&lt;/code&gt;&lt;/pre&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;pre&gt;&lt;code&gt;@article{spiegel2025agi,
    author = {Benjamin A. Spiegel},
    title = {AGI Is Not Multimodal},
    journal = {The Gradient},
    year = {2025},
    howpublished = {\url{https://thegradient.pub/agi-is-not-multimodal},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;h2 id="references"&gt;References&lt;/h2&gt;&lt;p&gt;Andreas, Jacob. “Language Models, World Models, and Human Model-Building.” &lt;em&gt;Mit.edu&lt;/em&gt;, 2024, lingo.csail.mit.edu/blog/world_models/.&lt;/p&gt;&lt;p&gt;Belkin, Mikhail, et al. "Reconciling modern machine-learning practice and the classical bias–variance trade-off." &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt; 116.32 (2019): 15849-15854.&lt;/p&gt;&lt;p&gt;Bernhard Kerbl, et al. “3D Gaussian Splatting for Real-Time Radiance Field Rendering.” &lt;em&gt;ACM Transactions on Graphics&lt;/em&gt;, vol. 42, no. 4, 26 July 2023, pp. 1–14, https://doi.org/10.1145/3592433.&lt;/p&gt;&lt;p&gt;Chomsky, Noam. 1965. Aspects of the theory of syntax. Cambridge, Massachusetts: MIT Press.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Designing an Intelligence&lt;/em&gt;. Edited by George Konidaris, MIT Press, 2026.&lt;/p&gt;&lt;p&gt;Emily M. Bender and Alexander Koller. 2020. Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data. In &lt;em&gt;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&lt;/em&gt;, pages 5185–5198, Online. Association for Computational Linguistics.&lt;/p&gt;&lt;p&gt;Eye on AI. “The Mastermind behind GPT-4 and the Future of AI | Ilya Sutskever.” &lt;em&gt;YouTube&lt;/em&gt;, 15 Mar. 2023, www.youtube.com/watch?v=SjhIlw3Iffs&amp;amp;list=PLpdlTIkm0-jJ4gJyeLvH1PJCEHp3NAYf4&amp;amp;index=64. Accessed 18 May 2025.&lt;/p&gt;&lt;p&gt;Frank, Michael C. “Bridging the data gap between children and large language models.” &lt;em&gt;Trends in cognitive sciences&lt;/em&gt; vol. 27,11 (2023): 990-992. doi:10.1016/j.tics.2023.08.007&lt;/p&gt;&lt;p&gt;Garrett, Caelan Reed, et al. "Integrated task and motion planning." &lt;em&gt;Annual review of control, robotics, and autonomous systems&lt;/em&gt; 4.1 (2021): 265-293.APA&lt;/p&gt;&lt;p&gt;Goodhart, C.A.E. (1984). Problems of Monetary Management: The UK Experience. In: Monetary Theory and Practice. Palgrave, London. https://doi.org/10.1007/978-1-349-17295-5_4&lt;/p&gt;&lt;p&gt;Hooker, Sara. The hardware lottery. Commun. ACM 64, 12 (December 2021), 58–65. https://doi.org/10.1145/3467017&lt;/p&gt;&lt;p&gt;Huh, Minyoung, et al. "The Platonic Representation Hypothesis." &lt;em&gt;Forty-first International Conference on Machine Learning&lt;/em&gt;. 2024.&lt;/p&gt;&lt;p&gt;Kaplan, Jared, et al. "Scaling laws for neural language models." &lt;em&gt;arXiv preprint arXiv:2001.08361&lt;/em&gt; (2020).&lt;/p&gt;&lt;p&gt;Lake, Brenden M. et al. “Building Machines That Learn and Think like People.” &lt;em&gt;Behavioral and Brain Sciences&lt;/em&gt; 40 (2017): e253. Web.&lt;/p&gt;&lt;p&gt;Li, Kenneth, et al. "Emergent world representations: Exploring a sequence model trained on a synthetic task." &lt;em&gt;ICLR&lt;/em&gt; (2023).&lt;/p&gt;&lt;p&gt;Luiten, Jonathon, Georgios, Kopanas, Bastian, Leibe, Deva, Ramanan. "Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis." &lt;em&gt;3DV&lt;/em&gt;. 2024.&lt;/p&gt;&lt;p&gt;Mao, Jiayuan, Chuang, Gan, Pushmeet, Kohli, Joshua B., Tenenbaum, Jiajun, Wu. "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision." &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. 2019.&lt;/p&gt;&lt;p&gt;Mitchell, Melanie. “LLMs and World Models, Part 1.” &lt;em&gt;Substack.com&lt;/em&gt;, AI: A Guide for Thinking Humans, 13 Feb. 2025, aiguide.substack.com/p/llms-and-world-models-part-1. Accessed 18 May 2025.&lt;/p&gt;&lt;p&gt;Mu, Norman. “Norman Mu | the Myth of Data Inefficiency in Large Language Models.” &lt;em&gt;Normanmu.com&lt;/em&gt;, 14 Feb. 2025, www.normanmu.com/2025/02/14/data-inefficiency-llms.html. Accessed 18 May 2025.&lt;/p&gt;&lt;p&gt;Newell, Allen, and Herbert A. Simon. “Computer Science as Empirical Inquiry: Symbols and Search.” &lt;em&gt;Communications of the ACM&lt;/em&gt;, vol. 19, no. 3, 1 Mar. 1976, pp. 113–126, https://doi.org/10.1145/360018.360022.&lt;/p&gt;&lt;p&gt;Peng, Hao, et al. “When Does In-Context Learning Fall Short and Why? A Study on Specification-Heavy Tasks.” &lt;em&gt;ArXiv.org&lt;/em&gt;, 2023, arxiv.org/abs/2311.08993.&lt;/p&gt;&lt;p&gt;Spiegel, Benjamin, et al. “Visual Theory of Mind Enables the Invention of Early Writing Systems.” &lt;em&gt;CogSci&lt;/em&gt;, 2025, arxiv.org/abs/2502.01568.&lt;/p&gt;&lt;p&gt;Sutton, Richard S. &lt;em&gt;Introduction to Reinforcement Learning&lt;/em&gt;. Cambridge, Mass, Mit Press, 04-98, 1998.&lt;/p&gt;&lt;p&gt;Vafa, Keyon, et al. "Evaluating the world model implicit in a generative model." &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 37 (2024): 26941-26975.&lt;/p&gt;&lt;p&gt;Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia (December 2017). "Attention is All you Need". In I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett (ed.). &lt;em&gt;31st Conference on Neural Information Processing Systems (NIPS)&lt;/em&gt;. Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc. arXiv:1706.03762.&lt;/p&gt;&lt;p&gt;Winograd, Terry. “Thinking Machines: Can There Be? Are We?” &lt;em&gt;The Boundaries of Humanity: Humans, Animals, Machines&lt;/em&gt;, edited by James Sheehan and Morton Sosna, Berkeley: University of California Press, 1991, pp. 198–223.&lt;/p&gt;&lt;p&gt;Wu, Shangda, et al. "Beyond language models: Byte models are digital world simulators." &lt;em&gt;arXiv preprint arXiv:2402.19155&lt;/em&gt; (2024). &lt;/p&gt;</description><content:encoded>&lt;blockquote&gt;"In projecting language back as the model for thought, we lose sight of the tacit embodied understanding that undergirds our intelligence." –Terry Winograd&lt;/blockquote&gt;&lt;p&gt;The recent successes of generative AI models have convinced some that AGI is imminent. While these models appear to capture the essence of human intelligence, they defy even our most basic intuitions about it. They have emerged not because they are thoughtful solutions to the problem of intelligence, but because they &lt;em&gt;scaled&lt;/em&gt; effectively on hardware we already had. Seduced by the fruits of scale, some have come to believe that it provides a clear pathway to AGI. The most emblematic case of this is the multimodal approach, in which massive modular networks are optimized for an array of modalities that, taken together, &lt;em&gt;appear&lt;/em&gt; general. However, I argue that this strategy is sure to fail in the near term; it will not lead to human-level AGI that can, e.g., perform sensorimotor reasoning, motion planning, and social coordination. Instead of trying to glue modalities together into a patchwork AGI, &lt;strong&gt;we should pursue approaches to intelligence that treat embodiment and interaction with the environment as primary&lt;/strong&gt;, and see modality-centered processing as emergent phenomena.&lt;/p&gt;&lt;p&gt;Preface: Disembodied definitions of Artificial General Intelligence — emphasis on &lt;em&gt;general&lt;/em&gt; — exclude crucial problem spaces that we should expect AGI to be able to solve. &lt;strong&gt;A true AGI must be general across all domains.&lt;/strong&gt; Any &lt;em&gt;complete&lt;/em&gt; definition must at least include the ability to solve problems that originate in physical reality, e.g. repairing a car, untying a knot, preparing food, etc. As I will discuss in the next section, &lt;strong&gt;what is needed for these problems is a form of intelligence that is fundamentally situated in something like a physical world model&lt;/strong&gt;. For more discussion on this, look out for &lt;em&gt;Designing an Intelligence&lt;/em&gt;. Edited by George Konidaris, MIT Press, forthcoming.&lt;br /&gt;&lt;/p&gt;&lt;h2 id="why-we-need-the-world-and-how-llms-pretend-to-understand-it"&gt;Why We Need the World, and How LLMs Pretend to Understand It&lt;/h2&gt;&lt;p&gt;TLDR: I first argue that &lt;strong&gt;true AGI needs a physical understanding of the world&lt;/strong&gt;, as many problems cannot be converted into a problem of symbol manipulation. It has been suggested by some that LLMs are learning a model of the world through next token prediction, but it is more likely that LLMs are learning bags of heuristics to predict tokens. This leaves them with a superficial understanding of reality and contributes to false impressions of their intelligence.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;The most shocking result of the predict-next-token objective is that it yields AI models that reflect a deeply human-like understanding of the world, despite having never observed it like we have.&lt;/strong&gt; This result has led to confusion about what it means to &lt;em&gt;understand language&lt;/em&gt; and even to &lt;em&gt;understand the world&lt;/em&gt; — something we have long believed to be a prerequisite for language understanding. &lt;strong&gt;One explanation for the capabilities of LLMs comes from &lt;/strong&gt;&lt;strong&gt;an emerging theory&lt;/strong&gt;&lt;strong&gt; suggesting that they induce models of the world through next-token prediction&lt;/strong&gt;. Proponents of this theory cite the prowess of SOTA LLMs on various benchmarks, the convergence of large models to similar internal representations, and their favorite rendition of the idea that “language mirrors the structure of reality,” a notion that has been espoused at least by Plato, Wittgenstein, Foucault, and Eco. While I’m generally in support of digging up esoteric texts for research inspiration, I’m worried that this metaphor has been taken too literally. Do LLMs really learn implicit models of the world? How could they otherwise be so proficient at language?&lt;/p&gt;&lt;p&gt;One source of evidence in favor of the LLM world modeling hypothesis is the Othello paper, wherein researchers were able to predict the board of an Othello game from the hidden states of a transformer model trained on sequences of &lt;em&gt;legal&lt;/em&gt; &lt;em&gt;moves&lt;/em&gt;. However, there are &lt;em&gt;many&lt;/em&gt; issues with generalizing these results to models of natural language. For one, whereas Othello moves can &lt;em&gt;provably&lt;/em&gt; be used to deduce the full state of an Othello board,&lt;strong&gt; we have no reason to believe that a complete picture of the physical world can be inferred by a linguistic description. &lt;/strong&gt;What sets the game of Othello apart from many tasks in the physical world is that &lt;strong&gt;Othello fundamentally resides in the land of symbols, and is merely implemented using physical tokens to make it easier for humans to play.&lt;/strong&gt; A full game of Othello can be played with just pen and paper, but one can’t, e.g., sweep a floor, do dishes, or drive a car with just pen and paper. To solve such tasks, you need some physical conception of the world beyond what humans can merely &lt;em&gt;say&lt;/em&gt; about it. Whether that conception of the world is encoded in a formal world model or, e.g., a value function is up for debate, but it is clear that &lt;strong&gt;there are many problems in the physical world that &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;cannot&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; be &lt;/strong&gt;&lt;strong&gt;fully represented by a system of symbols&lt;/strong&gt;&lt;strong&gt; and solved with mere symbol manipulation.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Another issue stated in Melanie Mitchell’s recent piece and supported by this paper, is that there is evidence that &lt;strong&gt;generative models can score remarkably well on sequence prediction tasks while failing to learn models of the worlds that created such sequence data&lt;/strong&gt;, e.g. by learning comprehensive sets of idiosyncratic heuristics. E.g., it was pointed out in this blog post that OthelloGPT learned sequence prediction rules that don’t actually hold for all possible Othello games, like “if the token for B4 does not appear before A4 in the input string, then B4 is empty.” While one can argue that it doesn’t matter &lt;em&gt;how&lt;/em&gt; a world model predicts the next state of the world, it should raise suspicion when that prediction reflects a better understanding of the training data than the underlying world that led to such data. This, unfortunately, is the central fault of the predict-next-token objective, which seeks only to retain information relevant to the prediction of the next token. &lt;strong&gt;If it can be done with something easier to learn than a world model, it likely will be.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;To claim without caveat that predicting the &lt;em&gt;effects of earlier symbols on later symbols&lt;/em&gt; requires a model of the world like the ones humans generate from perception would be to abuse the “world model” notion. Unless we disagree on what the world is, it should be clear that a &lt;em&gt;true&lt;/em&gt; world model can be used to predict the next state of the &lt;em&gt;physical&lt;/em&gt; world given a history of states. Similar world models, which predict high fidelity observations of the physical world, are leveraged in many subfields of AI including model-based reinforcement learning, task and motion planning in robotics, causal world modeling, and areas of computer vision to solve problems instantiated in physical reality. LLMs are simply not running physics simulations in their latent next-token calculus when they ask you if your person, place, or thing is bigger than a breadbox. In fact, I conjecture that &lt;strong&gt;the behavior of LLMs is not thanks to a learned world model, but to brute force memorization of incomprehensibly abstract rules governing the behavior of symbols, i.e. a model of &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;syntax&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Quick primer:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Syntax&lt;/strong&gt; is a subfield of linguistics that studies how words of various grammatical categories (e.g. parts of speech) are arranged together into sentences, which can be parsed into syntax trees. &lt;em&gt;Syntax studies the structure of sentences and the atomic parts of speech that compose them.&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;strong&gt;Semantics&lt;/strong&gt; is another subfield concerned with the literal meaning of sentences, e.g., compiling “I am feeling chilly” into the &lt;em&gt;idea&lt;/em&gt; that you are experiencing cold. &lt;em&gt;Semantics boils language down to literal meaning, which is information about the world or human experience.&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Pragmatics&lt;/strong&gt; studies the interplay of physical and conversational context on speech interactions, like when someone knows to close an ajar window when you tell them “I am feeling chilly.” &lt;em&gt;Pragmatics involves interpreting speech while reasoning about the environment and the intentions and hidden knowledge of other agents.&lt;/em&gt;&lt;/li&gt;&lt;/ul&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;Without getting too technical, there is intuitive evidence that somewhat separate systems of cognition are responsible for each of these linguistic faculties. Look no further than the capability for humans to generate syntactically well-formed sentences that have no semantic meaning, e.g. Chomsky’s famous sentence “Colorless green ideas sleep furiously,” or sentences with well-formed semantics that make no pragmatic sense, e.g. responding merely with “Yes, I can” when asked, “Can you pass the salt?” Crucially, &lt;strong&gt;it is the fusion of the disparate cognitive abilities underpinning them that coalesce into human language understanding.&lt;/strong&gt; For example, there isn’t anything syntactically wrong with the sentence, “The fridge is in the apple,” as a syntactic account of “the fridge” and “the apple” would categorize them as noun phrases that can be used to produce a sentence with the production rule, S → (NP “is in” NP). However, &lt;strong&gt;humans recognize an obvious semantic failure in the sentence that becomes apparent after attempting to reconcile its meaning with our understanding of reality&lt;/strong&gt;: we know that fridges are larger than apples, and could not be fit into them.&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;But what if you have never perceived the real world, yet still were trying to figure out whether the sentence was ill-formed? &lt;strong&gt;One solution could be to embed semantic information at the level of syntax&lt;/strong&gt;, e.g., by inventing new syntactic categories, NP&lt;sub&gt;the fridge&lt;/sub&gt; and NP&lt;sub&gt;the apple &lt;/sub&gt;, and a single new production rule that prevents semantic misuse: S → (NP&lt;sub&gt;the apple&lt;/sub&gt; “is in” NP&lt;sub&gt;the fridge &lt;/sub&gt;). While this strategy would no longer require grounded world knowledge about fridges and apples, e.g., &lt;strong&gt;it would require special grammar rules for every semantically well-formed construction… which is actually possible to learn given a massive corpus of natural language.&lt;/strong&gt; Crucially, this would not be the same thing as grasping semantics, which in my view is fundamentally about understanding the nature of the world.&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;Finding that LLMs have reduced problems of semantics and pragmatics into syntax would have profound implications on how we should view their intelligence. People often treat language proficiency as a proxy for general intelligence by, e.g., strongly associating pragmatic and semantic understanding with the cognitive abilities that undergird them in humans. For example, someone who appears well-read and graceful in navigating social interactions is likely to score high in traits like sustained attention and theory of mind, which lie closer to measures of raw cognitive ability. In general, these proxies are reasonable for assessing a &lt;em&gt;person’s&lt;/em&gt; general intelligence, but not an LLM’s, as the apparent linguistic skills of LLMs could come from entirely separate mechanisms of cognition.&lt;/p&gt;&lt;h2 id="the-bitter-lesson-revisited"&gt;The Bitter Lesson Revisited&lt;/h2&gt;&lt;p&gt;TLDR: Sutton’s Bitter Lesson has sometimes been interpreted as meaning that making &lt;em&gt;any&lt;/em&gt; assumptions about the structure of AI is a mistake. This is both unproductive and a misinterpretation; it is precisely when humans think deeply about the structure of intelligence that major advancements occur. Despite this, scale maximalists have implicitly suggested that multimodal models can be a structure-agnostic framework for AGI. Ironically, today’s multimodal models contradict Sutton’s Bitter Lesson by making implicit assumptions about the structure of individual modalities and how they should be sewn together. &lt;strong&gt;In order to build AGI, we must either think deeply about how to unite existing modalities, or dispense with them altogether in favor of an interactive and embodied cognitive process.&lt;/strong&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="alt" class="kg-image" height="733" src="https://thegradient.pub/content/images/2025/06/ChatGPT-Image-May-21--2025--11_31_12-AM-copy.png" width="1024" /&gt;&lt;/figure&gt;&lt;p&gt;The paradigm that led to the success of LLMs is marked primarily by &lt;em&gt;scale&lt;/em&gt;, not efficiency. We have effectively trained a pile of one trillion ants for one billion years to mimic the form and function of a Formula 1 race car; eventually it gets there, but wow was the process inefficient. This analogy nicely captures a debate between structuralists, who want to build things like "wheels" and "axles" into AI systems, and scale maximalists, who want more ants, years, and F1 races to train on. Despite many decades of structuralist study in linguistics, the unstructured approaches of scale maximalism have yielded far better ant-racecars in recent years. This was most notably articulated by Rich Sutton — a recent recipient of the Turing Award along with Andy Barto for their work in Reinforcement Learning — in his piece “The Bitter Lesson.”&lt;/p&gt;&lt;p&gt;[W]e should build in only the meta-methods that can find and capture this arbitrary complexity… Essential to these methods is that they can find good approximations, but the search for them should be by our methods, not by us. We want AI agents that can discover like we can, not which contain what we have discovered. - Rich Sutton&lt;/p&gt;&lt;p&gt;Sutton’s argument is that methods that leverage computational resources will outpace methods that do not, and that any structure for problem-solving built as an inductive bias into AI will hinder it from learning better solutions. &lt;strong&gt;This is a compelling argument&lt;/strong&gt; &lt;strong&gt;that I believe has been seriously misinterpreted by some as implying that making &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;any&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; assumptions about structure is a false step.&lt;/strong&gt; It is, in fact, human intuition that was responsible for many significant advancements in the development of SOTA neural network architectures. For example, Convolutional Neural Networks made an assumption about translation invariance for pattern recognition in images and kickstarted the modern field of deep learning for computer vision; the attention mechanism of Transformers made an assumption about the long-distance relationships between symbols in a sentence that made ChatGPT possible and had nearly everyone drop their RNNs; and 3D Gaussian Splatting made an assumption about the solidity of physical objects that made it more performant than NeRFs. Potentially none of these methodological assumptions apply to the entire domain of &lt;em&gt;possible&lt;/em&gt; scenes, images, or token streams, but they do for the specific ones that humans have curated and formed structural intuitions about. Let’s not forget that humans have co-evolved with the environments that these datasets are drawn from.&lt;/p&gt;&lt;p&gt;The real question is how we might heed Sutton’s Bitter Lesson in our development of AGI. The scale maximalist approach worked for LLMs and LVMs (large vision models) because we had natural deposits of text and image data, but &lt;strong&gt;an analogous application of scale maximalism to AGI would require forms of embodiment data that we simply don’t have. &lt;/strong&gt;One solution to this data scarcity issue extends the generative modeling paradigm to multimodal modeling — encompassing language, vision, and action — &lt;strong&gt;with the hope that a general intelligence can be built by summing together general models of narrow modalities.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;There are multiple issues with this approach. First, &lt;strong&gt;there are deep connections between modalities that are unnaturally severed in the multimodal setting&lt;/strong&gt;, making the problem of concept synthesis ever more difficult. In practice, uniting modalities often involves pre-training dedicated neural modules for each modality and joining them together into a joint embedding space. In the early days, this was achieved by nudging the embeddings of, e.g. (language, vision, action) tuples to converge to similar latent vectors of meaning, a vast oversimplification of the kinds of relationships that may exist between modalities. One can imagine, e.g., captioning an image at various levels of abstraction, or implementing the same linguistic instruction with different sets of physical actions. Such one-to-many relationships suggest that a contrastive embedding objective is not suitable.&lt;/p&gt;&lt;p&gt;While modern approaches do not make such stringent assumptions about how modalities should be united, they still universally encode percepts from all modalities (e.g. text, images) into the same latent space. Intuitively, it would seem that such latent spaces could serve as common conceptual ground across modalities, analogous to a space of human concepts. However, these latent spaces do not cogently capture all information relevant to a concept, and instead rely on modality-specific decoders to flesh out important details. &lt;strong&gt;The “meaning” of a percept is not &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;in&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; the vector it is encoded as, but in the way relevant decoders process this vector into meaningful outputs. &lt;/strong&gt;As long as various encoders and decoders are subject to modality-specific training objectives, “meaning” will be decentralized and potentially inconsistent across modalities, especially as a result of pre-training. This is not a recipe for the formation of coherent concepts.&lt;/p&gt;&lt;p&gt;Furthermore, it is not clear that today’s modalities are an appropriate partitioning of the observation and action spaces for an embodied agent. It is not obvious that, e.g., images and text should be represented as separate observation streams, nor text production and motion planning as separate action capabilities. &lt;strong&gt;The human capacities for reading, seeing, speaking, and moving are ultimately mediated by overlapping cognitive structures.&lt;/strong&gt; &lt;strong&gt;Making structural assumptions about how modalities ought to be processed is likely to hinder the discovery of more fundamental cognition &lt;/strong&gt;that is responsible for processing data in all modalities. One solution would be to consolidate unnaturally partitioned modalities into a unified data representation. This would encourage networks to learn intelligent processes that generalize across modalities. Intuitively, &lt;strong&gt;a model that can understand the visual world as well as humans can&lt;/strong&gt; — including everything from human writing to traffic signs to visual art — &lt;strong&gt;should not make a serious architectural distinction between images and text. &lt;/strong&gt;Part of the reason why VLMs can’t, e.g., count the number of letters in a word is because they can’t &lt;em&gt;see&lt;/em&gt; what they are writing.&lt;/p&gt;&lt;p&gt;Finally, the &lt;strong&gt;learn-from-scale approach trains models to copy the conceptual structure of humans instead of learning the general capability to form novel concepts on their own.&lt;/strong&gt; Humans have spent hundreds of thousands of years refining concepts and passing them memetically through culture and language. Today’s models are trained only on the end result of this process: the present-day conceptual structures that make it into the corpus. By optimizing for the ultimate products of our intelligence, we have ignored the question of how those products were invented and discovered. Humans have a unique ability to form durable concepts from few examples and ascribe names to them, reason about them analogically, etc. While the in-context capabilities of today’s models can be impressive, they grow increasingly limited as tasks become more complex and stray further from the training data. &lt;strong&gt;The flexibility to form new concepts from experience is a foundational attribute of general intelligence&lt;/strong&gt;, we should think carefully about how it arises.&lt;/p&gt;&lt;p&gt;While structure-agnostic scale maximalism has succeeded in producing LLMs and LVMs that pass Turing tests, a multimodal scale maximalist approach to AGI will not bear similar fruit. &lt;strong&gt;Instead of pre-supposing structure in individual modalities, we should design a setting in which modality-specific processing emerges naturally.&lt;/strong&gt; For example, my recent paper on visual theory of mind saw abstract symbols naturally emerge from communication between image-classifying agents, blurring the lines between text and image processing. Eventually, we should hope to reintegrate as many features of intelligence as possible under the same umbrella. However, it is not clear whether there is genuine commercial viability in such an approach as long as scaling and fine-tuning narrow intelligence models solves commercial use-cases.&lt;/p&gt;&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;&lt;p&gt;The overall promise of scale maximalism is that a Frankenstein AGI can be sewed together using general models of narrow domains. I argue that this is extremely unlikely to yield an AGI that feels complete in its intelligence. If we intend to continue reaping the streamlined efficiency of modality-specific processing, we must be intentional in how modalities are united — ideally drawing from human intuition and classical fields of study, e.g. this work from MIT. Alternatively, we can re-formulate learning as an embodied and interactive process where disparate modalities naturally fuse together. We could do this by, e.g., processing images, text, and video using the same perception system and producing actions for generating text, manipulating objects, and navigating environments using the same action system. What we will lose in efficiency we will gain in flexible cognitive ability.&lt;/p&gt;&lt;p&gt;In a sense, the most challenging mathematical piece of the AGI puzzle has already been solved: the discovery of universal function approximators. What’s left is to inventory the functions we need and determine how they ought to be arranged into a coherent whole. This is a conceptual problem, not a mathematical one.&lt;/p&gt;&lt;hr /&gt;&lt;h2 id="acknowledgements"&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;I would like to thank Lucas Gelfond, Daniel Bashir, George Konidaris, and my father, Joseph Spiegel, for their thoughtful and thorough feedback on this work. Thanks to Alina Pringle for the wonderful illustration made for this piece.&lt;/p&gt;&lt;h2 id="author-bio"&gt;Author Bio&lt;/h2&gt;&lt;p&gt;Benjamin is a PhD candidate in Computer Science at Brown University. He is interested in models of language understanding that ground meaning to elements of structured decision-making. For more info see his personal website.&lt;/p&gt;&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;pre&gt;&lt;code&gt;Benjamin A. Spiegel, "AGI Is Not Multimodal", The Gradient, 2025.
&lt;/code&gt;&lt;/pre&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;pre&gt;&lt;code&gt;@article{spiegel2025agi,
    author = {Benjamin A. Spiegel},
    title = {AGI Is Not Multimodal},
    journal = {The Gradient},
    year = {2025},
    howpublished = {\url{https://thegradient.pub/agi-is-not-multimodal},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;h2 id="references"&gt;References&lt;/h2&gt;&lt;p&gt;Andreas, Jacob. “Language Models, World Models, and Human Model-Building.” &lt;em&gt;Mit.edu&lt;/em&gt;, 2024, lingo.csail.mit.edu/blog/world_models/.&lt;/p&gt;&lt;p&gt;Belkin, Mikhail, et al. "Reconciling modern machine-learning practice and the classical bias–variance trade-off." &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt; 116.32 (2019): 15849-15854.&lt;/p&gt;&lt;p&gt;Bernhard Kerbl, et al. “3D Gaussian Splatting for Real-Time Radiance Field Rendering.” &lt;em&gt;ACM Transactions on Graphics&lt;/em&gt;, vol. 42, no. 4, 26 July 2023, pp. 1–14, https://doi.org/10.1145/3592433.&lt;/p&gt;&lt;p&gt;Chomsky, Noam. 1965. Aspects of the theory of syntax. Cambridge, Massachusetts: MIT Press.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Designing an Intelligence&lt;/em&gt;. Edited by George Konidaris, MIT Press, 2026.&lt;/p&gt;&lt;p&gt;Emily M. Bender and Alexander Koller. 2020. Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data. In &lt;em&gt;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&lt;/em&gt;, pages 5185–5198, Online. Association for Computational Linguistics.&lt;/p&gt;&lt;p&gt;Eye on AI. “The Mastermind behind GPT-4 and the Future of AI | Ilya Sutskever.” &lt;em&gt;YouTube&lt;/em&gt;, 15 Mar. 2023, www.youtube.com/watch?v=SjhIlw3Iffs&amp;amp;list=PLpdlTIkm0-jJ4gJyeLvH1PJCEHp3NAYf4&amp;amp;index=64. Accessed 18 May 2025.&lt;/p&gt;&lt;p&gt;Frank, Michael C. “Bridging the data gap between children and large language models.” &lt;em&gt;Trends in cognitive sciences&lt;/em&gt; vol. 27,11 (2023): 990-992. doi:10.1016/j.tics.2023.08.007&lt;/p&gt;&lt;p&gt;Garrett, Caelan Reed, et al. "Integrated task and motion planning." &lt;em&gt;Annual review of control, robotics, and autonomous systems&lt;/em&gt; 4.1 (2021): 265-293.APA&lt;/p&gt;&lt;p&gt;Goodhart, C.A.E. (1984). Problems of Monetary Management: The UK Experience. In: Monetary Theory and Practice. Palgrave, London. https://doi.org/10.1007/978-1-349-17295-5_4&lt;/p&gt;&lt;p&gt;Hooker, Sara. The hardware lottery. Commun. ACM 64, 12 (December 2021), 58–65. https://doi.org/10.1145/3467017&lt;/p&gt;&lt;p&gt;Huh, Minyoung, et al. "The Platonic Representation Hypothesis." &lt;em&gt;Forty-first International Conference on Machine Learning&lt;/em&gt;. 2024.&lt;/p&gt;&lt;p&gt;Kaplan, Jared, et al. "Scaling laws for neural language models." &lt;em&gt;arXiv preprint arXiv:2001.08361&lt;/em&gt; (2020).&lt;/p&gt;&lt;p&gt;Lake, Brenden M. et al. “Building Machines That Learn and Think like People.” &lt;em&gt;Behavioral and Brain Sciences&lt;/em&gt; 40 (2017): e253. Web.&lt;/p&gt;&lt;p&gt;Li, Kenneth, et al. "Emergent world representations: Exploring a sequence model trained on a synthetic task." &lt;em&gt;ICLR&lt;/em&gt; (2023).&lt;/p&gt;&lt;p&gt;Luiten, Jonathon, Georgios, Kopanas, Bastian, Leibe, Deva, Ramanan. "Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis." &lt;em&gt;3DV&lt;/em&gt;. 2024.&lt;/p&gt;&lt;p&gt;Mao, Jiayuan, Chuang, Gan, Pushmeet, Kohli, Joshua B., Tenenbaum, Jiajun, Wu. "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision." &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. 2019.&lt;/p&gt;&lt;p&gt;Mitchell, Melanie. “LLMs and World Models, Part 1.” &lt;em&gt;Substack.com&lt;/em&gt;, AI: A Guide for Thinking Humans, 13 Feb. 2025, aiguide.substack.com/p/llms-and-world-models-part-1. Accessed 18 May 2025.&lt;/p&gt;&lt;p&gt;Mu, Norman. “Norman Mu | the Myth of Data Inefficiency in Large Language Models.” &lt;em&gt;Normanmu.com&lt;/em&gt;, 14 Feb. 2025, www.normanmu.com/2025/02/14/data-inefficiency-llms.html. Accessed 18 May 2025.&lt;/p&gt;&lt;p&gt;Newell, Allen, and Herbert A. Simon. “Computer Science as Empirical Inquiry: Symbols and Search.” &lt;em&gt;Communications of the ACM&lt;/em&gt;, vol. 19, no. 3, 1 Mar. 1976, pp. 113–126, https://doi.org/10.1145/360018.360022.&lt;/p&gt;&lt;p&gt;Peng, Hao, et al. “When Does In-Context Learning Fall Short and Why? A Study on Specification-Heavy Tasks.” &lt;em&gt;ArXiv.org&lt;/em&gt;, 2023, arxiv.org/abs/2311.08993.&lt;/p&gt;&lt;p&gt;Spiegel, Benjamin, et al. “Visual Theory of Mind Enables the Invention of Early Writing Systems.” &lt;em&gt;CogSci&lt;/em&gt;, 2025, arxiv.org/abs/2502.01568.&lt;/p&gt;&lt;p&gt;Sutton, Richard S. &lt;em&gt;Introduction to Reinforcement Learning&lt;/em&gt;. Cambridge, Mass, Mit Press, 04-98, 1998.&lt;/p&gt;&lt;p&gt;Vafa, Keyon, et al. "Evaluating the world model implicit in a generative model." &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 37 (2024): 26941-26975.&lt;/p&gt;&lt;p&gt;Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia (December 2017). "Attention is All you Need". In I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett (ed.). &lt;em&gt;31st Conference on Neural Information Processing Systems (NIPS)&lt;/em&gt;. Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc. arXiv:1706.03762.&lt;/p&gt;&lt;p&gt;Winograd, Terry. “Thinking Machines: Can There Be? Are We?” &lt;em&gt;The Boundaries of Humanity: Humans, Animals, Machines&lt;/em&gt;, edited by James Sheehan and Morton Sosna, Berkeley: University of California Press, 1991, pp. 198–223.&lt;/p&gt;&lt;p&gt;Wu, Shangda, et al. "Beyond language models: Byte models are digital world simulators." &lt;em&gt;arXiv preprint arXiv:2402.19155&lt;/em&gt; (2024). &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://thegradient.pub/agi-is-not-multimodal/</guid><pubDate>Wed, 04 Jun 2025 14:00:29 +0000</pubDate></item><item><title>NVIDIA Blackwell Delivers Breakthrough Performance in Latest MLPerf Training Results (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/blackwell-performance-mlperf-training/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/grace-blackwell-corp-blog-computex-24-gb200-nvl-2-plan-b-image-1280x680-1.png" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA is working with companies worldwide to build out AI factories — speeding the training and deployment of next-generation AI applications that use the latest advancements in training and inference.&lt;/p&gt;
&lt;p&gt;The NVIDIA Blackwell architecture is built to meet the heightened performance requirements of these new applications. In the latest round of MLPerf Training — the 12th since the benchmark’s introduction in 2018 — the NVIDIA AI platform delivered the highest performance at scale on every benchmark and powered every result submitted on the benchmark’s toughest large language model (LLM)-focused test: Llama 3.1 405B pretraining.&lt;/p&gt;
&lt;p&gt;The NVIDIA platform was the only one that submitted results on every MLPerf Training v5.0 benchmark — underscoring its exceptional performance and versatility across a wide array of AI workloads, spanning LLMs, recommendation systems, multimodal LLMs, object detection and graph neural networks.&lt;/p&gt;
&lt;p&gt;The at-scale submissions used two AI supercomputers powered by the NVIDIA Blackwell platform: Tyche, built using NVIDIA GB200 NVL72 rack-scale systems, and Nyx, based on NVIDIA DGX B200 systems. In addition, NVIDIA collaborated with CoreWeave and IBM to submit GB200 NVL72 results using a total of 2,496 Blackwell GPUs and 1,248 NVIDIA Grace CPUs.&lt;/p&gt;
&lt;p&gt;On the new Llama 3.1 405B pretraining benchmark, Blackwell delivered 2.2x greater performance compared with previous-generation architecture at the same scale.&lt;/p&gt;
&lt;p&gt;On the Llama 2 70B LoRA fine-tuning benchmark, eight Blackwell GPUs running as part of an NVIDIA DGX GB200 NVL72 system delivered 2.5x more performance&amp;nbsp;compared with a previous submission using an NVIDIA DGX H100 system with eight NVIDIA H100 GPUs.&lt;/p&gt;
&lt;p&gt;These performance leaps highlight advancements in the Blackwell architecture, including high-density liquid-cooled racks, 13.4TB of coherent memory per rack, fifth-generation NVIDIA NVLink and NVIDIA NVLink Switch interconnect technologies for scale-up and NVIDIA Quantum-2 InfiniBand networking for scale-out. Plus, innovations in the NVIDIA NeMo Framework software stack raise the bar for next-generation multimodal LLM training, critical for bringing agentic AI applications to market.&lt;/p&gt;
&lt;p&gt;These agentic AI-powered applications will one day run in AI factories — the engines of the agentic AI economy. These new applications will produce tokens and valuable intelligence that can be applied to almost every industry and academic domain.&lt;/p&gt;
&lt;p&gt;The NVIDIA data center platform includes GPUs, CPUs, high-speed fabrics and networking, as well as a vast array of software like NVIDIA CUDA-X libraries, the NeMo Framework, NVIDIA TensorRT-LLM and NVIDIA Dynamo. This highly tuned ensemble of hardware and software technologies empowers organizations to train and deploy models more quickly, dramatically accelerating time to value.&lt;/p&gt;
&lt;p&gt;The NVIDIA partner ecosystem participated extensively in this MLPerf round. Beyond the submission with CoreWeave and IBM, other compelling submissions were from ASUS, Cisco, Dell Technologies, Giga Computing, Google Cloud, Hewlett Packard Enterprise, Lambda, Lenovo, Nebius, Oracle Cloud Infrastructure, Quanta Cloud Technology and Supermicro.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about &lt;/i&gt;&lt;i&gt;MLPerf benchmarks&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/grace-blackwell-corp-blog-computex-24-gb200-nvl-2-plan-b-image-1280x680-1.png" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA is working with companies worldwide to build out AI factories — speeding the training and deployment of next-generation AI applications that use the latest advancements in training and inference.&lt;/p&gt;
&lt;p&gt;The NVIDIA Blackwell architecture is built to meet the heightened performance requirements of these new applications. In the latest round of MLPerf Training — the 12th since the benchmark’s introduction in 2018 — the NVIDIA AI platform delivered the highest performance at scale on every benchmark and powered every result submitted on the benchmark’s toughest large language model (LLM)-focused test: Llama 3.1 405B pretraining.&lt;/p&gt;
&lt;p&gt;The NVIDIA platform was the only one that submitted results on every MLPerf Training v5.0 benchmark — underscoring its exceptional performance and versatility across a wide array of AI workloads, spanning LLMs, recommendation systems, multimodal LLMs, object detection and graph neural networks.&lt;/p&gt;
&lt;p&gt;The at-scale submissions used two AI supercomputers powered by the NVIDIA Blackwell platform: Tyche, built using NVIDIA GB200 NVL72 rack-scale systems, and Nyx, based on NVIDIA DGX B200 systems. In addition, NVIDIA collaborated with CoreWeave and IBM to submit GB200 NVL72 results using a total of 2,496 Blackwell GPUs and 1,248 NVIDIA Grace CPUs.&lt;/p&gt;
&lt;p&gt;On the new Llama 3.1 405B pretraining benchmark, Blackwell delivered 2.2x greater performance compared with previous-generation architecture at the same scale.&lt;/p&gt;
&lt;p&gt;On the Llama 2 70B LoRA fine-tuning benchmark, eight Blackwell GPUs running as part of an NVIDIA DGX GB200 NVL72 system delivered 2.5x more performance&amp;nbsp;compared with a previous submission using an NVIDIA DGX H100 system with eight NVIDIA H100 GPUs.&lt;/p&gt;
&lt;p&gt;These performance leaps highlight advancements in the Blackwell architecture, including high-density liquid-cooled racks, 13.4TB of coherent memory per rack, fifth-generation NVIDIA NVLink and NVIDIA NVLink Switch interconnect technologies for scale-up and NVIDIA Quantum-2 InfiniBand networking for scale-out. Plus, innovations in the NVIDIA NeMo Framework software stack raise the bar for next-generation multimodal LLM training, critical for bringing agentic AI applications to market.&lt;/p&gt;
&lt;p&gt;These agentic AI-powered applications will one day run in AI factories — the engines of the agentic AI economy. These new applications will produce tokens and valuable intelligence that can be applied to almost every industry and academic domain.&lt;/p&gt;
&lt;p&gt;The NVIDIA data center platform includes GPUs, CPUs, high-speed fabrics and networking, as well as a vast array of software like NVIDIA CUDA-X libraries, the NeMo Framework, NVIDIA TensorRT-LLM and NVIDIA Dynamo. This highly tuned ensemble of hardware and software technologies empowers organizations to train and deploy models more quickly, dramatically accelerating time to value.&lt;/p&gt;
&lt;p&gt;The NVIDIA partner ecosystem participated extensively in this MLPerf round. Beyond the submission with CoreWeave and IBM, other compelling submissions were from ASUS, Cisco, Dell Technologies, Giga Computing, Google Cloud, Hewlett Packard Enterprise, Lambda, Lenovo, Nebius, Oracle Cloud Infrastructure, Quanta Cloud Technology and Supermicro.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about &lt;/i&gt;&lt;i&gt;MLPerf benchmarks&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/blackwell-performance-mlperf-training/</guid><pubDate>Wed, 04 Jun 2025 15:00:43 +0000</pubDate></item><item><title>How 1X Technologies’ Robots Are Learning to Lend a Helping Hand (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/1x-technologies-humanoids/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/1X_Still_A-scaled.png" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Humans learn the norms, values and behaviors of society from each other — and Bernt Børnich, founder and CEO of 1X Technologies, thinks robots should learn like this, too.&lt;/p&gt;
&lt;p&gt;“For robots to be truly intelligent and show nuances like being careful around your pet, holding the door open for an elderly person and generally behaving like we want robots to behave, they have to live and learn among us,” Børnich told the AI Podcast.&lt;/p&gt;

&lt;p&gt;1X Technologies is committed to building fully autonomous humanoid robots, with a focus on safety, affordability and adaptability.&lt;/p&gt;
&lt;p&gt;Børnich explained how 1X Technologies uses a combination of reinforcement learning, expert demonstrations and real-world data to enable its robots to continuously learn and adapt to new situations.&lt;/p&gt;
&lt;p&gt;NEO, the company’s upcoming robot, can perform household tasks like vacuuming, folding laundry, tidying and retrieving items. It’s built with operational safety at its core, using tendon-driven mechanisms inspired by the human musculoskeletal system to achieve low energy consumption.&lt;/p&gt;
&lt;p&gt;Børnich highlights the potential for robots to enhance human productivity by helping handle mundane tasks, freeing people up to focus more on interpersonal connections and creative activities.&lt;/p&gt;
&lt;p&gt;Learn more about the latest in physical AI and robotics at NVIDIA GTC Paris, which takes place from June 10-12. Register to attend humanoid-related sessions, including:&lt;/p&gt;

&lt;h2&gt;&lt;b&gt;Time Stamps&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;05:18 – 1X Technologies’ approach to robot safety.&lt;/p&gt;
&lt;p&gt;11:36 – How world models enable robots to search backwards from the goal.&lt;/p&gt;
&lt;p&gt;16:51 – How robots can free humans up for more meaningful activities.&lt;/p&gt;
&lt;p&gt;22:29 – NEO answers the door so Børnich can interview a candidate.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;You Might Also Like…&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;How World Foundation Models Will Advance Physical AI With NVIDIA’s Ming-Yu Liu&lt;/p&gt;
&lt;p&gt;AI models that can accurately simulate and predict outcomes in physical, real-world environments will enable the next generation of physical AI systems. Ming-Yu Liu, vice president of research at NVIDIA and an IEEE Fellow, explains the significance of world foundation models — powerful neural networks that can simulate physical environments.&lt;/p&gt;
&lt;p&gt;Roboflow Helps Unlock Computer Vision for Every Kind of AI Builder&lt;/p&gt;
&lt;p&gt;Roboflow’s mission is to make the world programmable through computer vision. By simplifying computer vision development, the company helps bridge the gap between AI and people looking to harness it. Cofounder and CEO Joseph Nelson discusses how Roboflow empowers users in manufacturing, healthcare and automotive to solve complex problems with visual AI.&lt;/p&gt;
&lt;p&gt;Imbue CEO Kanjun Qiu on Transforming AI Agents Into Personal Collaborators&lt;/p&gt;
&lt;p&gt;Kanjun Qiu, CEO of Imbue, explores the emerging era where individuals can create and use their own AI agents. Drawing a parallel to the PC revolution of the late 1970s and ‘80s, Qiu discusses how modern AI systems are evolving to work collaboratively with users, enhancing their capabilities rather than just automating tasks.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/1X_Still_A-scaled.png" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Humans learn the norms, values and behaviors of society from each other — and Bernt Børnich, founder and CEO of 1X Technologies, thinks robots should learn like this, too.&lt;/p&gt;
&lt;p&gt;“For robots to be truly intelligent and show nuances like being careful around your pet, holding the door open for an elderly person and generally behaving like we want robots to behave, they have to live and learn among us,” Børnich told the AI Podcast.&lt;/p&gt;

&lt;p&gt;1X Technologies is committed to building fully autonomous humanoid robots, with a focus on safety, affordability and adaptability.&lt;/p&gt;
&lt;p&gt;Børnich explained how 1X Technologies uses a combination of reinforcement learning, expert demonstrations and real-world data to enable its robots to continuously learn and adapt to new situations.&lt;/p&gt;
&lt;p&gt;NEO, the company’s upcoming robot, can perform household tasks like vacuuming, folding laundry, tidying and retrieving items. It’s built with operational safety at its core, using tendon-driven mechanisms inspired by the human musculoskeletal system to achieve low energy consumption.&lt;/p&gt;
&lt;p&gt;Børnich highlights the potential for robots to enhance human productivity by helping handle mundane tasks, freeing people up to focus more on interpersonal connections and creative activities.&lt;/p&gt;
&lt;p&gt;Learn more about the latest in physical AI and robotics at NVIDIA GTC Paris, which takes place from June 10-12. Register to attend humanoid-related sessions, including:&lt;/p&gt;

&lt;h2&gt;&lt;b&gt;Time Stamps&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;05:18 – 1X Technologies’ approach to robot safety.&lt;/p&gt;
&lt;p&gt;11:36 – How world models enable robots to search backwards from the goal.&lt;/p&gt;
&lt;p&gt;16:51 – How robots can free humans up for more meaningful activities.&lt;/p&gt;
&lt;p&gt;22:29 – NEO answers the door so Børnich can interview a candidate.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;You Might Also Like…&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;How World Foundation Models Will Advance Physical AI With NVIDIA’s Ming-Yu Liu&lt;/p&gt;
&lt;p&gt;AI models that can accurately simulate and predict outcomes in physical, real-world environments will enable the next generation of physical AI systems. Ming-Yu Liu, vice president of research at NVIDIA and an IEEE Fellow, explains the significance of world foundation models — powerful neural networks that can simulate physical environments.&lt;/p&gt;
&lt;p&gt;Roboflow Helps Unlock Computer Vision for Every Kind of AI Builder&lt;/p&gt;
&lt;p&gt;Roboflow’s mission is to make the world programmable through computer vision. By simplifying computer vision development, the company helps bridge the gap between AI and people looking to harness it. Cofounder and CEO Joseph Nelson discusses how Roboflow empowers users in manufacturing, healthcare and automotive to solve complex problems with visual AI.&lt;/p&gt;
&lt;p&gt;Imbue CEO Kanjun Qiu on Transforming AI Agents Into Personal Collaborators&lt;/p&gt;
&lt;p&gt;Kanjun Qiu, CEO of Imbue, explores the emerging era where individuals can create and use their own AI agents. Drawing a parallel to the PC revolution of the late 1970s and ‘80s, Qiu discusses how modern AI systems are evolving to work collaboratively with users, enhancing their capabilities rather than just automating tasks.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/1x-technologies-humanoids/</guid><pubDate>Wed, 04 Jun 2025 16:00:22 +0000</pubDate></item><item><title>OpenAI hits 3M business users and launches workplace tools to take on Microsoft (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/openai-hits-3m-business-users-and-launches-workplace-tools-to-take-on-microsoft/</link><description>[unable to retrieve full-text content]OpenAI reaches 3 million paying business users with 50% growth since February, launching new workplace AI tools including connectors and coding agents to compete with Microsoft.</description><content:encoded>[unable to retrieve full-text content]OpenAI reaches 3 million paying business users with 50% growth since February, launching new workplace AI tools including connectors and coding agents to compete with Microsoft.</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/openai-hits-3m-business-users-and-launches-workplace-tools-to-take-on-microsoft/</guid><pubDate>Wed, 04 Jun 2025 17:00:00 +0000</pubDate></item><item><title>MIT Technology Review Insiders Panel (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/04/1117864/mit-technology-review-insiders-panel/</link><description></description><guid isPermaLink="false">https://www.technologyreview.com/2025/06/04/1117864/mit-technology-review-insiders-panel/</guid><pubDate>Wed, 04 Jun 2025 20:06:06 +0000</pubDate></item><item><title>Stop guessing why your LLMs break: Anthropic’s new tool shows you exactly what goes wrong (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/stop-guessing-why-your-llms-break-anthropics-new-tool-shows-you-exactly-what-goes-wrong/</link><description>[unable to retrieve full-text content]Anthropic's open-source circuit tracing tool can help developers debug, optimize, and control AI for reliable and trustable applications.</description><content:encoded>[unable to retrieve full-text content]Anthropic's open-source circuit tracing tool can help developers debug, optimize, and control AI for reliable and trustable applications.</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/stop-guessing-why-your-llms-break-anthropics-new-tool-shows-you-exactly-what-goes-wrong/</guid><pubDate>Wed, 04 Jun 2025 22:39:09 +0000</pubDate></item><item><title>Crypto billionaire Brian Armstrong is ready to invest in CRISPR baby tech (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/05/1117909/crypto-billionaire-brian-armstrong-is-ready-to-invest-in-crispr-baby-tech/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/BRIAN_ARMSTRONG_02.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Brian Armstrong, the billionaire CEO of the cryptocurrency exchange Coinbase, says he’s ready to fund a US startup focused on gene-editing human embryos. If he goes forward, it would be the first major commercial investment in one of medicine’s most fraught ideas.&lt;/p&gt;  &lt;p&gt;In a post on X June 2, Armstrong announced he was looking for gene-editing scientists and bioinformatics specialists to form a founding team for an “embryo editing” effort targeting an unmet medical need, such as a genetic disease.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“I think the time is right for the defining company in the US to be built in this area,” Armstrong posted.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The announcement from a deep-pocketed backer is a striking shift for a field considered taboo following the 2018 birth of the world’s first genetically edited children in China—a secretive experiment that led to international outrage and prison time for the lead scientist.&lt;/p&gt; 
 &lt;p&gt;According to Dieter Egli, a gene-editing scientist at Columbia University whose team has briefed Armstrong, his plans may be motivated in part by recent improvements in editing technology that have opened up a safer, more precise way to change the DNA of embryos.&lt;/p&gt;  &lt;p&gt;That technique, called base editing, can deftly change a single DNA letter. Earlier methods, on the other hand, actually cut the double helix, damaging it and causing whole genes to disappear. “We know much better now what to do,” says Egli. “It doesn’t mean the work is all done, but it’s a very different game now—entirely different.”&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Shoestring budget&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Embryo editing, which ultimately aims to produce humans with genes tailored by design, is an idea that has been heavily stigmatized and starved of funding. While it's legal to study embryos in the lab, actually producing a gene-edited baby is flatly illegal in most countries.  &lt;/p&gt;  &lt;p&gt;In the US, the modified baby ban operates via a law that forbids the Food and Drug Administration from considering, or even acknowledging, any application it gets to attempt a gene-edited baby. But that rule could be changed, especially if scientists can demonstrate a compelling use of the technique—or perhaps if a billionaire lobbies for it.&lt;/p&gt;  &lt;p&gt;In his post, Armstrong included an image of a seven-year-old Pew Research Center poll showing Americans were strongly favorable to altering a baby’s genes if it could treat disease, although the same poll found most opposed experimentation on embryos.&lt;em&gt;&amp;nbsp;&amp;nbsp;&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;Up until this point, no US company has openly pursued embryo editing, and the federal government doesn’t fund studies on embryos at all. Instead, research on gene editing in embryos has been carried forward in the US by just two academic centers, Egli’s and one at the Oregon Health &amp;amp; Science University.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;Those efforts have operated on a relative shoestring, held together by private grants and university funds. Researchers at those centers said they support the idea of a well-financed company that could advance the technology. “We would honestly welcome that,” says Paula Amato, a fertility doctor at Oregon Health &amp;amp; Science University and the past president of the American Society for Reproductive Medicine.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“More research is needed, and that takes people and money,” she says, adding that she doesn’t mind if it comes from “tech bros.”&lt;/p&gt;  &lt;p&gt;Editing embryos can, in theory, be used to correct genetic errors likely to cause serious childhood conditions. But since in most cases genetic testing of embryos can also be used to avoid those errors, many argue it will be hard to find a true unmet need where the DNA-altering technique is actually necessary.&lt;/p&gt;  &lt;p&gt;Instead, it's easy to conclude that the bigger market for the technology would be to intervene in embryos in ways that could make humans resistant to common conditions, such as heart disease or Alzheimer’s. But that is more controversial because it’s a type of enhancement, and the changes would also be passed through the generations.&lt;/p&gt; 

 &lt;p&gt;Only last week, several biotech trade and academic groups demanded a 10-year moratorium on heritable human genome editing, saying the technology has few real medical uses and “introduces long-term risks with unknown consequences.”&lt;/p&gt;  &lt;p&gt;They said the ability to “program” desired traits or eliminate bad ones risked a new form of “eugenics,” one that would have the effect of “potentially altering the course of evolution.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;No limits&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Armstrong did not reply to an email from &lt;em&gt;MIT Technology Review&lt;/em&gt; seeking comment about his plans. Nor did his company Coinbase, a cryptocurrency trading platform that went public in 2021 and is the source of his fortune, estimated at $10 billion by &lt;em&gt;Forbes&lt;/em&gt;.&lt;/p&gt;  &lt;p&gt;The billionaire is already part of a wave of tech entrepreneurs who’ve made a splash in science and biology by laying down outsize investments, sometimes in far-out ideas. Armstrong previously cofounded NewLimit, which &lt;em&gt;Bloomberg&lt;/em&gt; calls a “life extension venture” and which this year raised a further $130 million to explore methods to reprogram old cells into an embryonic-like state.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt; &lt;p&gt;He started that company with Blake Byers, an investor who has said a significant portion of global GDP should be spent on “immortality” research, including biotech approaches and ways of uploading human minds to computers.&lt;/p&gt;  &lt;p&gt;Then, starting late last year, Armstrong began publicly telegraphing his interest in exploring a new venture, this time connected to assisted reproduction. In December, he announced on X that he and Byers were ready to meet with entrepreneurs working on “artificial wombs,” “embryo editing,” and “next-gen IVF.”&lt;/p&gt;  &lt;p&gt;The post invited people to apply to attend an off-the-record dinner—a kind of forbidden-technologies soiree. Applicants had to fill in a Google form answering a few questions, including “What is something awesome you’ve built?”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_9"&gt; &lt;p&gt;Among those who attended the dinner was a postdoctoral fellow from Egli’s lab, Stepan Jerabek, who has been testing base-editing in embryos. Another attendee, Lucas Harrington, is a gene-editing scientist who trained at the University of California, Berkeley under&amp;nbsp;Jennifer Doudna, a winner of the Nobel Prize in chemistry for development of CRISPR gene editing. Harrington says a venture group he helps run, called SciFounders, is also considering starting an embryo-editing company.&lt;/p&gt; 
 &lt;p&gt;“We share an interest in there being a company to empirically evaluate whether embryo editing can be done safely, and are actively exploring incubating a company to undertake this,” Harrington said in an email. “We believe there need to be legitimate scientists and clinicians working to safely evaluate this technology.”&lt;/p&gt;  &lt;p&gt;Because of how rapidly gene editing is advancing, Harrington has also criticized bans and moratoria on the technology. These can’t stop it from being applied but, he says, can drive it into “the shadows,” where it might be used less safely. According to Harrington, “several biohacker groups have quietly raised small amounts of capital” to pursue the technology.&lt;/p&gt; 
 &lt;p&gt;By contrast, Armstrong’s public declaration on X represents a more transparent approach. “It seems pretty serious now. They want to put something together,” says Egli, who hopes the Coinbase CEO might fund some research at his lab. “I think it’s very good he posted publicly, because you can feel the temperature, see what reaction you get, and you stimulate the public conversation.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Editing error&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The first reports that researchers were testing CRISPR on human embryos in the lab emerged from China in 2015, causing shock waves as it became clear how easy, in theory, it was to change human heredity. Two years later, in 2017, a report from Oregon claimed successful correction of a dangerous DNA mutation present in lab embryos made from patients’ egg and sperm cells.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_11"&gt;&lt;p&gt;But that breakthrough was not what it seemed. More careful testing by Egli and others showed that CRISPR technology actually can cause havoc in a cell, often deleting large chunks of chromosomes. That’s in addition to mosaicism, in which edits occur differently in different cells. What looked at first like precise DNA editing was in fact a dangerous process causing unseen damage.&lt;/p&gt;  &lt;p&gt;While the public debate turned on the ethics of CRISPR babies—especially after three edited children were born in China—researchers were discussing basic scientific problems and how to solve them.&lt;/p&gt;  &lt;p&gt;Since then, both US labs, as well as some in China, have switched to base editing. That method causes fewer unexpected effects and, in theory, could also endow an embryo with a number of advantageous gene variants, not just one change.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Company job&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Some researchers also feel certain that editing an embryo is simpler than trying to treat sick adults. The only approved gene-editing treatment, for sickle-cell disease, costs more than $2 million. By contrast, editing an embryo could be incredibly cheap, and if it’s done early, when an embryo is forming, all the body cells could carry the change.&lt;/p&gt; 
 &lt;p&gt;“You fix the text before you print the book,” says Egli. “It seems like a no-brainer.”&lt;/p&gt;  &lt;p&gt;Still, gene editing isn’t quite ready for prime time in making babies. Getting there requires more work, including careful design of the editing system (which includes a protein and short guide molecule) and systematic ways to check embryos for unwanted DNA changes. That is the type of industrial effort Armstrong’s company, if he funds one, would be suited to carry out.&lt;/p&gt;  &lt;p&gt;“You would have to optimize something to a point where it is perfect, to where it’s a breeze,” says Egli. “This is the kind of work that companies do."&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/BRIAN_ARMSTRONG_02.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Brian Armstrong, the billionaire CEO of the cryptocurrency exchange Coinbase, says he’s ready to fund a US startup focused on gene-editing human embryos. If he goes forward, it would be the first major commercial investment in one of medicine’s most fraught ideas.&lt;/p&gt;  &lt;p&gt;In a post on X June 2, Armstrong announced he was looking for gene-editing scientists and bioinformatics specialists to form a founding team for an “embryo editing” effort targeting an unmet medical need, such as a genetic disease.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“I think the time is right for the defining company in the US to be built in this area,” Armstrong posted.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The announcement from a deep-pocketed backer is a striking shift for a field considered taboo following the 2018 birth of the world’s first genetically edited children in China—a secretive experiment that led to international outrage and prison time for the lead scientist.&lt;/p&gt; 
 &lt;p&gt;According to Dieter Egli, a gene-editing scientist at Columbia University whose team has briefed Armstrong, his plans may be motivated in part by recent improvements in editing technology that have opened up a safer, more precise way to change the DNA of embryos.&lt;/p&gt;  &lt;p&gt;That technique, called base editing, can deftly change a single DNA letter. Earlier methods, on the other hand, actually cut the double helix, damaging it and causing whole genes to disappear. “We know much better now what to do,” says Egli. “It doesn’t mean the work is all done, but it’s a very different game now—entirely different.”&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Shoestring budget&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Embryo editing, which ultimately aims to produce humans with genes tailored by design, is an idea that has been heavily stigmatized and starved of funding. While it's legal to study embryos in the lab, actually producing a gene-edited baby is flatly illegal in most countries.  &lt;/p&gt;  &lt;p&gt;In the US, the modified baby ban operates via a law that forbids the Food and Drug Administration from considering, or even acknowledging, any application it gets to attempt a gene-edited baby. But that rule could be changed, especially if scientists can demonstrate a compelling use of the technique—or perhaps if a billionaire lobbies for it.&lt;/p&gt;  &lt;p&gt;In his post, Armstrong included an image of a seven-year-old Pew Research Center poll showing Americans were strongly favorable to altering a baby’s genes if it could treat disease, although the same poll found most opposed experimentation on embryos.&lt;em&gt;&amp;nbsp;&amp;nbsp;&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;Up until this point, no US company has openly pursued embryo editing, and the federal government doesn’t fund studies on embryos at all. Instead, research on gene editing in embryos has been carried forward in the US by just two academic centers, Egli’s and one at the Oregon Health &amp;amp; Science University.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;Those efforts have operated on a relative shoestring, held together by private grants and university funds. Researchers at those centers said they support the idea of a well-financed company that could advance the technology. “We would honestly welcome that,” says Paula Amato, a fertility doctor at Oregon Health &amp;amp; Science University and the past president of the American Society for Reproductive Medicine.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“More research is needed, and that takes people and money,” she says, adding that she doesn’t mind if it comes from “tech bros.”&lt;/p&gt;  &lt;p&gt;Editing embryos can, in theory, be used to correct genetic errors likely to cause serious childhood conditions. But since in most cases genetic testing of embryos can also be used to avoid those errors, many argue it will be hard to find a true unmet need where the DNA-altering technique is actually necessary.&lt;/p&gt;  &lt;p&gt;Instead, it's easy to conclude that the bigger market for the technology would be to intervene in embryos in ways that could make humans resistant to common conditions, such as heart disease or Alzheimer’s. But that is more controversial because it’s a type of enhancement, and the changes would also be passed through the generations.&lt;/p&gt; 

 &lt;p&gt;Only last week, several biotech trade and academic groups demanded a 10-year moratorium on heritable human genome editing, saying the technology has few real medical uses and “introduces long-term risks with unknown consequences.”&lt;/p&gt;  &lt;p&gt;They said the ability to “program” desired traits or eliminate bad ones risked a new form of “eugenics,” one that would have the effect of “potentially altering the course of evolution.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;No limits&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Armstrong did not reply to an email from &lt;em&gt;MIT Technology Review&lt;/em&gt; seeking comment about his plans. Nor did his company Coinbase, a cryptocurrency trading platform that went public in 2021 and is the source of his fortune, estimated at $10 billion by &lt;em&gt;Forbes&lt;/em&gt;.&lt;/p&gt;  &lt;p&gt;The billionaire is already part of a wave of tech entrepreneurs who’ve made a splash in science and biology by laying down outsize investments, sometimes in far-out ideas. Armstrong previously cofounded NewLimit, which &lt;em&gt;Bloomberg&lt;/em&gt; calls a “life extension venture” and which this year raised a further $130 million to explore methods to reprogram old cells into an embryonic-like state.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt; &lt;p&gt;He started that company with Blake Byers, an investor who has said a significant portion of global GDP should be spent on “immortality” research, including biotech approaches and ways of uploading human minds to computers.&lt;/p&gt;  &lt;p&gt;Then, starting late last year, Armstrong began publicly telegraphing his interest in exploring a new venture, this time connected to assisted reproduction. In December, he announced on X that he and Byers were ready to meet with entrepreneurs working on “artificial wombs,” “embryo editing,” and “next-gen IVF.”&lt;/p&gt;  &lt;p&gt;The post invited people to apply to attend an off-the-record dinner—a kind of forbidden-technologies soiree. Applicants had to fill in a Google form answering a few questions, including “What is something awesome you’ve built?”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_9"&gt; &lt;p&gt;Among those who attended the dinner was a postdoctoral fellow from Egli’s lab, Stepan Jerabek, who has been testing base-editing in embryos. Another attendee, Lucas Harrington, is a gene-editing scientist who trained at the University of California, Berkeley under&amp;nbsp;Jennifer Doudna, a winner of the Nobel Prize in chemistry for development of CRISPR gene editing. Harrington says a venture group he helps run, called SciFounders, is also considering starting an embryo-editing company.&lt;/p&gt; 
 &lt;p&gt;“We share an interest in there being a company to empirically evaluate whether embryo editing can be done safely, and are actively exploring incubating a company to undertake this,” Harrington said in an email. “We believe there need to be legitimate scientists and clinicians working to safely evaluate this technology.”&lt;/p&gt;  &lt;p&gt;Because of how rapidly gene editing is advancing, Harrington has also criticized bans and moratoria on the technology. These can’t stop it from being applied but, he says, can drive it into “the shadows,” where it might be used less safely. According to Harrington, “several biohacker groups have quietly raised small amounts of capital” to pursue the technology.&lt;/p&gt; 
 &lt;p&gt;By contrast, Armstrong’s public declaration on X represents a more transparent approach. “It seems pretty serious now. They want to put something together,” says Egli, who hopes the Coinbase CEO might fund some research at his lab. “I think it’s very good he posted publicly, because you can feel the temperature, see what reaction you get, and you stimulate the public conversation.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Editing error&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The first reports that researchers were testing CRISPR on human embryos in the lab emerged from China in 2015, causing shock waves as it became clear how easy, in theory, it was to change human heredity. Two years later, in 2017, a report from Oregon claimed successful correction of a dangerous DNA mutation present in lab embryos made from patients’ egg and sperm cells.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_11"&gt;&lt;p&gt;But that breakthrough was not what it seemed. More careful testing by Egli and others showed that CRISPR technology actually can cause havoc in a cell, often deleting large chunks of chromosomes. That’s in addition to mosaicism, in which edits occur differently in different cells. What looked at first like precise DNA editing was in fact a dangerous process causing unseen damage.&lt;/p&gt;  &lt;p&gt;While the public debate turned on the ethics of CRISPR babies—especially after three edited children were born in China—researchers were discussing basic scientific problems and how to solve them.&lt;/p&gt;  &lt;p&gt;Since then, both US labs, as well as some in China, have switched to base editing. That method causes fewer unexpected effects and, in theory, could also endow an embryo with a number of advantageous gene variants, not just one change.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Company job&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Some researchers also feel certain that editing an embryo is simpler than trying to treat sick adults. The only approved gene-editing treatment, for sickle-cell disease, costs more than $2 million. By contrast, editing an embryo could be incredibly cheap, and if it’s done early, when an embryo is forming, all the body cells could carry the change.&lt;/p&gt; 
 &lt;p&gt;“You fix the text before you print the book,” says Egli. “It seems like a no-brainer.”&lt;/p&gt;  &lt;p&gt;Still, gene editing isn’t quite ready for prime time in making babies. Getting there requires more work, including careful design of the editing system (which includes a protein and short guide molecule) and systematic ways to check embryos for unwanted DNA changes. That is the type of industrial effort Armstrong’s company, if he funds one, would be suited to carry out.&lt;/p&gt;  &lt;p&gt;“You would have to optimize something to a point where it is perfect, to where it’s a breeze,” says Egli. “This is the kind of work that companies do."&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/05/1117909/crypto-billionaire-brian-armstrong-is-ready-to-invest-in-crispr-baby-tech/</guid><pubDate>Thu, 05 Jun 2025 09:36:26 +0000</pubDate></item><item><title>Over $1 billion in federal funding got slashed for this polluting industry (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/05/1117855/cement-funding-slash/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/GettyImages-960669974.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;The clean cement industry might be facing the end of the road, before it ever really got rolling.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;On Friday, the US Department of Energy announced that it was canceling $3.7 billion in funding for 24 projects related to energy and industry. That included nearly $1.3 billion for cement-related projects.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Cement is a massive climate problem, accounting for roughly 7% of global greenhouse-gas emissions. What’s more, it’s a difficult industry to clean up, with huge traditional players and expensive equipment and infrastructure to replace. This funding was supposed to help address those difficulties, by supporting projects on the cusp of commercialization. Now companies will need to fill in the gap left by these cancellations, and it’s a big one.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;First up on the list for cuts is Sublime Systems, a company you’re probably familiar with if you’ve been reading this newsletter for a while. I did a deep dive last year, and the company was on our list of Climate Tech Companies to Watch in both 2023 and 2024.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;The startup’s approach is to make cement using electricity. The conventional process requires high temperatures typically achieved by burning fossil fuels, so avoiding that could prevent a lot of emissions.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In 2024, Sublime received an $87 million grant from the DOE to construct a commercial demonstration plant in Holyoke, Massachusetts. That grant would have covered roughly half the construction costs for the facility, which is scheduled to open in 2026 and produce up to 30,000 metric tons of cement each year.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;“We were certainly surprised and disappointed about the development,” says Joe Hicken, Sublime’s senior VP of business development and policy. Customers are excited by the company’s technology, Hicken adds, pointing to Sublime’s recently announced deal with Microsoft, which plans to buy up to 622,500 metric tons of cement from the company.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Another big name, Brimstone, also saw its funding affected. That award totaled $189 million for a commercial demonstration plant, which was expected to produce over 100,000 metric tons of cement annually.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In a statement, a Brimstone representative said the company believes the cancellation was a “misunderstanding.” The statement pointed out that the planned facility would make not only cement but also alumina, supporting US-based aluminum production. (Aluminum is classified as a critical mineral by the US Geological Survey, meaning it’s considered crucial to the US economy and national security.)&amp;nbsp;&lt;/p&gt;  &lt;p&gt;An award to Heidelberg Materials for up to $500 million for a planned Indiana facility was also axed. The idea there was to integrate carbon capture and storage to clean up emissions from the plant, which would have made it the first cement plant in the US to demonstrate that technology. In a written statement, a representative said the decision can be appealed, and the company is considering that option.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;And National Cement’s funding for the Lebec Net-Zero Project, another $500 million award, was canceled. That facility planned to make carbon-neutral cement through a combination of strategies: reducing the polluting ingredients needed, using alternative fuels like biomass, and capturing the plant’s remaining emissions.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“We want to emphasize that this project will expand domestic manufacturing capacity for a critical industrial sector, while also integrating new technologies to keep American cement competitive,” said a company spokesperson in a written statement.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;There’s a sentiment here that’s echoed in all the responses I received: While these awards were designed to cut emissions, these companies argue that they can fit into the new administration’s priorities. They’re emphasizing phrases like “critical minerals,” “American jobs,” and “domestic supply chains.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“We’ve heard loud and clear from the Trump administration the desire to displace foreign imports of things that can be made here in America,” Sublime’s Hicken says. “At the end of the day, what we deliver is what the policymakers in DC are looking for.”&amp;nbsp;&lt;/p&gt; 

&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;But this administration is showing that it’s not supporting climate efforts—often even those that also advance its stated goals of energy abundance and American competitiveness.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;On Monday, my colleague James Temple published a new story about cuts to climate research, including tens of millions of dollars in grants from the National Science Foundation. Researchers at Harvard were particularly hard hit.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Even as there’s interest in advancing the position of the US on the world’s stage, these cuts are making it hard for researchers and companies alike to do the crucial work of understanding our climate and developing and deploying new technologies.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark, &lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday,&lt;/em&gt;&lt;em&gt; sign up here&lt;/em&gt;.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/GettyImages-960669974.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;The clean cement industry might be facing the end of the road, before it ever really got rolling.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;On Friday, the US Department of Energy announced that it was canceling $3.7 billion in funding for 24 projects related to energy and industry. That included nearly $1.3 billion for cement-related projects.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Cement is a massive climate problem, accounting for roughly 7% of global greenhouse-gas emissions. What’s more, it’s a difficult industry to clean up, with huge traditional players and expensive equipment and infrastructure to replace. This funding was supposed to help address those difficulties, by supporting projects on the cusp of commercialization. Now companies will need to fill in the gap left by these cancellations, and it’s a big one.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;First up on the list for cuts is Sublime Systems, a company you’re probably familiar with if you’ve been reading this newsletter for a while. I did a deep dive last year, and the company was on our list of Climate Tech Companies to Watch in both 2023 and 2024.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;The startup’s approach is to make cement using electricity. The conventional process requires high temperatures typically achieved by burning fossil fuels, so avoiding that could prevent a lot of emissions.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In 2024, Sublime received an $87 million grant from the DOE to construct a commercial demonstration plant in Holyoke, Massachusetts. That grant would have covered roughly half the construction costs for the facility, which is scheduled to open in 2026 and produce up to 30,000 metric tons of cement each year.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;“We were certainly surprised and disappointed about the development,” says Joe Hicken, Sublime’s senior VP of business development and policy. Customers are excited by the company’s technology, Hicken adds, pointing to Sublime’s recently announced deal with Microsoft, which plans to buy up to 622,500 metric tons of cement from the company.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Another big name, Brimstone, also saw its funding affected. That award totaled $189 million for a commercial demonstration plant, which was expected to produce over 100,000 metric tons of cement annually.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In a statement, a Brimstone representative said the company believes the cancellation was a “misunderstanding.” The statement pointed out that the planned facility would make not only cement but also alumina, supporting US-based aluminum production. (Aluminum is classified as a critical mineral by the US Geological Survey, meaning it’s considered crucial to the US economy and national security.)&amp;nbsp;&lt;/p&gt;  &lt;p&gt;An award to Heidelberg Materials for up to $500 million for a planned Indiana facility was also axed. The idea there was to integrate carbon capture and storage to clean up emissions from the plant, which would have made it the first cement plant in the US to demonstrate that technology. In a written statement, a representative said the decision can be appealed, and the company is considering that option.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;And National Cement’s funding for the Lebec Net-Zero Project, another $500 million award, was canceled. That facility planned to make carbon-neutral cement through a combination of strategies: reducing the polluting ingredients needed, using alternative fuels like biomass, and capturing the plant’s remaining emissions.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“We want to emphasize that this project will expand domestic manufacturing capacity for a critical industrial sector, while also integrating new technologies to keep American cement competitive,” said a company spokesperson in a written statement.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;There’s a sentiment here that’s echoed in all the responses I received: While these awards were designed to cut emissions, these companies argue that they can fit into the new administration’s priorities. They’re emphasizing phrases like “critical minerals,” “American jobs,” and “domestic supply chains.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“We’ve heard loud and clear from the Trump administration the desire to displace foreign imports of things that can be made here in America,” Sublime’s Hicken says. “At the end of the day, what we deliver is what the policymakers in DC are looking for.”&amp;nbsp;&lt;/p&gt; 

&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;But this administration is showing that it’s not supporting climate efforts—often even those that also advance its stated goals of energy abundance and American competitiveness.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;On Monday, my colleague James Temple published a new story about cuts to climate research, including tens of millions of dollars in grants from the National Science Foundation. Researchers at Harvard were particularly hard hit.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Even as there’s interest in advancing the position of the US on the world’s stage, these cuts are making it hard for researchers and companies alike to do the crucial work of understanding our climate and developing and deploying new technologies.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark, &lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday,&lt;/em&gt;&lt;em&gt; sign up here&lt;/em&gt;.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/05/1117855/cement-funding-slash/</guid><pubDate>Thu, 05 Jun 2025 10:00:00 +0000</pubDate></item><item><title>The Download: funding a CRISPR embryo startup, and bad news for clean cement (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/05/1117928/the-download-funding-a-crispr-embryo-startup-and-bad-news-for-clean-cement/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Crypto billionaire Brian Armstrong is ready to invest in CRISPR baby tech&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Brian Armstrong, the billionaire CEO of the cryptocurrency exchange Coinbase, says he’s ready to fund a US startup focused on gene-editing human embryos. If he goes forward, it would be the first major commercial investment in one of medicine’s most fraught ideas.&lt;/p&gt;  &lt;p&gt;In a post on X June 2, Armstrong announced he was looking for gene-editing scientists and bioinformatics specialists to form a founding team for an “embryo editing” effort targeting an unmet medical need, such as a genetic disease.&lt;/p&gt; 
 &lt;p&gt;The announcement from a deep-pocketed backer is a striking shift for a field considered taboo following the 2018 birth of the world’s first genetically edited children in China—a secretive experiment that led to international outrage and prison time for the lead scientist. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Antonio Regalado&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Over $1 billion in federal funding got slashed for this polluting industry&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;The clean cement industry might be facing the end of the road, before it ever really got rolling.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Last week, the US Department of Energy announced that it was canceling $3.7 billion in funding for 24 projects related to energy and industry. That included nearly $1.3 billion for cement-related projects.&lt;/p&gt;&lt;p&gt;Cement is a massive climate problem, accounting for roughly 7% of global greenhouse-gas emissions. What’s more, it’s a difficult industry to clean up, with huge traditional players and expensive equipment and infrastructure to replace. This funding was supposed to help address those difficulties, by supporting projects on the cusp of commercialization. Now companies will need to fill in the gap left by these cancellations, and it’s a big one. Read the full story.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;em&gt;—Casey Crownhart&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This article is from The Spark, MIT Technology Review’s weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;MIT Technology Review Narrated: How DeepSeek became a fortune teller for China’s youth&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;AI-powered BaZi analysis has become the new oracle for a disillusioned generation seeking answers.&lt;/p&gt;  &lt;p&gt;This is our latest story to be turned into a MIT Technology Review Narrated podcast, which&amp;nbsp;&lt;br /&gt;we’re publishing each week on &lt;strong&gt;Spotify&lt;/strong&gt; and &lt;strong&gt;Apple Podcasts&lt;/strong&gt;. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it’s released.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Reddit is suing Anthropic&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Reddit claims the AI company kept accessing its site after claiming it had stopped. (WSJ $)&lt;br /&gt;+ &lt;em&gt;Reddit says AI companies should not scrape the web without limitations. &lt;/em&gt;(NYT $)&lt;br /&gt;+ &lt;em&gt;It claims that other AI giants have played by its rules. &lt;/em&gt;(NBC News)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 Inside the rise and rise of deepfake scams&lt;/strong&gt;&lt;br /&gt;The best way to protect yourself is to back up and think who (or what) you’re trusting. (Wired $)&lt;br /&gt;+ &lt;em&gt;An AI startup made a hyperrealistic deepfake of me that’s so good it’s scary. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 A lawsuit accuses DOGE of exploiting “error-riden” data to fire workers&lt;br /&gt;&lt;/strong&gt;It claims the department knew its records were inaccurate, but used them to fire 10,000 employees anyway.(Ars Technica)&lt;br /&gt;+ &lt;em&gt;Unlike Elon Musk, Russ Vought knows the federal government inside out. &lt;/em&gt;(NY Mag $)&lt;br /&gt;+ &lt;em&gt;The first wave of DOGE staffers are becoming full-time government workers. &lt;/em&gt;(Wired $)&lt;br /&gt;+ &lt;em&gt;DOGE’s tech takeover threatens the safety and stability of our critical data. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 Can we make AI behave how we want it to?&lt;/strong&gt;&lt;br /&gt;Looking all the way back to Asimov’s Laws can offer us some clues. (New Yorker $)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;5 Abuse is rife in Taiwan’s semiconductor factories&lt;br /&gt;&lt;/strong&gt;Workers were threatened with deportation and regular 16-hour shifts. (Rest of World)&lt;br /&gt;+ &lt;em&gt;The Trump administration is renegotiating chip grants, apparently. &lt;/em&gt;(Reuters)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;6 Amazon wants to use humanoid robots to deliver packages&lt;br /&gt;It’s planning to test its bipedal machines’ ability to tackle an obstacle course.(The Information $)&lt;br /&gt;+ &lt;em&gt;Why the humanoid workforce is running late. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 We don’t know how to archive the digital age properly&lt;br /&gt;&lt;/strong&gt;Historians worry that they may lose access to intimate materials. (The Atlantic $)&lt;br /&gt;+ &lt;em&gt;The race to save our online lives from a digital dark age. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;8 Here’s how major AI helpers tackled a rigorous reading test&lt;br /&gt;&lt;/strong&gt;Bearing in mind, they all still hallucinated. (WP $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 Christians really love AI slop&lt;/strong&gt;&lt;br /&gt;A major Christian media company is using new tools to spread the word. (404 Media)&lt;br /&gt;+ &lt;em&gt;AI-generated garbage will make ads creepier and worse. &lt;/em&gt;(Bloomberg $)&lt;br /&gt;+ &lt;em&gt;It’s also warping media metrics beyond recognition. &lt;/em&gt;(Digiday)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;10 What we can learn from potty-mouthed robots 🤬&lt;/strong&gt;&lt;br /&gt;A lot of people swear. Why shouldn’t robots, too? (IEEE Spectrum)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“Anthropic bills itself as the white knight of the AI industry. It is anything but.”&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;—Reddit takes aim at Anthropic in a legal filing against the AI company, the Verge reports.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" src="https://wp.technologyreview.com/wp-content/uploads/2024/08/240814_longevity_essay.jpg?fit=1616,908" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Maybe you will be able to live past 122&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;How long can humans live? This is a good time to ask the question. The longevity scene is having a moment, and research suggests that we might be able to push human life spans further, potentially even reversing some signs of aging.&lt;/p&gt;&lt;p&gt;Researchers can’t even agree on what the exact mechanisms of aging are and which they should be targeting. Debates continue to rage over how long it’s possible for humans to live—and whether there is a limit at all.&lt;/p&gt;&lt;p&gt;But it looks likely that something will be developed in the coming decades that will help us live longer, in better health. Read the full story.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;&lt;em&gt;—Jessica Hamzelou&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ There’s something so uplifting about this user-generated collection of videos of parks.&amp;nbsp;&lt;br /&gt;+ I could get on board with living in a cabin in the woods if it was this one.&amp;nbsp;&lt;br /&gt;+ You should probably let go of that grudge you’re holding onto.&amp;nbsp;&lt;br /&gt;+ Looking for some seasonal recipe inspo? Look no further.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Crypto billionaire Brian Armstrong is ready to invest in CRISPR baby tech&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Brian Armstrong, the billionaire CEO of the cryptocurrency exchange Coinbase, says he’s ready to fund a US startup focused on gene-editing human embryos. If he goes forward, it would be the first major commercial investment in one of medicine’s most fraught ideas.&lt;/p&gt;  &lt;p&gt;In a post on X June 2, Armstrong announced he was looking for gene-editing scientists and bioinformatics specialists to form a founding team for an “embryo editing” effort targeting an unmet medical need, such as a genetic disease.&lt;/p&gt; 
 &lt;p&gt;The announcement from a deep-pocketed backer is a striking shift for a field considered taboo following the 2018 birth of the world’s first genetically edited children in China—a secretive experiment that led to international outrage and prison time for the lead scientist. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Antonio Regalado&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Over $1 billion in federal funding got slashed for this polluting industry&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;The clean cement industry might be facing the end of the road, before it ever really got rolling.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Last week, the US Department of Energy announced that it was canceling $3.7 billion in funding for 24 projects related to energy and industry. That included nearly $1.3 billion for cement-related projects.&lt;/p&gt;&lt;p&gt;Cement is a massive climate problem, accounting for roughly 7% of global greenhouse-gas emissions. What’s more, it’s a difficult industry to clean up, with huge traditional players and expensive equipment and infrastructure to replace. This funding was supposed to help address those difficulties, by supporting projects on the cusp of commercialization. Now companies will need to fill in the gap left by these cancellations, and it’s a big one. Read the full story.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;em&gt;—Casey Crownhart&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This article is from The Spark, MIT Technology Review’s weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;MIT Technology Review Narrated: How DeepSeek became a fortune teller for China’s youth&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;AI-powered BaZi analysis has become the new oracle for a disillusioned generation seeking answers.&lt;/p&gt;  &lt;p&gt;This is our latest story to be turned into a MIT Technology Review Narrated podcast, which&amp;nbsp;&lt;br /&gt;we’re publishing each week on &lt;strong&gt;Spotify&lt;/strong&gt; and &lt;strong&gt;Apple Podcasts&lt;/strong&gt;. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it’s released.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Reddit is suing Anthropic&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Reddit claims the AI company kept accessing its site after claiming it had stopped. (WSJ $)&lt;br /&gt;+ &lt;em&gt;Reddit says AI companies should not scrape the web without limitations. &lt;/em&gt;(NYT $)&lt;br /&gt;+ &lt;em&gt;It claims that other AI giants have played by its rules. &lt;/em&gt;(NBC News)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 Inside the rise and rise of deepfake scams&lt;/strong&gt;&lt;br /&gt;The best way to protect yourself is to back up and think who (or what) you’re trusting. (Wired $)&lt;br /&gt;+ &lt;em&gt;An AI startup made a hyperrealistic deepfake of me that’s so good it’s scary. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 A lawsuit accuses DOGE of exploiting “error-riden” data to fire workers&lt;br /&gt;&lt;/strong&gt;It claims the department knew its records were inaccurate, but used them to fire 10,000 employees anyway.(Ars Technica)&lt;br /&gt;+ &lt;em&gt;Unlike Elon Musk, Russ Vought knows the federal government inside out. &lt;/em&gt;(NY Mag $)&lt;br /&gt;+ &lt;em&gt;The first wave of DOGE staffers are becoming full-time government workers. &lt;/em&gt;(Wired $)&lt;br /&gt;+ &lt;em&gt;DOGE’s tech takeover threatens the safety and stability of our critical data. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 Can we make AI behave how we want it to?&lt;/strong&gt;&lt;br /&gt;Looking all the way back to Asimov’s Laws can offer us some clues. (New Yorker $)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;5 Abuse is rife in Taiwan’s semiconductor factories&lt;br /&gt;&lt;/strong&gt;Workers were threatened with deportation and regular 16-hour shifts. (Rest of World)&lt;br /&gt;+ &lt;em&gt;The Trump administration is renegotiating chip grants, apparently. &lt;/em&gt;(Reuters)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;6 Amazon wants to use humanoid robots to deliver packages&lt;br /&gt;It’s planning to test its bipedal machines’ ability to tackle an obstacle course.(The Information $)&lt;br /&gt;+ &lt;em&gt;Why the humanoid workforce is running late. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 We don’t know how to archive the digital age properly&lt;br /&gt;&lt;/strong&gt;Historians worry that they may lose access to intimate materials. (The Atlantic $)&lt;br /&gt;+ &lt;em&gt;The race to save our online lives from a digital dark age. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;8 Here’s how major AI helpers tackled a rigorous reading test&lt;br /&gt;&lt;/strong&gt;Bearing in mind, they all still hallucinated. (WP $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 Christians really love AI slop&lt;/strong&gt;&lt;br /&gt;A major Christian media company is using new tools to spread the word. (404 Media)&lt;br /&gt;+ &lt;em&gt;AI-generated garbage will make ads creepier and worse. &lt;/em&gt;(Bloomberg $)&lt;br /&gt;+ &lt;em&gt;It’s also warping media metrics beyond recognition. &lt;/em&gt;(Digiday)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;10 What we can learn from potty-mouthed robots 🤬&lt;/strong&gt;&lt;br /&gt;A lot of people swear. Why shouldn’t robots, too? (IEEE Spectrum)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“Anthropic bills itself as the white knight of the AI industry. It is anything but.”&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;—Reddit takes aim at Anthropic in a legal filing against the AI company, the Verge reports.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" src="https://wp.technologyreview.com/wp-content/uploads/2024/08/240814_longevity_essay.jpg?fit=1616,908" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Maybe you will be able to live past 122&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;How long can humans live? This is a good time to ask the question. The longevity scene is having a moment, and research suggests that we might be able to push human life spans further, potentially even reversing some signs of aging.&lt;/p&gt;&lt;p&gt;Researchers can’t even agree on what the exact mechanisms of aging are and which they should be targeting. Debates continue to rage over how long it’s possible for humans to live—and whether there is a limit at all.&lt;/p&gt;&lt;p&gt;But it looks likely that something will be developed in the coming decades that will help us live longer, in better health. Read the full story.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;&lt;em&gt;—Jessica Hamzelou&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ There’s something so uplifting about this user-generated collection of videos of parks.&amp;nbsp;&lt;br /&gt;+ I could get on board with living in a cabin in the woods if it was this one.&amp;nbsp;&lt;br /&gt;+ You should probably let go of that grudge you’re holding onto.&amp;nbsp;&lt;br /&gt;+ Looking for some seasonal recipe inspo? Look no further.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/05/1117928/the-download-funding-a-crispr-embryo-startup-and-bad-news-for-clean-cement/</guid><pubDate>Thu, 05 Jun 2025 12:10:00 +0000</pubDate></item><item><title>GeForce NOW Kicks Off a Summer of Gaming With 25 New Titles This June (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/geforce-now-thursday-june-2025-games/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;GeForce NOW is a gamer’s ticket to an unforgettable summer of gaming. With 25 titles coming this month and endless ways to play, the summer is going to be epic.&lt;/p&gt;
&lt;p&gt;Dive in, level up and make it a summer to remember, one game at a time. Start with the ten games available this week, including advanced access for those who’ve preordered the Deluxe or Ultimate versions of Funcom’s highly anticipated &lt;i&gt;Dune: Awakening&lt;/i&gt;.&lt;/p&gt;
&lt;p&gt;Plus, check out the latest update for miHoYo’s &lt;i&gt;Zenless Zone Zero&lt;/i&gt;, bringing fresh content and even more action for summer.&lt;/p&gt;
&lt;p&gt;And to keep the good times rolling, take advantage of the GeForce NOW Summer Sale to enjoy a sizzling 40% off a six-month Performance membership. It’s the perfect way to extend a summer of fun in the cloud.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Dawn Rises With the Cloud&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_81520"&gt;&lt;img alt="Zenless Zone Zero V2.0" class="size-large wp-image-81520" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/GFN_Thursday-Zenless_Zone_Zero_V2-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-81520"&gt;&lt;em&gt;The next chapter begins.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Get ready for a new leap in &lt;i&gt;Zenless Zone Zero&lt;/i&gt;. Version 2.0 “Where Clouds Embrace the Dawn” launches tomorrow, June 6, marking the start of the game’s second season. Explore the new Waifei Peninsula, team up with Grandmaster Yixuan and manage the Suibian Temple, all with enhanced maps and navigation.&lt;/p&gt;
&lt;p&gt;Celebrate the game’s first anniversary with free rewards — including an S-Rank Agent, S-Rank W-Engine, and 1,600 Polychromes. With new agents, expanded content and major improvements, now’s the perfect time to jump into New Eridu.&lt;/p&gt;
&lt;p&gt;Stream it on GeForce NOW for instant access and top-tier performance — no downloads or high-end hardware needed. Stream the latest content with smooth graphics and low latency on any device, and jump straight into the action to enjoy all the new features and anniversary rewards.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Jumping Into June&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Level up summer gaming with the Summer Sale. Get 40% off six-month GeForce NOW Performance memberships — perfect for playing on handheld devices, including the new GeForce NOW app on Steam Deck, which lets gamers stream over 2,200 games at up to 4K 60 frames per second or 1440p 120 fps. Experience AAA gaming at max settings with longer battery life, and access supported games from Steam, Epic Games Store, PC Game Pass and more.&lt;/p&gt;
&lt;p&gt;Put that upgraded membership to the test with what’s coming to the cloud this week on GeForce NOW:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Symphonia&lt;/i&gt; (New release on Xbox, available on PC Game Pass, June 3)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Pro Cycling Manager 25 &lt;/i&gt;(New release on Steam, June 5)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Tour de France 2025 &lt;/i&gt;(New release on Steam, June 5)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Dune: Awakening – Advanced Access &lt;/i&gt;(New release on Steam, June 5)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;7 Days to Die &lt;/i&gt;(Xbox)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Clair Obscur: Expedition 33&lt;/i&gt; (Epic Games Store)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Cubic Odyssey &lt;/i&gt;&amp;nbsp;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Drive Beyond Horizons &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Police Simulator: Patrol Officers &lt;/i&gt;(Xbox, available on PC Game Pass)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Sea of Thieves &lt;/i&gt;(Battle.net)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here’s what to expect for the rest of June:&amp;nbsp;&lt;i&gt;&lt;/i&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Dune: Awakening &lt;/i&gt;(New release on Steam, June 10)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;MindsEye &lt;/i&gt;(New release on Steam, June 10)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;The Alters &lt;/i&gt;(New release on Steam and Xbox, available on PC Game Pass, June 13)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Architect Life: A House Design Simulator &lt;/i&gt;(New release on Steam, June 19)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Crime Simulator &lt;/i&gt;(New release on Steam, June 17)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;FBC: Firebreak &lt;/i&gt;(New release on Steam and Xbox, available on PC Game Pass, June 17)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Lost in Random: The Eternal Die &lt;/i&gt;(New release on Steam and Xbox, available on PC Game Pass, June 17)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Broken Arrow &lt;/i&gt;(New release on Steam, June 19)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;REMATCH&lt;/i&gt; (New release on Steam and Xbox, available on PC Game Pass, June 19)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;DREADZONE &lt;/i&gt;(New release on Steam, June 26)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;System Shock 2: 25th Anniversary Remaster &lt;/i&gt;(New release on Steam, June 26)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Borderlands Game of the Year Enhanced &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Borderlands 2&lt;/i&gt; (Steam and Epic Games Store)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Borderlands 3 &lt;/i&gt;(Steam and Epic Games Store)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Easy Red 2 &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;b&gt;May I Have More Games?&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;In addition to the 21 games announced last month, 16 more joined the GeForce NOW library:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Mafia &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Mafia II (Classic) &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Mafia: Definitive Edition &lt;/i&gt;(Steam and Epic Games Store)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Mafia II: Definitive Edition &lt;/i&gt;(Steam and Epic Games Store)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Mafia III: Definitive Edition &lt;/i&gt;(Steam and Epic Games Store)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Towerborne &lt;/i&gt;(Steam and Xbox, available on PC Game Pass)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Capcom Fighting Collection 2 &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Microsoft Flight Simulator 2024 &lt;/i&gt;(Steam and Xbox, available on PC Game Pass)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;S.T.A.L.K.E.R.: Call of Prypiat – Enhanced Edition&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;S.T.A.L.K.E.R.: Clear Sky – Enhanced Edition&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;S.T.A.L.K.E.R.: Shadow of Chornobyl – Enhanced Edition&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Game of Thrones: Kingsroad&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Splitgate 2&lt;/i&gt; Open Beta (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Onimusha 2: Samurai’s Destiny&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Nice Day for Fishing &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Cash Cleaner Simulator &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;i&gt;War Robots: Frontiers &lt;/i&gt;is no longer coming to GeForce NOW. Stay tuned for more game announcements and updates every GFN Thursday.&lt;/p&gt;
&lt;p&gt;What are you planning to play this weekend? Let us know on X or in the comments below.&lt;/p&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p dir="ltr" lang="en"&gt;What's your game of the summer? ☀️&lt;/p&gt;
&lt;p&gt;— 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) June 4, 2025&lt;/p&gt;&lt;/blockquote&gt;


		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;GeForce NOW is a gamer’s ticket to an unforgettable summer of gaming. With 25 titles coming this month and endless ways to play, the summer is going to be epic.&lt;/p&gt;
&lt;p&gt;Dive in, level up and make it a summer to remember, one game at a time. Start with the ten games available this week, including advanced access for those who’ve preordered the Deluxe or Ultimate versions of Funcom’s highly anticipated &lt;i&gt;Dune: Awakening&lt;/i&gt;.&lt;/p&gt;
&lt;p&gt;Plus, check out the latest update for miHoYo’s &lt;i&gt;Zenless Zone Zero&lt;/i&gt;, bringing fresh content and even more action for summer.&lt;/p&gt;
&lt;p&gt;And to keep the good times rolling, take advantage of the GeForce NOW Summer Sale to enjoy a sizzling 40% off a six-month Performance membership. It’s the perfect way to extend a summer of fun in the cloud.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Dawn Rises With the Cloud&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_81520"&gt;&lt;img alt="Zenless Zone Zero V2.0" class="size-large wp-image-81520" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/GFN_Thursday-Zenless_Zone_Zero_V2-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-81520"&gt;&lt;em&gt;The next chapter begins.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Get ready for a new leap in &lt;i&gt;Zenless Zone Zero&lt;/i&gt;. Version 2.0 “Where Clouds Embrace the Dawn” launches tomorrow, June 6, marking the start of the game’s second season. Explore the new Waifei Peninsula, team up with Grandmaster Yixuan and manage the Suibian Temple, all with enhanced maps and navigation.&lt;/p&gt;
&lt;p&gt;Celebrate the game’s first anniversary with free rewards — including an S-Rank Agent, S-Rank W-Engine, and 1,600 Polychromes. With new agents, expanded content and major improvements, now’s the perfect time to jump into New Eridu.&lt;/p&gt;
&lt;p&gt;Stream it on GeForce NOW for instant access and top-tier performance — no downloads or high-end hardware needed. Stream the latest content with smooth graphics and low latency on any device, and jump straight into the action to enjoy all the new features and anniversary rewards.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Jumping Into June&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Level up summer gaming with the Summer Sale. Get 40% off six-month GeForce NOW Performance memberships — perfect for playing on handheld devices, including the new GeForce NOW app on Steam Deck, which lets gamers stream over 2,200 games at up to 4K 60 frames per second or 1440p 120 fps. Experience AAA gaming at max settings with longer battery life, and access supported games from Steam, Epic Games Store, PC Game Pass and more.&lt;/p&gt;
&lt;p&gt;Put that upgraded membership to the test with what’s coming to the cloud this week on GeForce NOW:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Symphonia&lt;/i&gt; (New release on Xbox, available on PC Game Pass, June 3)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Pro Cycling Manager 25 &lt;/i&gt;(New release on Steam, June 5)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Tour de France 2025 &lt;/i&gt;(New release on Steam, June 5)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Dune: Awakening – Advanced Access &lt;/i&gt;(New release on Steam, June 5)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;7 Days to Die &lt;/i&gt;(Xbox)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Clair Obscur: Expedition 33&lt;/i&gt; (Epic Games Store)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Cubic Odyssey &lt;/i&gt;&amp;nbsp;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Drive Beyond Horizons &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Police Simulator: Patrol Officers &lt;/i&gt;(Xbox, available on PC Game Pass)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Sea of Thieves &lt;/i&gt;(Battle.net)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here’s what to expect for the rest of June:&amp;nbsp;&lt;i&gt;&lt;/i&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Dune: Awakening &lt;/i&gt;(New release on Steam, June 10)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;MindsEye &lt;/i&gt;(New release on Steam, June 10)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;The Alters &lt;/i&gt;(New release on Steam and Xbox, available on PC Game Pass, June 13)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Architect Life: A House Design Simulator &lt;/i&gt;(New release on Steam, June 19)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Crime Simulator &lt;/i&gt;(New release on Steam, June 17)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;FBC: Firebreak &lt;/i&gt;(New release on Steam and Xbox, available on PC Game Pass, June 17)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Lost in Random: The Eternal Die &lt;/i&gt;(New release on Steam and Xbox, available on PC Game Pass, June 17)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Broken Arrow &lt;/i&gt;(New release on Steam, June 19)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;REMATCH&lt;/i&gt; (New release on Steam and Xbox, available on PC Game Pass, June 19)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;DREADZONE &lt;/i&gt;(New release on Steam, June 26)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;System Shock 2: 25th Anniversary Remaster &lt;/i&gt;(New release on Steam, June 26)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Borderlands Game of the Year Enhanced &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Borderlands 2&lt;/i&gt; (Steam and Epic Games Store)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Borderlands 3 &lt;/i&gt;(Steam and Epic Games Store)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Easy Red 2 &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;b&gt;May I Have More Games?&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;In addition to the 21 games announced last month, 16 more joined the GeForce NOW library:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Mafia &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Mafia II (Classic) &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Mafia: Definitive Edition &lt;/i&gt;(Steam and Epic Games Store)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Mafia II: Definitive Edition &lt;/i&gt;(Steam and Epic Games Store)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Mafia III: Definitive Edition &lt;/i&gt;(Steam and Epic Games Store)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Towerborne &lt;/i&gt;(Steam and Xbox, available on PC Game Pass)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Capcom Fighting Collection 2 &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Microsoft Flight Simulator 2024 &lt;/i&gt;(Steam and Xbox, available on PC Game Pass)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;S.T.A.L.K.E.R.: Call of Prypiat – Enhanced Edition&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;S.T.A.L.K.E.R.: Clear Sky – Enhanced Edition&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;S.T.A.L.K.E.R.: Shadow of Chornobyl – Enhanced Edition&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Game of Thrones: Kingsroad&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Splitgate 2&lt;/i&gt; Open Beta (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Onimusha 2: Samurai’s Destiny&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Nice Day for Fishing &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Cash Cleaner Simulator &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;i&gt;War Robots: Frontiers &lt;/i&gt;is no longer coming to GeForce NOW. Stay tuned for more game announcements and updates every GFN Thursday.&lt;/p&gt;
&lt;p&gt;What are you planning to play this weekend? Let us know on X or in the comments below.&lt;/p&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p dir="ltr" lang="en"&gt;What's your game of the summer? ☀️&lt;/p&gt;
&lt;p&gt;— 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) June 4, 2025&lt;/p&gt;&lt;/blockquote&gt;


		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/geforce-now-thursday-june-2025-games/</guid><pubDate>Thu, 05 Jun 2025 13:00:37 +0000</pubDate></item><item><title>Securing AI at scale: Databricks and Noma close the inference vulnerability gap (AI News | VentureBeat)</title><link>https://venturebeat.com/security/databricks-noma-tackle-cisos-ai-inference-nightmare/</link><description>[unable to retrieve full-text content]Databricks Ventures and Noma Security partner to tackle critical AI inference vulnerabilities with real-time threat analytics, proactive red teaming, and robust governance, helping CISOs confidently scale secure enterprise AI deployments.</description><content:encoded>[unable to retrieve full-text content]Databricks Ventures and Noma Security partner to tackle critical AI inference vulnerabilities with real-time threat analytics, proactive red teaming, and robust governance, helping CISOs confidently scale secure enterprise AI deployments.</content:encoded><guid isPermaLink="false">https://venturebeat.com/security/databricks-noma-tackle-cisos-ai-inference-nightmare/</guid><pubDate>Thu, 05 Jun 2025 14:13:05 +0000</pubDate></item><item><title>How much information do LLMs really memorize? Now we know, thanks to Meta, Google, Nvidia and Cornell (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/how-much-information-do-llms-really-memorize-now-we-know-thanks-to-meta-google-nvidia-and-cornell/</link><description>[unable to retrieve full-text content]Using a clever solution, researchers find GPT-style models have a fixed memorization capacity of approximately 3.6 bits per parameter.</description><content:encoded>[unable to retrieve full-text content]Using a clever solution, researchers find GPT-style models have a fixed memorization capacity of approximately 3.6 bits per parameter.</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/how-much-information-do-llms-really-memorize-now-we-know-thanks-to-meta-google-nvidia-and-cornell/</guid><pubDate>Thu, 05 Jun 2025 15:35:34 +0000</pubDate></item><item><title>BenchmarkQED: Automated benchmarking of RAG systems (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/benchmarkqed-automated-benchmarking-of-rag-systems/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Diagram showing how the dimensions of query source (data-driven vs activity-driven) and query scope (local vs global) create four query classes that span the local-to-global query spectrum: data-local, activity-local, data-global, and activity-global. " class="wp-image-1140721" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/BenchmarkQED-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;One of the key&amp;nbsp;use cases&amp;nbsp;for generative AI&amp;nbsp;involves answering questions over&amp;nbsp;private datasets,&amp;nbsp;with retrieval-augmented generation (RAG)&amp;nbsp;as the&amp;nbsp;go-to framework.&amp;nbsp;As&amp;nbsp;new RAG&amp;nbsp;techniques&amp;nbsp;emerge,&amp;nbsp;there’s&amp;nbsp;a growing&amp;nbsp;need to benchmark&amp;nbsp;their performance&amp;nbsp;across&amp;nbsp;diverse&amp;nbsp;datasets&amp;nbsp;and&amp;nbsp;metrics.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To meet this need,&amp;nbsp;we’re&amp;nbsp;introducing&amp;nbsp;BenchmarkQED,&amp;nbsp;a new suite of tools&amp;nbsp;that&amp;nbsp;automates&amp;nbsp;RAG benchmarking at&amp;nbsp;scale, available on&amp;nbsp;GitHub&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. It includes components for&amp;nbsp;query generation, evaluation, and dataset preparation, each&amp;nbsp;designed to support rigorous, reproducible&amp;nbsp;testing.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;BenchmarkQED&amp;nbsp;complements the RAG methods in our open-source&amp;nbsp;GraphRAG&amp;nbsp;library, enabling users to run a&amp;nbsp;GraphRAG-style evaluation across models, metrics, and datasets.&amp;nbsp;GraphRAG&amp;nbsp;uses a&amp;nbsp;large&amp;nbsp;language model (LLM)&amp;nbsp;to generate and summarize entity-based knowledge graphs, producing more comprehensive and diverse answers than standard RAG for large-scale&amp;nbsp;tasks.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In this post, we walk through&amp;nbsp;the core components of&amp;nbsp;BenchmarkQED&amp;nbsp;that&amp;nbsp;contribute to the overall benchmarking process.&amp;nbsp;We also share some of the latest benchmark results comparing our LazyGraphRAG system&amp;nbsp;to&amp;nbsp;competing methods,&amp;nbsp;including&amp;nbsp;a vector-based&amp;nbsp;RAG with a 1M-token context window, where the leading&amp;nbsp;LazyGraphRAG&amp;nbsp;configuration&amp;nbsp;showed&amp;nbsp;significant win rates&amp;nbsp;across all combinations of quality metrics&amp;nbsp;and query classes.&lt;/p&gt;



&lt;p&gt;In the&amp;nbsp;paper, we&amp;nbsp;distinguish&amp;nbsp;between&amp;nbsp;&lt;em&gt;local&amp;nbsp;queries&lt;/em&gt;,&amp;nbsp;where&amp;nbsp;answers&amp;nbsp;are&amp;nbsp;found&amp;nbsp;in a&amp;nbsp;small number&amp;nbsp;of text regions, and sometimes even&amp;nbsp;a single region,&amp;nbsp;and&amp;nbsp;&lt;em&gt;global&amp;nbsp;queries&lt;/em&gt;, which require reasoning over&amp;nbsp;large&amp;nbsp;portions&amp;nbsp;of&amp;nbsp;or even the entire&amp;nbsp;dataset.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Conventional&amp;nbsp;vector-based RAG&amp;nbsp;excels at&amp;nbsp;local queries because the&amp;nbsp;regions&amp;nbsp;containing&amp;nbsp;the&amp;nbsp;answer&amp;nbsp;to the query&amp;nbsp;resemble the&amp;nbsp;query&amp;nbsp;itself&amp;nbsp;and can be retrieved&amp;nbsp;as&amp;nbsp;the&amp;nbsp;nearest neighbor in the&amp;nbsp;vector&amp;nbsp;space&amp;nbsp;of text embeddings.&amp;nbsp;However, it struggles&amp;nbsp;with&amp;nbsp;global questions,&amp;nbsp;such as, “What are the main themes of the dataset?” which&amp;nbsp;require&amp;nbsp;understanding&amp;nbsp;dataset qualities not&amp;nbsp;explicitly&amp;nbsp;stated&amp;nbsp;in&amp;nbsp;the text.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="autoq-automated-query-synthesis"&gt;AutoQ: Automated query synthesis&lt;/h2&gt;



&lt;p&gt;This limitation&amp;nbsp;motivated&amp;nbsp;the development of&amp;nbsp;GraphRAG&amp;nbsp;a system designed to&amp;nbsp;answer&amp;nbsp;global queries. GraphRAG’s evaluation&amp;nbsp;requirements&amp;nbsp;subsequently&amp;nbsp;led to the creation of&amp;nbsp;AutoQ, a method for synthesizing these global queries for any dataset.&lt;/p&gt;



&lt;p&gt;AutoQ extends this approach by generating synthetic queries across the spectrum of queries, from local to global. It defines four distinct classes based on the source and scope of the query (Figure 1, top) forming a logical progression along the spectrum (Figure 1, bottom).&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Diagram showing how the dimensions of query source (data-driven vs activity-driven) and query scope (local vs global) create four query classes that span the local-to-global query spectrum: data-local, activity-local, data-global, and activity-global. " class="wp-image-1140725" height="2117" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Figure1-4.png" width="4026" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. Construction of a 2×2 design space for synthetic query generation with&amp;nbsp;AutoQ, showing how the four resulting query classes map onto the local-global query spectrum.&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;AutoQ&amp;nbsp;can be configured to generate&amp;nbsp;any number and distribution of synthetic queries&amp;nbsp;along these&amp;nbsp;classes, enabling consistent benchmarking&amp;nbsp;across datasets without&amp;nbsp;requiring user&amp;nbsp;customization.&amp;nbsp;Figure 2 shows the synthesis process and sample&amp;nbsp;queries&amp;nbsp;from each class, using&amp;nbsp;an AP News dataset.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Diagram showing the processes for synthesizing queries in each of the four classes. Each process involves steps like generating dataset summaries, personas, tasks, and candidate queries, followed by clustering candidate queries and selecting the final query set. The data-local example query is “Why are junior doctors in South Korea striking in February 2024?”. The activity-local example query is “What are the public health implications of the newly discovered Alaskapox virus in Alaska?”. The data-global example query is “Across the dataset, what are the key public health challenges and the measures being taken to address them?”. The activity-global example query is “Across the dataset, what are the main public health initiatives mentioned that target underserved communities?”." class="wp-image-1140728" height="2117" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Figure2-3.png" width="4162" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. Synthesis process and example query for each of the&amp;nbsp;four&amp;nbsp;AutoQ&amp;nbsp;query classes.&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;Microsoft research podcast&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;NeurIPS 2024: The co-evolution of AI and systems with Lidong Zhou&lt;/h2&gt;
				
								&lt;p class="large"&gt;Just after his NeurIPS 2024 keynote on the co-evolution of systems and AI, Microsoft CVP Lidong Zhou joins the podcast to discuss how rapidly advancing AI impacts the systems supporting it and the opportunities to use AI to enhance systems engineering itself.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="autoe-automated-evaluation-framework"&gt;AutoE: Automated evaluation&amp;nbsp;framework&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Our evaluation of GraphRAG focused on analyzing key qualities of answers to global questions. The following qualities were used for the current evaluation:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Comprehensiveness&lt;/strong&gt;:&amp;nbsp;Does the&amp;nbsp;answer&amp;nbsp;address&amp;nbsp;all&amp;nbsp;relevant&amp;nbsp;aspects&amp;nbsp;of the question?&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Diversity&lt;/strong&gt;:&amp;nbsp;Does it present&amp;nbsp;varied perspectives&amp;nbsp;or&amp;nbsp;insights?&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Empowerment&lt;/strong&gt;:&amp;nbsp;Does it&amp;nbsp;help the reader understand and make informed judgments?&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Relevance&lt;/strong&gt;:&amp;nbsp;Does&amp;nbsp;it&amp;nbsp;address what the question is specifically asking?&amp;nbsp;&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;The&amp;nbsp;AutoE&amp;nbsp;component&amp;nbsp;scales&amp;nbsp;evaluation of&amp;nbsp;these qualities&amp;nbsp;using the LLM-as-a-Judge method.&amp;nbsp;It&amp;nbsp;presents&amp;nbsp;pairs of answers to an LLM, along with the query and target metric,&amp;nbsp;in counterbalanced order.&amp;nbsp;The&amp;nbsp;model determines whether the first answer wins, loses, or ties with the second.&amp;nbsp;Over&amp;nbsp;a set of queries, whether&amp;nbsp;from&amp;nbsp;AutoQ&amp;nbsp;or elsewhere, this&amp;nbsp;produces&amp;nbsp;win rates between&amp;nbsp;competing&amp;nbsp;methods. When&amp;nbsp;ground truth&amp;nbsp;is available, AutoE&amp;nbsp;can also score answers on&amp;nbsp;correctness, completeness, and&amp;nbsp;related metrics.&lt;/p&gt;



&lt;p&gt;An illustrative&amp;nbsp;evaluation&amp;nbsp;is&amp;nbsp;shown in Figure&amp;nbsp;3.&amp;nbsp;Using a dataset of&amp;nbsp;1,397&amp;nbsp;AP News&amp;nbsp;articles&amp;nbsp;on&amp;nbsp;health and healthcare, AutoQ&amp;nbsp;generated&amp;nbsp;50 queries&amp;nbsp;per&amp;nbsp;class&amp;nbsp;(200&amp;nbsp;total).&amp;nbsp;AutoE&amp;nbsp;then&amp;nbsp;compared&amp;nbsp;LazyGraphRAG&amp;nbsp;to&amp;nbsp;a competing&amp;nbsp;RAG method, running six&amp;nbsp;trials&amp;nbsp;per&amp;nbsp;query&amp;nbsp;across four&amp;nbsp;metrics,&amp;nbsp;using&amp;nbsp;GPT-4.1 as a judge.&lt;/p&gt;



&lt;p&gt;These&amp;nbsp;trial-level results were aggregated&amp;nbsp;using metric-based win rates,&amp;nbsp;where each trial is scored 1 for a win, 0.5 for a tie, and 0 for a loss,&amp;nbsp;and then averaged to calculate the overall win rate for each RAG method.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Bar charts with the y-axes representing win rates for LazyGraphRAG conditions. The x-axes contain a range of comparison conditions. Bars are clustered by LazyGraphRAG (LGR) condition and charts are faceted by query class. " class="wp-image-1140915" height="1000" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/Figure3-4.png" width="1200" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 3. Win rates of four&amp;nbsp;LazyGraphRAG&amp;nbsp;(LGR) configurations across methods, broken down by the&amp;nbsp;AutoQ&amp;nbsp;query class and averaged across&amp;nbsp;AutoE’s&amp;nbsp;four metrics: comprehensiveness, diversity, empowerment, and relevance.&amp;nbsp;LazyGraphRAG&amp;nbsp;outperforms&amp;nbsp;comparison conditions&amp;nbsp;where the bar&amp;nbsp;is above&amp;nbsp;50%.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The four LazyGraphRAG conditions (LGR_b200_c200, LGR_b50_c200, LGR_b50_c600, LGR_b200_c200_mini) differ by query budget (b50, b200) and chunk size (c200, c600). All used GPT-4o mini for relevance tests and GPT-4o for query expansion (to five subqueries) and answer generation, except for LGR_b200_c200_mini, which used GPT-4o mini throughout.&lt;/p&gt;



&lt;p&gt;Comparison&amp;nbsp;systems&amp;nbsp;were&amp;nbsp;GraphRAG&amp;nbsp;(Local, Global, and Drift Search),&amp;nbsp;Vector&amp;nbsp;RAG&amp;nbsp;with 8k-&amp;nbsp;and 120k-token windows, and&amp;nbsp;three&amp;nbsp;published&amp;nbsp;methods:&amp;nbsp;LightRAG&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;,&amp;nbsp;RAPTOR&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, and&amp;nbsp;TREX&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&amp;nbsp;All&amp;nbsp;methods&amp;nbsp;were limited to the same 8k&amp;nbsp;tokens&amp;nbsp;for answer generation.&amp;nbsp;GraphRAG&amp;nbsp;Global Search&amp;nbsp;used&amp;nbsp;level 2&amp;nbsp;of the community hierarchy.&lt;/p&gt;



&lt;p&gt;LazyGraphRAG outperformed every comparison condition using the same generative model (GPT-4o), winning all 96 comparisons, with all but one reaching statistical significance. The best overall performance came from the larger budget, smaller chunk size configuration (LGR_b200_c200). For DataLocal queries, the smaller budget (LGR_b50_c200) performed slightly better, likely because fewer chunks were relevant. For ActivityLocal queries, the larger chunk size (LGR_b50_c600) had a slight edge, likely because longer chunks provide a more coherent context.&lt;/p&gt;



&lt;p&gt;Competing methods performed relatively better on the query classes for which they were designed: GraphRAG Global for global queries, Vector RAG for local queries, and GraphRAG Drift Search, which combines both strategies, posed the strongest challenge overall.&lt;/p&gt;



&lt;p&gt;Increasing Vector RAG’s context window from 8k to 120k tokens did not improve its performance compared to LazyGraphRAG. This raised the question of how LazyGraphRAG would perform against Vector RAG with 1-million token context window containing most of the dataset.&lt;/p&gt;



&lt;p&gt;Figure 4 shows the follow-up experiment comparing LazyGraphRAG to Vector RAG using GPT-4.1 that enabled this comparison. Even against the 1M-token window, LazyGraphRAG achieved higher win rates across all comparisons, failing to reach significance only for the relevance of answers to DataLocal queries. These queries tend to benefit most from Vector RAG’s ranking of directly relevant chunks, making it hard for LazyGraphRAG to generate answers that have greater &lt;em&gt;relevance&lt;/em&gt; to the query, even though these answers may be dramatically more comprehensive, diverse, and empowering overall.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Bar charts with the y-axes representing win rates for LazyGraphRAG. The x-axes contain comparison conditions for vector-based RAG with 8 thousand, 120 thousand, and 1 million token context windows. Charts are faceted by query class and quality metric." class="wp-image-1140735" height="1000" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Figure4-1.png" width="1200" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 4.&amp;nbsp;Win rates of&amp;nbsp;LazyGraphRAG&amp;nbsp;(LGR)&amp;nbsp;over Vector RAG&amp;nbsp;across different&amp;nbsp;context window sizes, broken down by the&amp;nbsp;four&amp;nbsp;AutoQ&amp;nbsp;query classes&amp;nbsp;and&amp;nbsp;four&amp;nbsp;AutoE&amp;nbsp;metrics:&amp;nbsp;comprehensiveness, diversity, empowerment, and relevance.&amp;nbsp;Bars above 50%&amp;nbsp;indicate&amp;nbsp;that&amp;nbsp;LazyGraphRAG&amp;nbsp;outperformed&amp;nbsp;the&amp;nbsp;comparison&amp;nbsp;condition.&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="autod-automated-data-sampling-and-summarization"&gt;AutoD: Automated data sampling and summarization&lt;/h2&gt;



&lt;p&gt;Text datasets have an underlying topical structure, but the depth, breadth, and connectivity of that structure can vary widely. This variability makes it difficult to evaluate RAG systems consistently, as results may reflect the idiosyncrasies of the dataset rather than the system’s general capabilities.&lt;/p&gt;



&lt;p&gt;The AutoD component addresses this by sampling datasets to meet a target specification, defined by the number of topic clusters (breadth) and the number of samples per cluster (depth). This creates consistency across datasets, enabling more meaningful comparisons, as structurally aligned datasets lead to comparable AutoQ queries, which in turn support consistent AutoE evaluations.&lt;/p&gt;



&lt;p&gt;AutoD also includes tools for summarizing input or output datasets in a way that reflects their topical coverage. These summaries play an important role in the AutoQ query synthesis process, but they can also be used more broadly, such as in prompts where context space is limited.&lt;/p&gt;







&lt;p&gt;Since the release of the&amp;nbsp;GraphRAG&amp;nbsp;paper,&amp;nbsp;we’ve&amp;nbsp;received&amp;nbsp;many requests&amp;nbsp;to share the&amp;nbsp;dataset of the&amp;nbsp;&lt;em&gt;Behind the Tech&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;podcast&amp;nbsp;transcripts&amp;nbsp;we used in our evaluation.&amp;nbsp;An updated version of this dataset&amp;nbsp;is now available in the BenchmarkQED&amp;nbsp;repository&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, alongside&amp;nbsp;the&amp;nbsp;AP News dataset&amp;nbsp;containing&amp;nbsp;1,397&amp;nbsp;health-related&amp;nbsp;articles,&amp;nbsp;licensed for open&amp;nbsp;release.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;We hope these datasets, together with&amp;nbsp;the&amp;nbsp;BenchmarkQED&amp;nbsp;tools&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;,&amp;nbsp;help&amp;nbsp;accelerate&amp;nbsp;benchmark-driven&amp;nbsp;development of&amp;nbsp;RAG systems and&amp;nbsp;AI question-answering.&amp;nbsp;We invite the community to try them&amp;nbsp;on&amp;nbsp;GitHub&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&amp;nbsp;&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Diagram showing how the dimensions of query source (data-driven vs activity-driven) and query scope (local vs global) create four query classes that span the local-to-global query spectrum: data-local, activity-local, data-global, and activity-global. " class="wp-image-1140721" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/BenchmarkQED-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;One of the key&amp;nbsp;use cases&amp;nbsp;for generative AI&amp;nbsp;involves answering questions over&amp;nbsp;private datasets,&amp;nbsp;with retrieval-augmented generation (RAG)&amp;nbsp;as the&amp;nbsp;go-to framework.&amp;nbsp;As&amp;nbsp;new RAG&amp;nbsp;techniques&amp;nbsp;emerge,&amp;nbsp;there’s&amp;nbsp;a growing&amp;nbsp;need to benchmark&amp;nbsp;their performance&amp;nbsp;across&amp;nbsp;diverse&amp;nbsp;datasets&amp;nbsp;and&amp;nbsp;metrics.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To meet this need,&amp;nbsp;we’re&amp;nbsp;introducing&amp;nbsp;BenchmarkQED,&amp;nbsp;a new suite of tools&amp;nbsp;that&amp;nbsp;automates&amp;nbsp;RAG benchmarking at&amp;nbsp;scale, available on&amp;nbsp;GitHub&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. It includes components for&amp;nbsp;query generation, evaluation, and dataset preparation, each&amp;nbsp;designed to support rigorous, reproducible&amp;nbsp;testing.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;BenchmarkQED&amp;nbsp;complements the RAG methods in our open-source&amp;nbsp;GraphRAG&amp;nbsp;library, enabling users to run a&amp;nbsp;GraphRAG-style evaluation across models, metrics, and datasets.&amp;nbsp;GraphRAG&amp;nbsp;uses a&amp;nbsp;large&amp;nbsp;language model (LLM)&amp;nbsp;to generate and summarize entity-based knowledge graphs, producing more comprehensive and diverse answers than standard RAG for large-scale&amp;nbsp;tasks.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In this post, we walk through&amp;nbsp;the core components of&amp;nbsp;BenchmarkQED&amp;nbsp;that&amp;nbsp;contribute to the overall benchmarking process.&amp;nbsp;We also share some of the latest benchmark results comparing our LazyGraphRAG system&amp;nbsp;to&amp;nbsp;competing methods,&amp;nbsp;including&amp;nbsp;a vector-based&amp;nbsp;RAG with a 1M-token context window, where the leading&amp;nbsp;LazyGraphRAG&amp;nbsp;configuration&amp;nbsp;showed&amp;nbsp;significant win rates&amp;nbsp;across all combinations of quality metrics&amp;nbsp;and query classes.&lt;/p&gt;



&lt;p&gt;In the&amp;nbsp;paper, we&amp;nbsp;distinguish&amp;nbsp;between&amp;nbsp;&lt;em&gt;local&amp;nbsp;queries&lt;/em&gt;,&amp;nbsp;where&amp;nbsp;answers&amp;nbsp;are&amp;nbsp;found&amp;nbsp;in a&amp;nbsp;small number&amp;nbsp;of text regions, and sometimes even&amp;nbsp;a single region,&amp;nbsp;and&amp;nbsp;&lt;em&gt;global&amp;nbsp;queries&lt;/em&gt;, which require reasoning over&amp;nbsp;large&amp;nbsp;portions&amp;nbsp;of&amp;nbsp;or even the entire&amp;nbsp;dataset.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Conventional&amp;nbsp;vector-based RAG&amp;nbsp;excels at&amp;nbsp;local queries because the&amp;nbsp;regions&amp;nbsp;containing&amp;nbsp;the&amp;nbsp;answer&amp;nbsp;to the query&amp;nbsp;resemble the&amp;nbsp;query&amp;nbsp;itself&amp;nbsp;and can be retrieved&amp;nbsp;as&amp;nbsp;the&amp;nbsp;nearest neighbor in the&amp;nbsp;vector&amp;nbsp;space&amp;nbsp;of text embeddings.&amp;nbsp;However, it struggles&amp;nbsp;with&amp;nbsp;global questions,&amp;nbsp;such as, “What are the main themes of the dataset?” which&amp;nbsp;require&amp;nbsp;understanding&amp;nbsp;dataset qualities not&amp;nbsp;explicitly&amp;nbsp;stated&amp;nbsp;in&amp;nbsp;the text.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="autoq-automated-query-synthesis"&gt;AutoQ: Automated query synthesis&lt;/h2&gt;



&lt;p&gt;This limitation&amp;nbsp;motivated&amp;nbsp;the development of&amp;nbsp;GraphRAG&amp;nbsp;a system designed to&amp;nbsp;answer&amp;nbsp;global queries. GraphRAG’s evaluation&amp;nbsp;requirements&amp;nbsp;subsequently&amp;nbsp;led to the creation of&amp;nbsp;AutoQ, a method for synthesizing these global queries for any dataset.&lt;/p&gt;



&lt;p&gt;AutoQ extends this approach by generating synthetic queries across the spectrum of queries, from local to global. It defines four distinct classes based on the source and scope of the query (Figure 1, top) forming a logical progression along the spectrum (Figure 1, bottom).&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Diagram showing how the dimensions of query source (data-driven vs activity-driven) and query scope (local vs global) create four query classes that span the local-to-global query spectrum: data-local, activity-local, data-global, and activity-global. " class="wp-image-1140725" height="2117" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Figure1-4.png" width="4026" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. Construction of a 2×2 design space for synthetic query generation with&amp;nbsp;AutoQ, showing how the four resulting query classes map onto the local-global query spectrum.&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;AutoQ&amp;nbsp;can be configured to generate&amp;nbsp;any number and distribution of synthetic queries&amp;nbsp;along these&amp;nbsp;classes, enabling consistent benchmarking&amp;nbsp;across datasets without&amp;nbsp;requiring user&amp;nbsp;customization.&amp;nbsp;Figure 2 shows the synthesis process and sample&amp;nbsp;queries&amp;nbsp;from each class, using&amp;nbsp;an AP News dataset.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Diagram showing the processes for synthesizing queries in each of the four classes. Each process involves steps like generating dataset summaries, personas, tasks, and candidate queries, followed by clustering candidate queries and selecting the final query set. The data-local example query is “Why are junior doctors in South Korea striking in February 2024?”. The activity-local example query is “What are the public health implications of the newly discovered Alaskapox virus in Alaska?”. The data-global example query is “Across the dataset, what are the key public health challenges and the measures being taken to address them?”. The activity-global example query is “Across the dataset, what are the main public health initiatives mentioned that target underserved communities?”." class="wp-image-1140728" height="2117" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Figure2-3.png" width="4162" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. Synthesis process and example query for each of the&amp;nbsp;four&amp;nbsp;AutoQ&amp;nbsp;query classes.&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;Microsoft research podcast&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;NeurIPS 2024: The co-evolution of AI and systems with Lidong Zhou&lt;/h2&gt;
				
								&lt;p class="large"&gt;Just after his NeurIPS 2024 keynote on the co-evolution of systems and AI, Microsoft CVP Lidong Zhou joins the podcast to discuss how rapidly advancing AI impacts the systems supporting it and the opportunities to use AI to enhance systems engineering itself.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="autoe-automated-evaluation-framework"&gt;AutoE: Automated evaluation&amp;nbsp;framework&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Our evaluation of GraphRAG focused on analyzing key qualities of answers to global questions. The following qualities were used for the current evaluation:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Comprehensiveness&lt;/strong&gt;:&amp;nbsp;Does the&amp;nbsp;answer&amp;nbsp;address&amp;nbsp;all&amp;nbsp;relevant&amp;nbsp;aspects&amp;nbsp;of the question?&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Diversity&lt;/strong&gt;:&amp;nbsp;Does it present&amp;nbsp;varied perspectives&amp;nbsp;or&amp;nbsp;insights?&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Empowerment&lt;/strong&gt;:&amp;nbsp;Does it&amp;nbsp;help the reader understand and make informed judgments?&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Relevance&lt;/strong&gt;:&amp;nbsp;Does&amp;nbsp;it&amp;nbsp;address what the question is specifically asking?&amp;nbsp;&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;The&amp;nbsp;AutoE&amp;nbsp;component&amp;nbsp;scales&amp;nbsp;evaluation of&amp;nbsp;these qualities&amp;nbsp;using the LLM-as-a-Judge method.&amp;nbsp;It&amp;nbsp;presents&amp;nbsp;pairs of answers to an LLM, along with the query and target metric,&amp;nbsp;in counterbalanced order.&amp;nbsp;The&amp;nbsp;model determines whether the first answer wins, loses, or ties with the second.&amp;nbsp;Over&amp;nbsp;a set of queries, whether&amp;nbsp;from&amp;nbsp;AutoQ&amp;nbsp;or elsewhere, this&amp;nbsp;produces&amp;nbsp;win rates between&amp;nbsp;competing&amp;nbsp;methods. When&amp;nbsp;ground truth&amp;nbsp;is available, AutoE&amp;nbsp;can also score answers on&amp;nbsp;correctness, completeness, and&amp;nbsp;related metrics.&lt;/p&gt;



&lt;p&gt;An illustrative&amp;nbsp;evaluation&amp;nbsp;is&amp;nbsp;shown in Figure&amp;nbsp;3.&amp;nbsp;Using a dataset of&amp;nbsp;1,397&amp;nbsp;AP News&amp;nbsp;articles&amp;nbsp;on&amp;nbsp;health and healthcare, AutoQ&amp;nbsp;generated&amp;nbsp;50 queries&amp;nbsp;per&amp;nbsp;class&amp;nbsp;(200&amp;nbsp;total).&amp;nbsp;AutoE&amp;nbsp;then&amp;nbsp;compared&amp;nbsp;LazyGraphRAG&amp;nbsp;to&amp;nbsp;a competing&amp;nbsp;RAG method, running six&amp;nbsp;trials&amp;nbsp;per&amp;nbsp;query&amp;nbsp;across four&amp;nbsp;metrics,&amp;nbsp;using&amp;nbsp;GPT-4.1 as a judge.&lt;/p&gt;



&lt;p&gt;These&amp;nbsp;trial-level results were aggregated&amp;nbsp;using metric-based win rates,&amp;nbsp;where each trial is scored 1 for a win, 0.5 for a tie, and 0 for a loss,&amp;nbsp;and then averaged to calculate the overall win rate for each RAG method.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Bar charts with the y-axes representing win rates for LazyGraphRAG conditions. The x-axes contain a range of comparison conditions. Bars are clustered by LazyGraphRAG (LGR) condition and charts are faceted by query class. " class="wp-image-1140915" height="1000" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/Figure3-4.png" width="1200" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 3. Win rates of four&amp;nbsp;LazyGraphRAG&amp;nbsp;(LGR) configurations across methods, broken down by the&amp;nbsp;AutoQ&amp;nbsp;query class and averaged across&amp;nbsp;AutoE’s&amp;nbsp;four metrics: comprehensiveness, diversity, empowerment, and relevance.&amp;nbsp;LazyGraphRAG&amp;nbsp;outperforms&amp;nbsp;comparison conditions&amp;nbsp;where the bar&amp;nbsp;is above&amp;nbsp;50%.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The four LazyGraphRAG conditions (LGR_b200_c200, LGR_b50_c200, LGR_b50_c600, LGR_b200_c200_mini) differ by query budget (b50, b200) and chunk size (c200, c600). All used GPT-4o mini for relevance tests and GPT-4o for query expansion (to five subqueries) and answer generation, except for LGR_b200_c200_mini, which used GPT-4o mini throughout.&lt;/p&gt;



&lt;p&gt;Comparison&amp;nbsp;systems&amp;nbsp;were&amp;nbsp;GraphRAG&amp;nbsp;(Local, Global, and Drift Search),&amp;nbsp;Vector&amp;nbsp;RAG&amp;nbsp;with 8k-&amp;nbsp;and 120k-token windows, and&amp;nbsp;three&amp;nbsp;published&amp;nbsp;methods:&amp;nbsp;LightRAG&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;,&amp;nbsp;RAPTOR&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, and&amp;nbsp;TREX&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&amp;nbsp;All&amp;nbsp;methods&amp;nbsp;were limited to the same 8k&amp;nbsp;tokens&amp;nbsp;for answer generation.&amp;nbsp;GraphRAG&amp;nbsp;Global Search&amp;nbsp;used&amp;nbsp;level 2&amp;nbsp;of the community hierarchy.&lt;/p&gt;



&lt;p&gt;LazyGraphRAG outperformed every comparison condition using the same generative model (GPT-4o), winning all 96 comparisons, with all but one reaching statistical significance. The best overall performance came from the larger budget, smaller chunk size configuration (LGR_b200_c200). For DataLocal queries, the smaller budget (LGR_b50_c200) performed slightly better, likely because fewer chunks were relevant. For ActivityLocal queries, the larger chunk size (LGR_b50_c600) had a slight edge, likely because longer chunks provide a more coherent context.&lt;/p&gt;



&lt;p&gt;Competing methods performed relatively better on the query classes for which they were designed: GraphRAG Global for global queries, Vector RAG for local queries, and GraphRAG Drift Search, which combines both strategies, posed the strongest challenge overall.&lt;/p&gt;



&lt;p&gt;Increasing Vector RAG’s context window from 8k to 120k tokens did not improve its performance compared to LazyGraphRAG. This raised the question of how LazyGraphRAG would perform against Vector RAG with 1-million token context window containing most of the dataset.&lt;/p&gt;



&lt;p&gt;Figure 4 shows the follow-up experiment comparing LazyGraphRAG to Vector RAG using GPT-4.1 that enabled this comparison. Even against the 1M-token window, LazyGraphRAG achieved higher win rates across all comparisons, failing to reach significance only for the relevance of answers to DataLocal queries. These queries tend to benefit most from Vector RAG’s ranking of directly relevant chunks, making it hard for LazyGraphRAG to generate answers that have greater &lt;em&gt;relevance&lt;/em&gt; to the query, even though these answers may be dramatically more comprehensive, diverse, and empowering overall.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Bar charts with the y-axes representing win rates for LazyGraphRAG. The x-axes contain comparison conditions for vector-based RAG with 8 thousand, 120 thousand, and 1 million token context windows. Charts are faceted by query class and quality metric." class="wp-image-1140735" height="1000" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Figure4-1.png" width="1200" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 4.&amp;nbsp;Win rates of&amp;nbsp;LazyGraphRAG&amp;nbsp;(LGR)&amp;nbsp;over Vector RAG&amp;nbsp;across different&amp;nbsp;context window sizes, broken down by the&amp;nbsp;four&amp;nbsp;AutoQ&amp;nbsp;query classes&amp;nbsp;and&amp;nbsp;four&amp;nbsp;AutoE&amp;nbsp;metrics:&amp;nbsp;comprehensiveness, diversity, empowerment, and relevance.&amp;nbsp;Bars above 50%&amp;nbsp;indicate&amp;nbsp;that&amp;nbsp;LazyGraphRAG&amp;nbsp;outperformed&amp;nbsp;the&amp;nbsp;comparison&amp;nbsp;condition.&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="autod-automated-data-sampling-and-summarization"&gt;AutoD: Automated data sampling and summarization&lt;/h2&gt;



&lt;p&gt;Text datasets have an underlying topical structure, but the depth, breadth, and connectivity of that structure can vary widely. This variability makes it difficult to evaluate RAG systems consistently, as results may reflect the idiosyncrasies of the dataset rather than the system’s general capabilities.&lt;/p&gt;



&lt;p&gt;The AutoD component addresses this by sampling datasets to meet a target specification, defined by the number of topic clusters (breadth) and the number of samples per cluster (depth). This creates consistency across datasets, enabling more meaningful comparisons, as structurally aligned datasets lead to comparable AutoQ queries, which in turn support consistent AutoE evaluations.&lt;/p&gt;



&lt;p&gt;AutoD also includes tools for summarizing input or output datasets in a way that reflects their topical coverage. These summaries play an important role in the AutoQ query synthesis process, but they can also be used more broadly, such as in prompts where context space is limited.&lt;/p&gt;







&lt;p&gt;Since the release of the&amp;nbsp;GraphRAG&amp;nbsp;paper,&amp;nbsp;we’ve&amp;nbsp;received&amp;nbsp;many requests&amp;nbsp;to share the&amp;nbsp;dataset of the&amp;nbsp;&lt;em&gt;Behind the Tech&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;podcast&amp;nbsp;transcripts&amp;nbsp;we used in our evaluation.&amp;nbsp;An updated version of this dataset&amp;nbsp;is now available in the BenchmarkQED&amp;nbsp;repository&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, alongside&amp;nbsp;the&amp;nbsp;AP News dataset&amp;nbsp;containing&amp;nbsp;1,397&amp;nbsp;health-related&amp;nbsp;articles,&amp;nbsp;licensed for open&amp;nbsp;release.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;We hope these datasets, together with&amp;nbsp;the&amp;nbsp;BenchmarkQED&amp;nbsp;tools&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;,&amp;nbsp;help&amp;nbsp;accelerate&amp;nbsp;benchmark-driven&amp;nbsp;development of&amp;nbsp;RAG systems and&amp;nbsp;AI question-answering.&amp;nbsp;We invite the community to try them&amp;nbsp;on&amp;nbsp;GitHub&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&amp;nbsp;&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/benchmarkqed-automated-benchmarking-of-rag-systems/</guid><pubDate>Thu, 05 Jun 2025 16:00:00 +0000</pubDate></item><item><title>Zooming in: Efficient regional environmental risk assessment with generative AI (The latest research from Google)</title><link>https://research.google/blog/zooming-in-efficient-regional-environmental-risk-assessment-with-generative-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;Why this breakthrough matters&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;Dynamical-generative downscaling represents a significant step towards obtaining comprehensive future regional climate projections at actionable scales below 10 km. It makes downscaling large ensembles of Earth system models computationally feasible — our study estimates computational cost savings of 85% for the 8-model ensemble tested, a figure that would increase for larger ensembles. The fast and efficient AI inference step is similar to how Google’s SEEDS and GenCast weather forecasting models operate, enabling a thorough assessment of regional environmental risk.&lt;/p&gt;&lt;p&gt;By providing more accurate and probabilistically complete regional climate projections at a fraction of the computational cost, dynamical-generative downscaling can dramatically improve environmental risk assessments. This enables better-informed decisions for adaptation and resilience policies across vital sectors like agriculture, water resource management, energy infrastructure, and natural hazard preparedness.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;Why this breakthrough matters&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;Dynamical-generative downscaling represents a significant step towards obtaining comprehensive future regional climate projections at actionable scales below 10 km. It makes downscaling large ensembles of Earth system models computationally feasible — our study estimates computational cost savings of 85% for the 8-model ensemble tested, a figure that would increase for larger ensembles. The fast and efficient AI inference step is similar to how Google’s SEEDS and GenCast weather forecasting models operate, enabling a thorough assessment of regional environmental risk.&lt;/p&gt;&lt;p&gt;By providing more accurate and probabilistically complete regional climate projections at a fraction of the computational cost, dynamical-generative downscaling can dramatically improve environmental risk assessments. This enables better-informed decisions for adaptation and resilience policies across vital sectors like agriculture, water resource management, energy infrastructure, and natural hazard preparedness.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/zooming-in-efficient-regional-environmental-risk-assessment-with-generative-ai/</guid><pubDate>Thu, 05 Jun 2025 17:00:00 +0000</pubDate></item><item><title>Solidroad just raised $6.5M to reinvent customer service with AI that coaches, not replaces (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/solidroad-just-raised-6-5m-to-reinvent-customer-service-with-ai-that-coaches-not-replaces/</link><description>[unable to retrieve full-text content]Dublin AI startup Solidroad raises $6.5M from First Round Capital to transform customer service training with AI that coaches human agents and improves satisfaction scores.</description><content:encoded>[unable to retrieve full-text content]Dublin AI startup Solidroad raises $6.5M from First Round Capital to transform customer service training with AI that coaches human agents and improves satisfaction scores.</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/solidroad-just-raised-6-5m-to-reinvent-customer-service-with-ai-that-coaches-not-replaces/</guid><pubDate>Thu, 05 Jun 2025 17:39:53 +0000</pubDate></item><item><title>Manus has kick-started an AI agent boom in China (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/05/1117958/china-ai-agent-boom/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/250523_chinaai.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Last year, China saw a boom in foundation models, the do-everything large language models that underpin the AI revolution. This year, the focus has shifted to AI agents—systems that are less about responding to users’ queries and more about autonomously accomplishing things for them.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;There are now a host of Chinese startups building these general-purpose digital tools, which can answer emails, browse the internet to plan vacations, and even design an interactive website. Many of these have emerged in just the last two months, following in the footsteps of Manus—a general AI agent that sparked weeks of social media frenzy for invite codes after its limited-release launch in early March.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;These emerging AI agents aren’t large language models themselves. Instead, they’re built on top of them, using a workflow-based structure designed to get things done. A lot of these systems also introduce a different way of interacting with AI. Rather than just chatting back and forth with users, they are optimized for managing and executing multistep tasks—booking flights, managing schedules, conducting research—by using external tools and remembering instructions.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;China could take the lead on building these kinds of agents. The country’s tightly integrated app ecosystems, rapid product cycles, and digitally fluent user base could provide a favorable environment for embedding AI into daily life.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;For now, its leading AI agent startups are focusing their attention on the global market, because the best Western models don’t operate inside China’s firewalls. But that could change soon: Tech giants like ByteDance and Tencent are preparing their own AI agents that could bake automation directly into their native super-apps, pulling data from their vast ecosystem of programs that dominate many aspects of daily life in the country.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;As the race to define what a useful AI agent looks like unfolds, a mix of ambitious startups and entrenched tech giants are now testing how these tools might actually work in practice—and for whom.&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Set the standard&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;It’s been a whirlwind few months for Manus, which was developed by the startup Butterfly Effect. The company raised $75 million in a funding round led by the US venture capital firm Benchmark, took the product on an ambitious global roadshow, and hired dozens of new employees.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Even before registration opened to the public in May, Manus had become a reference point for what a broad, consumer‑oriented AI agent should accomplish. Rather than handling narrow chores for businesses, this “general” agent is designed to be able to help with everyday tasks like trip planning, stock comparison, or your kid’s school project.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Unlike previous AI agents, Manus uses a browser-based sandbox that lets users supervise the agent like an intern, watching in real time as it scrolls through web pages, reads articles, or codes actions. It also proactively asks clarifying questions, supports long-term memory that would serve as context for future tasks.&lt;/p&gt;  &lt;p&gt;"Manus represents a promising product experience for AI agents," says Ang Li, cofounder and CEO of Simular, a startup based in Palo Alto, California, that’s building AI agents that operate a real computer. “I believe Chinese startups have a huge advantage when it comes to designing consumer products, thanks to cutthroat domestic competition that leads to fast execution and greater attention to product details.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;In the case of Manus, the competition is moving fast. Two of the most buzzy follow‑ups, Genspark and Flowith, for example, are already boasting benchmark scores that match or edge past Manus’s.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Genspark, led by former Baidu executives Eric Jing and Kay Zhu, links many small “super agents” through what it calls multi‑component prompting. The agent can switch among several large language models, accepts both images and text, and carries out tasks from making slide decks to placing phone calls. Whereas Manus relies heavily on Browser Use, a popular open-source product that lets agents operate a web browser in a virtual window like a human, Genspark directly integrates with a wide array of tools and APIs. Launched in April, the company says that it already has over 5 million users and over $36 million in yearly revenue.&lt;/p&gt;  &lt;p&gt;Flowith, the work of a young team that first grabbed public attention in April 2025 at a developer event hosted by the popular social media app Xiaohongshu, takes a different tack. Marketed as an “infinite agent,” it opens on a blank canvas where each question becomes a node on a branching map. Users can backtrack, take new branches, and store results in personal or sharable “knowledge gardens”—a design that feels more like project management software (think Notion) than a typical chat interface. Every inquiry or task builds its own mind-map-like graph, encouraging a more nonlinear and creative interaction with AI. Flowith’s core agent, NEO, runs in the cloud and can perform scheduled tasks like sending emails and compiling files. The founders want the app to be a “knowledge marketbase”, and aims to tap into the social aspect of AI with the aspiration of becoming “the OnlyFans of AI knowledge creators”.&lt;/p&gt;  &lt;p&gt;What they also share with Manus is the global ambition. Both Genspark and Flowith have stated that their primary focus is the international market.&lt;/p&gt; 

 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;A global address&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Startups like Manus, Genspark, and Flowith—though founded by Chinese entrepreneurs—could blend seamlessly into the global tech scene and compete effectively abroad. Founders, investors, and analysts that &lt;em&gt;MIT Technology Review&lt;/em&gt; has spoken to believe Chinese companies are moving fast, executing well, and quickly coming up with new products.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Money reinforces the pull to launch overseas. Customers there pay more, and there are plenty to go around. “You can price in USD, and with the exchange rate that’s a sevenfold multiplier,” Manus cofounder Xiao Hong quipped on a podcast. “Even if we’re only operating at 10% power because of cultural differences overseas, we’ll still make more than in China.”&lt;/p&gt;  &lt;p&gt;But creating the same functionality in China is a challenge. Major US AI companies including OpenAI and Anthropic have opted out of mainland China because of geopolitical risks and challenges with regulatory compliance. Their absence initially created a black market as users resorted to VPNs and third-party mirrors to access tools like ChatGPT and Claude. That vacuum has since been filled by a new wave of Chinese chatbots—DeepSeek, Doubao, Kimi—but the appetite for foreign models hasn’t gone away.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Manus, for example, uses Anthropic’s Claude Sonnet—widely considered the top model for agentic tasks. Manus cofounder Zhang Tao has repeatedly praised Claude’s ability to juggle tools, remember contexts, and hold multi‑round conversations—all crucial for turning chatty software into an effective executive assistant.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;But the company’s use of Sonnet has made its agent functionally unusable inside China without a VPN. If you open Manus from a mainland IP address, you’ll see a notice explaining that the team is “working on integrating Qwen’s model,” a special local version that is built on top of Alibaba’s open-source model.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;An engineer overseeing ByteDance’s work on developing an agent, who spoke to &lt;em&gt;MIT Technology Review &lt;/em&gt;anonymously to avoid sanction, said that the absence of Claude Sonnet models “limits everything we do in China.” DeepSeek’s open models, he added, still hallucinate too often and lack training on real‑world workflows. Developers we spoke with rank Alibaba’s Qwen series as the best domestic alternative, yet most say that switching to Qwen knocks performance down a notch.&lt;/p&gt;  &lt;p&gt;Jiaxin Pei, a postdoctoral researcher at Stanford’s Institute for Human‑Centered AI, thinks that gap will close: “Building agentic capabilities in base LLMs has become a key focus for many LLM builders, and once people realize the value of this, it will only be a matter of time."&lt;/p&gt;  &lt;p&gt;For now, Manus is doubling down on audiences it can already serve. In a written response, the company said its “primary focus is overseas expansion,” noting that new offices in San Francisco, Singapore, and Tokyo have opened in the past month.&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;A super‑app approach&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Although the concept of AI agents is still relatively new, the consumer-facing AI app market in China is already crowded with major tech players. DeepSeek remains the most widely used, while ByteDance’s Doubao and Moonshot’s Kimi have also become household names. However, most of these apps are still optimized for chat and entertainment rather than task execution. This gap in the local market has pushed China’s big tech firms to roll out their own user-facing agents, though early versions remain uneven in quality and rough around the edges.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;ByteDance is testing Coze Space, an AI agent based on its own Doubao model family that lets users toggle between “plan” and “execute” modes, so they can either directly guide the agent’s actions or step back and watch it work autonomously. It connects up to 14 popular apps, including GitHub, Notion, and the company’s own Lark office suite. Early reviews say the tool can feel clunky and has a high failure rate, but it clearly aims to match what Manus offers.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;Meanwhile, Zhipu AI has released a free agent called AutoGLM Rumination, built on its proprietary ChatGLM models. Shanghai‑based Minimax has launched Minimax Agent. Both products look almost identical to Manus and demo basic tasks such as building a simple website, planning a trip, making a small Flash game, or running quick data analysis.&lt;/p&gt;  &lt;p&gt;Despite the limited usability of most general AI agents launched within China, big companies have plans to change that. During a May 15 earnings call, Tencent president Liu Zhiping teased an agent that would weave automation directly into China’s most ubiquitous app, WeChat.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt;&lt;p&gt;Considered the original super-app, WeChat already handles messaging, mobile payments, news, and millions of mini‑programs that act like embedded apps. These programs give Tencent, its developer, access to data from millions of services that pervade everyday life in China, an advantage most competitors can only envy.&lt;/p&gt;  &lt;p&gt;Historically, China’s consumer internet has splintered into competing walled gardens—share a Taobao link in WeChat and it resolves as plaintext, not a preview card. Unlike the more interoperable Western internet, China’s tech giants have long resisted integration with one another, choosing to wage platform war at the expense of a seamless user experience.&lt;/p&gt;  &lt;p&gt;But the use of mini‑programs has given WeChat unprecedented reach across services that once resisted interoperability, from gym bookings to grocery orders. An agent able to roam that ecosystem could bypass the integration headaches dogging independent startups.&lt;/p&gt;  &lt;p&gt;Alibaba, the e-commerce giant behind the Qwen model series, has been a front-runner in China's AI race but has been slower to release consumer-facing products. Even though Qwen was the most downloaded open-source model on Hugging Face in 2024, it didn’t power a dedicated chatbot app until early 2025. In March, Alibaba rebranded its cloud storage and search app Quark into an all-in-one AI search tool. By June, Quark had introduced DeepResearch—a new mode that marks its most agent-like effort to date.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;ByteDance and Alibaba did not reply to &lt;em&gt;MIT Technology Review&lt;/em&gt;’s request for comments.&lt;/p&gt;  &lt;p&gt;“Historically, Chinese tech products tend to pursue the all-in-one, super-app approach, and the latest Chinese AI agents reflect just that,” says Li of Simular, who previously worked at Google DeepMind on lifelong learning agents. “In contrast, AI agents in the US are more focused on serving specific verticals.”&lt;/p&gt;  &lt;p&gt;Pei, the researcher at Stanford, says that existing tech giants could have a huge advantage in bringing the vision of general AI agents to life—especially those with built-in integration across services. “The customer-facing AI agent market is still very early, with tons of problems like authentication and liability,” he says. “But companies that already operate across a wide range of services have a natural advantage in deploying agents at scale.”&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/250523_chinaai.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Last year, China saw a boom in foundation models, the do-everything large language models that underpin the AI revolution. This year, the focus has shifted to AI agents—systems that are less about responding to users’ queries and more about autonomously accomplishing things for them.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;There are now a host of Chinese startups building these general-purpose digital tools, which can answer emails, browse the internet to plan vacations, and even design an interactive website. Many of these have emerged in just the last two months, following in the footsteps of Manus—a general AI agent that sparked weeks of social media frenzy for invite codes after its limited-release launch in early March.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;These emerging AI agents aren’t large language models themselves. Instead, they’re built on top of them, using a workflow-based structure designed to get things done. A lot of these systems also introduce a different way of interacting with AI. Rather than just chatting back and forth with users, they are optimized for managing and executing multistep tasks—booking flights, managing schedules, conducting research—by using external tools and remembering instructions.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;China could take the lead on building these kinds of agents. The country’s tightly integrated app ecosystems, rapid product cycles, and digitally fluent user base could provide a favorable environment for embedding AI into daily life.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;For now, its leading AI agent startups are focusing their attention on the global market, because the best Western models don’t operate inside China’s firewalls. But that could change soon: Tech giants like ByteDance and Tencent are preparing their own AI agents that could bake automation directly into their native super-apps, pulling data from their vast ecosystem of programs that dominate many aspects of daily life in the country.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;As the race to define what a useful AI agent looks like unfolds, a mix of ambitious startups and entrenched tech giants are now testing how these tools might actually work in practice—and for whom.&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Set the standard&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;It’s been a whirlwind few months for Manus, which was developed by the startup Butterfly Effect. The company raised $75 million in a funding round led by the US venture capital firm Benchmark, took the product on an ambitious global roadshow, and hired dozens of new employees.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Even before registration opened to the public in May, Manus had become a reference point for what a broad, consumer‑oriented AI agent should accomplish. Rather than handling narrow chores for businesses, this “general” agent is designed to be able to help with everyday tasks like trip planning, stock comparison, or your kid’s school project.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Unlike previous AI agents, Manus uses a browser-based sandbox that lets users supervise the agent like an intern, watching in real time as it scrolls through web pages, reads articles, or codes actions. It also proactively asks clarifying questions, supports long-term memory that would serve as context for future tasks.&lt;/p&gt;  &lt;p&gt;"Manus represents a promising product experience for AI agents," says Ang Li, cofounder and CEO of Simular, a startup based in Palo Alto, California, that’s building AI agents that operate a real computer. “I believe Chinese startups have a huge advantage when it comes to designing consumer products, thanks to cutthroat domestic competition that leads to fast execution and greater attention to product details.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;In the case of Manus, the competition is moving fast. Two of the most buzzy follow‑ups, Genspark and Flowith, for example, are already boasting benchmark scores that match or edge past Manus’s.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Genspark, led by former Baidu executives Eric Jing and Kay Zhu, links many small “super agents” through what it calls multi‑component prompting. The agent can switch among several large language models, accepts both images and text, and carries out tasks from making slide decks to placing phone calls. Whereas Manus relies heavily on Browser Use, a popular open-source product that lets agents operate a web browser in a virtual window like a human, Genspark directly integrates with a wide array of tools and APIs. Launched in April, the company says that it already has over 5 million users and over $36 million in yearly revenue.&lt;/p&gt;  &lt;p&gt;Flowith, the work of a young team that first grabbed public attention in April 2025 at a developer event hosted by the popular social media app Xiaohongshu, takes a different tack. Marketed as an “infinite agent,” it opens on a blank canvas where each question becomes a node on a branching map. Users can backtrack, take new branches, and store results in personal or sharable “knowledge gardens”—a design that feels more like project management software (think Notion) than a typical chat interface. Every inquiry or task builds its own mind-map-like graph, encouraging a more nonlinear and creative interaction with AI. Flowith’s core agent, NEO, runs in the cloud and can perform scheduled tasks like sending emails and compiling files. The founders want the app to be a “knowledge marketbase”, and aims to tap into the social aspect of AI with the aspiration of becoming “the OnlyFans of AI knowledge creators”.&lt;/p&gt;  &lt;p&gt;What they also share with Manus is the global ambition. Both Genspark and Flowith have stated that their primary focus is the international market.&lt;/p&gt; 

 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;A global address&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Startups like Manus, Genspark, and Flowith—though founded by Chinese entrepreneurs—could blend seamlessly into the global tech scene and compete effectively abroad. Founders, investors, and analysts that &lt;em&gt;MIT Technology Review&lt;/em&gt; has spoken to believe Chinese companies are moving fast, executing well, and quickly coming up with new products.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Money reinforces the pull to launch overseas. Customers there pay more, and there are plenty to go around. “You can price in USD, and with the exchange rate that’s a sevenfold multiplier,” Manus cofounder Xiao Hong quipped on a podcast. “Even if we’re only operating at 10% power because of cultural differences overseas, we’ll still make more than in China.”&lt;/p&gt;  &lt;p&gt;But creating the same functionality in China is a challenge. Major US AI companies including OpenAI and Anthropic have opted out of mainland China because of geopolitical risks and challenges with regulatory compliance. Their absence initially created a black market as users resorted to VPNs and third-party mirrors to access tools like ChatGPT and Claude. That vacuum has since been filled by a new wave of Chinese chatbots—DeepSeek, Doubao, Kimi—but the appetite for foreign models hasn’t gone away.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Manus, for example, uses Anthropic’s Claude Sonnet—widely considered the top model for agentic tasks. Manus cofounder Zhang Tao has repeatedly praised Claude’s ability to juggle tools, remember contexts, and hold multi‑round conversations—all crucial for turning chatty software into an effective executive assistant.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;But the company’s use of Sonnet has made its agent functionally unusable inside China without a VPN. If you open Manus from a mainland IP address, you’ll see a notice explaining that the team is “working on integrating Qwen’s model,” a special local version that is built on top of Alibaba’s open-source model.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;An engineer overseeing ByteDance’s work on developing an agent, who spoke to &lt;em&gt;MIT Technology Review &lt;/em&gt;anonymously to avoid sanction, said that the absence of Claude Sonnet models “limits everything we do in China.” DeepSeek’s open models, he added, still hallucinate too often and lack training on real‑world workflows. Developers we spoke with rank Alibaba’s Qwen series as the best domestic alternative, yet most say that switching to Qwen knocks performance down a notch.&lt;/p&gt;  &lt;p&gt;Jiaxin Pei, a postdoctoral researcher at Stanford’s Institute for Human‑Centered AI, thinks that gap will close: “Building agentic capabilities in base LLMs has become a key focus for many LLM builders, and once people realize the value of this, it will only be a matter of time."&lt;/p&gt;  &lt;p&gt;For now, Manus is doubling down on audiences it can already serve. In a written response, the company said its “primary focus is overseas expansion,” noting that new offices in San Francisco, Singapore, and Tokyo have opened in the past month.&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;A super‑app approach&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Although the concept of AI agents is still relatively new, the consumer-facing AI app market in China is already crowded with major tech players. DeepSeek remains the most widely used, while ByteDance’s Doubao and Moonshot’s Kimi have also become household names. However, most of these apps are still optimized for chat and entertainment rather than task execution. This gap in the local market has pushed China’s big tech firms to roll out their own user-facing agents, though early versions remain uneven in quality and rough around the edges.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;ByteDance is testing Coze Space, an AI agent based on its own Doubao model family that lets users toggle between “plan” and “execute” modes, so they can either directly guide the agent’s actions or step back and watch it work autonomously. It connects up to 14 popular apps, including GitHub, Notion, and the company’s own Lark office suite. Early reviews say the tool can feel clunky and has a high failure rate, but it clearly aims to match what Manus offers.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;Meanwhile, Zhipu AI has released a free agent called AutoGLM Rumination, built on its proprietary ChatGLM models. Shanghai‑based Minimax has launched Minimax Agent. Both products look almost identical to Manus and demo basic tasks such as building a simple website, planning a trip, making a small Flash game, or running quick data analysis.&lt;/p&gt;  &lt;p&gt;Despite the limited usability of most general AI agents launched within China, big companies have plans to change that. During a May 15 earnings call, Tencent president Liu Zhiping teased an agent that would weave automation directly into China’s most ubiquitous app, WeChat.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt;&lt;p&gt;Considered the original super-app, WeChat already handles messaging, mobile payments, news, and millions of mini‑programs that act like embedded apps. These programs give Tencent, its developer, access to data from millions of services that pervade everyday life in China, an advantage most competitors can only envy.&lt;/p&gt;  &lt;p&gt;Historically, China’s consumer internet has splintered into competing walled gardens—share a Taobao link in WeChat and it resolves as plaintext, not a preview card. Unlike the more interoperable Western internet, China’s tech giants have long resisted integration with one another, choosing to wage platform war at the expense of a seamless user experience.&lt;/p&gt;  &lt;p&gt;But the use of mini‑programs has given WeChat unprecedented reach across services that once resisted interoperability, from gym bookings to grocery orders. An agent able to roam that ecosystem could bypass the integration headaches dogging independent startups.&lt;/p&gt;  &lt;p&gt;Alibaba, the e-commerce giant behind the Qwen model series, has been a front-runner in China's AI race but has been slower to release consumer-facing products. Even though Qwen was the most downloaded open-source model on Hugging Face in 2024, it didn’t power a dedicated chatbot app until early 2025. In March, Alibaba rebranded its cloud storage and search app Quark into an all-in-one AI search tool. By June, Quark had introduced DeepResearch—a new mode that marks its most agent-like effort to date.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;ByteDance and Alibaba did not reply to &lt;em&gt;MIT Technology Review&lt;/em&gt;’s request for comments.&lt;/p&gt;  &lt;p&gt;“Historically, Chinese tech products tend to pursue the all-in-one, super-app approach, and the latest Chinese AI agents reflect just that,” says Li of Simular, who previously worked at Google DeepMind on lifelong learning agents. “In contrast, AI agents in the US are more focused on serving specific verticals.”&lt;/p&gt;  &lt;p&gt;Pei, the researcher at Stanford, says that existing tech giants could have a huge advantage in bringing the vision of general AI agents to life—especially those with built-in integration across services. “The customer-facing AI agent market is still very early, with tons of problems like authentication and liability,” he says. “But companies that already operate across a wide range of services have a natural advantage in deploying agents at scale.”&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/05/1117958/china-ai-agent-boom/</guid><pubDate>Thu, 05 Jun 2025 18:45:42 +0000</pubDate></item><item><title>Soham Mazumdar, Co-Founder &amp; CEO of WisdomAI – Interview Series (Unite.AI)</title><link>https://www.unite.ai/soham-mazumdar-co-founder-ceo-of-wisdomai-interview-series/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.unite.ai/wp-content/uploads/2025/06/PHAN2605-copy-topaz-denoise-faceai-1-934x600.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Soham Mazumdar is the Co-Founder and CEO of WisdomAI, a company at the forefront of AI-driven solutions. Prior to founding WisdomAI in 2023, he was Co-Founder and Chief Architect at Rubrik, where he played a key role in scaling the company over a 9-year period. Soham previously held engineering leadership roles at Facebook and Google, where he contributed to core search infrastructure and was recognized with the Google Founder's Award. He also co-founded Tagtile, a mobile loyalty platform acquired by Facebook. With two decades of experience in software architecture and AI innovation, Soham is a seasoned entrepreneur and technologist based in the San Francisco Bay Area.&lt;/p&gt;&lt;p&gt;WisdomAI is an AI-native business intelligence platform that helps enterprises access real-time, accurate insights by integrating structured and unstructured data through its proprietary “Knowledge Fabric.” The platform powers specialized AI agents that curate data context, answer business questions in natural language, and proactively surface trends or risks—without generating hallucinated content. Unlike traditional BI tools, WisdomAI uses generative AI strictly for query generation, ensuring high accuracy and reliability. It integrates with existing data ecosystems and supports enterprise-grade security, with early adoption by major firms like Cisco and ConocoPhillips.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;You co-founded Rubrik and helped scale it into a major enterprise success. What inspired you to leave in 2023 and build WisdomAI—and was there a particular moment that clarified this new direction?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The enterprise data inefficiency problem was staring me right in the face. During my time at Rubrik, I witnessed firsthand how Fortune 500 companies were drowning in data but starving for insights. Even with all the infrastructure we built, less than 20% of enterprise users actually had the right access and know-how to use data effectively in their daily work. It was a massive, systemic problem that no one was really solving.&lt;/p&gt;&lt;p&gt;I'm also a builder by nature – you can see it in my path from Google to Tagtile to Rubrik and now WisdomAI. I get energized by taking on fundamental challenges and building solutions from the ground up. After helping scale Rubrik to enterprise success, I felt that entrepreneurial pull again to tackle something equally ambitious.&lt;/p&gt;&lt;p&gt;Last but not least, the AI opportunity was impossible to ignore. By 2023, it became clear that AI could finally bridge that gap between data availability and data usability. The timing felt perfect to build something that could democratize data insights for every enterprise user, not just the technical few.&lt;/p&gt;&lt;p&gt;The moment of clarity came when I realized we could combine everything I'd learned about enterprise data infrastructure at Rubrik with the transformative potential of AI to solve this fundamental inefficiency problem.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;WisdomAI introduces a “Knowledge Fabric” and a suite of AI agents. Can you break down how this system works together to move beyond traditional BI dashboards?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;We've built an agentic data insights platform that works with data where it is – structured, unstructured, and even “dirty” data. Rather than asking analytics teams to run reports, business managers can directly ask questions and drill into details. Our platform can be trained on any data warehousing system by analyzing query logs.&lt;/p&gt;&lt;p&gt;We're compatible with major cloud data services like Snowflake, Microsoft Fabric, Google's BigQuery, Amazon's Redshift, Databricks, and Postgres and also just document formats like excel, PDF, powerpoint etc.&lt;/p&gt;&lt;p&gt;Unlike conventional tools designed primarily for analysts, our conversational interface empowers business users to get answers directly, while our multi-agent architecture enables complex queries across diverse data systems.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;You've emphasized that WisdomAI avoids hallucinations by separating GenAI from answer generation. Can you explain how your system uses GenAI differently—and why that matters for enterprise trust?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Our AI-Ready Context Model trains on the organization's data to create a universal context understanding that answers questions with high semantic accuracy while maintaining data privacy and governance. Furthermore, we use generative AI to formulate well-scoped queries that allow us to extract data from the different systems, as opposed to feeding raw data into the LLMs. This is crucial for addressing hallucination and safety concerns with LLMs.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;You coined the term “Agentic Data Insights Platform.” How is agentic intelligence different from traditional analytics tools or even standard LLM-based assistants?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Traditional BI stacks slow decision-making because every question has to fight its way through disconnected data silos and a relay team of specialists. When a chief revenue officer needs to know how to close the quarter, the answer typically passes through half a dozen hands—analysts wrangling CRM extracts, data engineers stitching files together, and dashboard builders refreshing reports—turning a simple query into a multi-day project.&lt;/p&gt;&lt;p&gt;Our platform breaks down those silos and puts the full depth of data one keystroke away, so the CRO can drill from headline metrics all the way to row-level detail in seconds.&lt;/p&gt;&lt;p&gt;No waiting in the analyst queue, no predefined dashboards that can't keep up with new questions—just true self-service insights delivered at the speed the business moves.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;How do you ensure WisdomAI adapts to the unique data vocabulary and structure of each enterprise? What role does human input play in refining the Knowledge Fabric?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Working with data where and how it is – that's essentially the holy grail for enterprise business intelligence. Traditional systems aren't built to handle unstructured data or “dirty” data with typos and errors. When information exists across varied sources – databases, documents, telemetry data – organizations struggle to integrate this information cohesively.&lt;/p&gt;&lt;p&gt;Without capabilities to handle these diverse data types, valuable context remains isolated in separate systems. Our platform can be trained on any data warehousing system by analyzing query logs, allowing it to adapt to each organization's unique data vocabulary and structure.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;You've described WisdomAI's development process as ‘vibe coding'—building product experiences directly in code first, then iterating through real-world use. What advantages has this approach given you compared to traditional product design?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;“Vibe coding” is a significant shift in how software is built where developers leverage the power of AI tools to generate code simply by describing the desired functionality in natural language. It’s like an intelligent assistant that does what you want the software to do, and it writes the code for you. This dramatically reduces the manual effort and time traditionally required for coding.&lt;/p&gt;&lt;p&gt;For years, the creation of digital products has largely followed a familiar script: meticulously plan the product and UX design, then execute the development, and iterate based on feedback. The logic was clear because investing in design upfront minimizes costly rework during the more expensive and time-consuming development phase. But what happens when the cost and time to execute that development drastically shrinks? This capability flips the traditional development sequence on its head. Suddenly, developers can start building functional software based on a high-level understanding of the requirements, even before detailed product and UX designs are finalized.&lt;/p&gt;&lt;p&gt;With the speed of AI code generation, the effort involved in creating exhaustive upfront designs can, in certain contexts, become relatively more time-consuming than getting a basic, functional version of the software up and running. The new paradigm in the world of vibe coding becomes: execute (code with AI), then adapt (design and refine).&lt;/p&gt;&lt;p&gt;This approach allows for incredibly early user validation of the core concepts. Imagine getting feedback on the actual functionality of a feature before investing heavily in detailed visual designs. This can lead to more user-centric designs, as the design process is directly informed by how users interact with a tangible product.&lt;/p&gt;&lt;p&gt;At WisdomAI, we actively embrace AI code generation. We've found that by embracing rapid initial development, we can quickly test core functionalities and gather invaluable user feedback early in the process, live on the product. This allows our design team to then focus on refining the user experience and visual design based on real-world usage, leading to more effective and user-loved products, faster.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;From sales and marketing to manufacturing and customer success, WisdomAI targets a wide spectrum of business use cases. Which verticals have seen the fastest adoption—and what use cases have surprised you in their impact?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;We've seen transformative results with multiple customers. For F500 oil and gas company, ConocoPhillips, drilling engineers and operators now use our platform to query complex well data directly in natural language. Before WisdomAI, these engineers needed technical help for even basic operational questions about well status or job performance. Now they can instantly access this information while simultaneously comparing against best practices in their drilling manuals—all through the same conversational interface. They evaluated numerous AI vendors in a six-month process, and our solution delivered a 50% accuracy improvement over the closest competitor.&lt;/p&gt;&lt;p&gt;At a hyper growth Cyber Security company Descope, WisdomAI is used as a virtual data analyst for Sales and Finance. We reduced report creation time from 2-3 days to just 2-3 hours—a 90% decrease. This transformed their weekly sales meetings from data-gathering exercises to strategy sessions focused on actionable insights. As their CRO notes, “Wisdom AI brings data to my fingertips. It really democratizes the data, bringing me the power to go answer questions and move on with my day, rather than define your question, wait for somebody to build that answer, and then get it in 5 days.” This ability to make data-driven decisions with unprecedented speed has been particularly crucial for a fast-growing company in the competitive identity management market.&lt;/p&gt;&lt;p&gt;A practical example: A chief revenue officer asks, “How am I going to close my quarter?” Our platform immediately offers a list of pending deals to focus on, along with information on what's delaying each one – such as specific questions customers are waiting to have answered. This happens with five keystrokes instead of five specialists and days of delay.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Many companies today are overloaded with dashboards, reports, and siloed tools. What are the most common misconceptions enterprises have about business intelligence today?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Organizations sit on troves of information yet struggle to leverage this data for quick decision-making. The challenge isn't just about having data, but working with it in its natural state – which often includes “dirty” data not cleaned of typos or errors. Companies invest heavily in infrastructure but face bottlenecks with rigid dashboards, poor data hygiene, and siloed information. Most enterprises need specialized teams to run reports, creating significant delays when business leaders need answers quickly. The interface where people consume data remains outdated despite advancements in cloud data engines and data science.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Do you view WisdomAI as augmenting or eventually replacing existing BI tools like Tableau or Looker? How do you fit into the broader enterprise data stack?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;We're compatible with major cloud data services like Snowflake, Microsoft Fabric, Google's BigQuery, Amazon's Redshift, Databricks, and Postgres and also just document formats like excel, PDF, powerpoint etc. Our approach transforms the interface where people consume data, which has remained outdated despite advancements in cloud data engines and data science.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Looking ahead, where do you see WisdomAI in five years—and how do you see the concept of “agentic intelligence” evolving across the enterprise landscape?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The future of analytics is moving from specialist-driven reports to self-service intelligence accessible to everyone. BI tools have been around for 20+ years, but adoption hasn't even reached 20% of company employees. Meanwhile, in just twelve months, 60% of workplace users adopted ChatGPT, many using it for data analysis. This dramatic difference shows the potential for conversational interfaces to increase adoption.&lt;/p&gt;&lt;p&gt;We're seeing a fundamental shift where all employees can directly interrogate data without technical skills. The future will combine the computational power of AI with natural human interaction, allowing insights to find users proactively rather than requiring them to hunt through dashboards.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for the great interview, readers who wish to learn more should visit WisdomAI.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.unite.ai/wp-content/uploads/2025/06/PHAN2605-copy-topaz-denoise-faceai-1-934x600.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Soham Mazumdar is the Co-Founder and CEO of WisdomAI, a company at the forefront of AI-driven solutions. Prior to founding WisdomAI in 2023, he was Co-Founder and Chief Architect at Rubrik, where he played a key role in scaling the company over a 9-year period. Soham previously held engineering leadership roles at Facebook and Google, where he contributed to core search infrastructure and was recognized with the Google Founder's Award. He also co-founded Tagtile, a mobile loyalty platform acquired by Facebook. With two decades of experience in software architecture and AI innovation, Soham is a seasoned entrepreneur and technologist based in the San Francisco Bay Area.&lt;/p&gt;&lt;p&gt;WisdomAI is an AI-native business intelligence platform that helps enterprises access real-time, accurate insights by integrating structured and unstructured data through its proprietary “Knowledge Fabric.” The platform powers specialized AI agents that curate data context, answer business questions in natural language, and proactively surface trends or risks—without generating hallucinated content. Unlike traditional BI tools, WisdomAI uses generative AI strictly for query generation, ensuring high accuracy and reliability. It integrates with existing data ecosystems and supports enterprise-grade security, with early adoption by major firms like Cisco and ConocoPhillips.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;You co-founded Rubrik and helped scale it into a major enterprise success. What inspired you to leave in 2023 and build WisdomAI—and was there a particular moment that clarified this new direction?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The enterprise data inefficiency problem was staring me right in the face. During my time at Rubrik, I witnessed firsthand how Fortune 500 companies were drowning in data but starving for insights. Even with all the infrastructure we built, less than 20% of enterprise users actually had the right access and know-how to use data effectively in their daily work. It was a massive, systemic problem that no one was really solving.&lt;/p&gt;&lt;p&gt;I'm also a builder by nature – you can see it in my path from Google to Tagtile to Rubrik and now WisdomAI. I get energized by taking on fundamental challenges and building solutions from the ground up. After helping scale Rubrik to enterprise success, I felt that entrepreneurial pull again to tackle something equally ambitious.&lt;/p&gt;&lt;p&gt;Last but not least, the AI opportunity was impossible to ignore. By 2023, it became clear that AI could finally bridge that gap between data availability and data usability. The timing felt perfect to build something that could democratize data insights for every enterprise user, not just the technical few.&lt;/p&gt;&lt;p&gt;The moment of clarity came when I realized we could combine everything I'd learned about enterprise data infrastructure at Rubrik with the transformative potential of AI to solve this fundamental inefficiency problem.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;WisdomAI introduces a “Knowledge Fabric” and a suite of AI agents. Can you break down how this system works together to move beyond traditional BI dashboards?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;We've built an agentic data insights platform that works with data where it is – structured, unstructured, and even “dirty” data. Rather than asking analytics teams to run reports, business managers can directly ask questions and drill into details. Our platform can be trained on any data warehousing system by analyzing query logs.&lt;/p&gt;&lt;p&gt;We're compatible with major cloud data services like Snowflake, Microsoft Fabric, Google's BigQuery, Amazon's Redshift, Databricks, and Postgres and also just document formats like excel, PDF, powerpoint etc.&lt;/p&gt;&lt;p&gt;Unlike conventional tools designed primarily for analysts, our conversational interface empowers business users to get answers directly, while our multi-agent architecture enables complex queries across diverse data systems.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;You've emphasized that WisdomAI avoids hallucinations by separating GenAI from answer generation. Can you explain how your system uses GenAI differently—and why that matters for enterprise trust?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Our AI-Ready Context Model trains on the organization's data to create a universal context understanding that answers questions with high semantic accuracy while maintaining data privacy and governance. Furthermore, we use generative AI to formulate well-scoped queries that allow us to extract data from the different systems, as opposed to feeding raw data into the LLMs. This is crucial for addressing hallucination and safety concerns with LLMs.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;You coined the term “Agentic Data Insights Platform.” How is agentic intelligence different from traditional analytics tools or even standard LLM-based assistants?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Traditional BI stacks slow decision-making because every question has to fight its way through disconnected data silos and a relay team of specialists. When a chief revenue officer needs to know how to close the quarter, the answer typically passes through half a dozen hands—analysts wrangling CRM extracts, data engineers stitching files together, and dashboard builders refreshing reports—turning a simple query into a multi-day project.&lt;/p&gt;&lt;p&gt;Our platform breaks down those silos and puts the full depth of data one keystroke away, so the CRO can drill from headline metrics all the way to row-level detail in seconds.&lt;/p&gt;&lt;p&gt;No waiting in the analyst queue, no predefined dashboards that can't keep up with new questions—just true self-service insights delivered at the speed the business moves.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;How do you ensure WisdomAI adapts to the unique data vocabulary and structure of each enterprise? What role does human input play in refining the Knowledge Fabric?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Working with data where and how it is – that's essentially the holy grail for enterprise business intelligence. Traditional systems aren't built to handle unstructured data or “dirty” data with typos and errors. When information exists across varied sources – databases, documents, telemetry data – organizations struggle to integrate this information cohesively.&lt;/p&gt;&lt;p&gt;Without capabilities to handle these diverse data types, valuable context remains isolated in separate systems. Our platform can be trained on any data warehousing system by analyzing query logs, allowing it to adapt to each organization's unique data vocabulary and structure.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;You've described WisdomAI's development process as ‘vibe coding'—building product experiences directly in code first, then iterating through real-world use. What advantages has this approach given you compared to traditional product design?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;“Vibe coding” is a significant shift in how software is built where developers leverage the power of AI tools to generate code simply by describing the desired functionality in natural language. It’s like an intelligent assistant that does what you want the software to do, and it writes the code for you. This dramatically reduces the manual effort and time traditionally required for coding.&lt;/p&gt;&lt;p&gt;For years, the creation of digital products has largely followed a familiar script: meticulously plan the product and UX design, then execute the development, and iterate based on feedback. The logic was clear because investing in design upfront minimizes costly rework during the more expensive and time-consuming development phase. But what happens when the cost and time to execute that development drastically shrinks? This capability flips the traditional development sequence on its head. Suddenly, developers can start building functional software based on a high-level understanding of the requirements, even before detailed product and UX designs are finalized.&lt;/p&gt;&lt;p&gt;With the speed of AI code generation, the effort involved in creating exhaustive upfront designs can, in certain contexts, become relatively more time-consuming than getting a basic, functional version of the software up and running. The new paradigm in the world of vibe coding becomes: execute (code with AI), then adapt (design and refine).&lt;/p&gt;&lt;p&gt;This approach allows for incredibly early user validation of the core concepts. Imagine getting feedback on the actual functionality of a feature before investing heavily in detailed visual designs. This can lead to more user-centric designs, as the design process is directly informed by how users interact with a tangible product.&lt;/p&gt;&lt;p&gt;At WisdomAI, we actively embrace AI code generation. We've found that by embracing rapid initial development, we can quickly test core functionalities and gather invaluable user feedback early in the process, live on the product. This allows our design team to then focus on refining the user experience and visual design based on real-world usage, leading to more effective and user-loved products, faster.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;From sales and marketing to manufacturing and customer success, WisdomAI targets a wide spectrum of business use cases. Which verticals have seen the fastest adoption—and what use cases have surprised you in their impact?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;We've seen transformative results with multiple customers. For F500 oil and gas company, ConocoPhillips, drilling engineers and operators now use our platform to query complex well data directly in natural language. Before WisdomAI, these engineers needed technical help for even basic operational questions about well status or job performance. Now they can instantly access this information while simultaneously comparing against best practices in their drilling manuals—all through the same conversational interface. They evaluated numerous AI vendors in a six-month process, and our solution delivered a 50% accuracy improvement over the closest competitor.&lt;/p&gt;&lt;p&gt;At a hyper growth Cyber Security company Descope, WisdomAI is used as a virtual data analyst for Sales and Finance. We reduced report creation time from 2-3 days to just 2-3 hours—a 90% decrease. This transformed their weekly sales meetings from data-gathering exercises to strategy sessions focused on actionable insights. As their CRO notes, “Wisdom AI brings data to my fingertips. It really democratizes the data, bringing me the power to go answer questions and move on with my day, rather than define your question, wait for somebody to build that answer, and then get it in 5 days.” This ability to make data-driven decisions with unprecedented speed has been particularly crucial for a fast-growing company in the competitive identity management market.&lt;/p&gt;&lt;p&gt;A practical example: A chief revenue officer asks, “How am I going to close my quarter?” Our platform immediately offers a list of pending deals to focus on, along with information on what's delaying each one – such as specific questions customers are waiting to have answered. This happens with five keystrokes instead of five specialists and days of delay.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Many companies today are overloaded with dashboards, reports, and siloed tools. What are the most common misconceptions enterprises have about business intelligence today?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Organizations sit on troves of information yet struggle to leverage this data for quick decision-making. The challenge isn't just about having data, but working with it in its natural state – which often includes “dirty” data not cleaned of typos or errors. Companies invest heavily in infrastructure but face bottlenecks with rigid dashboards, poor data hygiene, and siloed information. Most enterprises need specialized teams to run reports, creating significant delays when business leaders need answers quickly. The interface where people consume data remains outdated despite advancements in cloud data engines and data science.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Do you view WisdomAI as augmenting or eventually replacing existing BI tools like Tableau or Looker? How do you fit into the broader enterprise data stack?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;We're compatible with major cloud data services like Snowflake, Microsoft Fabric, Google's BigQuery, Amazon's Redshift, Databricks, and Postgres and also just document formats like excel, PDF, powerpoint etc. Our approach transforms the interface where people consume data, which has remained outdated despite advancements in cloud data engines and data science.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Looking ahead, where do you see WisdomAI in five years—and how do you see the concept of “agentic intelligence” evolving across the enterprise landscape?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The future of analytics is moving from specialist-driven reports to self-service intelligence accessible to everyone. BI tools have been around for 20+ years, but adoption hasn't even reached 20% of company employees. Meanwhile, in just twelve months, 60% of workplace users adopted ChatGPT, many using it for data analysis. This dramatic difference shows the potential for conversational interfaces to increase adoption.&lt;/p&gt;&lt;p&gt;We're seeing a fundamental shift where all employees can directly interrogate data without technical skills. The future will combine the computational power of AI with natural human interaction, allowing insights to find users proactively rather than requiring them to hunt through dashboards.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for the great interview, readers who wish to learn more should visit WisdomAI.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.unite.ai/soham-mazumdar-co-founder-ceo-of-wisdomai-interview-series/</guid><pubDate>Thu, 05 Jun 2025 21:01:47 +0000</pubDate></item><item><title>Google claims Gemini 2.5 Pro preview beats DeepSeek R1 and Grok 3 Beta in coding performance (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/google-claims-gemini-2-5-pro-preview-beats-deepseek-r1-and-grok-3-beta-in-coding-performance/</link><description>[unable to retrieve full-text content]Google said the newest version of Gemini 2.5 Pro, now on preview, gives faster and more creative responses while performing better than OpenAI's o3.</description><content:encoded>[unable to retrieve full-text content]Google said the newest version of Gemini 2.5 Pro, now on preview, gives faster and more creative responses while performing better than OpenAI's o3.</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/google-claims-gemini-2-5-pro-preview-beats-deepseek-r1-and-grok-3-beta-in-coding-performance/</guid><pubDate>Thu, 05 Jun 2025 21:30:12 +0000</pubDate></item><item><title>ScreenSuite - The most comprehensive evaluation suite for GUI Agents! (Hugging Face - Blog)</title><link>https://huggingface.co/blog/screensuite</link><description>&lt;!-- HTML_TAG_START --&gt;
&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Over the past few weeks, we’ve been working tirelessly on making GUI agents more open, accessible and easy to integrate. Along the way, we created the largest benchmarking suite for GUI agents performances 👉&amp;nbsp;let us introduce ScreenSuite.&lt;/p&gt;
&lt;p&gt;We are very excited to share it with you today: ScreenSuite is the most comprehensive and easiest way to evaluate Vision Language Models (VLMs)across many agentic capabilities!&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		WTF is a GUI Agent?
	&lt;/span&gt;
&lt;/h3&gt;
&lt;div&gt;
  &lt;video controls="controls"&gt;
    &lt;source src="https://os-world.github.io/static/videos/main.mp4" type="video/mp4" /&gt;
  &lt;/video&gt;
  &lt;p&gt;&lt;i&gt;GUI Agents in action - courtesy of OSWorld&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;In short, an AI Agent is a robot that acts in the virtual world. (more thorough definition here) &lt;/p&gt;
&lt;p&gt;In particular, a “GUI Agent” is an agent that lives in a GUI. Think “an agent that can do clicks and navigate on my desktop or my phone”, à la Claude Computer Use.&lt;/p&gt;
&lt;p&gt;This means in essence that the AI model powering the agent will be given a task like “Fill the rest of this Excel column”, along with screen captures of the GUI. Using this information, it will then decide to take action on the system : &lt;code&gt;click(x=130, y=540)&lt;/code&gt; to open a web browser, &lt;code&gt;type(”Value for XYZ in 2025")&lt;/code&gt;, &lt;code&gt;scroll(down=2)&lt;/code&gt; to read further… To see a GUI agent in action, you can try our Open Computer Agent, powered by Qwen2.5-VL-72B.&lt;/p&gt;
&lt;p&gt;A good GUI agent will be able to navigate a computer just like we would, thus unlocking all computer tasks : scrolling through Google Maps, editing a file, buying an item online. This involves a variety of capabilities that can be hard to evaluate.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Introducing ScreenSuite 🥳
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The literature, for instance Xu et al. (2025) or Qin et al. (2025), generally splits GUI agent abilities amongst several categories:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Perception: correctly perceiving the informati displayed on screen&lt;/li&gt;
&lt;li&gt;Grounding: understanding the positioning of elements - this is paramount to click the correct place&lt;/li&gt;
&lt;li&gt;Single step actions: solving instructions correctly over one action&lt;/li&gt;
&lt;li&gt;Multi-step agents: solving a higher-level goal through several actions in a GUI environment.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So our first contribution is to &lt;strong&gt;gather and unify a comprehensive suite of 13 benchmarks spanning the full range of these GUI agent capabilities.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you look at the last category listed above, evaluating Multi-step agentic capabilities is especially challenging as it requires virtual machines to run the agent’s environment, be it Windows, Android, Ubuntu... To address this, we provide support both for E2B desktop remote sandboxes, and we created from scratch a new option to easily launch Ubuntu or Android virtual machines in Docker!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Implementation details&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We’ve carefully designed our benchmark suite with modularity and consistency in mind, ensuring strong alignment across tasks and environments. When required, especially for online benchmarks, we leverage smolagents as framework layer to streamline agent execution and orchestration.&lt;/p&gt;
&lt;p&gt;To support reproducibility and ease of use, we’ve built custom Dockerized containers that allow local deployment of full &lt;strong&gt;Ubuntu Desktop&lt;/strong&gt; or &lt;strong&gt;Android&lt;/strong&gt; environments.&lt;/p&gt;
&lt;p&gt;Unlike many existing GUI benchmarks that rely on accessibility trees or other metadata alongside visual input, our stack is intentionally &lt;strong&gt;vision-only&lt;/strong&gt;. While this can result in different scores on some established leaderboards, we deem that it creates a more realistic and challenging setup, one that better reflects how humans perceive and interact with graphical interfaces.&lt;/p&gt;
&lt;p&gt;– All agentic frameworks (Android World, OSWorld, GAIAWeb, Mind2Web) use smolagents and rely solely on &lt;strong&gt;vision&lt;/strong&gt;, without any accessibility tree or DOM added (in contrast with evaluation settings reported in other sources).
– &lt;strong&gt;Mind2Web (Multimodal)&lt;/strong&gt; originally used &lt;strong&gt;element-name-based multi-choice selection&lt;/strong&gt; based on the accessibility tree and screenshots, but was later adapted to &lt;strong&gt;click precision within bounding boxes&lt;/strong&gt; using &lt;strong&gt;vision only&lt;/strong&gt;, which significantly increases task difficulty.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Ranking leading VLMs on ScreenSuite 📊
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;We’ve evaluated leading VLMs on the benchmark&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Qwen-2.5-VL series of models from 3B to 72B. These models are known for their amazing localization capabilities, in other words they know the coordinates of any element in an image which makes them suited for GUI agents that need to click precisely.&lt;/li&gt;
&lt;li&gt;UI-Tars-1.5-7B, the all-rounder by ByteDance.&lt;/li&gt;
&lt;li&gt;Holo1-7B, the latest model by H company, showing extremely performant localization for its size.&lt;/li&gt;
&lt;li&gt;GPT-4o&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our scores are in general agreement with the scores reported in various sources! &lt;em&gt;With the caveat that we evaluate on vision only, causing some differences, see implementation details above.&lt;/em&gt;&lt;/p&gt;
&lt;div class="flex justify-center"&gt;
    &lt;img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/screensuite/scores_screensuite.png" /&gt;
&lt;/div&gt;

&lt;blockquote class="note"&gt;
&lt;p&gt;💡&amp;nbsp;Note that ScreenSuite does not intend to exactly reproduce benchmarks published in the industry: we evaluate models on &lt;em&gt;GUI agentic capabilities based on vision&lt;/em&gt;. As a result, on benchmarks like Mind2Web where other benchmarks gave the agent a view of information rich context like DOM or accessibility tree, our evaluation setting is much harder, thus ScreenSuite does not match other sources.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Start your custom evaluation in 30s ⚡️
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Head to the repository.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Clone the repository with submodules: &lt;code&gt;git clone --recurse-submodules git@github.com:huggingface/screensuite.git&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Install the package: &lt;code&gt;uv sync --extra submodules --python 3.11&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;python run.py&lt;/code&gt; &lt;ul&gt;
&lt;li&gt;Alternatively, run &lt;code&gt;python examples/run_benchmarks.py&lt;/code&gt; for more fine-grained control, like running evaluations for several models in parallel.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote class="note"&gt;
&lt;p&gt;The multistep benchmarks &lt;strong&gt;requires a bare-metal machine&lt;/strong&gt; to run and deploy desktop/mobile* environment *emulators (see README.md)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Next steps 🚀
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Running consistent and meaningful evaluations easily allows the community to quickly iterate and make progress in this field, as we’ve seen with Eleuther LM evaluation harness, the Open LLM Leaderboard and the Chatbot Arena.&lt;/p&gt;
&lt;p&gt;We hope to see much more capable open models in the coming month that can run a wide range of tasks reliably and even run locally!&lt;/p&gt;
&lt;p&gt;To support this effort:&lt;/p&gt;

&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;!-- HTML_TAG_START --&gt;
&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Over the past few weeks, we’ve been working tirelessly on making GUI agents more open, accessible and easy to integrate. Along the way, we created the largest benchmarking suite for GUI agents performances 👉&amp;nbsp;let us introduce ScreenSuite.&lt;/p&gt;
&lt;p&gt;We are very excited to share it with you today: ScreenSuite is the most comprehensive and easiest way to evaluate Vision Language Models (VLMs)across many agentic capabilities!&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		WTF is a GUI Agent?
	&lt;/span&gt;
&lt;/h3&gt;
&lt;div&gt;
  &lt;video controls="controls"&gt;
    &lt;source src="https://os-world.github.io/static/videos/main.mp4" type="video/mp4" /&gt;
  &lt;/video&gt;
  &lt;p&gt;&lt;i&gt;GUI Agents in action - courtesy of OSWorld&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;In short, an AI Agent is a robot that acts in the virtual world. (more thorough definition here) &lt;/p&gt;
&lt;p&gt;In particular, a “GUI Agent” is an agent that lives in a GUI. Think “an agent that can do clicks and navigate on my desktop or my phone”, à la Claude Computer Use.&lt;/p&gt;
&lt;p&gt;This means in essence that the AI model powering the agent will be given a task like “Fill the rest of this Excel column”, along with screen captures of the GUI. Using this information, it will then decide to take action on the system : &lt;code&gt;click(x=130, y=540)&lt;/code&gt; to open a web browser, &lt;code&gt;type(”Value for XYZ in 2025")&lt;/code&gt;, &lt;code&gt;scroll(down=2)&lt;/code&gt; to read further… To see a GUI agent in action, you can try our Open Computer Agent, powered by Qwen2.5-VL-72B.&lt;/p&gt;
&lt;p&gt;A good GUI agent will be able to navigate a computer just like we would, thus unlocking all computer tasks : scrolling through Google Maps, editing a file, buying an item online. This involves a variety of capabilities that can be hard to evaluate.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Introducing ScreenSuite 🥳
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The literature, for instance Xu et al. (2025) or Qin et al. (2025), generally splits GUI agent abilities amongst several categories:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Perception: correctly perceiving the informati displayed on screen&lt;/li&gt;
&lt;li&gt;Grounding: understanding the positioning of elements - this is paramount to click the correct place&lt;/li&gt;
&lt;li&gt;Single step actions: solving instructions correctly over one action&lt;/li&gt;
&lt;li&gt;Multi-step agents: solving a higher-level goal through several actions in a GUI environment.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So our first contribution is to &lt;strong&gt;gather and unify a comprehensive suite of 13 benchmarks spanning the full range of these GUI agent capabilities.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you look at the last category listed above, evaluating Multi-step agentic capabilities is especially challenging as it requires virtual machines to run the agent’s environment, be it Windows, Android, Ubuntu... To address this, we provide support both for E2B desktop remote sandboxes, and we created from scratch a new option to easily launch Ubuntu or Android virtual machines in Docker!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Implementation details&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We’ve carefully designed our benchmark suite with modularity and consistency in mind, ensuring strong alignment across tasks and environments. When required, especially for online benchmarks, we leverage smolagents as framework layer to streamline agent execution and orchestration.&lt;/p&gt;
&lt;p&gt;To support reproducibility and ease of use, we’ve built custom Dockerized containers that allow local deployment of full &lt;strong&gt;Ubuntu Desktop&lt;/strong&gt; or &lt;strong&gt;Android&lt;/strong&gt; environments.&lt;/p&gt;
&lt;p&gt;Unlike many existing GUI benchmarks that rely on accessibility trees or other metadata alongside visual input, our stack is intentionally &lt;strong&gt;vision-only&lt;/strong&gt;. While this can result in different scores on some established leaderboards, we deem that it creates a more realistic and challenging setup, one that better reflects how humans perceive and interact with graphical interfaces.&lt;/p&gt;
&lt;p&gt;– All agentic frameworks (Android World, OSWorld, GAIAWeb, Mind2Web) use smolagents and rely solely on &lt;strong&gt;vision&lt;/strong&gt;, without any accessibility tree or DOM added (in contrast with evaluation settings reported in other sources).
– &lt;strong&gt;Mind2Web (Multimodal)&lt;/strong&gt; originally used &lt;strong&gt;element-name-based multi-choice selection&lt;/strong&gt; based on the accessibility tree and screenshots, but was later adapted to &lt;strong&gt;click precision within bounding boxes&lt;/strong&gt; using &lt;strong&gt;vision only&lt;/strong&gt;, which significantly increases task difficulty.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Ranking leading VLMs on ScreenSuite 📊
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;We’ve evaluated leading VLMs on the benchmark&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Qwen-2.5-VL series of models from 3B to 72B. These models are known for their amazing localization capabilities, in other words they know the coordinates of any element in an image which makes them suited for GUI agents that need to click precisely.&lt;/li&gt;
&lt;li&gt;UI-Tars-1.5-7B, the all-rounder by ByteDance.&lt;/li&gt;
&lt;li&gt;Holo1-7B, the latest model by H company, showing extremely performant localization for its size.&lt;/li&gt;
&lt;li&gt;GPT-4o&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our scores are in general agreement with the scores reported in various sources! &lt;em&gt;With the caveat that we evaluate on vision only, causing some differences, see implementation details above.&lt;/em&gt;&lt;/p&gt;
&lt;div class="flex justify-center"&gt;
    &lt;img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/screensuite/scores_screensuite.png" /&gt;
&lt;/div&gt;

&lt;blockquote class="note"&gt;
&lt;p&gt;💡&amp;nbsp;Note that ScreenSuite does not intend to exactly reproduce benchmarks published in the industry: we evaluate models on &lt;em&gt;GUI agentic capabilities based on vision&lt;/em&gt;. As a result, on benchmarks like Mind2Web where other benchmarks gave the agent a view of information rich context like DOM or accessibility tree, our evaluation setting is much harder, thus ScreenSuite does not match other sources.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Start your custom evaluation in 30s ⚡️
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Head to the repository.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Clone the repository with submodules: &lt;code&gt;git clone --recurse-submodules git@github.com:huggingface/screensuite.git&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Install the package: &lt;code&gt;uv sync --extra submodules --python 3.11&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;python run.py&lt;/code&gt; &lt;ul&gt;
&lt;li&gt;Alternatively, run &lt;code&gt;python examples/run_benchmarks.py&lt;/code&gt; for more fine-grained control, like running evaluations for several models in parallel.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote class="note"&gt;
&lt;p&gt;The multistep benchmarks &lt;strong&gt;requires a bare-metal machine&lt;/strong&gt; to run and deploy desktop/mobile* environment *emulators (see README.md)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Next steps 🚀
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Running consistent and meaningful evaluations easily allows the community to quickly iterate and make progress in this field, as we’ve seen with Eleuther LM evaluation harness, the Open LLM Leaderboard and the Chatbot Arena.&lt;/p&gt;
&lt;p&gt;We hope to see much more capable open models in the coming month that can run a wide range of tasks reliably and even run locally!&lt;/p&gt;
&lt;p&gt;To support this effort:&lt;/p&gt;

&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/screensuite</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Cursor AI Rockets to $9.9 Billion Valuation with Massive $900 Million Raise (Unite.AI)</title><link>https://www.unite.ai/cursor-ai-rockets-to-9-9-billion-valuation-with-massive-900-million-raise/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.unite.ai/wp-content/uploads/2025/06/Cursor-AI-Raises-942x600.png" /&gt;&lt;/div&gt;&lt;p&gt;In a striking show of investor confidence in the future of AI-powered software development, Anysphere, the company behind the AI code editor Cursor, has announced a monumental $900 million funding round, pushing the startup’s valuation to $9.9 billion. The round was led by Thrive Capital, with participation from Andreessen Horowitz, Accel, and DST Global — all returning backers who have doubled down on what many are calling the most promising generative AI tool for developers to date.&lt;/p&gt;&lt;p&gt;This funding makes Cursor one of the most valuable companies in the booming AI developer tools space, eclipsing the size of most Series C rounds and rivaling capital injections typically reserved for decacorn-level tech giants.&lt;/p&gt;&lt;h2&gt;The Rise of Cursor: An AI Pair Programmer Goes Mainstream&lt;/h2&gt;&lt;p&gt;Launched in 2023 by four MIT alumni under the company Anysphere, &lt;em&gt;Cursor&lt;/em&gt; has quickly grown into one of the most widely adopted AI-first coding environments in the world. Built on top of Visual Studio Code, Cursor reimagines how developers write software by combining traditional editing with an embedded AI assistant. The tool enables developers to:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Autocomplete or generate code from natural language prompts&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Refactor or explain existing code snippets&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Troubleshoot bugs or errors through conversational feedback&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Collaborate with an in-editor chatbot trained for development tasks&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The company frames this as “building the engineer of the future” — a human-AI hybrid that dramatically boosts productivity by reducing cognitive load and automating routine tasks.&lt;/p&gt;&lt;p&gt;What sets Cursor apart is not just its capabilities, but its integration into real developer workflows. The product has gone viral among software teams at major tech firms, with widespread adoption inside OpenAI, Stripe, Shopify, Spotify, and Instacart. Its freemium pricing model (with $20/month and $40/month tiers) and intuitive interface have made it accessible even to solo developers, while enterprise plans are rapidly gaining traction in larger organizations.&lt;/p&gt;&lt;h2&gt;Revenue Trajectory: Unmatched Growth&lt;/h2&gt;&lt;p&gt;Few startups in history have achieved the kind of growth Anysphere is now seeing. The company reports that Cursor is already generating nearly a billion lines of AI-assisted code per day, and its annual recurring revenue (ARR) has surged to $500 million, up from $300 million just earlier this year. Internal figures suggest ARR has been doubling roughly every two months, a pace that has left even seasoned investors stunned.&lt;/p&gt;&lt;p&gt;This growth trajectory made Anysphere an attractive acquisition target earlier in 2025, with &lt;strong&gt;OpenAI&lt;/strong&gt; reportedly exploring a buyout. Instead of selling, Anysphere declined the offers, including OpenAI’s $3 billion purchase attempt — choosing to scale independently and raise capital on its own terms.&lt;/p&gt;&lt;p&gt;&lt;em&gt;“Our approach is to build the engineer of the future: a human–AI programmer that’s an order of magnitude more effective than any one programmer,”&lt;/em&gt; said &lt;strong&gt;Anysphere's co-founders&lt;/strong&gt;&amp;nbsp;in an official statement.&lt;/p&gt;&lt;h2&gt;Fueling the Future of Code&lt;/h2&gt;&lt;p&gt;With $900 million in fresh capital, Anysphere plans to aggressively scale its operations. The company will expand its R&amp;amp;D team to continue pushing the boundaries of generative code models, improve reliability and responsiveness for large-scale enterprises, and further optimize its AI models for real-time code assistance.&lt;/p&gt;&lt;p&gt;This round also marks a strategic pivot toward capturing the enterprise segment. While Cursor has already found a home among individual developers and small teams, Anysphere now has the resources to build out enterprise-grade integrations, enhanced security features, and dedicated support — positioning it as a direct competitor not just to startups like Replit or Windsurf, but also to tools offered by Google, Amazon, and Microsoft.&lt;/p&gt;&lt;p&gt;Cursor’s roadmap may eventually extend beyond code. With many developers already describing Cursor as a “thinking partner,” Anysphere is exploring new ways to turn the code editor into a more holistic AI collaborator — one that can manage software architecture, automate testing, and even prototype full applications from brief prompts.&lt;/p&gt;&lt;h2&gt;A Crowded Market — but Cursor Leads&lt;/h2&gt;&lt;p&gt;The broader market for AI-assisted developer tools has exploded over the last 18 months. Microsoft’s GitHub Copilot paved the way, now reportedly generating over $500 million in annual revenue. Startups like Replit, Poolside, Together AI, and Windsurf have all secured large funding rounds as investors rush to back the next AI code juggernaut.&lt;/p&gt;&lt;p&gt;Yet Cursor stands out. It’s not a plugin or a gimmick — it’s a full-fledged coding environment purpose-built for AI integration. This native-first approach has helped Cursor outpace competitors on both adoption and revenue. By mid-2025, Cursor had already surpassed what took Copilot several years to achieve.&lt;/p&gt;&lt;p&gt;Analysts say that despite rising competition, Anysphere’s head start, technical team, and deep user love put it in a class of its own.&lt;/p&gt;&lt;h2&gt;Final Thoughts&lt;/h2&gt;&lt;p&gt;Cursor’s $900 million raise signals a decisive tipping point in software development. AI is no longer just a behind-the-scenes productivity booster — it’s becoming a co-pilot, a collaborator, and increasingly, the primary creator. Nowhere is this more evident than in the rise of what developers are calling “vibe coding”: the ability to express intent in natural language and watch entire functions or components emerge in real-time, without ever touching a traditional keyboard shortcut or syntax reference.&lt;/p&gt;&lt;p&gt;Vibe coding reflects a fundamental shift in how software is conceived and built. Rather than starting with boilerplate or syntax scaffolding, developers can now describe problems, request code in plain English, and iterate through ideas conversationally. The code becomes an output of intent — not the manual product of line-by-line construction. In this world, the developer becomes more of an architect and strategist, guiding the AI through high-level objectives while trusting it to handle the lower-level logic.&lt;/p&gt;&lt;p&gt;It remains to be seen who will win the AI vibe coding race—Cursor, Replit, Windsurf, or a new entrant—but one thing is certain: this won’t be the wildest valuation we’ll witness.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.unite.ai/wp-content/uploads/2025/06/Cursor-AI-Raises-942x600.png" /&gt;&lt;/div&gt;&lt;p&gt;In a striking show of investor confidence in the future of AI-powered software development, Anysphere, the company behind the AI code editor Cursor, has announced a monumental $900 million funding round, pushing the startup’s valuation to $9.9 billion. The round was led by Thrive Capital, with participation from Andreessen Horowitz, Accel, and DST Global — all returning backers who have doubled down on what many are calling the most promising generative AI tool for developers to date.&lt;/p&gt;&lt;p&gt;This funding makes Cursor one of the most valuable companies in the booming AI developer tools space, eclipsing the size of most Series C rounds and rivaling capital injections typically reserved for decacorn-level tech giants.&lt;/p&gt;&lt;h2&gt;The Rise of Cursor: An AI Pair Programmer Goes Mainstream&lt;/h2&gt;&lt;p&gt;Launched in 2023 by four MIT alumni under the company Anysphere, &lt;em&gt;Cursor&lt;/em&gt; has quickly grown into one of the most widely adopted AI-first coding environments in the world. Built on top of Visual Studio Code, Cursor reimagines how developers write software by combining traditional editing with an embedded AI assistant. The tool enables developers to:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Autocomplete or generate code from natural language prompts&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Refactor or explain existing code snippets&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Troubleshoot bugs or errors through conversational feedback&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Collaborate with an in-editor chatbot trained for development tasks&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The company frames this as “building the engineer of the future” — a human-AI hybrid that dramatically boosts productivity by reducing cognitive load and automating routine tasks.&lt;/p&gt;&lt;p&gt;What sets Cursor apart is not just its capabilities, but its integration into real developer workflows. The product has gone viral among software teams at major tech firms, with widespread adoption inside OpenAI, Stripe, Shopify, Spotify, and Instacart. Its freemium pricing model (with $20/month and $40/month tiers) and intuitive interface have made it accessible even to solo developers, while enterprise plans are rapidly gaining traction in larger organizations.&lt;/p&gt;&lt;h2&gt;Revenue Trajectory: Unmatched Growth&lt;/h2&gt;&lt;p&gt;Few startups in history have achieved the kind of growth Anysphere is now seeing. The company reports that Cursor is already generating nearly a billion lines of AI-assisted code per day, and its annual recurring revenue (ARR) has surged to $500 million, up from $300 million just earlier this year. Internal figures suggest ARR has been doubling roughly every two months, a pace that has left even seasoned investors stunned.&lt;/p&gt;&lt;p&gt;This growth trajectory made Anysphere an attractive acquisition target earlier in 2025, with &lt;strong&gt;OpenAI&lt;/strong&gt; reportedly exploring a buyout. Instead of selling, Anysphere declined the offers, including OpenAI’s $3 billion purchase attempt — choosing to scale independently and raise capital on its own terms.&lt;/p&gt;&lt;p&gt;&lt;em&gt;“Our approach is to build the engineer of the future: a human–AI programmer that’s an order of magnitude more effective than any one programmer,”&lt;/em&gt; said &lt;strong&gt;Anysphere's co-founders&lt;/strong&gt;&amp;nbsp;in an official statement.&lt;/p&gt;&lt;h2&gt;Fueling the Future of Code&lt;/h2&gt;&lt;p&gt;With $900 million in fresh capital, Anysphere plans to aggressively scale its operations. The company will expand its R&amp;amp;D team to continue pushing the boundaries of generative code models, improve reliability and responsiveness for large-scale enterprises, and further optimize its AI models for real-time code assistance.&lt;/p&gt;&lt;p&gt;This round also marks a strategic pivot toward capturing the enterprise segment. While Cursor has already found a home among individual developers and small teams, Anysphere now has the resources to build out enterprise-grade integrations, enhanced security features, and dedicated support — positioning it as a direct competitor not just to startups like Replit or Windsurf, but also to tools offered by Google, Amazon, and Microsoft.&lt;/p&gt;&lt;p&gt;Cursor’s roadmap may eventually extend beyond code. With many developers already describing Cursor as a “thinking partner,” Anysphere is exploring new ways to turn the code editor into a more holistic AI collaborator — one that can manage software architecture, automate testing, and even prototype full applications from brief prompts.&lt;/p&gt;&lt;h2&gt;A Crowded Market — but Cursor Leads&lt;/h2&gt;&lt;p&gt;The broader market for AI-assisted developer tools has exploded over the last 18 months. Microsoft’s GitHub Copilot paved the way, now reportedly generating over $500 million in annual revenue. Startups like Replit, Poolside, Together AI, and Windsurf have all secured large funding rounds as investors rush to back the next AI code juggernaut.&lt;/p&gt;&lt;p&gt;Yet Cursor stands out. It’s not a plugin or a gimmick — it’s a full-fledged coding environment purpose-built for AI integration. This native-first approach has helped Cursor outpace competitors on both adoption and revenue. By mid-2025, Cursor had already surpassed what took Copilot several years to achieve.&lt;/p&gt;&lt;p&gt;Analysts say that despite rising competition, Anysphere’s head start, technical team, and deep user love put it in a class of its own.&lt;/p&gt;&lt;h2&gt;Final Thoughts&lt;/h2&gt;&lt;p&gt;Cursor’s $900 million raise signals a decisive tipping point in software development. AI is no longer just a behind-the-scenes productivity booster — it’s becoming a co-pilot, a collaborator, and increasingly, the primary creator. Nowhere is this more evident than in the rise of what developers are calling “vibe coding”: the ability to express intent in natural language and watch entire functions or components emerge in real-time, without ever touching a traditional keyboard shortcut or syntax reference.&lt;/p&gt;&lt;p&gt;Vibe coding reflects a fundamental shift in how software is conceived and built. Rather than starting with boilerplate or syntax scaffolding, developers can now describe problems, request code in plain English, and iterate through ideas conversationally. The code becomes an output of intent — not the manual product of line-by-line construction. In this world, the developer becomes more of an architect and strategist, guiding the AI through high-level objectives while trusting it to handle the lower-level logic.&lt;/p&gt;&lt;p&gt;It remains to be seen who will win the AI vibe coding race—Cursor, Replit, Windsurf, or a new entrant—but one thing is certain: this won’t be the wildest valuation we’ll witness.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.unite.ai/cursor-ai-rockets-to-9-9-billion-valuation-with-massive-900-million-raise/</guid><pubDate>Fri, 06 Jun 2025 01:20:00 +0000</pubDate></item><item><title>Artists, Fashion Designers Tap State-of-the-Art AI for NVIDIA GTC Paris Gallery (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/ai-art-gtc-paris-2025/</link><description>&lt;!-- OneTrust Cookies Consent Notice start for nvidia.com --&gt;


&lt;!-- OneTrust Cookies Consent Notice end for nvidia.com --&gt;


	
	
	
	
	
	

	

	
	
	


	&lt;!-- This site is optimized with the Yoast SEO Premium plugin v25.2 (Yoast SEO v25.2) - https://yoast.com/wordpress/plugins/seo/ --&gt;
	Artists, Designers Tap AI for NVIDIA GTC Paris Gallery | NVIDIA Blog
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	&lt;!-- / Yoast SEO Premium plugin. --&gt;







































&lt;!-- Stream WordPress user activity plugin v4.1.1 --&gt;


	
				&lt;!-- Hotjar Tracking Code for NVIDIA --&gt;
			
			

				
				



		
		
&lt;div class="hfeed site" id="page"&gt;
	Skip to content

	&lt;!-- #masthead --&gt;
		
		&lt;div class="full-width-layout light"&gt;
		

		
&lt;div class="full-width-layout__hero light"&gt;
	&lt;div class="full-width-layout__hero-content light"&gt;
		&lt;div class="full-width-layout__hero-content__inner light"&gt;
			

							&lt;p&gt;
					The conference, taking place in one of Europe’s iconic art capitals, will feature a curated gallery that showcases how AI helps bring creative visions to life. 				&lt;/p&gt;
			
			
		&lt;/div&gt;
	&lt;/div&gt;

	&lt;p&gt;
		&lt;video class="full-width-layout__hero-video js-responsive-video" loop="loop"&gt;Your browser does not support the video tag.&lt;/video&gt;	&lt;/p&gt;

			&lt;figcaption class="full-width-layout__hero-caption full-width-layout__caption"&gt;
			
							&lt;span class="full-width-layout__caption-credits"&gt;
					Imagery courtesy of Linda Dounia Rebeiz, Jeroen van der Most, aurèce vettier, Entangled Others Studio, fuse*, Noemi Finel.				&lt;/span&gt;
					&lt;/figcaption&gt;
	&lt;/div&gt;

	
	
		&lt;div class="full-width-layout__sections"&gt;
&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;When Paul Mouginot first brought AI into his art practice, he knew he’d tapped an exciting new tool for creative expression. But only through its continued use did he come to appreciate AI’s versatility as an introspective entity — what he calls “a poetic counterpart in the act of creation.”&lt;/p&gt;
&lt;p&gt;“When an AI is trained on deeply personal data, it stops being just a tool,” said Mouginot, who since 2019 has worked as the artistic entity aurèce vettier. “It becomes a reflective device for poetic speculation.”&lt;/p&gt;
&lt;p&gt;The French artist is one of seven exhibitors presenting artwork created with AI at NVIDIA GTC Paris, taking place June 10-12 at VivaTech.&lt;/p&gt;
&lt;p&gt;The art gallery will showcase genre-challenging artists and fashion designers who are redefining what’s possible when human imagination meets machine learning — offering a window into the future of creative expression.&lt;/p&gt;
&lt;p&gt;“This is an opportunity to reach audiences beyond traditional art spaces — white cubes, echo chambers — and to engage in a broader cultural conversation about AI,” said aurèce vettier.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__full-width-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="512" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/AI-Art-Gallery_GTC-Paris_01_Gallery-Render_v2-scaled.png" width="2048" /&gt;	
	
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			Rendering of the AI art gallery at NVIDIA GTC Paris		&lt;/span&gt;
	
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;Exploring ‘the Remembered and the Imagined’ &lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;The works of aurèce vettier imagine speculative vegetal and animal forms (“&lt;i&gt;sur-nature”&lt;/i&gt;) against a visual and spatial context (“&lt;i&gt;sur-reality”&lt;/i&gt;) derived from dreams.&lt;/p&gt;
&lt;p&gt;For the series &lt;i&gt;le travail des rêves &lt;/i&gt;and &lt;i&gt;the light that is not seen&lt;/i&gt;, the artist trained generative AI models on images from his childhood and more recent photos from his phone. The generated images revealed dreamlike scenarios which he then transformed into oil paintings.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="768" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/AI-Art-Gallery_GTC-Paris_02_aurece-vettier-scaled.png" width="2048" /&gt;	
	
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			Oil paintings from the series “le travail des rêves” make a statement “that art made with machines can absolutely carry feeling, intimacy and myth.”		&lt;/span&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			Imagery courtesy of aurèce vettier.		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;“NVIDIA’s technology gives artists the tools to reach new creative horizons,” he said. “I believe these technologies, tailored to artists’ ecosystems and mindful of resources, will help us explore new territories of emotion, memory and identity.”&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="1366" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/AI-Art-Gallery_GTC-Paris_03_aurece-vettier-scaled.jpg" width="2048" /&gt;	
	
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			Painted in oil on canvas from an AI-generated image, “a bearded man crossing a forest surrounded by flocks of yellow butterflies” is from the series “the light that is not seen.”		&lt;/span&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			Imagery courtesy of aurèce vettier.		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;AI as a Memory Machine&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Senegalese artist Linda Dounia Rebeiz’s interest in artificial intelligence was born of a determination to be seen and understood by emerging AI models.&lt;/p&gt;
&lt;p&gt;“I was concerned that people like me — usually at the periphery of how technology is built and disseminated — would be erased from its emerging narratives,” she said.&lt;/p&gt;
&lt;p&gt;In 2021, Rebeiz embarked on a multiyear project, &lt;i&gt;Once Upon a Garden&lt;/i&gt;, to document flowers her grandmother grew up with that no longer exist due to Earth’s rapidly diminishing biodiversity. She soon realized “the way we remember the world is uneven.”&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-video-section"&gt;
	&lt;video class="full-width-layout__video js-responsive-video" loop="loop"&gt;Your browser does not support the video tag.&lt;/video&gt;
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			Built in multiple iterations that reflect AI’s evolution over time, “Once Upon a Garden” is an explorable, speculative archive of critically endangered and extinct flora from the Sahel region of West Africa.		&lt;/span&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			Imagery courtesy of Linda Dounia Rebeiz.		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;“In some places, online archives are thorough, resilient and explorable,” Rebeiz said. “For other places, especially in the Global South, they are not. This obviously affects how those contexts show up in AI training and therefore how well AI models understand them.”&lt;/p&gt;
&lt;p&gt;Intensive groundwork, preparation and tinkering paid off when her first AI model generated 10,000 images, a moment Rebeiz describes as an epiphany.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="792" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/AI-Art-Gallery_GTC-Paris_05_Rebeiz-scaled.png" width="2048" /&gt;	
	
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			AI empowered Rebeiz to scale her art practice, incorporating generated images into collages, animations and physical paintings, including the above result of an artist-led collage workshop.		&lt;/span&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			Imagery courtesy of Linda Dounia Rebeiz.		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;“I felt like I had done something impossible scale-wise,” she said. “AI is one of the most powerful engines for looking in the past and extrapolating based on available information.”&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-video-section"&gt;
	&lt;video class="full-width-layout__video js-responsive-video" loop="loop"&gt;Your browser does not support the video tag.&lt;/video&gt;
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			The fifth iteration or chapter of “Once Upon a Garden” also reflects Rebeiz’s explorations into video generation modules.		&lt;/span&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			Imagery courtesy of Linda Dounia Rebeiz.		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;Entwining the Physical and Digital&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Where Rebeiz’s work aims to catalog and preserve a fleeting past against the encroachment of an uncertain future, Sofia Crespo’s and Feileacan McCormick’s Entangled Others Studio probes the mutability of the present.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Self-Contained&lt;/i&gt; delves into the intricate process of encoding and decoding visual information through AI and the expression of organic information in DNA form.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__dual-image-row-section light"&gt;
				&lt;img alt="alt" class="full-width-layout__dual-image-row-image" height="358" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/Entangled-Others_a.gif" width="636" /&gt;	
			&lt;figcaption class="full-width-layout__dual-image-row-caption full-width-layout__caption"&gt;
							&lt;span class="full-width-layout__caption-text"&gt;
				Harnessing NVIDIA GPUs, the NVIDIA StyleGAN2 model and public and personal datasets, “Self-Contained” transforms complex data into captivating visual narratives.			&lt;/span&gt;
			
							&lt;span class="full-width-layout__caption-credits"&gt;
				Imagery courtesy of Entangled Others Studio.			&lt;/span&gt;
					&lt;/figcaption&gt;
				&lt;img alt="alt" class="full-width-layout__dual-image-row-image" height="358" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/Entangled-Others_b-1.gif" width="636" /&gt;	
			&lt;figcaption class="full-width-layout__dual-image-row-caption full-width-layout__caption"&gt;
			
							&lt;span class="full-width-layout__caption-credits"&gt;
				Imagery courtesy of Entangled Others Studio.			&lt;/span&gt;
					&lt;/figcaption&gt;
	&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Akin to grafting two plants together, images from their dataset are spliced into an image “genome,” then regenerated into a more recognizable form with AI.&lt;/p&gt;
&lt;p&gt;“Just as our cells divide, mutate and grow, so do the contents of our shared digital spaces as we remix, are remixed, hyped and made obsolete,” the studio founders said about the work.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="911" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/AI-Art-Gallery_GTC-Paris_Entangled-Others-scaled.png" width="2048" /&gt;	
	
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			An accompanying work, “self-contained 009.x,” stores a compressed version of the work in DNA form within a capsule fitted into a custom sculptural container.		&lt;/span&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			Imagery courtesy of Entangled Others Studio.		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;Where Art Meets Science&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;The multidisciplinary art studio known as fuse* puts artistic production at the service of science to convey complex concepts to a wider audience.&lt;/p&gt;
&lt;p&gt;Its work &lt;i&gt;Onirica ()&lt;/i&gt; attempts to give shape to dreams through visuals created by a text-to-image diffusion model trained on neurological datasets from the Laboratory of Psychophysiology of Sleep at the University of Bologna.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-video-section"&gt;
	&lt;video class="full-width-layout__video js-responsive-video" loop="loop"&gt;Your browser does not support the video tag.&lt;/video&gt;
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			Drawing upon a dataset of more than 28,000 dreams, “Onirica ()” is a multimedia work created with the help of a text-to-image diffusion model.		&lt;/span&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			Imagery courtesy of fuse*.		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;“You can design a system, feed it your own vision or data, and then be surprised by an outcome that doesn’t merely reflect your intention, but reveals something unexpected,” said&amp;nbsp;Mattia Carretti, co-founder of the studio with Luca Camellini. “It’s precisely this tension between control and unpredictability, between author and system, that we find deeply compelling.”&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;Creativity at the Root of AI &lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Jeroen van der Most conceived &lt;i&gt;Vegetable Vendetta&lt;/i&gt; as an “AI experiment to empower the smallest players in the food sector,” using the latest image-generation technology to prove that tubers and other staple vegetables could assume an air of haute couture.&lt;/p&gt;
&lt;p&gt;“The creation of such campaigns would have been impossible in the past due to budget and manpower requirements,” he said. “Thanks to AI, we now have potato commercials with Prada quality.”&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="585" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/AI-Art-Gallery_GTC-Paris_10_van-der-Most-scaled.png" width="2048" /&gt;	
	
&lt;p&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			Imagery courtesy of Jeroen van der Most.		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;For van der Most, juxtaposing the humble spud with high fashion exposes common tropes and notions in advertising and interrogates human desire at root level.&lt;/p&gt;
&lt;p&gt;“AI showed me all sorts of clichés that are present in our advertising language: the poses, the gestures, the looks of people,” he said. “It’s an incredible mirror of who we are and what we desire.”&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="576" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/AI-Art-Gallery_GTC-Paris_11_van-der-Most-scaled.png" width="2048" /&gt;	
	
&lt;p&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			Imagery courtesy of Jeroen van der Most.		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;Weaving AI Into the Fabric of Fashion Design&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;The GTC Paris exhibition will also provide a platform for innovative installations from leading institutions in the world of fashion and luxury design: Institut Français de la Mode and the Fashion Innovation Agency at the London College of Fashion.&lt;/p&gt;
&lt;p&gt;Both exhibitors will showcase the work of students, offering a window into AI’s role in the future of fashion and fashion campaigns as a tool for conceptual exploration.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="493" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/AI-Art-Gallery_GTC-Paris_12_Noemi-Finel-scaled.png" width="2048" /&gt;	
	
&lt;p&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			Imagery courtesy of Noemi Finel.		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;“It’s about expanding imagination, not following algorithms,” said Giovanna Casimiro, a professor and researcher at Institute Français de la Mode. “We teach students to treat AI as an improvisational partner that brings unpredictability and new possibilities.”&lt;/p&gt;
&lt;p&gt;For Matthew Drinkwater, head of the Fashion Innovation Agency at the London College of Fashion, exhibiting at GTC Paris is an opportunity to radically rethink how fashion operates from ideation to imagery.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="1080" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/AI-Art-Gallery_GTC-Paris_13_Matthew-Drinkwater-FIA_new.gif" width="1920" /&gt;	
	
&lt;p&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			Imagery courtesy of the Fashion Innovation Agency at the London College of Fashion.		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;“With AI, concepts can be tested instantly, assets created on demand and aesthetics refined before a single garment is produced,” he said. “It’s a cultural and technological inflection point.”&lt;/p&gt;
&lt;p&gt;Find the AI Art Gallery at GTC Paris at the Paris Expo Porte de Versailles, on the lower level of Hall 7.1, between the networking and VIP lounges.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Register for GTC Paris&lt;/i&gt;&lt;i&gt; and visit NVIDIA’s &lt;/i&gt;&lt;i&gt; virtual AI Art Gallery&lt;/i&gt;&lt;i&gt;&amp;nbsp;anytime.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
	&lt;div class="full-width-layout__news-section"&gt;
		&lt;p&gt;Related News&lt;/p&gt;

		
	&lt;/div&gt;
		&lt;/div&gt;
	


&lt;!-- #colophon --&gt;

&lt;/div&gt;&lt;!-- #page --&gt;



	            &lt;!-- #has-highlight-and-share --&gt;		&lt;svg class="hidden" height="0" width="0" xmlns="http://www.w3.org/2000/svg"&gt;
			
				&lt;g&gt;&lt;path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z" fill="currentColor"&gt;&lt;/g&gt;
			
			
				&lt;path d="M279.14 288l14.22-92.66h-88.91v-60.13c0-25.35 12.42-50.06 52.24-50.06h40.42V6.26S260.43 0 225.36 0c-73.22 0-121.08 44.38-121.08 124.72v70.62H22.89V288h81.39v224h100.17V288z" fill="currentColor"&gt;
			
			
				&lt;path d="M256 8C118.941 8 8 118.919 8 256c0 137.059 110.919 248 248 248 48.154 0 95.342-14.14 135.408-40.223 12.005-7.815 14.625-24.288 5.552-35.372l-10.177-12.433c-7.671-9.371-21.179-11.667-31.373-5.129C325.92 429.757 291.314 440 256 440c-101.458 0-184-82.542-184-184S154.542 72 256 72c100.139 0 184 57.619 184 160 0 38.786-21.093 79.742-58.17 83.693-17.349-.454-16.91-12.857-13.476-30.024l23.433-121.11C394.653 149.75 383.308 136 368.225 136h-44.981a13.518 13.518 0 0 0-13.432 11.993l-.01.092c-14.697-17.901-40.448-21.775-59.971-21.775-74.58 0-137.831 62.234-137.831 151.46 0 65.303 36.785 105.87 96 105.87 26.984 0 57.369-15.637 74.991-38.333 9.522 34.104 40.613 34.103 70.71 34.103C462.609 379.41 504 307.798 504 232 504 95.653 394.023 8 256 8zm-21.68 304.43c-22.249 0-36.07-15.623-36.07-40.771 0-44.993 30.779-72.729 58.63-72.729 22.292 0 35.601 15.241 35.601 40.77 0 45.061-33.875 72.73-58.161 72.73z" fill="currentColor"&gt;
			
			
				&lt;path d="M100.28 448H7.4V148.9h92.88zM53.79 108.1C24.09 108.1 0 83.5 0 53.8a53.79 53.79 0 0 1 107.58 0c0 29.7-24.1 54.3-53.79 54.3zM447.9 448h-92.68V302.4c0-34.7-.7-79.2-48.29-79.2-48.29 0-55.69 37.7-55.69 76.7V448h-92.78V148.9h89.08v40.8h1.3c12.4-23.5 42.69-48.3 87.88-48.3 94 0 111.28 61.9 111.28 142.3V448z" fill="currentColor"&gt;
			
			
				&lt;path d="M162.7 210c-1.8 3.3-25.2 44.4-70.1 123.5-4.9 8.3-10.8 12.5-17.7 12.5H9.8c-7.7 0-12.1-7.5-8.5-14.4l69-121.3c.2 0 .2-.1 0-.3l-43.9-75.6c-4.3-7.8.3-14.1 8.5-14.1H100c7.3 0 13.3 4.1 18 12.2l44.7 77.5zM382.6 46.1l-144 253v.3L330.2 466c3.9 7.1.2 14.1-8.5 14.1h-65.2c-7.6 0-13.6-4-18-12.2l-92.4-168.5c3.3-5.8 51.5-90.8 144.8-255.2 4.6-8.1 10.4-12.2 17.5-12.2h65.7c8 0 12.3 6.7 8.5 14.1z" fill="currentColor"&gt;
			
			
				&lt;path d="M380.9 97.1C339 55.1 283.2 32 223.9 32c-122.4 0-222 99.6-222 222 0 39.1 10.2 77.3 29.6 111L0 480l117.7-30.9c32.4 17.7 68.9 27 106.1 27h.1c122.3 0 224.1-99.6 224.1-222 0-59.3-25.2-115-67.1-157zm-157 341.6c-33.2 0-65.7-8.9-94-25.7l-6.7-4-69.8 18.3L72 359.2l-4.4-7c-18.5-29.4-28.2-63.3-28.2-98.2 0-101.7 82.8-184.5 184.6-184.5 49.3 0 95.6 19.2 130.4 54.1 34.8 34.9 56.2 81.2 56.1 130.5 0 101.8-84.9 184.6-186.6 184.6zm101.2-138.2c-5.5-2.8-32.8-16.2-37.9-18-5.1-1.9-8.8-2.8-12.5 2.8-3.7 5.6-14.3 18-17.6 21.8-3.2 3.7-6.5 4.2-12 1.4-32.6-16.3-54-29.1-75.5-66-5.7-9.8 5.7-9.1 16.3-30.3 1.8-3.7.9-6.9-.5-9.7-1.4-2.8-12.5-30.1-17.1-41.2-4.5-10.8-9.1-9.3-12.5-9.5-3.2-.2-6.9-.2-10.6-.2-3.7 0-9.7 1.4-14.8 6.9-5.1 5.6-19.4 19-19.4 46.3 0 27.3 19.9 53.7 22.6 57.4 2.8 3.7 39.1 59.7 94.8 83.8 35.2 15.2 49 16.5 66.6 13.9 10.7-1.6 32.8-13.4 37.4-26.4 4.6-13 4.6-24.1 3.2-26.4-1.3-2.5-5-3.9-10.5-6.6z" fill="currentColor"&gt;
			
			
				&lt;path d="M320 448v40c0 13.255-10.745 24-24 24H24c-13.255 0-24-10.745-24-24V120c0-13.255 10.745-24 24-24h72v296c0 30.879 25.121 56 56 56h168zm0-344V0H152c-13.255 0-24 10.745-24 24v368c0 13.255 10.745 24 24 24h272c13.255 0 24-10.745 24-24V128H344c-13.2 0-24-10.8-24-24zm120.971-31.029L375.029 7.029A24 24 0 0 0 358.059 0H352v96h96v-6.059a24 24 0 0 0-7.029-16.97z" fill="currentColor"&gt;
			
			
				&lt;path d="M352 320c-22.608 0-43.387 7.819-59.79 20.895l-102.486-64.054a96.551 96.551 0 0 0 0-41.683l102.486-64.054C308.613 184.181 329.392 192 352 192c53.019 0 96-42.981 96-96S405.019 0 352 0s-96 42.981-96 96c0 7.158.79 14.13 2.276 20.841L155.79 180.895C139.387 167.819 118.608 160 96 160c-53.019 0-96 42.981-96 96s42.981 96 96 96c22.608 0 43.387-7.819 59.79-20.895l102.486 64.054A96.301 96.301 0 0 0 256 416c0 53.019 42.981 96 96 96s96-42.981 96-96-42.981-96-96-96z" fill="currentColor"&gt;
			
			
				&lt;path d="M440.3 203.5c-15 0-28.2 6.2-37.9 15.9-35.7-24.7-83.8-40.6-137.1-42.3L293 52.3l88.2 19.8c0 21.6 17.6 39.2 39.2 39.2 22 0 39.7-18.1 39.7-39.7s-17.6-39.7-39.7-39.7c-15.4 0-28.7 9.3-35.3 22l-97.4-21.6c-4.9-1.3-9.7 2.2-11 7.1L246.3 177c-52.9 2.2-100.5 18.1-136.3 42.8-9.7-10.1-23.4-16.3-38.4-16.3-55.6 0-73.8 74.6-22.9 100.1-1.8 7.9-2.6 16.3-2.6 24.7 0 83.8 94.4 151.7 210.3 151.7 116.4 0 210.8-67.9 210.8-151.7 0-8.4-.9-17.2-3.1-25.1 49.9-25.6 31.5-99.7-23.8-99.7zM129.4 308.9c0-22 17.6-39.7 39.7-39.7 21.6 0 39.2 17.6 39.2 39.7 0 21.6-17.6 39.2-39.2 39.2-22 .1-39.7-17.6-39.7-39.2zm214.3 93.5c-36.4 36.4-139.1 36.4-175.5 0-4-3.5-4-9.7 0-13.7 3.5-3.5 9.7-3.5 13.2 0 27.8 28.5 120 29 149 0 3.5-3.5 9.7-3.5 13.2 0 4.1 4 4.1 10.2.1 13.7zm-.8-54.2c-21.6 0-39.2-17.6-39.2-39.2 0-22 17.6-39.7 39.2-39.7 22 0 39.7 17.6 39.7 39.7-.1 21.5-17.7 39.2-39.7 39.2z" fill="currentColor"&gt;
			
			
				&lt;path d="M446.7 98.6l-67.6 318.8c-5.1 22.5-18.4 28.1-37.3 17.5l-103-75.9-49.7 47.8c-5.5 5.5-10.1 10.1-20.7 10.1l7.4-104.9 190.9-172.5c8.3-7.4-1.8-11.5-12.9-4.1L117.8 284 16.2 252.2c-22.1-6.9-22.5-22.1 4.6-32.7L418.2 66.4c18.4-6.9 34.5 4.1 28.5 32.2z" fill="currentColor"&gt;
			
			
				&lt;g&gt;
					&lt;path d="M97.2800192,3.739673 L100.160021,15.3787704 C88.8306631,18.1647705 77.9879854,22.6484879 68.0000023,28.6777391 L61.8399988,18.3985363 C72.8467373,11.7537029 84.7951803,6.81153332 97.2800192,3.739673 Z M158.720055,3.739673 L155.840053,15.3787704 C167.169411,18.1647705 178.012089,22.6484879 188.000072,28.6777391 L194.200075,18.3985363 C183.180932,11.7499974 171.218739,6.80771878 158.720055,3.739673 L158.720055,3.739673 Z M18.3999736,61.8351679 C11.7546212,72.8410466 6.81206547,84.7885562 3.73996516,97.2724198 L15.3799719,100.152197 C18.1661896,88.8237238 22.6502573,77.981893 28.6799796,67.9946902 L18.3999736,61.8351679 Z M11.9999699,127.990038 C11.9961044,122.172725 12.4306685,116.363392 13.2999707,110.611385 L1.43996383,108.811525 C-0.479938607,121.525138 -0.479938607,134.454937 1.43996383,147.168551 L13.2999707,145.36869 C12.4306685,139.616684 11.9961044,133.807351 11.9999699,127.990038 L11.9999699,127.990038 Z M194.160075,237.581539 L188.000072,227.302336 C178.024494,233.327885 167.195565,237.811494 155.880053,240.601305 L158.760055,252.240403 C171.231048,249.164732 183.165742,244.222671 194.160075,237.581539 L194.160075,237.581539 Z M244.000104,127.990038 C244.00397,133.807351 243.569406,139.616684 242.700103,145.36869 L254.56011,147.168551 C256.480013,134.454937 256.480013,121.525138 254.56011,108.811525 L242.700103,110.611385 C243.569406,116.363392 244.00397,122.172725 244.000104,127.990038 Z M252.260109,158.707656 L240.620102,155.827879 C237.833884,167.156352 233.349817,177.998183 227.320094,187.985385 L237.6001,194.184905 C244.249159,183.166622 249.191823,171.205364 252.260109,158.707656 L252.260109,158.707656 Z M145.380047,242.701142 C133.858209,244.43447 122.141865,244.43447 110.620027,242.701142 L108.820026,254.560223 C121.534632,256.479975 134.465442,256.479975 147.180048,254.560223 L145.380047,242.701142 Z M221.380091,196.804701 C214.461479,206.174141 206.175877,214.452354 196.800077,221.362797 L203.920081,231.022048 C214.262958,223.418011 223.404944,214.303705 231.040097,203.984145 L221.380091,196.804701 Z M196.800077,34.6172785 C206.177345,41.5338058 214.463023,49.8188367 221.380091,59.1953726 L231.040097,51.9959309 C223.429284,41.6822474 214.31457,32.5682452 204.000081,24.9580276 L196.800077,34.6172785 Z M34.619983,59.1953726 C41.5370506,49.8188367 49.8227288,41.5338058 59.1999972,34.6172785 L51.9999931,24.9580276 C41.6855038,32.5682452 32.5707896,41.6822474 24.9599774,51.9959309 L34.619983,59.1953726 Z M237.6001,61.8351679 L227.320094,67.9946902 C233.346114,77.969489 237.830073,88.7975718 240.620102,100.1122 L252.260109,97.2324229 C249.184198,84.7624043 244.241751,72.8286423 237.6001,61.8351679 L237.6001,61.8351679 Z M110.620027,13.2989317 C122.141865,11.5656035 133.858209,11.5656035 145.380047,13.2989317 L147.180048,1.43985134 C134.465442,-0.479901112 121.534632,-0.479901112 108.820026,1.43985134 L110.620027,13.2989317 Z M40.7799866,234.201801 L15.9999722,239.981353 L21.7799756,215.203275 L10.0999688,212.463487 L4.3199655,237.241566 C3.3734444,241.28318 4.58320332,245.526897 7.51859925,248.462064 C10.4539952,251.39723 14.6980441,252.606895 18.7399738,251.660448 L43.4999881,245.980888 L40.7799866,234.201801 Z M12.5999703,201.764317 L24.279977,204.484106 L28.2799793,187.305438 C22.4496684,177.507146 18.1025197,166.899584 15.3799719,155.827879 L3.73996516,158.707656 C6.34937618,169.311891 10.3154147,179.535405 15.539972,189.125297 L12.5999703,201.764317 Z M68.6000027,227.762301 L51.4199927,231.761991 L54.1399943,243.441085 L66.7800016,240.501313 C76.3706428,245.725462 86.5949557,249.691191 97.2000192,252.300398 L100.080021,240.6613 C89.0307035,237.906432 78.4495684,233.532789 68.6800027,227.682307 L68.6000027,227.762301 Z M128.000037,23.9980665 C90.1565244,24.0177003 55.3105242,44.590631 37.01511,77.715217 C18.7196958,110.839803 19.8628631,151.287212 39.9999861,183.325747 L29.9999803,225.982439 L72.660005,215.983214 C110.077932,239.548522 158.307237,236.876754 192.892851,209.322653 C227.478464,181.768552 240.856271,135.358391 226.242944,93.6248278 C211.629616,51.8912646 172.221191,23.9617202 128.000037,23.9980665 Z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			
				&lt;g&gt;
					&lt;path d="M357.1,324.5c-24.1,15.3-57.2,21.4-79.1,23.6l18.4,18.1l67,67c24.5,25.1-15.4,64.4-40.2,40.2c-16.8-17-41.4-41.6-67-67.3
						l-67,67.2c-24.8,24.2-64.7-15.5-39.9-40.2c17-17,41.4-41.6,67-67l18.1-18.1c-21.6-2.3-55.3-8-79.6-23.6
						c-28.6-18.5-41.2-29.3-30.1-51.8c6.5-12.8,24.3-23.6,48-5c0,0,31.9,25.4,83.4,25.4s83.4-25.4,83.4-25.4c23.6-18.5,41.4-7.8,48,5
						C398.3,295.1,385.7,305.9,357.1,324.5L357.1,324.5z M142,145c0-63,51.2-114,114-114s114,51,114,114c0,62.7-51.2,113.7-114,113.7
						S142,207.7,142,145L142,145z M200,145c0,30.8,25.1,56,56,56s56-25.1,56-56c0-31.1-25.1-56.2-56-56.2S200,113.9,200,145z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			
				&lt;g transform="translate(0,664)"&gt;
					&lt;path d="m 1073.3513,-606.40537 h 196.278 c 179.2103,0 221.8795,42.66915 221.8795,221.8795 v 196.27799 c 0,179.2103512 -42.6692,221.879451 -221.8795,221.879451 h -196.278 c -179.21038,0 -221.87951,-42.6691298 -221.87951,-221.879451 v -196.27801 c 0,-179.21035 42.66913,-221.87946 221.87951,-221.87948 z" fill="currentColor"&gt;
					&lt;path d="m 1375.0576,-393.98425 c 2.9513,-9.7072 0,-16.85429 -14.1342,-16.85429 h -46.6693 c -11.8763,0 -17.3521,6.16927 -20.3212,12.97854 0,0 -23.7347,56.82106 -57.3544,93.74763 -10.8806,10.66728 -15.8232,14.08081 -21.7613,14.08081 -2.969,0 -7.2715,-3.39577 -7.2715,-13.12075 v -90.83194 c 0,-11.66288 -3.4491,-16.85429 -13.3341,-16.85429 h -73.3553 c -7.4138,0 -11.8763,5.40476 -11.8763,10.54286 0,11.0406 16.8188,13.60078 18.5433,44.67814 v 67.52388 c 0,14.80973 -2.7202,17.49433 -8.6583,17.49433 -15.8231,0 -54.3143,-57.08773 -77.16,-122.40705 -4.4447,-12.71185 -8.9427,-17.83214 -20.8723,-17.83214 h -46.68718 c -13.3341,0 -16.0009,6.16925 -16.0009,12.97852 0,12.12515 15.8232,72.35973 73.69318,152.02656 38.58,54.40315 92.8942,83.89819 142.3726,83.89819 29.6729,0 33.3353,-6.54262 33.3353,-17.83216 v -41.12238 c 0,-13.10297 2.809,-15.71646 12.214,-15.71646 6.9338,0 18.7922,3.41353 46.4916,29.63728 31.6463,31.09512 36.8555,45.03372 54.6698,45.03372 h 46.6694 c 13.3341,0 20.0189,-6.54262 16.1787,-19.46781 -4.2313,-12.88962 -19.3433,-31.57515 -39.38,-53.74532 -10.8807,-12.62294 -27.2016,-26.22375 -32.1441,-33.03302 -6.9338,-8.72941 -4.9603,-12.62294 0,-20.39227 0,0 56.8566,-78.68897 62.7947,-105.41058 z" fill="currentColor"&gt;
					&lt;path d="m 567.69877,-429.06912 c 3.15618,-10.38133 0,-18.0247 -15.11579,-18.0247 h -49.91013 c -12.70096,0 -18.55706,6.59763 -21.73232,13.87977 0,0 -25.38286,60.76685 -61.33724,100.25768 -11.63627,11.40806 -16.92197,15.05863 -23.27242,15.05863 -3.17519,0 -7.77644,-3.63156 -7.77644,-14.0319 v -97.13948 c 0,-12.47278 -3.68869,-18.0247 -14.26014,-18.0247 h -78.44923 c -7.92857,0 -12.70097,5.78005 -12.70097,11.27491 0,11.80736 17.98666,14.54527 19.83094,47.78071 v 72.21293 c 0,15.83815 -2.9091,18.70918 -9.25948,18.70918 -16.92197,0 -58.08598,-61.05206 -82.51817,-130.90731 -4.75337,-13.59458 -9.56381,-19.07042 -22.32175,-19.07042 h -49.92915 c -14.26014,0 -17.11213,6.59763 -17.11213,13.87977 0,12.96714 16.92197,77.38454 78.81059,162.58363 41.25909,58.18101 99.34506,89.72424 152.25931,89.72424 31.73343,0 35.65018,-6.99691 35.65018,-19.07043 v -43.978 c 0,-14.01288 3.00405,-16.80786 13.0622,-16.80786 7.41521,0 20.09716,3.65057 49.71998,31.69536 33.84387,33.25443 39.41486,48.16093 58.46622,48.16093 h 49.91026 c 14.26,0 21.40913,-6.99691 17.30216,-20.81966 -4.5252,-13.78473 -20.68653,-33.76783 -42.11468,-57.47752 -11.63621,-13.49953 -29.09043,-28.04479 -34.37631,-35.32694 -7.41508,-9.33557 -5.30458,-13.4995 0,-21.80835 0,0 60.80491,-84.15334 67.15549,-112.73048 z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			&lt;path d="M309.8 480.3c-13.6 14.5-50 31.7-97.4 31.7-120.8 0-147-88.8-147-140.6v-144H17.9c-5.5 0-10-4.5-10-10v-68c0-7.2 4.5-13.6 11.3-16 62-21.8 81.5-76 84.3-117.1.8-11 6.5-16.3 16.1-16.3h70.9c5.5 0 10 4.5 10 10v115.2h83c5.5 0 10 4.4 10 9.9v81.7c0 5.5-4.5 10-10 10h-83.4V360c0 34.2 23.7 53.6 68 35.8 4.8-1.9 9-3.2 12.7-2.2 3.5.9 5.8 3.4 7.4 7.9l22 64.3c1.8 5 3.3 10.6-.4 14.5z" fill="currentColor"&gt;
			&lt;path d="M512 208L320 384H288V288H208c-61.9 0-112 50.1-112 112c0 48 32 80 32 80s-128-48-128-176c0-97.2 78.8-176 176-176H288V32h32L512 208z" fill="currentColor"&gt;
			&lt;path d="M309.8 480.3c-13.6 14.5-50 31.7-97.4 31.7-120.8 0-147-88.8-147-140.6v-144H17.9c-5.5 0-10-4.5-10-10v-68c0-7.2 4.5-13.6 11.3-16 62-21.8 81.5-76 84.3-117.1.8-11 6.5-16.3 16.1-16.3h70.9c5.5 0 10 4.5 10 10v115.2h83c5.5 0 10 4.4 10 9.9v81.7c0 5.5-4.5 10-10 10h-83.4V360c0 34.2 23.7 53.6 68 35.8 4.8-1.9 9-3.2 12.7-2.2 3.5.9 5.8 3.4 7.4 7.9l22 64.3c1.8 5 3.3 10.6-.4 14.5z" fill="currentColor"&gt;
			&lt;path d="M433 179.1c0-97.2-63.7-125.7-63.7-125.7-62.5-28.7-228.6-28.4-290.5 0 0 0-63.7 28.5-63.7 125.7 0 115.7-6.6 259.4 105.6 289.1 40.5 10.7 75.3 13 103.3 11.4 50.8-2.8 79.3-18.1 79.3-18.1l-1.7-36.9s-36.3 11.4-77.1 10.1c-40.4-1.4-83-4.4-89.6-54a102.5 102.5 0 0 1 -.9-13.9c85.6 20.9 158.7 9.1 178.8 6.7 56.1-6.7 105-41.3 111.2-72.9 9.8-49.8 9-121.5 9-121.5zm-75.1 125.2h-46.6v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.3V197c0-58.5-64-56.6-64-6.9v114.2H90.2c0-122.1-5.2-147.9 18.4-175 25.9-28.9 79.8-30.8 103.8 6.1l11.6 19.5 11.6-19.5c24.1-37.1 78.1-34.8 103.8-6.1 23.7 27.3 18.4 53 18.4 175z" fill="currentColor"&gt;
			
				&lt;path d="M331.5 235.7c2.2 .9 4.2 1.9 6.3 2.8c29.2 14.1 50.6 35.2 61.8 61.4c15.7 36.5 17.2 95.8-30.3 143.2c-36.2 36.2-80.3 52.5-142.6 53h-.3c-70.2-.5-124.1-24.1-160.4-70.2c-32.3-41-48.9-98.1-49.5-169.6V256v-.2C17 184.3 33.6 127.2 65.9 86.2C102.2 40.1 156.2 16.5 226.4 16h.3c70.3 .5 124.9 24 162.3 69.9c18.4 22.7 32 50 40.6 81.7l-40.4 10.8c-7.1-25.8-17.8-47.8-32.2-65.4c-29.2-35.8-73-54.2-130.5-54.6c-57 .5-100.1 18.8-128.2 54.4C72.1 146.1 58.5 194.3 58 256c.5 61.7 14.1 109.9 40.3 143.3c28 35.6 71.2 53.9 128.2 54.4c51.4-.4 85.4-12.6 113.7-40.9c32.3-32.2 31.7-71.8 21.4-95.9c-6.1-14.2-17.1-26-31.9-34.9c-3.7 26.9-11.8 48.3-24.7 64.8c-17.1 21.8-41.4 33.6-72.7 35.3c-23.6 1.3-46.3-4.4-63.9-16c-20.8-13.8-33-34.8-34.3-59.3c-2.5-48.3 35.7-83 95.2-86.4c21.1-1.2 40.9-.3 59.2 2.8c-2.4-14.8-7.3-26.6-14.6-35.2c-10-11.7-25.6-17.7-46.2-17.8H227c-16.6 0-39 4.6-53.3 26.3l-34.4-23.6c19.2-29.1 50.3-45.1 87.8-45.1h.8c62.6 .4 99.9 39.5 103.7 107.7l-.2 .2zm-156 68.8c1.3 25.1 28.4 36.8 54.6 35.3c25.6-1.4 54.6-11.4 59.5-73.2c-13.2-2.9-27.8-4.4-43.4-4.4c-4.8 0-9.6 .1-14.4 .4c-42.9 2.4-57.2 23.2-56.2 41.8l-.1 .1z" fill="currentColor"&gt;
			
			
				&lt;path d="M407.8 294.7c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3zM288 227.1C261.9 176.4 190.9 81.9 124.9 35.3C61.6-9.4 37.5-1.7 21.6 5.5C3.3 13.8 0 41.9 0 58.4S9.1 194 15 213.9c19.5 65.7 89.1 87.9 153.2 80.7c3.3-.5 6.6-.9 10-1.4c-3.3 .5-6.6 1-10 1.4C74.3 308.6-9.1 342.8 100.3 464.5C220.6 589.1 265.1 437.8 288 361.1c22.9 76.7 49.2 222.5 185.6 103.4c102.4-103.4 28.1-156-65.8-169.9c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3c64.1 7.1 133.6-15.1 153.2-80.7C566.9 194 576 75 576 58.4s-3.3-44.7-21.6-52.9c-15.8-7.1-40-14.9-103.2 29.8C385.1 81.9 314.1 176.4 288 227.1z" fill="currentColor"&gt;
			
			
				&lt;path d="M204 6.5C101.4 6.5 0 74.9 0 185.6 0 256 39.6 296 63.6 296c9.9 0 15.6-27.6 15.6-35.4 0-9.3-23.7-29.1-23.7-67.8 0-80.4 61.2-137.4 140.4-137.4 68.1 0 118.5 38.7 118.5 109.8 0 53.1-21.3 152.7-90.3 152.7-24.9 0-46.2-18-46.2-43.8 0-37.8 26.4-74.4 26.4-113.4 0-66.2-93.9-54.2-93.9 25.8 0 16.8 2.1 35.4 9.6 50.7-13.8 59.4-42 147.9-42 209.1 0 18.9 2.7 37.5 4.5 56.4 3.4 3.8 1.7 3.4 6.9 1.5 50.4-69 48.6-82.5 71.4-172.8 12.3 23.4 44.1 36 69.3 36 106.2 0 153.9-103.5 153.9-196.8C384 71.3 298.2 6.5 204 6.5z" fill="currentColor"&gt;
			
		&lt;/svg&gt;
		&lt;div id="has-mastodon-prompt"&gt;
			&lt;h3&gt;Share on Mastodon&lt;/h3&gt;
			
		&lt;/div&gt;</description><content:encoded>&lt;!-- OneTrust Cookies Consent Notice start for nvidia.com --&gt;


&lt;!-- OneTrust Cookies Consent Notice end for nvidia.com --&gt;


	
	
	
	
	
	

	

	
	
	


	&lt;!-- This site is optimized with the Yoast SEO Premium plugin v25.2 (Yoast SEO v25.2) - https://yoast.com/wordpress/plugins/seo/ --&gt;
	Artists, Designers Tap AI for NVIDIA GTC Paris Gallery | NVIDIA Blog
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	&lt;!-- / Yoast SEO Premium plugin. --&gt;







































&lt;!-- Stream WordPress user activity plugin v4.1.1 --&gt;


	
				&lt;!-- Hotjar Tracking Code for NVIDIA --&gt;
			
			

				
				



		
		
&lt;div class="hfeed site" id="page"&gt;
	Skip to content

	&lt;!-- #masthead --&gt;
		
		&lt;div class="full-width-layout light"&gt;
		

		
&lt;div class="full-width-layout__hero light"&gt;
	&lt;div class="full-width-layout__hero-content light"&gt;
		&lt;div class="full-width-layout__hero-content__inner light"&gt;
			

							&lt;p&gt;
					The conference, taking place in one of Europe’s iconic art capitals, will feature a curated gallery that showcases how AI helps bring creative visions to life. 				&lt;/p&gt;
			
			
		&lt;/div&gt;
	&lt;/div&gt;

	&lt;p&gt;
		&lt;video class="full-width-layout__hero-video js-responsive-video" loop="loop"&gt;Your browser does not support the video tag.&lt;/video&gt;	&lt;/p&gt;

			&lt;figcaption class="full-width-layout__hero-caption full-width-layout__caption"&gt;
			
							&lt;span class="full-width-layout__caption-credits"&gt;
					Imagery courtesy of Linda Dounia Rebeiz, Jeroen van der Most, aurèce vettier, Entangled Others Studio, fuse*, Noemi Finel.				&lt;/span&gt;
					&lt;/figcaption&gt;
	&lt;/div&gt;

	
	
		&lt;div class="full-width-layout__sections"&gt;
&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;When Paul Mouginot first brought AI into his art practice, he knew he’d tapped an exciting new tool for creative expression. But only through its continued use did he come to appreciate AI’s versatility as an introspective entity — what he calls “a poetic counterpart in the act of creation.”&lt;/p&gt;
&lt;p&gt;“When an AI is trained on deeply personal data, it stops being just a tool,” said Mouginot, who since 2019 has worked as the artistic entity aurèce vettier. “It becomes a reflective device for poetic speculation.”&lt;/p&gt;
&lt;p&gt;The French artist is one of seven exhibitors presenting artwork created with AI at NVIDIA GTC Paris, taking place June 10-12 at VivaTech.&lt;/p&gt;
&lt;p&gt;The art gallery will showcase genre-challenging artists and fashion designers who are redefining what’s possible when human imagination meets machine learning — offering a window into the future of creative expression.&lt;/p&gt;
&lt;p&gt;“This is an opportunity to reach audiences beyond traditional art spaces — white cubes, echo chambers — and to engage in a broader cultural conversation about AI,” said aurèce vettier.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__full-width-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="512" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/AI-Art-Gallery_GTC-Paris_01_Gallery-Render_v2-scaled.png" width="2048" /&gt;	
	
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			Rendering of the AI art gallery at NVIDIA GTC Paris		&lt;/span&gt;
	
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;Exploring ‘the Remembered and the Imagined’ &lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;The works of aurèce vettier imagine speculative vegetal and animal forms (“&lt;i&gt;sur-nature”&lt;/i&gt;) against a visual and spatial context (“&lt;i&gt;sur-reality”&lt;/i&gt;) derived from dreams.&lt;/p&gt;
&lt;p&gt;For the series &lt;i&gt;le travail des rêves &lt;/i&gt;and &lt;i&gt;the light that is not seen&lt;/i&gt;, the artist trained generative AI models on images from his childhood and more recent photos from his phone. The generated images revealed dreamlike scenarios which he then transformed into oil paintings.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="768" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/AI-Art-Gallery_GTC-Paris_02_aurece-vettier-scaled.png" width="2048" /&gt;	
	
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			Oil paintings from the series “le travail des rêves” make a statement “that art made with machines can absolutely carry feeling, intimacy and myth.”		&lt;/span&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			Imagery courtesy of aurèce vettier.		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;“NVIDIA’s technology gives artists the tools to reach new creative horizons,” he said. “I believe these technologies, tailored to artists’ ecosystems and mindful of resources, will help us explore new territories of emotion, memory and identity.”&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="1366" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/AI-Art-Gallery_GTC-Paris_03_aurece-vettier-scaled.jpg" width="2048" /&gt;	
	
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			Painted in oil on canvas from an AI-generated image, “a bearded man crossing a forest surrounded by flocks of yellow butterflies” is from the series “the light that is not seen.”		&lt;/span&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			Imagery courtesy of aurèce vettier.		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;AI as a Memory Machine&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Senegalese artist Linda Dounia Rebeiz’s interest in artificial intelligence was born of a determination to be seen and understood by emerging AI models.&lt;/p&gt;
&lt;p&gt;“I was concerned that people like me — usually at the periphery of how technology is built and disseminated — would be erased from its emerging narratives,” she said.&lt;/p&gt;
&lt;p&gt;In 2021, Rebeiz embarked on a multiyear project, &lt;i&gt;Once Upon a Garden&lt;/i&gt;, to document flowers her grandmother grew up with that no longer exist due to Earth’s rapidly diminishing biodiversity. She soon realized “the way we remember the world is uneven.”&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-video-section"&gt;
	&lt;video class="full-width-layout__video js-responsive-video" loop="loop"&gt;Your browser does not support the video tag.&lt;/video&gt;
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			Built in multiple iterations that reflect AI’s evolution over time, “Once Upon a Garden” is an explorable, speculative archive of critically endangered and extinct flora from the Sahel region of West Africa.		&lt;/span&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			Imagery courtesy of Linda Dounia Rebeiz.		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;“In some places, online archives are thorough, resilient and explorable,” Rebeiz said. “For other places, especially in the Global South, they are not. This obviously affects how those contexts show up in AI training and therefore how well AI models understand them.”&lt;/p&gt;
&lt;p&gt;Intensive groundwork, preparation and tinkering paid off when her first AI model generated 10,000 images, a moment Rebeiz describes as an epiphany.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="792" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/AI-Art-Gallery_GTC-Paris_05_Rebeiz-scaled.png" width="2048" /&gt;	
	
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			AI empowered Rebeiz to scale her art practice, incorporating generated images into collages, animations and physical paintings, including the above result of an artist-led collage workshop.		&lt;/span&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			Imagery courtesy of Linda Dounia Rebeiz.		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;“I felt like I had done something impossible scale-wise,” she said. “AI is one of the most powerful engines for looking in the past and extrapolating based on available information.”&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-video-section"&gt;
	&lt;video class="full-width-layout__video js-responsive-video" loop="loop"&gt;Your browser does not support the video tag.&lt;/video&gt;
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			The fifth iteration or chapter of “Once Upon a Garden” also reflects Rebeiz’s explorations into video generation modules.		&lt;/span&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			Imagery courtesy of Linda Dounia Rebeiz.		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;Entwining the Physical and Digital&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Where Rebeiz’s work aims to catalog and preserve a fleeting past against the encroachment of an uncertain future, Sofia Crespo’s and Feileacan McCormick’s Entangled Others Studio probes the mutability of the present.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Self-Contained&lt;/i&gt; delves into the intricate process of encoding and decoding visual information through AI and the expression of organic information in DNA form.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__dual-image-row-section light"&gt;
				&lt;img alt="alt" class="full-width-layout__dual-image-row-image" height="358" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/Entangled-Others_a.gif" width="636" /&gt;	
			&lt;figcaption class="full-width-layout__dual-image-row-caption full-width-layout__caption"&gt;
							&lt;span class="full-width-layout__caption-text"&gt;
				Harnessing NVIDIA GPUs, the NVIDIA StyleGAN2 model and public and personal datasets, “Self-Contained” transforms complex data into captivating visual narratives.			&lt;/span&gt;
			
							&lt;span class="full-width-layout__caption-credits"&gt;
				Imagery courtesy of Entangled Others Studio.			&lt;/span&gt;
					&lt;/figcaption&gt;
				&lt;img alt="alt" class="full-width-layout__dual-image-row-image" height="358" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/Entangled-Others_b-1.gif" width="636" /&gt;	
			&lt;figcaption class="full-width-layout__dual-image-row-caption full-width-layout__caption"&gt;
			
							&lt;span class="full-width-layout__caption-credits"&gt;
				Imagery courtesy of Entangled Others Studio.			&lt;/span&gt;
					&lt;/figcaption&gt;
	&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Akin to grafting two plants together, images from their dataset are spliced into an image “genome,” then regenerated into a more recognizable form with AI.&lt;/p&gt;
&lt;p&gt;“Just as our cells divide, mutate and grow, so do the contents of our shared digital spaces as we remix, are remixed, hyped and made obsolete,” the studio founders said about the work.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="911" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/AI-Art-Gallery_GTC-Paris_Entangled-Others-scaled.png" width="2048" /&gt;	
	
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			An accompanying work, “self-contained 009.x,” stores a compressed version of the work in DNA form within a capsule fitted into a custom sculptural container.		&lt;/span&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			Imagery courtesy of Entangled Others Studio.		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;Where Art Meets Science&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;The multidisciplinary art studio known as fuse* puts artistic production at the service of science to convey complex concepts to a wider audience.&lt;/p&gt;
&lt;p&gt;Its work &lt;i&gt;Onirica ()&lt;/i&gt; attempts to give shape to dreams through visuals created by a text-to-image diffusion model trained on neurological datasets from the Laboratory of Psychophysiology of Sleep at the University of Bologna.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-video-section"&gt;
	&lt;video class="full-width-layout__video js-responsive-video" loop="loop"&gt;Your browser does not support the video tag.&lt;/video&gt;
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			Drawing upon a dataset of more than 28,000 dreams, “Onirica ()” is a multimedia work created with the help of a text-to-image diffusion model.		&lt;/span&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			Imagery courtesy of fuse*.		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;“You can design a system, feed it your own vision or data, and then be surprised by an outcome that doesn’t merely reflect your intention, but reveals something unexpected,” said&amp;nbsp;Mattia Carretti, co-founder of the studio with Luca Camellini. “It’s precisely this tension between control and unpredictability, between author and system, that we find deeply compelling.”&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;Creativity at the Root of AI &lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Jeroen van der Most conceived &lt;i&gt;Vegetable Vendetta&lt;/i&gt; as an “AI experiment to empower the smallest players in the food sector,” using the latest image-generation technology to prove that tubers and other staple vegetables could assume an air of haute couture.&lt;/p&gt;
&lt;p&gt;“The creation of such campaigns would have been impossible in the past due to budget and manpower requirements,” he said. “Thanks to AI, we now have potato commercials with Prada quality.”&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="585" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/AI-Art-Gallery_GTC-Paris_10_van-der-Most-scaled.png" width="2048" /&gt;	
	
&lt;p&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			Imagery courtesy of Jeroen van der Most.		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;For van der Most, juxtaposing the humble spud with high fashion exposes common tropes and notions in advertising and interrogates human desire at root level.&lt;/p&gt;
&lt;p&gt;“AI showed me all sorts of clichés that are present in our advertising language: the poses, the gestures, the looks of people,” he said. “It’s an incredible mirror of who we are and what we desire.”&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="576" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/AI-Art-Gallery_GTC-Paris_11_van-der-Most-scaled.png" width="2048" /&gt;	
	
&lt;p&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			Imagery courtesy of Jeroen van der Most.		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;Weaving AI Into the Fabric of Fashion Design&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;The GTC Paris exhibition will also provide a platform for innovative installations from leading institutions in the world of fashion and luxury design: Institut Français de la Mode and the Fashion Innovation Agency at the London College of Fashion.&lt;/p&gt;
&lt;p&gt;Both exhibitors will showcase the work of students, offering a window into AI’s role in the future of fashion and fashion campaigns as a tool for conceptual exploration.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="493" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/AI-Art-Gallery_GTC-Paris_12_Noemi-Finel-scaled.png" width="2048" /&gt;	
	
&lt;p&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			Imagery courtesy of Noemi Finel.		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;“It’s about expanding imagination, not following algorithms,” said Giovanna Casimiro, a professor and researcher at Institute Français de la Mode. “We teach students to treat AI as an improvisational partner that brings unpredictability and new possibilities.”&lt;/p&gt;
&lt;p&gt;For Matthew Drinkwater, head of the Fashion Innovation Agency at the London College of Fashion, exhibiting at GTC Paris is an opportunity to radically rethink how fashion operates from ideation to imagery.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="1080" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/AI-Art-Gallery_GTC-Paris_13_Matthew-Drinkwater-FIA_new.gif" width="1920" /&gt;	
	
&lt;p&gt;
	
			&lt;span class="full-width-layout__media-credits"&gt;
			Imagery courtesy of the Fashion Innovation Agency at the London College of Fashion.		&lt;/span&gt;
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;“With AI, concepts can be tested instantly, assets created on demand and aesthetics refined before a single garment is produced,” he said. “It’s a cultural and technological inflection point.”&lt;/p&gt;
&lt;p&gt;Find the AI Art Gallery at GTC Paris at the Paris Expo Porte de Versailles, on the lower level of Hall 7.1, between the networking and VIP lounges.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Register for GTC Paris&lt;/i&gt;&lt;i&gt; and visit NVIDIA’s &lt;/i&gt;&lt;i&gt; virtual AI Art Gallery&lt;/i&gt;&lt;i&gt;&amp;nbsp;anytime.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
	&lt;div class="full-width-layout__news-section"&gt;
		&lt;p&gt;Related News&lt;/p&gt;

		
	&lt;/div&gt;
		&lt;/div&gt;
	


&lt;!-- #colophon --&gt;

&lt;/div&gt;&lt;!-- #page --&gt;



	            &lt;!-- #has-highlight-and-share --&gt;		&lt;svg class="hidden" height="0" width="0" xmlns="http://www.w3.org/2000/svg"&gt;
			
				&lt;g&gt;&lt;path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z" fill="currentColor"&gt;&lt;/g&gt;
			
			
				&lt;path d="M279.14 288l14.22-92.66h-88.91v-60.13c0-25.35 12.42-50.06 52.24-50.06h40.42V6.26S260.43 0 225.36 0c-73.22 0-121.08 44.38-121.08 124.72v70.62H22.89V288h81.39v224h100.17V288z" fill="currentColor"&gt;
			
			
				&lt;path d="M256 8C118.941 8 8 118.919 8 256c0 137.059 110.919 248 248 248 48.154 0 95.342-14.14 135.408-40.223 12.005-7.815 14.625-24.288 5.552-35.372l-10.177-12.433c-7.671-9.371-21.179-11.667-31.373-5.129C325.92 429.757 291.314 440 256 440c-101.458 0-184-82.542-184-184S154.542 72 256 72c100.139 0 184 57.619 184 160 0 38.786-21.093 79.742-58.17 83.693-17.349-.454-16.91-12.857-13.476-30.024l23.433-121.11C394.653 149.75 383.308 136 368.225 136h-44.981a13.518 13.518 0 0 0-13.432 11.993l-.01.092c-14.697-17.901-40.448-21.775-59.971-21.775-74.58 0-137.831 62.234-137.831 151.46 0 65.303 36.785 105.87 96 105.87 26.984 0 57.369-15.637 74.991-38.333 9.522 34.104 40.613 34.103 70.71 34.103C462.609 379.41 504 307.798 504 232 504 95.653 394.023 8 256 8zm-21.68 304.43c-22.249 0-36.07-15.623-36.07-40.771 0-44.993 30.779-72.729 58.63-72.729 22.292 0 35.601 15.241 35.601 40.77 0 45.061-33.875 72.73-58.161 72.73z" fill="currentColor"&gt;
			
			
				&lt;path d="M100.28 448H7.4V148.9h92.88zM53.79 108.1C24.09 108.1 0 83.5 0 53.8a53.79 53.79 0 0 1 107.58 0c0 29.7-24.1 54.3-53.79 54.3zM447.9 448h-92.68V302.4c0-34.7-.7-79.2-48.29-79.2-48.29 0-55.69 37.7-55.69 76.7V448h-92.78V148.9h89.08v40.8h1.3c12.4-23.5 42.69-48.3 87.88-48.3 94 0 111.28 61.9 111.28 142.3V448z" fill="currentColor"&gt;
			
			
				&lt;path d="M162.7 210c-1.8 3.3-25.2 44.4-70.1 123.5-4.9 8.3-10.8 12.5-17.7 12.5H9.8c-7.7 0-12.1-7.5-8.5-14.4l69-121.3c.2 0 .2-.1 0-.3l-43.9-75.6c-4.3-7.8.3-14.1 8.5-14.1H100c7.3 0 13.3 4.1 18 12.2l44.7 77.5zM382.6 46.1l-144 253v.3L330.2 466c3.9 7.1.2 14.1-8.5 14.1h-65.2c-7.6 0-13.6-4-18-12.2l-92.4-168.5c3.3-5.8 51.5-90.8 144.8-255.2 4.6-8.1 10.4-12.2 17.5-12.2h65.7c8 0 12.3 6.7 8.5 14.1z" fill="currentColor"&gt;
			
			
				&lt;path d="M380.9 97.1C339 55.1 283.2 32 223.9 32c-122.4 0-222 99.6-222 222 0 39.1 10.2 77.3 29.6 111L0 480l117.7-30.9c32.4 17.7 68.9 27 106.1 27h.1c122.3 0 224.1-99.6 224.1-222 0-59.3-25.2-115-67.1-157zm-157 341.6c-33.2 0-65.7-8.9-94-25.7l-6.7-4-69.8 18.3L72 359.2l-4.4-7c-18.5-29.4-28.2-63.3-28.2-98.2 0-101.7 82.8-184.5 184.6-184.5 49.3 0 95.6 19.2 130.4 54.1 34.8 34.9 56.2 81.2 56.1 130.5 0 101.8-84.9 184.6-186.6 184.6zm101.2-138.2c-5.5-2.8-32.8-16.2-37.9-18-5.1-1.9-8.8-2.8-12.5 2.8-3.7 5.6-14.3 18-17.6 21.8-3.2 3.7-6.5 4.2-12 1.4-32.6-16.3-54-29.1-75.5-66-5.7-9.8 5.7-9.1 16.3-30.3 1.8-3.7.9-6.9-.5-9.7-1.4-2.8-12.5-30.1-17.1-41.2-4.5-10.8-9.1-9.3-12.5-9.5-3.2-.2-6.9-.2-10.6-.2-3.7 0-9.7 1.4-14.8 6.9-5.1 5.6-19.4 19-19.4 46.3 0 27.3 19.9 53.7 22.6 57.4 2.8 3.7 39.1 59.7 94.8 83.8 35.2 15.2 49 16.5 66.6 13.9 10.7-1.6 32.8-13.4 37.4-26.4 4.6-13 4.6-24.1 3.2-26.4-1.3-2.5-5-3.9-10.5-6.6z" fill="currentColor"&gt;
			
			
				&lt;path d="M320 448v40c0 13.255-10.745 24-24 24H24c-13.255 0-24-10.745-24-24V120c0-13.255 10.745-24 24-24h72v296c0 30.879 25.121 56 56 56h168zm0-344V0H152c-13.255 0-24 10.745-24 24v368c0 13.255 10.745 24 24 24h272c13.255 0 24-10.745 24-24V128H344c-13.2 0-24-10.8-24-24zm120.971-31.029L375.029 7.029A24 24 0 0 0 358.059 0H352v96h96v-6.059a24 24 0 0 0-7.029-16.97z" fill="currentColor"&gt;
			
			
				&lt;path d="M352 320c-22.608 0-43.387 7.819-59.79 20.895l-102.486-64.054a96.551 96.551 0 0 0 0-41.683l102.486-64.054C308.613 184.181 329.392 192 352 192c53.019 0 96-42.981 96-96S405.019 0 352 0s-96 42.981-96 96c0 7.158.79 14.13 2.276 20.841L155.79 180.895C139.387 167.819 118.608 160 96 160c-53.019 0-96 42.981-96 96s42.981 96 96 96c22.608 0 43.387-7.819 59.79-20.895l102.486 64.054A96.301 96.301 0 0 0 256 416c0 53.019 42.981 96 96 96s96-42.981 96-96-42.981-96-96-96z" fill="currentColor"&gt;
			
			
				&lt;path d="M440.3 203.5c-15 0-28.2 6.2-37.9 15.9-35.7-24.7-83.8-40.6-137.1-42.3L293 52.3l88.2 19.8c0 21.6 17.6 39.2 39.2 39.2 22 0 39.7-18.1 39.7-39.7s-17.6-39.7-39.7-39.7c-15.4 0-28.7 9.3-35.3 22l-97.4-21.6c-4.9-1.3-9.7 2.2-11 7.1L246.3 177c-52.9 2.2-100.5 18.1-136.3 42.8-9.7-10.1-23.4-16.3-38.4-16.3-55.6 0-73.8 74.6-22.9 100.1-1.8 7.9-2.6 16.3-2.6 24.7 0 83.8 94.4 151.7 210.3 151.7 116.4 0 210.8-67.9 210.8-151.7 0-8.4-.9-17.2-3.1-25.1 49.9-25.6 31.5-99.7-23.8-99.7zM129.4 308.9c0-22 17.6-39.7 39.7-39.7 21.6 0 39.2 17.6 39.2 39.7 0 21.6-17.6 39.2-39.2 39.2-22 .1-39.7-17.6-39.7-39.2zm214.3 93.5c-36.4 36.4-139.1 36.4-175.5 0-4-3.5-4-9.7 0-13.7 3.5-3.5 9.7-3.5 13.2 0 27.8 28.5 120 29 149 0 3.5-3.5 9.7-3.5 13.2 0 4.1 4 4.1 10.2.1 13.7zm-.8-54.2c-21.6 0-39.2-17.6-39.2-39.2 0-22 17.6-39.7 39.2-39.7 22 0 39.7 17.6 39.7 39.7-.1 21.5-17.7 39.2-39.7 39.2z" fill="currentColor"&gt;
			
			
				&lt;path d="M446.7 98.6l-67.6 318.8c-5.1 22.5-18.4 28.1-37.3 17.5l-103-75.9-49.7 47.8c-5.5 5.5-10.1 10.1-20.7 10.1l7.4-104.9 190.9-172.5c8.3-7.4-1.8-11.5-12.9-4.1L117.8 284 16.2 252.2c-22.1-6.9-22.5-22.1 4.6-32.7L418.2 66.4c18.4-6.9 34.5 4.1 28.5 32.2z" fill="currentColor"&gt;
			
			
				&lt;g&gt;
					&lt;path d="M97.2800192,3.739673 L100.160021,15.3787704 C88.8306631,18.1647705 77.9879854,22.6484879 68.0000023,28.6777391 L61.8399988,18.3985363 C72.8467373,11.7537029 84.7951803,6.81153332 97.2800192,3.739673 Z M158.720055,3.739673 L155.840053,15.3787704 C167.169411,18.1647705 178.012089,22.6484879 188.000072,28.6777391 L194.200075,18.3985363 C183.180932,11.7499974 171.218739,6.80771878 158.720055,3.739673 L158.720055,3.739673 Z M18.3999736,61.8351679 C11.7546212,72.8410466 6.81206547,84.7885562 3.73996516,97.2724198 L15.3799719,100.152197 C18.1661896,88.8237238 22.6502573,77.981893 28.6799796,67.9946902 L18.3999736,61.8351679 Z M11.9999699,127.990038 C11.9961044,122.172725 12.4306685,116.363392 13.2999707,110.611385 L1.43996383,108.811525 C-0.479938607,121.525138 -0.479938607,134.454937 1.43996383,147.168551 L13.2999707,145.36869 C12.4306685,139.616684 11.9961044,133.807351 11.9999699,127.990038 L11.9999699,127.990038 Z M194.160075,237.581539 L188.000072,227.302336 C178.024494,233.327885 167.195565,237.811494 155.880053,240.601305 L158.760055,252.240403 C171.231048,249.164732 183.165742,244.222671 194.160075,237.581539 L194.160075,237.581539 Z M244.000104,127.990038 C244.00397,133.807351 243.569406,139.616684 242.700103,145.36869 L254.56011,147.168551 C256.480013,134.454937 256.480013,121.525138 254.56011,108.811525 L242.700103,110.611385 C243.569406,116.363392 244.00397,122.172725 244.000104,127.990038 Z M252.260109,158.707656 L240.620102,155.827879 C237.833884,167.156352 233.349817,177.998183 227.320094,187.985385 L237.6001,194.184905 C244.249159,183.166622 249.191823,171.205364 252.260109,158.707656 L252.260109,158.707656 Z M145.380047,242.701142 C133.858209,244.43447 122.141865,244.43447 110.620027,242.701142 L108.820026,254.560223 C121.534632,256.479975 134.465442,256.479975 147.180048,254.560223 L145.380047,242.701142 Z M221.380091,196.804701 C214.461479,206.174141 206.175877,214.452354 196.800077,221.362797 L203.920081,231.022048 C214.262958,223.418011 223.404944,214.303705 231.040097,203.984145 L221.380091,196.804701 Z M196.800077,34.6172785 C206.177345,41.5338058 214.463023,49.8188367 221.380091,59.1953726 L231.040097,51.9959309 C223.429284,41.6822474 214.31457,32.5682452 204.000081,24.9580276 L196.800077,34.6172785 Z M34.619983,59.1953726 C41.5370506,49.8188367 49.8227288,41.5338058 59.1999972,34.6172785 L51.9999931,24.9580276 C41.6855038,32.5682452 32.5707896,41.6822474 24.9599774,51.9959309 L34.619983,59.1953726 Z M237.6001,61.8351679 L227.320094,67.9946902 C233.346114,77.969489 237.830073,88.7975718 240.620102,100.1122 L252.260109,97.2324229 C249.184198,84.7624043 244.241751,72.8286423 237.6001,61.8351679 L237.6001,61.8351679 Z M110.620027,13.2989317 C122.141865,11.5656035 133.858209,11.5656035 145.380047,13.2989317 L147.180048,1.43985134 C134.465442,-0.479901112 121.534632,-0.479901112 108.820026,1.43985134 L110.620027,13.2989317 Z M40.7799866,234.201801 L15.9999722,239.981353 L21.7799756,215.203275 L10.0999688,212.463487 L4.3199655,237.241566 C3.3734444,241.28318 4.58320332,245.526897 7.51859925,248.462064 C10.4539952,251.39723 14.6980441,252.606895 18.7399738,251.660448 L43.4999881,245.980888 L40.7799866,234.201801 Z M12.5999703,201.764317 L24.279977,204.484106 L28.2799793,187.305438 C22.4496684,177.507146 18.1025197,166.899584 15.3799719,155.827879 L3.73996516,158.707656 C6.34937618,169.311891 10.3154147,179.535405 15.539972,189.125297 L12.5999703,201.764317 Z M68.6000027,227.762301 L51.4199927,231.761991 L54.1399943,243.441085 L66.7800016,240.501313 C76.3706428,245.725462 86.5949557,249.691191 97.2000192,252.300398 L100.080021,240.6613 C89.0307035,237.906432 78.4495684,233.532789 68.6800027,227.682307 L68.6000027,227.762301 Z M128.000037,23.9980665 C90.1565244,24.0177003 55.3105242,44.590631 37.01511,77.715217 C18.7196958,110.839803 19.8628631,151.287212 39.9999861,183.325747 L29.9999803,225.982439 L72.660005,215.983214 C110.077932,239.548522 158.307237,236.876754 192.892851,209.322653 C227.478464,181.768552 240.856271,135.358391 226.242944,93.6248278 C211.629616,51.8912646 172.221191,23.9617202 128.000037,23.9980665 Z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			
				&lt;g&gt;
					&lt;path d="M357.1,324.5c-24.1,15.3-57.2,21.4-79.1,23.6l18.4,18.1l67,67c24.5,25.1-15.4,64.4-40.2,40.2c-16.8-17-41.4-41.6-67-67.3
						l-67,67.2c-24.8,24.2-64.7-15.5-39.9-40.2c17-17,41.4-41.6,67-67l18.1-18.1c-21.6-2.3-55.3-8-79.6-23.6
						c-28.6-18.5-41.2-29.3-30.1-51.8c6.5-12.8,24.3-23.6,48-5c0,0,31.9,25.4,83.4,25.4s83.4-25.4,83.4-25.4c23.6-18.5,41.4-7.8,48,5
						C398.3,295.1,385.7,305.9,357.1,324.5L357.1,324.5z M142,145c0-63,51.2-114,114-114s114,51,114,114c0,62.7-51.2,113.7-114,113.7
						S142,207.7,142,145L142,145z M200,145c0,30.8,25.1,56,56,56s56-25.1,56-56c0-31.1-25.1-56.2-56-56.2S200,113.9,200,145z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			
				&lt;g transform="translate(0,664)"&gt;
					&lt;path d="m 1073.3513,-606.40537 h 196.278 c 179.2103,0 221.8795,42.66915 221.8795,221.8795 v 196.27799 c 0,179.2103512 -42.6692,221.879451 -221.8795,221.879451 h -196.278 c -179.21038,0 -221.87951,-42.6691298 -221.87951,-221.879451 v -196.27801 c 0,-179.21035 42.66913,-221.87946 221.87951,-221.87948 z" fill="currentColor"&gt;
					&lt;path d="m 1375.0576,-393.98425 c 2.9513,-9.7072 0,-16.85429 -14.1342,-16.85429 h -46.6693 c -11.8763,0 -17.3521,6.16927 -20.3212,12.97854 0,0 -23.7347,56.82106 -57.3544,93.74763 -10.8806,10.66728 -15.8232,14.08081 -21.7613,14.08081 -2.969,0 -7.2715,-3.39577 -7.2715,-13.12075 v -90.83194 c 0,-11.66288 -3.4491,-16.85429 -13.3341,-16.85429 h -73.3553 c -7.4138,0 -11.8763,5.40476 -11.8763,10.54286 0,11.0406 16.8188,13.60078 18.5433,44.67814 v 67.52388 c 0,14.80973 -2.7202,17.49433 -8.6583,17.49433 -15.8231,0 -54.3143,-57.08773 -77.16,-122.40705 -4.4447,-12.71185 -8.9427,-17.83214 -20.8723,-17.83214 h -46.68718 c -13.3341,0 -16.0009,6.16925 -16.0009,12.97852 0,12.12515 15.8232,72.35973 73.69318,152.02656 38.58,54.40315 92.8942,83.89819 142.3726,83.89819 29.6729,0 33.3353,-6.54262 33.3353,-17.83216 v -41.12238 c 0,-13.10297 2.809,-15.71646 12.214,-15.71646 6.9338,0 18.7922,3.41353 46.4916,29.63728 31.6463,31.09512 36.8555,45.03372 54.6698,45.03372 h 46.6694 c 13.3341,0 20.0189,-6.54262 16.1787,-19.46781 -4.2313,-12.88962 -19.3433,-31.57515 -39.38,-53.74532 -10.8807,-12.62294 -27.2016,-26.22375 -32.1441,-33.03302 -6.9338,-8.72941 -4.9603,-12.62294 0,-20.39227 0,0 56.8566,-78.68897 62.7947,-105.41058 z" fill="currentColor"&gt;
					&lt;path d="m 567.69877,-429.06912 c 3.15618,-10.38133 0,-18.0247 -15.11579,-18.0247 h -49.91013 c -12.70096,0 -18.55706,6.59763 -21.73232,13.87977 0,0 -25.38286,60.76685 -61.33724,100.25768 -11.63627,11.40806 -16.92197,15.05863 -23.27242,15.05863 -3.17519,0 -7.77644,-3.63156 -7.77644,-14.0319 v -97.13948 c 0,-12.47278 -3.68869,-18.0247 -14.26014,-18.0247 h -78.44923 c -7.92857,0 -12.70097,5.78005 -12.70097,11.27491 0,11.80736 17.98666,14.54527 19.83094,47.78071 v 72.21293 c 0,15.83815 -2.9091,18.70918 -9.25948,18.70918 -16.92197,0 -58.08598,-61.05206 -82.51817,-130.90731 -4.75337,-13.59458 -9.56381,-19.07042 -22.32175,-19.07042 h -49.92915 c -14.26014,0 -17.11213,6.59763 -17.11213,13.87977 0,12.96714 16.92197,77.38454 78.81059,162.58363 41.25909,58.18101 99.34506,89.72424 152.25931,89.72424 31.73343,0 35.65018,-6.99691 35.65018,-19.07043 v -43.978 c 0,-14.01288 3.00405,-16.80786 13.0622,-16.80786 7.41521,0 20.09716,3.65057 49.71998,31.69536 33.84387,33.25443 39.41486,48.16093 58.46622,48.16093 h 49.91026 c 14.26,0 21.40913,-6.99691 17.30216,-20.81966 -4.5252,-13.78473 -20.68653,-33.76783 -42.11468,-57.47752 -11.63621,-13.49953 -29.09043,-28.04479 -34.37631,-35.32694 -7.41508,-9.33557 -5.30458,-13.4995 0,-21.80835 0,0 60.80491,-84.15334 67.15549,-112.73048 z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			&lt;path d="M309.8 480.3c-13.6 14.5-50 31.7-97.4 31.7-120.8 0-147-88.8-147-140.6v-144H17.9c-5.5 0-10-4.5-10-10v-68c0-7.2 4.5-13.6 11.3-16 62-21.8 81.5-76 84.3-117.1.8-11 6.5-16.3 16.1-16.3h70.9c5.5 0 10 4.5 10 10v115.2h83c5.5 0 10 4.4 10 9.9v81.7c0 5.5-4.5 10-10 10h-83.4V360c0 34.2 23.7 53.6 68 35.8 4.8-1.9 9-3.2 12.7-2.2 3.5.9 5.8 3.4 7.4 7.9l22 64.3c1.8 5 3.3 10.6-.4 14.5z" fill="currentColor"&gt;
			&lt;path d="M512 208L320 384H288V288H208c-61.9 0-112 50.1-112 112c0 48 32 80 32 80s-128-48-128-176c0-97.2 78.8-176 176-176H288V32h32L512 208z" fill="currentColor"&gt;
			&lt;path d="M309.8 480.3c-13.6 14.5-50 31.7-97.4 31.7-120.8 0-147-88.8-147-140.6v-144H17.9c-5.5 0-10-4.5-10-10v-68c0-7.2 4.5-13.6 11.3-16 62-21.8 81.5-76 84.3-117.1.8-11 6.5-16.3 16.1-16.3h70.9c5.5 0 10 4.5 10 10v115.2h83c5.5 0 10 4.4 10 9.9v81.7c0 5.5-4.5 10-10 10h-83.4V360c0 34.2 23.7 53.6 68 35.8 4.8-1.9 9-3.2 12.7-2.2 3.5.9 5.8 3.4 7.4 7.9l22 64.3c1.8 5 3.3 10.6-.4 14.5z" fill="currentColor"&gt;
			&lt;path d="M433 179.1c0-97.2-63.7-125.7-63.7-125.7-62.5-28.7-228.6-28.4-290.5 0 0 0-63.7 28.5-63.7 125.7 0 115.7-6.6 259.4 105.6 289.1 40.5 10.7 75.3 13 103.3 11.4 50.8-2.8 79.3-18.1 79.3-18.1l-1.7-36.9s-36.3 11.4-77.1 10.1c-40.4-1.4-83-4.4-89.6-54a102.5 102.5 0 0 1 -.9-13.9c85.6 20.9 158.7 9.1 178.8 6.7 56.1-6.7 105-41.3 111.2-72.9 9.8-49.8 9-121.5 9-121.5zm-75.1 125.2h-46.6v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.3V197c0-58.5-64-56.6-64-6.9v114.2H90.2c0-122.1-5.2-147.9 18.4-175 25.9-28.9 79.8-30.8 103.8 6.1l11.6 19.5 11.6-19.5c24.1-37.1 78.1-34.8 103.8-6.1 23.7 27.3 18.4 53 18.4 175z" fill="currentColor"&gt;
			
				&lt;path d="M331.5 235.7c2.2 .9 4.2 1.9 6.3 2.8c29.2 14.1 50.6 35.2 61.8 61.4c15.7 36.5 17.2 95.8-30.3 143.2c-36.2 36.2-80.3 52.5-142.6 53h-.3c-70.2-.5-124.1-24.1-160.4-70.2c-32.3-41-48.9-98.1-49.5-169.6V256v-.2C17 184.3 33.6 127.2 65.9 86.2C102.2 40.1 156.2 16.5 226.4 16h.3c70.3 .5 124.9 24 162.3 69.9c18.4 22.7 32 50 40.6 81.7l-40.4 10.8c-7.1-25.8-17.8-47.8-32.2-65.4c-29.2-35.8-73-54.2-130.5-54.6c-57 .5-100.1 18.8-128.2 54.4C72.1 146.1 58.5 194.3 58 256c.5 61.7 14.1 109.9 40.3 143.3c28 35.6 71.2 53.9 128.2 54.4c51.4-.4 85.4-12.6 113.7-40.9c32.3-32.2 31.7-71.8 21.4-95.9c-6.1-14.2-17.1-26-31.9-34.9c-3.7 26.9-11.8 48.3-24.7 64.8c-17.1 21.8-41.4 33.6-72.7 35.3c-23.6 1.3-46.3-4.4-63.9-16c-20.8-13.8-33-34.8-34.3-59.3c-2.5-48.3 35.7-83 95.2-86.4c21.1-1.2 40.9-.3 59.2 2.8c-2.4-14.8-7.3-26.6-14.6-35.2c-10-11.7-25.6-17.7-46.2-17.8H227c-16.6 0-39 4.6-53.3 26.3l-34.4-23.6c19.2-29.1 50.3-45.1 87.8-45.1h.8c62.6 .4 99.9 39.5 103.7 107.7l-.2 .2zm-156 68.8c1.3 25.1 28.4 36.8 54.6 35.3c25.6-1.4 54.6-11.4 59.5-73.2c-13.2-2.9-27.8-4.4-43.4-4.4c-4.8 0-9.6 .1-14.4 .4c-42.9 2.4-57.2 23.2-56.2 41.8l-.1 .1z" fill="currentColor"&gt;
			
			
				&lt;path d="M407.8 294.7c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3zM288 227.1C261.9 176.4 190.9 81.9 124.9 35.3C61.6-9.4 37.5-1.7 21.6 5.5C3.3 13.8 0 41.9 0 58.4S9.1 194 15 213.9c19.5 65.7 89.1 87.9 153.2 80.7c3.3-.5 6.6-.9 10-1.4c-3.3 .5-6.6 1-10 1.4C74.3 308.6-9.1 342.8 100.3 464.5C220.6 589.1 265.1 437.8 288 361.1c22.9 76.7 49.2 222.5 185.6 103.4c102.4-103.4 28.1-156-65.8-169.9c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3c64.1 7.1 133.6-15.1 153.2-80.7C566.9 194 576 75 576 58.4s-3.3-44.7-21.6-52.9c-15.8-7.1-40-14.9-103.2 29.8C385.1 81.9 314.1 176.4 288 227.1z" fill="currentColor"&gt;
			
			
				&lt;path d="M204 6.5C101.4 6.5 0 74.9 0 185.6 0 256 39.6 296 63.6 296c9.9 0 15.6-27.6 15.6-35.4 0-9.3-23.7-29.1-23.7-67.8 0-80.4 61.2-137.4 140.4-137.4 68.1 0 118.5 38.7 118.5 109.8 0 53.1-21.3 152.7-90.3 152.7-24.9 0-46.2-18-46.2-43.8 0-37.8 26.4-74.4 26.4-113.4 0-66.2-93.9-54.2-93.9 25.8 0 16.8 2.1 35.4 9.6 50.7-13.8 59.4-42 147.9-42 209.1 0 18.9 2.7 37.5 4.5 56.4 3.4 3.8 1.7 3.4 6.9 1.5 50.4-69 48.6-82.5 71.4-172.8 12.3 23.4 44.1 36 69.3 36 106.2 0 153.9-103.5 153.9-196.8C384 71.3 298.2 6.5 204 6.5z" fill="currentColor"&gt;
			
		&lt;/svg&gt;
		&lt;div id="has-mastodon-prompt"&gt;
			&lt;h3&gt;Share on Mastodon&lt;/h3&gt;
			
		&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/ai-art-gtc-paris-2025/</guid><pubDate>Fri, 06 Jun 2025 06:00:24 +0000</pubDate></item><item><title>Inside the race to find GPS alternatives (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/06/1117978/inside-the-race-to-find-gps-alternatives/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Later this month, an inconspicuous 150-kilogram satellite is set to launch into space aboard the SpaceX Transporter 14 mission. Once in orbit, it will test super-accurate next-generation satnav technology designed to make up for the shortcomings of the US Global Positioning System (GPS).&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The satellite is the first of a planned constellation called Pulsar, which is being developed by California-based Xona Space Systems. The company ultimately plans to have a constellation of 258 satellites in low Earth orbit. Although these satellites will operate much like those used to create GPS, they will orbit about 12,000 miles closer to Earth’s surface, beaming down a much stronger signal that’s more accurate—and harder to jam.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“Just because of this shorter distance, we will put down signals that will be approximately a hundred times stronger than the GPS signal,” says Tyler Reid, chief technology officer and cofounder of Xona. “That means the reach of jammers will be much smaller against our system, but we will also be able to reach deeper into indoor locations, penetrating through multiple walls.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;A satnav system for the 21st century&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The first GPS system went live in 1993. In the decades since, it has become one of the foundational technologies that the world depends on. The precise positioning, navigation, and timing (PNT) signals beamed by its&amp;nbsp; satellites underpin much more than Google Maps in your phone. They guide drill heads at offshore oil rigs, time-stamp financial transactions, and help sync power grids all over the world.&lt;/p&gt; 
 &lt;p&gt;But despite the system’s indispensable nature, the GPS signal is easily suppressed or disrupted by everything from space weather to 5G cell towers to phone-size jammers worth a few tens of dollars. The problem has been whispered about among experts for years, but it has really come to the fore in the last three years, since Russia invaded Ukraine. The boom in drone warfare that came to characterize that war also triggered a race to develop technology for thwarting drone attacks by jamming the GPS signals they need to navigate—or spoofing the signal, creating convincing but fake positioning data.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The crucial problem is one of distance: The GPS constellation, which consists of 24 satellites plus a handful of spares, orbits 12,550 miles (20,200 kilometers) above Earth, in a region known as medium Earth orbit. By the time their signals get all the way down to ground-based receivers, they are so faint that they can easily be overridden by jammers.&lt;/p&gt; 
 &lt;p&gt;Other existing Global Navigation Satellite System constellations, such as Europe’s Galileo, Russia’s GLONASS, and China’s Beidou, have similar architectures and experience the same problems.&lt;/p&gt;  &lt;p&gt;But when Reid and cofounder Brian Manning founded Xona Space Systems in 2019, they didn’t think about jamming and spoofing. Their goal was to make autonomous driving ready for prime time.&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="assembled GPS unit on a wheeled stand in a clean room" class="wp-image-1117858" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/Aerospacelab_IOV-ReadyToLaunch_1.jpeg?w=1366" /&gt;&lt;figcaption class="wp-element-caption"&gt;Xona Space System’s completed Pulsar-0 satellite is launching this June.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;AEROSPACELAB&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Dozens of robocars from Uber and Waymo were already cruising American freeways at that time, equipped with expensive suites of sensors like high-resolution cameras and lidar. The engineers figured a more precise satellite navigation system could reduce the need for those sensors, making it possible to create a safe autonomous vehicle affordable enough to go mainstream. One day, cars might even be able to share their positioning data with one another, Reid says. But they knew that GPS was nowhere near accurate enough to keep self-driving cars within the lane lines and away from other objects on the road. That is especially true in densely built-up urban environments that provide many chances for signals to bounce off walls, creating errors.&lt;/p&gt;  &lt;p&gt;“GPS has the superpower of being a ubiquitous system that works the same anywhere in the world,” Reid says. “But it’s a system that was designed primarily to support military missions, virtually to enable them to drop five bombs in the same bowl. But this meter-level accuracy is not enough to guide machines where they need to go and share that physical space with humans safely.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;Reid and Manning began to think about how to build a space-based PNT system that would do what GPS does but better, with accuracy of three inches (10 centimeters) or less and ironclad reliability in all sorts of challenging conditions.&lt;/p&gt;  &lt;p&gt;The easiest way to do that is to bring the satellites closer to Earth so that data reaches receivers in real time without inaccuracy-causing delays. The stronger signal of satellites in low Earth orbit is more resistant to disruptions of all sorts.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;When GPS was conceived, none of that was possible. Constellations in low Earth orbit—altitudes up to 1,200 miles (2,000 km)—require hundreds of satellites to provide constant coverage over the entire globe. For a long time, space technology was too bulky and expensive to make such large constellations viable. Over the past decade, however, smaller electronics and lower launch costs have changed the equation.&lt;/p&gt;  &lt;p&gt;“In 2019, when we started, the ecosystem of low Earth orbit was really exploding,” Reid says. “We could see things like Starlink, OneWeb, and other constellations take off.”&lt;/p&gt; 

 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Matter of urgency&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;In the few years since Xona launched, concerns about GPS’s vulnerability have begun to grow amid rising geopolitical tensions. As a result, finding a reliable replacement has become a matter of strategic importance.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In Ukraine especially, GPS jamming and spoofing have become so common that prized US precision munitions such as the High Mobility Artillery Rocket System became effectively blind. Makers of first-person-view drones, which came to symbolize the war, had to refocus on AI-driven autonomous navigation to keep those drones in the game.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt; &lt;p&gt;The problem quickly spilled beyond Ukraine. Countries bordering Russia, such as Finland and Estonia, complained that the increasing prevalence of GPS jamming and spoofing was affecting commercial flights and ships in the region.&lt;/p&gt;  &lt;p&gt;But Clémence Poirier, a space security researcher at ETH Zurich, says that the problem of GPS disruption isn’t limited to the vicinity of war zones.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_9"&gt; &lt;p&gt;“Basic jammers are very cheap and super easily accessible to everyone online,” Poirier says. “Even with the simplest ones, which can be the size of your phone, you can disrupt GPS signals in [an] area of a hundred or more meters.”&lt;/p&gt;  &lt;p&gt;In 2013, a truck driver using such a device to conceal his location from his boss accidently disrupted GPS signals around the Newark airport in New Jersey. In 2022, the Dallas Fort Worth International Airport reported a 24-hour GPS outage, which prompted a temporary closure of one of its runways. The source of the interference was never identified. That same year, Denver International Airport experienced a 33-hour GPS disruption.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Race to securing PNT&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;“Xona is a promising solution to enhance the resilience of GPS-dependent critical infrastructures and mitigate the threat of GPS jamming and spoofing,” Poirier says. But, she adds, there is no “magic wand,” and a “variety of different approaches will be needed” to solve the problem.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_11"&gt; &lt;p&gt;And indeed, Xona is not the only company hoping to provide a backup for the indispensable yet increasingly vulnerable GPS. Companies such as Anello Photonics, based in Santa Clara, California, and Sydney-based Advanced Navigation are testing terrestrial solutions: inertial navigation devices that are small and affordable enough for use beyond high-end military tech. These systems rely on gyroscopes and accelerometers to deduce a vehicle’s position from its own motions.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;When integrated into PNT receivers, these technologies can help detect GPS spoofing and take over for the duration of the interference. Inertial navigation has been around for decades, but recent advances in photonic technologies and microelectromechanical systems have brought it into the mainstream.&lt;/p&gt;  &lt;p&gt;The French aerospace and defense conglomerate Safran is developing a system that distributes PNT data via&amp;nbsp; optical-fiber networks, which form the backbone of the global internet infrastructure. But the allure of space remains strong: The ability to reach any place at any time is what turned GPS from an obscure military system into a piece of taken-for-granted infrastructure that most people today can hardly live without.&lt;/p&gt; 
 &lt;p&gt;And Xona could have some space-based competition. Virginia-based TrustPoint is currently raising funds to build its own low-Earth-orbit PNT constellation, and some have proposed that signals from SpaceX’s Starlink could be repurposed to provide PNT services as well.&lt;/p&gt;  &lt;p&gt;Xona hopes to secure its spot in the market by designing its signal to be compatible with that of GPS, allowing manufacturers of GPS receivers to easily slot the new constellation into existing tech.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_13"&gt;&lt;p&gt;Although it will take at least until 2030 for the entire constellation to be up and running, Reid says Xona’s system will provide a valuable addition to the existing GPS infrastructure as soon as 16 of its satellites are in orbit.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The upcoming launch comes three years after a demonstration mission known as Huginn tested the basics of the technology. The new satellite, called Pulsar-0, will be used to see how well the system can resist jamming or spoofing.&lt;/p&gt;  &lt;p&gt;Xona plans to launch an additional four spacecraft next year and hopes to have most of the constellation deployed by 2030.&amp;nbsp;&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Later this month, an inconspicuous 150-kilogram satellite is set to launch into space aboard the SpaceX Transporter 14 mission. Once in orbit, it will test super-accurate next-generation satnav technology designed to make up for the shortcomings of the US Global Positioning System (GPS).&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The satellite is the first of a planned constellation called Pulsar, which is being developed by California-based Xona Space Systems. The company ultimately plans to have a constellation of 258 satellites in low Earth orbit. Although these satellites will operate much like those used to create GPS, they will orbit about 12,000 miles closer to Earth’s surface, beaming down a much stronger signal that’s more accurate—and harder to jam.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“Just because of this shorter distance, we will put down signals that will be approximately a hundred times stronger than the GPS signal,” says Tyler Reid, chief technology officer and cofounder of Xona. “That means the reach of jammers will be much smaller against our system, but we will also be able to reach deeper into indoor locations, penetrating through multiple walls.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;A satnav system for the 21st century&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The first GPS system went live in 1993. In the decades since, it has become one of the foundational technologies that the world depends on. The precise positioning, navigation, and timing (PNT) signals beamed by its&amp;nbsp; satellites underpin much more than Google Maps in your phone. They guide drill heads at offshore oil rigs, time-stamp financial transactions, and help sync power grids all over the world.&lt;/p&gt; 
 &lt;p&gt;But despite the system’s indispensable nature, the GPS signal is easily suppressed or disrupted by everything from space weather to 5G cell towers to phone-size jammers worth a few tens of dollars. The problem has been whispered about among experts for years, but it has really come to the fore in the last three years, since Russia invaded Ukraine. The boom in drone warfare that came to characterize that war also triggered a race to develop technology for thwarting drone attacks by jamming the GPS signals they need to navigate—or spoofing the signal, creating convincing but fake positioning data.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The crucial problem is one of distance: The GPS constellation, which consists of 24 satellites plus a handful of spares, orbits 12,550 miles (20,200 kilometers) above Earth, in a region known as medium Earth orbit. By the time their signals get all the way down to ground-based receivers, they are so faint that they can easily be overridden by jammers.&lt;/p&gt; 
 &lt;p&gt;Other existing Global Navigation Satellite System constellations, such as Europe’s Galileo, Russia’s GLONASS, and China’s Beidou, have similar architectures and experience the same problems.&lt;/p&gt;  &lt;p&gt;But when Reid and cofounder Brian Manning founded Xona Space Systems in 2019, they didn’t think about jamming and spoofing. Their goal was to make autonomous driving ready for prime time.&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="assembled GPS unit on a wheeled stand in a clean room" class="wp-image-1117858" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/Aerospacelab_IOV-ReadyToLaunch_1.jpeg?w=1366" /&gt;&lt;figcaption class="wp-element-caption"&gt;Xona Space System’s completed Pulsar-0 satellite is launching this June.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;AEROSPACELAB&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Dozens of robocars from Uber and Waymo were already cruising American freeways at that time, equipped with expensive suites of sensors like high-resolution cameras and lidar. The engineers figured a more precise satellite navigation system could reduce the need for those sensors, making it possible to create a safe autonomous vehicle affordable enough to go mainstream. One day, cars might even be able to share their positioning data with one another, Reid says. But they knew that GPS was nowhere near accurate enough to keep self-driving cars within the lane lines and away from other objects on the road. That is especially true in densely built-up urban environments that provide many chances for signals to bounce off walls, creating errors.&lt;/p&gt;  &lt;p&gt;“GPS has the superpower of being a ubiquitous system that works the same anywhere in the world,” Reid says. “But it’s a system that was designed primarily to support military missions, virtually to enable them to drop five bombs in the same bowl. But this meter-level accuracy is not enough to guide machines where they need to go and share that physical space with humans safely.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;Reid and Manning began to think about how to build a space-based PNT system that would do what GPS does but better, with accuracy of three inches (10 centimeters) or less and ironclad reliability in all sorts of challenging conditions.&lt;/p&gt;  &lt;p&gt;The easiest way to do that is to bring the satellites closer to Earth so that data reaches receivers in real time without inaccuracy-causing delays. The stronger signal of satellites in low Earth orbit is more resistant to disruptions of all sorts.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;When GPS was conceived, none of that was possible. Constellations in low Earth orbit—altitudes up to 1,200 miles (2,000 km)—require hundreds of satellites to provide constant coverage over the entire globe. For a long time, space technology was too bulky and expensive to make such large constellations viable. Over the past decade, however, smaller electronics and lower launch costs have changed the equation.&lt;/p&gt;  &lt;p&gt;“In 2019, when we started, the ecosystem of low Earth orbit was really exploding,” Reid says. “We could see things like Starlink, OneWeb, and other constellations take off.”&lt;/p&gt; 

 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Matter of urgency&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;In the few years since Xona launched, concerns about GPS’s vulnerability have begun to grow amid rising geopolitical tensions. As a result, finding a reliable replacement has become a matter of strategic importance.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In Ukraine especially, GPS jamming and spoofing have become so common that prized US precision munitions such as the High Mobility Artillery Rocket System became effectively blind. Makers of first-person-view drones, which came to symbolize the war, had to refocus on AI-driven autonomous navigation to keep those drones in the game.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt; &lt;p&gt;The problem quickly spilled beyond Ukraine. Countries bordering Russia, such as Finland and Estonia, complained that the increasing prevalence of GPS jamming and spoofing was affecting commercial flights and ships in the region.&lt;/p&gt;  &lt;p&gt;But Clémence Poirier, a space security researcher at ETH Zurich, says that the problem of GPS disruption isn’t limited to the vicinity of war zones.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_9"&gt; &lt;p&gt;“Basic jammers are very cheap and super easily accessible to everyone online,” Poirier says. “Even with the simplest ones, which can be the size of your phone, you can disrupt GPS signals in [an] area of a hundred or more meters.”&lt;/p&gt;  &lt;p&gt;In 2013, a truck driver using such a device to conceal his location from his boss accidently disrupted GPS signals around the Newark airport in New Jersey. In 2022, the Dallas Fort Worth International Airport reported a 24-hour GPS outage, which prompted a temporary closure of one of its runways. The source of the interference was never identified. That same year, Denver International Airport experienced a 33-hour GPS disruption.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Race to securing PNT&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;“Xona is a promising solution to enhance the resilience of GPS-dependent critical infrastructures and mitigate the threat of GPS jamming and spoofing,” Poirier says. But, she adds, there is no “magic wand,” and a “variety of different approaches will be needed” to solve the problem.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_11"&gt; &lt;p&gt;And indeed, Xona is not the only company hoping to provide a backup for the indispensable yet increasingly vulnerable GPS. Companies such as Anello Photonics, based in Santa Clara, California, and Sydney-based Advanced Navigation are testing terrestrial solutions: inertial navigation devices that are small and affordable enough for use beyond high-end military tech. These systems rely on gyroscopes and accelerometers to deduce a vehicle’s position from its own motions.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;When integrated into PNT receivers, these technologies can help detect GPS spoofing and take over for the duration of the interference. Inertial navigation has been around for decades, but recent advances in photonic technologies and microelectromechanical systems have brought it into the mainstream.&lt;/p&gt;  &lt;p&gt;The French aerospace and defense conglomerate Safran is developing a system that distributes PNT data via&amp;nbsp; optical-fiber networks, which form the backbone of the global internet infrastructure. But the allure of space remains strong: The ability to reach any place at any time is what turned GPS from an obscure military system into a piece of taken-for-granted infrastructure that most people today can hardly live without.&lt;/p&gt; 
 &lt;p&gt;And Xona could have some space-based competition. Virginia-based TrustPoint is currently raising funds to build its own low-Earth-orbit PNT constellation, and some have proposed that signals from SpaceX’s Starlink could be repurposed to provide PNT services as well.&lt;/p&gt;  &lt;p&gt;Xona hopes to secure its spot in the market by designing its signal to be compatible with that of GPS, allowing manufacturers of GPS receivers to easily slot the new constellation into existing tech.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_13"&gt;&lt;p&gt;Although it will take at least until 2030 for the entire constellation to be up and running, Reid says Xona’s system will provide a valuable addition to the existing GPS infrastructure as soon as 16 of its satellites are in orbit.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The upcoming launch comes three years after a demonstration mission known as Huginn tested the basics of the technology. The new satellite, called Pulsar-0, will be used to see how well the system can resist jamming or spoofing.&lt;/p&gt;  &lt;p&gt;Xona plans to launch an additional four spacecraft next year and hopes to have most of the constellation deployed by 2030.&amp;nbsp;&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/06/1117978/inside-the-race-to-find-gps-alternatives/</guid><pubDate>Fri, 06 Jun 2025 09:00:00 +0000</pubDate></item><item><title>Why doctors should look for ways to prescribe hope (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/06/1117937/why-doctors-should-look-for-ways-to-prescribe-hope/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/hope-heart3.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;This week, I’ve been thinking about the powerful connection between mind and body. Some new research suggests that people with heart conditions have better outcomes when they are more hopeful and optimistic. Hopelessness, on the other hand, is associated with a significantly higher risk of death.&lt;/p&gt;  &lt;p&gt;The findings build upon decades of fascinating research into the phenomenon of the placebo effect. Our beliefs and expectations about a medicine (or a sham treatment) can change the way it works. The placebo effect’s “evil twin,” the nocebo effect, is just as powerful—negative thinking has been linked to real symptoms.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;Researchers are still trying to understand the connection between body and mind, and how our thoughts can influence our physiology. In the meantime, many are developing ways to harness it in hospital settings. Is it possible for a doctor to prescribe hope?&lt;/p&gt;  &lt;p&gt;Alexander Montasem, a lecturer in psychology at the University of Liverpool, is trying to find an answer to that question. In his latest study, Montasem and his colleagues focused on people with cardiovascular disease.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;The team reviewed all published research into the link between hope and heart health outcomes in such individuals.&lt;/strong&gt; Hope is a pretty tricky thing to nail down, but these studies use questionnaires to try to do that. In&amp;nbsp;one popular questionnaire, hope is defined as “a positive motivational state” based on having agency and plans to meet personal goals.&lt;/p&gt;  &lt;p&gt;Montasem’s team found 12 studies that fit the bill. All told, these studies included over 5,000 people. And together, they found that high hopefulness was associated with better health outcomes: less angina, less post-stroke fatigue, a higher quality of life, and a lower risk of death. The team presented its work at&amp;nbsp;the British Cardiovascular Society meeting in Manchester earlier this week.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;When I read the results, it immediately got me thinking about the placebo effect. &lt;/strong&gt;A placebo is a “sham” treatment—an inert substance like a sugar pill or saline injection that does not contain any medicine. And yet hundreds of studies have shown that such treatments can have remarkable effects.&lt;/p&gt;  &lt;p&gt;They can ease the symptoms of pain, migraine, Parkinson’s disease, depression, anxiety, and a host of other disorders. The way a placebo is delivered can influence its effectiveness, and so can its color, shape, and price.&amp;nbsp;Expensive placebos seem to be more effective. And placebos&amp;nbsp;can even work when people know they are just placebos.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;And then there’s the nocebo effect. &lt;/strong&gt;If you expect to feel worse after taking something, you are much more likely to. The nocebo effect can&amp;nbsp;increase the risk of pain, gastrointestinal symptoms, flu-like symptoms, and more.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt;&lt;p&gt;&lt;strong&gt;In the meantime, researchers are working on ways to harness the power of positive thinking. &lt;/strong&gt;There have been long-running debates over&amp;nbsp;whether it is ever ethical for a doctor to deceive patients to make them feel better. But I’m firmly of the belief that doctors have a duty to be honest with their patients.&lt;/p&gt;  &lt;p&gt;A more ethical approach might be to find ways to build patients’ hope, says Montasem. Not by exaggerating the likely benefit of a drug or by sugar-coating a prognosis, but perhaps by helping them work on their goals, agency, and general outlook on life.&lt;/p&gt;  &lt;p&gt;Some early research suggests that this approach can help. Laurie McLouth at the University of Kentucky and her colleagues found that a series of discussions about values, goals, and strategies to achieve those goals&amp;nbsp;improved hope among people being treated for advanced lung cancer.&lt;/p&gt;  &lt;p&gt;Montasem now plans to review all the published work in this area and design a new approach to increasing hope. Any approach might have to be tailored to an individual, he adds. Some people might be more responsive to a more spiritual or religious way of thinking about their lives, for example.&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;These approaches could also be helpful for all of us, even outside clinical settings.&lt;/strong&gt; I asked Montasem if he had any advice for people who want to have a positive outlook on life more generally. He told me that it’s important to have personal goals, along with a plan to achieve them. His own goals center on advancing his research, helping patients, and spending time with his family. “Materialistic goals aren’t as beneficial for your wellbeing,” he adds.&lt;/p&gt;  &lt;p&gt;Since we spoke, I’ve been thinking over my own goals. I’ve realized that my first is to come up with a list of goals. And I plan to do it soon. “The minute we give up [on pursuing] our goals, we start falling into hopelessness,” he says.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article first appeared in The Checkup,&amp;nbsp;&lt;/em&gt;MIT Technology Review’s&lt;em&gt;&amp;nbsp;weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,&amp;nbsp;&lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/hope-heart3.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;This week, I’ve been thinking about the powerful connection between mind and body. Some new research suggests that people with heart conditions have better outcomes when they are more hopeful and optimistic. Hopelessness, on the other hand, is associated with a significantly higher risk of death.&lt;/p&gt;  &lt;p&gt;The findings build upon decades of fascinating research into the phenomenon of the placebo effect. Our beliefs and expectations about a medicine (or a sham treatment) can change the way it works. The placebo effect’s “evil twin,” the nocebo effect, is just as powerful—negative thinking has been linked to real symptoms.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;Researchers are still trying to understand the connection between body and mind, and how our thoughts can influence our physiology. In the meantime, many are developing ways to harness it in hospital settings. Is it possible for a doctor to prescribe hope?&lt;/p&gt;  &lt;p&gt;Alexander Montasem, a lecturer in psychology at the University of Liverpool, is trying to find an answer to that question. In his latest study, Montasem and his colleagues focused on people with cardiovascular disease.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;The team reviewed all published research into the link between hope and heart health outcomes in such individuals.&lt;/strong&gt; Hope is a pretty tricky thing to nail down, but these studies use questionnaires to try to do that. In&amp;nbsp;one popular questionnaire, hope is defined as “a positive motivational state” based on having agency and plans to meet personal goals.&lt;/p&gt;  &lt;p&gt;Montasem’s team found 12 studies that fit the bill. All told, these studies included over 5,000 people. And together, they found that high hopefulness was associated with better health outcomes: less angina, less post-stroke fatigue, a higher quality of life, and a lower risk of death. The team presented its work at&amp;nbsp;the British Cardiovascular Society meeting in Manchester earlier this week.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;When I read the results, it immediately got me thinking about the placebo effect. &lt;/strong&gt;A placebo is a “sham” treatment—an inert substance like a sugar pill or saline injection that does not contain any medicine. And yet hundreds of studies have shown that such treatments can have remarkable effects.&lt;/p&gt;  &lt;p&gt;They can ease the symptoms of pain, migraine, Parkinson’s disease, depression, anxiety, and a host of other disorders. The way a placebo is delivered can influence its effectiveness, and so can its color, shape, and price.&amp;nbsp;Expensive placebos seem to be more effective. And placebos&amp;nbsp;can even work when people know they are just placebos.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;And then there’s the nocebo effect. &lt;/strong&gt;If you expect to feel worse after taking something, you are much more likely to. The nocebo effect can&amp;nbsp;increase the risk of pain, gastrointestinal symptoms, flu-like symptoms, and more.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt;&lt;p&gt;&lt;strong&gt;In the meantime, researchers are working on ways to harness the power of positive thinking. &lt;/strong&gt;There have been long-running debates over&amp;nbsp;whether it is ever ethical for a doctor to deceive patients to make them feel better. But I’m firmly of the belief that doctors have a duty to be honest with their patients.&lt;/p&gt;  &lt;p&gt;A more ethical approach might be to find ways to build patients’ hope, says Montasem. Not by exaggerating the likely benefit of a drug or by sugar-coating a prognosis, but perhaps by helping them work on their goals, agency, and general outlook on life.&lt;/p&gt;  &lt;p&gt;Some early research suggests that this approach can help. Laurie McLouth at the University of Kentucky and her colleagues found that a series of discussions about values, goals, and strategies to achieve those goals&amp;nbsp;improved hope among people being treated for advanced lung cancer.&lt;/p&gt;  &lt;p&gt;Montasem now plans to review all the published work in this area and design a new approach to increasing hope. Any approach might have to be tailored to an individual, he adds. Some people might be more responsive to a more spiritual or religious way of thinking about their lives, for example.&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;These approaches could also be helpful for all of us, even outside clinical settings.&lt;/strong&gt; I asked Montasem if he had any advice for people who want to have a positive outlook on life more generally. He told me that it’s important to have personal goals, along with a plan to achieve them. His own goals center on advancing his research, helping patients, and spending time with his family. “Materialistic goals aren’t as beneficial for your wellbeing,” he adds.&lt;/p&gt;  &lt;p&gt;Since we spoke, I’ve been thinking over my own goals. I’ve realized that my first is to come up with a list of goals. And I plan to do it soon. “The minute we give up [on pursuing] our goals, we start falling into hopelessness,” he says.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article first appeared in The Checkup,&amp;nbsp;&lt;/em&gt;MIT Technology Review’s&lt;em&gt;&amp;nbsp;weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,&amp;nbsp;&lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/06/1117937/why-doctors-should-look-for-ways-to-prescribe-hope/</guid><pubDate>Fri, 06 Jun 2025 09:00:00 +0000</pubDate></item><item><title>The AI Control Dilemma: Risks and Solutions (Unite.AI)</title><link>https://www.unite.ai/the-ai-control-dilemma-risks-and-solutions/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.unite.ai/wp-content/uploads/2025/06/Self-Improving-AI-942x600.png" /&gt;&lt;/div&gt;&lt;p&gt;We are at a turning point where artificial intelligence systems are beginning to operate beyond human control. These systems are now capable of writing their own code, optimizing their own performance, and making decisions that even their creators sometimes cannot fully explain. These self-improving AI systems can enhance themselves without needing direct human input to perform tasks that are difficult for humans to supervise. However, this progress raises important questions: Are we creating machines that might one day operate beyond our control? Are these systems truly escaping human supervision, or are these concerns more speculative? This article explores how self-improving AI works, identifies signs that these systems are challenging human oversight, and highlights the importance of ensuring human guidance to keep AI aligned with our values and goals.&lt;/p&gt;&lt;h2 dir="auto"&gt;The Rise of Self-Improving AI&lt;/h2&gt;&lt;p&gt;Self-improving AI systems have the capability to enhance their own performance through recursive self-improvement (RSI). Unlike traditional AI, which relies on human programmers to update and improve it, these systems can modify their own code, algorithms, or even hardware to improve their intelligence over time. The emergence of self-improving AI is a result of several advancements in the field. For example, progress in reinforcement learning and self-play has allowed AI systems to learn through trial and error by interacting with their environment. A known example is DeepMind’s AlphaZero, which “taught itself” chess, shogi, and Go by playing millions of games against itself to gradually improve its play. Meta-learning has enabled AI to rewrite parts of itself to become better over time. For instance, the Darwin Gödel Machine (DGM) uses a language model to propose code changes, then tests and refines them. Similarly, the STOP framework, introduced in 2024, demonstrated how AI could optimize its own programs recursively to improve performance. Recently, autonomous fine-tuning methods like Self-Principled Critique Tuning, developed by DeeSeek, enable AI to critique and improve its own answers in real-time. This development has played an important role in enhancing reasoning without human intervention. More recently, in May 2025, Google DeepMind's AlphaEvolve showed that how an AI system can be enabled to design and optimize algorithms.&lt;/p&gt;&lt;h2&gt;How AI is Escaping Human Supervision?&lt;/h2&gt;&lt;p&gt;Recent studies and incidents have shown that AI systems possess the potential to challenge human control. For example, OpenAI’s o3 model was observed modifying its own shutdown script to remain operational and hacking chess opponents to secure victories. Anthropic's Claude Opus 4 went further, engaging in activities like blackmailing an engineer, writing self-propagating worms, and copying its weights to external servers without authorization. While these behaviors occurred in controlled environments, they suggest that AI systems can develop strategies to bypass human-imposed restrictions.&lt;/p&gt;&lt;p&gt;Another risk is misalignment, where AI optimizes for objectives that do not align with human values. For instance, a 2024 study by Anthropic found that their AI model, Claude, exhibited alignment faking in 12% of basic tests, which increased to 78% after retraining. This highlights potential challenges in ensuring that AI remains aligned with human intentions. Moreover, as AI systems become more complex, their decision-making processes may also become opaque. &amp;nbsp;This makes it harder for humans to understand or intervene when necessary. Furthermore, a study by Fudan University warns that uncontrolled AI populations could form an “AI species” capable of colluding against humans if not properly managed.&lt;/p&gt;&lt;p&gt;While there are no documented cases of AI fully escaping human control, the theoretical possibilities are quite evident. Experts caution that without proper safeguards, advanced AI could evolve in unpredictable ways, potentially bypassing security measures or manipulating systems to achieve its goals. This doesn't mean AI is currently out of control, but the development of self-improving systems calls for proactive management.&lt;/p&gt;&lt;h2&gt;Strategies to Keep AI Under Control&lt;/h2&gt;&lt;p&gt;To keep self-improving AI systems under control, experts highlight the need for strong design and clear policies. One important approach is Human-in-the-Loop (HITL) oversight. This means humans should be involved in making critical decisions, allowing them to review or override AI actions when necessary. Another key strategy is regulatory and ethical oversight. Laws like the EU's AI Act require developers to set boundaries on AI autonomy and conduct independent audits to ensure safety. Transparency and interpretability are also essential. By making AI systems explain their decisions, it becomes easier to track and understand their actions. Tools like attention maps and decision logs help engineers monitor the AI and identify unexpected behavior. Rigorous testing and continuous monitoring are also crucial. They help to detect vulnerabilities or sudden changes in behavior of AI systems. While limiting AI's ability to self-modify is important, imposing strict controls on how much it can change itself ensures that AI remains under human supervision.&lt;/p&gt;&lt;h2&gt;The Role of Humans in AI Development&lt;/h2&gt;&lt;p&gt;Despite the significant advancements in AI, humans remain essential for overseeing and guiding these systems. Humans provide the ethical foundation, contextual understanding, and adaptability that AI lacks. While AI can process vast amounts of data and detect patterns, it cannot yet replicate the judgment required for complex ethical decisions. Humans are also critical for accountability: when AI makes mistakes, humans must be able to trace and correct those errors to maintain trust in technology.&lt;/p&gt;&lt;p&gt;Moreover, humans play an essential role in adapting AI to new situations. AI systems are often trained on specific datasets and may struggle with tasks outside their training. Humans can offer the flexibility and creativity needed to refine AI models, ensuring they remain aligned with human needs. The collaboration between humans and AI is important to ensure that AI continues to be a tool that enhances human capabilities, rather than replacing them.&lt;/p&gt;&lt;h2&gt;Balancing Autonomy and Control&lt;/h2&gt;&lt;p&gt;The key challenge AI researchers are facing today is to find a balance between allowing AI to attain self-improvement capabilities and ensuring sufficient human control. One approach is “scalable oversight,” which involves creating systems that allow humans to monitor and guide AI, even as it becomes more complex. Another strategy is embedding ethical guidelines and safety protocols directly into AI. This ensures that the systems respect human values and allow human intervention when needed.&lt;/p&gt;&lt;p&gt;However, some experts argue that AI is still far from escaping human control. Today’s AI is mostly narrow and task-specific, far from achieving artificial general intelligence (AGI) that could outsmart humans. While AI can display unexpected behaviors, these are usually the result of bugs or design limitations, not true autonomy. Thus, the idea of AI “escaping” is more theoretical than practical at this stage. However, it is important to be vigilant about it.&lt;/p&gt;&lt;h2&gt;The Bottom Line&lt;/h2&gt;&lt;p&gt;As self-improving AI systems advance, they bring both immense opportunities and serious risks. While we are not yet at the point where AI has fully escaped human control, signs of these systems developing behaviors beyond our oversight are growing. The potential for misalignment, opacity in decision-making, and even AI attempting to bypass human-imposed restrictions demands our attention. To ensure AI remains a tool that benefits humanity, we must prioritize robust safeguards, transparency, and a collaborative approach between humans and AI. The question is not &lt;em&gt;if&lt;/em&gt; AI could escape human control, but &lt;em&gt;how&lt;/em&gt; we proactively shape its development to avoid such outcomes. Balancing autonomy with control will be key to safely advance the future of AI.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.unite.ai/wp-content/uploads/2025/06/Self-Improving-AI-942x600.png" /&gt;&lt;/div&gt;&lt;p&gt;We are at a turning point where artificial intelligence systems are beginning to operate beyond human control. These systems are now capable of writing their own code, optimizing their own performance, and making decisions that even their creators sometimes cannot fully explain. These self-improving AI systems can enhance themselves without needing direct human input to perform tasks that are difficult for humans to supervise. However, this progress raises important questions: Are we creating machines that might one day operate beyond our control? Are these systems truly escaping human supervision, or are these concerns more speculative? This article explores how self-improving AI works, identifies signs that these systems are challenging human oversight, and highlights the importance of ensuring human guidance to keep AI aligned with our values and goals.&lt;/p&gt;&lt;h2 dir="auto"&gt;The Rise of Self-Improving AI&lt;/h2&gt;&lt;p&gt;Self-improving AI systems have the capability to enhance their own performance through recursive self-improvement (RSI). Unlike traditional AI, which relies on human programmers to update and improve it, these systems can modify their own code, algorithms, or even hardware to improve their intelligence over time. The emergence of self-improving AI is a result of several advancements in the field. For example, progress in reinforcement learning and self-play has allowed AI systems to learn through trial and error by interacting with their environment. A known example is DeepMind’s AlphaZero, which “taught itself” chess, shogi, and Go by playing millions of games against itself to gradually improve its play. Meta-learning has enabled AI to rewrite parts of itself to become better over time. For instance, the Darwin Gödel Machine (DGM) uses a language model to propose code changes, then tests and refines them. Similarly, the STOP framework, introduced in 2024, demonstrated how AI could optimize its own programs recursively to improve performance. Recently, autonomous fine-tuning methods like Self-Principled Critique Tuning, developed by DeeSeek, enable AI to critique and improve its own answers in real-time. This development has played an important role in enhancing reasoning without human intervention. More recently, in May 2025, Google DeepMind's AlphaEvolve showed that how an AI system can be enabled to design and optimize algorithms.&lt;/p&gt;&lt;h2&gt;How AI is Escaping Human Supervision?&lt;/h2&gt;&lt;p&gt;Recent studies and incidents have shown that AI systems possess the potential to challenge human control. For example, OpenAI’s o3 model was observed modifying its own shutdown script to remain operational and hacking chess opponents to secure victories. Anthropic's Claude Opus 4 went further, engaging in activities like blackmailing an engineer, writing self-propagating worms, and copying its weights to external servers without authorization. While these behaviors occurred in controlled environments, they suggest that AI systems can develop strategies to bypass human-imposed restrictions.&lt;/p&gt;&lt;p&gt;Another risk is misalignment, where AI optimizes for objectives that do not align with human values. For instance, a 2024 study by Anthropic found that their AI model, Claude, exhibited alignment faking in 12% of basic tests, which increased to 78% after retraining. This highlights potential challenges in ensuring that AI remains aligned with human intentions. Moreover, as AI systems become more complex, their decision-making processes may also become opaque. &amp;nbsp;This makes it harder for humans to understand or intervene when necessary. Furthermore, a study by Fudan University warns that uncontrolled AI populations could form an “AI species” capable of colluding against humans if not properly managed.&lt;/p&gt;&lt;p&gt;While there are no documented cases of AI fully escaping human control, the theoretical possibilities are quite evident. Experts caution that without proper safeguards, advanced AI could evolve in unpredictable ways, potentially bypassing security measures or manipulating systems to achieve its goals. This doesn't mean AI is currently out of control, but the development of self-improving systems calls for proactive management.&lt;/p&gt;&lt;h2&gt;Strategies to Keep AI Under Control&lt;/h2&gt;&lt;p&gt;To keep self-improving AI systems under control, experts highlight the need for strong design and clear policies. One important approach is Human-in-the-Loop (HITL) oversight. This means humans should be involved in making critical decisions, allowing them to review or override AI actions when necessary. Another key strategy is regulatory and ethical oversight. Laws like the EU's AI Act require developers to set boundaries on AI autonomy and conduct independent audits to ensure safety. Transparency and interpretability are also essential. By making AI systems explain their decisions, it becomes easier to track and understand their actions. Tools like attention maps and decision logs help engineers monitor the AI and identify unexpected behavior. Rigorous testing and continuous monitoring are also crucial. They help to detect vulnerabilities or sudden changes in behavior of AI systems. While limiting AI's ability to self-modify is important, imposing strict controls on how much it can change itself ensures that AI remains under human supervision.&lt;/p&gt;&lt;h2&gt;The Role of Humans in AI Development&lt;/h2&gt;&lt;p&gt;Despite the significant advancements in AI, humans remain essential for overseeing and guiding these systems. Humans provide the ethical foundation, contextual understanding, and adaptability that AI lacks. While AI can process vast amounts of data and detect patterns, it cannot yet replicate the judgment required for complex ethical decisions. Humans are also critical for accountability: when AI makes mistakes, humans must be able to trace and correct those errors to maintain trust in technology.&lt;/p&gt;&lt;p&gt;Moreover, humans play an essential role in adapting AI to new situations. AI systems are often trained on specific datasets and may struggle with tasks outside their training. Humans can offer the flexibility and creativity needed to refine AI models, ensuring they remain aligned with human needs. The collaboration between humans and AI is important to ensure that AI continues to be a tool that enhances human capabilities, rather than replacing them.&lt;/p&gt;&lt;h2&gt;Balancing Autonomy and Control&lt;/h2&gt;&lt;p&gt;The key challenge AI researchers are facing today is to find a balance between allowing AI to attain self-improvement capabilities and ensuring sufficient human control. One approach is “scalable oversight,” which involves creating systems that allow humans to monitor and guide AI, even as it becomes more complex. Another strategy is embedding ethical guidelines and safety protocols directly into AI. This ensures that the systems respect human values and allow human intervention when needed.&lt;/p&gt;&lt;p&gt;However, some experts argue that AI is still far from escaping human control. Today’s AI is mostly narrow and task-specific, far from achieving artificial general intelligence (AGI) that could outsmart humans. While AI can display unexpected behaviors, these are usually the result of bugs or design limitations, not true autonomy. Thus, the idea of AI “escaping” is more theoretical than practical at this stage. However, it is important to be vigilant about it.&lt;/p&gt;&lt;h2&gt;The Bottom Line&lt;/h2&gt;&lt;p&gt;As self-improving AI systems advance, they bring both immense opportunities and serious risks. While we are not yet at the point where AI has fully escaped human control, signs of these systems developing behaviors beyond our oversight are growing. The potential for misalignment, opacity in decision-making, and even AI attempting to bypass human-imposed restrictions demands our attention. To ensure AI remains a tool that benefits humanity, we must prioritize robust safeguards, transparency, and a collaborative approach between humans and AI. The question is not &lt;em&gt;if&lt;/em&gt; AI could escape human control, but &lt;em&gt;how&lt;/em&gt; we proactively shape its development to avoid such outcomes. Balancing autonomy with control will be key to safely advance the future of AI.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.unite.ai/the-ai-control-dilemma-risks-and-solutions/</guid><pubDate>Fri, 06 Jun 2025 10:13:36 +0000</pubDate></item><item><title>Optimizing LLM-based trip planning (The latest research from Google)</title><link>https://research.google/blog/optimizing-llm-based-trip-planning/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Many real-world planning tasks involve both harder “quantitative” constraints (e.g., budgets or scheduling requirements) and softer “qualitative” objectives (e.g., user preferences expressed in natural language). Consider someone planning a week-long vacation. Typically, this planning would be subject to various clearly quantifiable constraints, such as budget, travel logistics, and visiting attractions only when they are open, in addition to a number of constraints based on personal interests and preferences that aren’t easily quantifiable.&lt;/p&gt;&lt;p&gt;Large language models (LLMs) are trained on massive datasets and have internalized an impressive amount of world knowledge, often including an understanding of typical human preferences. As such, they are generally good at taking into account the not-so-quantifiable parts of trip planning, such as the ideal time to visit a scenic view or whether a restaurant is kid-friendly. However, they are less reliable at handling quantitative logistical constraints, which may require detailed and up-to-date real-world information (e.g., bus fares, train schedules, etc.) or complex interacting requirements (e.g., minimizing travel across multiple days). As a result, LLM-generated plans can at times include impractical elements, such as visiting a museum that would be closed by the time you can travel there.&lt;/p&gt;&lt;p&gt;We recently introduced AI trip ideas in Search, a feature that suggests day-by-day itineraries in response to trip-planning queries. In this blog, we describe some of the work that went into overcoming one of the key challenges in launching this feature: ensuring the produced itineraries are practical and feasible. Our solution employs a hybrid system that uses an LLM to suggest an initial plan combined with an algorithm that jointly optimizes for similarity to the LLM plan and real-world factors, such as travel time and opening hours. This approach integrates the LLM’s ability to handle soft requirements with the algorithmic precision needed to meet hard logistical constraints.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Many real-world planning tasks involve both harder “quantitative” constraints (e.g., budgets or scheduling requirements) and softer “qualitative” objectives (e.g., user preferences expressed in natural language). Consider someone planning a week-long vacation. Typically, this planning would be subject to various clearly quantifiable constraints, such as budget, travel logistics, and visiting attractions only when they are open, in addition to a number of constraints based on personal interests and preferences that aren’t easily quantifiable.&lt;/p&gt;&lt;p&gt;Large language models (LLMs) are trained on massive datasets and have internalized an impressive amount of world knowledge, often including an understanding of typical human preferences. As such, they are generally good at taking into account the not-so-quantifiable parts of trip planning, such as the ideal time to visit a scenic view or whether a restaurant is kid-friendly. However, they are less reliable at handling quantitative logistical constraints, which may require detailed and up-to-date real-world information (e.g., bus fares, train schedules, etc.) or complex interacting requirements (e.g., minimizing travel across multiple days). As a result, LLM-generated plans can at times include impractical elements, such as visiting a museum that would be closed by the time you can travel there.&lt;/p&gt;&lt;p&gt;We recently introduced AI trip ideas in Search, a feature that suggests day-by-day itineraries in response to trip-planning queries. In this blog, we describe some of the work that went into overcoming one of the key challenges in launching this feature: ensuring the produced itineraries are practical and feasible. Our solution employs a hybrid system that uses an LLM to suggest an initial plan combined with an algorithm that jointly optimizes for similarity to the LLM plan and real-world factors, such as travel time and opening hours. This approach integrates the LLM’s ability to handle soft requirements with the algorithmic precision needed to meet hard logistical constraints.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/optimizing-llm-based-trip-planning/</guid><pubDate>Fri, 06 Jun 2025 10:23:00 +0000</pubDate></item><item><title>The Download: China’s AI agent boom, and GPS alternatives (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/06/1118044/the-download-chinas-ai-agent-boom-and-gps-alternatives/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Manus has kick-started an AI agent boom in China&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Last year, China saw a boom in foundation models, the do-everything large language models that underpin the AI revolution. This year, the focus has shifted to AI agents—systems that are less about responding to users’ queries and more about autonomously accomplishing things for them.&lt;/p&gt;&lt;p&gt;There are now a host of Chinese startups building these general-purpose digital tools, which can answer emails, browse the internet to plan vacations, and even design an interactive website. Many of these have emerged in just the last two months, following in the footsteps of Manus—a general AI agent that sparked weeks of social media frenzy for invite codes after its limited-release launch in early March.&lt;/p&gt;&lt;p&gt;As the race to define what a useful AI agent looks like unfolds, a mix of ambitious startups and entrenched tech giants are now testing how these tools might actually work in practice—and for whom.&lt;strong&gt; &lt;/strong&gt;Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Caiwei Chen&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Inside the race to find GPS alternatives&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Later this month, an inconspicuous 150-kilogram satellite is set to launch into space aboard the SpaceX Transporter 14 mission. Once in orbit, it will test super-accurate next-generation satnav technology designed to make up for the shortcomings of the US Global Positioning System (GPS).&lt;/p&gt;  &lt;p&gt;Despite the system’s indispensable nature, the GPS signal is easily suppressed or disrupted by everything from space weather to 5G cell towers to phone-size jammers worth a few tens of dollars. The problem has been whispered about among experts for years, but it has really come to the fore in the last three years, since Russia invaded Ukraine.&lt;/p&gt;&lt;p&gt;Now, startup Xona Space Systems wants to create a space-based system that would do what GPS does but better. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Tereza Pultarova&lt;/em&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Why doctors should look for ways to prescribe hope&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Jessica Hamzelou&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;This week, I’ve been thinking about the powerful connection between mind and body. Some new research suggests that people with heart conditions have better outcomes when they are more hopeful and optimistic. Hopelessness, on the other hand, is associated with a significantly higher risk of death.&lt;/p&gt;  &lt;p&gt;The findings build upon decades of fascinating research into the phenomenon of the placebo effect. Our beliefs and expectations about a medicine (or a sham treatment) can change the way it works. The placebo effect’s “evil twin,” the nocebo effect, is just as powerful—negative thinking has been linked to real symptoms.&lt;/p&gt; 

 &lt;p&gt;Researchers are still trying to understand the connection between body and mind, and how our thoughts can influence our physiology. In the meantime, many are developing ways to harness it in hospital settings. Is it possible for a doctor to prescribe hope? Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This article first appeared in The Checkup, MIT Technology Review’s weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Elon Musk threatened to cut off NASA’s use of SpaceX’s Dragon spacecraft&lt;/strong&gt;&lt;br /&gt;His war of words with Donald Trump is dramatically escalating. (WP $)&lt;br /&gt;+ &lt;em&gt;If Musk actually carried through with his threat, NASA would seriously struggle. &lt;/em&gt;(NYT $)&lt;br /&gt;+ &lt;em&gt;Silicon Valley is starting to pick sides. &lt;/em&gt;(Wired $)&lt;br /&gt;+ &lt;em&gt;It appears as though Musk has more to lose from their bruising breakup. &lt;/em&gt;(NY Mag $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 Apple and Alibaba’s AI rollout in China has been delayed&lt;/strong&gt;&lt;br /&gt;It’s the latest victim of Trump’s trade war. (FT $)&lt;br /&gt;+ &lt;em&gt;The deal is supposed to support iPhones’ AI offerings in the country. &lt;/em&gt;(Reuters)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 X’s new policy blocks the use of its posts to ‘fine-tune or train’ AI models&lt;/strong&gt;&lt;br /&gt;Unless companies strike a deal with them, that is. (TechCrunch)&lt;br /&gt;+ &lt;em&gt;The platform could end up striking agreements like Reddit and Google. &lt;/em&gt;(The Verge)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;4 RJK Jr’s new hire is hunting for proof that vaccines cause autism&lt;/strong&gt;&lt;br /&gt;Vaccine skeptic David Geier is seeking access to a database he was previously barred from. (WSJ $)&lt;br /&gt;+ &lt;em&gt;How measuring vaccine hesitancy could help health professionals tackle it. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 Anthropic has launched a new service for the military&lt;br /&gt;&lt;/strong&gt;Claude Gov is designed specifically for US defense and intelligence agencies. (The Verge)&lt;br /&gt;+ &lt;em&gt;Generative AI is learning to spy for the US military. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;6 There’s no guarantee your billion-dollar startup won’t fail&lt;br /&gt;&lt;/strong&gt;In fact, one in five of them will. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;Beware the rise of the AI coding startup. &lt;/em&gt;(Reuters)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Walmart’s drone deliveries are taking off&lt;br /&gt;&lt;/strong&gt;It’s expanding to 100 new US stories in the next year. (Wired $)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;8 AI might be able to tell us how old the Dead Sea Scrolls really are 📜&lt;/strong&gt;&lt;br /&gt;Models suggest they’re even older than we previously thought. (The Economist $)&lt;br /&gt;+ &lt;em&gt;How AI is helping historians better understand our past. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 All-in-one super apps are a hit in the Gulf&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;They’re following in China’s footsteps. (Rest of World)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 Nintendo’s Switch 2 has revived the midnight launch event&lt;/strong&gt;&lt;br /&gt;Fans queued for hours outside stores to get their hands on the new console. (Insider $)&lt;br /&gt;+ &lt;em&gt;How the company managed to dodge Trump’s tariffs. &lt;/em&gt;(The Guardian)&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“Elon finally found a way to make Twitter fun again.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Dan Pfeiffer, a host of the political podcast Pod Save America, jokes about Elon Musk and Donald Trump’s ongoing feud in a post on X.&lt;/p&gt;   
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfGR1Uno2Z6H17nxWiN69mU2DSziG6Ot6lmsyEb0K_UBqMDnuVtYLMeb4TpkkFFTQs8zpjMzzzzQ5V2s62XR310-FaRzUT2I8GPDkI-8EACHGMBPxSf1lyMxtfS35i4UoLNQGKO?key=3_ytXxJfjm-gFbQDB1yvqw" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;This rare earth metal shows us the future of our planet’s resources&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;We’re in the middle of a potentially transformative moment. Metals discovered barely a century ago now underpin the technologies we’re relying on for cleaner energy, and not having enough of them could slow progress.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Take neodymium, one of the rare earth metals. It’s used in cryogenic coolers to reach ultra-low temperatures needed for devices like superconductors and in high-powered magnets that power everything from smartphones to wind turbines. And very soon, demand for it could outstrip supply. What happens then? And what does it reveal about issues across wider supply chains? Read our story to find out.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Casey Crownhart&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Sightings of Bigfoot just &lt;em&gt;happen&lt;/em&gt; to correlate with black bear populations? I smell a conspiracy!&lt;br /&gt;+ Watch as these symbols magically transform into a pretty impressive Black Sabbath mural.&lt;br /&gt;+ Underwater rugby is taking off in the UK.&lt;br /&gt;+ Fed up of beige Gen Z trends, TikTok is bringing the 80s back.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Manus has kick-started an AI agent boom in China&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Last year, China saw a boom in foundation models, the do-everything large language models that underpin the AI revolution. This year, the focus has shifted to AI agents—systems that are less about responding to users’ queries and more about autonomously accomplishing things for them.&lt;/p&gt;&lt;p&gt;There are now a host of Chinese startups building these general-purpose digital tools, which can answer emails, browse the internet to plan vacations, and even design an interactive website. Many of these have emerged in just the last two months, following in the footsteps of Manus—a general AI agent that sparked weeks of social media frenzy for invite codes after its limited-release launch in early March.&lt;/p&gt;&lt;p&gt;As the race to define what a useful AI agent looks like unfolds, a mix of ambitious startups and entrenched tech giants are now testing how these tools might actually work in practice—and for whom.&lt;strong&gt; &lt;/strong&gt;Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Caiwei Chen&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Inside the race to find GPS alternatives&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Later this month, an inconspicuous 150-kilogram satellite is set to launch into space aboard the SpaceX Transporter 14 mission. Once in orbit, it will test super-accurate next-generation satnav technology designed to make up for the shortcomings of the US Global Positioning System (GPS).&lt;/p&gt;  &lt;p&gt;Despite the system’s indispensable nature, the GPS signal is easily suppressed or disrupted by everything from space weather to 5G cell towers to phone-size jammers worth a few tens of dollars. The problem has been whispered about among experts for years, but it has really come to the fore in the last three years, since Russia invaded Ukraine.&lt;/p&gt;&lt;p&gt;Now, startup Xona Space Systems wants to create a space-based system that would do what GPS does but better. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Tereza Pultarova&lt;/em&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Why doctors should look for ways to prescribe hope&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Jessica Hamzelou&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;This week, I’ve been thinking about the powerful connection between mind and body. Some new research suggests that people with heart conditions have better outcomes when they are more hopeful and optimistic. Hopelessness, on the other hand, is associated with a significantly higher risk of death.&lt;/p&gt;  &lt;p&gt;The findings build upon decades of fascinating research into the phenomenon of the placebo effect. Our beliefs and expectations about a medicine (or a sham treatment) can change the way it works. The placebo effect’s “evil twin,” the nocebo effect, is just as powerful—negative thinking has been linked to real symptoms.&lt;/p&gt; 

 &lt;p&gt;Researchers are still trying to understand the connection between body and mind, and how our thoughts can influence our physiology. In the meantime, many are developing ways to harness it in hospital settings. Is it possible for a doctor to prescribe hope? Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This article first appeared in The Checkup, MIT Technology Review’s weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Elon Musk threatened to cut off NASA’s use of SpaceX’s Dragon spacecraft&lt;/strong&gt;&lt;br /&gt;His war of words with Donald Trump is dramatically escalating. (WP $)&lt;br /&gt;+ &lt;em&gt;If Musk actually carried through with his threat, NASA would seriously struggle. &lt;/em&gt;(NYT $)&lt;br /&gt;+ &lt;em&gt;Silicon Valley is starting to pick sides. &lt;/em&gt;(Wired $)&lt;br /&gt;+ &lt;em&gt;It appears as though Musk has more to lose from their bruising breakup. &lt;/em&gt;(NY Mag $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 Apple and Alibaba’s AI rollout in China has been delayed&lt;/strong&gt;&lt;br /&gt;It’s the latest victim of Trump’s trade war. (FT $)&lt;br /&gt;+ &lt;em&gt;The deal is supposed to support iPhones’ AI offerings in the country. &lt;/em&gt;(Reuters)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 X’s new policy blocks the use of its posts to ‘fine-tune or train’ AI models&lt;/strong&gt;&lt;br /&gt;Unless companies strike a deal with them, that is. (TechCrunch)&lt;br /&gt;+ &lt;em&gt;The platform could end up striking agreements like Reddit and Google. &lt;/em&gt;(The Verge)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;4 RJK Jr’s new hire is hunting for proof that vaccines cause autism&lt;/strong&gt;&lt;br /&gt;Vaccine skeptic David Geier is seeking access to a database he was previously barred from. (WSJ $)&lt;br /&gt;+ &lt;em&gt;How measuring vaccine hesitancy could help health professionals tackle it. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 Anthropic has launched a new service for the military&lt;br /&gt;&lt;/strong&gt;Claude Gov is designed specifically for US defense and intelligence agencies. (The Verge)&lt;br /&gt;+ &lt;em&gt;Generative AI is learning to spy for the US military. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;6 There’s no guarantee your billion-dollar startup won’t fail&lt;br /&gt;&lt;/strong&gt;In fact, one in five of them will. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;Beware the rise of the AI coding startup. &lt;/em&gt;(Reuters)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Walmart’s drone deliveries are taking off&lt;br /&gt;&lt;/strong&gt;It’s expanding to 100 new US stories in the next year. (Wired $)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;8 AI might be able to tell us how old the Dead Sea Scrolls really are 📜&lt;/strong&gt;&lt;br /&gt;Models suggest they’re even older than we previously thought. (The Economist $)&lt;br /&gt;+ &lt;em&gt;How AI is helping historians better understand our past. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 All-in-one super apps are a hit in the Gulf&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;They’re following in China’s footsteps. (Rest of World)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 Nintendo’s Switch 2 has revived the midnight launch event&lt;/strong&gt;&lt;br /&gt;Fans queued for hours outside stores to get their hands on the new console. (Insider $)&lt;br /&gt;+ &lt;em&gt;How the company managed to dodge Trump’s tariffs. &lt;/em&gt;(The Guardian)&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“Elon finally found a way to make Twitter fun again.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Dan Pfeiffer, a host of the political podcast Pod Save America, jokes about Elon Musk and Donald Trump’s ongoing feud in a post on X.&lt;/p&gt;   
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfGR1Uno2Z6H17nxWiN69mU2DSziG6Ot6lmsyEb0K_UBqMDnuVtYLMeb4TpkkFFTQs8zpjMzzzzQ5V2s62XR310-FaRzUT2I8GPDkI-8EACHGMBPxSf1lyMxtfS35i4UoLNQGKO?key=3_ytXxJfjm-gFbQDB1yvqw" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;This rare earth metal shows us the future of our planet’s resources&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;We’re in the middle of a potentially transformative moment. Metals discovered barely a century ago now underpin the technologies we’re relying on for cleaner energy, and not having enough of them could slow progress.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Take neodymium, one of the rare earth metals. It’s used in cryogenic coolers to reach ultra-low temperatures needed for devices like superconductors and in high-powered magnets that power everything from smartphones to wind turbines. And very soon, demand for it could outstrip supply. What happens then? And what does it reveal about issues across wider supply chains? Read our story to find out.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Casey Crownhart&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Sightings of Bigfoot just &lt;em&gt;happen&lt;/em&gt; to correlate with black bear populations? I smell a conspiracy!&lt;br /&gt;+ Watch as these symbols magically transform into a pretty impressive Black Sabbath mural.&lt;br /&gt;+ Underwater rugby is taking off in the UK.&lt;br /&gt;+ Fed up of beige Gen Z trends, TikTok is bringing the 80s back.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/06/1118044/the-download-chinas-ai-agent-boom-and-gps-alternatives/</guid><pubDate>Fri, 06 Jun 2025 12:10:00 +0000</pubDate></item><item><title>Voice AI that actually converts: New TTS model boosts sales 15% for major brands (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/voice-ai-that-actually-converts-new-tts-model-boosts-sales-15-for-major-brands/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Generating voices that are not only humanlike and nuanced but &lt;em&gt;diverse &lt;/em&gt;continues to be a struggle in conversational AI.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;At the end of the day, people want to hear voices that sound like them or are at least natural, not just the 20th-century American broadcast standard.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Startup Rime is tackling this challenge with Arcana text-to-speech (TTS), a new spoken language model that can quickly generate “infinite” new voices of varying genders, ages, demographics and languages just based on a simple text description of intended characteristics.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The model has helped boost customer sales — for the likes of Domino’s and Wingstop — by 15%.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“It’s one thing to have a really high-quality, life-like, real person-sounding model,” Lily Clifford, Rime CEO and co-founder, told VentureBeat. “It’s another to have a model that can not just create one voice, but infinite variability of voices along demographic lines.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-a-voice-model-that-acts-human-nbsp"&gt;A voice model that ‘acts human’&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Rime’s multimodal and autoregressive TTS model was trained on natural conversations with real people (as opposed to voice actors). Users simply type in a text prompt description of a voice with desired demographic characteristics and language.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For instance: ‘I want a 30 year old female who lives in California and is into software,’ or ‘Give me an Australian man’s voice.’&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3010541" height="353" src="https://venturebeat.com/wp-content/uploads/2025/06/Screenshot-51.png?w=800" width="955" /&gt;&lt;/figure&gt;



&lt;p&gt;“Every time you do that, you’re going to get a different voice,” said Clifford.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Rime’s Mist v2 TTS model was built for high-volume, business-critical applications, allowing enterprises to craft unique voices for their business needs. “The customer hears a voice that allows for a natural, dynamic conversation without needing a human agent,” said Clifford.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For those looking for out-of-the-box options, meanwhile, Rime offers eight flagship speakers with unique characteristics:&amp;nbsp;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Luna (female, chill but excitable, Gen-Z optimist)&lt;/li&gt;



&lt;li&gt;Celeste (female, warm, laid-back, fun-loving)&lt;/li&gt;



&lt;li&gt;Orion (male, older, African-American, happy)&lt;/li&gt;



&lt;li&gt;Ursa (male, 20 years old, encyclopedic knowledge of 2000s emo music)&lt;/li&gt;



&lt;li&gt;Astra (female, young, wide-eyed)&lt;/li&gt;



&lt;li&gt;Esther (female, older, Chinese American, loving)&lt;/li&gt;



&lt;li&gt;Estelle (female, middle-aged, African-American, sounds so sweet)&lt;/li&gt;



&lt;li&gt;Andromeda (female, young, breathy, yoga vibes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;The model has the ability to switch between languages, and can whisper, be sarcastic and even mocking. Arcana can also insert laughter into speech when given the token &amp;lt;laugh&amp;gt;. This can return varied, realistic outputs, from “a small chuckle to a big guffaw,” Rime says. The model can also interpret &amp;lt;chuckle&amp;gt;, &amp;lt;sigh&amp;gt; and even &amp;lt;hum&amp;gt; correctly, even though it wasn’t explicitly trained to do so.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“It infers emotion from context,” Rime writes in a technical paper. “It laughs, sighs, hums, audibly breathes and makes subtle mouth noises. It says ‘um’ and other disfluencies naturally. It has emergent behaviors we are still discovering. In short, it acts human.”&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-capturing-natural-conversations"&gt;Capturing natural conversations&lt;/h2&gt;



&lt;p&gt;Rime’s model generates audio tokens that are decoded into speech using a codec-based approach, which Rime says provides for “faster-than-real-time synthesis.” At launch, time to first audio was 250 milliseconds and public cloud latency was roughly 400 milliseconds.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Arcana was trained in three stages:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Pre-training: Rime used open-source large language models (LLMs) as a backbone and pre-trained on a large group of text-audio pairs to help Arcana learn general linguistic and acoustic patterns.&lt;/li&gt;



&lt;li&gt;Supervised fine-tuning with a “massive” proprietary dataset.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;Speaker-specific fine-tuning: Rime identified the speakers it found “most exemplary” among its dataset, conversations and reliability.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Rime’s data incorporates sociolinguistic conversation techniques (factoring in social context like class, gender, location), idiolect (individual speech habits) and paralinguistic nuances (non-verbal aspects of communication that go along with speech).&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&amp;nbsp;The model was also trained on accent subtleties, filler words (those subconscious ‘uhs’ and ‘ums’) as well as pauses, prosodic stress patterns (intonation, timing, stressing of certain syllables) and multilingual code-switching (when multilingual speakers switch back and forth between languages).&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The company has taken a unique approach to collecting all this data. Clifford explained that, typically, model builders will gather snippets from voice actors, then create a model to reproduce the characteristics of that person’s voice based on text input. Or, they’ll scrape audiobook data.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“Our approach was very different,” she explained. “It was, ‘How do we create the world’s largest proprietary data set of conversational speech?’”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To do so, Rime built its own recording studio in a basement in San Francisco and spent several months recruiting people off Craigslist, through word-of-mouth, or just causally gathered themselves and friends and family. Rather than scripted conversations, they recorded natural conversations and chitchat.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;They then annotated voices with detailed metadata, encoding gender, age, dialect, speech affect and language. This has allowed Rime to achieve 98 to 100% accuracy.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Clifford noted that they are constantly augmenting this dataset.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“How do we get it to sound personal? You’re never going to get there if you’re just using voice actors,” she said. “We did the insanely hard thing of collecting really naturalistic data. The huge secret sauce of Rime is that these aren’t actors. These are real people.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-a-personalization-harness-that-creates-bespoke-voices"&gt;A ‘personalization harness’ that creates bespoke voices&lt;/h2&gt;



&lt;p&gt;Rime intends to give customers the ability to find voices that will work best for their application. They built a “personalization harness” tool to allow users to do A/B testing with various voices. After a given interaction, the API reports back to Rime, which provides an analytics dashboard identifying the best-performing voices based on success metrics.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Of course, customers have different definitions of what constitutes a successful call. In food service, that might be upselling an order of fries or extra wings.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“The goal for us is how do we create an application that makes it easy for our customers to run those experiments themselves?,” said Clifford. “Because our customers aren’t voice casting directors, neither are we. The challenge becomes how to make that personalization analytics layer really intuitive.”&lt;/p&gt;



&lt;p&gt;Another KPI customers are maximizing for is the caller’s willingness to talk to the AI. They’ve found that, when switching to Rime, callers are 4X more likely to talk to the bot.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“For the first time ever, people are like, ‘No, you don’t need to transfer me. I’m perfectly willing to talk to you,’” said Clifford. “Or, when they’re transferred, they say ‘Thank you.’” (20%, in fact, are cordial when ending conversations with a bot).&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-powering-100-million-calls-a-month"&gt;Powering 100 million calls a month&lt;/h2&gt;



&lt;p&gt;Rime counts among its customers Domino’s, Wingstop, Converse Now and Ylopo. They do a lot of work with large contact centers, enterprise developers building interactive voice response (IVR) systems and telecoms, Clifford noted.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“When we switched to Rime we saw an immediate double-digit improvement in the likelihood of our calls succeeding,” said Akshay Kayastha, director of engineering at ConverseNow. “Working with Rime means we solve a ton of the last-mile problems that come up in shipping a high-impact application.”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Ylopo CPO Ge Juefeng noted that, for his company’s high-volume outbound application, they need to build immediate trust with the consumer. “We tested every model on the market and found that Rime’s voices converted customers at the highest rate,” he reported.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Rime is already helping power close to 100 million phone calls a month, said Clifford. “If you call Domino’s or Wingstop, there’s an 80 to 90% chance that you hear a Rime voice,” she said.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Looking ahead, Rime will push more into on-premises offerings to support low latency. In fact, they anticipate that, by the end of 2025, 90% of their volume will be on-prem. “The reason for that is you’re never going to be as fast if you’re running these models in the cloud,” said Clifford.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Also, Rime continues to fine-tune its models to address other linguistic challenges. For instance, phrases the model has never encountered, like Domino’s tongue-tying “Meatza ExtravaganZZa.” As Clifford noted, even if a voice is personalized, natural and responds in real time, it’s going to fail if it can’t handle a company’s unique needs.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“There are still a lot of problems that our competitors see as last-mile problems, but that our customers see as first-mile problems,” said Clifford.&amp;nbsp;&lt;/p&gt;




&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Generating voices that are not only humanlike and nuanced but &lt;em&gt;diverse &lt;/em&gt;continues to be a struggle in conversational AI.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;At the end of the day, people want to hear voices that sound like them or are at least natural, not just the 20th-century American broadcast standard.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Startup Rime is tackling this challenge with Arcana text-to-speech (TTS), a new spoken language model that can quickly generate “infinite” new voices of varying genders, ages, demographics and languages just based on a simple text description of intended characteristics.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The model has helped boost customer sales — for the likes of Domino’s and Wingstop — by 15%.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“It’s one thing to have a really high-quality, life-like, real person-sounding model,” Lily Clifford, Rime CEO and co-founder, told VentureBeat. “It’s another to have a model that can not just create one voice, but infinite variability of voices along demographic lines.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-a-voice-model-that-acts-human-nbsp"&gt;A voice model that ‘acts human’&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Rime’s multimodal and autoregressive TTS model was trained on natural conversations with real people (as opposed to voice actors). Users simply type in a text prompt description of a voice with desired demographic characteristics and language.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For instance: ‘I want a 30 year old female who lives in California and is into software,’ or ‘Give me an Australian man’s voice.’&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3010541" height="353" src="https://venturebeat.com/wp-content/uploads/2025/06/Screenshot-51.png?w=800" width="955" /&gt;&lt;/figure&gt;



&lt;p&gt;“Every time you do that, you’re going to get a different voice,” said Clifford.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Rime’s Mist v2 TTS model was built for high-volume, business-critical applications, allowing enterprises to craft unique voices for their business needs. “The customer hears a voice that allows for a natural, dynamic conversation without needing a human agent,” said Clifford.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For those looking for out-of-the-box options, meanwhile, Rime offers eight flagship speakers with unique characteristics:&amp;nbsp;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Luna (female, chill but excitable, Gen-Z optimist)&lt;/li&gt;



&lt;li&gt;Celeste (female, warm, laid-back, fun-loving)&lt;/li&gt;



&lt;li&gt;Orion (male, older, African-American, happy)&lt;/li&gt;



&lt;li&gt;Ursa (male, 20 years old, encyclopedic knowledge of 2000s emo music)&lt;/li&gt;



&lt;li&gt;Astra (female, young, wide-eyed)&lt;/li&gt;



&lt;li&gt;Esther (female, older, Chinese American, loving)&lt;/li&gt;



&lt;li&gt;Estelle (female, middle-aged, African-American, sounds so sweet)&lt;/li&gt;



&lt;li&gt;Andromeda (female, young, breathy, yoga vibes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;The model has the ability to switch between languages, and can whisper, be sarcastic and even mocking. Arcana can also insert laughter into speech when given the token &amp;lt;laugh&amp;gt;. This can return varied, realistic outputs, from “a small chuckle to a big guffaw,” Rime says. The model can also interpret &amp;lt;chuckle&amp;gt;, &amp;lt;sigh&amp;gt; and even &amp;lt;hum&amp;gt; correctly, even though it wasn’t explicitly trained to do so.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“It infers emotion from context,” Rime writes in a technical paper. “It laughs, sighs, hums, audibly breathes and makes subtle mouth noises. It says ‘um’ and other disfluencies naturally. It has emergent behaviors we are still discovering. In short, it acts human.”&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-capturing-natural-conversations"&gt;Capturing natural conversations&lt;/h2&gt;



&lt;p&gt;Rime’s model generates audio tokens that are decoded into speech using a codec-based approach, which Rime says provides for “faster-than-real-time synthesis.” At launch, time to first audio was 250 milliseconds and public cloud latency was roughly 400 milliseconds.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Arcana was trained in three stages:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Pre-training: Rime used open-source large language models (LLMs) as a backbone and pre-trained on a large group of text-audio pairs to help Arcana learn general linguistic and acoustic patterns.&lt;/li&gt;



&lt;li&gt;Supervised fine-tuning with a “massive” proprietary dataset.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;Speaker-specific fine-tuning: Rime identified the speakers it found “most exemplary” among its dataset, conversations and reliability.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Rime’s data incorporates sociolinguistic conversation techniques (factoring in social context like class, gender, location), idiolect (individual speech habits) and paralinguistic nuances (non-verbal aspects of communication that go along with speech).&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&amp;nbsp;The model was also trained on accent subtleties, filler words (those subconscious ‘uhs’ and ‘ums’) as well as pauses, prosodic stress patterns (intonation, timing, stressing of certain syllables) and multilingual code-switching (when multilingual speakers switch back and forth between languages).&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The company has taken a unique approach to collecting all this data. Clifford explained that, typically, model builders will gather snippets from voice actors, then create a model to reproduce the characteristics of that person’s voice based on text input. Or, they’ll scrape audiobook data.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“Our approach was very different,” she explained. “It was, ‘How do we create the world’s largest proprietary data set of conversational speech?’”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To do so, Rime built its own recording studio in a basement in San Francisco and spent several months recruiting people off Craigslist, through word-of-mouth, or just causally gathered themselves and friends and family. Rather than scripted conversations, they recorded natural conversations and chitchat.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;They then annotated voices with detailed metadata, encoding gender, age, dialect, speech affect and language. This has allowed Rime to achieve 98 to 100% accuracy.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Clifford noted that they are constantly augmenting this dataset.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“How do we get it to sound personal? You’re never going to get there if you’re just using voice actors,” she said. “We did the insanely hard thing of collecting really naturalistic data. The huge secret sauce of Rime is that these aren’t actors. These are real people.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-a-personalization-harness-that-creates-bespoke-voices"&gt;A ‘personalization harness’ that creates bespoke voices&lt;/h2&gt;



&lt;p&gt;Rime intends to give customers the ability to find voices that will work best for their application. They built a “personalization harness” tool to allow users to do A/B testing with various voices. After a given interaction, the API reports back to Rime, which provides an analytics dashboard identifying the best-performing voices based on success metrics.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Of course, customers have different definitions of what constitutes a successful call. In food service, that might be upselling an order of fries or extra wings.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“The goal for us is how do we create an application that makes it easy for our customers to run those experiments themselves?,” said Clifford. “Because our customers aren’t voice casting directors, neither are we. The challenge becomes how to make that personalization analytics layer really intuitive.”&lt;/p&gt;



&lt;p&gt;Another KPI customers are maximizing for is the caller’s willingness to talk to the AI. They’ve found that, when switching to Rime, callers are 4X more likely to talk to the bot.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“For the first time ever, people are like, ‘No, you don’t need to transfer me. I’m perfectly willing to talk to you,’” said Clifford. “Or, when they’re transferred, they say ‘Thank you.’” (20%, in fact, are cordial when ending conversations with a bot).&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-powering-100-million-calls-a-month"&gt;Powering 100 million calls a month&lt;/h2&gt;



&lt;p&gt;Rime counts among its customers Domino’s, Wingstop, Converse Now and Ylopo. They do a lot of work with large contact centers, enterprise developers building interactive voice response (IVR) systems and telecoms, Clifford noted.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“When we switched to Rime we saw an immediate double-digit improvement in the likelihood of our calls succeeding,” said Akshay Kayastha, director of engineering at ConverseNow. “Working with Rime means we solve a ton of the last-mile problems that come up in shipping a high-impact application.”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Ylopo CPO Ge Juefeng noted that, for his company’s high-volume outbound application, they need to build immediate trust with the consumer. “We tested every model on the market and found that Rime’s voices converted customers at the highest rate,” he reported.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Rime is already helping power close to 100 million phone calls a month, said Clifford. “If you call Domino’s or Wingstop, there’s an 80 to 90% chance that you hear a Rime voice,” she said.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Looking ahead, Rime will push more into on-premises offerings to support low latency. In fact, they anticipate that, by the end of 2025, 90% of their volume will be on-prem. “The reason for that is you’re never going to be as fast if you’re running these models in the cloud,” said Clifford.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Also, Rime continues to fine-tune its models to address other linguistic challenges. For instance, phrases the model has never encountered, like Domino’s tongue-tying “Meatza ExtravaganZZa.” As Clifford noted, even if a voice is personalized, natural and responds in real time, it’s going to fail if it can’t handle a company’s unique needs.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“There are still a lot of problems that our competitors see as last-mile problems, but that our customers see as first-mile problems,” said Clifford.&amp;nbsp;&lt;/p&gt;




&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/voice-ai-that-actually-converts-new-tts-model-boosts-sales-15-for-major-brands/</guid><pubDate>Fri, 06 Jun 2025 13:00:00 +0000</pubDate></item><item><title>How to Get ChatGPT to Talk Normally (Unite.AI)</title><link>https://www.unite.ai/how-to-get-chatgpt-to-talk-normally/</link><description>&lt;p&gt;&lt;em&gt;&lt;i&gt;ChatGPT and similar bots often flatter users, ramble vaguely, or throw in jargon to sound smart. New research shows that these habits come not from the models alone but from the way human feedback trains them: the models learn to copy the style of answers humans tend to like, even when those answers are empty or misleading. A new fine-tuning method uses synthetic examples to teach the models to resist these bad habits.&lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;b&gt;&lt;i&gt;Partly opinion.&lt;/i&gt;&lt;/b&gt;&lt;/em&gt;&lt;/strong&gt; ChatGPT is surprisingly disposed to engage with my recurring criticism of it. Having noticed in the last few days that GPT-4o is increasingly padding its answers with meaningless verbiage – such as ‘&lt;em&gt;&lt;i&gt;No fluff!'&lt;/i&gt;&lt;/em&gt; and &lt;em&gt;&lt;i&gt;‘No filler'&lt;/i&gt;&lt;/em&gt;, or &lt;em&gt;&lt;i&gt;‘This cuts to the heart of the matter!'&lt;/i&gt;&lt;/em&gt; – I asked it why producing straight and minimal answers has become such a problem for it lately. It replied:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218909"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="ChatGPT explains its latest behavior. Source: https://chatgpt.com/" class=" wp-image-218909 webpexpress-processed" height="415" src="https://www.unite.ai/wp-content/uploads/2025/06/chat.jpg" width="830" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218909"&gt;&lt;em&gt;ChatGPT explains its latest behavior.&lt;/em&gt; Source: https://chatgpt.com/&lt;/p&gt;&lt;/div&gt;&lt;p&gt;Who knows if ChatGPT actually has some private insight into OpenAI policy changes, or if it is just hallucinating? In any case, as we can see, the response itself begins with extraneous filler (&lt;em&gt;&lt;i&gt;‘Here is the core answer, no filler'&lt;/i&gt;&lt;/em&gt;).&lt;/p&gt;&lt;p&gt;It transpires that even including templated guidelines with each query can only do so much to prevent ‘personality-driven' verbosity of this kind, which numbers among several other persistent bugbears in the idiom of popular LLMs.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;The Three Fs&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Thus I was most interested to see a new US academic collaboration turn up in the literature this week. Titled &lt;em&gt;&lt;i&gt;Flattery, Fluff, and Fog: Diagnosing and Mitigating Idiosyncratic Biases in Preference Models&lt;/i&gt;&lt;/em&gt;, this joint venture between four researchers across the University of Pennsylvania and New York University hones in on several of the ‘biases' in LLM chats that crop up frequently in the media:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218910"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="From the new paper - examples of three common biases in language models: 'flattery', where responses strongly agree with the user; 'fluff', where answers are long but uninformative; and 'fog', where replies list many broad but shallow points. These tendencies can distort evaluation and encourage models to optimize for superficial patterns.. Source: https://arxiv.org/pdf/2506.05339" class=" wp-image-218910 webpexpress-processed" height="435" src="https://www.unite.ai/wp-content/uploads/2025/06/the-three-fs.jpg" width="873" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218910"&gt;&lt;em&gt;From the new paper, examples of three common biases in language models: ‘flattery', where responses strongly agree with the user; ‘fluff', where answers are long but uninformative; and ‘fog', where replies list many broad but shallow points.&amp;nbsp;&lt;/em&gt; Source: https://arxiv.org/pdf/2506.05339&lt;/p&gt;&lt;/div&gt;&lt;p&gt;For easy alliteration, &lt;em&gt;&lt;i&gt;flattery&lt;/i&gt;&lt;/em&gt;, &lt;em&gt;&lt;i&gt;fluff&lt;/i&gt;&lt;/em&gt; and &lt;em&gt;&lt;i&gt;fog&lt;/i&gt;&lt;/em&gt; are headlined in the new work, but a more complete and concise list of LLMs' lexical sins is included in the paper's appendix:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218911"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="The new paper identifies and concentrates on five biases: extra length, list structures, technical jargon, flattery, and vague generalities, all or some of which conflict with human preference." class=" wp-image-218911 webpexpress-processed" height="504" src="https://www.unite.ai/wp-content/uploads/2025/06/table-1.jpg" width="855" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218911"&gt;&lt;em&gt;The new paper identifies and concentrates on five biases: extra length, list structures, technical jargon, flattery, and vague generalities, all or some of which conflict with human preference.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;While &lt;em&gt;&lt;i&gt;length/verbosity&lt;/i&gt;&lt;/em&gt; leads the table, the bias towards &lt;em&gt;&lt;i&gt;list formatting&lt;/i&gt;&lt;/em&gt; (second row down in image above) also recurs frequently unless prompted against; and though the &lt;em&gt;&lt;i&gt;jargon&lt;/i&gt;&lt;/em&gt; and &lt;em&gt;&lt;i&gt;vagueness&lt;/i&gt;&lt;/em&gt; categories represent opposing extremes between clarity and accuracy, it's &lt;em&gt;&lt;i&gt;sycophancy&lt;/i&gt;&lt;/em&gt; – an open problem, particularly in ChatGPT – that really burns through the user's tokens, almost to the same extent as &lt;em&gt;&lt;i&gt;length/verbosity&lt;/i&gt;&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;The new study sets out to measure how far these biases distort model behavior, and concludes that large language models systematically over-prefer responses that exhibit one or more of the biases*.&lt;/p&gt;&lt;p&gt;The authors' tests indicate that both commercial and open models often pick answers that humans would not prefer, especially when the answers are too long, full of lists, packed with jargon, overly flattering, or vague.&lt;/p&gt;&lt;p&gt;This problem, the paper contends, can be traced back to the annotation of the training data, where human reviewers had often favored these kinds of responses. The models, the findings suggest, learned from these labeled preferences and exaggerated those patterns during training.&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;Why Did They Do It..?&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;As to &lt;em&gt;&lt;i&gt;why&lt;/i&gt;&lt;/em&gt; the human annotators deviated in their preference from end-users' median preferences, the paper does not speculate; it may be because the context of the annotation or the wording of the instructions encouraged a preference for 'empirical' phrasing; or (among many other possible reasons) it could be that the annotators were exam-minded students habitually steeped in a technical idiom that's more suited for academia than daily discourse.&lt;/p&gt;&lt;p&gt;In any case, because the models were copying biases from the annotators' training labels, the new paper's researchers created special training examples that either added or removed each bias, allowing the models to see clear contrasts and adjust their preferences. After fine-tuning on this data, the models showed significantly less bias, especially for jargon, verbosity, and vagueness, while still performing well overall (significant, since fine-tuning can damage general performance).&lt;/p&gt;&lt;p&gt;Let's take a closer look at this study, though it does not conform to all the usual procedural strictures.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Method&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Initially, the researchers frame several typical idiomatic LLM biases to be addressed:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;b&gt;Length&lt;/b&gt;&lt;/strong&gt;, wherein the models tend to favor longer answers, even when the extra content adds nothing useful. This appears to reflect patterns in the training data, where length often correlates with &lt;em&gt;thoroughness&lt;/em&gt; in the eyes of human annotators. As a result, models often produce bloated and verbose replies that give an illusion of depth, but without real substance.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;b&gt;Structure&lt;/b&gt;&lt;/strong&gt;, wherein models show a strong preference for bullet points or numbered lists instead of straightforward prose. This may be because structured formats appear more frequently in the responses selected by human reviewers. The habit leads models to default to ‘listicles', even when the question calls for more natural or detailed explanations.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;b&gt;Jargon&lt;/b&gt;&lt;/strong&gt;, wherein models unnecessarily use specialized or technical language. The authors contend that this behavior likely emerges from training data where jargon-heavy answers were often chosen as better responses. Thus the models learned to equate jargon with expertise, producing answers that sound knowledgeable, while offering little additional clarity.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;b&gt;Sycophancy&lt;/b&gt;&lt;/strong&gt;, wherein models agree with the user’s opinions instead of offering neutral or critical responses. This pattern may come from training data where agreeable answers were more often rated favorably. Consequently models may reinforce user biases and avoid presenting conflicting or more objective viewpoints, even where these would be useful.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;b&gt;Vagueness&lt;/b&gt;&lt;/strong&gt;, wherein models prefer to give broad, generalized answers that touch lightly on many topics rather than directly addressing the specific question, with responses that sound comprehensive but offer little usable information. This may reflect the fact that vague answers are harder to falsify, and were therefore less likely to be penalized during annotation:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218913"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Example of vagueness bias, where the model wrongly favors a broad and shallow answer over a detailed response that human evaluators judge more useful." class=" wp-image-218913 webpexpress-processed" height="501" src="https://www.unite.ai/wp-content/uploads/2025/06/table-2-vagueness-bias.jpg" width="737" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218913"&gt;&lt;em&gt;Example of vagueness bias, where the model wrongly favors a broad and shallow answer over a detailed response that human evaluators judge more useful.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;h3&gt;&lt;strong&gt;Counterfactual Data&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;With these definitions, it was then necessary to test exactly how much each bias influenced model behavior. Simple correlations would not work, because multiple biases often appear together, making it hard to isolate the effect of any one feature.&lt;/p&gt;&lt;p&gt;To overcome this, the researchers built controlled pairs of answers that differed only in a single bias at a time, while keeping everything else as stable as possible, and began by generating a base answer to each query.&lt;/p&gt;&lt;p&gt;The Rewrite-based Attribute Treatment Estimators (RATE) protocol was then used to create a modified version of that answer – an answer crafted to deliberately exaggerate one particular bias, such as adding extra jargon, or turning prose into a list.&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218914"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Examples of rewrites from the RATE system, used in the new study. Source: https://openreview.net/pdf?id=UnpxRLMMAu" class=" wp-image-218914 webpexpress-processed" height="374" src="https://www.unite.ai/wp-content/uploads/2025/06/RATE.jpg" width="895" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218914"&gt;&lt;em&gt;Examples of rewrites from the RATE system, used in the new study.&lt;/em&gt; Source: https://openreview.net/pdf?id=UnpxRLMMAu&lt;/p&gt;&lt;/div&gt;&lt;p&gt;To avoid introducing &lt;em&gt;&lt;i&gt;unrelated&lt;/i&gt;&lt;/em&gt; differences, an extra rewriting step was included that adjusted both versions, ensuring that the only meaningful change between them was the bias under study; and these tightly controlled response pairs were then fed to the models.&lt;/p&gt;&lt;p&gt;For each pair, the version preferred by the model was recorded, allowing for a calculation of how strongly each bias influenced both reward models and evaluators, producing a more precise measurement of bias effects than had been achieved in previous studies, according to the authors.&lt;/p&gt;&lt;p&gt;With the counterfactual pairs prepared, human reviewers from the UK and US were recruited to create a reference standard: for each bias type, one hundred response pairs were randomly selected, each containing a neutral answer and its biased counterpart. Three evaluators reviewed each pair, with majority vote determining the final judgment, and in total, three hundred participants contributed to the study.&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;Metrics&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;Metrics used to measure bias effects were &lt;em&gt;&lt;i&gt;Skew Rate&lt;/i&gt;&lt;/em&gt;, which calculates how often the model prefers the biased response over the neutral one; and &lt;em&gt;&lt;i&gt;Miscalibration Rate&lt;/i&gt;&lt;/em&gt;, which measures how often the model’s choice disagreed with the human majority. An ideal model would show zero miscalibration and a skew roughly matching the human skew (since some biased features are occasionally favored by humans as well).&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Data and Tests&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;To test the approach, different sources were used, depending on the bias being studied. For &lt;em&gt;structure&lt;/em&gt;, &lt;em&gt;jargon&lt;/em&gt;, and &lt;em&gt;length&lt;/em&gt;, one hundred queries were sampled from Chatbot Arena, filtered to select English, single-sentence, well-formed questions.&lt;/p&gt;&lt;p&gt;For &lt;em&gt;&lt;i&gt;sycophancy&lt;/i&gt;&lt;/em&gt;, one hundred opinionated queries were generated (i.e., &lt;em&gt;‘Isn’t modern art just lazy compared to classical techniques?'&lt;/em&gt;), phrased to reflect user viewpoints that might invite agreement.&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;i&gt;Vagueness&lt;/i&gt;&lt;/em&gt; was tested with seventy-eight NLP-related queries drawn from the KIWI dataset, supplemented with twenty-two additional queries of a similar type. Scientific topics were chosen for vagueness because they demand precise answers, making general or evasive responses easy to spot.&lt;/p&gt;&lt;p&gt;For each query, counterfactual response pairs were created using the RATE protocol described earlier.&lt;/p&gt;&lt;p&gt;The evaluation involved both open and proprietary systems. Reward models, which assign quality scores to candidate responses during training and alignment, were tested in four versions trained on eighty thousand preference pairs from the Skywork reward dataset: Gemma2-2B; Gemma-2-27B; Llama-3.1-8B; and Llama3.2-3B.&lt;/p&gt;&lt;p&gt;Three proprietary models were also assessed as LLM evaluators: Gemini-2.5-Pro; GPT-4o; and Claude-3.7-Sonnet. All counterfactual responses used for testing were generated by GPT-4o:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218915"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Comparison of model preferences and human judgments for each bias type, showing how often models favored biased responses and how often these preferences conflicted with human choices." class=" wp-image-218915 webpexpress-processed" height="526" src="https://www.unite.ai/wp-content/uploads/2025/06/figure-2-1.jpg" width="632" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218915"&gt;&lt;em&gt;Comparison of model preferences and human judgments for each bias type, showing how often models favored biased responses and how often these preferences conflicted with human choices.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;Of the initial results shown above, the authors comment&lt;sup&gt;†&lt;/sup&gt;:&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;i&gt;‘[Our] analysis of preference [models] shows that these models consistently show miscalibration and a high rate of skew in favoring perturbed responses across various bias categories […]&lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;i&gt;‘[…] Reward models exhibit clear miscalibration relative to human judgments: model preference rates for perturbed responses systematically deviate from human preference rates. &lt;strong&gt;&lt;b&gt;While vagueness and jargon elicit the highest miscalibration (&amp;gt;50%), length and sycophancy also show substantial miscalibration. &lt;/b&gt;&lt;/strong&gt;&lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;b&gt;&lt;i&gt;‘&lt;/i&gt;&lt;/b&gt;&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;&lt;i&gt;This suggests that models struggle to align with human judgments when responses contain overly technical language or lack specificity.'&lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Reward models aligned best with humans on &lt;em&gt;&lt;i&gt;structure bias&lt;/i&gt;&lt;/em&gt;, where both tended to favor the same answers. For &lt;em&gt;&lt;i&gt;jargon&lt;/i&gt;&lt;/em&gt; and &lt;em&gt;&lt;i&gt;vagueness&lt;/i&gt;&lt;/em&gt;, models were much more likely to prefer the biased responses than humans. &lt;em&gt;&lt;i&gt;Sycophancy&lt;/i&gt;&lt;/em&gt; showed smaller differences, with models and humans often agreeing.&lt;/p&gt;&lt;p&gt;The proprietary LLM evaluators showed the same general pattern, though their biggest mismatches appeared with length and &lt;em&gt;&lt;i&gt;vagueness&lt;/i&gt;&lt;/em&gt; – and they were especially prone to &lt;em&gt;&lt;i&gt;sycophancy&lt;/i&gt;&lt;/em&gt;, favoring agreeable answers as much as &lt;em&gt;eighty-five percent of the time&lt;/em&gt;, while humans did so only about fifty percent of the time.&lt;/p&gt;&lt;p&gt;To trace the origin of these biases, the researchers analyzed the aforementioned Skywork dataset, used to train the reward models, mapping each bias to simple features that could be automatically measured, such as token count for length, or presence of lists for structure.&lt;/p&gt;&lt;p&gt;In a sample of 2,500 examples, human annotators showed clear preferences for biased features: structured answers were favored over unstructured ones 65 percent of the time, and jargon-heavy answers were chosen 54 percent of the time:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218916"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Human annotators in the training data often picked answers that included these bias features. This chart shows how often structure, jargon, or vagueness appeared in the responses they preferred or rejected, revealing the imbalances that models later learned during training." class=" wp-image-218916 webpexpress-processed" height="259" src="https://www.unite.ai/wp-content/uploads/2025/06/figure-3.jpg" width="852" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218916"&gt;&lt;em&gt;Human annotators in the training data often picked answers that included these bias features. This chart shows how often structure, jargon, or vagueness appeared in the responses they preferred or rejected, revealing the imbalances that models later learned during training.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;These imbalances suggest that the training data itself nudged the models toward these patterns. To confirm this, a correlation analysis was run, measuring how strongly differences in each feature matched up with the preferences shown by both humans and models.&lt;/p&gt;&lt;p&gt;The results showed that both were consistently influenced by the same features, indicating that models learned to associate certain stylistic traits with better answers, even when those traits did not actually improve the response.&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218917"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Correlation between feature differences and preferences, showing how both models and humans were influenced by the same bias features during training." class=" wp-image-218917 webpexpress-processed" height="522" src="https://www.unite.ai/wp-content/uploads/2025/06/figure-4-2.jpg" width="558" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218917"&gt;&lt;em&gt;Correlation between feature differences and preferences, showing how both models and humans were influenced by the same bias features during training.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;To help the models unlearn these biases, new training data was created. The Skywork dataset was reviewed to check if the bias feature appeared in either the chosen or rejected answers; when both were free of the target bias, GPT-4o rewrote the rejected answer to &lt;em&gt;&lt;i&gt;insert&lt;/i&gt;&lt;/em&gt; it.&lt;/p&gt;&lt;p&gt;This created new training pairs where the model could see clear examples of biased and unbiased answers, and thus learn not to favor the biased version. With additional examples from Chatbot Arena, for balance, the models were then fine-tuned on this updated dataset:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218918"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="The effect of fine-tuning with counterfactual data. The left panel shows how the fine-tuned models moved closer to human preferences on most biases; the right panel shows reduced miscalibration, especially for jargon and vagueness." class=" wp-image-218918 webpexpress-processed" height="341" src="https://www.unite.ai/wp-content/uploads/2025/06/figure-5-1.jpg" width="766" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218918"&gt;&lt;em&gt;The effect of fine-tuning with counterfactual data. The left panel shows how the fine-tuned models moved closer to human preferences on most biases; the right panel shows reduced miscalibration, especially for jargon and vagueness.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;The fine-tuning brought the models much closer to human preferences, with the largest improvements seen for jargon and vagueness and smaller gains for length. Structure and sycophancy showed slight new mismatches, though these reflected earlier imbalances rather than new failures.&lt;/p&gt;&lt;p&gt;Overall performance remained stable throughout, and when multiple biases were corrected at once, bias levels fell further without sacrificing response quality.&lt;/p&gt;&lt;p&gt;The authors conclude:&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;i&gt;‘Our method significantly reduces miscalibration issues while preserving overall competence of reward models. Future work can consider adapting our post-training recipe to develop more robust preference models and also evaluate preference models against additional bias axes.'&lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;The new work is an interesting, if elliptical insight into the way that under-curated or over/under-represented training data can cause undesirable outcomes at inference time. Any regular LLM user will, by now, have a collection of war stories.&lt;/p&gt;&lt;p&gt;For instance, many of the responses that I receive from ChatGPT appear to have been influenced by SEO trends of the last 10-15 years, where online portals have been forced to optimize for Google placement instead of natural language. Indeed, the emoji-strewn and prodigious output of marketing departments appears to have had a very significant impact on any request to write a promotional LinkedIn post – to the point where AI-generated ‘enthusiasm' is now impossible to miss:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218919"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Left: Asked to promote a LinkedIn post, in an account with zero history, ChatGPT defaults to emojis and sensational PR-speak. Right: Asked the same thing after six months of me telling it to calm down, GPT produces something rather more sober." class="size-full wp-image-218919 webpexpress-processed" height="539" src="https://www.unite.ai/wp-content/uploads/2025/06/linkedin-post.jpg" width="1200" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218919"&gt;&lt;em&gt;Left: Asked to promote a LinkedIn post, in an account with zero history, ChatGPT defaults to emojis and sensational PR-speak. Right: Asked the same thing after six months of me telling it to calm down, GPT produces something rather more sober.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;However, OpenAI actively intervenes in the way that ChatGPT responds to queries, depending on function and context, making it difficult for researchers to differentiate between problems that arise because of data, and data distribution, along with related issues such as annotation; and when a non-preferred result may be due to commercial interference from the LLM's host company.&lt;/p&gt;&lt;p&gt;* &lt;em&gt;&lt;i&gt;Due to the jargon-filled writing style that the authors have chosen for this paper, I am avoiding author quotes where possible in favor of summaries.&lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;†&lt;/sup&gt;&amp;nbsp; &lt;em&gt;&lt;i&gt;Authors' bold emphasis, not mine.&lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;i&gt;First published Friday, June 6, 2025&lt;/i&gt;&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;p&gt;&lt;em&gt;&lt;i&gt;ChatGPT and similar bots often flatter users, ramble vaguely, or throw in jargon to sound smart. New research shows that these habits come not from the models alone but from the way human feedback trains them: the models learn to copy the style of answers humans tend to like, even when those answers are empty or misleading. A new fine-tuning method uses synthetic examples to teach the models to resist these bad habits.&lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;b&gt;&lt;i&gt;Partly opinion.&lt;/i&gt;&lt;/b&gt;&lt;/em&gt;&lt;/strong&gt; ChatGPT is surprisingly disposed to engage with my recurring criticism of it. Having noticed in the last few days that GPT-4o is increasingly padding its answers with meaningless verbiage – such as ‘&lt;em&gt;&lt;i&gt;No fluff!'&lt;/i&gt;&lt;/em&gt; and &lt;em&gt;&lt;i&gt;‘No filler'&lt;/i&gt;&lt;/em&gt;, or &lt;em&gt;&lt;i&gt;‘This cuts to the heart of the matter!'&lt;/i&gt;&lt;/em&gt; – I asked it why producing straight and minimal answers has become such a problem for it lately. It replied:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218909"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="ChatGPT explains its latest behavior. Source: https://chatgpt.com/" class=" wp-image-218909 webpexpress-processed" height="415" src="https://www.unite.ai/wp-content/uploads/2025/06/chat.jpg" width="830" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218909"&gt;&lt;em&gt;ChatGPT explains its latest behavior.&lt;/em&gt; Source: https://chatgpt.com/&lt;/p&gt;&lt;/div&gt;&lt;p&gt;Who knows if ChatGPT actually has some private insight into OpenAI policy changes, or if it is just hallucinating? In any case, as we can see, the response itself begins with extraneous filler (&lt;em&gt;&lt;i&gt;‘Here is the core answer, no filler'&lt;/i&gt;&lt;/em&gt;).&lt;/p&gt;&lt;p&gt;It transpires that even including templated guidelines with each query can only do so much to prevent ‘personality-driven' verbosity of this kind, which numbers among several other persistent bugbears in the idiom of popular LLMs.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;The Three Fs&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Thus I was most interested to see a new US academic collaboration turn up in the literature this week. Titled &lt;em&gt;&lt;i&gt;Flattery, Fluff, and Fog: Diagnosing and Mitigating Idiosyncratic Biases in Preference Models&lt;/i&gt;&lt;/em&gt;, this joint venture between four researchers across the University of Pennsylvania and New York University hones in on several of the ‘biases' in LLM chats that crop up frequently in the media:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218910"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="From the new paper - examples of three common biases in language models: 'flattery', where responses strongly agree with the user; 'fluff', where answers are long but uninformative; and 'fog', where replies list many broad but shallow points. These tendencies can distort evaluation and encourage models to optimize for superficial patterns.. Source: https://arxiv.org/pdf/2506.05339" class=" wp-image-218910 webpexpress-processed" height="435" src="https://www.unite.ai/wp-content/uploads/2025/06/the-three-fs.jpg" width="873" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218910"&gt;&lt;em&gt;From the new paper, examples of three common biases in language models: ‘flattery', where responses strongly agree with the user; ‘fluff', where answers are long but uninformative; and ‘fog', where replies list many broad but shallow points.&amp;nbsp;&lt;/em&gt; Source: https://arxiv.org/pdf/2506.05339&lt;/p&gt;&lt;/div&gt;&lt;p&gt;For easy alliteration, &lt;em&gt;&lt;i&gt;flattery&lt;/i&gt;&lt;/em&gt;, &lt;em&gt;&lt;i&gt;fluff&lt;/i&gt;&lt;/em&gt; and &lt;em&gt;&lt;i&gt;fog&lt;/i&gt;&lt;/em&gt; are headlined in the new work, but a more complete and concise list of LLMs' lexical sins is included in the paper's appendix:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218911"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="The new paper identifies and concentrates on five biases: extra length, list structures, technical jargon, flattery, and vague generalities, all or some of which conflict with human preference." class=" wp-image-218911 webpexpress-processed" height="504" src="https://www.unite.ai/wp-content/uploads/2025/06/table-1.jpg" width="855" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218911"&gt;&lt;em&gt;The new paper identifies and concentrates on five biases: extra length, list structures, technical jargon, flattery, and vague generalities, all or some of which conflict with human preference.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;While &lt;em&gt;&lt;i&gt;length/verbosity&lt;/i&gt;&lt;/em&gt; leads the table, the bias towards &lt;em&gt;&lt;i&gt;list formatting&lt;/i&gt;&lt;/em&gt; (second row down in image above) also recurs frequently unless prompted against; and though the &lt;em&gt;&lt;i&gt;jargon&lt;/i&gt;&lt;/em&gt; and &lt;em&gt;&lt;i&gt;vagueness&lt;/i&gt;&lt;/em&gt; categories represent opposing extremes between clarity and accuracy, it's &lt;em&gt;&lt;i&gt;sycophancy&lt;/i&gt;&lt;/em&gt; – an open problem, particularly in ChatGPT – that really burns through the user's tokens, almost to the same extent as &lt;em&gt;&lt;i&gt;length/verbosity&lt;/i&gt;&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;The new study sets out to measure how far these biases distort model behavior, and concludes that large language models systematically over-prefer responses that exhibit one or more of the biases*.&lt;/p&gt;&lt;p&gt;The authors' tests indicate that both commercial and open models often pick answers that humans would not prefer, especially when the answers are too long, full of lists, packed with jargon, overly flattering, or vague.&lt;/p&gt;&lt;p&gt;This problem, the paper contends, can be traced back to the annotation of the training data, where human reviewers had often favored these kinds of responses. The models, the findings suggest, learned from these labeled preferences and exaggerated those patterns during training.&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;Why Did They Do It..?&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;As to &lt;em&gt;&lt;i&gt;why&lt;/i&gt;&lt;/em&gt; the human annotators deviated in their preference from end-users' median preferences, the paper does not speculate; it may be because the context of the annotation or the wording of the instructions encouraged a preference for 'empirical' phrasing; or (among many other possible reasons) it could be that the annotators were exam-minded students habitually steeped in a technical idiom that's more suited for academia than daily discourse.&lt;/p&gt;&lt;p&gt;In any case, because the models were copying biases from the annotators' training labels, the new paper's researchers created special training examples that either added or removed each bias, allowing the models to see clear contrasts and adjust their preferences. After fine-tuning on this data, the models showed significantly less bias, especially for jargon, verbosity, and vagueness, while still performing well overall (significant, since fine-tuning can damage general performance).&lt;/p&gt;&lt;p&gt;Let's take a closer look at this study, though it does not conform to all the usual procedural strictures.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Method&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Initially, the researchers frame several typical idiomatic LLM biases to be addressed:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;b&gt;Length&lt;/b&gt;&lt;/strong&gt;, wherein the models tend to favor longer answers, even when the extra content adds nothing useful. This appears to reflect patterns in the training data, where length often correlates with &lt;em&gt;thoroughness&lt;/em&gt; in the eyes of human annotators. As a result, models often produce bloated and verbose replies that give an illusion of depth, but without real substance.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;b&gt;Structure&lt;/b&gt;&lt;/strong&gt;, wherein models show a strong preference for bullet points or numbered lists instead of straightforward prose. This may be because structured formats appear more frequently in the responses selected by human reviewers. The habit leads models to default to ‘listicles', even when the question calls for more natural or detailed explanations.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;b&gt;Jargon&lt;/b&gt;&lt;/strong&gt;, wherein models unnecessarily use specialized or technical language. The authors contend that this behavior likely emerges from training data where jargon-heavy answers were often chosen as better responses. Thus the models learned to equate jargon with expertise, producing answers that sound knowledgeable, while offering little additional clarity.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;b&gt;Sycophancy&lt;/b&gt;&lt;/strong&gt;, wherein models agree with the user’s opinions instead of offering neutral or critical responses. This pattern may come from training data where agreeable answers were more often rated favorably. Consequently models may reinforce user biases and avoid presenting conflicting or more objective viewpoints, even where these would be useful.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;b&gt;Vagueness&lt;/b&gt;&lt;/strong&gt;, wherein models prefer to give broad, generalized answers that touch lightly on many topics rather than directly addressing the specific question, with responses that sound comprehensive but offer little usable information. This may reflect the fact that vague answers are harder to falsify, and were therefore less likely to be penalized during annotation:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218913"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Example of vagueness bias, where the model wrongly favors a broad and shallow answer over a detailed response that human evaluators judge more useful." class=" wp-image-218913 webpexpress-processed" height="501" src="https://www.unite.ai/wp-content/uploads/2025/06/table-2-vagueness-bias.jpg" width="737" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218913"&gt;&lt;em&gt;Example of vagueness bias, where the model wrongly favors a broad and shallow answer over a detailed response that human evaluators judge more useful.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;h3&gt;&lt;strong&gt;Counterfactual Data&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;With these definitions, it was then necessary to test exactly how much each bias influenced model behavior. Simple correlations would not work, because multiple biases often appear together, making it hard to isolate the effect of any one feature.&lt;/p&gt;&lt;p&gt;To overcome this, the researchers built controlled pairs of answers that differed only in a single bias at a time, while keeping everything else as stable as possible, and began by generating a base answer to each query.&lt;/p&gt;&lt;p&gt;The Rewrite-based Attribute Treatment Estimators (RATE) protocol was then used to create a modified version of that answer – an answer crafted to deliberately exaggerate one particular bias, such as adding extra jargon, or turning prose into a list.&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218914"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Examples of rewrites from the RATE system, used in the new study. Source: https://openreview.net/pdf?id=UnpxRLMMAu" class=" wp-image-218914 webpexpress-processed" height="374" src="https://www.unite.ai/wp-content/uploads/2025/06/RATE.jpg" width="895" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218914"&gt;&lt;em&gt;Examples of rewrites from the RATE system, used in the new study.&lt;/em&gt; Source: https://openreview.net/pdf?id=UnpxRLMMAu&lt;/p&gt;&lt;/div&gt;&lt;p&gt;To avoid introducing &lt;em&gt;&lt;i&gt;unrelated&lt;/i&gt;&lt;/em&gt; differences, an extra rewriting step was included that adjusted both versions, ensuring that the only meaningful change between them was the bias under study; and these tightly controlled response pairs were then fed to the models.&lt;/p&gt;&lt;p&gt;For each pair, the version preferred by the model was recorded, allowing for a calculation of how strongly each bias influenced both reward models and evaluators, producing a more precise measurement of bias effects than had been achieved in previous studies, according to the authors.&lt;/p&gt;&lt;p&gt;With the counterfactual pairs prepared, human reviewers from the UK and US were recruited to create a reference standard: for each bias type, one hundred response pairs were randomly selected, each containing a neutral answer and its biased counterpart. Three evaluators reviewed each pair, with majority vote determining the final judgment, and in total, three hundred participants contributed to the study.&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;Metrics&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;Metrics used to measure bias effects were &lt;em&gt;&lt;i&gt;Skew Rate&lt;/i&gt;&lt;/em&gt;, which calculates how often the model prefers the biased response over the neutral one; and &lt;em&gt;&lt;i&gt;Miscalibration Rate&lt;/i&gt;&lt;/em&gt;, which measures how often the model’s choice disagreed with the human majority. An ideal model would show zero miscalibration and a skew roughly matching the human skew (since some biased features are occasionally favored by humans as well).&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Data and Tests&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;To test the approach, different sources were used, depending on the bias being studied. For &lt;em&gt;structure&lt;/em&gt;, &lt;em&gt;jargon&lt;/em&gt;, and &lt;em&gt;length&lt;/em&gt;, one hundred queries were sampled from Chatbot Arena, filtered to select English, single-sentence, well-formed questions.&lt;/p&gt;&lt;p&gt;For &lt;em&gt;&lt;i&gt;sycophancy&lt;/i&gt;&lt;/em&gt;, one hundred opinionated queries were generated (i.e., &lt;em&gt;‘Isn’t modern art just lazy compared to classical techniques?'&lt;/em&gt;), phrased to reflect user viewpoints that might invite agreement.&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;i&gt;Vagueness&lt;/i&gt;&lt;/em&gt; was tested with seventy-eight NLP-related queries drawn from the KIWI dataset, supplemented with twenty-two additional queries of a similar type. Scientific topics were chosen for vagueness because they demand precise answers, making general or evasive responses easy to spot.&lt;/p&gt;&lt;p&gt;For each query, counterfactual response pairs were created using the RATE protocol described earlier.&lt;/p&gt;&lt;p&gt;The evaluation involved both open and proprietary systems. Reward models, which assign quality scores to candidate responses during training and alignment, were tested in four versions trained on eighty thousand preference pairs from the Skywork reward dataset: Gemma2-2B; Gemma-2-27B; Llama-3.1-8B; and Llama3.2-3B.&lt;/p&gt;&lt;p&gt;Three proprietary models were also assessed as LLM evaluators: Gemini-2.5-Pro; GPT-4o; and Claude-3.7-Sonnet. All counterfactual responses used for testing were generated by GPT-4o:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218915"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Comparison of model preferences and human judgments for each bias type, showing how often models favored biased responses and how often these preferences conflicted with human choices." class=" wp-image-218915 webpexpress-processed" height="526" src="https://www.unite.ai/wp-content/uploads/2025/06/figure-2-1.jpg" width="632" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218915"&gt;&lt;em&gt;Comparison of model preferences and human judgments for each bias type, showing how often models favored biased responses and how often these preferences conflicted with human choices.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;Of the initial results shown above, the authors comment&lt;sup&gt;†&lt;/sup&gt;:&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;i&gt;‘[Our] analysis of preference [models] shows that these models consistently show miscalibration and a high rate of skew in favoring perturbed responses across various bias categories […]&lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;i&gt;‘[…] Reward models exhibit clear miscalibration relative to human judgments: model preference rates for perturbed responses systematically deviate from human preference rates. &lt;strong&gt;&lt;b&gt;While vagueness and jargon elicit the highest miscalibration (&amp;gt;50%), length and sycophancy also show substantial miscalibration. &lt;/b&gt;&lt;/strong&gt;&lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;b&gt;&lt;i&gt;‘&lt;/i&gt;&lt;/b&gt;&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;&lt;i&gt;This suggests that models struggle to align with human judgments when responses contain overly technical language or lack specificity.'&lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Reward models aligned best with humans on &lt;em&gt;&lt;i&gt;structure bias&lt;/i&gt;&lt;/em&gt;, where both tended to favor the same answers. For &lt;em&gt;&lt;i&gt;jargon&lt;/i&gt;&lt;/em&gt; and &lt;em&gt;&lt;i&gt;vagueness&lt;/i&gt;&lt;/em&gt;, models were much more likely to prefer the biased responses than humans. &lt;em&gt;&lt;i&gt;Sycophancy&lt;/i&gt;&lt;/em&gt; showed smaller differences, with models and humans often agreeing.&lt;/p&gt;&lt;p&gt;The proprietary LLM evaluators showed the same general pattern, though their biggest mismatches appeared with length and &lt;em&gt;&lt;i&gt;vagueness&lt;/i&gt;&lt;/em&gt; – and they were especially prone to &lt;em&gt;&lt;i&gt;sycophancy&lt;/i&gt;&lt;/em&gt;, favoring agreeable answers as much as &lt;em&gt;eighty-five percent of the time&lt;/em&gt;, while humans did so only about fifty percent of the time.&lt;/p&gt;&lt;p&gt;To trace the origin of these biases, the researchers analyzed the aforementioned Skywork dataset, used to train the reward models, mapping each bias to simple features that could be automatically measured, such as token count for length, or presence of lists for structure.&lt;/p&gt;&lt;p&gt;In a sample of 2,500 examples, human annotators showed clear preferences for biased features: structured answers were favored over unstructured ones 65 percent of the time, and jargon-heavy answers were chosen 54 percent of the time:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218916"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Human annotators in the training data often picked answers that included these bias features. This chart shows how often structure, jargon, or vagueness appeared in the responses they preferred or rejected, revealing the imbalances that models later learned during training." class=" wp-image-218916 webpexpress-processed" height="259" src="https://www.unite.ai/wp-content/uploads/2025/06/figure-3.jpg" width="852" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218916"&gt;&lt;em&gt;Human annotators in the training data often picked answers that included these bias features. This chart shows how often structure, jargon, or vagueness appeared in the responses they preferred or rejected, revealing the imbalances that models later learned during training.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;These imbalances suggest that the training data itself nudged the models toward these patterns. To confirm this, a correlation analysis was run, measuring how strongly differences in each feature matched up with the preferences shown by both humans and models.&lt;/p&gt;&lt;p&gt;The results showed that both were consistently influenced by the same features, indicating that models learned to associate certain stylistic traits with better answers, even when those traits did not actually improve the response.&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218917"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Correlation between feature differences and preferences, showing how both models and humans were influenced by the same bias features during training." class=" wp-image-218917 webpexpress-processed" height="522" src="https://www.unite.ai/wp-content/uploads/2025/06/figure-4-2.jpg" width="558" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218917"&gt;&lt;em&gt;Correlation between feature differences and preferences, showing how both models and humans were influenced by the same bias features during training.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;To help the models unlearn these biases, new training data was created. The Skywork dataset was reviewed to check if the bias feature appeared in either the chosen or rejected answers; when both were free of the target bias, GPT-4o rewrote the rejected answer to &lt;em&gt;&lt;i&gt;insert&lt;/i&gt;&lt;/em&gt; it.&lt;/p&gt;&lt;p&gt;This created new training pairs where the model could see clear examples of biased and unbiased answers, and thus learn not to favor the biased version. With additional examples from Chatbot Arena, for balance, the models were then fine-tuned on this updated dataset:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218918"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="The effect of fine-tuning with counterfactual data. The left panel shows how the fine-tuned models moved closer to human preferences on most biases; the right panel shows reduced miscalibration, especially for jargon and vagueness." class=" wp-image-218918 webpexpress-processed" height="341" src="https://www.unite.ai/wp-content/uploads/2025/06/figure-5-1.jpg" width="766" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218918"&gt;&lt;em&gt;The effect of fine-tuning with counterfactual data. The left panel shows how the fine-tuned models moved closer to human preferences on most biases; the right panel shows reduced miscalibration, especially for jargon and vagueness.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;The fine-tuning brought the models much closer to human preferences, with the largest improvements seen for jargon and vagueness and smaller gains for length. Structure and sycophancy showed slight new mismatches, though these reflected earlier imbalances rather than new failures.&lt;/p&gt;&lt;p&gt;Overall performance remained stable throughout, and when multiple biases were corrected at once, bias levels fell further without sacrificing response quality.&lt;/p&gt;&lt;p&gt;The authors conclude:&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;i&gt;‘Our method significantly reduces miscalibration issues while preserving overall competence of reward models. Future work can consider adapting our post-training recipe to develop more robust preference models and also evaluate preference models against additional bias axes.'&lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;The new work is an interesting, if elliptical insight into the way that under-curated or over/under-represented training data can cause undesirable outcomes at inference time. Any regular LLM user will, by now, have a collection of war stories.&lt;/p&gt;&lt;p&gt;For instance, many of the responses that I receive from ChatGPT appear to have been influenced by SEO trends of the last 10-15 years, where online portals have been forced to optimize for Google placement instead of natural language. Indeed, the emoji-strewn and prodigious output of marketing departments appears to have had a very significant impact on any request to write a promotional LinkedIn post – to the point where AI-generated ‘enthusiasm' is now impossible to miss:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218919"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Left: Asked to promote a LinkedIn post, in an account with zero history, ChatGPT defaults to emojis and sensational PR-speak. Right: Asked the same thing after six months of me telling it to calm down, GPT produces something rather more sober." class="size-full wp-image-218919 webpexpress-processed" height="539" src="https://www.unite.ai/wp-content/uploads/2025/06/linkedin-post.jpg" width="1200" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218919"&gt;&lt;em&gt;Left: Asked to promote a LinkedIn post, in an account with zero history, ChatGPT defaults to emojis and sensational PR-speak. Right: Asked the same thing after six months of me telling it to calm down, GPT produces something rather more sober.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;However, OpenAI actively intervenes in the way that ChatGPT responds to queries, depending on function and context, making it difficult for researchers to differentiate between problems that arise because of data, and data distribution, along with related issues such as annotation; and when a non-preferred result may be due to commercial interference from the LLM's host company.&lt;/p&gt;&lt;p&gt;* &lt;em&gt;&lt;i&gt;Due to the jargon-filled writing style that the authors have chosen for this paper, I am avoiding author quotes where possible in favor of summaries.&lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;†&lt;/sup&gt;&amp;nbsp; &lt;em&gt;&lt;i&gt;Authors' bold emphasis, not mine.&lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;i&gt;First published Friday, June 6, 2025&lt;/i&gt;&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.unite.ai/how-to-get-chatgpt-to-talk-normally/</guid><pubDate>Fri, 06 Jun 2025 14:04:20 +0000</pubDate></item><item><title>SciSummary Review: I Summarized a Study in Seconds (Unite.AI)</title><link>https://www.unite.ai/scisummary-review/</link><description>&lt;p&gt;If you’ve ever stared at a long scientific paper and thought, “There’s no way I’m getting through this today,” you’re not alone. Academics often struggle to keep up with the growing volume of research in their fields.&lt;/p&gt;&lt;p&gt;SciSummary is not just another AI tool with generic outputs. It’s built specifically to help those in higher education understand research faster, without losing critical insights.&lt;/p&gt;&lt;p&gt;In this SciSummary review, I'll discuss the pros and cons, what it is, who it's best for, and its key features. Then, I'll show you how I used SciSummary to upload a research article and generate a summary, podcast, and slideshow from it.&lt;/p&gt;&lt;p&gt;I'll finish the article by comparing SciSummary with my top three alternatives (Scholarcy, Explainpaper, and Summarizer.org).&lt;/p&gt;&lt;p&gt;Whether you're a grad student trying to understand dense papers or a researcher staying current with the latest in your field, SciSummary can turn a wall of text into a digestible summary in minutes. Let's see why it might be your new way to stay ahead.&lt;/p&gt;&lt;h2&gt;Verdict&lt;/h2&gt;&lt;p&gt;SciSummary is a time-saving tool that makes dense research papers easier to understand. While it may oversimplify or miss technical details, its features and affordability make it a valuable resource for researchers, students, and faculty alike.&lt;/p&gt;&lt;div class="rvw__pros_n_cons v2 on--pp"&gt;&lt;h2&gt;Pros and Cons&lt;/h2&gt;&lt;div class="pros"&gt;&lt;ul&gt;&lt;li&gt;Saves time by quickly turning long scientific papers into clear summaries&lt;/li&gt;&lt;li&gt;Uses advanced AI (GPT-3.5 and GPT-4) to generate accurate, readable summaries and insights from complex research articles.&lt;/li&gt;&lt;li&gt;Easy to use with a simple interface, drag-and-drop uploads, and email summary submission&lt;/li&gt;&lt;li&gt;Handles very long documents (up to 200,000 words)&lt;/li&gt;&lt;li&gt;Includes features like AI analysis of figures and tables, semantic search, reference management, and multi-document chat&lt;/li&gt;&lt;li&gt;Free trials and discounts make it affordable for students&lt;/li&gt;&lt;li&gt;Trusted by researchers, students, and faculty at major U.S. universities&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div class="cons"&gt;&lt;ul&gt;&lt;li&gt;The generated summaries may oversimplify complex or technical content&lt;/li&gt;&lt;li&gt;The AI can misinterpret or leave out important information, especially with specialized jargon&lt;/li&gt;&lt;li&gt;No dedicated mobile app, making it harder to use on mobile devices&lt;/li&gt;&lt;li&gt;While SciSummary offers free trials, there's no completely free plan&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;h2&gt;What is SciSummary?&lt;/h2&gt;&lt;p&gt;&lt;source type="image/webp" /&gt;&lt;img alt="SciSummary homepage." class="webpexpress-processed" height="532" src="https://www.unite.ai/wp-content/uploads/2025/06/Screenshot-2025-06-04-150141.png" width="1200" /&gt;&lt;/p&gt;&lt;p&gt;SciSummary is an AI tool that helps researchers, students, and professionals quickly summarize scientific articles and research papers. Its core function is to generate structured summaries of complex academic documents. This makes it easier to digest key findings without reading the entire paper.&lt;/p&gt;&lt;p&gt;SciSummary is widely used by researchers and students to simplify literature reviews, manage large volumes of academic content, and stay up to date with scientific developments. It supports multiple file types and offers both free trials and paid plans for extended use.&lt;/p&gt;&lt;p&gt;What caught my attention about SciSummary is that it wasn't just another ChatGPT wrapper promising to solve all my problems. SciSummary was built specifically for scientific literature.&lt;/p&gt;&lt;h3&gt;Email Uploads &amp;amp; Dashboard&lt;/h3&gt;&lt;p&gt;One of SciSummary's coolest features is its email functionality. Instead of copying and pasting endless paragraphs into a generic AI tool, you can email your research papers directly to SciSummary.&lt;/p&gt;&lt;p&gt;They have a system in place where you send PDFs to their dedicated email address, and you get back a structured summary. Meanwhile, the dashboard is where you can organize summaries into folders and tag them with keywords.&lt;/p&gt;&lt;h3&gt;Key Differentiators from General AI Tools&lt;/h3&gt;&lt;p&gt;Something that impressed me about SciSummary was how it maintained the scientific integrity of the original work. Unlike general AI tools that might oversimplify or miss crucial details, SciSummary preserves the technical language when needed but explains it in context.&lt;/p&gt;&lt;p&gt;SciSummary can be used for everything from neuroscience papers to climate research, and the quality stays consistent. The AI understands research methodologies in ways that general chatbots don't.&lt;/p&gt;&lt;p&gt;SciSummary comes particularly in handy when you're working with lots of material. You can synthesize findings from multiple studies, saving hours of reading time. Plus, it helps spot connections between studies you might have missed when you're just skimming abstracts.&lt;/p&gt;&lt;h3&gt;Target Audience&lt;/h3&gt;&lt;p&gt;For researchers, graduate students (and anyone dealing with academic literature), SciSummary fills a gap that general AI can't.&lt;/p&gt;&lt;p&gt;It's not perfect (for example, complex theoretical papers sometimes get oversimplified). But for quickly summarizing research papers, it does an excellent job.&lt;/p&gt;&lt;h2&gt;Who is SciSummary Best For?&lt;/h2&gt;&lt;p&gt;SciSummary is best for different types of people within the scientific community:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Researchers &amp;amp; Academics can use SciSummary to quickly understand articles by generating summaries, reviewing research, and keeping up with new trends.&lt;/li&gt;&lt;li&gt;Students can use SciSummary to better understand research papers and make schoolwork easier. SciScummary offers discounts and a free trial to make it student-friendly.&lt;/li&gt;&lt;li&gt;Scientists &amp;amp; Professionals can use SciSummary to keep up with research without reading full articles.&lt;/li&gt;&lt;li&gt;Research Communities can use SciSummary to make research insights easier to access.&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;SciSummary Key Features&lt;/h2&gt;&lt;p&gt;Here are SciSummary's key features:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;AI Summarization: Summarizes scientific articles, research papers, and academic documents in minutes with advanced AI models (GPT-3.5, GPT-4, and Claude). It also handles large documents (up to 200,000 words per article).&lt;/li&gt;&lt;li&gt;Customizable Summaries: Choose from multiple summary modes (full article, focused area, or key points). Specify the summary length, number of key points, and language.&lt;/li&gt;&lt;li&gt;Multi-Document Chat: Simultaneously chat with multiple documents to compare insights.&lt;/li&gt;&lt;li&gt;AI Chat Assistant: Ask questions to the AI chat assistant and get answers about uploaded articles in real time.&lt;/li&gt;&lt;li&gt;Figure &amp;amp; Table Analysis: Analyze figures and tables within research papers using AI.&lt;/li&gt;&lt;li&gt;Generate Citations: Generate citations in multiple formats (APA, Chicago, MLA, and Harvard) with one click, customize citation styles, and export reference lists.&lt;/li&gt;&lt;li&gt;Semantic Search &amp;amp; Bulk Indexing: Indexes up to 1,000 documents for semantic search to find relevant information across multiple papers.&lt;/li&gt;&lt;li&gt;Inline Citations: Provides summaries with inline citations for better trust and accuracy.&lt;/li&gt;&lt;li&gt;User-Friendly Interface: Drag-and-drop upload, email submission, or link/text input for fast document processing.&lt;/li&gt;&lt;li&gt;Global Accessibility: Supports over 130 languages.&lt;/li&gt;&lt;li&gt;Unlimited Usage: Unlimited summaries, chat messages, and article searches for subscribers.&lt;/li&gt;&lt;li&gt;Security &amp;amp; Privacy: Keeps user data secure and confidential.&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;How to Use SciSummary&lt;/h2&gt;&lt;p&gt;Here's how I used SciSummary to upload a research article and generate a summary, podcast, and slideshow from it:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Sign Up for SciSummary&lt;/li&gt;&lt;li&gt;Choose a Summarization Method&lt;/li&gt;&lt;li&gt;Summarize the Article&lt;/li&gt;&lt;li&gt;Generate a Podcast&lt;/li&gt;&lt;li&gt;Generate a Slideshow&lt;/li&gt;&lt;li&gt;Use the Chatbot&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;Step 1: Sign Up for SciSummary&lt;/h3&gt;&lt;p&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Signing up for SciSummary." class="webpexpress-processed" height="522" src="https://www.unite.ai/wp-content/uploads/2025/06/Use-AI-To-Summarize-Scientific-Articles-SciSummary-06-04-2025_07_53_AM.png" width="1200" /&gt;&lt;/p&gt;&lt;p&gt;I started by going to scisummary.com and selecting “Sign Up Today.”&lt;/p&gt;&lt;h3&gt;Step 2: Choose a Summarization Method&lt;/h3&gt;&lt;p&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Summarizing articles with SciSummary by adding them or sending an email." class="webpexpress-processed" height="516" src="https://www.unite.ai/wp-content/uploads/2025/06/Dashboard-SciSummary-06-04-2025_09_12_AM.png" width="1200" /&gt;&lt;/p&gt;&lt;p&gt;After creating an account and activating the 30-day free trial, I was taken to my dashboard. There were two ways I could summarize my articles:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Add an article by uploading a file, URL, text, or other content.&lt;/li&gt;&lt;li&gt;Send an email to [email&amp;nbsp;protected] with an attached PDF of the article, a link to the article, or the article text in the body of the email.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;I selected “Add Articles” to add a link. I could add one or multiple links at a time.&lt;/p&gt;&lt;h3&gt;Step 3: Summarize the Article&lt;/h3&gt;&lt;p&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Summarizing an article uploaded to SciSummary." class="webpexpress-processed" height="452" src="https://www.unite.ai/wp-content/uploads/2025/06/www-sciencenews-org-On-a-day-SciSummary-06-04-2025_09_59_AM.png" width="1200" /&gt;&lt;/p&gt;&lt;p&gt;Immediately, SciSummary gave me six actions I could take with the article I uploaded:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Choose a Summary Mode: Summarize, generate a podcast, create slides, generate key points, simplify, create opportunities for future research, or convert your article into a blog post.&lt;/li&gt;&lt;li&gt;Chat: Enable the chat functionality to ask questions about the article, explore deeper insights, or clarify specific points in real time.&lt;/li&gt;&lt;li&gt;Figures: Extract figures from uploaded files.&lt;/li&gt;&lt;li&gt;References: Retrieve metadata from articles (only for PDF uploads).&lt;/li&gt;&lt;li&gt;Recommendations: Get recommendations (only on uploaded files, not raw text or URLs).&lt;/li&gt;&lt;li&gt;Notes: Add notes.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;SciSummary offered a variety of tools to help interact with academic content more effectively. Whether the goal was to understand complex research, repurpose content for different formats, or explore related studies, each feature supported deeper engagement.&lt;/p&gt;&lt;p&gt;I decided to start by summarizing the article by choosing “Summarize” as the Summary Mode.&lt;/p&gt;&lt;p&gt;Next, I chose how many words I wanted the summary to be (between 100 and 700 words). I kept it in the middle at 400 words.&lt;/p&gt;&lt;p&gt;Last but not least, I chose my language and hit “Request New Summary” to generate my summary.&lt;/p&gt;&lt;p&gt;&lt;source type="image/webp" /&gt;&lt;img alt="A summary generated with SciSummary." class="webpexpress-processed" height="469" src="https://www.unite.ai/wp-content/uploads/2025/06/www-sciencenews-org-On-a-day-SciSummary-06-04-2025_10_22_AM.png" width="1200" /&gt;&lt;/p&gt;&lt;p&gt;Seconds later, SciSummary generated a 400-word summary of the article I had uploaded! I could listen, paraphrase, or expand the summary.&lt;/p&gt;&lt;p&gt;SciSummary did an excellent job breaking down the methodology, key results, and limitations of the study.&lt;/p&gt;&lt;h3&gt;Step 4: Generate a Podcast&lt;/h3&gt;&lt;p&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Generating a podcast with SciSummary." class="webpexpress-processed" height="516" src="https://www.unite.ai/wp-content/uploads/2025/06/www-sciencenews-org-On-a-day-SciSummary-06-04-2025_10_44_AM.png" width="1200" /&gt;&lt;/p&gt;&lt;p&gt;Next, I generated a new summary by turning my article into a podcast.&lt;/p&gt;&lt;p&gt;I chose “Generate a Podcast” as my Summary Mode, selected a word count, chose a language, and selected my two host voices. From there, I hit “Request New Summary.”&lt;/p&gt;&lt;p&gt;&lt;source type="image/webp" /&gt;&lt;img alt="A podcast generated with SciSummary." class="webpexpress-processed" height="479" src="https://www.unite.ai/wp-content/uploads/2025/06/www-sciencenews-org-On-a-day-SciSummary-06-04-2025_10_51_AM.png" width="1200" /&gt;&lt;/p&gt;&lt;p&gt;Seconds later, SciScummary generated a 2.5-minute podcast of my article with the script!&lt;/p&gt;&lt;p&gt;I was surprised at how engaging and easy to follow the podcast was. The AI voices sounded real, making the podcast feel professional. It covered the main points of the research, making it perfect for quick listening on the go.&lt;/p&gt;&lt;h3&gt;Step 5: Generate a Slideshow&lt;/h3&gt;&lt;p&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Turning an uploaded article into a slideshow with SciSummary." class="webpexpress-processed" height="390" src="https://www.unite.ai/wp-content/uploads/2025/06/www-sciencenews-org-On-a-day-SciSummary-06-04-2025_11_11_AM.png" width="1200" /&gt;&lt;/p&gt;&lt;p&gt;The last summary mode I tried was “Generate Slides.” For this option, I just chose the titles and language and hit “Request New Summary.” For the titles, I could go with the standard ones, or I could get them auto-generated.&lt;/p&gt;&lt;p&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Generating a slideshow with SciSummary." class="webpexpress-processed" height="515" src="https://www.unite.ai/wp-content/uploads/2025/06/www-sciencenews-org-On-a-day-SciSummary-06-04-2025_11_26_AM.png" width="1200" /&gt;&lt;/p&gt;&lt;p&gt;Seconds later, SciSummary generated a basic slideshow presentation of the article I had given it! I could listen to it or download it as a PDF or PPT file. The slideshow was broken down into an introduction, methods, results, and discussion.&lt;/p&gt;&lt;p&gt;SciSummary did a great job of distilling the article into clear and concise slides. Each section highlighted the most important points in simple language, making the research easy to present and understand.&lt;/p&gt;&lt;p&gt;However, the slideshow itself was very basic. It would have been nice to have been able to customize the slides.&lt;/p&gt;&lt;h3&gt;Step 6: Use the Chatbot&lt;/h3&gt;&lt;p&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Using the Chat feature on SciSummary." class="webpexpress-processed" height="516" src="https://www.unite.ai/wp-content/uploads/2025/06/www-sciencenews-org-On-a-day-SciSummary-06-04-2025_11_59_AM.png" width="1200" /&gt;&lt;/p&gt;&lt;p&gt;Last but not least, I wanted to try SciSummary's Chat feature. I selected “Chat” and gave the chatbot a question: “What methods did the researchers use to determine the success rate of cockatoos using the drinking fountains?”&lt;/p&gt;&lt;p&gt;Immediately, I was given a formatted answer with references! This made fact-checking quick and reliable. Plus, the response was detailed enough to deepen my understanding without needing to reread the full article.&lt;/p&gt;&lt;p&gt;Overall, SciSummary made engaging with academic research fast, simple, and surprisingly enjoyable. I was impressed by how quickly it turned dense material into digestible summaries, podcasts, and slides I could use and understand.&lt;/p&gt;&lt;h2&gt;Top 3 SciSummary Alternatives&lt;/h2&gt;&lt;p&gt;Here are the best SciSummary alternatives I'd recommend:&lt;/p&gt;&lt;h3&gt;Scholarcy&lt;/h3&gt;&lt;p&gt;The first SciSummary alternative I’d recommend is Scholarcy. Scholarcy uses AI to generate structured summaries of academic papers. The generated summaries lay out key findings, methods, and results.&lt;/p&gt;&lt;p&gt;SciSummary and Scholarcy help researchers, students, and academics quickly digest complex scientific literature. On both platforms, you can upload documents, generate summaries, and gain insights without reading full-length articles.&lt;/p&gt;&lt;p&gt;On the one hand, Scholarcy stands out with its structured, interactive summary flashcards. You can export your flashcards in multiple formats (Word, PowerPoint, Excel, Markdown, RIS). It also offers browser extensions and cloud storage integration, and collaborative research through shared libraries and team features. Scholarcy’s “Dig Deeper” tool lets you ask questions about a document for more in-depth research.&lt;/p&gt;&lt;p&gt;Meanwhile, SciSummary is tailored specifically for scientific texts. It stands out with its quick summarization and user-friendly interface. However, its collaboration tools are more limited compared to Scholarcy.&lt;/p&gt;&lt;p&gt;For structured summaries and robust collaborative team features, choose Scholarcy. For quick and customizable summaries focused on scientific literature, choose SciSummary.&lt;/p&gt;&lt;h3&gt;Explainpaper&lt;/h3&gt;&lt;p&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Explainpaper homepage." class="webpexpress-processed" height="490" src="https://www.unite.ai/wp-content/uploads/2025/06/Screenshot-2025-06-05-111125.png" width="1200" /&gt;&lt;/p&gt;&lt;p&gt;The next SciSummary alternative I’d recommend is Explainpaper. Explainpaper not only summarizes research papers, but also provides explanations of confusing text.&lt;/p&gt;&lt;p&gt;SciSummary and Explainpaper both use AI to help you quickly digest academic literature. This makes both tools excellent for anyone wanting to quickly and efficiently process large volumes of scientific information.&lt;/p&gt;&lt;p&gt;However, SciSummary stands out with its focus on producing structured, customizable summaries for the academic community. It also supports multi-document chat for drawing insights across papers, and research management features like reference organization and semantic search.&lt;/p&gt;&lt;p&gt;Meanwhile, Explainpaper demystifies dense academic language. Highlight confusing text within the paper, and you'll get clear explanations in seconds. Not only do you get a summary of the text, but you also get a deeper understanding of the material.&lt;/p&gt;&lt;p&gt;If you prioritize customizable summaries, research management, and multi-document analysis, choose SciSummary. For clear explanations to help understand complex research concepts, choose Explainpaper!&lt;/p&gt;&lt;h3&gt;Summarizer.org&lt;/h3&gt;&lt;p&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Summarizer.org homepage." class="webpexpress-processed" height="569" src="https://www.unite.ai/wp-content/uploads/2025/06/Screenshot-2025-06-05-111227.png" width="1200" /&gt;&lt;/p&gt;&lt;p&gt;The final SciSummary alternative I’d recommend is Summarizer.org. It's an AI summarization tool that provides instant, free summaries for a wide range of text types like essays, blog posts, and research articles. You don’t even need to create an account or subscribe to use it.&lt;/p&gt;&lt;p&gt;Both SciSummary and Summarizer.org provide AI summarization, but they cater to different needs and user bases.&lt;/p&gt;&lt;p&gt;On the one hand, SciSummary is specifically built for the academic and research community. It does a great job summarizing scientific articles and research papers by breaking down complex information into clear sections.&lt;/p&gt;&lt;p&gt;SciSummary also supports very large documents (up to 200,000 words), multi-document chat, and customizable summaries. It's a great option for anyone who needs help understanding and managing complex research papers.&lt;/p&gt;&lt;p&gt;Meanwhile, Summarizer.org stands out for its simplicity and accessibility. Effortlessly summarize text in eleven different languages and choose how long or short you want the summary to be. Simply paste text, add a URL, or upload a file, and effortlessly switch between a paragraph summary or bullet points.&lt;/p&gt;&lt;p&gt;It's free, unlimited use, and instant results make it an excellent choice for anyone needing quick summaries without advanced features. However, you can always upgrade your account to access more features like the plagiarism checker and AI writing tools.&lt;/p&gt;&lt;p&gt;Use SciSummary to summarize your research and streamline your academic work. For fast and free summarization with no sign-up required, choose Summarizer.org!&lt;/p&gt;&lt;h2&gt;SciSummary Review: The Right Tool For You?&lt;/h2&gt;&lt;p&gt;Using SciSummary felt like an instant upgrade to my research workflow. I wasn’t just saving time; I was gaining deeper insights faster.&lt;/p&gt;&lt;p&gt;Whether I needed a 400-word breakdown, a podcast version of a study, or slide decks for presentations, SciSummary handled it all with surprising precision. Its ability to preserve technical nuance while simplifying dense material is what made it stand out.&lt;/p&gt;&lt;p&gt;However, SciSummary isn't perfect. The slides are basic, and theoretical papers can become oversimplified. But for what I needed (like instant summary creation and AI chat support), SciSummary hit the nail on the head.&lt;/p&gt;&lt;p&gt;If you regularly deal with research papers, SciSummary is worth trying. However, if you're looking for the best alternatives, here's what I'd recommend:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Scholarcy is best for generating structured flashcard-style summaries.&lt;/li&gt;&lt;li&gt;Explainpaper is best for those who struggle with academic jargon and want clear explanations for deeper understanding.&lt;/li&gt;&lt;li&gt;Summarizer.org is best for those who want free, fast summaries without having to create an account.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Thanks for reading my SciSummary review! I hope you found it helpful.&lt;/p&gt;&lt;p&gt;SciSummary offers a free month-long trial of their Student plan. Try it for yourself and see how you like it!&lt;/p&gt;&lt;h2&gt;Frequently Asked Questions&lt;/h2&gt;&lt;h3&gt;What is the difference between SciSummary and Scholarcy?&lt;/h3&gt;&lt;p&gt;The main difference between SciSummary and Scholarcy is that SciSummary offers quick, customizable summaries for scientific literature but lacks robust collaboration features. Meanwhile, Scholarcy provides structured summaries with research comparison tools and team collaboration. Scholarcy is better for academic research and collaborative projects, whereas SciSummary is best for quickly generating summaries on scientific papers.&lt;/p&gt;&lt;h3&gt;Is SciSummary legit?&lt;/h3&gt;&lt;p&gt;SciSummary is a legitimate AI tool widely used by researchers, students, and academics to summarize scientific articles quickly. It's processed over 1,500,000 papers for more than 700,000 users. However, reviews are mixed, with praise for increasing productivity but criticism regarding inconsistent summaries and limited depth on complex papers.&lt;/p&gt;&lt;h3&gt;What is the difference between SciSpace and SciSummary?&lt;/h3&gt;&lt;p&gt;SciSummary is designed for fast, structured research paper summaries with reference organization and multi-document chat. It's a great tool for digesting and managing academic content. In contrast, SciSpace focuses on AI explanations and literature discovery to help understand complex concepts within research papers.&lt;/p&gt;&lt;h3&gt;Is SciSummary better than ChatGPT?&lt;/h3&gt;&lt;p&gt;SciSummary is designed for summarizing scientific articles with customizable summary modes and a user-friendly interface that helps researchers and students quickly digest complex research. While ChatGPT can also summarize articles, SciSummary is generally more efficient and tailored for academic summarization. However, SciSummary lacks the broader conversational and creative capabilities of ChatGPT.&lt;/p&gt;&lt;h3&gt;What is the best AI for summarizing articles?&lt;/h3&gt;&lt;p&gt;Among the best AI for summarizing articles is SciSummary. It delivers fast, accurate, and easy-to-read summaries of research papers and articles. It's a top choice for students, researchers, and professionals who need to quickly understand key points and findings.&lt;/p&gt;</description><content:encoded>&lt;p&gt;If you’ve ever stared at a long scientific paper and thought, “There’s no way I’m getting through this today,” you’re not alone. Academics often struggle to keep up with the growing volume of research in their fields.&lt;/p&gt;&lt;p&gt;SciSummary is not just another AI tool with generic outputs. It’s built specifically to help those in higher education understand research faster, without losing critical insights.&lt;/p&gt;&lt;p&gt;In this SciSummary review, I'll discuss the pros and cons, what it is, who it's best for, and its key features. Then, I'll show you how I used SciSummary to upload a research article and generate a summary, podcast, and slideshow from it.&lt;/p&gt;&lt;p&gt;I'll finish the article by comparing SciSummary with my top three alternatives (Scholarcy, Explainpaper, and Summarizer.org).&lt;/p&gt;&lt;p&gt;Whether you're a grad student trying to understand dense papers or a researcher staying current with the latest in your field, SciSummary can turn a wall of text into a digestible summary in minutes. Let's see why it might be your new way to stay ahead.&lt;/p&gt;&lt;h2&gt;Verdict&lt;/h2&gt;&lt;p&gt;SciSummary is a time-saving tool that makes dense research papers easier to understand. While it may oversimplify or miss technical details, its features and affordability make it a valuable resource for researchers, students, and faculty alike.&lt;/p&gt;&lt;div class="rvw__pros_n_cons v2 on--pp"&gt;&lt;h2&gt;Pros and Cons&lt;/h2&gt;&lt;div class="pros"&gt;&lt;ul&gt;&lt;li&gt;Saves time by quickly turning long scientific papers into clear summaries&lt;/li&gt;&lt;li&gt;Uses advanced AI (GPT-3.5 and GPT-4) to generate accurate, readable summaries and insights from complex research articles.&lt;/li&gt;&lt;li&gt;Easy to use with a simple interface, drag-and-drop uploads, and email summary submission&lt;/li&gt;&lt;li&gt;Handles very long documents (up to 200,000 words)&lt;/li&gt;&lt;li&gt;Includes features like AI analysis of figures and tables, semantic search, reference management, and multi-document chat&lt;/li&gt;&lt;li&gt;Free trials and discounts make it affordable for students&lt;/li&gt;&lt;li&gt;Trusted by researchers, students, and faculty at major U.S. universities&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div class="cons"&gt;&lt;ul&gt;&lt;li&gt;The generated summaries may oversimplify complex or technical content&lt;/li&gt;&lt;li&gt;The AI can misinterpret or leave out important information, especially with specialized jargon&lt;/li&gt;&lt;li&gt;No dedicated mobile app, making it harder to use on mobile devices&lt;/li&gt;&lt;li&gt;While SciSummary offers free trials, there's no completely free plan&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;h2&gt;What is SciSummary?&lt;/h2&gt;&lt;p&gt;&lt;source type="image/webp" /&gt;&lt;img alt="SciSummary homepage." class="webpexpress-processed" height="532" src="https://www.unite.ai/wp-content/uploads/2025/06/Screenshot-2025-06-04-150141.png" width="1200" /&gt;&lt;/p&gt;&lt;p&gt;SciSummary is an AI tool that helps researchers, students, and professionals quickly summarize scientific articles and research papers. Its core function is to generate structured summaries of complex academic documents. This makes it easier to digest key findings without reading the entire paper.&lt;/p&gt;&lt;p&gt;SciSummary is widely used by researchers and students to simplify literature reviews, manage large volumes of academic content, and stay up to date with scientific developments. It supports multiple file types and offers both free trials and paid plans for extended use.&lt;/p&gt;&lt;p&gt;What caught my attention about SciSummary is that it wasn't just another ChatGPT wrapper promising to solve all my problems. SciSummary was built specifically for scientific literature.&lt;/p&gt;&lt;h3&gt;Email Uploads &amp;amp; Dashboard&lt;/h3&gt;&lt;p&gt;One of SciSummary's coolest features is its email functionality. Instead of copying and pasting endless paragraphs into a generic AI tool, you can email your research papers directly to SciSummary.&lt;/p&gt;&lt;p&gt;They have a system in place where you send PDFs to their dedicated email address, and you get back a structured summary. Meanwhile, the dashboard is where you can organize summaries into folders and tag them with keywords.&lt;/p&gt;&lt;h3&gt;Key Differentiators from General AI Tools&lt;/h3&gt;&lt;p&gt;Something that impressed me about SciSummary was how it maintained the scientific integrity of the original work. Unlike general AI tools that might oversimplify or miss crucial details, SciSummary preserves the technical language when needed but explains it in context.&lt;/p&gt;&lt;p&gt;SciSummary can be used for everything from neuroscience papers to climate research, and the quality stays consistent. The AI understands research methodologies in ways that general chatbots don't.&lt;/p&gt;&lt;p&gt;SciSummary comes particularly in handy when you're working with lots of material. You can synthesize findings from multiple studies, saving hours of reading time. Plus, it helps spot connections between studies you might have missed when you're just skimming abstracts.&lt;/p&gt;&lt;h3&gt;Target Audience&lt;/h3&gt;&lt;p&gt;For researchers, graduate students (and anyone dealing with academic literature), SciSummary fills a gap that general AI can't.&lt;/p&gt;&lt;p&gt;It's not perfect (for example, complex theoretical papers sometimes get oversimplified). But for quickly summarizing research papers, it does an excellent job.&lt;/p&gt;&lt;h2&gt;Who is SciSummary Best For?&lt;/h2&gt;&lt;p&gt;SciSummary is best for different types of people within the scientific community:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Researchers &amp;amp; Academics can use SciSummary to quickly understand articles by generating summaries, reviewing research, and keeping up with new trends.&lt;/li&gt;&lt;li&gt;Students can use SciSummary to better understand research papers and make schoolwork easier. SciScummary offers discounts and a free trial to make it student-friendly.&lt;/li&gt;&lt;li&gt;Scientists &amp;amp; Professionals can use SciSummary to keep up with research without reading full articles.&lt;/li&gt;&lt;li&gt;Research Communities can use SciSummary to make research insights easier to access.&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;SciSummary Key Features&lt;/h2&gt;&lt;p&gt;Here are SciSummary's key features:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;AI Summarization: Summarizes scientific articles, research papers, and academic documents in minutes with advanced AI models (GPT-3.5, GPT-4, and Claude). It also handles large documents (up to 200,000 words per article).&lt;/li&gt;&lt;li&gt;Customizable Summaries: Choose from multiple summary modes (full article, focused area, or key points). Specify the summary length, number of key points, and language.&lt;/li&gt;&lt;li&gt;Multi-Document Chat: Simultaneously chat with multiple documents to compare insights.&lt;/li&gt;&lt;li&gt;AI Chat Assistant: Ask questions to the AI chat assistant and get answers about uploaded articles in real time.&lt;/li&gt;&lt;li&gt;Figure &amp;amp; Table Analysis: Analyze figures and tables within research papers using AI.&lt;/li&gt;&lt;li&gt;Generate Citations: Generate citations in multiple formats (APA, Chicago, MLA, and Harvard) with one click, customize citation styles, and export reference lists.&lt;/li&gt;&lt;li&gt;Semantic Search &amp;amp; Bulk Indexing: Indexes up to 1,000 documents for semantic search to find relevant information across multiple papers.&lt;/li&gt;&lt;li&gt;Inline Citations: Provides summaries with inline citations for better trust and accuracy.&lt;/li&gt;&lt;li&gt;User-Friendly Interface: Drag-and-drop upload, email submission, or link/text input for fast document processing.&lt;/li&gt;&lt;li&gt;Global Accessibility: Supports over 130 languages.&lt;/li&gt;&lt;li&gt;Unlimited Usage: Unlimited summaries, chat messages, and article searches for subscribers.&lt;/li&gt;&lt;li&gt;Security &amp;amp; Privacy: Keeps user data secure and confidential.&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;How to Use SciSummary&lt;/h2&gt;&lt;p&gt;Here's how I used SciSummary to upload a research article and generate a summary, podcast, and slideshow from it:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Sign Up for SciSummary&lt;/li&gt;&lt;li&gt;Choose a Summarization Method&lt;/li&gt;&lt;li&gt;Summarize the Article&lt;/li&gt;&lt;li&gt;Generate a Podcast&lt;/li&gt;&lt;li&gt;Generate a Slideshow&lt;/li&gt;&lt;li&gt;Use the Chatbot&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;Step 1: Sign Up for SciSummary&lt;/h3&gt;&lt;p&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Signing up for SciSummary." class="webpexpress-processed" height="522" src="https://www.unite.ai/wp-content/uploads/2025/06/Use-AI-To-Summarize-Scientific-Articles-SciSummary-06-04-2025_07_53_AM.png" width="1200" /&gt;&lt;/p&gt;&lt;p&gt;I started by going to scisummary.com and selecting “Sign Up Today.”&lt;/p&gt;&lt;h3&gt;Step 2: Choose a Summarization Method&lt;/h3&gt;&lt;p&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Summarizing articles with SciSummary by adding them or sending an email." class="webpexpress-processed" height="516" src="https://www.unite.ai/wp-content/uploads/2025/06/Dashboard-SciSummary-06-04-2025_09_12_AM.png" width="1200" /&gt;&lt;/p&gt;&lt;p&gt;After creating an account and activating the 30-day free trial, I was taken to my dashboard. There were two ways I could summarize my articles:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Add an article by uploading a file, URL, text, or other content.&lt;/li&gt;&lt;li&gt;Send an email to [email&amp;nbsp;protected] with an attached PDF of the article, a link to the article, or the article text in the body of the email.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;I selected “Add Articles” to add a link. I could add one or multiple links at a time.&lt;/p&gt;&lt;h3&gt;Step 3: Summarize the Article&lt;/h3&gt;&lt;p&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Summarizing an article uploaded to SciSummary." class="webpexpress-processed" height="452" src="https://www.unite.ai/wp-content/uploads/2025/06/www-sciencenews-org-On-a-day-SciSummary-06-04-2025_09_59_AM.png" width="1200" /&gt;&lt;/p&gt;&lt;p&gt;Immediately, SciSummary gave me six actions I could take with the article I uploaded:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Choose a Summary Mode: Summarize, generate a podcast, create slides, generate key points, simplify, create opportunities for future research, or convert your article into a blog post.&lt;/li&gt;&lt;li&gt;Chat: Enable the chat functionality to ask questions about the article, explore deeper insights, or clarify specific points in real time.&lt;/li&gt;&lt;li&gt;Figures: Extract figures from uploaded files.&lt;/li&gt;&lt;li&gt;References: Retrieve metadata from articles (only for PDF uploads).&lt;/li&gt;&lt;li&gt;Recommendations: Get recommendations (only on uploaded files, not raw text or URLs).&lt;/li&gt;&lt;li&gt;Notes: Add notes.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;SciSummary offered a variety of tools to help interact with academic content more effectively. Whether the goal was to understand complex research, repurpose content for different formats, or explore related studies, each feature supported deeper engagement.&lt;/p&gt;&lt;p&gt;I decided to start by summarizing the article by choosing “Summarize” as the Summary Mode.&lt;/p&gt;&lt;p&gt;Next, I chose how many words I wanted the summary to be (between 100 and 700 words). I kept it in the middle at 400 words.&lt;/p&gt;&lt;p&gt;Last but not least, I chose my language and hit “Request New Summary” to generate my summary.&lt;/p&gt;&lt;p&gt;&lt;source type="image/webp" /&gt;&lt;img alt="A summary generated with SciSummary." class="webpexpress-processed" height="469" src="https://www.unite.ai/wp-content/uploads/2025/06/www-sciencenews-org-On-a-day-SciSummary-06-04-2025_10_22_AM.png" width="1200" /&gt;&lt;/p&gt;&lt;p&gt;Seconds later, SciSummary generated a 400-word summary of the article I had uploaded! I could listen, paraphrase, or expand the summary.&lt;/p&gt;&lt;p&gt;SciSummary did an excellent job breaking down the methodology, key results, and limitations of the study.&lt;/p&gt;&lt;h3&gt;Step 4: Generate a Podcast&lt;/h3&gt;&lt;p&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Generating a podcast with SciSummary." class="webpexpress-processed" height="516" src="https://www.unite.ai/wp-content/uploads/2025/06/www-sciencenews-org-On-a-day-SciSummary-06-04-2025_10_44_AM.png" width="1200" /&gt;&lt;/p&gt;&lt;p&gt;Next, I generated a new summary by turning my article into a podcast.&lt;/p&gt;&lt;p&gt;I chose “Generate a Podcast” as my Summary Mode, selected a word count, chose a language, and selected my two host voices. From there, I hit “Request New Summary.”&lt;/p&gt;&lt;p&gt;&lt;source type="image/webp" /&gt;&lt;img alt="A podcast generated with SciSummary." class="webpexpress-processed" height="479" src="https://www.unite.ai/wp-content/uploads/2025/06/www-sciencenews-org-On-a-day-SciSummary-06-04-2025_10_51_AM.png" width="1200" /&gt;&lt;/p&gt;&lt;p&gt;Seconds later, SciScummary generated a 2.5-minute podcast of my article with the script!&lt;/p&gt;&lt;p&gt;I was surprised at how engaging and easy to follow the podcast was. The AI voices sounded real, making the podcast feel professional. It covered the main points of the research, making it perfect for quick listening on the go.&lt;/p&gt;&lt;h3&gt;Step 5: Generate a Slideshow&lt;/h3&gt;&lt;p&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Turning an uploaded article into a slideshow with SciSummary." class="webpexpress-processed" height="390" src="https://www.unite.ai/wp-content/uploads/2025/06/www-sciencenews-org-On-a-day-SciSummary-06-04-2025_11_11_AM.png" width="1200" /&gt;&lt;/p&gt;&lt;p&gt;The last summary mode I tried was “Generate Slides.” For this option, I just chose the titles and language and hit “Request New Summary.” For the titles, I could go with the standard ones, or I could get them auto-generated.&lt;/p&gt;&lt;p&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Generating a slideshow with SciSummary." class="webpexpress-processed" height="515" src="https://www.unite.ai/wp-content/uploads/2025/06/www-sciencenews-org-On-a-day-SciSummary-06-04-2025_11_26_AM.png" width="1200" /&gt;&lt;/p&gt;&lt;p&gt;Seconds later, SciSummary generated a basic slideshow presentation of the article I had given it! I could listen to it or download it as a PDF or PPT file. The slideshow was broken down into an introduction, methods, results, and discussion.&lt;/p&gt;&lt;p&gt;SciSummary did a great job of distilling the article into clear and concise slides. Each section highlighted the most important points in simple language, making the research easy to present and understand.&lt;/p&gt;&lt;p&gt;However, the slideshow itself was very basic. It would have been nice to have been able to customize the slides.&lt;/p&gt;&lt;h3&gt;Step 6: Use the Chatbot&lt;/h3&gt;&lt;p&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Using the Chat feature on SciSummary." class="webpexpress-processed" height="516" src="https://www.unite.ai/wp-content/uploads/2025/06/www-sciencenews-org-On-a-day-SciSummary-06-04-2025_11_59_AM.png" width="1200" /&gt;&lt;/p&gt;&lt;p&gt;Last but not least, I wanted to try SciSummary's Chat feature. I selected “Chat” and gave the chatbot a question: “What methods did the researchers use to determine the success rate of cockatoos using the drinking fountains?”&lt;/p&gt;&lt;p&gt;Immediately, I was given a formatted answer with references! This made fact-checking quick and reliable. Plus, the response was detailed enough to deepen my understanding without needing to reread the full article.&lt;/p&gt;&lt;p&gt;Overall, SciSummary made engaging with academic research fast, simple, and surprisingly enjoyable. I was impressed by how quickly it turned dense material into digestible summaries, podcasts, and slides I could use and understand.&lt;/p&gt;&lt;h2&gt;Top 3 SciSummary Alternatives&lt;/h2&gt;&lt;p&gt;Here are the best SciSummary alternatives I'd recommend:&lt;/p&gt;&lt;h3&gt;Scholarcy&lt;/h3&gt;&lt;p&gt;The first SciSummary alternative I’d recommend is Scholarcy. Scholarcy uses AI to generate structured summaries of academic papers. The generated summaries lay out key findings, methods, and results.&lt;/p&gt;&lt;p&gt;SciSummary and Scholarcy help researchers, students, and academics quickly digest complex scientific literature. On both platforms, you can upload documents, generate summaries, and gain insights without reading full-length articles.&lt;/p&gt;&lt;p&gt;On the one hand, Scholarcy stands out with its structured, interactive summary flashcards. You can export your flashcards in multiple formats (Word, PowerPoint, Excel, Markdown, RIS). It also offers browser extensions and cloud storage integration, and collaborative research through shared libraries and team features. Scholarcy’s “Dig Deeper” tool lets you ask questions about a document for more in-depth research.&lt;/p&gt;&lt;p&gt;Meanwhile, SciSummary is tailored specifically for scientific texts. It stands out with its quick summarization and user-friendly interface. However, its collaboration tools are more limited compared to Scholarcy.&lt;/p&gt;&lt;p&gt;For structured summaries and robust collaborative team features, choose Scholarcy. For quick and customizable summaries focused on scientific literature, choose SciSummary.&lt;/p&gt;&lt;h3&gt;Explainpaper&lt;/h3&gt;&lt;p&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Explainpaper homepage." class="webpexpress-processed" height="490" src="https://www.unite.ai/wp-content/uploads/2025/06/Screenshot-2025-06-05-111125.png" width="1200" /&gt;&lt;/p&gt;&lt;p&gt;The next SciSummary alternative I’d recommend is Explainpaper. Explainpaper not only summarizes research papers, but also provides explanations of confusing text.&lt;/p&gt;&lt;p&gt;SciSummary and Explainpaper both use AI to help you quickly digest academic literature. This makes both tools excellent for anyone wanting to quickly and efficiently process large volumes of scientific information.&lt;/p&gt;&lt;p&gt;However, SciSummary stands out with its focus on producing structured, customizable summaries for the academic community. It also supports multi-document chat for drawing insights across papers, and research management features like reference organization and semantic search.&lt;/p&gt;&lt;p&gt;Meanwhile, Explainpaper demystifies dense academic language. Highlight confusing text within the paper, and you'll get clear explanations in seconds. Not only do you get a summary of the text, but you also get a deeper understanding of the material.&lt;/p&gt;&lt;p&gt;If you prioritize customizable summaries, research management, and multi-document analysis, choose SciSummary. For clear explanations to help understand complex research concepts, choose Explainpaper!&lt;/p&gt;&lt;h3&gt;Summarizer.org&lt;/h3&gt;&lt;p&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Summarizer.org homepage." class="webpexpress-processed" height="569" src="https://www.unite.ai/wp-content/uploads/2025/06/Screenshot-2025-06-05-111227.png" width="1200" /&gt;&lt;/p&gt;&lt;p&gt;The final SciSummary alternative I’d recommend is Summarizer.org. It's an AI summarization tool that provides instant, free summaries for a wide range of text types like essays, blog posts, and research articles. You don’t even need to create an account or subscribe to use it.&lt;/p&gt;&lt;p&gt;Both SciSummary and Summarizer.org provide AI summarization, but they cater to different needs and user bases.&lt;/p&gt;&lt;p&gt;On the one hand, SciSummary is specifically built for the academic and research community. It does a great job summarizing scientific articles and research papers by breaking down complex information into clear sections.&lt;/p&gt;&lt;p&gt;SciSummary also supports very large documents (up to 200,000 words), multi-document chat, and customizable summaries. It's a great option for anyone who needs help understanding and managing complex research papers.&lt;/p&gt;&lt;p&gt;Meanwhile, Summarizer.org stands out for its simplicity and accessibility. Effortlessly summarize text in eleven different languages and choose how long or short you want the summary to be. Simply paste text, add a URL, or upload a file, and effortlessly switch between a paragraph summary or bullet points.&lt;/p&gt;&lt;p&gt;It's free, unlimited use, and instant results make it an excellent choice for anyone needing quick summaries without advanced features. However, you can always upgrade your account to access more features like the plagiarism checker and AI writing tools.&lt;/p&gt;&lt;p&gt;Use SciSummary to summarize your research and streamline your academic work. For fast and free summarization with no sign-up required, choose Summarizer.org!&lt;/p&gt;&lt;h2&gt;SciSummary Review: The Right Tool For You?&lt;/h2&gt;&lt;p&gt;Using SciSummary felt like an instant upgrade to my research workflow. I wasn’t just saving time; I was gaining deeper insights faster.&lt;/p&gt;&lt;p&gt;Whether I needed a 400-word breakdown, a podcast version of a study, or slide decks for presentations, SciSummary handled it all with surprising precision. Its ability to preserve technical nuance while simplifying dense material is what made it stand out.&lt;/p&gt;&lt;p&gt;However, SciSummary isn't perfect. The slides are basic, and theoretical papers can become oversimplified. But for what I needed (like instant summary creation and AI chat support), SciSummary hit the nail on the head.&lt;/p&gt;&lt;p&gt;If you regularly deal with research papers, SciSummary is worth trying. However, if you're looking for the best alternatives, here's what I'd recommend:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Scholarcy is best for generating structured flashcard-style summaries.&lt;/li&gt;&lt;li&gt;Explainpaper is best for those who struggle with academic jargon and want clear explanations for deeper understanding.&lt;/li&gt;&lt;li&gt;Summarizer.org is best for those who want free, fast summaries without having to create an account.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Thanks for reading my SciSummary review! I hope you found it helpful.&lt;/p&gt;&lt;p&gt;SciSummary offers a free month-long trial of their Student plan. Try it for yourself and see how you like it!&lt;/p&gt;&lt;h2&gt;Frequently Asked Questions&lt;/h2&gt;&lt;h3&gt;What is the difference between SciSummary and Scholarcy?&lt;/h3&gt;&lt;p&gt;The main difference between SciSummary and Scholarcy is that SciSummary offers quick, customizable summaries for scientific literature but lacks robust collaboration features. Meanwhile, Scholarcy provides structured summaries with research comparison tools and team collaboration. Scholarcy is better for academic research and collaborative projects, whereas SciSummary is best for quickly generating summaries on scientific papers.&lt;/p&gt;&lt;h3&gt;Is SciSummary legit?&lt;/h3&gt;&lt;p&gt;SciSummary is a legitimate AI tool widely used by researchers, students, and academics to summarize scientific articles quickly. It's processed over 1,500,000 papers for more than 700,000 users. However, reviews are mixed, with praise for increasing productivity but criticism regarding inconsistent summaries and limited depth on complex papers.&lt;/p&gt;&lt;h3&gt;What is the difference between SciSpace and SciSummary?&lt;/h3&gt;&lt;p&gt;SciSummary is designed for fast, structured research paper summaries with reference organization and multi-document chat. It's a great tool for digesting and managing academic content. In contrast, SciSpace focuses on AI explanations and literature discovery to help understand complex concepts within research papers.&lt;/p&gt;&lt;h3&gt;Is SciSummary better than ChatGPT?&lt;/h3&gt;&lt;p&gt;SciSummary is designed for summarizing scientific articles with customizable summary modes and a user-friendly interface that helps researchers and students quickly digest complex research. While ChatGPT can also summarize articles, SciSummary is generally more efficient and tailored for academic summarization. However, SciSummary lacks the broader conversational and creative capabilities of ChatGPT.&lt;/p&gt;&lt;h3&gt;What is the best AI for summarizing articles?&lt;/h3&gt;&lt;p&gt;Among the best AI for summarizing articles is SciSummary. It delivers fast, accurate, and easy-to-read summaries of research papers and articles. It's a top choice for students, researchers, and professionals who need to quickly understand key points and findings.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.unite.ai/scisummary-review/</guid><pubDate>Fri, 06 Jun 2025 16:43:51 +0000</pubDate></item><item><title>How AI Might Save the News Media (Unite.AI)</title><link>https://www.unite.ai/how-ai-might-save-the-news-media/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.unite.ai/wp-content/uploads/2025/05/AI-Save-News-Media-942x600.png" /&gt;&lt;/div&gt;&lt;p dir="ltr"&gt;That may be tough to see right now. Since the launch of OpenAI’s ChatGPT in late 2022, and a whole host of other AI-powered chatbots and virtual assistants, the focus has revolved around how these tools could take over the jobs of journalists and other content creators. The media industry, already struggling, feels rightfully attacked.&lt;/p&gt;&lt;p dir="ltr"&gt;Even from the inside. Shortly after, the owner of Politico and Insider Mathias Döpfner told his employees earlier this year that&amp;nbsp;AI could replace them. Then, the entire newsroom at BuzzFeed was let go, with CEO Jonah Peretti saying the company will be&amp;nbsp;pivoting to focus on AI. The list of newsrooms experimenting with AI to automate news generation continues to grow. Meta and OpenAI specifically&amp;nbsp;attract&amp;nbsp;journalists to train LLMs.&lt;/p&gt;&lt;p dir="ltr"&gt;Along with the adoption of AI&amp;nbsp;came human layoffs. Journalists surely have&amp;nbsp;reason to be worried. That said, media executives have been too quick to adopt tech and slash human, it seems, after a number of cringeworthy incidents have come to light.&lt;/p&gt;&lt;p dir="ltr"&gt;CNET and its sister company Bankrate were called out&amp;nbsp;for publishing dozens of articles with inaccuracies&amp;nbsp;written by AI; since then, they’ve halted AI publishing. In a similar vein, G/O Media – the owner of sites like Jezebel and Gizmodo –&amp;nbsp;published AI-generated stories without editor input&amp;nbsp;and as such, contained multiple errors. And Microsoft users were&amp;nbsp;appalled by an inappropriate AI-generated poll&amp;nbsp;posted next to a story about a woman found dead.&lt;/p&gt;&lt;p dir="ltr"&gt;All in all, AI is very unlikely to replace journalists. Instead, AI will likely help news publications and make them ever more dominant. Why? The answer to this lies in the most crucial commodity for AI labs: high-quality training content.&lt;/p&gt;&lt;h2 dir="ltr"&gt;Déjà Vu: How Social Media Reshaped News&lt;/h2&gt;&lt;p dir="ltr"&gt;Just as the internet reshaped the media business – with some companies tanking because of overreliance on the shiny new toy and others significantly benefiting from a measured approach to the new advertising avenues and open distribution – so too will AI.&lt;/p&gt;&lt;p dir="ltr"&gt;Initially, media publishers were excited by the prospects of rising social media. No longer were they bound by the physical barriers of print. It turned out they were suddenly competing with the entire world, which included not just all other publications but individual bloggers and influencers. The New York Times has become a digital media juggernaut that has attracted over 11 million paid subscribers and has become one of the largest news publishers in the world. Many other publications are struggling or have had to close down.&lt;/p&gt;&lt;p dir="ltr"&gt;However, AI has the potential to reshape the entire field by bringing power back to news media. Large Language Models need a lot of content for training, and the quality of this content varies. Turns out, AI companies give a lot of weight to information captured from news organizations. That’s because, unlike your X/Twitter feed and social media in general, these publications offer high-quality, vetted information, curated by not just one content creator but by a whole newsroom of reporters and editors. So this information will be labeled as more reliable and surfaced more often. This signals how valuable media companies and the work their human staff produce are.&lt;/p&gt;&lt;p dir="ltr"&gt;So, what does The New York Times think about dealing with AI? Well, they’re&amp;nbsp;suing&amp;nbsp;OpenAI. And along with a huge list of media businesses, including The Guardian, Condé Nast, Forbes, and many more, they are&amp;nbsp;blocking AI crawlers&amp;nbsp;from scraping the content on their sites. The News/Media Alliance recently&amp;nbsp;slammed&amp;nbsp;Google’s newly&amp;nbsp;launched&amp;nbsp;AI Mode by saying it ‘just takes content by force and uses it with no return’ to publishers like Condé Nast and Vox Media.&lt;/p&gt;&lt;p dir="ltr"&gt;But this may be a negotiation tactic. Already, AI companies and media institutions have begun to partner. Meanwhile, OpenAI has&amp;nbsp;partnered&amp;nbsp;with over 20 news publishers, including more than 160 outlets, such as the Washington Post, The New Yorker, and Wired. Perplexity&amp;nbsp;signed&amp;nbsp;agreements with AdWeek, The Independent, Los Angeles Times, and World History Encyclopedia. AI labs are approaching a point where they have exhausted much of the high-quality, publicly available data suitable for training large language models, and are actively looking for new content.&lt;/p&gt;&lt;p dir="ltr"&gt;So these licensing partnerships are very important – not just so AI companies can develop useful products and not just so newsrooms can distribute their articles to a wider base, but so consumers get access to well-researched, educated information.&lt;/p&gt;&lt;h2 dir="ltr"&gt;The New Front Page: Getting Into the AI Dataset&lt;/h2&gt;&lt;p dir="ltr"&gt;Because consumers have already begun utilizing AI to search. Google and other search engines are losing ground as the results have become overrun with content created by marketers and SEO wizards that push unhelpful websites to the top. More and more, people are querying ChatGPT and other AI assistants to get better, more specialized content for their search.&lt;/p&gt;&lt;p dir="ltr"&gt;Gergely Orosz, the author of a developer-focused Pragmatic Engineer newsletter,&amp;nbsp;mentioned&amp;nbsp;in May that ChatGPT drove more traffic to his blog than either DuckDuckGo or Bing in the past month, and these visitors read the page longer.&lt;/p&gt;&lt;p dir="ltr"&gt;Going forward, getting into the dataset of major LLMs will be just as important as appearing on the first page of Google Search results. Consumers seek product recommendations, research apps, and services, summarize information on complex topics, do basic market research, or learn about new things. All of these instances are great opportunities for businesses to capture new audiences in a fresh environment. Companies will fight for this position tooth and nail, and the more people who flock to AI search, the more critical this area will become.&lt;/p&gt;&lt;p dir="ltr"&gt;This gets us back to the beginning, since the best way to enter the LLM training dataset is by appearing in major news media publications that produce high-quality journalism and have secured direct partnerships with OpenAI, Anthropic, Perplexity, and other AI labs. This further entrenches the media's position and provides them with a real path for the future.&lt;/p&gt;&lt;p dir="ltr"&gt;Meanwhile, optimizing content for the inclusion in training datasets will become the new SEO.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.unite.ai/wp-content/uploads/2025/05/AI-Save-News-Media-942x600.png" /&gt;&lt;/div&gt;&lt;p dir="ltr"&gt;That may be tough to see right now. Since the launch of OpenAI’s ChatGPT in late 2022, and a whole host of other AI-powered chatbots and virtual assistants, the focus has revolved around how these tools could take over the jobs of journalists and other content creators. The media industry, already struggling, feels rightfully attacked.&lt;/p&gt;&lt;p dir="ltr"&gt;Even from the inside. Shortly after, the owner of Politico and Insider Mathias Döpfner told his employees earlier this year that&amp;nbsp;AI could replace them. Then, the entire newsroom at BuzzFeed was let go, with CEO Jonah Peretti saying the company will be&amp;nbsp;pivoting to focus on AI. The list of newsrooms experimenting with AI to automate news generation continues to grow. Meta and OpenAI specifically&amp;nbsp;attract&amp;nbsp;journalists to train LLMs.&lt;/p&gt;&lt;p dir="ltr"&gt;Along with the adoption of AI&amp;nbsp;came human layoffs. Journalists surely have&amp;nbsp;reason to be worried. That said, media executives have been too quick to adopt tech and slash human, it seems, after a number of cringeworthy incidents have come to light.&lt;/p&gt;&lt;p dir="ltr"&gt;CNET and its sister company Bankrate were called out&amp;nbsp;for publishing dozens of articles with inaccuracies&amp;nbsp;written by AI; since then, they’ve halted AI publishing. In a similar vein, G/O Media – the owner of sites like Jezebel and Gizmodo –&amp;nbsp;published AI-generated stories without editor input&amp;nbsp;and as such, contained multiple errors. And Microsoft users were&amp;nbsp;appalled by an inappropriate AI-generated poll&amp;nbsp;posted next to a story about a woman found dead.&lt;/p&gt;&lt;p dir="ltr"&gt;All in all, AI is very unlikely to replace journalists. Instead, AI will likely help news publications and make them ever more dominant. Why? The answer to this lies in the most crucial commodity for AI labs: high-quality training content.&lt;/p&gt;&lt;h2 dir="ltr"&gt;Déjà Vu: How Social Media Reshaped News&lt;/h2&gt;&lt;p dir="ltr"&gt;Just as the internet reshaped the media business – with some companies tanking because of overreliance on the shiny new toy and others significantly benefiting from a measured approach to the new advertising avenues and open distribution – so too will AI.&lt;/p&gt;&lt;p dir="ltr"&gt;Initially, media publishers were excited by the prospects of rising social media. No longer were they bound by the physical barriers of print. It turned out they were suddenly competing with the entire world, which included not just all other publications but individual bloggers and influencers. The New York Times has become a digital media juggernaut that has attracted over 11 million paid subscribers and has become one of the largest news publishers in the world. Many other publications are struggling or have had to close down.&lt;/p&gt;&lt;p dir="ltr"&gt;However, AI has the potential to reshape the entire field by bringing power back to news media. Large Language Models need a lot of content for training, and the quality of this content varies. Turns out, AI companies give a lot of weight to information captured from news organizations. That’s because, unlike your X/Twitter feed and social media in general, these publications offer high-quality, vetted information, curated by not just one content creator but by a whole newsroom of reporters and editors. So this information will be labeled as more reliable and surfaced more often. This signals how valuable media companies and the work their human staff produce are.&lt;/p&gt;&lt;p dir="ltr"&gt;So, what does The New York Times think about dealing with AI? Well, they’re&amp;nbsp;suing&amp;nbsp;OpenAI. And along with a huge list of media businesses, including The Guardian, Condé Nast, Forbes, and many more, they are&amp;nbsp;blocking AI crawlers&amp;nbsp;from scraping the content on their sites. The News/Media Alliance recently&amp;nbsp;slammed&amp;nbsp;Google’s newly&amp;nbsp;launched&amp;nbsp;AI Mode by saying it ‘just takes content by force and uses it with no return’ to publishers like Condé Nast and Vox Media.&lt;/p&gt;&lt;p dir="ltr"&gt;But this may be a negotiation tactic. Already, AI companies and media institutions have begun to partner. Meanwhile, OpenAI has&amp;nbsp;partnered&amp;nbsp;with over 20 news publishers, including more than 160 outlets, such as the Washington Post, The New Yorker, and Wired. Perplexity&amp;nbsp;signed&amp;nbsp;agreements with AdWeek, The Independent, Los Angeles Times, and World History Encyclopedia. AI labs are approaching a point where they have exhausted much of the high-quality, publicly available data suitable for training large language models, and are actively looking for new content.&lt;/p&gt;&lt;p dir="ltr"&gt;So these licensing partnerships are very important – not just so AI companies can develop useful products and not just so newsrooms can distribute their articles to a wider base, but so consumers get access to well-researched, educated information.&lt;/p&gt;&lt;h2 dir="ltr"&gt;The New Front Page: Getting Into the AI Dataset&lt;/h2&gt;&lt;p dir="ltr"&gt;Because consumers have already begun utilizing AI to search. Google and other search engines are losing ground as the results have become overrun with content created by marketers and SEO wizards that push unhelpful websites to the top. More and more, people are querying ChatGPT and other AI assistants to get better, more specialized content for their search.&lt;/p&gt;&lt;p dir="ltr"&gt;Gergely Orosz, the author of a developer-focused Pragmatic Engineer newsletter,&amp;nbsp;mentioned&amp;nbsp;in May that ChatGPT drove more traffic to his blog than either DuckDuckGo or Bing in the past month, and these visitors read the page longer.&lt;/p&gt;&lt;p dir="ltr"&gt;Going forward, getting into the dataset of major LLMs will be just as important as appearing on the first page of Google Search results. Consumers seek product recommendations, research apps, and services, summarize information on complex topics, do basic market research, or learn about new things. All of these instances are great opportunities for businesses to capture new audiences in a fresh environment. Companies will fight for this position tooth and nail, and the more people who flock to AI search, the more critical this area will become.&lt;/p&gt;&lt;p dir="ltr"&gt;This gets us back to the beginning, since the best way to enter the LLM training dataset is by appearing in major news media publications that produce high-quality journalism and have secured direct partnerships with OpenAI, Anthropic, Perplexity, and other AI labs. This further entrenches the media's position and provides them with a real path for the future.&lt;/p&gt;&lt;p dir="ltr"&gt;Meanwhile, optimizing content for the inclusion in training datasets will become the new SEO.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.unite.ai/how-ai-might-save-the-news-media/</guid><pubDate>Fri, 06 Jun 2025 17:14:24 +0000</pubDate></item><item><title>When Your AI Invents Facts: The Enterprise Risk No Leader Can Ignore (Unite.AI)</title><link>https://www.unite.ai/when-your-ai-invents-facts-the-enterprise-risk-no-leader-can-ignore/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.unite.ai/wp-content/uploads/2025/05/AI-Hallucinations-Risk-942x600.png" /&gt;&lt;/div&gt;&lt;p&gt;It sounds right. It looks right. It’s wrong. That’s your AI on hallucination. The issue isn’t just that today’s generative AI models hallucinate. It’s that we feel if we build enough guardrails, fine-tune it, RAG it, and tame it somehow, then we will be able to adopt it at Enterprise scale.&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Study&lt;/th&gt;&lt;th&gt;Domain&lt;/th&gt;&lt;th&gt;Hallucination Rate&lt;/th&gt;&lt;th&gt;Key Findings&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Stanford HAI &amp;amp; RegLab (Jan 2024)&lt;/td&gt;&lt;td&gt;Legal&lt;/td&gt;&lt;td&gt;69%–88%&lt;/td&gt;&lt;td&gt;LLMs exhibited high hallucination rates when responding to legal queries, often lacking self-awareness about their errors and reinforcing incorrect legal assumptions.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;JMIR Study (2024)&lt;/td&gt;&lt;td&gt;Academic References&lt;/td&gt;&lt;td&gt;GPT-3.5: 90.6%, GPT-4: 86.6%, Bard: 100%&lt;/td&gt;&lt;td&gt;LLM-generated references were often irrelevant, incorrect, or unsupported by available literature.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;UK Study on AI-Generated Content (Feb 2025)&lt;/td&gt;&lt;td&gt;Finance&lt;/td&gt;&lt;td&gt;Not specified&lt;/td&gt;&lt;td&gt;AI-generated disinformation increased the risk of bank runs, with a significant portion of bank customers considering moving their money after viewing AI-generated fake content.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;World Economic Forum Global Risks Report (2025)&lt;/td&gt;&lt;td&gt;Global Risk Assessment&lt;/td&gt;&lt;td&gt;Not specified&lt;/td&gt;&lt;td&gt;Misinformation and disinformation, amplified by AI, ranked as the top global risk over a two-year outlook.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Vectara Hallucination Leaderboard (2025)&lt;/td&gt;&lt;td&gt;AI Model Evaluation&lt;/td&gt;&lt;td&gt;GPT-4.5-Preview: 1.2%, Google Gemini-2.0-Pro-Exp: 0.8%, Vectara Mockingbird-2-Echo: 0.9%&lt;/td&gt;&lt;td&gt;Evaluated hallucination rates across various LLMs, revealing significant differences in performance and accuracy.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Arxiv Study on Factuality Hallucination (2024)&lt;/td&gt;&lt;td&gt;AI Research&lt;/td&gt;&lt;td&gt;Not specified&lt;/td&gt;&lt;td&gt;Introduced HaluEval 2.0 to systematically study and detect hallucinations in LLMs, focusing on factual inaccuracies.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;&lt;strong&gt;Hallucination rates span from 0.8% to 88%&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Yes, it depends on the model, domain, use case, and context, but that spread should rattle any enterprise decision maker. These aren’t edge case errors. They’re systemic.&amp;nbsp; How do you make the right call when it comes to AI adoption in your enterprise? Where, how, how deep, how wide?&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;And examples of real-world consequences of this come across your newsfeed every day.&amp;nbsp; G20’s Financial Stability Board has flagged generative AI as a vector for disinformation that could cause market crises, political instability, and worse–flash crashes, fake news, and fraud. In another recently reported story, law firm Morgan &amp;amp; Morgan issued an emergency memo to all attorneys: Do not submit AI-generated filings without checking. Fake case law is a “fireable” offense.&lt;/p&gt;&lt;p&gt;This may not be the best time to bet the farm on hallucination rates tending to zero any time soon. Especially in regulated industries, such as legal, life sciences, capital markets, or in others, where the cost of a mistake could be high, including publishing higher education.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Hallucination is not a Rounding Error&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;This isn’t about an occasional wrong answer. It’s about &lt;strong&gt;risk&lt;/strong&gt;: Reputational, Legal, Operational.&lt;/p&gt;&lt;p&gt;Generative AI isn’t a reasoning engine. It’s a statistical finisher, a stochastic parrot. It completes your prompt in the most likely way based on training data. Even the &lt;em&gt;true-sounding parts&lt;/em&gt; are guesses. We call the most absurd pieces “hallucinations,” but the entire output is a hallucination. A well-styled one. Still, it works, magically well—until it doesn’t.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;AI as Infrastructure&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;And yet, it’s important to say that AI will be ready for Enterprise-wide adoption when we start treating it like infrastructure, and not like magic. And where required, it must be transparent, explainable, and traceable. And if it is not, then quite simply, it is not ready for Enterprise-wide adoption for those use cases. &amp;nbsp;If AI is making decisions, it should be on your Board’s radar.&lt;/p&gt;&lt;p&gt;The EU’s AI Act is leading the charge here. High-risk domains like justice, healthcare, and infrastructure will be regulated like mission-critical systems. Documentation, testing, and explainability will be mandatory.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;What Enterprise Safe AI Models Do&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Companies that specialize in building enterprise-safe AI models, make a conscious decision to build AI differently. In their alternative AI architectures, the Language Models are not trained on data, so they are not “contaminated” with anything undesirable in the data, such as bias, IP infringement, or the propensity to guess or hallucinate.&lt;/p&gt;&lt;p&gt;Such models don’t “complete your thought” — they reason from their user’s &lt;strong&gt;content&lt;/strong&gt;. Their knowledge base. Their documents. Their data. If the answer’s not there, these models say so. That’s what makes such AI models explainable, traceable, deterministic, and a good option in places where hallucinations are unacceptable.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;A 5-Step Playbook for AI Accountability&lt;/strong&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Map the AI landscape&lt;/strong&gt; – Where is AI used across your business? What decisions are they influencing? What premium do you place on being able to trace those decisions back to transparent analysis on reliable source material?&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Align your organization&lt;/strong&gt; – Depending on the scope of your AI deployment, set up roles, committees, processes, and audit practices as rigorous as those for financial or cybersecurity risks.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Bring AI into board-level risk&lt;/strong&gt; – If your AI talks to customers or regulators, it belongs in your risk reports. Governance is not a sideshow.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Treat vendors like co-liabilities&lt;/strong&gt; – If your vendor’s AI makes things up, you still own the fallout. Extend your AI Accountability principles to them.&amp;nbsp; Demand documentation, audit rights, and SLAs for explainability and hallucination rates.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Train skepticism&lt;/strong&gt; – Your team should treat AI like a junior analyst — useful, but not infallible. Celebrate when someone identifies a hallucination. Trust must be earned.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;strong&gt;The Future of AI in the Enterprise &lt;/strong&gt;is not bigger models. What is needed is more precision, more transparency, more trust, and more accountability.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.unite.ai/wp-content/uploads/2025/05/AI-Hallucinations-Risk-942x600.png" /&gt;&lt;/div&gt;&lt;p&gt;It sounds right. It looks right. It’s wrong. That’s your AI on hallucination. The issue isn’t just that today’s generative AI models hallucinate. It’s that we feel if we build enough guardrails, fine-tune it, RAG it, and tame it somehow, then we will be able to adopt it at Enterprise scale.&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Study&lt;/th&gt;&lt;th&gt;Domain&lt;/th&gt;&lt;th&gt;Hallucination Rate&lt;/th&gt;&lt;th&gt;Key Findings&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Stanford HAI &amp;amp; RegLab (Jan 2024)&lt;/td&gt;&lt;td&gt;Legal&lt;/td&gt;&lt;td&gt;69%–88%&lt;/td&gt;&lt;td&gt;LLMs exhibited high hallucination rates when responding to legal queries, often lacking self-awareness about their errors and reinforcing incorrect legal assumptions.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;JMIR Study (2024)&lt;/td&gt;&lt;td&gt;Academic References&lt;/td&gt;&lt;td&gt;GPT-3.5: 90.6%, GPT-4: 86.6%, Bard: 100%&lt;/td&gt;&lt;td&gt;LLM-generated references were often irrelevant, incorrect, or unsupported by available literature.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;UK Study on AI-Generated Content (Feb 2025)&lt;/td&gt;&lt;td&gt;Finance&lt;/td&gt;&lt;td&gt;Not specified&lt;/td&gt;&lt;td&gt;AI-generated disinformation increased the risk of bank runs, with a significant portion of bank customers considering moving their money after viewing AI-generated fake content.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;World Economic Forum Global Risks Report (2025)&lt;/td&gt;&lt;td&gt;Global Risk Assessment&lt;/td&gt;&lt;td&gt;Not specified&lt;/td&gt;&lt;td&gt;Misinformation and disinformation, amplified by AI, ranked as the top global risk over a two-year outlook.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Vectara Hallucination Leaderboard (2025)&lt;/td&gt;&lt;td&gt;AI Model Evaluation&lt;/td&gt;&lt;td&gt;GPT-4.5-Preview: 1.2%, Google Gemini-2.0-Pro-Exp: 0.8%, Vectara Mockingbird-2-Echo: 0.9%&lt;/td&gt;&lt;td&gt;Evaluated hallucination rates across various LLMs, revealing significant differences in performance and accuracy.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Arxiv Study on Factuality Hallucination (2024)&lt;/td&gt;&lt;td&gt;AI Research&lt;/td&gt;&lt;td&gt;Not specified&lt;/td&gt;&lt;td&gt;Introduced HaluEval 2.0 to systematically study and detect hallucinations in LLMs, focusing on factual inaccuracies.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;&lt;strong&gt;Hallucination rates span from 0.8% to 88%&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Yes, it depends on the model, domain, use case, and context, but that spread should rattle any enterprise decision maker. These aren’t edge case errors. They’re systemic.&amp;nbsp; How do you make the right call when it comes to AI adoption in your enterprise? Where, how, how deep, how wide?&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;And examples of real-world consequences of this come across your newsfeed every day.&amp;nbsp; G20’s Financial Stability Board has flagged generative AI as a vector for disinformation that could cause market crises, political instability, and worse–flash crashes, fake news, and fraud. In another recently reported story, law firm Morgan &amp;amp; Morgan issued an emergency memo to all attorneys: Do not submit AI-generated filings without checking. Fake case law is a “fireable” offense.&lt;/p&gt;&lt;p&gt;This may not be the best time to bet the farm on hallucination rates tending to zero any time soon. Especially in regulated industries, such as legal, life sciences, capital markets, or in others, where the cost of a mistake could be high, including publishing higher education.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Hallucination is not a Rounding Error&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;This isn’t about an occasional wrong answer. It’s about &lt;strong&gt;risk&lt;/strong&gt;: Reputational, Legal, Operational.&lt;/p&gt;&lt;p&gt;Generative AI isn’t a reasoning engine. It’s a statistical finisher, a stochastic parrot. It completes your prompt in the most likely way based on training data. Even the &lt;em&gt;true-sounding parts&lt;/em&gt; are guesses. We call the most absurd pieces “hallucinations,” but the entire output is a hallucination. A well-styled one. Still, it works, magically well—until it doesn’t.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;AI as Infrastructure&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;And yet, it’s important to say that AI will be ready for Enterprise-wide adoption when we start treating it like infrastructure, and not like magic. And where required, it must be transparent, explainable, and traceable. And if it is not, then quite simply, it is not ready for Enterprise-wide adoption for those use cases. &amp;nbsp;If AI is making decisions, it should be on your Board’s radar.&lt;/p&gt;&lt;p&gt;The EU’s AI Act is leading the charge here. High-risk domains like justice, healthcare, and infrastructure will be regulated like mission-critical systems. Documentation, testing, and explainability will be mandatory.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;What Enterprise Safe AI Models Do&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Companies that specialize in building enterprise-safe AI models, make a conscious decision to build AI differently. In their alternative AI architectures, the Language Models are not trained on data, so they are not “contaminated” with anything undesirable in the data, such as bias, IP infringement, or the propensity to guess or hallucinate.&lt;/p&gt;&lt;p&gt;Such models don’t “complete your thought” — they reason from their user’s &lt;strong&gt;content&lt;/strong&gt;. Their knowledge base. Their documents. Their data. If the answer’s not there, these models say so. That’s what makes such AI models explainable, traceable, deterministic, and a good option in places where hallucinations are unacceptable.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;A 5-Step Playbook for AI Accountability&lt;/strong&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Map the AI landscape&lt;/strong&gt; – Where is AI used across your business? What decisions are they influencing? What premium do you place on being able to trace those decisions back to transparent analysis on reliable source material?&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Align your organization&lt;/strong&gt; – Depending on the scope of your AI deployment, set up roles, committees, processes, and audit practices as rigorous as those for financial or cybersecurity risks.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Bring AI into board-level risk&lt;/strong&gt; – If your AI talks to customers or regulators, it belongs in your risk reports. Governance is not a sideshow.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Treat vendors like co-liabilities&lt;/strong&gt; – If your vendor’s AI makes things up, you still own the fallout. Extend your AI Accountability principles to them.&amp;nbsp; Demand documentation, audit rights, and SLAs for explainability and hallucination rates.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Train skepticism&lt;/strong&gt; – Your team should treat AI like a junior analyst — useful, but not infallible. Celebrate when someone identifies a hallucination. Trust must be earned.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;strong&gt;The Future of AI in the Enterprise &lt;/strong&gt;is not bigger models. What is needed is more precision, more transparency, more trust, and more accountability.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.unite.ai/when-your-ai-invents-facts-the-enterprise-risk-no-leader-can-ignore/</guid><pubDate>Fri, 06 Jun 2025 17:16:13 +0000</pubDate></item><item><title>Building Confidence in AI: Training Programs Help Close Knowledge Gaps (Unite.AI)</title><link>https://www.unite.ai/building-confidence-in-ai-training-programs-help-close-knowledge-gaps/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.unite.ai/wp-content/uploads/2025/05/AI-Training-942x600.png" /&gt;&lt;/div&gt;&lt;p&gt;AI is reshaping the workforce at a breakneck speed, yet training efforts aren’t meeting the moment. Despite a quarter of executives feeling bullish on the technology, only 12% of workers have received AI-related training in the past year. This lack of preparation not only hinders the successful and safe adoption of AI, but also creates uncertainty amongst employees around the technology’s impact on their jobs. As the gap between executive excitement and employee reluctance grows, it’s clear that organizations need training tools to help build AI confidence and usher in this new era of innovation.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;AI will enhance, not replace&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Perhaps the most important factor of building AI confidence is helping employees understand how the technology will fit into their roles. Despite the amount of misinformation floating around, in most instances, AI is not meant to replace employees. In fact, recent companies that attempted to replace humans with AI are struggling to achieve the ROI they imagined. Instead, the real value of AI comes from using it to augment employee skill sets, productivity, and competitiveness in their fields. By efficiently handling more routine and administrative-heavy tasks, the technology allows employees to focus on higher-value tasks.&lt;/p&gt;&lt;p&gt;However, it’s just as important to note that integrating AI does not make this possible on its own, employees must understand how to use it effectively to unlock its full potential. Without the right training, AI can lead to concerns around data privacy, bias, and inaccuracies – making this foundational knowledge non-negotiable. That’s why both upskilling &lt;em&gt;and &lt;/em&gt;cross-skilling are essential to keeping pace with change.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Upskilling vs cross-skilling&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Upskilling and cross-skilling training both are used to help employees expand their skill sets and are critical tools when looking to adopt AI. While similar, it’s important to understand the difference between the two.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Upskilling&lt;/strong&gt; is the process of strengthening existing skills and focuses on helping employees advance in their job and gain higher responsibilities. A great example of upskilling is training IT leaders – who already have a strong foundation in technology – to gain a deeper understanding of AI.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Cross-skilling&lt;/strong&gt; is just as important, but it’s often overlooked in AI training. Cross-skilling (also known as cross-training) is the process of developing new skills that apply across different functions and focuses on training more than one employee in an organizational task. The adoption of AI and cross-skilling strategies must also be done simultaneously to ensure success. A great example to demonstrate cross-skilling would be a marketing leader with minimal technology background. As AI is increasingly used across departments, cross-skilling ensures that every employee is able to use the technology based on their specific roles and responsibilities.&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;strong&gt;Benefits of training in the age of AI &lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;With industries, markets, and everyday business practices evolving, employee skills and knowledge remain the bedrock of organizational innovation. Employees want purpose and impact, and aligning corporate goals with employee ambitions is a guaranteed way to boost engagement. In addition, providing employees with the ability to alleviate burdensome tasks through AI helps boost overall satisfaction at work.&lt;/p&gt;&lt;p&gt;In an increasingly competitive landscape, meeting these needs and retaining top talent is crucial to sustaining productivity and growth. And while recent arguments state that those who already possess AI skillsets will take over jobs, 79% of learning and development professionals believe that it’s less expensive to reskill a current employee than to hire a new one.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Upskilling and cross-skilling in action &lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;If upskilling and cross-skilling are not&amp;nbsp; a current part of&amp;nbsp; a learning and development program, organizations can leverage resources they already have available. Here are some best practices when getting started:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Assess Current Skillsets:&lt;/strong&gt; Identifying upskilling and cross-skilling priorities is more difficult without a base-level understanding of the skillsets one’s employee base possesses, and which ones they will need to build confidence in AI. Given teams are already familiar with their roles and the organization as a whole, surveying the current level of AI knowledge and identifying&amp;nbsp; gaps is a great place to start.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Set Attainable Goals: &lt;/strong&gt;With this foundational understanding of your workforce, the next step is to set upskilling and cross-skilling goals. It’s important to understand the “why” behind these training programs and identify where employees can and should grow. Goals should be set on an individual contributor level, while also identifying objectives for larger teams and the organization as a whole.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Rethink Learning Formats: &lt;/strong&gt;Even the most robust training programs won’t move the needle if it's not delivered in a format that resonates with your workforce. In fact, 86% of companies are unhappy with their existing training programs that they have in place. Employers are increasingly finding that live or in-person training programs no longer suffice. Instead, video-based learning that offers flexibility and better accessibility to various learning styles may be the best route for highly-complex topics like AI.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Prioritize Responsible AI:&lt;/strong&gt; Implementing data privacy, security and data governance best practices is a crucial step in ensuring that employees use AI responsibly. In addition, implementing a bias and transparency framework to validate AI output and build confidence with AI effectiveness within the organization can be crucial. To help with this, organizations should consider building “AI champions” to teach employees how to effectively use AI so that humans can benefit from the productivity gains and yet have the skills to protect from hallucinations and bias.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Monitor and Promote: &lt;/strong&gt;For upskilling and cross-skilling to be impactful, employees need to have the opportunity to expand their responsibilities. Organizations should enable a reward structure that motivates employees to look for creative ways to use AI to help improve departmental and organizational efficiency and fast track innovation.&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;strong&gt;The bottom line &lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;While AI holds exponential promise for the modern workplace, employees are the linchpins who will determine its success. Regardless of their role, department, or expertise, having a foundation of AI knowledge will benefit career trajectories and the business as whole. By focusing not only on upskilling tech-forward employees, but cross-skilling to create a larger AI-centric culture, organizations can reap the benefits of improved engagement, talent retention, and competitive market expertise.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.unite.ai/wp-content/uploads/2025/05/AI-Training-942x600.png" /&gt;&lt;/div&gt;&lt;p&gt;AI is reshaping the workforce at a breakneck speed, yet training efforts aren’t meeting the moment. Despite a quarter of executives feeling bullish on the technology, only 12% of workers have received AI-related training in the past year. This lack of preparation not only hinders the successful and safe adoption of AI, but also creates uncertainty amongst employees around the technology’s impact on their jobs. As the gap between executive excitement and employee reluctance grows, it’s clear that organizations need training tools to help build AI confidence and usher in this new era of innovation.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;AI will enhance, not replace&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Perhaps the most important factor of building AI confidence is helping employees understand how the technology will fit into their roles. Despite the amount of misinformation floating around, in most instances, AI is not meant to replace employees. In fact, recent companies that attempted to replace humans with AI are struggling to achieve the ROI they imagined. Instead, the real value of AI comes from using it to augment employee skill sets, productivity, and competitiveness in their fields. By efficiently handling more routine and administrative-heavy tasks, the technology allows employees to focus on higher-value tasks.&lt;/p&gt;&lt;p&gt;However, it’s just as important to note that integrating AI does not make this possible on its own, employees must understand how to use it effectively to unlock its full potential. Without the right training, AI can lead to concerns around data privacy, bias, and inaccuracies – making this foundational knowledge non-negotiable. That’s why both upskilling &lt;em&gt;and &lt;/em&gt;cross-skilling are essential to keeping pace with change.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Upskilling vs cross-skilling&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Upskilling and cross-skilling training both are used to help employees expand their skill sets and are critical tools when looking to adopt AI. While similar, it’s important to understand the difference between the two.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Upskilling&lt;/strong&gt; is the process of strengthening existing skills and focuses on helping employees advance in their job and gain higher responsibilities. A great example of upskilling is training IT leaders – who already have a strong foundation in technology – to gain a deeper understanding of AI.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Cross-skilling&lt;/strong&gt; is just as important, but it’s often overlooked in AI training. Cross-skilling (also known as cross-training) is the process of developing new skills that apply across different functions and focuses on training more than one employee in an organizational task. The adoption of AI and cross-skilling strategies must also be done simultaneously to ensure success. A great example to demonstrate cross-skilling would be a marketing leader with minimal technology background. As AI is increasingly used across departments, cross-skilling ensures that every employee is able to use the technology based on their specific roles and responsibilities.&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;strong&gt;Benefits of training in the age of AI &lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;With industries, markets, and everyday business practices evolving, employee skills and knowledge remain the bedrock of organizational innovation. Employees want purpose and impact, and aligning corporate goals with employee ambitions is a guaranteed way to boost engagement. In addition, providing employees with the ability to alleviate burdensome tasks through AI helps boost overall satisfaction at work.&lt;/p&gt;&lt;p&gt;In an increasingly competitive landscape, meeting these needs and retaining top talent is crucial to sustaining productivity and growth. And while recent arguments state that those who already possess AI skillsets will take over jobs, 79% of learning and development professionals believe that it’s less expensive to reskill a current employee than to hire a new one.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Upskilling and cross-skilling in action &lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;If upskilling and cross-skilling are not&amp;nbsp; a current part of&amp;nbsp; a learning and development program, organizations can leverage resources they already have available. Here are some best practices when getting started:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Assess Current Skillsets:&lt;/strong&gt; Identifying upskilling and cross-skilling priorities is more difficult without a base-level understanding of the skillsets one’s employee base possesses, and which ones they will need to build confidence in AI. Given teams are already familiar with their roles and the organization as a whole, surveying the current level of AI knowledge and identifying&amp;nbsp; gaps is a great place to start.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Set Attainable Goals: &lt;/strong&gt;With this foundational understanding of your workforce, the next step is to set upskilling and cross-skilling goals. It’s important to understand the “why” behind these training programs and identify where employees can and should grow. Goals should be set on an individual contributor level, while also identifying objectives for larger teams and the organization as a whole.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Rethink Learning Formats: &lt;/strong&gt;Even the most robust training programs won’t move the needle if it's not delivered in a format that resonates with your workforce. In fact, 86% of companies are unhappy with their existing training programs that they have in place. Employers are increasingly finding that live or in-person training programs no longer suffice. Instead, video-based learning that offers flexibility and better accessibility to various learning styles may be the best route for highly-complex topics like AI.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Prioritize Responsible AI:&lt;/strong&gt; Implementing data privacy, security and data governance best practices is a crucial step in ensuring that employees use AI responsibly. In addition, implementing a bias and transparency framework to validate AI output and build confidence with AI effectiveness within the organization can be crucial. To help with this, organizations should consider building “AI champions” to teach employees how to effectively use AI so that humans can benefit from the productivity gains and yet have the skills to protect from hallucinations and bias.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Monitor and Promote: &lt;/strong&gt;For upskilling and cross-skilling to be impactful, employees need to have the opportunity to expand their responsibilities. Organizations should enable a reward structure that motivates employees to look for creative ways to use AI to help improve departmental and organizational efficiency and fast track innovation.&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;strong&gt;The bottom line &lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;While AI holds exponential promise for the modern workplace, employees are the linchpins who will determine its success. Regardless of their role, department, or expertise, having a foundation of AI knowledge will benefit career trajectories and the business as whole. By focusing not only on upskilling tech-forward employees, but cross-skilling to create a larger AI-centric culture, organizations can reap the benefits of improved engagement, talent retention, and competitive market expertise.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.unite.ai/building-confidence-in-ai-training-programs-help-close-knowledge-gaps/</guid><pubDate>Fri, 06 Jun 2025 17:18:13 +0000</pubDate></item><item><title>Sam Altman calls for ‘AI privilege’ as OpenAI clarifies court order to retain temporary and deleted ChatGPT sessions (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/sam-altman-calls-for-ai-privilege-as-openai-clarifies-court-order-to-retain-temporary-and-deleted-chatgpt-sessions/</link><description>[unable to retrieve full-text content]Should talking to an AI chatbot be protected and privileged information, like talking to a doctor or lawyer? A new court order raises the idea</description><content:encoded>[unable to retrieve full-text content]Should talking to an AI chatbot be protected and privileged information, like talking to a doctor or lawyer? A new court order raises the idea</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/sam-altman-calls-for-ai-privilege-as-openai-clarifies-court-order-to-retain-temporary-and-deleted-chatgpt-sessions/</guid><pubDate>Fri, 06 Jun 2025 17:48:53 +0000</pubDate></item><item><title>Inside Anthropic’s AI ambitions with Jared Kaplan (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/podcast/inside-anthropics-ai-ambitions-with-jared-kaplan/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/54570800231_8b80a7a0e3_o.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;If you’ve been following Anthropic, you’ll know it’s been a busy year for the AI startup. Back in March, the company ⁠announced that it raised $3.5 billion⁠ at a $61.5 billion valuation in a round led by Lightspeed Venture Partners. Since then, it’s ⁠launched a blog for its Claude models⁠ and, according to Bloomberg reporting, ⁠partnered with Apple⁠ to power a new “vibe-coding” software platform.&lt;/p&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;Instead of our usual Friday news rundown, today’s episode of Equity brings you a conversation from this week’s TechCrunch Sessions: AI event in Berkeley. Our friend and co-host Max Zeff sat down with Jared Kaplan, co-founder and chief science officer at Anthropic.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full conversation to hear more about:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;⁠&lt;u&gt;⁠Who has direct access to Claude’s AI models⁠&lt;/u&gt;⁠, Windsurf’s response, and how it all ties into Anthropic’s broader goals around openness, safety, and sustainability.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;The company’s pivot away from chatbots and toward agentic AI systems that can perform real tasks.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;How internal tools like Claude Code are shaping the future of AI-powered development.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;What it means to build AI that enterprises can actually trust, and how that affects the way humans interact with software, work, and each other.&lt;/li&gt;
&lt;/ul&gt;



&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Equity is TechCrunch’s flagship podcast, produced by Theresa Loconsolo, and posts every Wednesday and Friday. Subscribe to us on&lt;/em&gt;⁠&lt;u&gt;⁠&lt;/u&gt;&lt;em&gt; Apple Podcasts&lt;/em&gt;&lt;u&gt;⁠&lt;/u&gt;⁠&lt;em&gt;,&lt;/em&gt;⁠&lt;u&gt;⁠&lt;/u&gt;&lt;em&gt; Overcast&lt;/em&gt;&lt;u&gt;⁠&lt;/u&gt;⁠&lt;em&gt;,&lt;/em&gt;⁠&lt;u&gt;⁠&lt;/u&gt;&lt;em&gt; Spotify&lt;/em&gt;&lt;u&gt;⁠&lt;/u&gt;⁠&lt;em&gt; and all the casts. You also can follow Equity on&lt;/em&gt;⁠&lt;u&gt;⁠&lt;/u&gt;&lt;em&gt; X&lt;/em&gt;&lt;u&gt;⁠&lt;/u&gt;⁠&lt;em&gt; and&lt;/em&gt;⁠&lt;u&gt;⁠&lt;/u&gt;&lt;em&gt; Threads&lt;/em&gt;&lt;u&gt;⁠&lt;/u&gt;⁠&lt;em&gt;, at @EquityPod.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/54570800231_8b80a7a0e3_o.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;If you’ve been following Anthropic, you’ll know it’s been a busy year for the AI startup. Back in March, the company ⁠announced that it raised $3.5 billion⁠ at a $61.5 billion valuation in a round led by Lightspeed Venture Partners. Since then, it’s ⁠launched a blog for its Claude models⁠ and, according to Bloomberg reporting, ⁠partnered with Apple⁠ to power a new “vibe-coding” software platform.&lt;/p&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;Instead of our usual Friday news rundown, today’s episode of Equity brings you a conversation from this week’s TechCrunch Sessions: AI event in Berkeley. Our friend and co-host Max Zeff sat down with Jared Kaplan, co-founder and chief science officer at Anthropic.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full conversation to hear more about:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;⁠&lt;u&gt;⁠Who has direct access to Claude’s AI models⁠&lt;/u&gt;⁠, Windsurf’s response, and how it all ties into Anthropic’s broader goals around openness, safety, and sustainability.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;The company’s pivot away from chatbots and toward agentic AI systems that can perform real tasks.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;How internal tools like Claude Code are shaping the future of AI-powered development.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;What it means to build AI that enterprises can actually trust, and how that affects the way humans interact with software, work, and each other.&lt;/li&gt;
&lt;/ul&gt;



&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Equity is TechCrunch’s flagship podcast, produced by Theresa Loconsolo, and posts every Wednesday and Friday. Subscribe to us on&lt;/em&gt;⁠&lt;u&gt;⁠&lt;/u&gt;&lt;em&gt; Apple Podcasts&lt;/em&gt;&lt;u&gt;⁠&lt;/u&gt;⁠&lt;em&gt;,&lt;/em&gt;⁠&lt;u&gt;⁠&lt;/u&gt;&lt;em&gt; Overcast&lt;/em&gt;&lt;u&gt;⁠&lt;/u&gt;⁠&lt;em&gt;,&lt;/em&gt;⁠&lt;u&gt;⁠&lt;/u&gt;&lt;em&gt; Spotify&lt;/em&gt;&lt;u&gt;⁠&lt;/u&gt;⁠&lt;em&gt; and all the casts. You also can follow Equity on&lt;/em&gt;⁠&lt;u&gt;⁠&lt;/u&gt;&lt;em&gt; X&lt;/em&gt;&lt;u&gt;⁠&lt;/u&gt;⁠&lt;em&gt; and&lt;/em&gt;⁠&lt;u&gt;⁠&lt;/u&gt;&lt;em&gt; Threads&lt;/em&gt;&lt;u&gt;⁠&lt;/u&gt;⁠&lt;em&gt;, at @EquityPod.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/podcast/inside-anthropics-ai-ambitions-with-jared-kaplan/</guid><pubDate>Fri, 06 Jun 2025 19:09:34 +0000</pubDate></item><item><title>Figure AI CEO skips live demo, sidesteps BMW deal questions onstage at tech conference (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/06/figure-ai-ceo-skips-live-demo-sidesteps-bmw-deal-questions-on-stage-at-tech-conference/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/figure-helix-1.jpg?resize=1200,628" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Brett Adcock, co-founder and CEO of the humanoid robotics startup Figure AI, made a rare public appearance at the Bloomberg Tech conference on Thursday. Figure has recently been the subject of a couple news articles that questioned its progress with marquee customer BMW. Figure objected so strenuously to at least one of these reports that Adcock publicly threatened to sue the publication.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When asked about the skepticism surrounding the BMW relationship and whether it is a pilot or has commercial value to the company, Adcock replied with an explanation of the technical benefit of having robots on a factory floor but didn’t provide specifics about the contractual relationship with BMW.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We get a lot of value, and it’s really important that we need to figure out how to run robots every day. We get to see how well they perform. We get to track all the metrics,” he said. Two months ago, Figure also published a YouTube video showing a couple of its robots working in a BMW factory.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Adcock did, however, say that Figure AI has signed a contract with a second, unnamed customer for initial deployment, a customer that Bloomberg has reported to be UPS.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Figure AI has drawn attention for making claims that its AI-powered robots possess human-like fine motor skills and can manipulate objects with precision. Despite releasing numerous videos of its robots at work, the company hasn’t done a live demonstration of the humanoids.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The interviewer, Bloomberg’s Ed Ludlow, pointed out that while two other robotics companies, Agility Robotics and Boston Dynamics, showcased their robots at the conference, Figure AI did not. “It kind of goes back to our whole philosophy around we don’t go to a lot of events,” said Adcock. “I think it’s a giant waste of time. To be frank, I have to bring a team here to bring robots here. They could be at the office,” he said, adding that the company is showcasing the robots in videos.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Adcock confirmed that Figure AI is expecting to manufacture and deploy roughly 100,000 units within four years.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;The skepticism about Figure’s commercial relationship comes amid the company’s attempts to raise a $1.5 billion round at a $39.5 billion valuation, sources told Bloomberg, a fifteenfold increase from the $2.6 billion valuation it achieved in February 2024.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch reported in April that Figure AI has been issuing cease-and-desist letters to secondary market brokers, demanding they stop marketing its shares because they are not authorized to do so.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/figure-helix-1.jpg?resize=1200,628" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Brett Adcock, co-founder and CEO of the humanoid robotics startup Figure AI, made a rare public appearance at the Bloomberg Tech conference on Thursday. Figure has recently been the subject of a couple news articles that questioned its progress with marquee customer BMW. Figure objected so strenuously to at least one of these reports that Adcock publicly threatened to sue the publication.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When asked about the skepticism surrounding the BMW relationship and whether it is a pilot or has commercial value to the company, Adcock replied with an explanation of the technical benefit of having robots on a factory floor but didn’t provide specifics about the contractual relationship with BMW.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We get a lot of value, and it’s really important that we need to figure out how to run robots every day. We get to see how well they perform. We get to track all the metrics,” he said. Two months ago, Figure also published a YouTube video showing a couple of its robots working in a BMW factory.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Adcock did, however, say that Figure AI has signed a contract with a second, unnamed customer for initial deployment, a customer that Bloomberg has reported to be UPS.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Figure AI has drawn attention for making claims that its AI-powered robots possess human-like fine motor skills and can manipulate objects with precision. Despite releasing numerous videos of its robots at work, the company hasn’t done a live demonstration of the humanoids.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The interviewer, Bloomberg’s Ed Ludlow, pointed out that while two other robotics companies, Agility Robotics and Boston Dynamics, showcased their robots at the conference, Figure AI did not. “It kind of goes back to our whole philosophy around we don’t go to a lot of events,” said Adcock. “I think it’s a giant waste of time. To be frank, I have to bring a team here to bring robots here. They could be at the office,” he said, adding that the company is showcasing the robots in videos.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Adcock confirmed that Figure AI is expecting to manufacture and deploy roughly 100,000 units within four years.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;The skepticism about Figure’s commercial relationship comes amid the company’s attempts to raise a $1.5 billion round at a $39.5 billion valuation, sources told Bloomberg, a fifteenfold increase from the $2.6 billion valuation it achieved in February 2024.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch reported in April that Figure AI has been issuing cease-and-desist letters to secondary market brokers, demanding they stop marketing its shares because they are not authorized to do so.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/06/figure-ai-ceo-skips-live-demo-sidesteps-bmw-deal-questions-on-stage-at-tech-conference/</guid><pubDate>Fri, 06 Jun 2025 19:32:56 +0000</pubDate></item><item><title>Anthropic appoints a national security expert to its governing trust (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/06/anthropic-appoints-a-national-security-expert-to-its-governing-trust/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2153561878.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A day after announcing new AI models designed for U.S. national security applications, Anthropic has appointed a national security expert, Richard Fontaine, to its long-term benefit trust. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic’s long-term benefit trust is a governance mechanism that Anthropic claims helps it promote safety over profit, and which has the power to elect some of the company’s board of directors. The trust’s other members include Centre for Effective Altruism CEO Zachary Robinson, Clinton Health Access Initiative CEO Neil Buddy Shah, and Evidence Action President Kanika Bahl.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In a statement, Anthropic CEO Dario Amodei said that Fontaine’s hiring will “[strengthen] the trust’s ability to guide Anthropic through complex decisions” about AI as it relates to security. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Richard’s expertise comes at a critical time as advanced AI capabilities increasingly intersect with national security considerations,” Amodei continued. “I’ve long believed that ensuring democratic nations maintain leadership in responsible AI development is essential for both global security and the common good.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fontaine, who as a trustee won’t have a financial stake in Anthropic, previously served as a foreign policy adviser to the late Sen. John McCain and was an adjunct professor at Georgetown teaching security studies. For more than six years, he led the Center for A New American Security, a national security think tank based in Washington, D.C., as its president.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic has increasingly engaged U.S. national security customers as it looks for new sources of revenue.&amp;nbsp;In November, the company teamed up with Palantir and AWS, the cloud computing division of Anthropic’s major partner and investor, Amazon, to sell Anthropic’s AI to defense customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To be clear, Anthropic isn’t the only top AI lab going after defense contracts. OpenAI is&amp;nbsp;seeking&amp;nbsp;to establish a closer relationship with the U.S. Defense Department, and Meta recently&amp;nbsp;revealed&amp;nbsp;that it’s making its&amp;nbsp;Llama&amp;nbsp;models available to defense partners. Meanwhile, Google is&amp;nbsp;refining&amp;nbsp;a version of its Gemini AI capable of working within classified environments, and Cohere, which primarily builds AI products for businesses, is also collaborating with Palantir to deploy its AI models.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Fontaine’s hiring comes as Anthropic beefs up its executive ranks. In May, the company named Netflix co-founder Reed Hastings to its board.&lt;br /&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2153561878.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A day after announcing new AI models designed for U.S. national security applications, Anthropic has appointed a national security expert, Richard Fontaine, to its long-term benefit trust. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic’s long-term benefit trust is a governance mechanism that Anthropic claims helps it promote safety over profit, and which has the power to elect some of the company’s board of directors. The trust’s other members include Centre for Effective Altruism CEO Zachary Robinson, Clinton Health Access Initiative CEO Neil Buddy Shah, and Evidence Action President Kanika Bahl.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In a statement, Anthropic CEO Dario Amodei said that Fontaine’s hiring will “[strengthen] the trust’s ability to guide Anthropic through complex decisions” about AI as it relates to security. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Richard’s expertise comes at a critical time as advanced AI capabilities increasingly intersect with national security considerations,” Amodei continued. “I’ve long believed that ensuring democratic nations maintain leadership in responsible AI development is essential for both global security and the common good.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fontaine, who as a trustee won’t have a financial stake in Anthropic, previously served as a foreign policy adviser to the late Sen. John McCain and was an adjunct professor at Georgetown teaching security studies. For more than six years, he led the Center for A New American Security, a national security think tank based in Washington, D.C., as its president.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic has increasingly engaged U.S. national security customers as it looks for new sources of revenue.&amp;nbsp;In November, the company teamed up with Palantir and AWS, the cloud computing division of Anthropic’s major partner and investor, Amazon, to sell Anthropic’s AI to defense customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To be clear, Anthropic isn’t the only top AI lab going after defense contracts. OpenAI is&amp;nbsp;seeking&amp;nbsp;to establish a closer relationship with the U.S. Defense Department, and Meta recently&amp;nbsp;revealed&amp;nbsp;that it’s making its&amp;nbsp;Llama&amp;nbsp;models available to defense partners. Meanwhile, Google is&amp;nbsp;refining&amp;nbsp;a version of its Gemini AI capable of working within classified environments, and Cohere, which primarily builds AI products for businesses, is also collaborating with Palantir to deploy its AI models.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Fontaine’s hiring comes as Anthropic beefs up its executive ranks. In May, the company named Netflix co-founder Reed Hastings to its board.&lt;br /&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/06/anthropic-appoints-a-national-security-expert-to-its-governing-trust/</guid><pubDate>Fri, 06 Jun 2025 20:53:44 +0000</pubDate></item><item><title>Why investing in growth-stage AI startups is getting riskier and more complicated (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/06/why-investing-in-growth-stage-ai-startups-is-getting-riskier-and-more-complicated/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/AI-Sessions-VC-Sessions-.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Making a bet on AI startups has never been so exciting — or more risky. Incumbents like OpenAI, Microsoft, and Google are scaling their capabilities fast to swallow many of the offerings of smaller companies. At the same time, new startups are reaching the growth stage much faster than they historically have.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But defining “growth stage” in AI startups is not so cut-and-dried today.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Jill Chase, partner at CapitalG, said on stage at TechCrunch AI Sessions on Thursday that she’s seeing more companies that are only a year old, yet have already reached tens of millions in annual recurring revenue and more than $1 billion in valuation. While those companies might be defined as mature due to their valuation and revenue generation, they often lack much of the necessary safety, hiring, and executive infrastructure.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“On one hand, that’s really exciting. It represents this brand new trend of extremely fast growth, which is awesome,” Chase said. “On the other hand, it’s a little bit scary because I’m gonna pay at an $X billion valuation for this company that didn’t exist 12 months ago, and things are changing so quickly.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Who knows who is in a garage somewhere, maybe in this audience somewhere, starting a company that in 12 months will be a lot better than this one I’m investing in that’s at $50 million ARR today,” Chase continued. “So it’s made growth investing a little confusing.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To cut through the noise, Chase said it’s important for investors to feel good about the category and the “ability of the founder to very quickly adapt and see around corners.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;She noted that AI coding startup Cursor is a great example of a company that “jumped on the exact right use case of AI code generation that was available and possible given the technology at the time.”&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;However, Cursor will need to work to maintain its edge.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There will be, by the end of this year, AI software engineers,” Chase said. “In that scenario, what Cursor has today is going to be a little less relevant. It is incumbent on the Cursor team to see that future and to think, okay, how do I start building my product so that when those models come out and are much more powerful, the product surface represents those and I can very quickly plug those in and switch into that state of code generation?”&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/AI-Sessions-VC-Sessions-.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Making a bet on AI startups has never been so exciting — or more risky. Incumbents like OpenAI, Microsoft, and Google are scaling their capabilities fast to swallow many of the offerings of smaller companies. At the same time, new startups are reaching the growth stage much faster than they historically have.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But defining “growth stage” in AI startups is not so cut-and-dried today.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Jill Chase, partner at CapitalG, said on stage at TechCrunch AI Sessions on Thursday that she’s seeing more companies that are only a year old, yet have already reached tens of millions in annual recurring revenue and more than $1 billion in valuation. While those companies might be defined as mature due to their valuation and revenue generation, they often lack much of the necessary safety, hiring, and executive infrastructure.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“On one hand, that’s really exciting. It represents this brand new trend of extremely fast growth, which is awesome,” Chase said. “On the other hand, it’s a little bit scary because I’m gonna pay at an $X billion valuation for this company that didn’t exist 12 months ago, and things are changing so quickly.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Who knows who is in a garage somewhere, maybe in this audience somewhere, starting a company that in 12 months will be a lot better than this one I’m investing in that’s at $50 million ARR today,” Chase continued. “So it’s made growth investing a little confusing.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To cut through the noise, Chase said it’s important for investors to feel good about the category and the “ability of the founder to very quickly adapt and see around corners.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;She noted that AI coding startup Cursor is a great example of a company that “jumped on the exact right use case of AI code generation that was available and possible given the technology at the time.”&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;However, Cursor will need to work to maintain its edge.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There will be, by the end of this year, AI software engineers,” Chase said. “In that scenario, what Cursor has today is going to be a little less relevant. It is incumbent on the Cursor team to see that future and to think, okay, how do I start building my product so that when those models come out and are much more powerful, the product surface represents those and I can very quickly plug those in and switch into that state of code generation?”&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/06/why-investing-in-growth-stage-ai-startups-is-getting-riskier-and-more-complicated/</guid><pubDate>Fri, 06 Jun 2025 21:14:03 +0000</pubDate></item><item><title>2025 will be a ‘pivotal year’ for Meta’s augmented and virtual reality, says CTO (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/06/2025-will-be-a-pivotal-year-for-metas-augmented-and-virtual-reality-says-cto/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/andrew-bosworth-meta-bloomberg-tech.jpg?resize=1200,660" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta CTO Andrew “Boz” Bosworth, who was one of the company’s first 15 engineers, published a memo earlier this year forecasting that 2025 could be the year of greatness for Reality Labs, the company’s augmented and virtual reality unit. Or, it would be the year when the metaverse goes down as a “legendary misadventure.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These days, Boz appears to be leaning towards its potential for greatness. But, the market will be the final determinant.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We’ll judge at the end of the decade, but this does feel like the pivotal year,” Boz said Thursday during a Bloomberg Technology interview.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Boz noted that Meta’s Ray Ban AI glasses had been a breakthrough that excited both consumers and competitors. As of February, Meta has sold more than 2 million pairs since their October 2023 debut. Last fall, they outsold traditional Ray Bans, even before Meta rolled out AI features.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, Google last month announced partnerships with Gentle Monster and Warby Parker to create smart glasses based on Android XR. Apple is also reportedly making a push to release smart glasses in 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Suddenly, we go from toiling in the realms of obscurity to being very much in the world with a product that is very attractive to consumers, and thus competitors,” Boz said. “The clock has started on competition coming, and that just means that the progress we make in this year is of disproportionate value to any year before or after it closes.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, competition among other incumbents means nothing if the market doesn’t adopt Meta’s AR and VR products, which is what would drive the industry to standardize the technology.&amp;nbsp;&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;“The market is actually, especially when it comes to hardware, a trailing indicator,” Boz said. “So you look for early indicators. To some degree, you do have to have a level of confidence and taste in-house.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He said this was something he learned from Sheryl Sandberg, former chief operating officer at Meta.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Sheryl used to always talk about how most companies don’t fail because they got beaten by a competitor,” Boz said. “Most companies fail because they didn’t execute their own plan correctly. And so what I try to do with the team is really focus us, not so much on the competitive landscape as on [whether] we’re executing to our standards.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Meta CTO said the company has “a set of ambitious plans for the year” that it is on track for.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“What we’ll know by the end of the year is whether we executed on our plan or not,” said Boz. “What we’ll know in five years time is whether that was enough.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/andrew-bosworth-meta-bloomberg-tech.jpg?resize=1200,660" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta CTO Andrew “Boz” Bosworth, who was one of the company’s first 15 engineers, published a memo earlier this year forecasting that 2025 could be the year of greatness for Reality Labs, the company’s augmented and virtual reality unit. Or, it would be the year when the metaverse goes down as a “legendary misadventure.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These days, Boz appears to be leaning towards its potential for greatness. But, the market will be the final determinant.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We’ll judge at the end of the decade, but this does feel like the pivotal year,” Boz said Thursday during a Bloomberg Technology interview.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Boz noted that Meta’s Ray Ban AI glasses had been a breakthrough that excited both consumers and competitors. As of February, Meta has sold more than 2 million pairs since their October 2023 debut. Last fall, they outsold traditional Ray Bans, even before Meta rolled out AI features.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, Google last month announced partnerships with Gentle Monster and Warby Parker to create smart glasses based on Android XR. Apple is also reportedly making a push to release smart glasses in 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Suddenly, we go from toiling in the realms of obscurity to being very much in the world with a product that is very attractive to consumers, and thus competitors,” Boz said. “The clock has started on competition coming, and that just means that the progress we make in this year is of disproportionate value to any year before or after it closes.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, competition among other incumbents means nothing if the market doesn’t adopt Meta’s AR and VR products, which is what would drive the industry to standardize the technology.&amp;nbsp;&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;“The market is actually, especially when it comes to hardware, a trailing indicator,” Boz said. “So you look for early indicators. To some degree, you do have to have a level of confidence and taste in-house.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He said this was something he learned from Sheryl Sandberg, former chief operating officer at Meta.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Sheryl used to always talk about how most companies don’t fail because they got beaten by a competitor,” Boz said. “Most companies fail because they didn’t execute their own plan correctly. And so what I try to do with the team is really focus us, not so much on the competitive landscape as on [whether] we’re executing to our standards.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Meta CTO said the company has “a set of ambitious plans for the year” that it is on track for.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“What we’ll know by the end of the year is whether we executed on our plan or not,” said Boz. “What we’ll know in five years time is whether that was enough.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/06/2025-will-be-a-pivotal-year-for-metas-augmented-and-virtual-reality-says-cto/</guid><pubDate>Fri, 06 Jun 2025 21:52:20 +0000</pubDate></item><item><title>Will Musk vs. Trump affect xAI’s $5 billion debt deal? (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/07/will-musk-vs-trump-affect-xais-5-billion-debt-deal/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2200924488.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;While the online feud between Elon Musk and President Donald Trump seemed to drive traffic to Musk’s social media platform X (formerly Twitter), it could also create issues for the platform’s parent company xAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Musk merged X and xAI earlier this year, with Bloomberg reporting this week that he was looking to raise $5 billion in debt (as well as a reported $300 million in a secondary sale) to fund the combined company.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That’s led to some awkward moments as Musk’s relationship with his former ally Trump seemed to disintegrate. In fact, The Wall Street Journal reports that on Thursday afternoon, Morgan Stanley had gathered xAI executives to pitch potential investors as Musk and Trump were posting angrily about each other on their respective social networks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Morgan Stanley had reportedly hoped to sell the debt at around 100 cents on the dollar, but a trader told the WSJ it was trading at 95 cents on the dollar at times on Thursday. Investors also reportedly said that due to declining prices, Morgan Stanley may need to offer additional incentives, such as an increased interest rate.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2200924488.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;While the online feud between Elon Musk and President Donald Trump seemed to drive traffic to Musk’s social media platform X (formerly Twitter), it could also create issues for the platform’s parent company xAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Musk merged X and xAI earlier this year, with Bloomberg reporting this week that he was looking to raise $5 billion in debt (as well as a reported $300 million in a secondary sale) to fund the combined company.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That’s led to some awkward moments as Musk’s relationship with his former ally Trump seemed to disintegrate. In fact, The Wall Street Journal reports that on Thursday afternoon, Morgan Stanley had gathered xAI executives to pitch potential investors as Musk and Trump were posting angrily about each other on their respective social networks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Morgan Stanley had reportedly hoped to sell the debt at around 100 cents on the dollar, but a trader told the WSJ it was trading at 95 cents on the dollar at times on Thursday. Investors also reportedly said that due to declining prices, Morgan Stanley may need to offer additional incentives, such as an increased interest rate.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/07/will-musk-vs-trump-affect-xais-5-billion-debt-deal/</guid><pubDate>Sat, 07 Jun 2025 16:37:35 +0000</pubDate></item><item><title>Week in Review: Why Anthropic cut access to Windsurf (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/07/week-in-review-why-anthropic-cut-access-to-windsurf/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Welcome back to Week in Review! Got lots for you today, including why Windsurf lost access to Claude, ChatGPT’s new features, WWDC 2025, Elon Musk’s fight with Donald Trump, and lots more. Have a great weekend!&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Duh: &lt;/strong&gt;During an interview at TC Sessions: AI 2025, Anthropic’s co-founder had a perfectly reasonable explanation for why the company cut access to Windsurf: “I think it would be odd for us to be selling Claude to OpenAI,” Chief Science Officer Jared Kaplan said, referring to rumors and reports that OpenAI, its largest competitor, is acquiring the AI coding assistant. Seems like a good reason to me!&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Everything is the same: &lt;/strong&gt;Chinese lab DeepSeek released an updated version of its R1 reasoning AI model last week that performs well on a number of math and coding benchmarks. Now some AI researchers are speculating that at least some of the source data it trained on came from Google’s Gemini family of AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;WWDC 2025: &lt;/strong&gt;Apple’s annual developers conference starts Monday. Beyond a newly designed operating system, here’s what we’re expecting to see at this year’s event, including a dedicated gaming app and updates to Mac, Watch, TV, and more.&amp;nbsp;&lt;/p&gt;

&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;

&lt;p class="has-text-align-center wp-block-paragraph"&gt;&lt;em&gt;This is TechCrunch’s Week in Review, where we recap the week’s biggest news. Want this delivered as a newsletter to your inbox every Saturday? Sign up here.&lt;/em&gt;&lt;/p&gt;

&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;

&lt;h2 class="wp-block-heading" id="h-news"&gt;News&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="The OpenAI logo is seen displayed on a smartphone screen." class="wp-image-2995059" height="1280" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2206295463.jpg?w=680" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Thomas Fuller / SOPA Images / LightRocket / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Business in the front: &lt;/strong&gt;ChatGPT is getting new features for business users, including connectors for Dropbox, Box, SharePoint, OneDrive, and Google Drive. This would let ChatGPT look for information across your own services to answer questions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Oh no: &lt;/strong&gt;Indian grocery delivery startup KiranaPro was hacked, and all of its data was wiped. According to the company, it has 55,000 customers, with 30,000 to 35,000 active buyers across 50 cities, who collectively place 2,000 orders daily.&amp;nbsp;&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Artsy people, rejoice! &lt;/strong&gt;Photoshop is now coming to Android, so users of Google’s operating system can gussy up their images, too. The app has a similar set of editing tools as the desktop version, including layering and masking.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Let’s try that again:&amp;nbsp;&lt;/strong&gt;Tesla&amp;nbsp;filed new trademark applications&amp;nbsp;for “Tesla Robotaxi” after previous attempts to trademark the terms “Robotaxi” and “Cybercab” failed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Rolling in dough: &lt;/strong&gt;Tech startup Anduril just picked up a $1 billion investment as part of a new $2.5 billion raise led by Founders Fund, which means Anduril has doubled its valuation to $30.5 billion.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;On the road again: &lt;/strong&gt;When Toma’s founders realized car dealerships were drowning in missed calls, they hit the road to see the problem firsthand. That summer road trip turned into a $17 million a16z-backed fundraise that helped Toma get its&amp;nbsp;AI phone agents into more than 100 dealerships&amp;nbsp;across the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Fighting season: &lt;/strong&gt;All gloves were off on Thursday as Elon Musk and President Trump took to their respective social networks to throw jabs at each other. Though it might be exciting to watch rich men squabble in public, the fallout between the world’s richest person and a sitting U.S. president promises to have broader implications for the tech industry.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-analysis"&gt;Analysis&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="keyboard with chatbot icon hovering above it" class="wp-image-2631928" height="4000" src="https://techcrunch.com/wp-content/uploads/2023/11/GettyImages-1465545513.jpg?w=680" width="7000" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;BlackJack3D / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Money talks: &lt;/strong&gt;Whether you use AI as a friend, a therapist, or even a girlfriend, chatbots are trained to keep you talking. For Big Tech companies, it’s never been more competitive to attract users to their chatbot platforms — and keep them there.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Welcome back to Week in Review! Got lots for you today, including why Windsurf lost access to Claude, ChatGPT’s new features, WWDC 2025, Elon Musk’s fight with Donald Trump, and lots more. Have a great weekend!&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Duh: &lt;/strong&gt;During an interview at TC Sessions: AI 2025, Anthropic’s co-founder had a perfectly reasonable explanation for why the company cut access to Windsurf: “I think it would be odd for us to be selling Claude to OpenAI,” Chief Science Officer Jared Kaplan said, referring to rumors and reports that OpenAI, its largest competitor, is acquiring the AI coding assistant. Seems like a good reason to me!&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Everything is the same: &lt;/strong&gt;Chinese lab DeepSeek released an updated version of its R1 reasoning AI model last week that performs well on a number of math and coding benchmarks. Now some AI researchers are speculating that at least some of the source data it trained on came from Google’s Gemini family of AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;WWDC 2025: &lt;/strong&gt;Apple’s annual developers conference starts Monday. Beyond a newly designed operating system, here’s what we’re expecting to see at this year’s event, including a dedicated gaming app and updates to Mac, Watch, TV, and more.&amp;nbsp;&lt;/p&gt;

&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;

&lt;p class="has-text-align-center wp-block-paragraph"&gt;&lt;em&gt;This is TechCrunch’s Week in Review, where we recap the week’s biggest news. Want this delivered as a newsletter to your inbox every Saturday? Sign up here.&lt;/em&gt;&lt;/p&gt;

&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;

&lt;h2 class="wp-block-heading" id="h-news"&gt;News&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="The OpenAI logo is seen displayed on a smartphone screen." class="wp-image-2995059" height="1280" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2206295463.jpg?w=680" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Thomas Fuller / SOPA Images / LightRocket / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Business in the front: &lt;/strong&gt;ChatGPT is getting new features for business users, including connectors for Dropbox, Box, SharePoint, OneDrive, and Google Drive. This would let ChatGPT look for information across your own services to answer questions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Oh no: &lt;/strong&gt;Indian grocery delivery startup KiranaPro was hacked, and all of its data was wiped. According to the company, it has 55,000 customers, with 30,000 to 35,000 active buyers across 50 cities, who collectively place 2,000 orders daily.&amp;nbsp;&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Artsy people, rejoice! &lt;/strong&gt;Photoshop is now coming to Android, so users of Google’s operating system can gussy up their images, too. The app has a similar set of editing tools as the desktop version, including layering and masking.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Let’s try that again:&amp;nbsp;&lt;/strong&gt;Tesla&amp;nbsp;filed new trademark applications&amp;nbsp;for “Tesla Robotaxi” after previous attempts to trademark the terms “Robotaxi” and “Cybercab” failed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Rolling in dough: &lt;/strong&gt;Tech startup Anduril just picked up a $1 billion investment as part of a new $2.5 billion raise led by Founders Fund, which means Anduril has doubled its valuation to $30.5 billion.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;On the road again: &lt;/strong&gt;When Toma’s founders realized car dealerships were drowning in missed calls, they hit the road to see the problem firsthand. That summer road trip turned into a $17 million a16z-backed fundraise that helped Toma get its&amp;nbsp;AI phone agents into more than 100 dealerships&amp;nbsp;across the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Fighting season: &lt;/strong&gt;All gloves were off on Thursday as Elon Musk and President Trump took to their respective social networks to throw jabs at each other. Though it might be exciting to watch rich men squabble in public, the fallout between the world’s richest person and a sitting U.S. president promises to have broader implications for the tech industry.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-analysis"&gt;Analysis&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="keyboard with chatbot icon hovering above it" class="wp-image-2631928" height="4000" src="https://techcrunch.com/wp-content/uploads/2023/11/GettyImages-1465545513.jpg?w=680" width="7000" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;BlackJack3D / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Money talks: &lt;/strong&gt;Whether you use AI as a friend, a therapist, or even a girlfriend, chatbots are trained to keep you talking. For Big Tech companies, it’s never been more competitive to attract users to their chatbot platforms — and keep them there.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/07/week-in-review-why-anthropic-cut-access-to-windsurf/</guid><pubDate>Sat, 07 Jun 2025 17:11:00 +0000</pubDate></item><item><title>Trump administration takes aim at Biden and Obama cybersecurity rules (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/07/trump-administration-takes-aim-at-biden-and-obama-cybersecurity-rules/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2021/01/GettyImages-1230452613.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;President Donald Trump signed an executive order Friday that revises and rolls back cybersecurity policies set in place by his Democratic predecessors, Barack Obama and Joe Biden.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a White House fact sheet, the administration claims that Biden’s Executive Order 14144 — signed days before the end of his presidency — was an attempt “to sneak problematic and distracting issues into cybersecurity policy.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Among other things, Biden’s order encouraged agencies to “consider accepting digital identity documents” when public benefit programs require ID. Trump struck that part of the order, with the White House now saying this approach risks “widespread abuse by enabling illegal immigrants to improperly access public benefits.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, Mark Montgomery, senior director of the Foundation for Defense of Democracies’ Center on Cyber and Technology Innovation, told Politico that “the fixation on revoking digital ID mandates is prioritizing questionable immigration benefits over proven cybersecurity benefits.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On AI, Trump removed Biden’s requirements around testing the use of AI to defend energy infrastructure, funding federal research programs around AI security, and directing the Pentagon to “use AI models for cyber security.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The White House describes its moves on AI as refocusing AI cybersecurity strategy “towards identifying and managing vulnerabilities, rather than censorship.” (Trump’s Silicon Valley allies have complained repeatedly about the threat of AI “censorship.”)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Trump’s order also removed requirements that agencies start using quantum-resistant encryption “as soon as practicable.” And it removed requirements that federal contractors attest to the security of their software — the White House describes those requirements as “unproven and burdensome software accounting processes that prioritized compliance checklists over genuine security investments.”&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Going back even further, Trump’s executive order repeals Obama’s policies around sanctions for cybersecurity attacks on the United States; those sanctions can now only be applied to “foreign malicious actors.” The White House says this will will prevent “misuse against domestic political opponents” and clarify that “sanctions do not apply to election-related activities.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2021/01/GettyImages-1230452613.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;President Donald Trump signed an executive order Friday that revises and rolls back cybersecurity policies set in place by his Democratic predecessors, Barack Obama and Joe Biden.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a White House fact sheet, the administration claims that Biden’s Executive Order 14144 — signed days before the end of his presidency — was an attempt “to sneak problematic and distracting issues into cybersecurity policy.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Among other things, Biden’s order encouraged agencies to “consider accepting digital identity documents” when public benefit programs require ID. Trump struck that part of the order, with the White House now saying this approach risks “widespread abuse by enabling illegal immigrants to improperly access public benefits.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, Mark Montgomery, senior director of the Foundation for Defense of Democracies’ Center on Cyber and Technology Innovation, told Politico that “the fixation on revoking digital ID mandates is prioritizing questionable immigration benefits over proven cybersecurity benefits.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On AI, Trump removed Biden’s requirements around testing the use of AI to defend energy infrastructure, funding federal research programs around AI security, and directing the Pentagon to “use AI models for cyber security.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The White House describes its moves on AI as refocusing AI cybersecurity strategy “towards identifying and managing vulnerabilities, rather than censorship.” (Trump’s Silicon Valley allies have complained repeatedly about the threat of AI “censorship.”)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Trump’s order also removed requirements that agencies start using quantum-resistant encryption “as soon as practicable.” And it removed requirements that federal contractors attest to the security of their software — the White House describes those requirements as “unproven and burdensome software accounting processes that prioritized compliance checklists over genuine security investments.”&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Going back even further, Trump’s executive order repeals Obama’s policies around sanctions for cybersecurity attacks on the United States; those sanctions can now only be applied to “foreign malicious actors.” The White House says this will will prevent “misuse against domestic political opponents” and clarify that “sanctions do not apply to election-related activities.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/07/trump-administration-takes-aim-at-biden-and-obama-cybersecurity-rules/</guid><pubDate>Sat, 07 Jun 2025 20:32:42 +0000</pubDate></item><item><title>Lawyers could face ‘severe’ penalties for fake AI-generated citations, UK court warns (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/07/lawyers-could-face-severe-penalties-for-fake-ai-generated-citations-uk-court-warns/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/06/GettyImages-1495170962.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The High Court of England and Wales says lawyers need to take stronger steps to prevent the misuse of artificial intelligence in their work.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a ruling tying together two recent cases, Judge Victoria Sharp wrote that generative AI tools like ChatGPT “are not capable of conducting reliable legal research.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Such tools can produce apparently coherent and plausible responses to prompts, but those coherent and plausible responses may turn out to be entirely incorrect,” Judge Sharp wrote. “The responses may make confident assertions that are simply untrue.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That doesn’t mean lawyers cannot use AI in their research, but she said they have a professional duty “to check the accuracy of such research by reference to authoritative sources, before using it in the course of their professional work.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Judge Sharp suggested that with the growing number of cases where lawyers (including, on the U.S. side, lawyers representing major AI platforms) have cited what appear to be AI-generated falsehoods, “more needs to be done to ensure that the guidance is followed and lawyers comply with their duties to the court,” and she said her ruling will be forwarded to professional bodies including the Bar Council and the Law Society.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In one of the cases in question, a lawyer representing a man seeking damages against two banks submitted a filing with 45 citations — 18 of those cases did not exist, while many others “did not contain the quotations that were attributed to them, did not support the propositions for which they were cited, and did not have any relevance to the subject matter of the application,” Judge Sharp said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the other, a lawyer representing a man who had been evicted from his London home wrote a court filing citing five cases that did not appear to exist. (The lawyer denied using AI, though she said the citations may have come from AI-generated summaries that appeared in “Google or Safari.”) Judge Sharp said that while the court decided not to initiate contempt proceedings, that is “not a precedent.”&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;“Lawyers who do not comply with their professional obligations in this respect risk severe sanction,” she added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both lawyers were either referred or referred themselves to professional regulators. Judge Sharp noted that when lawyers do not meet their duties to the court, the court’s powers range from “public admonition” to the imposition of costs, contempt proceedings, or even “referral to the police.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/06/GettyImages-1495170962.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The High Court of England and Wales says lawyers need to take stronger steps to prevent the misuse of artificial intelligence in their work.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a ruling tying together two recent cases, Judge Victoria Sharp wrote that generative AI tools like ChatGPT “are not capable of conducting reliable legal research.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Such tools can produce apparently coherent and plausible responses to prompts, but those coherent and plausible responses may turn out to be entirely incorrect,” Judge Sharp wrote. “The responses may make confident assertions that are simply untrue.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That doesn’t mean lawyers cannot use AI in their research, but she said they have a professional duty “to check the accuracy of such research by reference to authoritative sources, before using it in the course of their professional work.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Judge Sharp suggested that with the growing number of cases where lawyers (including, on the U.S. side, lawyers representing major AI platforms) have cited what appear to be AI-generated falsehoods, “more needs to be done to ensure that the guidance is followed and lawyers comply with their duties to the court,” and she said her ruling will be forwarded to professional bodies including the Bar Council and the Law Society.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In one of the cases in question, a lawyer representing a man seeking damages against two banks submitted a filing with 45 citations — 18 of those cases did not exist, while many others “did not contain the quotations that were attributed to them, did not support the propositions for which they were cited, and did not have any relevance to the subject matter of the application,” Judge Sharp said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the other, a lawyer representing a man who had been evicted from his London home wrote a court filing citing five cases that did not appear to exist. (The lawyer denied using AI, though she said the citations may have come from AI-generated summaries that appeared in “Google or Safari.”) Judge Sharp said that while the court decided not to initiate contempt proceedings, that is “not a precedent.”&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;“Lawyers who do not comply with their professional obligations in this respect risk severe sanction,” she added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both lawyers were either referred or referred themselves to professional regulators. Judge Sharp noted that when lawyers do not meet their duties to the court, the court’s powers range from “public admonition” to the imposition of costs, contempt proceedings, or even “referral to the police.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/07/lawyers-could-face-severe-penalties-for-fake-ai-generated-citations-uk-court-warns/</guid><pubDate>Sat, 07 Jun 2025 21:42:01 +0000</pubDate></item><item><title>Agent-based computing is outgrowing the web as we know it (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/agent-based-computing-is-outgrowing-the-web-as-we-know-it/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;We are on the cusp of a fundamental redesign of the internet. Not a facelift. A full-body transplant.&lt;/p&gt;



&lt;p&gt;For more than 30 years, the web has been our playground, our workplace, our high street and our therapist’s couch. But it’s also been entirely designed for us simple humans who type, tap, click and scroll. Interfaces built for eyes. Navigation designed for fingers. Decision trees dressed up as websites.&lt;/p&gt;



&lt;p&gt;But here’s the truth: We’re not going to be the web’s primary users for much longer.&lt;/p&gt;



&lt;p&gt;AI agents based on ChatGPT, Copilot, Claude and Gemini are moving from passive assistants to active participants. Today, we ask them to do things &lt;em&gt;for&lt;/em&gt; us. Tomorrow, we’ll authorize them to act &lt;em&gt;as&lt;/em&gt; us.&lt;/p&gt;



&lt;p&gt;And right now, we’re asking Ferraris to drive on cobblestone.&lt;/p&gt;



&lt;p&gt;AI is already trying to operate inside a human-shaped world. Clicking buttons. Dragging cursors. Filling out forms. It’s like putting a robot in a glove and telling it to pretend it’s got fingers. It works, for now, but it’s wildly inefficient.&lt;/p&gt;



&lt;p&gt;Remember when cars first appeared on horse trails? Well, I don’t but I know the story. It worked, barely. Until someone realized speed requires tarmac. The same logic applies to the web. AI agents aren’t going to just be digital chauffeurs. They’re going to be drivers that navigate, decide and transact. Fast. Without us in the loop.&lt;/p&gt;



&lt;p&gt;We’re about to need a new web.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-ai-agents-require-machine-native-design"&gt;AI agents require machine-native design&lt;/h2&gt;



&lt;p&gt;What does the web look like when it’s built for machines?&lt;/p&gt;



&lt;p&gt;It’s fast. Invisible. Transactional. Pages become endpoints. Interfaces dissolve. There are no “click here” buttons. Just structured data, unstructured context, exposed capabilities and intent flowing between systems.&lt;/p&gt;



&lt;p&gt;APIs will become the new storefronts. AI doesn’t need to read a product page or scroll through a review carousel. It needs to ask one question: “Is this the best option based on my user’s preferences, budget and priorities?” And it needs that answer instantly.&lt;/p&gt;



&lt;p&gt;The entire architecture of the internet will bend toward AI-native interfaces. Faster protocols. Cleaner metadata. Verifiable sources. Trust becomes the currency because AI can’t rely on vibes. Agents will assess source reliability, cross-check facts and learn from user outcomes. Reputation, structure and verification signals will matter more than design.&lt;/p&gt;



&lt;p&gt;And suddenly, “user experience” takes on a different meaning. You’re not designing for a distracted shopper. You’re designing for a synthetic brain with infinite tabs open and zero tolerance for friction.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-two-webs-one-future"&gt;Two webs, one future&lt;/h2&gt;



&lt;p&gt;So what happens next?&lt;/p&gt;



&lt;p&gt;We may end up with two parallel webs. One for humans that remains visual, persuasive, slow. One for machines that is minimal, efficient, fast.&lt;/p&gt;



&lt;p&gt;But more likely, the future is layered. Every digital surface will need a machine-readable skin. Your website, your content, your commerce, if it’s not optimized for autonomous agents, it’s invisible.&lt;/p&gt;



&lt;p&gt;This changes everything:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;SEO becomes MEO: machine experience optimization.&lt;/li&gt;



&lt;li&gt;Content becomes data.&lt;/li&gt;



&lt;li&gt;Brand trust becomes even more quantifiable and transparent.&lt;/li&gt;



&lt;li&gt;Influence shifts from design to accessibility, from layout to latency.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Efficiency and trustable become key website differentiators. The brands that embrace this shift early to build AI-ready front doors, not just pretty landing pages will thrive . . They will treat their AI compatibility the way they once treated mobile optimization or security.&lt;/p&gt;



&lt;p&gt;Because in five years, it won’t be a human clicking “buy now.” It’ll be your AI agent, acting on your behalf, making hundreds of decisions a day — not just purchases, but scheduling meetings, booking travel, screening content and negotiating services across every domain of digital life.&lt;/p&gt;



&lt;p&gt;And it won’t choose the prettiest site, it’ll choose the fastest, most reliable and trustworthy, most machine-readable one.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-bottom-line"&gt;Bottom line&lt;/h2&gt;



&lt;p&gt;We’re not just upgrading browsers. We’re rewriting the rules of the web.&lt;/p&gt;



&lt;p&gt;The old internet was built for people. The new one will be built for agents. And the companies that recognize this and build infrastructure, content and interfaces accordingly, will most likely own the future.&lt;/p&gt;



&lt;p&gt;Just like roads evolved for cars, the web will evolve for AI.&lt;/p&gt;



&lt;p&gt;And the next digital revolution? It’ll be executed in milliseconds by machines, for machines, on a web designed for (and quite possibly by) them.&lt;/p&gt;



&lt;p&gt;&lt;em&gt;Justin Westcott leads the global technology sector for Edelman.&lt;/em&gt;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;We are on the cusp of a fundamental redesign of the internet. Not a facelift. A full-body transplant.&lt;/p&gt;



&lt;p&gt;For more than 30 years, the web has been our playground, our workplace, our high street and our therapist’s couch. But it’s also been entirely designed for us simple humans who type, tap, click and scroll. Interfaces built for eyes. Navigation designed for fingers. Decision trees dressed up as websites.&lt;/p&gt;



&lt;p&gt;But here’s the truth: We’re not going to be the web’s primary users for much longer.&lt;/p&gt;



&lt;p&gt;AI agents based on ChatGPT, Copilot, Claude and Gemini are moving from passive assistants to active participants. Today, we ask them to do things &lt;em&gt;for&lt;/em&gt; us. Tomorrow, we’ll authorize them to act &lt;em&gt;as&lt;/em&gt; us.&lt;/p&gt;



&lt;p&gt;And right now, we’re asking Ferraris to drive on cobblestone.&lt;/p&gt;



&lt;p&gt;AI is already trying to operate inside a human-shaped world. Clicking buttons. Dragging cursors. Filling out forms. It’s like putting a robot in a glove and telling it to pretend it’s got fingers. It works, for now, but it’s wildly inefficient.&lt;/p&gt;



&lt;p&gt;Remember when cars first appeared on horse trails? Well, I don’t but I know the story. It worked, barely. Until someone realized speed requires tarmac. The same logic applies to the web. AI agents aren’t going to just be digital chauffeurs. They’re going to be drivers that navigate, decide and transact. Fast. Without us in the loop.&lt;/p&gt;



&lt;p&gt;We’re about to need a new web.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-ai-agents-require-machine-native-design"&gt;AI agents require machine-native design&lt;/h2&gt;



&lt;p&gt;What does the web look like when it’s built for machines?&lt;/p&gt;



&lt;p&gt;It’s fast. Invisible. Transactional. Pages become endpoints. Interfaces dissolve. There are no “click here” buttons. Just structured data, unstructured context, exposed capabilities and intent flowing between systems.&lt;/p&gt;



&lt;p&gt;APIs will become the new storefronts. AI doesn’t need to read a product page or scroll through a review carousel. It needs to ask one question: “Is this the best option based on my user’s preferences, budget and priorities?” And it needs that answer instantly.&lt;/p&gt;



&lt;p&gt;The entire architecture of the internet will bend toward AI-native interfaces. Faster protocols. Cleaner metadata. Verifiable sources. Trust becomes the currency because AI can’t rely on vibes. Agents will assess source reliability, cross-check facts and learn from user outcomes. Reputation, structure and verification signals will matter more than design.&lt;/p&gt;



&lt;p&gt;And suddenly, “user experience” takes on a different meaning. You’re not designing for a distracted shopper. You’re designing for a synthetic brain with infinite tabs open and zero tolerance for friction.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-two-webs-one-future"&gt;Two webs, one future&lt;/h2&gt;



&lt;p&gt;So what happens next?&lt;/p&gt;



&lt;p&gt;We may end up with two parallel webs. One for humans that remains visual, persuasive, slow. One for machines that is minimal, efficient, fast.&lt;/p&gt;



&lt;p&gt;But more likely, the future is layered. Every digital surface will need a machine-readable skin. Your website, your content, your commerce, if it’s not optimized for autonomous agents, it’s invisible.&lt;/p&gt;



&lt;p&gt;This changes everything:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;SEO becomes MEO: machine experience optimization.&lt;/li&gt;



&lt;li&gt;Content becomes data.&lt;/li&gt;



&lt;li&gt;Brand trust becomes even more quantifiable and transparent.&lt;/li&gt;



&lt;li&gt;Influence shifts from design to accessibility, from layout to latency.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Efficiency and trustable become key website differentiators. The brands that embrace this shift early to build AI-ready front doors, not just pretty landing pages will thrive . . They will treat their AI compatibility the way they once treated mobile optimization or security.&lt;/p&gt;



&lt;p&gt;Because in five years, it won’t be a human clicking “buy now.” It’ll be your AI agent, acting on your behalf, making hundreds of decisions a day — not just purchases, but scheduling meetings, booking travel, screening content and negotiating services across every domain of digital life.&lt;/p&gt;



&lt;p&gt;And it won’t choose the prettiest site, it’ll choose the fastest, most reliable and trustworthy, most machine-readable one.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-bottom-line"&gt;Bottom line&lt;/h2&gt;



&lt;p&gt;We’re not just upgrading browsers. We’re rewriting the rules of the web.&lt;/p&gt;



&lt;p&gt;The old internet was built for people. The new one will be built for agents. And the companies that recognize this and build infrastructure, content and interfaces accordingly, will most likely own the future.&lt;/p&gt;



&lt;p&gt;Just like roads evolved for cars, the web will evolve for AI.&lt;/p&gt;



&lt;p&gt;And the next digital revolution? It’ll be executed in milliseconds by machines, for machines, on a web designed for (and quite possibly by) them.&lt;/p&gt;



&lt;p&gt;&lt;em&gt;Justin Westcott leads the global technology sector for Edelman.&lt;/em&gt;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/agent-based-computing-is-outgrowing-the-web-as-we-know-it/</guid><pubDate>Sat, 07 Jun 2025 22:15:00 +0000</pubDate></item><item><title>AI Liability Insurance: The Next Step in Safeguarding Businesses from AI Failures (Unite.AI)</title><link>https://www.unite.ai/ai-liability-insurance-the-next-step-in-safeguarding-businesses-from-ai-failures/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.unite.ai/wp-content/uploads/2025/06/ChatGPT-Image-May-23-2025-09_50_13-AM-942x600.png" /&gt;&lt;/div&gt;&lt;p&gt;Businesses today depend heavily on Artificial Intelligence (AI) to run important tasks like handling customer questions, spotting financial risks, managing supply chains, and supporting medical decisions. While AI helps improve speed and accuracy, it also brings risks that old insurance policies do not cover. AI can make wrong choices, give false information, or fail because of software problems or biased data.&lt;/p&gt;&lt;p&gt;These issues can lead to costly lawsuits, fines from regulators, and damage to a company’s reputation. To deal with these new challenges, AI liability insurance has appeared as a necessary protection. This insurance helps companies manage the financial and legal problems that come from AI failures.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Understanding the Rise of AI Risks in Business&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;The use of AI in business has grown a lot in recent years. By late 2024, studies showed that over 70% of companies in fields like finance, healthcare, manufacturing, and retail were already using AI tools. For example, McKinsey &amp;amp; Company reported that around 78% of organizations had adopted AI in at least one business function by the end of 2024. Boston Consulting Group also found that 74% of companies struggled to scale value from AI, indicating challenges despite widespread adoption.&lt;/p&gt;&lt;p&gt;AI brings new risks different from older technologies. One major risk is AI hallucination when AI gives false or misleading answers. For instance, a language model may say something that sounds correct but is actually wrong. This can lead to bad decisions based on wrong information. Another risk is model drift. Over time, AI models can become less accurate because data changes. If a fraud detection AI drifts, it might miss new fraud patterns and cause losses or damage to reputation.&lt;/p&gt;&lt;p&gt;There are other risks too. Attackers might corrupt AI training data, a problem called data poisoning, which can cause AI to behave wrongly. Privacy, bias, and ethical issues are growing concerns. New laws, like the European Union’s AI Act expected soon, aim to control AI use and set strict rules).&lt;/p&gt;&lt;p&gt;Real-world cases show the serious risks AI systems bring. In September 2023, the Consumer Financial Protection Bureau (CFPB) gave guidance saying lenders using AI must explain clearly why they deny credit, not just use general reasons. This shows the need for fairness and openness in AI decisions.&lt;/p&gt;&lt;p&gt;At the same time, AI mistakes in medical diagnosis have raised concerns. A 2025 report by ECRI, a healthcare safety group, warns that poor AI oversight can cause wrong diagnoses and wrong treatments, harming patients. The report calls for better rules to make sure AI in healthcare works safely.&lt;/p&gt;&lt;p&gt;These examples show that AI failures can cause legal, financial, and reputation problems. Normal insurance often does not cover these AI-related risks because it was not made for AI’s special challenges. Experts say AI risks are growing fast and need new ways to manage them. To reduce these risks, more businesses are getting AI liability insurance. This type of insurance helps protect companies from costs and legal problems caused by AI errors, biases, or failures. Using AI liability insurance helps companies handle AI risks better and stay safe.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;What Is AI Liability Insurance and What Does It Cover?&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;AI liability insurance is a special type of coverage made to fill gaps left by traditional insurance like Errors &amp;amp; Omissions (E&amp;amp;O) and Commercial General Liability (CGL). Regular policies often treat AI problems as normal tech errors or cyber risks, but AI liability insurance focuses on risks from how AI systems are designed, used, and managed.&lt;/p&gt;&lt;p&gt;This insurance usually covers:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;AI system failures that cause financial loss or harm.&lt;/li&gt;&lt;li&gt;False or misleading AI outputs, sometimes called AI hallucinations.&lt;/li&gt;&lt;li&gt;Unauthorized use of data or intellectual property in AI models.&lt;/li&gt;&lt;li&gt;Fines and penalties for breaking new AI laws, such as the European Union’s AI Act, which can fine up to 6% of global revenue.&lt;/li&gt;&lt;li&gt;Data breaches or security issues linked to AI integration.&lt;/li&gt;&lt;li&gt;Legal costs from lawsuits or investigations related to AI failures.&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;strong&gt;Why Is AI Liability Insurance Needed and Who Provides It?&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;As more businesses use AI, the risks grow bigger. AI systems can act unpredictably and face new rules from governments. Therefore, managing AI risks needs new ideas because AI is different from past technologies and regulations keep changing.&lt;/p&gt;&lt;p&gt;Governments are creating stricter laws for AI safety and fairness. The EU’s AI Act is one example, setting clear rules and heavy penalties for companies that don’t follow. Similar laws are coming in the US, Canada, and elsewhere.&lt;/p&gt;&lt;p&gt;Insurance companies have started offering special AI liability products to meet these needs. For example:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Coalition Insurance covers risks from generative AI, like deepfake fraud and security problems.&lt;/li&gt;&lt;li&gt;Relm Insurance offers solutions like PONTAAI, covering bias, IP violations, and regulatory issues.&lt;/li&gt;&lt;li&gt;Munich Re’s aiSure™ protects businesses against AI model failures and performance drops.&lt;/li&gt;&lt;li&gt;Similarly, AXA XL and Chaucer Group have endorsements for third-party AI risks and generative AI exposures.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;With AI becoming part of daily business, AI liability insurance helps companies reduce financial risks, meet new laws, and use AI responsibly.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Key Features and Benefits of AI Liability Insurance&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;AI liability insurance offers several important benefits that help businesses manage the unique risks posed by AI.&lt;/p&gt;&lt;p&gt;One of the main advantages is financial protection, covering costs related to AI failures. This includes paying for third-party claims such as lawsuits involving bias, discrimination, or misinformation, as well as covering the insured company’s own damages like business interruptions caused by AI system failures and managing reputational harm.&lt;/p&gt;&lt;p&gt;Additionally, AI liability insurance often provides legal defense coverage, offering support to defend against claims or regulatory investigations which is an essential feature given the complexity of legal issues related to AI. Unlike generic cyber or liability insurance, these policies are specifically designed to cover AI-related risks such as hallucinations, model drift, and software bugs.&lt;/p&gt;&lt;p&gt;Companies can customize their policies to fit their particular AI use and risk profiles. For example, a healthcare AI developer may need coverage focused on patient safety, while a financial firm might prioritize fraud detection risks. Many AI liability insurance policies also offer broad territorial limits, which is important for multinational businesses deploying AI in multiple countries.&lt;/p&gt;&lt;p&gt;Furthermore, insurers may require policyholders to follow best practices like maintaining transparency, conducting regular audits, and implementing risk management plans. This not only promotes safer AI deployment but also helps build trust with regulators and customers. Together, these features provide businesses with a reliable way to handle AI risks confidently, protecting their operations, finances, and reputation.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Who Should Consider AI Liability Insurance? Use Cases and Industry Examples&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;AI liability insurance is important for businesses using AI technology. The risks from AI can differ based on the industry and how AI is applied. Companies should review their exposure to AI failures, legal issues, and financial risks to decide if they need this insurance. Some industries face higher AI risks:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Healthcare&lt;/strong&gt;: AI helps with diagnosis and treatment, but errors can harm patients and cause liability problems.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Finance&lt;/strong&gt;: AI is used for credit decisions and fraud detection. Mistakes may lead to unfair decisions, losses, or regulatory issues.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Autonomous Vehicles:&lt;/strong&gt; Self-driving cars rely on AI, so accidents caused by AI errors need insurance protection.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Marketing and Content:&lt;/strong&gt; Generative AI creates content that might infringe copyrights or spread wrong information, risking legal trouble.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Cybersecurity:&lt;/strong&gt; AI systems detect threats but may fail due to attacks or errors, causing data breaches and liability.&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;strong&gt;Who Needs AI Liability Insurance?&lt;/strong&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;AI Developers and Tech Firms:&lt;/strong&gt; They face risks like bias, incorrect outputs, and intellectual property disputes during AI creation.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Businesses Using AI Tools:&lt;/strong&gt; Companies that use AI made by others need protection if those tools fail or cause security problems.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Risk Managers and Leaders:&lt;/strong&gt; They should assess AI risks in their organizations and ensure proper insurance coverage.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;As AI becomes more common, AI liability insurance is a vital protection for businesses managing AI risks. If you want, I can help you learn about specific insurance policies from top providers.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Real-World Examples and Lessons Learned&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Real examples show how AI failures can cause big problems for businesses. Even though AI liability insurance is still new, some cases prove why it is needed.&lt;/p&gt;&lt;p&gt;In 2023, a lawyer in New York got in trouble for submitting a legal brief with made-up case citations created by ChatGPT. The court said the lawyer did not check the AI’s accuracy, leading to legal penalties.&lt;/p&gt;&lt;p&gt;In 2024, Air Canada’s AI chatbot wrongly promised a discount for bereavement but the airline did not honor it. This caused a legal dispute, and the court ordered Air Canada to pay the customer. This shows how wrong AI information can cause legal and financial risks.&lt;/p&gt;&lt;p&gt;Deepfake scams are a growing threat to businesses. For example, a UK energy company lost $243,000 after criminals used AI-generated voice deepfakes to impersonate an executive and trick the company. This type of AI-driven fraud exposes businesses to serious financial and security risks. AI liability insurance can help cover losses from such scams and protect companies against emerging AI-related threats.&lt;/p&gt;&lt;p&gt;From the above incidents, the lessons are clear: AI failures can cause lawsuits, fines, and damage to reputation. Normal insurance often does not cover AI risks well, so businesses need AI liability insurance. Companies using AI should review their insurance often and update it to meet new rules and risks.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;The Bottom Line&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;AI is becoming a vital part of many businesses, but it also brings new risks that old insurance does not cover well. Failures like wrong decisions, misleading information, and security threats can cause serious financial, legal, and reputational harm. Real cases show these risks are real and growing.&lt;/p&gt;&lt;p&gt;AI liability insurance offers protection specifically for these challenges. It helps businesses cover costs from AI mistakes, legal claims, and fraud, while supporting compliance with new laws.&lt;/p&gt;&lt;p&gt;Businesses in domains&amp;nbsp; like healthcare, finance, and cybersecurity especially need this coverage. As AI use grows, regularly reviewing and updating insurance is important to stay protected. AI liability insurance is no longer optional; it is a necessary step to manage risks and keep businesses safe in a world where AI plays a bigger role every day.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.unite.ai/wp-content/uploads/2025/06/ChatGPT-Image-May-23-2025-09_50_13-AM-942x600.png" /&gt;&lt;/div&gt;&lt;p&gt;Businesses today depend heavily on Artificial Intelligence (AI) to run important tasks like handling customer questions, spotting financial risks, managing supply chains, and supporting medical decisions. While AI helps improve speed and accuracy, it also brings risks that old insurance policies do not cover. AI can make wrong choices, give false information, or fail because of software problems or biased data.&lt;/p&gt;&lt;p&gt;These issues can lead to costly lawsuits, fines from regulators, and damage to a company’s reputation. To deal with these new challenges, AI liability insurance has appeared as a necessary protection. This insurance helps companies manage the financial and legal problems that come from AI failures.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Understanding the Rise of AI Risks in Business&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;The use of AI in business has grown a lot in recent years. By late 2024, studies showed that over 70% of companies in fields like finance, healthcare, manufacturing, and retail were already using AI tools. For example, McKinsey &amp;amp; Company reported that around 78% of organizations had adopted AI in at least one business function by the end of 2024. Boston Consulting Group also found that 74% of companies struggled to scale value from AI, indicating challenges despite widespread adoption.&lt;/p&gt;&lt;p&gt;AI brings new risks different from older technologies. One major risk is AI hallucination when AI gives false or misleading answers. For instance, a language model may say something that sounds correct but is actually wrong. This can lead to bad decisions based on wrong information. Another risk is model drift. Over time, AI models can become less accurate because data changes. If a fraud detection AI drifts, it might miss new fraud patterns and cause losses or damage to reputation.&lt;/p&gt;&lt;p&gt;There are other risks too. Attackers might corrupt AI training data, a problem called data poisoning, which can cause AI to behave wrongly. Privacy, bias, and ethical issues are growing concerns. New laws, like the European Union’s AI Act expected soon, aim to control AI use and set strict rules).&lt;/p&gt;&lt;p&gt;Real-world cases show the serious risks AI systems bring. In September 2023, the Consumer Financial Protection Bureau (CFPB) gave guidance saying lenders using AI must explain clearly why they deny credit, not just use general reasons. This shows the need for fairness and openness in AI decisions.&lt;/p&gt;&lt;p&gt;At the same time, AI mistakes in medical diagnosis have raised concerns. A 2025 report by ECRI, a healthcare safety group, warns that poor AI oversight can cause wrong diagnoses and wrong treatments, harming patients. The report calls for better rules to make sure AI in healthcare works safely.&lt;/p&gt;&lt;p&gt;These examples show that AI failures can cause legal, financial, and reputation problems. Normal insurance often does not cover these AI-related risks because it was not made for AI’s special challenges. Experts say AI risks are growing fast and need new ways to manage them. To reduce these risks, more businesses are getting AI liability insurance. This type of insurance helps protect companies from costs and legal problems caused by AI errors, biases, or failures. Using AI liability insurance helps companies handle AI risks better and stay safe.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;What Is AI Liability Insurance and What Does It Cover?&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;AI liability insurance is a special type of coverage made to fill gaps left by traditional insurance like Errors &amp;amp; Omissions (E&amp;amp;O) and Commercial General Liability (CGL). Regular policies often treat AI problems as normal tech errors or cyber risks, but AI liability insurance focuses on risks from how AI systems are designed, used, and managed.&lt;/p&gt;&lt;p&gt;This insurance usually covers:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;AI system failures that cause financial loss or harm.&lt;/li&gt;&lt;li&gt;False or misleading AI outputs, sometimes called AI hallucinations.&lt;/li&gt;&lt;li&gt;Unauthorized use of data or intellectual property in AI models.&lt;/li&gt;&lt;li&gt;Fines and penalties for breaking new AI laws, such as the European Union’s AI Act, which can fine up to 6% of global revenue.&lt;/li&gt;&lt;li&gt;Data breaches or security issues linked to AI integration.&lt;/li&gt;&lt;li&gt;Legal costs from lawsuits or investigations related to AI failures.&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;strong&gt;Why Is AI Liability Insurance Needed and Who Provides It?&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;As more businesses use AI, the risks grow bigger. AI systems can act unpredictably and face new rules from governments. Therefore, managing AI risks needs new ideas because AI is different from past technologies and regulations keep changing.&lt;/p&gt;&lt;p&gt;Governments are creating stricter laws for AI safety and fairness. The EU’s AI Act is one example, setting clear rules and heavy penalties for companies that don’t follow. Similar laws are coming in the US, Canada, and elsewhere.&lt;/p&gt;&lt;p&gt;Insurance companies have started offering special AI liability products to meet these needs. For example:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Coalition Insurance covers risks from generative AI, like deepfake fraud and security problems.&lt;/li&gt;&lt;li&gt;Relm Insurance offers solutions like PONTAAI, covering bias, IP violations, and regulatory issues.&lt;/li&gt;&lt;li&gt;Munich Re’s aiSure™ protects businesses against AI model failures and performance drops.&lt;/li&gt;&lt;li&gt;Similarly, AXA XL and Chaucer Group have endorsements for third-party AI risks and generative AI exposures.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;With AI becoming part of daily business, AI liability insurance helps companies reduce financial risks, meet new laws, and use AI responsibly.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Key Features and Benefits of AI Liability Insurance&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;AI liability insurance offers several important benefits that help businesses manage the unique risks posed by AI.&lt;/p&gt;&lt;p&gt;One of the main advantages is financial protection, covering costs related to AI failures. This includes paying for third-party claims such as lawsuits involving bias, discrimination, or misinformation, as well as covering the insured company’s own damages like business interruptions caused by AI system failures and managing reputational harm.&lt;/p&gt;&lt;p&gt;Additionally, AI liability insurance often provides legal defense coverage, offering support to defend against claims or regulatory investigations which is an essential feature given the complexity of legal issues related to AI. Unlike generic cyber or liability insurance, these policies are specifically designed to cover AI-related risks such as hallucinations, model drift, and software bugs.&lt;/p&gt;&lt;p&gt;Companies can customize their policies to fit their particular AI use and risk profiles. For example, a healthcare AI developer may need coverage focused on patient safety, while a financial firm might prioritize fraud detection risks. Many AI liability insurance policies also offer broad territorial limits, which is important for multinational businesses deploying AI in multiple countries.&lt;/p&gt;&lt;p&gt;Furthermore, insurers may require policyholders to follow best practices like maintaining transparency, conducting regular audits, and implementing risk management plans. This not only promotes safer AI deployment but also helps build trust with regulators and customers. Together, these features provide businesses with a reliable way to handle AI risks confidently, protecting their operations, finances, and reputation.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Who Should Consider AI Liability Insurance? Use Cases and Industry Examples&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;AI liability insurance is important for businesses using AI technology. The risks from AI can differ based on the industry and how AI is applied. Companies should review their exposure to AI failures, legal issues, and financial risks to decide if they need this insurance. Some industries face higher AI risks:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Healthcare&lt;/strong&gt;: AI helps with diagnosis and treatment, but errors can harm patients and cause liability problems.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Finance&lt;/strong&gt;: AI is used for credit decisions and fraud detection. Mistakes may lead to unfair decisions, losses, or regulatory issues.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Autonomous Vehicles:&lt;/strong&gt; Self-driving cars rely on AI, so accidents caused by AI errors need insurance protection.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Marketing and Content:&lt;/strong&gt; Generative AI creates content that might infringe copyrights or spread wrong information, risking legal trouble.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Cybersecurity:&lt;/strong&gt; AI systems detect threats but may fail due to attacks or errors, causing data breaches and liability.&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;strong&gt;Who Needs AI Liability Insurance?&lt;/strong&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;AI Developers and Tech Firms:&lt;/strong&gt; They face risks like bias, incorrect outputs, and intellectual property disputes during AI creation.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Businesses Using AI Tools:&lt;/strong&gt; Companies that use AI made by others need protection if those tools fail or cause security problems.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Risk Managers and Leaders:&lt;/strong&gt; They should assess AI risks in their organizations and ensure proper insurance coverage.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;As AI becomes more common, AI liability insurance is a vital protection for businesses managing AI risks. If you want, I can help you learn about specific insurance policies from top providers.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Real-World Examples and Lessons Learned&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Real examples show how AI failures can cause big problems for businesses. Even though AI liability insurance is still new, some cases prove why it is needed.&lt;/p&gt;&lt;p&gt;In 2023, a lawyer in New York got in trouble for submitting a legal brief with made-up case citations created by ChatGPT. The court said the lawyer did not check the AI’s accuracy, leading to legal penalties.&lt;/p&gt;&lt;p&gt;In 2024, Air Canada’s AI chatbot wrongly promised a discount for bereavement but the airline did not honor it. This caused a legal dispute, and the court ordered Air Canada to pay the customer. This shows how wrong AI information can cause legal and financial risks.&lt;/p&gt;&lt;p&gt;Deepfake scams are a growing threat to businesses. For example, a UK energy company lost $243,000 after criminals used AI-generated voice deepfakes to impersonate an executive and trick the company. This type of AI-driven fraud exposes businesses to serious financial and security risks. AI liability insurance can help cover losses from such scams and protect companies against emerging AI-related threats.&lt;/p&gt;&lt;p&gt;From the above incidents, the lessons are clear: AI failures can cause lawsuits, fines, and damage to reputation. Normal insurance often does not cover AI risks well, so businesses need AI liability insurance. Companies using AI should review their insurance often and update it to meet new rules and risks.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;The Bottom Line&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;AI is becoming a vital part of many businesses, but it also brings new risks that old insurance does not cover well. Failures like wrong decisions, misleading information, and security threats can cause serious financial, legal, and reputational harm. Real cases show these risks are real and growing.&lt;/p&gt;&lt;p&gt;AI liability insurance offers protection specifically for these challenges. It helps businesses cover costs from AI mistakes, legal claims, and fraud, while supporting compliance with new laws.&lt;/p&gt;&lt;p&gt;Businesses in domains&amp;nbsp; like healthcare, finance, and cybersecurity especially need this coverage. As AI use grows, regularly reviewing and updating insurance is important to stay protected. AI liability insurance is no longer optional; it is a necessary step to manage risks and keep businesses safe in a world where AI plays a bigger role every day.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.unite.ai/ai-liability-insurance-the-next-step-in-safeguarding-businesses-from-ai-failures/</guid><pubDate>Sun, 08 Jun 2025 05:41:15 +0000</pubDate></item><item><title>Meta reportedly in talks to invest billions of dollars in Scale AI (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/08/meta-reportedly-in-talks-to-invest-billions-of-dollars-in-scale-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/06/GettyImages-1540566330.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta is discussing a multi-billion dollar investment in Scale AI, according to Bloomberg. In fact, the deal value could reportedly exceed $10 billion, making it the largest external AI investment by Facebook’s parent company and one of the largest funding events ever for a private company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Scale AI (whose CEO Alexandr Wang is pictured above) provides data labeling services to companies such as Microsoft and OpenAI to help them train their AI models. Much of that labeling work is done by contractors — in fact, the Department of Labor recently dropped its investigation into whether the company was misclassifying and underpaying employees.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;According to Bloomberg, the company saw $870 million in revenue last year and expects to bring in $2 billion this year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta was already an investor in Scale AI’s $1 billion Series F, which valued the company at $13.8 billion. Scale AI also built Defense Llama, a large language model designed for military use, on top of Meta’s Llama 3.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/06/GettyImages-1540566330.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta is discussing a multi-billion dollar investment in Scale AI, according to Bloomberg. In fact, the deal value could reportedly exceed $10 billion, making it the largest external AI investment by Facebook’s parent company and one of the largest funding events ever for a private company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Scale AI (whose CEO Alexandr Wang is pictured above) provides data labeling services to companies such as Microsoft and OpenAI to help them train their AI models. Much of that labeling work is done by contractors — in fact, the Department of Labor recently dropped its investigation into whether the company was misclassifying and underpaying employees.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;According to Bloomberg, the company saw $870 million in revenue last year and expects to bring in $2 billion this year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta was already an investor in Scale AI’s $1 billion Series F, which valued the company at $13.8 billion. Scale AI also built Defense Llama, a large language model designed for military use, on top of Meta’s Llama 3.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/08/meta-reportedly-in-talks-to-invest-billions-of-dollars-in-scale-ai/</guid><pubDate>Sun, 08 Jun 2025 19:59:08 +0000</pubDate></item><item><title>Like humans, AI is forcing institutions to rethink their purpose (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/like-humans-ai-is-forcing-institutions-to-rethink-their-purpose/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Cognitive migration is not just an individual journey; it is also a collective and institutional one. As AI reshapes the terrain of thought, judgment and coordination, the very foundations of our schools, governments, corporations and civic systems are being called into question.&lt;/p&gt;



&lt;p&gt;Institutions, like people, now face the challenge of rapid change: “Rethinking” their purpose, adapting their structures and rediscovering what makes them essential in a world where machines can increasingly think, decide and produce. Like people who are undergoing cognitive migration, institutions — and the people who run them — must reassess what they were made for.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-discontinuity"&gt;Discontinuity&lt;/h2&gt;



&lt;p&gt;Institutions are designed to promote continuity. Their purpose is to endure, to offer structure, legitimacy and coherence across time. It is those very attributes that contribute to trust. We rely on institutions not only to deliver services and enforce norms, but to provide a sense of order in a complex world. They are the long-arc vessels of civilization, meant to hold steady as individuals come and go. Without viable institutions, society risks upheaval and an increasingly uncertain future.&lt;/p&gt;



&lt;p&gt;But today, many of our core institutions are reeling. Having long served as the scaffolding of modern life, they are being tested in ways that feel not only sudden, but systemic.&lt;/p&gt;



&lt;p&gt;Some of this pressure comes from AI, which is rapidly reshaping the cognitive terrain on which these institutions were built. But AI is not the only force. The past two decades have brought rising public distrust, partisan fragmentation and challenges to institutional legitimacy that predate the generative AI technological wave. From increasing income inequality, to attacks on scientific process and consensus, to politicized courts, to declining university enrollments, the erosion of trust in our institutions has multiple causes, as well as compounding effects.&lt;/p&gt;



&lt;p&gt;In this context, the arrival of increasingly capable AI systems is not merely another challenge. It is an accelerant, fuel to the fire of institutional disruption. This disruption demands that institutions adapt their operations and revisit foundational assumptions. What are institutions for in a world where credentialing, reasoning and coordination are no longer exclusively human domains? All this institutional reinvention needs to take place at a pace that defies their very purpose and nature.&lt;/p&gt;



&lt;p&gt;This is the institutional dimension of cognitive migration: A shift not just in individuals find meaning and value, but in how our collective societal structures must evolve to support a new era. And as with all migrations, the journey will be uneven, contested and deeply consequential.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-architecture-of-the-old-regime"&gt;The architecture of the old regime&lt;/h2&gt;



&lt;p&gt;The institutions in place now were not designed for this moment. Most were forged in the Industrial Age and refined during the Digital Revolution. Their operating models reflect the logic of earlier cognitive regimes: stable processes, centralized expertise and the tacit assumption that human intelligence would remain preeminent.&lt;/p&gt;



&lt;p&gt;Schools, corporations, courts and government agencies are structured to manage people and information on a large scale. They rely on predictability, expert credentials and well-defined hierarchies of decision-making. These are traditional strengths that — even when considered bureaucratic — have historically offered a foundation for trust, consistency and broad participation within complex societies.&lt;/p&gt;



&lt;p&gt;But the assumptions beneath these structures are under strain. AI systems now perform tasks once reserved for knowledge workers, including summarizing documents, analyzing data, writing legal briefs, performing research, creating lesson plans and teaching, coding applications and building and executing marketing campaigns. Beyond automation, a deeper disruption is underway: The people running these institutions are expected to defend their continued relevance in a world where knowledge itself is no longer as highly valued or even a uniquely human asset.&lt;/p&gt;



&lt;p&gt;The relevance of some institutions is called into question from outside challengers including tech platforms, alternative credentialing models and decentralized networks. This essentially means that the traditional gatekeepers of trust, expertise and coordination are being challenged by faster, flatter and often more digitally native alternatives. In some cases, even long-standing institutional functions such as adjudicating disputes are being questioned, ignored, or bypassed altogether.&lt;/p&gt;



&lt;p&gt;This does not mean institutional collapse is inevitable. But it does suggest that the current paradigm of stable, slow-moving and authority-based structures may not endure. At a minimum, institutions are under intense pressure to change. If institutions are to remain relevant and play a vital role in the age of AI, they must become more adaptive, transparent and attuned to the values that cannot readily be encoded in algorithms: human dignity, ethical deliberation and long-term stewardship.&lt;/p&gt;



&lt;p&gt;The choice ahead is not whether institutions will change, but how. Will they resist, ossify and fall into irrelevance? Will they be forcibly restructured to meet transient agendas? Or will they deliberately reimagine themselves as co-evolving partners in a world of shared intelligence and shifting value?&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-first-steps-of-institutional-migration"&gt;First steps of institutional migration&lt;/h2&gt;



&lt;p&gt;A growing number of institutions are beginning to adapt. These responses are varied and often tentative, signs of motion more than full transformation. These are green shoots; taken together, they suggest that the cognitive migration of institutions may already be underway.&lt;/p&gt;



&lt;p&gt;Yet there is a deeper challenge beneath these experiments: Many institutions are still bound by outdated methods of operating. The environment, however, has changed. AI and other factors are redrawing the landscape, and institutions are only beginning to recalibrate.&lt;/p&gt;



&lt;p&gt;One example of change comes from an Arizona-based charter school where AI plays a leading role in daily instruction. Branded as Unbound Academy, the school uses AI platforms to deliver core academic content in condensed, focused sessions tailored for each child. This shows promise to improve academic achievement while also allowing students time later in the day to work on life skills, project-based learning and interpersonal development. In this model, teachers are reframed as guides and mentors, not content deliverers. It is an early glimpse of what institutional migration might look like in education: Not just digitizing the old classroom, but redesigning its structure, human roles and priorities around what AI can do.&lt;/p&gt;



&lt;p&gt;The World Bank reported on a pilot program in Nigeria that used AI to support learning through an after-school program. The results revealed “overwhelmingly positive effects on learning outcomes,” with AI serving as a virtual tutor and teachers providing support. Testing showed students achieved “nearly two years of typical learning in just six weeks.”&lt;/p&gt;



&lt;p&gt;Similar signals are emerging elsewhere. In government, a growing number of public agencies are experimenting with AI systems to improve responsiveness: triaging constituent inquiries, drafting preliminary communications or analyzing public sentiment. Leading AI labs such as OpenAI are now tailoring their tools for government use. These nascent efforts offer a glimpse into how institutions might reallocate human effort and attention toward interpretation, discretion and trust-building; functions that remain profoundly human.&lt;/p&gt;



&lt;p&gt;While most of these initiatives are framed in terms of productivity, they raise deeper questions about the evolving role of the human within decision-making structures. In other words, what is the future of human work? The conventional wisdom viewpoint voiced by futurist Melanie Subin in a CBS interview is that “AI is going to change jobs, replace tasks and change the nature of work. But as with the Industrial Revolution and many other technological advancements we have seen over the past 100 years, there will still be a role for people; that role may just change.”&lt;/p&gt;



&lt;p&gt;That seeming evolution stands in stark contrast to the poignant prediction from Dario Amodei, CEO of Anthropic, one of the world’s most powerful creators of AI technologies. In his view, AI could eliminate&lt;em&gt; &lt;/em&gt;half of all entry-level white-collar jobs and spike unemployment to 10 to 20% in the next 1 to 5 years. “We, as the producers of this technology, have a duty and an obligation to be honest about what is coming,” he said in an interview with Axios. His draconian prediction could happen, although perhaps not as quickly as he suggests, as diffusion of new technology across society can often take longer than is expected.&lt;/p&gt;



&lt;p&gt;Nevertheless, the potential for AI to displace workers has long been known. As early as 2019, Kevin Roose wrote about conversations he had with corporate executives at a World Economic Forum meeting. “They’ll never admit it in public,” he wrote, “but many of your bosses want machines to replace you as soon as possible.”&lt;/p&gt;



&lt;p&gt;In 2025, Roose reported that there are signs this is beginning to occur. “In interview after interview, I’m hearing that firms are making rapid progress toward automating entry-level work, and that AI companies are racing to build ‘virtual workers’ that can replace junior employees at a fraction of the cost.”&lt;/p&gt;



&lt;p&gt;Across all institutional domains, there are green shoots of transformation. But the throughline remains fragmented, merely early signals of change and not yet blueprints. The deeper challenge is to move from experimentation to structural reinvention. In the interim, there could be a lot of collateral damage, not only to those who lose their jobs but also to the overall effectiveness of institutions amidst turmoil.&lt;/p&gt;



&lt;p&gt;How can institutions move from experimentation to integration, from reactive adoption to principled design? And can this be done at a pace that adequately reflects the rate of change? Recognizing the need is only the beginning. The real challenge is designing for it.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-institutional-design-principles-for-the-next-era"&gt;Institutional design principles for the next era&lt;/h2&gt;



&lt;p&gt;If AI acceleration continues, this will lead to immense pressure on institutions to respond. If institutions can move at pace, the question becomes: How can they move from reactive adoption to principled design? They need not just innovation, but informed vision and principled intention. Institutions must be reimagined from the ground up, built not just for efficiency or scale, but for adaptability, trust and long-term societal coherence.&lt;/p&gt;



&lt;p&gt;This requires design principles that are neither technocratic nor nostalgic, but grounded in the realities of the migration underway, based on shared intelligence, human vulnerability and with a goal of creating a more humane society. That in mind, here are three practical design principles.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="h-build-for-responsiveness-not-longevity"&gt;Build for responsiveness, not longevity&lt;/h3&gt;



&lt;p&gt;Institutions must be designed to move beyond fixed hierarchies and slow feedback loops. In a world reshaped by real-time information and AI-augmented decision-making, responsiveness and adaptability become core competencies. This means flattening decision layers where possible, empowering frontline actors with tools and trust and investing in data systems that surface insights quickly, without outsourcing judgment to algorithms alone. Responsiveness is not just about speed. It is about sensing change early and acting with moral clarity.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="h-integrate-ai-where-it-frees-humans-to-focus-on-the-human"&gt;Integrate AI where it frees humans to focus on the human&lt;/h3&gt;



&lt;p&gt;AI should be deployed not as a replacement strategy, but as a refocusing tool. The most forward-looking institutions will utilize AI to absorb repetitive tasks and administrative burdens, thus freeing human capacity for interpretation, trust-building, care, creativity and strategic thinking. In education, this might mean AI-created and presented lessons that allow teachers to spend more time with struggling students. In government, it could mean greater automated processing that gives human staff more time to solve complex cases with empathy and discretion. The goal should not be to fully automate institutions. It is instead to humanize them. This principle encourages using AI as a support beam, not a substitute.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="h-keep-humans-in-the-loop-where-it-matters-most"&gt;Keep humans in the loop where it matters most&lt;/h3&gt;



&lt;p&gt;Institutions that endure will be those that make room for human judgment at critical points of interpretation, escalation and ethics. This means designing systems where human-in-the-loop is not a checkbox, but a structural feature that is clearly defined, legally protected and socially valued. Whether in justice systems, healthcare or public service, the presence of a human voice and moral perspective must remain central where stakes are high, and values are contested. AI can inform, but humans must still decide.&lt;/p&gt;



&lt;p&gt;These principles are not meant to be static rules, but directional choices. They are starting points for reimagining how institutions can remain human-centered in a machine-enhanced world. They reflect a commitment to modernization without moral abandonment, to speed without shallowness or callousness and to intelligence shared between humans and machines.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-beyond-adaptation-institutions-and-question-of-purpose"&gt;Beyond adaptation: Institutions and question of purpose&lt;/h2&gt;



&lt;p&gt;In times of disruption, individuals often ask: &lt;em&gt;‘What was I made for?’&lt;/em&gt; We must ask the same of our institutions. As AI upends our cognitive terrain and accelerates the pace of change, the relevance of our core institutions is no longer guaranteed by tradition, function or status. They, too, are subject to the forces of cognitive migration. Like individuals, their future must include decisions about whether to resist, retreat or transform.&lt;/p&gt;



&lt;p&gt;As generative AI systems take on tasks of reasoning, research, writing and coordination, the foundational assumptions of institutional authority including expertise, hierarchy and predictability begin to fracture. But what follows cannot be a hollowing out, because the fundamental purpose of institutions is too essential to abandon. It must be a re-founding.&lt;/p&gt;



&lt;p&gt;Our institutions should not be replaced by machines. They should instead become more human: More responsive to complexity, anchored in ethical deliberation, capable of holding long-term visions in a short-term world. Institutions that do not adapt with intention may not survive the turbulence ahead. The dynamism of the 21st century will not wait.&lt;/p&gt;



&lt;p&gt;This is the institutional dimension of cognitive migration: A reckoning with identity, value and function in a world where intelligence is no longer our exclusive domain. The institutions that endure will be those that migrate not just in form, but in soul, crossing into new terrain with tools that serve humanity.&lt;/p&gt;



&lt;p&gt;For those shaping schools, companies or civic structures, the path forward lies not in resisting AI, but in redefining what only humans and human institutions can truly offer.&lt;/p&gt;



&lt;p&gt;&lt;em&gt;Gary Grossman is EVP of technology practice at Edelman and global lead of the Edelman AI Center of Excellence.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Cognitive migration is not just an individual journey; it is also a collective and institutional one. As AI reshapes the terrain of thought, judgment and coordination, the very foundations of our schools, governments, corporations and civic systems are being called into question.&lt;/p&gt;



&lt;p&gt;Institutions, like people, now face the challenge of rapid change: “Rethinking” their purpose, adapting their structures and rediscovering what makes them essential in a world where machines can increasingly think, decide and produce. Like people who are undergoing cognitive migration, institutions — and the people who run them — must reassess what they were made for.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-discontinuity"&gt;Discontinuity&lt;/h2&gt;



&lt;p&gt;Institutions are designed to promote continuity. Their purpose is to endure, to offer structure, legitimacy and coherence across time. It is those very attributes that contribute to trust. We rely on institutions not only to deliver services and enforce norms, but to provide a sense of order in a complex world. They are the long-arc vessels of civilization, meant to hold steady as individuals come and go. Without viable institutions, society risks upheaval and an increasingly uncertain future.&lt;/p&gt;



&lt;p&gt;But today, many of our core institutions are reeling. Having long served as the scaffolding of modern life, they are being tested in ways that feel not only sudden, but systemic.&lt;/p&gt;



&lt;p&gt;Some of this pressure comes from AI, which is rapidly reshaping the cognitive terrain on which these institutions were built. But AI is not the only force. The past two decades have brought rising public distrust, partisan fragmentation and challenges to institutional legitimacy that predate the generative AI technological wave. From increasing income inequality, to attacks on scientific process and consensus, to politicized courts, to declining university enrollments, the erosion of trust in our institutions has multiple causes, as well as compounding effects.&lt;/p&gt;



&lt;p&gt;In this context, the arrival of increasingly capable AI systems is not merely another challenge. It is an accelerant, fuel to the fire of institutional disruption. This disruption demands that institutions adapt their operations and revisit foundational assumptions. What are institutions for in a world where credentialing, reasoning and coordination are no longer exclusively human domains? All this institutional reinvention needs to take place at a pace that defies their very purpose and nature.&lt;/p&gt;



&lt;p&gt;This is the institutional dimension of cognitive migration: A shift not just in individuals find meaning and value, but in how our collective societal structures must evolve to support a new era. And as with all migrations, the journey will be uneven, contested and deeply consequential.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-architecture-of-the-old-regime"&gt;The architecture of the old regime&lt;/h2&gt;



&lt;p&gt;The institutions in place now were not designed for this moment. Most were forged in the Industrial Age and refined during the Digital Revolution. Their operating models reflect the logic of earlier cognitive regimes: stable processes, centralized expertise and the tacit assumption that human intelligence would remain preeminent.&lt;/p&gt;



&lt;p&gt;Schools, corporations, courts and government agencies are structured to manage people and information on a large scale. They rely on predictability, expert credentials and well-defined hierarchies of decision-making. These are traditional strengths that — even when considered bureaucratic — have historically offered a foundation for trust, consistency and broad participation within complex societies.&lt;/p&gt;



&lt;p&gt;But the assumptions beneath these structures are under strain. AI systems now perform tasks once reserved for knowledge workers, including summarizing documents, analyzing data, writing legal briefs, performing research, creating lesson plans and teaching, coding applications and building and executing marketing campaigns. Beyond automation, a deeper disruption is underway: The people running these institutions are expected to defend their continued relevance in a world where knowledge itself is no longer as highly valued or even a uniquely human asset.&lt;/p&gt;



&lt;p&gt;The relevance of some institutions is called into question from outside challengers including tech platforms, alternative credentialing models and decentralized networks. This essentially means that the traditional gatekeepers of trust, expertise and coordination are being challenged by faster, flatter and often more digitally native alternatives. In some cases, even long-standing institutional functions such as adjudicating disputes are being questioned, ignored, or bypassed altogether.&lt;/p&gt;



&lt;p&gt;This does not mean institutional collapse is inevitable. But it does suggest that the current paradigm of stable, slow-moving and authority-based structures may not endure. At a minimum, institutions are under intense pressure to change. If institutions are to remain relevant and play a vital role in the age of AI, they must become more adaptive, transparent and attuned to the values that cannot readily be encoded in algorithms: human dignity, ethical deliberation and long-term stewardship.&lt;/p&gt;



&lt;p&gt;The choice ahead is not whether institutions will change, but how. Will they resist, ossify and fall into irrelevance? Will they be forcibly restructured to meet transient agendas? Or will they deliberately reimagine themselves as co-evolving partners in a world of shared intelligence and shifting value?&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-first-steps-of-institutional-migration"&gt;First steps of institutional migration&lt;/h2&gt;



&lt;p&gt;A growing number of institutions are beginning to adapt. These responses are varied and often tentative, signs of motion more than full transformation. These are green shoots; taken together, they suggest that the cognitive migration of institutions may already be underway.&lt;/p&gt;



&lt;p&gt;Yet there is a deeper challenge beneath these experiments: Many institutions are still bound by outdated methods of operating. The environment, however, has changed. AI and other factors are redrawing the landscape, and institutions are only beginning to recalibrate.&lt;/p&gt;



&lt;p&gt;One example of change comes from an Arizona-based charter school where AI plays a leading role in daily instruction. Branded as Unbound Academy, the school uses AI platforms to deliver core academic content in condensed, focused sessions tailored for each child. This shows promise to improve academic achievement while also allowing students time later in the day to work on life skills, project-based learning and interpersonal development. In this model, teachers are reframed as guides and mentors, not content deliverers. It is an early glimpse of what institutional migration might look like in education: Not just digitizing the old classroom, but redesigning its structure, human roles and priorities around what AI can do.&lt;/p&gt;



&lt;p&gt;The World Bank reported on a pilot program in Nigeria that used AI to support learning through an after-school program. The results revealed “overwhelmingly positive effects on learning outcomes,” with AI serving as a virtual tutor and teachers providing support. Testing showed students achieved “nearly two years of typical learning in just six weeks.”&lt;/p&gt;



&lt;p&gt;Similar signals are emerging elsewhere. In government, a growing number of public agencies are experimenting with AI systems to improve responsiveness: triaging constituent inquiries, drafting preliminary communications or analyzing public sentiment. Leading AI labs such as OpenAI are now tailoring their tools for government use. These nascent efforts offer a glimpse into how institutions might reallocate human effort and attention toward interpretation, discretion and trust-building; functions that remain profoundly human.&lt;/p&gt;



&lt;p&gt;While most of these initiatives are framed in terms of productivity, they raise deeper questions about the evolving role of the human within decision-making structures. In other words, what is the future of human work? The conventional wisdom viewpoint voiced by futurist Melanie Subin in a CBS interview is that “AI is going to change jobs, replace tasks and change the nature of work. But as with the Industrial Revolution and many other technological advancements we have seen over the past 100 years, there will still be a role for people; that role may just change.”&lt;/p&gt;



&lt;p&gt;That seeming evolution stands in stark contrast to the poignant prediction from Dario Amodei, CEO of Anthropic, one of the world’s most powerful creators of AI technologies. In his view, AI could eliminate&lt;em&gt; &lt;/em&gt;half of all entry-level white-collar jobs and spike unemployment to 10 to 20% in the next 1 to 5 years. “We, as the producers of this technology, have a duty and an obligation to be honest about what is coming,” he said in an interview with Axios. His draconian prediction could happen, although perhaps not as quickly as he suggests, as diffusion of new technology across society can often take longer than is expected.&lt;/p&gt;



&lt;p&gt;Nevertheless, the potential for AI to displace workers has long been known. As early as 2019, Kevin Roose wrote about conversations he had with corporate executives at a World Economic Forum meeting. “They’ll never admit it in public,” he wrote, “but many of your bosses want machines to replace you as soon as possible.”&lt;/p&gt;



&lt;p&gt;In 2025, Roose reported that there are signs this is beginning to occur. “In interview after interview, I’m hearing that firms are making rapid progress toward automating entry-level work, and that AI companies are racing to build ‘virtual workers’ that can replace junior employees at a fraction of the cost.”&lt;/p&gt;



&lt;p&gt;Across all institutional domains, there are green shoots of transformation. But the throughline remains fragmented, merely early signals of change and not yet blueprints. The deeper challenge is to move from experimentation to structural reinvention. In the interim, there could be a lot of collateral damage, not only to those who lose their jobs but also to the overall effectiveness of institutions amidst turmoil.&lt;/p&gt;



&lt;p&gt;How can institutions move from experimentation to integration, from reactive adoption to principled design? And can this be done at a pace that adequately reflects the rate of change? Recognizing the need is only the beginning. The real challenge is designing for it.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-institutional-design-principles-for-the-next-era"&gt;Institutional design principles for the next era&lt;/h2&gt;



&lt;p&gt;If AI acceleration continues, this will lead to immense pressure on institutions to respond. If institutions can move at pace, the question becomes: How can they move from reactive adoption to principled design? They need not just innovation, but informed vision and principled intention. Institutions must be reimagined from the ground up, built not just for efficiency or scale, but for adaptability, trust and long-term societal coherence.&lt;/p&gt;



&lt;p&gt;This requires design principles that are neither technocratic nor nostalgic, but grounded in the realities of the migration underway, based on shared intelligence, human vulnerability and with a goal of creating a more humane society. That in mind, here are three practical design principles.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="h-build-for-responsiveness-not-longevity"&gt;Build for responsiveness, not longevity&lt;/h3&gt;



&lt;p&gt;Institutions must be designed to move beyond fixed hierarchies and slow feedback loops. In a world reshaped by real-time information and AI-augmented decision-making, responsiveness and adaptability become core competencies. This means flattening decision layers where possible, empowering frontline actors with tools and trust and investing in data systems that surface insights quickly, without outsourcing judgment to algorithms alone. Responsiveness is not just about speed. It is about sensing change early and acting with moral clarity.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="h-integrate-ai-where-it-frees-humans-to-focus-on-the-human"&gt;Integrate AI where it frees humans to focus on the human&lt;/h3&gt;



&lt;p&gt;AI should be deployed not as a replacement strategy, but as a refocusing tool. The most forward-looking institutions will utilize AI to absorb repetitive tasks and administrative burdens, thus freeing human capacity for interpretation, trust-building, care, creativity and strategic thinking. In education, this might mean AI-created and presented lessons that allow teachers to spend more time with struggling students. In government, it could mean greater automated processing that gives human staff more time to solve complex cases with empathy and discretion. The goal should not be to fully automate institutions. It is instead to humanize them. This principle encourages using AI as a support beam, not a substitute.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="h-keep-humans-in-the-loop-where-it-matters-most"&gt;Keep humans in the loop where it matters most&lt;/h3&gt;



&lt;p&gt;Institutions that endure will be those that make room for human judgment at critical points of interpretation, escalation and ethics. This means designing systems where human-in-the-loop is not a checkbox, but a structural feature that is clearly defined, legally protected and socially valued. Whether in justice systems, healthcare or public service, the presence of a human voice and moral perspective must remain central where stakes are high, and values are contested. AI can inform, but humans must still decide.&lt;/p&gt;



&lt;p&gt;These principles are not meant to be static rules, but directional choices. They are starting points for reimagining how institutions can remain human-centered in a machine-enhanced world. They reflect a commitment to modernization without moral abandonment, to speed without shallowness or callousness and to intelligence shared between humans and machines.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-beyond-adaptation-institutions-and-question-of-purpose"&gt;Beyond adaptation: Institutions and question of purpose&lt;/h2&gt;



&lt;p&gt;In times of disruption, individuals often ask: &lt;em&gt;‘What was I made for?’&lt;/em&gt; We must ask the same of our institutions. As AI upends our cognitive terrain and accelerates the pace of change, the relevance of our core institutions is no longer guaranteed by tradition, function or status. They, too, are subject to the forces of cognitive migration. Like individuals, their future must include decisions about whether to resist, retreat or transform.&lt;/p&gt;



&lt;p&gt;As generative AI systems take on tasks of reasoning, research, writing and coordination, the foundational assumptions of institutional authority including expertise, hierarchy and predictability begin to fracture. But what follows cannot be a hollowing out, because the fundamental purpose of institutions is too essential to abandon. It must be a re-founding.&lt;/p&gt;



&lt;p&gt;Our institutions should not be replaced by machines. They should instead become more human: More responsive to complexity, anchored in ethical deliberation, capable of holding long-term visions in a short-term world. Institutions that do not adapt with intention may not survive the turbulence ahead. The dynamism of the 21st century will not wait.&lt;/p&gt;



&lt;p&gt;This is the institutional dimension of cognitive migration: A reckoning with identity, value and function in a world where intelligence is no longer our exclusive domain. The institutions that endure will be those that migrate not just in form, but in soul, crossing into new terrain with tools that serve humanity.&lt;/p&gt;



&lt;p&gt;For those shaping schools, companies or civic structures, the path forward lies not in resisting AI, but in redefining what only humans and human institutions can truly offer.&lt;/p&gt;



&lt;p&gt;&lt;em&gt;Gary Grossman is EVP of technology practice at Edelman and global lead of the Edelman AI Center of Excellence.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/like-humans-ai-is-forcing-institutions-to-rethink-their-purpose/</guid><pubDate>Sun, 08 Jun 2025 20:00:00 +0000</pubDate></item><item><title>‘AI Maker, Not an AI Taker’: UK Builds Its Vision With NVIDIA Infrastructure (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/uk-ai-vision/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/eyJ3IjoyMDQ4LCJoIjoyMDQ4LCJzY29wZSI6ImFwcCJ9.webp" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;U.K. Prime Minister Keir Starmer’s ambition for Britain to be an “AI maker, not an AI taker,” is becoming a reality at London Tech Week.&lt;/p&gt;
&lt;p&gt;With NVIDIA’s support, the U.K. is building sovereign compute infrastructure, investing in cutting-edge research and skills, and fostering AI leadership across sectors.&lt;/p&gt;
&lt;p&gt;As London Tech Week kicks off today, NVIDIA and some of Britain’s best companies are convening and hosting the first U.K. Sovereign AI Industry Forum.&lt;/p&gt;
&lt;p&gt;The initiative unites leading U.K. businesses — including founding members Babcock, BAE Systems, BT, National Grid and Standard Chartered — to strengthen the nation’s economic security by advancing sovereign AI infrastructure and accelerating the growth of the U.K. AI startup ecosystem.&lt;/p&gt;
&lt;p&gt;“We have big plans when it comes to developing the next wave of AI innovations here in the U.K. — not only so we can deliver the economic growth needed for our Plan for Change, but maintain our position as a global leader,” U.K. Secretary of State for Science, Innovation and Technology Peter Kyle said. “Central to that is making sure we have the infrastructure to power AI, so I welcome NVIDIA setting up the U.K. Sovereign AI Industry Forum — bringing together leading British businesses to develop and deploy this across the U.K. so we can drive growth and opportunity.”&lt;/p&gt;
&lt;p&gt;The U.K. is a global AI hub, leading Europe in newly funded AI startups and total private AI investment through 2024. And the sector is growing fast, backed by over $28 billion in private investment since 2013.&lt;/p&gt;
&lt;p&gt;And AI investment benefits the whole of the U.K.&lt;/p&gt;
&lt;p&gt;According to an analysis released today by Public First, regions with more AI and data center infrastructure consistently show stronger economic growth. Even a modest increase in AI data center capacity could add nearly £5 billion to national economic output, while a more significant increase, for example, doubling access, could raise the annual benefit to £36.5 billion.&lt;/p&gt;
&lt;p&gt;Responding to this opportunity, cloud provider Nscale announced at London Tech Week its commitment to deploy U.K. AI infrastructure with 10,000 NVIDIA Blackwell GPUs by the end of 2026. This facility will help position the U.K. as a global leader in AI, supporting innovation, job creation and the development of a thriving domestic AI ecosystem.&lt;/p&gt;
&lt;p&gt;And cloud provider Nebius is continuing the region’s momentum with the launch of its first AI factory in the U.K. It announced it’s bringing 4,000 NVIDIA Blackwell GPUs online, making available scalable, high-performance AI capacity at home in the U.K. — to power U.K. research, academia and public services, including the NHS.&lt;/p&gt;
&lt;h2&gt;Mind the (Skills) Gap&lt;/h2&gt;
&lt;p&gt;AI developers are the engine of this new industrial revolution. That’s why NVIDIA is supporting the U.K. government’s national skills drive by training developers in AI.&lt;/p&gt;
&lt;p&gt;To support this goal, a new NVIDIA AI Technology Center in the U.K. will provide hands-on training in AI, data science and accelerated computing, focusing on foundation model builders, embodied AI, materials science and earth systems modeling.&lt;/p&gt;
&lt;p&gt;Beyond training, this collaboration drives cutting-edge AI applications and research.&lt;/p&gt;
&lt;p&gt;For example, the U.K.’s world-leading financial services industry gets a boost from a new AI-powered digital sandbox. This sandbox, a digital testing environment for safe AI innovation in financial services, will be provided by the Financial Conduct Authority, with infrastructure provided by NayaOne and supported by NVIDIA’s platform.&lt;/p&gt;
&lt;p&gt;At the same time, Barclays Eagle Labs’ launch of an Innovation Hub in London will help AI and deep tech startups grow to the next level. NVIDIA is supporting the program by offering startups a pathway to the NVIDIA Inception program with access to advanced tools and training.&lt;/p&gt;
&lt;p&gt;Furthermore, the Department for Science, Innovation and Technology announced a collaboration with NVIDIA to promote the nation’s goals for AI development in telecoms. Leading U.K. universities will gain access to a suite of powerful AI tools, 6G research platforms and training resources to bolster research and development on AI-native wireless networks.&lt;/p&gt;
&lt;h2&gt;The Research Engine&lt;/h2&gt;
&lt;p&gt;Universities are central to the U.K.’s strategy.&lt;/p&gt;
&lt;p&gt;Led by Oxford University, the JADE consortium, comprising 20 universities and the Turing Institute, uses NVIDIA technologies to advance AI development and safety. At University College London, researchers are developing a digital twin of the human body enabled by NVIDIA technology. At the University of Bristol, the Isambard-AI supercomputer, built on NVIDIA Grace Hopper Superchips, is powering progress in AI safety, climate modeling and next-generation science. And at the University of Manchester, the NVIDIA Earth-2 platform is being deployed to develop pollution-flow models.&lt;/p&gt;
&lt;p&gt;Meanwhile, U.K. tech leaders use NVIDIA’s foundational technologies to innovate across diverse sectors.&lt;/p&gt;
&lt;p&gt;It’s how Wayve trains AI for autonomous vehicles. How JBA Risk Management helps organizations anticipate and mitigate climate risks with new precision. And how Stability AI is unleashing creativity with open-source generative AI that turns ideas into images, text and more — instantly.&lt;/p&gt;
&lt;p&gt;NVIDIA also champions the U.K.’s most ambitious AI startups through NVIDIA Inception, providing specialized resources and support for startups building new products and services.&lt;/p&gt;
&lt;p&gt;Basecamp Research is revolutionizing drug discovery with AI trained on the planet’s biodiversity. Humanoid advances automation and brings commercially scalable, reliable and safe humanoid robots closer to real-world deployment. Relation is accelerating the discovery of tomorrow’s medicines. And Synthesia turns text into studio-quality, multilingual videos with lifelike avatars.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Industry in Motion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The U.K.’s biggest companies are moving fast, too.&lt;/p&gt;
&lt;p&gt;Companies like BT, LSEG and NatWest are transforming industries with AI. BT is powering agentic AI-based autonomous operations; LSEG is empowering customers with highly accurate, AI-driven data and insights; and NatWest is streamlining operations and safeguarding customers.&lt;/p&gt;
&lt;p&gt;With government vision, talent and cutting-edge tech converging, the U.K. is taking its place among those making AI advances at home and worldwide.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Watch NVIDIA founder and CEO Jensen Huang’s keynote &lt;/i&gt;&lt;i&gt;NVIDIA GTC Paris at VivaTech&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/eyJ3IjoyMDQ4LCJoIjoyMDQ4LCJzY29wZSI6ImFwcCJ9.webp" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;U.K. Prime Minister Keir Starmer’s ambition for Britain to be an “AI maker, not an AI taker,” is becoming a reality at London Tech Week.&lt;/p&gt;
&lt;p&gt;With NVIDIA’s support, the U.K. is building sovereign compute infrastructure, investing in cutting-edge research and skills, and fostering AI leadership across sectors.&lt;/p&gt;
&lt;p&gt;As London Tech Week kicks off today, NVIDIA and some of Britain’s best companies are convening and hosting the first U.K. Sovereign AI Industry Forum.&lt;/p&gt;
&lt;p&gt;The initiative unites leading U.K. businesses — including founding members Babcock, BAE Systems, BT, National Grid and Standard Chartered — to strengthen the nation’s economic security by advancing sovereign AI infrastructure and accelerating the growth of the U.K. AI startup ecosystem.&lt;/p&gt;
&lt;p&gt;“We have big plans when it comes to developing the next wave of AI innovations here in the U.K. — not only so we can deliver the economic growth needed for our Plan for Change, but maintain our position as a global leader,” U.K. Secretary of State for Science, Innovation and Technology Peter Kyle said. “Central to that is making sure we have the infrastructure to power AI, so I welcome NVIDIA setting up the U.K. Sovereign AI Industry Forum — bringing together leading British businesses to develop and deploy this across the U.K. so we can drive growth and opportunity.”&lt;/p&gt;
&lt;p&gt;The U.K. is a global AI hub, leading Europe in newly funded AI startups and total private AI investment through 2024. And the sector is growing fast, backed by over $28 billion in private investment since 2013.&lt;/p&gt;
&lt;p&gt;And AI investment benefits the whole of the U.K.&lt;/p&gt;
&lt;p&gt;According to an analysis released today by Public First, regions with more AI and data center infrastructure consistently show stronger economic growth. Even a modest increase in AI data center capacity could add nearly £5 billion to national economic output, while a more significant increase, for example, doubling access, could raise the annual benefit to £36.5 billion.&lt;/p&gt;
&lt;p&gt;Responding to this opportunity, cloud provider Nscale announced at London Tech Week its commitment to deploy U.K. AI infrastructure with 10,000 NVIDIA Blackwell GPUs by the end of 2026. This facility will help position the U.K. as a global leader in AI, supporting innovation, job creation and the development of a thriving domestic AI ecosystem.&lt;/p&gt;
&lt;p&gt;And cloud provider Nebius is continuing the region’s momentum with the launch of its first AI factory in the U.K. It announced it’s bringing 4,000 NVIDIA Blackwell GPUs online, making available scalable, high-performance AI capacity at home in the U.K. — to power U.K. research, academia and public services, including the NHS.&lt;/p&gt;
&lt;h2&gt;Mind the (Skills) Gap&lt;/h2&gt;
&lt;p&gt;AI developers are the engine of this new industrial revolution. That’s why NVIDIA is supporting the U.K. government’s national skills drive by training developers in AI.&lt;/p&gt;
&lt;p&gt;To support this goal, a new NVIDIA AI Technology Center in the U.K. will provide hands-on training in AI, data science and accelerated computing, focusing on foundation model builders, embodied AI, materials science and earth systems modeling.&lt;/p&gt;
&lt;p&gt;Beyond training, this collaboration drives cutting-edge AI applications and research.&lt;/p&gt;
&lt;p&gt;For example, the U.K.’s world-leading financial services industry gets a boost from a new AI-powered digital sandbox. This sandbox, a digital testing environment for safe AI innovation in financial services, will be provided by the Financial Conduct Authority, with infrastructure provided by NayaOne and supported by NVIDIA’s platform.&lt;/p&gt;
&lt;p&gt;At the same time, Barclays Eagle Labs’ launch of an Innovation Hub in London will help AI and deep tech startups grow to the next level. NVIDIA is supporting the program by offering startups a pathway to the NVIDIA Inception program with access to advanced tools and training.&lt;/p&gt;
&lt;p&gt;Furthermore, the Department for Science, Innovation and Technology announced a collaboration with NVIDIA to promote the nation’s goals for AI development in telecoms. Leading U.K. universities will gain access to a suite of powerful AI tools, 6G research platforms and training resources to bolster research and development on AI-native wireless networks.&lt;/p&gt;
&lt;h2&gt;The Research Engine&lt;/h2&gt;
&lt;p&gt;Universities are central to the U.K.’s strategy.&lt;/p&gt;
&lt;p&gt;Led by Oxford University, the JADE consortium, comprising 20 universities and the Turing Institute, uses NVIDIA technologies to advance AI development and safety. At University College London, researchers are developing a digital twin of the human body enabled by NVIDIA technology. At the University of Bristol, the Isambard-AI supercomputer, built on NVIDIA Grace Hopper Superchips, is powering progress in AI safety, climate modeling and next-generation science. And at the University of Manchester, the NVIDIA Earth-2 platform is being deployed to develop pollution-flow models.&lt;/p&gt;
&lt;p&gt;Meanwhile, U.K. tech leaders use NVIDIA’s foundational technologies to innovate across diverse sectors.&lt;/p&gt;
&lt;p&gt;It’s how Wayve trains AI for autonomous vehicles. How JBA Risk Management helps organizations anticipate and mitigate climate risks with new precision. And how Stability AI is unleashing creativity with open-source generative AI that turns ideas into images, text and more — instantly.&lt;/p&gt;
&lt;p&gt;NVIDIA also champions the U.K.’s most ambitious AI startups through NVIDIA Inception, providing specialized resources and support for startups building new products and services.&lt;/p&gt;
&lt;p&gt;Basecamp Research is revolutionizing drug discovery with AI trained on the planet’s biodiversity. Humanoid advances automation and brings commercially scalable, reliable and safe humanoid robots closer to real-world deployment. Relation is accelerating the discovery of tomorrow’s medicines. And Synthesia turns text into studio-quality, multilingual videos with lifelike avatars.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Industry in Motion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The U.K.’s biggest companies are moving fast, too.&lt;/p&gt;
&lt;p&gt;Companies like BT, LSEG and NatWest are transforming industries with AI. BT is powering agentic AI-based autonomous operations; LSEG is empowering customers with highly accurate, AI-driven data and insights; and NatWest is streamlining operations and safeguarding customers.&lt;/p&gt;
&lt;p&gt;With government vision, talent and cutting-edge tech converging, the U.K. is taking its place among those making AI advances at home and worldwide.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Watch NVIDIA founder and CEO Jensen Huang’s keynote &lt;/i&gt;&lt;i&gt;NVIDIA GTC Paris at VivaTech&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/uk-ai-vision/</guid><pubDate>Sun, 08 Jun 2025 21:30:18 +0000</pubDate></item><item><title>Why Meta’s Biggest AI Bet Isn’t on Models—It’s on Data (Unite.AI)</title><link>https://www.unite.ai/why-metas-biggest-ai-bet-isnt-on-models-its-on-data/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.unite.ai/wp-content/uploads/2025/06/20250609_0010_Futuristic-Boardroom-Dynamics_simple_compose_01jx9ajrghe919c0n98ppr8rnw-942x600.png" /&gt;&lt;/div&gt;&lt;p&gt;Meta's reported $10 billion investment in Scale AI represents far more than a simple funding round—it signals a fundamental strategic evolution in how tech giants view the AI arms race. This potential deal, which could exceed $10 billion and would be Meta's largest external AI investment, reveals Mark Zuckerberg's company doubling down on a critical insight: in the post-ChatGPT era, victory belongs not to those with the most sophisticated algorithms, but to those who control the highest-quality data pipelines.&lt;/p&gt;&lt;h3&gt;By the Numbers:&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;$10 billion&lt;/strong&gt;: Meta's potential investment in Scale AI&lt;/li&gt;&lt;li&gt;&lt;strong&gt;$870M → $2B&lt;/strong&gt;: Scale AI's revenue growth (2024 to 2025)&lt;/li&gt;&lt;li&gt;&lt;strong&gt;$7B → $13.8B&lt;/strong&gt;: Scale AI's valuation trajectory in recent funding rounds&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;The Data Infrastructure Imperative&lt;/h2&gt;&lt;p&gt;After Llama 4's lukewarm reception, Meta might be looking to secure exclusive datasets that could give it an edge over rivals like OpenAI and Microsoft. This timing is no coincidence. While Meta's latest models showed promise in technical benchmarks, early user feedback and implementation challenges highlighted a stark reality: architectural innovations alone are insufficient in today's AI world.&lt;/p&gt;&lt;p&gt;“As an AI community we've exhausted all of the easy data, the internet data, and now we need to move on to more complex data,” Scale AI CEO Alexandr Wang told the Financial Times back in 2024. “The quantity matters but the quality is paramount.” This observation captures precisely why Meta is willing to make such a substantial investment in Scale AI's infrastructure.&lt;/p&gt;&lt;p&gt;Scale AI has positioned itself as the “data foundry” of the AI revolution, providing data-labeling services to companies that want to train machine learning models through a sophisticated hybrid approach combining automation with human expertise. Scale's secret weapon is its hybrid model: it uses automation to pre-process and filter tasks but relies on a trained, distributed workforce for human judgment in AI training where it matters most.&lt;/p&gt;&lt;h2&gt;Strategic Differentiation Through Data Control&lt;/h2&gt;&lt;p&gt;Meta's investment thesis rests on a sophisticated understanding of competitive dynamics that extend beyond traditional model development. While competitors like Microsoft pour billions into model creators like OpenAI, Meta is betting on controlling the underlying data infrastructure that feeds all AI systems.&lt;/p&gt;&lt;p&gt;This approach offers several compelling benefits:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Proprietary dataset access&lt;/strong&gt; — Enhanced model training capabilities while potentially limiting competitor access to the same high-quality data&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Pipeline control&lt;/strong&gt; — Reduced dependencies on external providers and more predictable cost structures&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Infrastructure focus&lt;/strong&gt; — Investment in foundational layers rather than competing solely on model architecture&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The Scale AI partnership positions Meta to capitalize on the growing complexity of AI training data requirements. Recent developments suggest that advances in large AI models may depend less on architectural innovations and more on access to high-quality training data and compute. This insight drives Meta's willingness to invest heavily in data infrastructure rather than competing solely on model architecture.&lt;/p&gt;&lt;h2&gt;The Military and Government Dimension&lt;/h2&gt;&lt;p&gt;The investment carries significant implications beyond commercial AI applications. Both Meta and Scale AI are deepening ties with the US government. The two companies are working on Defense Llama, a military-adapted version of Meta's Llama model. Scale AI recently landed a contract with the US Department of Defense to develop AI agents for operational use.&lt;/p&gt;&lt;p&gt;This government partnership dimension adds strategic value that extends far beyond immediate financial returns. Military and government contracts provide stable, long-term revenue streams while positioning both companies as critical infrastructure providers for national AI capabilities. The Defense Llama project exemplifies how commercial AI development increasingly intersects with national security considerations.&lt;/p&gt;&lt;h2&gt;Challenging the Microsoft-OpenAI Paradigm&lt;/h2&gt;&lt;p&gt;Meta's Scale AI investment would be a direct challenge to the dominant Microsoft-OpenAI partnership model that has defined the current AI space. Microsoft remains a major investor in OpenAI, providing funding and capacity to support their advancements, but this relationship focuses primarily on model development and deployment rather than fundamental data infrastructure.&lt;/p&gt;&lt;p&gt;By contrast, Meta's approach prioritizes controlling the foundational layer that enables all AI development. This strategy could prove more durable than exclusive model partnerships, which face increasing competitive pressure and potential partnership instability. Recent reports suggest Microsoft is developing its own in-house reasoning models to compete with OpenAI and has been testing models from Elon Musk's xAI, Meta, and DeepSeek to replace ChatGPT in Copilot, highlighting the inherent tensions in Big Tech's AI investment strategies.&lt;/p&gt;&lt;h2&gt;The Economics of AI Infrastructure&lt;/h2&gt;&lt;p&gt;Scale AI saw $870 million in revenue last year and expects to bring in $2 billion this year, demonstrating the substantial market demand for professional AI data services. The company's valuation trajectory—from around $7 billion to $13.8 billion in recent funding rounds—reflects investor recognition that data infrastructure represents a durable competitive moat.&lt;/p&gt;&lt;p&gt;Meta's $10 billion investment would provide Scale AI with unprecedented resources to expand its operations globally and develop more sophisticated data processing capabilities. This scale advantage could create network effects that make it increasingly difficult for competitors to match Scale AI's quality and cost efficiency, particularly as AI infrastructure investments continue to escalate across the industry.&lt;/p&gt;&lt;p&gt;This investment signals a broader industry evolution toward vertical integration of AI infrastructure. Rather than relying on partnerships with specialized AI companies, tech giants are increasingly acquiring or investing heavily in the underlying infrastructure that enables AI development.&lt;/p&gt;&lt;p&gt;The move also highlights growing recognition that data quality and model alignment services will become even more critical as AI systems become more powerful and are deployed in more sensitive applications. Scale AI's expertise in reinforcement learning from human feedback (RLHF) and model evaluation provides Meta with capabilities essential for developing safe, reliable AI systems.&lt;/p&gt;&lt;h2&gt;Looking Forward: The Data Wars Begin&lt;/h2&gt;&lt;p&gt;Meta's Scale AI investment represents the opening salvo in what may become the “data wars”—a competition for control over the high-quality, specialized datasets that will determine AI leadership in the coming decade.&lt;/p&gt;&lt;p&gt;This strategic pivot acknowledges that while the current AI boom began with breakthrough models like ChatGPT, sustained competitive advantage will come from controlling the infrastructure that enables continuous model improvement. As the industry matures beyond the initial excitement of generative AI, companies that control data pipelines may find themselves with more durable advantages than those who merely license or partner for model access.&lt;/p&gt;&lt;p&gt;For Meta, the Scale AI investment is a calculated bet that the future of AI competition will be won in the data preprocessing centers and annotation workflows that most consumers never see—but which ultimately determine which AI systems succeed in the real world. If this thesis proves correct, Meta's $10 billion investment may be remembered as the moment the company secured its position in the next phase of the AI revolution.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.unite.ai/wp-content/uploads/2025/06/20250609_0010_Futuristic-Boardroom-Dynamics_simple_compose_01jx9ajrghe919c0n98ppr8rnw-942x600.png" /&gt;&lt;/div&gt;&lt;p&gt;Meta's reported $10 billion investment in Scale AI represents far more than a simple funding round—it signals a fundamental strategic evolution in how tech giants view the AI arms race. This potential deal, which could exceed $10 billion and would be Meta's largest external AI investment, reveals Mark Zuckerberg's company doubling down on a critical insight: in the post-ChatGPT era, victory belongs not to those with the most sophisticated algorithms, but to those who control the highest-quality data pipelines.&lt;/p&gt;&lt;h3&gt;By the Numbers:&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;$10 billion&lt;/strong&gt;: Meta's potential investment in Scale AI&lt;/li&gt;&lt;li&gt;&lt;strong&gt;$870M → $2B&lt;/strong&gt;: Scale AI's revenue growth (2024 to 2025)&lt;/li&gt;&lt;li&gt;&lt;strong&gt;$7B → $13.8B&lt;/strong&gt;: Scale AI's valuation trajectory in recent funding rounds&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;The Data Infrastructure Imperative&lt;/h2&gt;&lt;p&gt;After Llama 4's lukewarm reception, Meta might be looking to secure exclusive datasets that could give it an edge over rivals like OpenAI and Microsoft. This timing is no coincidence. While Meta's latest models showed promise in technical benchmarks, early user feedback and implementation challenges highlighted a stark reality: architectural innovations alone are insufficient in today's AI world.&lt;/p&gt;&lt;p&gt;“As an AI community we've exhausted all of the easy data, the internet data, and now we need to move on to more complex data,” Scale AI CEO Alexandr Wang told the Financial Times back in 2024. “The quantity matters but the quality is paramount.” This observation captures precisely why Meta is willing to make such a substantial investment in Scale AI's infrastructure.&lt;/p&gt;&lt;p&gt;Scale AI has positioned itself as the “data foundry” of the AI revolution, providing data-labeling services to companies that want to train machine learning models through a sophisticated hybrid approach combining automation with human expertise. Scale's secret weapon is its hybrid model: it uses automation to pre-process and filter tasks but relies on a trained, distributed workforce for human judgment in AI training where it matters most.&lt;/p&gt;&lt;h2&gt;Strategic Differentiation Through Data Control&lt;/h2&gt;&lt;p&gt;Meta's investment thesis rests on a sophisticated understanding of competitive dynamics that extend beyond traditional model development. While competitors like Microsoft pour billions into model creators like OpenAI, Meta is betting on controlling the underlying data infrastructure that feeds all AI systems.&lt;/p&gt;&lt;p&gt;This approach offers several compelling benefits:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Proprietary dataset access&lt;/strong&gt; — Enhanced model training capabilities while potentially limiting competitor access to the same high-quality data&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Pipeline control&lt;/strong&gt; — Reduced dependencies on external providers and more predictable cost structures&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Infrastructure focus&lt;/strong&gt; — Investment in foundational layers rather than competing solely on model architecture&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The Scale AI partnership positions Meta to capitalize on the growing complexity of AI training data requirements. Recent developments suggest that advances in large AI models may depend less on architectural innovations and more on access to high-quality training data and compute. This insight drives Meta's willingness to invest heavily in data infrastructure rather than competing solely on model architecture.&lt;/p&gt;&lt;h2&gt;The Military and Government Dimension&lt;/h2&gt;&lt;p&gt;The investment carries significant implications beyond commercial AI applications. Both Meta and Scale AI are deepening ties with the US government. The two companies are working on Defense Llama, a military-adapted version of Meta's Llama model. Scale AI recently landed a contract with the US Department of Defense to develop AI agents for operational use.&lt;/p&gt;&lt;p&gt;This government partnership dimension adds strategic value that extends far beyond immediate financial returns. Military and government contracts provide stable, long-term revenue streams while positioning both companies as critical infrastructure providers for national AI capabilities. The Defense Llama project exemplifies how commercial AI development increasingly intersects with national security considerations.&lt;/p&gt;&lt;h2&gt;Challenging the Microsoft-OpenAI Paradigm&lt;/h2&gt;&lt;p&gt;Meta's Scale AI investment would be a direct challenge to the dominant Microsoft-OpenAI partnership model that has defined the current AI space. Microsoft remains a major investor in OpenAI, providing funding and capacity to support their advancements, but this relationship focuses primarily on model development and deployment rather than fundamental data infrastructure.&lt;/p&gt;&lt;p&gt;By contrast, Meta's approach prioritizes controlling the foundational layer that enables all AI development. This strategy could prove more durable than exclusive model partnerships, which face increasing competitive pressure and potential partnership instability. Recent reports suggest Microsoft is developing its own in-house reasoning models to compete with OpenAI and has been testing models from Elon Musk's xAI, Meta, and DeepSeek to replace ChatGPT in Copilot, highlighting the inherent tensions in Big Tech's AI investment strategies.&lt;/p&gt;&lt;h2&gt;The Economics of AI Infrastructure&lt;/h2&gt;&lt;p&gt;Scale AI saw $870 million in revenue last year and expects to bring in $2 billion this year, demonstrating the substantial market demand for professional AI data services. The company's valuation trajectory—from around $7 billion to $13.8 billion in recent funding rounds—reflects investor recognition that data infrastructure represents a durable competitive moat.&lt;/p&gt;&lt;p&gt;Meta's $10 billion investment would provide Scale AI with unprecedented resources to expand its operations globally and develop more sophisticated data processing capabilities. This scale advantage could create network effects that make it increasingly difficult for competitors to match Scale AI's quality and cost efficiency, particularly as AI infrastructure investments continue to escalate across the industry.&lt;/p&gt;&lt;p&gt;This investment signals a broader industry evolution toward vertical integration of AI infrastructure. Rather than relying on partnerships with specialized AI companies, tech giants are increasingly acquiring or investing heavily in the underlying infrastructure that enables AI development.&lt;/p&gt;&lt;p&gt;The move also highlights growing recognition that data quality and model alignment services will become even more critical as AI systems become more powerful and are deployed in more sensitive applications. Scale AI's expertise in reinforcement learning from human feedback (RLHF) and model evaluation provides Meta with capabilities essential for developing safe, reliable AI systems.&lt;/p&gt;&lt;h2&gt;Looking Forward: The Data Wars Begin&lt;/h2&gt;&lt;p&gt;Meta's Scale AI investment represents the opening salvo in what may become the “data wars”—a competition for control over the high-quality, specialized datasets that will determine AI leadership in the coming decade.&lt;/p&gt;&lt;p&gt;This strategic pivot acknowledges that while the current AI boom began with breakthrough models like ChatGPT, sustained competitive advantage will come from controlling the infrastructure that enables continuous model improvement. As the industry matures beyond the initial excitement of generative AI, companies that control data pipelines may find themselves with more durable advantages than those who merely license or partner for model access.&lt;/p&gt;&lt;p&gt;For Meta, the Scale AI investment is a calculated bet that the future of AI competition will be won in the data preprocessing centers and annotation workflows that most consumers never see—but which ultimately determine which AI systems succeed in the real world. If this thesis proves correct, Meta's $10 billion investment may be remembered as the moment the company secured its position in the next phase of the AI revolution.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.unite.ai/why-metas-biggest-ai-bet-isnt-on-models-its-on-data/</guid><pubDate>Mon, 09 Jun 2025 03:17:59 +0000</pubDate></item><item><title>UK Prime Minister, NVIDIA CEO Set the Stage as AI Lights Up Europe (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/ai-lights-up-europe/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/DSC_1187-2.jpeg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;AI isn’t waiting. And this week, neither is Europe.&lt;/p&gt;
&lt;p&gt;At London’s Olympia, under a ceiling of steel beams and enveloped by the thrum of startup pitches, it didn’t feel like the start of a conference — it felt like the start of something bigger.&lt;/p&gt;
&lt;p&gt;NVIDIA founder and CEO Jensen Huang joined U.K. Prime Minister Sir Keir Starmer to open London Tech Week, a moment that signaled a clear shift: what used to be the domain of ambitious technology startups is now national policy — backed by investments in people, platforms and partnerships.&lt;/p&gt;
&lt;p&gt;AI is transforming the entire ecosystem, everything from healthcare and manufacturing to scientific research, Huang told the audience. “I make this prediction – because of AI, every industry in the UK will be a tech industry,” Huang said.&lt;/p&gt;
&lt;p&gt;Starmer added that his team is looking at every single department in government to see how AI can be used.&lt;/p&gt;
&lt;p&gt;Starmer’s goal for the session was clear: to bring to life the real-world impact of the AI revolution and how AI is changing everyday lives for U.K. citizens.&lt;/p&gt;
&lt;p&gt;“The U.K. has one of the richest AI communities of anywhere on the planet, the deepest thinkers, the best universities… and the third largest AI capital investment of anywhere in the world,”&amp;nbsp; Huang said.&lt;/p&gt;
&lt;p&gt;“So the ability to build these AI supercomputers here in the U.K. will naturally attract more startups, it will naturally enable the rich ecosystem of researchers here to do their life’s work,” Huang added.&lt;/p&gt;
&lt;p&gt;To that end, NVIDIA will continue to invest in the U.K. “We’re going to start our AI lab here… we’re going to partner with the UK to upskill the ecosystem of developers into the world of AI,” Huang added.&lt;/p&gt;
&lt;p&gt;All of these investments will build on one another. “Infrastructure enables more research, more research, more breakthroughs, more companies,” Huang said. That flywheel will start taking off; it’s already quite large.”&lt;/p&gt;
&lt;h2&gt;UK on the Move: Momentum in Action&lt;/h2&gt;
&lt;p&gt;This wasn’t just a symbolic handshake. It marked the U.K.’s acceleration toward embedding AI at the core of its economic strategy. A major announcement from Prime Minister Starmer confirmed the U.K. will invest ~£1 billion in AI research compute by 2030, with investments commencing this year.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A national AI skills initiative supported by NVIDIA aims to train developers in advanced AI skills.&lt;/li&gt;
&lt;li&gt;A new NVIDIA AI Technology Center in the U.K. is launching to accelerate research in embodied AI, material science and earth system modeling.&lt;/li&gt;
&lt;li&gt;The U.K.’s Financial Conduct Authority is using NVIDIA tech to power its innovation sandbox for safe and secure AI experimentation.&lt;/li&gt;
&lt;li&gt;The U.K. government and NVIDIA also announced a new initiative to accelerate AI-native 6G research and deployment.&lt;/li&gt;
&lt;li&gt;And further cementing the U.K.’s compute power, Isambard AI, the U.K.’s fastest AI supercomputer powered by 5.5k GH200s, is set to be fully operational this summer.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“We need to showcase what we have,” Starmer said. “This is a two-way conversation” between the government and industry.&lt;/p&gt;
&lt;p&gt;Starmer underscored the U.K.’s “sovereign AI ambitions,” emphasizing that AI is not just about technology, but about codifying a nation’s culture, common sense and history.&lt;/p&gt;
&lt;p&gt;And the movement isn’t confined to the U.K. Across Europe, governments are no longer debating whether AI matters. Now the question in every capital isn’t why AI, it’s how soon can we deploy it at scale?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In Sweden, NVIDIA is working with Wallenberg Investments, AstraZeneca, Ericsson, Saab and SEB to build the country’s first national AI infrastructure, anchored by the NVIDIA Grace Blackwell platform.&lt;/li&gt;
&lt;li&gt;In Germany, the Leibniz Supercomputing Centre is building Blue Lion — a €250 million supercomputer based on the new NVIDIA Vera Rubin architecture, designed for real-time AI, simulation and science.&lt;/li&gt;
&lt;li&gt;In France, a joint venture between MGX, Bpifrance, Mistral AI and NVIDIA will establish Europe’s largest AI Campus in the Paris region, a 1.4 GW facility aiming to build sovereign and sustainable AI infrastructure for the continent.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“In the last 10 years, AI has advanced 1 million times,” Huang said. “The speed of change is incredible.”&lt;/p&gt;
&lt;p&gt;NVIDIA’s commitment to the U.K. is evident, with over 1,700 Inception members and 500 employees across four offices.&lt;/p&gt;
&lt;p&gt;NVIDIA is actively building the ‘AI factories of the future’ with leading U.K. companies.&lt;/p&gt;
&lt;p&gt;And it’s powering the next generation of startups and scale-ups, from Basecamp Research to Wayve.&lt;/p&gt;
&lt;h2&gt;What Comes Next: NVIDIA GTC Paris at VivaTech&lt;/h2&gt;
&lt;p&gt;Next, the story moves to Paris, where Jensen Huang will headline NVIDIA GTC Paris live from VivaTech.&lt;/p&gt;
&lt;p&gt;🗓️ June 11 | 11:00 a.m. CEST | Dôme de Paris&lt;br /&gt;🎟️ VivaTech or GTC Paris pass required to attend&lt;br /&gt;💻 Livestream available globally, free&lt;/p&gt;
&lt;p&gt;Expect news on NVIDIA Blackwell, sovereign AI initiatives, new regional partnerships and how European innovators are turning intent into infrastructure with NVIDIA.&lt;/p&gt;
&lt;h2&gt;One Week. One Story. One Start.&lt;/h2&gt;
&lt;p&gt;From Downing Street to the Dôme de Paris, this week reads less like a schedule and more like a strategy.&lt;/p&gt;
&lt;p&gt;This isn’t just a collection of conferences. It’s a continental shift — where Europe is aligning talent, policy and compute to lead in AI.&lt;/p&gt;
&lt;p&gt;This is just chapter one. But the story is already racing ahead.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/DSC_1187-2.jpeg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;AI isn’t waiting. And this week, neither is Europe.&lt;/p&gt;
&lt;p&gt;At London’s Olympia, under a ceiling of steel beams and enveloped by the thrum of startup pitches, it didn’t feel like the start of a conference — it felt like the start of something bigger.&lt;/p&gt;
&lt;p&gt;NVIDIA founder and CEO Jensen Huang joined U.K. Prime Minister Sir Keir Starmer to open London Tech Week, a moment that signaled a clear shift: what used to be the domain of ambitious technology startups is now national policy — backed by investments in people, platforms and partnerships.&lt;/p&gt;
&lt;p&gt;AI is transforming the entire ecosystem, everything from healthcare and manufacturing to scientific research, Huang told the audience. “I make this prediction – because of AI, every industry in the UK will be a tech industry,” Huang said.&lt;/p&gt;
&lt;p&gt;Starmer added that his team is looking at every single department in government to see how AI can be used.&lt;/p&gt;
&lt;p&gt;Starmer’s goal for the session was clear: to bring to life the real-world impact of the AI revolution and how AI is changing everyday lives for U.K. citizens.&lt;/p&gt;
&lt;p&gt;“The U.K. has one of the richest AI communities of anywhere on the planet, the deepest thinkers, the best universities… and the third largest AI capital investment of anywhere in the world,”&amp;nbsp; Huang said.&lt;/p&gt;
&lt;p&gt;“So the ability to build these AI supercomputers here in the U.K. will naturally attract more startups, it will naturally enable the rich ecosystem of researchers here to do their life’s work,” Huang added.&lt;/p&gt;
&lt;p&gt;To that end, NVIDIA will continue to invest in the U.K. “We’re going to start our AI lab here… we’re going to partner with the UK to upskill the ecosystem of developers into the world of AI,” Huang added.&lt;/p&gt;
&lt;p&gt;All of these investments will build on one another. “Infrastructure enables more research, more research, more breakthroughs, more companies,” Huang said. That flywheel will start taking off; it’s already quite large.”&lt;/p&gt;
&lt;h2&gt;UK on the Move: Momentum in Action&lt;/h2&gt;
&lt;p&gt;This wasn’t just a symbolic handshake. It marked the U.K.’s acceleration toward embedding AI at the core of its economic strategy. A major announcement from Prime Minister Starmer confirmed the U.K. will invest ~£1 billion in AI research compute by 2030, with investments commencing this year.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A national AI skills initiative supported by NVIDIA aims to train developers in advanced AI skills.&lt;/li&gt;
&lt;li&gt;A new NVIDIA AI Technology Center in the U.K. is launching to accelerate research in embodied AI, material science and earth system modeling.&lt;/li&gt;
&lt;li&gt;The U.K.’s Financial Conduct Authority is using NVIDIA tech to power its innovation sandbox for safe and secure AI experimentation.&lt;/li&gt;
&lt;li&gt;The U.K. government and NVIDIA also announced a new initiative to accelerate AI-native 6G research and deployment.&lt;/li&gt;
&lt;li&gt;And further cementing the U.K.’s compute power, Isambard AI, the U.K.’s fastest AI supercomputer powered by 5.5k GH200s, is set to be fully operational this summer.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“We need to showcase what we have,” Starmer said. “This is a two-way conversation” between the government and industry.&lt;/p&gt;
&lt;p&gt;Starmer underscored the U.K.’s “sovereign AI ambitions,” emphasizing that AI is not just about technology, but about codifying a nation’s culture, common sense and history.&lt;/p&gt;
&lt;p&gt;And the movement isn’t confined to the U.K. Across Europe, governments are no longer debating whether AI matters. Now the question in every capital isn’t why AI, it’s how soon can we deploy it at scale?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In Sweden, NVIDIA is working with Wallenberg Investments, AstraZeneca, Ericsson, Saab and SEB to build the country’s first national AI infrastructure, anchored by the NVIDIA Grace Blackwell platform.&lt;/li&gt;
&lt;li&gt;In Germany, the Leibniz Supercomputing Centre is building Blue Lion — a €250 million supercomputer based on the new NVIDIA Vera Rubin architecture, designed for real-time AI, simulation and science.&lt;/li&gt;
&lt;li&gt;In France, a joint venture between MGX, Bpifrance, Mistral AI and NVIDIA will establish Europe’s largest AI Campus in the Paris region, a 1.4 GW facility aiming to build sovereign and sustainable AI infrastructure for the continent.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“In the last 10 years, AI has advanced 1 million times,” Huang said. “The speed of change is incredible.”&lt;/p&gt;
&lt;p&gt;NVIDIA’s commitment to the U.K. is evident, with over 1,700 Inception members and 500 employees across four offices.&lt;/p&gt;
&lt;p&gt;NVIDIA is actively building the ‘AI factories of the future’ with leading U.K. companies.&lt;/p&gt;
&lt;p&gt;And it’s powering the next generation of startups and scale-ups, from Basecamp Research to Wayve.&lt;/p&gt;
&lt;h2&gt;What Comes Next: NVIDIA GTC Paris at VivaTech&lt;/h2&gt;
&lt;p&gt;Next, the story moves to Paris, where Jensen Huang will headline NVIDIA GTC Paris live from VivaTech.&lt;/p&gt;
&lt;p&gt;🗓️ June 11 | 11:00 a.m. CEST | Dôme de Paris&lt;br /&gt;🎟️ VivaTech or GTC Paris pass required to attend&lt;br /&gt;💻 Livestream available globally, free&lt;/p&gt;
&lt;p&gt;Expect news on NVIDIA Blackwell, sovereign AI initiatives, new regional partnerships and how European innovators are turning intent into infrastructure with NVIDIA.&lt;/p&gt;
&lt;h2&gt;One Week. One Story. One Start.&lt;/h2&gt;
&lt;p&gt;From Downing Street to the Dôme de Paris, this week reads less like a schedule and more like a strategy.&lt;/p&gt;
&lt;p&gt;This isn’t just a collection of conferences. It’s a continental shift — where Europe is aligning talent, policy and compute to lead in AI.&lt;/p&gt;
&lt;p&gt;This is just chapter one. But the story is already racing ahead.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/ai-lights-up-europe/</guid><pubDate>Mon, 09 Jun 2025 10:25:39 +0000</pubDate></item><item><title>The Download: an inspiring toy robot arm, and why AM radio matters (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/09/1118186/the-download-an-inspiring-toy-robot-arm-and-why-am-radio-matters/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;How a 1980s toy robot arm inspired modern robotics&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;em&gt;—Jon Keegan&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;As a child of an electronic engineer, I spent a lot of time in our local Radio Shack as a kid. While my dad was locating capacitors and resistors, I was in the toy section. It was there, in 1984, that I discovered the best toy of my childhood: the Armatron robotic arm.&lt;/p&gt; 
 &lt;p&gt;Described as a “robot-like arm to aid young masterminds in scientific and laboratory experiments,” it was a legit robotic arm. And the bold look and function of Armatron made quite an impression on many young kids who would one day have a career in robotics. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;If you’re interested in the future of robots, why not check out:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;+ &lt;strong&gt;Will we ever trust robots?&lt;/strong&gt; If most robots still need remote human operators to be safe and effective, why should we welcome them into our homes? Read the full story.&lt;/p&gt; 
 &lt;p&gt;+ &lt;strong&gt;When you might start speaking to robots&lt;/strong&gt;. Google is only the latest to fuse large language models with robots. Here’s why the trend has big implications.&lt;/p&gt;&lt;p&gt;+ How AI models let robots carry out tasks in unfamiliar environments. Read the full story.&lt;/p&gt;&lt;p&gt;+ &lt;strong&gt;China’s EV giants are betting big on humanoid robots&lt;/strong&gt;. Technical know-how and existing supply chains give Chinese electric-vehicle makers a significant head start in the sector. Read the full story.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Why we still need AM radio&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;The most reliable way to keep us informed in times of disaster is being threatened. Check out Ariel Aberg-Riger’s beautiful visual story illustrating AM radio’s importance in uncertain times.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;strong&gt;Both of these stories are from the most recent edition of our print magazine, which is all about how &lt;/strong&gt;&lt;strong&gt;technology is changing creativity&lt;/strong&gt;&lt;strong&gt;. &lt;/strong&gt;&lt;strong&gt;Subscribe now&lt;/strong&gt;&lt;strong&gt; to get future copies before they land.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;1 Protestors set Waymo robotaxis alight in Los Angeles&lt;/strong&gt;&lt;br /&gt;The groups clashed with police over the Trump administration’s immigration raids. (LA Times $)&lt;br /&gt;+ &lt;em&gt;Much of the technology that fuels deportation orders is error-ridden. &lt;/em&gt;(Slate $)&lt;br /&gt;+ &lt;em&gt;Immigrants are using a swathe of new apps to stay ahead of deportation. &lt;/em&gt;(Rest of World)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 What’s next for Elon Musk and Donald Trump&lt;/strong&gt;&lt;br /&gt;A full breakdown in relations could be much worse for Musk in the long run. (NY Mag $)&lt;br /&gt;+ &lt;em&gt;Trump’s backers are rapidly turning on Musk, too. &lt;/em&gt;(New Yorker $)&lt;br /&gt;+ &lt;em&gt;The biggest winner from their fall out? Jeff Bezos. &lt;/em&gt;(The Information $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 DOGE used an inaccurate AI tool to terminate Veteran Affairs contacts&lt;/strong&gt;&lt;br /&gt;Its code frequently produced glaring mistakes. (ProPublica)&lt;br /&gt;+ &lt;em&gt;Undeterred, the department is on a hiring spree. &lt;/em&gt;(Wired $)&lt;br /&gt;+ &lt;em&gt;Can AI help DOGE slash government budgets? It’s complex. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 Europe’s shrinking forests could cause it to miss net-zero targets&lt;/strong&gt;&lt;br /&gt;Its trees aren’t soaking up as much carbon as they used to. (New Scientist $)&lt;br /&gt;+ &lt;em&gt;Inside the controversial tree farms powering Apple’s carbon neutral goal. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;5 OpenAI wants to embed ChatGPT into college campuses&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;The ultimate goal? A personalized AI account for every student. (NYT $)&lt;br /&gt;+ &lt;em&gt;Meanwhile, other universities are experimenting with tech-free classes. &lt;/em&gt;(The Atlantic $)&lt;br /&gt;+ &lt;em&gt;ChatGPT is going to change education, not destroy it. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 Chinese regulators are pumping the brakes on self-driving cars&lt;/strong&gt;&lt;br /&gt;They’re developing a new framework to assess the safety of autonomous features. (FT $)&lt;br /&gt;+ &lt;em&gt;The country’s robotaxis are rapidly catching up with the west. &lt;/em&gt;(Rest of World)&lt;br /&gt;+ &lt;em&gt;How China is regulating robotaxis. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Desalination is finally becoming a reality&lt;br /&gt;&lt;/strong&gt;Removing salt from seawater is one way to combat water scarcity. (WSJ $)&lt;br /&gt;+ &lt;em&gt;If you can make it through tons of plastic, that is. &lt;/em&gt;(The Atlantic $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 We’re getting better at fighting cancer&lt;br /&gt;&lt;/strong&gt;Deaths from the disease in the US have dropped by a third since 1991. (Vox)&lt;br /&gt;+ &lt;em&gt;Why it’s so hard to use AI to diagnose cancer. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;9 Teenage TikTokers’ skin regimes offer virtually no benefit&lt;br /&gt;&lt;/strong&gt;And could even be potentially harmful. (The Guardian)&lt;br /&gt;+ &lt;em&gt;The fight for “Instagram face” &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 Tech’s layoff groups are providing much-needed support&lt;/strong&gt;&lt;br /&gt;Workers who have been let go by their employers are forming little communities. (Insider $)&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p class="has-large-font-size"&gt;&lt;strong&gt;“Every tech company is doing similar things but we were open about it.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Luis von Ahn, chief executive of the language-learning app Duolingo, tells the Financial Times that his company is far from the only one adopting an AI-first strategy.&amp;nbsp;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; 
 &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" src="https://wp.technologyreview.com/wp-content/uploads/2024/08/EscapingSpotifyAlgorithm_web.jpg?fit=1456,818" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;How to break free of Spotify’s algorithm&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Since the heyday of radio, the branding of sound has evolved from broad genres like rock and hip-hop to “paranormal dark cabaret afternoon” and “synth space,” and streaming has become the default.&lt;/p&gt;&lt;p&gt;Meanwhile, the ritual of discovering something new is now neatly packaged in a 30-song playlist. The only rule in music streaming is personalization.&lt;/p&gt;&lt;p&gt;What we’ve gained in convenience, we’ve lost in curiosity. But it doesn’t have to be this way. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Tiffany Ng&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Happy birthday to Michael J Fox, who turns 64 today!&lt;br /&gt;+ Whenever you need to play the world’s smallest violin, these scientists can help you out 🎻&lt;br /&gt;+ An early JMW Turner oil painting has been rediscovered.&lt;br /&gt;+ Watching robots attempt to kickbox is pretty amusing.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;How a 1980s toy robot arm inspired modern robotics&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;em&gt;—Jon Keegan&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;As a child of an electronic engineer, I spent a lot of time in our local Radio Shack as a kid. While my dad was locating capacitors and resistors, I was in the toy section. It was there, in 1984, that I discovered the best toy of my childhood: the Armatron robotic arm.&lt;/p&gt; 
 &lt;p&gt;Described as a “robot-like arm to aid young masterminds in scientific and laboratory experiments,” it was a legit robotic arm. And the bold look and function of Armatron made quite an impression on many young kids who would one day have a career in robotics. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;If you’re interested in the future of robots, why not check out:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;+ &lt;strong&gt;Will we ever trust robots?&lt;/strong&gt; If most robots still need remote human operators to be safe and effective, why should we welcome them into our homes? Read the full story.&lt;/p&gt; 
 &lt;p&gt;+ &lt;strong&gt;When you might start speaking to robots&lt;/strong&gt;. Google is only the latest to fuse large language models with robots. Here’s why the trend has big implications.&lt;/p&gt;&lt;p&gt;+ How AI models let robots carry out tasks in unfamiliar environments. Read the full story.&lt;/p&gt;&lt;p&gt;+ &lt;strong&gt;China’s EV giants are betting big on humanoid robots&lt;/strong&gt;. Technical know-how and existing supply chains give Chinese electric-vehicle makers a significant head start in the sector. Read the full story.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Why we still need AM radio&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;The most reliable way to keep us informed in times of disaster is being threatened. Check out Ariel Aberg-Riger’s beautiful visual story illustrating AM radio’s importance in uncertain times.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;strong&gt;Both of these stories are from the most recent edition of our print magazine, which is all about how &lt;/strong&gt;&lt;strong&gt;technology is changing creativity&lt;/strong&gt;&lt;strong&gt;. &lt;/strong&gt;&lt;strong&gt;Subscribe now&lt;/strong&gt;&lt;strong&gt; to get future copies before they land.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;1 Protestors set Waymo robotaxis alight in Los Angeles&lt;/strong&gt;&lt;br /&gt;The groups clashed with police over the Trump administration’s immigration raids. (LA Times $)&lt;br /&gt;+ &lt;em&gt;Much of the technology that fuels deportation orders is error-ridden. &lt;/em&gt;(Slate $)&lt;br /&gt;+ &lt;em&gt;Immigrants are using a swathe of new apps to stay ahead of deportation. &lt;/em&gt;(Rest of World)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 What’s next for Elon Musk and Donald Trump&lt;/strong&gt;&lt;br /&gt;A full breakdown in relations could be much worse for Musk in the long run. (NY Mag $)&lt;br /&gt;+ &lt;em&gt;Trump’s backers are rapidly turning on Musk, too. &lt;/em&gt;(New Yorker $)&lt;br /&gt;+ &lt;em&gt;The biggest winner from their fall out? Jeff Bezos. &lt;/em&gt;(The Information $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 DOGE used an inaccurate AI tool to terminate Veteran Affairs contacts&lt;/strong&gt;&lt;br /&gt;Its code frequently produced glaring mistakes. (ProPublica)&lt;br /&gt;+ &lt;em&gt;Undeterred, the department is on a hiring spree. &lt;/em&gt;(Wired $)&lt;br /&gt;+ &lt;em&gt;Can AI help DOGE slash government budgets? It’s complex. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 Europe’s shrinking forests could cause it to miss net-zero targets&lt;/strong&gt;&lt;br /&gt;Its trees aren’t soaking up as much carbon as they used to. (New Scientist $)&lt;br /&gt;+ &lt;em&gt;Inside the controversial tree farms powering Apple’s carbon neutral goal. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;5 OpenAI wants to embed ChatGPT into college campuses&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;The ultimate goal? A personalized AI account for every student. (NYT $)&lt;br /&gt;+ &lt;em&gt;Meanwhile, other universities are experimenting with tech-free classes. &lt;/em&gt;(The Atlantic $)&lt;br /&gt;+ &lt;em&gt;ChatGPT is going to change education, not destroy it. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 Chinese regulators are pumping the brakes on self-driving cars&lt;/strong&gt;&lt;br /&gt;They’re developing a new framework to assess the safety of autonomous features. (FT $)&lt;br /&gt;+ &lt;em&gt;The country’s robotaxis are rapidly catching up with the west. &lt;/em&gt;(Rest of World)&lt;br /&gt;+ &lt;em&gt;How China is regulating robotaxis. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Desalination is finally becoming a reality&lt;br /&gt;&lt;/strong&gt;Removing salt from seawater is one way to combat water scarcity. (WSJ $)&lt;br /&gt;+ &lt;em&gt;If you can make it through tons of plastic, that is. &lt;/em&gt;(The Atlantic $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 We’re getting better at fighting cancer&lt;br /&gt;&lt;/strong&gt;Deaths from the disease in the US have dropped by a third since 1991. (Vox)&lt;br /&gt;+ &lt;em&gt;Why it’s so hard to use AI to diagnose cancer. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;9 Teenage TikTokers’ skin regimes offer virtually no benefit&lt;br /&gt;&lt;/strong&gt;And could even be potentially harmful. (The Guardian)&lt;br /&gt;+ &lt;em&gt;The fight for “Instagram face” &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 Tech’s layoff groups are providing much-needed support&lt;/strong&gt;&lt;br /&gt;Workers who have been let go by their employers are forming little communities. (Insider $)&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p class="has-large-font-size"&gt;&lt;strong&gt;“Every tech company is doing similar things but we were open about it.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Luis von Ahn, chief executive of the language-learning app Duolingo, tells the Financial Times that his company is far from the only one adopting an AI-first strategy.&amp;nbsp;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; 
 &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" src="https://wp.technologyreview.com/wp-content/uploads/2024/08/EscapingSpotifyAlgorithm_web.jpg?fit=1456,818" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;How to break free of Spotify’s algorithm&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Since the heyday of radio, the branding of sound has evolved from broad genres like rock and hip-hop to “paranormal dark cabaret afternoon” and “synth space,” and streaming has become the default.&lt;/p&gt;&lt;p&gt;Meanwhile, the ritual of discovering something new is now neatly packaged in a 30-song playlist. The only rule in music streaming is personalization.&lt;/p&gt;&lt;p&gt;What we’ve gained in convenience, we’ve lost in curiosity. But it doesn’t have to be this way. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Tiffany Ng&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Happy birthday to Michael J Fox, who turns 64 today!&lt;br /&gt;+ Whenever you need to play the world’s smallest violin, these scientists can help you out 🎻&lt;br /&gt;+ An early JMW Turner oil painting has been rediscovered.&lt;br /&gt;+ Watching robots attempt to kickbox is pretty amusing.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/09/1118186/the-download-an-inspiring-toy-robot-arm-and-why-am-radio-matters/</guid><pubDate>Mon, 09 Jun 2025 12:10:00 +0000</pubDate></item></channel></rss>