<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 10 Oct 2025 18:30:02 +0000</lastBuildDate><item><title>This test could reveal the health of your immune system (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/10/10/1125559/test-health-immune-system/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/Spot_1A_color_checkup.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Attentive readers might have noticed my absence over the last couple of weeks. I’ve been trying to recover from a bout of illness.&lt;/p&gt;  &lt;p&gt;It got me thinking about the immune system, and how little I know about my own immune health. The vast array of cells, proteins, and biomolecules that works to defend us from disease is mind-bogglingly complicated. Immunologists are still getting to grips with how it all works.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;Those of us who aren’t immunologists are even more in the dark. I had my flu jab last week and have no idea how my immune system responded. Will it protect me from the flu virus this winter? Is it “stressed” from whatever other bugs it has encountered in the last few months? And since my husband had his shot at the same time, I can’t help wondering how our responses will compare.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;So I was intrigued to hear about a new test that is being developed to measure immune health. One that even gives you a score.&lt;/p&gt; 
 &lt;p&gt;Writer David Ewing Duncan hoped that the test would reveal more about his health than any other he’d ever taken. He described the experience in a piece published jointly by &lt;em&gt;MIT Technology Review&lt;/em&gt; and &lt;em&gt;Aventine&lt;/em&gt;.&lt;/p&gt;  &lt;p&gt;The test David took was developed by John Tsang at Yale University and his colleagues. The team wanted to work out a way of measuring how healthy a person’s immune system might be.&lt;/p&gt; 
 &lt;p&gt;It’s a difficult thing to do, for several reasons. First, there’s the definition of “healthy.” I find it’s a loose concept that becomes more complicated the more you think about it. Yes, we all have a general sense of what it means to be in good health. But is it just the absence of disease? Is it about resilience? Does it have something to do with withstanding the impact of aging?&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;Tsang and his colleagues wanted to measure “deviation from health.” They looked at blood samples from 228 people who had immune diseases that were caused by single-gene mutations, as well as 42 other people who were free from disease. All those individuals could be considered along a health spectrum.&lt;/p&gt;  &lt;p&gt;Another major challenge lies in trying to capture the complexity of the immune system, which involves hundreds of proteins and cells interacting in various ways. (Side note: Last year, &lt;em&gt;MIT Technology Review&lt;/em&gt; recognized Ang Cui at Harvard University as one of our Innovators under 35 for her attempts to make sense of it all using machine learning. She created the Immune Dictionary to describe how hundreds of proteins affect immune cells—something she likens to a “periodic table” for the immune system.)&lt;/p&gt;  &lt;p&gt;Tsang and his colleagues tackled this by running a series of tests on those blood samples. The vast scope of these tests is what sets them apart from the blood tests you might get during a visit to the doctor. The team looked at how genes were expressed by cells in the blood. They measured a range of immune cells and more than 1,300 proteins.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt;&lt;p&gt;The team members used machine learning to find correlations between these measurements and health, allowing them to create an immune health score for each of the volunteers. They call it the immune health metric, or IHM.&lt;/p&gt;  &lt;p&gt;When they used this approach to find the immune scores of people who had already volunteered in other studies, they found that the IHM seemed to align with other measures of health, such as how people respond to diseases, treatments, and vaccines. The study was published in the journal &lt;em&gt;Nature Medicine&lt;/em&gt; last year.&lt;/p&gt;  &lt;p&gt;The researchers behind it hope that a test like this could one day help identify people who are at risk of cancer and other diseases, or explain why some people respond differently to treatments or immunizations.&lt;/p&gt;  &lt;p&gt;But the test isn’t ready for clinical use. If, like me, you’re finding yourself curious to know your own IHM, you’ll just have to wait.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article first appeared in The Checkup,&amp;nbsp;&lt;/em&gt;MIT Technology Review’s&lt;em&gt;&amp;nbsp;weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,&amp;nbsp;&lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/Spot_1A_color_checkup.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Attentive readers might have noticed my absence over the last couple of weeks. I’ve been trying to recover from a bout of illness.&lt;/p&gt;  &lt;p&gt;It got me thinking about the immune system, and how little I know about my own immune health. The vast array of cells, proteins, and biomolecules that works to defend us from disease is mind-bogglingly complicated. Immunologists are still getting to grips with how it all works.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;Those of us who aren’t immunologists are even more in the dark. I had my flu jab last week and have no idea how my immune system responded. Will it protect me from the flu virus this winter? Is it “stressed” from whatever other bugs it has encountered in the last few months? And since my husband had his shot at the same time, I can’t help wondering how our responses will compare.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;So I was intrigued to hear about a new test that is being developed to measure immune health. One that even gives you a score.&lt;/p&gt; 
 &lt;p&gt;Writer David Ewing Duncan hoped that the test would reveal more about his health than any other he’d ever taken. He described the experience in a piece published jointly by &lt;em&gt;MIT Technology Review&lt;/em&gt; and &lt;em&gt;Aventine&lt;/em&gt;.&lt;/p&gt;  &lt;p&gt;The test David took was developed by John Tsang at Yale University and his colleagues. The team wanted to work out a way of measuring how healthy a person’s immune system might be.&lt;/p&gt; 
 &lt;p&gt;It’s a difficult thing to do, for several reasons. First, there’s the definition of “healthy.” I find it’s a loose concept that becomes more complicated the more you think about it. Yes, we all have a general sense of what it means to be in good health. But is it just the absence of disease? Is it about resilience? Does it have something to do with withstanding the impact of aging?&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;Tsang and his colleagues wanted to measure “deviation from health.” They looked at blood samples from 228 people who had immune diseases that were caused by single-gene mutations, as well as 42 other people who were free from disease. All those individuals could be considered along a health spectrum.&lt;/p&gt;  &lt;p&gt;Another major challenge lies in trying to capture the complexity of the immune system, which involves hundreds of proteins and cells interacting in various ways. (Side note: Last year, &lt;em&gt;MIT Technology Review&lt;/em&gt; recognized Ang Cui at Harvard University as one of our Innovators under 35 for her attempts to make sense of it all using machine learning. She created the Immune Dictionary to describe how hundreds of proteins affect immune cells—something she likens to a “periodic table” for the immune system.)&lt;/p&gt;  &lt;p&gt;Tsang and his colleagues tackled this by running a series of tests on those blood samples. The vast scope of these tests is what sets them apart from the blood tests you might get during a visit to the doctor. The team looked at how genes were expressed by cells in the blood. They measured a range of immune cells and more than 1,300 proteins.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt;&lt;p&gt;The team members used machine learning to find correlations between these measurements and health, allowing them to create an immune health score for each of the volunteers. They call it the immune health metric, or IHM.&lt;/p&gt;  &lt;p&gt;When they used this approach to find the immune scores of people who had already volunteered in other studies, they found that the IHM seemed to align with other measures of health, such as how people respond to diseases, treatments, and vaccines. The study was published in the journal &lt;em&gt;Nature Medicine&lt;/em&gt; last year.&lt;/p&gt;  &lt;p&gt;The researchers behind it hope that a test like this could one day help identify people who are at risk of cancer and other diseases, or explain why some people respond differently to treatments or immunizations.&lt;/p&gt;  &lt;p&gt;But the test isn’t ready for clinical use. If, like me, you’re finding yourself curious to know your own IHM, you’ll just have to wait.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article first appeared in The Checkup,&amp;nbsp;&lt;/em&gt;MIT Technology Review’s&lt;em&gt;&amp;nbsp;weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,&amp;nbsp;&lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/10/10/1125559/test-health-immune-system/</guid><pubDate>Fri, 10 Oct 2025 09:00:00 +0000</pubDate></item><item><title>How do our bodies remember? (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/10/10/1124963/muscles-remember-explained/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/Thumbnail_MIT-Muscle-Final.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;MIT Technology Review&lt;/em&gt;&amp;nbsp;Explains: Let our writers untangle the complex, messy world of technology to help you understand what’s coming next. You can read&amp;nbsp;&lt;/strong&gt;&lt;strong&gt;more from the series here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;“Like riding a bike” is shorthand for the remarkable way that our bodies remember how to move. Most of the time when we talk about muscle memory, we’re not talking about the muscles themselves but about the memory of a coordinated movement pattern that lives in the motor neurons, which control our muscles.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Yet in recent years, scientists have discovered that &lt;em&gt;our muscles themselves&lt;/em&gt; have a memory for movement and exercise.&lt;/p&gt;  &lt;p&gt;When we move a muscle, the movement may appear to begin and end, but all these little changes are actually continuing to happen inside our muscle cells. And the more we move, as with riding a bike or other kinds of exercise, the more those cells begin to make a memory of that exercise.&lt;/p&gt; 
 &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;When we move a muscle, the movement may appear to begin and end, but all these little changes are actually continuing to happen inside our muscle cells.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;We all know from experience that a muscle gets bigger and stronger with repeated work. As the pioneering muscle scientist Adam Sharples—a professor at the Norwegian School of Sport Sciences in Oslo and a former professional rugby player in the UK—explained to me, skeletal muscle cells are unique in the human body: They’re long and skinny, like fibers, and have multiple nuclei. The fibers grow larger not by dividing but by recruiting muscle satellite cells—stem cells specific to muscle that are dormant until activated in response to stress or injury—to contribute their own nuclei and support muscle growth and regeneration. Those nuclei often stick around for a while in the muscle fibers, even after periods of inactivity, and there is evidence that they may help accelerate the return to growth once you start training again.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Sharples’s research focuses on what’s called epigenetic muscle memory.&lt;strong&gt; “&lt;/strong&gt;Epigenetic” refers to changes in gene expression that are caused by behavior and environment—the genes themselves don’t change, but the way they work does. In general, exercise switches on genes that help make muscles grow more easily. When you lift weights, for example, small molecules called methyl groups detach from the outside of certain genes, making them more likely to turn on and produce proteins that affect muscle growth (also known as hypertrophy). Those changes persist; if you start lifting weights again, you’ll add muscle mass more quickly than before.&lt;/p&gt; 
 &lt;p&gt;In 2018, Sharples’s muscle lab was the first to show that human skeletal muscle has an epigenetic memory of muscle growth after exercise: Muscle cells are primed to respond more rapidly to exercise in the future, even after a monthslong (and maybe even yearslong) pause. In other words: Your muscles remember how to do it.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;Subsequent studies from Sharples and others have replicated similar findings in mice and older humans, offering further supporting evidence of epigenetic muscle memory across species and into later life. Even aging muscles have the capacity to remember when you work out.&lt;/p&gt;  &lt;p&gt;At the same time, Sharples points to intriguing new evidence that muscles also remember periods of atrophy—and that young and old muscles remember this &lt;em&gt;differently&lt;/em&gt;. While young human muscle seems to have what he calls a “positive” memory of wasting—“in that it recovers well after a first period of atrophy and doesn’t experience greater loss in a repeated atrophy period,” he explains—aged muscle in rats seems to have a more pronounced “negative” memory of atrophy, in which it appears “more susceptible to greater loss and a more exaggerated molecular response when muscle wasting is repeated.” Basically, young muscle tends to bounce back from periods of muscle loss—“ignoring” it, in a sense—while older muscle is more sensitive to it and might be more susceptible to further loss in the future.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Illness can also lead to this kind of “negative” muscle memory; in a study of breast cancer survivors more than a decade after diagnosis and treatment, participants showed an epigenetic muscle profile of people much older than their chronological age. But get this: After five months of aerobic exercise training, participants were able to reset the epigenetic profile of their muscle back toward that of muscle seen in an age-matched control group of healthy women. &amp;nbsp;&lt;/p&gt;  &lt;p&gt;What this shows is that “positive” muscle memories can help counteract “negative” ones. The takeaway? Your muscles have their own kind of intelligence. The more you use them, the more they can harness it to become a lasting beneficial resource for your body in the future.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Bonnie Tsui is the author of&lt;/em&gt; On Muscle: The Stuff That Moves Us and Why It Matters &lt;em&gt;(Algonquin Books, 2025)&lt;/em&gt;.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/Thumbnail_MIT-Muscle-Final.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;MIT Technology Review&lt;/em&gt;&amp;nbsp;Explains: Let our writers untangle the complex, messy world of technology to help you understand what’s coming next. You can read&amp;nbsp;&lt;/strong&gt;&lt;strong&gt;more from the series here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;“Like riding a bike” is shorthand for the remarkable way that our bodies remember how to move. Most of the time when we talk about muscle memory, we’re not talking about the muscles themselves but about the memory of a coordinated movement pattern that lives in the motor neurons, which control our muscles.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Yet in recent years, scientists have discovered that &lt;em&gt;our muscles themselves&lt;/em&gt; have a memory for movement and exercise.&lt;/p&gt;  &lt;p&gt;When we move a muscle, the movement may appear to begin and end, but all these little changes are actually continuing to happen inside our muscle cells. And the more we move, as with riding a bike or other kinds of exercise, the more those cells begin to make a memory of that exercise.&lt;/p&gt; 
 &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;When we move a muscle, the movement may appear to begin and end, but all these little changes are actually continuing to happen inside our muscle cells.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;We all know from experience that a muscle gets bigger and stronger with repeated work. As the pioneering muscle scientist Adam Sharples—a professor at the Norwegian School of Sport Sciences in Oslo and a former professional rugby player in the UK—explained to me, skeletal muscle cells are unique in the human body: They’re long and skinny, like fibers, and have multiple nuclei. The fibers grow larger not by dividing but by recruiting muscle satellite cells—stem cells specific to muscle that are dormant until activated in response to stress or injury—to contribute their own nuclei and support muscle growth and regeneration. Those nuclei often stick around for a while in the muscle fibers, even after periods of inactivity, and there is evidence that they may help accelerate the return to growth once you start training again.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Sharples’s research focuses on what’s called epigenetic muscle memory.&lt;strong&gt; “&lt;/strong&gt;Epigenetic” refers to changes in gene expression that are caused by behavior and environment—the genes themselves don’t change, but the way they work does. In general, exercise switches on genes that help make muscles grow more easily. When you lift weights, for example, small molecules called methyl groups detach from the outside of certain genes, making them more likely to turn on and produce proteins that affect muscle growth (also known as hypertrophy). Those changes persist; if you start lifting weights again, you’ll add muscle mass more quickly than before.&lt;/p&gt; 
 &lt;p&gt;In 2018, Sharples’s muscle lab was the first to show that human skeletal muscle has an epigenetic memory of muscle growth after exercise: Muscle cells are primed to respond more rapidly to exercise in the future, even after a monthslong (and maybe even yearslong) pause. In other words: Your muscles remember how to do it.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;Subsequent studies from Sharples and others have replicated similar findings in mice and older humans, offering further supporting evidence of epigenetic muscle memory across species and into later life. Even aging muscles have the capacity to remember when you work out.&lt;/p&gt;  &lt;p&gt;At the same time, Sharples points to intriguing new evidence that muscles also remember periods of atrophy—and that young and old muscles remember this &lt;em&gt;differently&lt;/em&gt;. While young human muscle seems to have what he calls a “positive” memory of wasting—“in that it recovers well after a first period of atrophy and doesn’t experience greater loss in a repeated atrophy period,” he explains—aged muscle in rats seems to have a more pronounced “negative” memory of atrophy, in which it appears “more susceptible to greater loss and a more exaggerated molecular response when muscle wasting is repeated.” Basically, young muscle tends to bounce back from periods of muscle loss—“ignoring” it, in a sense—while older muscle is more sensitive to it and might be more susceptible to further loss in the future.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Illness can also lead to this kind of “negative” muscle memory; in a study of breast cancer survivors more than a decade after diagnosis and treatment, participants showed an epigenetic muscle profile of people much older than their chronological age. But get this: After five months of aerobic exercise training, participants were able to reset the epigenetic profile of their muscle back toward that of muscle seen in an age-matched control group of healthy women. &amp;nbsp;&lt;/p&gt;  &lt;p&gt;What this shows is that “positive” muscle memories can help counteract “negative” ones. The takeaway? Your muscles have their own kind of intelligence. The more you use them, the more they can harness it to become a lasting beneficial resource for your body in the future.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Bonnie Tsui is the author of&lt;/em&gt; On Muscle: The Stuff That Moves Us and Why It Matters &lt;em&gt;(Algonquin Books, 2025)&lt;/em&gt;.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/10/10/1124963/muscles-remember-explained/</guid><pubDate>Fri, 10 Oct 2025 10:00:00 +0000</pubDate></item><item><title>“Like putting on glasses for the first time”—how AI improves earthquake detection (AI – Ars Technica)</title><link>https://arstechnica.com/science/2025/10/like-putting-on-glasses-for-the-first-time-how-ai-improves-earthquake-detection/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        AI is “comically good” at detecting small earthquakes—here’s why that matters.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/earthquake-detection-640x360.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/earthquake-detection-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson | Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;On January 1, 2008, at 1:59 am in Calipatria, California, an earthquake happened. You haven’t heard of this earthquake; even if you had been living in Calipatria, you wouldn’t have felt anything. It was magnitude -0.53, about the same amount of shaking as a truck passing by. Still, this earthquake is notable, not because it was large but because it was small—and yet we know about it.&lt;/p&gt;
&lt;p&gt;Over the past seven years, AI tools based on computer imaging have almost completely automated one of the fundamental tasks of seismology: detecting earthquakes. What used to be the task of human analysts—and later, simpler computer programs—can now be done automatically and quickly by machine-learning tools.&lt;/p&gt;
&lt;p&gt;These machine-learning tools can detect smaller earthquakes than human analysts, especially in noisy environments like cities. Earthquakes give valuable information about the composition of the Earth and what hazards might occur in the future.&lt;/p&gt;
&lt;p&gt;“In the best-case scenario, when you adopt these new techniques, even on the same old data, it’s kind of like putting on glasses for the first time, and you can see the leaves on the trees,” said Kyle Bradley, co-author of the&amp;nbsp;Earthquake Insights newsletter.&lt;/p&gt;
&lt;p&gt;I talked with several earthquake scientists, and they all agreed that machine-learning methods have replaced humans for the better in these specific tasks.&lt;/p&gt;
&lt;p&gt;“It’s really remarkable,” Judith Hubbard, a Cornell University professor and Bradley’s co-author, told me.&lt;/p&gt;
&lt;p&gt;Less certain is what comes next. Earthquake detection is a fundamental part of seismology, but there are many other data processing tasks that have yet to be disrupted. The biggest potential impacts, all the way to earthquake forecasting, haven’t materialized yet.&lt;/p&gt;
&lt;p&gt;“It really was a revolution,” said Joe Byrnes, a professor at the University of Texas at Dallas. “But the revolution is ongoing.”&lt;/p&gt;
&lt;h2 class="header-anchor-post"&gt;What do seismologists do?&lt;/h2&gt;
&lt;p&gt;When an earthquake happens in one place, the shaking passes through the ground, similar to how sound waves pass through the air. In both cases, it’s possible to draw inferences about the materials the waves pass through.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Imagine tapping a wall to figure out if it’s hollow. Because a solid wall vibrates differently than a hollow wall, you can figure out the structure by sound.&lt;/p&gt;
&lt;p&gt;With earthquakes, this same principle holds. Seismic waves pass through different materials (rock, oil, magma, etc.) differently, and scientists use these vibrations to image the Earth’s interior.&lt;/p&gt;
&lt;p&gt;The main tool that scientists traditionally use is a&amp;nbsp;&lt;em&gt;seismometer&lt;/em&gt;. These record the movement of the Earth in three directions: up–down, north–south, and east–west. If an earthquake happens, seismometers can measure the shaking in that particular location.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2120119 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="768" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/Kinemetrics_seismograph.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      An old-fashioned physical seismometer. Today, seismometers record data digitally.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Yamaguchi先生 on&amp;nbsp;Wikimedia CC BY-SA 3.0

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Scientists then process raw seismometer information to identify earthquakes.&lt;/p&gt;
&lt;p&gt;Earthquakes produce multiple types of shaking, which travel at different speeds. Two types, Primary (P) waves and Secondary (S) waves are particularly important, and scientists like to identify the start of each of these phases.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2 class="header-anchor-post"&gt;Finding quakes before machine learning&lt;/h2&gt;
&lt;p&gt;Before good algorithms, earthquake cataloging had to happen by hand. Byrnes said that “traditionally, something like the lab at the United States Geological Survey would have an army of mostly undergraduate students or interns looking at seismograms.”&lt;/p&gt;
&lt;p&gt;However, there are only so many earthquakes you can find and classify manually. Creating algorithms to effectively find and process earthquakes has long been a priority in the field—especially since the arrival of computers in the early 1950s.&lt;/p&gt;
&lt;p&gt;“The field of seismology historically has always advanced as computing has advanced,” Bradley told me.&lt;/p&gt;
&lt;p&gt;There’s a big challenge with traditional algorithms, though: They can’t easily find smaller quakes, especially in noisy environments.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2120121 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="480" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/4730ac1e-6fae-4249-a6b6-653fc65aae6b_640x480.webp" width="640" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Composite seismogram of common events. Note how each event has a slightly different shape.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          EarthScope Consortium CC BY 4.0

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;As we see in the seismogram above, many different events can cause seismic signals. If a method is too sensitive, it risks falsely detecting events as earthquakes. The problem is especially bad in cities, where the constant hum of traffic and buildings can drown out small earthquakes.&lt;/p&gt;
&lt;p&gt;However, earthquakes have a characteristic “shape.” The magnitude 7.7 earthquake above looks quite different from the helicopter landing, for instance.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;So one idea scientists had was to make templates from human-labeled datasets. If a new waveform correlates closely with an existing template, it’s almost certainly an earthquake.&lt;/p&gt;
&lt;p&gt;Template matching works very well if you have enough human-labeled examples. In 2019, Zach Ross’ lab at Caltech used template matching to find 10 times as many earthquakes in Southern California as had previously been known, including the earthquake at the start of this story. Almost all of the new 1.6 million quakes they found were very small, magnitude 1 and below.&lt;/p&gt;
&lt;p&gt;If you don’t have an extensive pre-existing dataset of templates, however, you can’t easily apply template matching. That isn’t a problem in Southern California—which already had a basically complete record of earthquakes down to magnitude 1.7—but it’s a challenge elsewhere.&lt;/p&gt;
&lt;p&gt;Also, template matching is computationally expensive. Creating a Southern California quake dataset using template matching took 200 Nvidia P100 GPUs running for days on end.&lt;/p&gt;
&lt;p&gt;There had to be a better way.&lt;/p&gt;
&lt;h2 class="header-anchor-post"&gt;Breaking down the Earthquake Transformer&lt;/h2&gt;
&lt;p&gt;AI detection models solve all of these problems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p class="sc-1o72qur-1 hDESMK"&gt;They are faster than template matching.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p class="sc-1o72qur-1 hDESMK"&gt;Because AI detection models are very small (around 350,000 parameters compared to billions in LLMs like GPT4.0), they can be run on consumer CPUs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p class="sc-1o72qur-1 hDESMK"&gt;AI models generalize well to regions not represented in the original dataset.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As an added bonus, AI models can give better information about when the different types of earthquake shaking arrive. Timing the arrivals of the two most important waves—P and S waves—is called &lt;em&gt;phase picking.&lt;/em&gt;&amp;nbsp;It allows scientists to draw inferences about the structure of the quake. AI models can do this alongside earthquake detection.&lt;/p&gt;
&lt;p&gt;The basic task of earthquake detection (and phase picking) looks like this:&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2120127 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="455" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/a57b3db6-c689-4c5d-90a9-90146cf4cbfe_554x455.webp" width="554" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Cropped figure from Earthquake Transformer—an attentive deep-learning model for simultaneous earthquake detection and phase picking.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Nature Communications

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p class="sc-1o72qur-1 hDESMK"&gt;The first three rows represent different directions of vibration (east–west, north–south, and up–down respectively). Given these three dimensions of vibration, can we determine if an earthquake occurred, and if so, when it started?&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;

&lt;p class="sc-1o72qur-1 hDESMK"&gt;We want to detect the initial P wave, which arrives directly from the site of the earthquake. But this can be tricky because echoes of the P wave may get reflected off other rock layers and arrive later, making the waveform more complicated.&lt;/p&gt;
&lt;p class="sc-1o72qur-1 hDESMK"&gt;Ideally, then, our model outputs three things at every time step in the sample:&lt;/p&gt;
&lt;ol class="sc-n2r23q-11 fHhWJP"&gt;
&lt;li&gt;
&lt;p class="sc-1o72qur-1 hDESMK"&gt;The probability that an earthquake is occurring at that moment.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p class="sc-1o72qur-1 hDESMK"&gt;The probability that the first P wave arrives at that moment.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p class="sc-1o72qur-1 hDESMK"&gt;The probability that the first S wave arrives at that moment.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p class="sc-1o72qur-1 hDESMK"&gt;We see all three outputs in the fourth row: the detection in green, the P wave arrival in blue, and the S wave arrival in red. (There are two earthquakes in this sample.)&lt;/p&gt;
&lt;p class="sc-1o72qur-1 hDESMK"&gt;To train an AI model, scientists take large amounts of labeled data, like what’s above, and do supervised training. I’ll describe one of the most used models: Earthquake Transformer, which was developed around 2020 by a Stanford University team led by S. Mostafa Mousavi, who later became a Harvard professor.&lt;/p&gt;
&lt;p&gt;Like many earthquake detection models, Earthquake Transformer adapts ideas from image classification. Readers may be familiar with AlexNet, a famous image-recognition model that kicked off the deep-learning boom&amp;nbsp;in 2012.&lt;/p&gt;
&lt;p&gt;AlexNet used convolutions, a neural network architecture that’s based on the idea that pixels that are physically close together are more likely to be related. The first convolutional layer of AlexNet broke an image down into small chunks—11 pixels on a side—and classified each chunk based on the presence of simple features like edges or gradients.&lt;/p&gt;
&lt;p&gt;The next layer took the first layer’s classifications as input and checked for higher-level concepts such as textures or simple shapes.&lt;/p&gt;
&lt;p&gt;Each convolutional layer analyzed a larger portion of the image and operated at a higher level of abstraction. By the final layers, the network was looking at the entire image and identifying objects like “mushroom” and “container ship.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Images are two-dimensional, so AlexNet is based on two-dimensional convolutions. By contrast, seismograph data is one-dimensional, so Earthquake Transformer uses one-dimensional convolutions over the time dimension. The first layer analyzes vibration data in 0.1-second chunks, while later layers identify patterns over progressively longer time periods.&lt;/p&gt;
&lt;p&gt;It’s difficult to say what exact patterns the earthquake model is picking out, but we can analogize this to a hypothetical audio transcription model using one-dimensional convolutions. That model might first identify consonants, then syllables, then words, then sentences over increasing time scales.&lt;/p&gt;
&lt;p&gt;Earthquake Transformer converts raw waveform data into a collection of high-level representations that indicate the likelihood of earthquakes and other seismologically significant events. This is followed by a series of deconvolution layers that pinpoint exactly when an earthquake—and its all-important P and S waves—occurred.&lt;/p&gt;
&lt;p&gt;The model also uses an attention layer in the middle of the model to mix information between different parts of the time series. The attention mechanism is most famous in&amp;nbsp;large language models, where it helps pass information between words. It plays a similar role in seismographic detection. Earthquake seismograms have a general structure: P waves followed by S waves followed by other types of shaking. So if a segment looks like the start of a P wave, the attention mechanism helps it check that it fits into a broader earthquake pattern.&lt;/p&gt;
&lt;h2 class="header-anchor-post"&gt;Scaling earthquake data&lt;/h2&gt;
&lt;p&gt;All of the Earthquake Transformer’s components are standard designs from the neural network literature. Other successful detection models, like&amp;nbsp;PhaseNet, are even simpler. PhaseNet uses only one-dimensional convolutions to pick the arrival times of earthquake waves. There are no attention layers.&lt;/p&gt;
&lt;p&gt;Generally, there hasn’t been “much need to invent new architectures for seismology,” according to Byrnes. The techniques derived from image processing have been sufficient.&lt;/p&gt;
&lt;p&gt;What made these generic architectures work so well then? Data. Lots of it.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Ars has previously reported&amp;nbsp;on how the introduction of&amp;nbsp;ImageNet, an image recognition benchmark, helped spark the deep learning boom. Large, publicly available earthquake datasets have played a similar role in seismology.&lt;/p&gt;
&lt;p&gt;Earthquake Transformer was trained using the&amp;nbsp;Stanford Earthquake Dataset (STEAD), which contains 1.2 million human-labeled segments of seismogram data from around the world. (The paper for STEAD explicitly&amp;nbsp;mentions ImageNet as an inspiration). Other models, like PhaseNet, were also trained on hundreds of thousands or millions of labeled segments.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2120129 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="380" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/3817d79e-07ac-41dd-b871-6938fffdaa58_720x380.webp" width="720" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      All recorded earthquakes in the&amp;nbsp;Stanford Earthquake Dataset.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          IEEE (CC BY 4.0)

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The combination of the data and the architecture just works. The current models are “comically good” at identifying and classifying earthquakes, according to Byrnes. Typically, machine-learning methods find 10 or more times the quakes that were previously identified in an area. You can see this directly in an Italian earthquake catalog:&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2120132 align-none"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="none large" height="654" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/96c357de-fe05-48f3-a2d0-edeb227f50ba_1352x864-1024x654.webp" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      From&amp;nbsp;Machine learning and earthquake forecasting—next steps&amp;nbsp;by Beroza et al.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Nature Communications (CC-BY 4.0)

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;AI tools won’t necessarily detect more earthquakes than template matching. But AI-based techniques are much less compute- and labor-intensive, making them more accessible to the average research project and easier to apply in regions around the world.&lt;/p&gt;
&lt;p&gt;All in all, these machine-learning models are so good that they’ve almost completely supplanted traditional methods for detecting and phase-picking earthquakes, especially for smaller magnitudes.&lt;/p&gt;
&lt;h2 class="header-anchor-post"&gt;What does all this AI stuff do?&lt;/h2&gt;
&lt;p&gt;The holy grail of earthquake science is earthquake prediction. For instance, scientists&amp;nbsp;know that a large quake will happen near Seattle but have little ability to know whether it will happen tomorrow or in a hundred years. It would be helpful if we could predict earthquakes precisely enough to allow people in affected areas to evacuate.&lt;/p&gt;
&lt;p&gt;You might think AI tools would help predict earthquakes, but that doesn’t seem to have happened yet.&lt;/p&gt;
&lt;p&gt;The applications are more technical and less flashy, said Cornell’s Judith Hubbard.&lt;/p&gt;
&lt;p&gt;Better AI models have given seismologists much more comprehensive earthquake catalogs, which have unlocked “a lot of different techniques,” Bradley said.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;One of the coolest applications is in understanding and imaging volcanoes. Volcanic activity produces a large number of small earthquakes, whose locations help scientists understand the structure of the magma system. In a 2022&amp;nbsp;paper, John Wilding and co-authors used a large AI-generated earthquake catalog to create this incredible image of the structure of the Hawaiian volcanic system.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2120134 align-none"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="none large" height="587" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/9b4ff1f3-45ab-4851-a288-4ffb920d6a61_1250x717-1024x587.webp" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Each dot represents an individual earthquake.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Wilding et al.,&amp;nbsp;The magmatic web beneath Hawai‘i.

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;They provided direct evidence of a previously hypothesized magma connection between the deep Pāhala sill complex and Mauna Loa’s shallow volcanic structure. You can see this in the image with the arrow labeled as Pāhala-Mauna Loa seismicity band. The authors were also able to clarify the structure of the Pāhala sill complex into discrete sheets of magma. This level of detail could potentially facilitate better real-time monitoring of earthquakes and more accurate eruption forecasting.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Another promising area is lowering the cost of dealing with huge datasets.&amp;nbsp;Distributed Acoustic Sensing (DAS) is a powerful technique that uses fiber-optic cables to measure seismic activity across the entire length of the cable. A single DAS array can produce “hundreds of gigabytes of data” a day, according to Jiaxuan Li, a professor at the University of Houston. That much data can produce extremely high-resolution datasets—enough to pick out individual footsteps.&lt;/p&gt;
&lt;p&gt;AI tools make it possible to very accurately time earthquakes in DAS data. Before the introduction of AI techniques for phase picking in DAS data, Li and some of his collaborators attempted to use traditional techniques. While these “work roughly,” they weren’t accurate enough for their downstream analysis. Without AI, much of his work would have been “much harder,” he told me.&lt;/p&gt;
&lt;p&gt;Li is also optimistic that AI tools will be able to help him isolate “new types of signals” in the rich DAS data in the future.&lt;/p&gt;
&lt;h2&gt;Not all AI techniques have paid off&lt;/h2&gt;
&lt;p&gt;As in many other scientific fields, seismologists face some pressure to adopt AI methods, whether or not they are relevant to their research.&lt;/p&gt;
&lt;p&gt;“The schools want you to put the word AI in front of everything,” Byrnes said. “It’s a little out of control.”&lt;/p&gt;
&lt;p&gt;This can lead to papers that are technically sound but practically useless. Hubbard and Bradley told me that they’ve seen a lot of papers based on AI techniques that “reveal a fundamental misunderstanding of how earthquakes work.”&lt;/p&gt;
&lt;p&gt;They pointed out that graduate students can feel pressure to specialize in AI methods at the cost of learning less about the fundamentals of the scientific field. They fear that if this type of AI-driven research becomes entrenched, older methods will get “out-competed by a kind of meaninglessness.”&lt;/p&gt;
&lt;p&gt;While these are real issues, and ones Understanding AI has&amp;nbsp;reported on before, I don’t think they detract from the success of AI earthquake detection. In the last five years, an AI-based workflow has almost completely replaced one of the fundamental tasks in seismology for the better.&lt;/p&gt;
&lt;p&gt;That’s pretty cool.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Kai Williams is a reporter for Understanding AI, a Substack newsletter founded by Ars Technica alum Timothy B. Lee. His work is supported by a Tarbell Fellowship. Subscribe to Understanding AI to get more from Tim and Kai.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;








  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        AI is “comically good” at detecting small earthquakes—here’s why that matters.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/earthquake-detection-640x360.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/earthquake-detection-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson | Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;On January 1, 2008, at 1:59 am in Calipatria, California, an earthquake happened. You haven’t heard of this earthquake; even if you had been living in Calipatria, you wouldn’t have felt anything. It was magnitude -0.53, about the same amount of shaking as a truck passing by. Still, this earthquake is notable, not because it was large but because it was small—and yet we know about it.&lt;/p&gt;
&lt;p&gt;Over the past seven years, AI tools based on computer imaging have almost completely automated one of the fundamental tasks of seismology: detecting earthquakes. What used to be the task of human analysts—and later, simpler computer programs—can now be done automatically and quickly by machine-learning tools.&lt;/p&gt;
&lt;p&gt;These machine-learning tools can detect smaller earthquakes than human analysts, especially in noisy environments like cities. Earthquakes give valuable information about the composition of the Earth and what hazards might occur in the future.&lt;/p&gt;
&lt;p&gt;“In the best-case scenario, when you adopt these new techniques, even on the same old data, it’s kind of like putting on glasses for the first time, and you can see the leaves on the trees,” said Kyle Bradley, co-author of the&amp;nbsp;Earthquake Insights newsletter.&lt;/p&gt;
&lt;p&gt;I talked with several earthquake scientists, and they all agreed that machine-learning methods have replaced humans for the better in these specific tasks.&lt;/p&gt;
&lt;p&gt;“It’s really remarkable,” Judith Hubbard, a Cornell University professor and Bradley’s co-author, told me.&lt;/p&gt;
&lt;p&gt;Less certain is what comes next. Earthquake detection is a fundamental part of seismology, but there are many other data processing tasks that have yet to be disrupted. The biggest potential impacts, all the way to earthquake forecasting, haven’t materialized yet.&lt;/p&gt;
&lt;p&gt;“It really was a revolution,” said Joe Byrnes, a professor at the University of Texas at Dallas. “But the revolution is ongoing.”&lt;/p&gt;
&lt;h2 class="header-anchor-post"&gt;What do seismologists do?&lt;/h2&gt;
&lt;p&gt;When an earthquake happens in one place, the shaking passes through the ground, similar to how sound waves pass through the air. In both cases, it’s possible to draw inferences about the materials the waves pass through.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Imagine tapping a wall to figure out if it’s hollow. Because a solid wall vibrates differently than a hollow wall, you can figure out the structure by sound.&lt;/p&gt;
&lt;p&gt;With earthquakes, this same principle holds. Seismic waves pass through different materials (rock, oil, magma, etc.) differently, and scientists use these vibrations to image the Earth’s interior.&lt;/p&gt;
&lt;p&gt;The main tool that scientists traditionally use is a&amp;nbsp;&lt;em&gt;seismometer&lt;/em&gt;. These record the movement of the Earth in three directions: up–down, north–south, and east–west. If an earthquake happens, seismometers can measure the shaking in that particular location.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2120119 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="768" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/Kinemetrics_seismograph.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      An old-fashioned physical seismometer. Today, seismometers record data digitally.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Yamaguchi先生 on&amp;nbsp;Wikimedia CC BY-SA 3.0

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Scientists then process raw seismometer information to identify earthquakes.&lt;/p&gt;
&lt;p&gt;Earthquakes produce multiple types of shaking, which travel at different speeds. Two types, Primary (P) waves and Secondary (S) waves are particularly important, and scientists like to identify the start of each of these phases.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2 class="header-anchor-post"&gt;Finding quakes before machine learning&lt;/h2&gt;
&lt;p&gt;Before good algorithms, earthquake cataloging had to happen by hand. Byrnes said that “traditionally, something like the lab at the United States Geological Survey would have an army of mostly undergraduate students or interns looking at seismograms.”&lt;/p&gt;
&lt;p&gt;However, there are only so many earthquakes you can find and classify manually. Creating algorithms to effectively find and process earthquakes has long been a priority in the field—especially since the arrival of computers in the early 1950s.&lt;/p&gt;
&lt;p&gt;“The field of seismology historically has always advanced as computing has advanced,” Bradley told me.&lt;/p&gt;
&lt;p&gt;There’s a big challenge with traditional algorithms, though: They can’t easily find smaller quakes, especially in noisy environments.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2120121 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="480" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/4730ac1e-6fae-4249-a6b6-653fc65aae6b_640x480.webp" width="640" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Composite seismogram of common events. Note how each event has a slightly different shape.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          EarthScope Consortium CC BY 4.0

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;As we see in the seismogram above, many different events can cause seismic signals. If a method is too sensitive, it risks falsely detecting events as earthquakes. The problem is especially bad in cities, where the constant hum of traffic and buildings can drown out small earthquakes.&lt;/p&gt;
&lt;p&gt;However, earthquakes have a characteristic “shape.” The magnitude 7.7 earthquake above looks quite different from the helicopter landing, for instance.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;So one idea scientists had was to make templates from human-labeled datasets. If a new waveform correlates closely with an existing template, it’s almost certainly an earthquake.&lt;/p&gt;
&lt;p&gt;Template matching works very well if you have enough human-labeled examples. In 2019, Zach Ross’ lab at Caltech used template matching to find 10 times as many earthquakes in Southern California as had previously been known, including the earthquake at the start of this story. Almost all of the new 1.6 million quakes they found were very small, magnitude 1 and below.&lt;/p&gt;
&lt;p&gt;If you don’t have an extensive pre-existing dataset of templates, however, you can’t easily apply template matching. That isn’t a problem in Southern California—which already had a basically complete record of earthquakes down to magnitude 1.7—but it’s a challenge elsewhere.&lt;/p&gt;
&lt;p&gt;Also, template matching is computationally expensive. Creating a Southern California quake dataset using template matching took 200 Nvidia P100 GPUs running for days on end.&lt;/p&gt;
&lt;p&gt;There had to be a better way.&lt;/p&gt;
&lt;h2 class="header-anchor-post"&gt;Breaking down the Earthquake Transformer&lt;/h2&gt;
&lt;p&gt;AI detection models solve all of these problems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p class="sc-1o72qur-1 hDESMK"&gt;They are faster than template matching.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p class="sc-1o72qur-1 hDESMK"&gt;Because AI detection models are very small (around 350,000 parameters compared to billions in LLMs like GPT4.0), they can be run on consumer CPUs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p class="sc-1o72qur-1 hDESMK"&gt;AI models generalize well to regions not represented in the original dataset.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As an added bonus, AI models can give better information about when the different types of earthquake shaking arrive. Timing the arrivals of the two most important waves—P and S waves—is called &lt;em&gt;phase picking.&lt;/em&gt;&amp;nbsp;It allows scientists to draw inferences about the structure of the quake. AI models can do this alongside earthquake detection.&lt;/p&gt;
&lt;p&gt;The basic task of earthquake detection (and phase picking) looks like this:&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2120127 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="455" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/a57b3db6-c689-4c5d-90a9-90146cf4cbfe_554x455.webp" width="554" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Cropped figure from Earthquake Transformer—an attentive deep-learning model for simultaneous earthquake detection and phase picking.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Nature Communications

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p class="sc-1o72qur-1 hDESMK"&gt;The first three rows represent different directions of vibration (east–west, north–south, and up–down respectively). Given these three dimensions of vibration, can we determine if an earthquake occurred, and if so, when it started?&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;

&lt;p class="sc-1o72qur-1 hDESMK"&gt;We want to detect the initial P wave, which arrives directly from the site of the earthquake. But this can be tricky because echoes of the P wave may get reflected off other rock layers and arrive later, making the waveform more complicated.&lt;/p&gt;
&lt;p class="sc-1o72qur-1 hDESMK"&gt;Ideally, then, our model outputs three things at every time step in the sample:&lt;/p&gt;
&lt;ol class="sc-n2r23q-11 fHhWJP"&gt;
&lt;li&gt;
&lt;p class="sc-1o72qur-1 hDESMK"&gt;The probability that an earthquake is occurring at that moment.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p class="sc-1o72qur-1 hDESMK"&gt;The probability that the first P wave arrives at that moment.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p class="sc-1o72qur-1 hDESMK"&gt;The probability that the first S wave arrives at that moment.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p class="sc-1o72qur-1 hDESMK"&gt;We see all three outputs in the fourth row: the detection in green, the P wave arrival in blue, and the S wave arrival in red. (There are two earthquakes in this sample.)&lt;/p&gt;
&lt;p class="sc-1o72qur-1 hDESMK"&gt;To train an AI model, scientists take large amounts of labeled data, like what’s above, and do supervised training. I’ll describe one of the most used models: Earthquake Transformer, which was developed around 2020 by a Stanford University team led by S. Mostafa Mousavi, who later became a Harvard professor.&lt;/p&gt;
&lt;p&gt;Like many earthquake detection models, Earthquake Transformer adapts ideas from image classification. Readers may be familiar with AlexNet, a famous image-recognition model that kicked off the deep-learning boom&amp;nbsp;in 2012.&lt;/p&gt;
&lt;p&gt;AlexNet used convolutions, a neural network architecture that’s based on the idea that pixels that are physically close together are more likely to be related. The first convolutional layer of AlexNet broke an image down into small chunks—11 pixels on a side—and classified each chunk based on the presence of simple features like edges or gradients.&lt;/p&gt;
&lt;p&gt;The next layer took the first layer’s classifications as input and checked for higher-level concepts such as textures or simple shapes.&lt;/p&gt;
&lt;p&gt;Each convolutional layer analyzed a larger portion of the image and operated at a higher level of abstraction. By the final layers, the network was looking at the entire image and identifying objects like “mushroom” and “container ship.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Images are two-dimensional, so AlexNet is based on two-dimensional convolutions. By contrast, seismograph data is one-dimensional, so Earthquake Transformer uses one-dimensional convolutions over the time dimension. The first layer analyzes vibration data in 0.1-second chunks, while later layers identify patterns over progressively longer time periods.&lt;/p&gt;
&lt;p&gt;It’s difficult to say what exact patterns the earthquake model is picking out, but we can analogize this to a hypothetical audio transcription model using one-dimensional convolutions. That model might first identify consonants, then syllables, then words, then sentences over increasing time scales.&lt;/p&gt;
&lt;p&gt;Earthquake Transformer converts raw waveform data into a collection of high-level representations that indicate the likelihood of earthquakes and other seismologically significant events. This is followed by a series of deconvolution layers that pinpoint exactly when an earthquake—and its all-important P and S waves—occurred.&lt;/p&gt;
&lt;p&gt;The model also uses an attention layer in the middle of the model to mix information between different parts of the time series. The attention mechanism is most famous in&amp;nbsp;large language models, where it helps pass information between words. It plays a similar role in seismographic detection. Earthquake seismograms have a general structure: P waves followed by S waves followed by other types of shaking. So if a segment looks like the start of a P wave, the attention mechanism helps it check that it fits into a broader earthquake pattern.&lt;/p&gt;
&lt;h2 class="header-anchor-post"&gt;Scaling earthquake data&lt;/h2&gt;
&lt;p&gt;All of the Earthquake Transformer’s components are standard designs from the neural network literature. Other successful detection models, like&amp;nbsp;PhaseNet, are even simpler. PhaseNet uses only one-dimensional convolutions to pick the arrival times of earthquake waves. There are no attention layers.&lt;/p&gt;
&lt;p&gt;Generally, there hasn’t been “much need to invent new architectures for seismology,” according to Byrnes. The techniques derived from image processing have been sufficient.&lt;/p&gt;
&lt;p&gt;What made these generic architectures work so well then? Data. Lots of it.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Ars has previously reported&amp;nbsp;on how the introduction of&amp;nbsp;ImageNet, an image recognition benchmark, helped spark the deep learning boom. Large, publicly available earthquake datasets have played a similar role in seismology.&lt;/p&gt;
&lt;p&gt;Earthquake Transformer was trained using the&amp;nbsp;Stanford Earthquake Dataset (STEAD), which contains 1.2 million human-labeled segments of seismogram data from around the world. (The paper for STEAD explicitly&amp;nbsp;mentions ImageNet as an inspiration). Other models, like PhaseNet, were also trained on hundreds of thousands or millions of labeled segments.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2120129 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="380" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/3817d79e-07ac-41dd-b871-6938fffdaa58_720x380.webp" width="720" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      All recorded earthquakes in the&amp;nbsp;Stanford Earthquake Dataset.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          IEEE (CC BY 4.0)

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The combination of the data and the architecture just works. The current models are “comically good” at identifying and classifying earthquakes, according to Byrnes. Typically, machine-learning methods find 10 or more times the quakes that were previously identified in an area. You can see this directly in an Italian earthquake catalog:&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2120132 align-none"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="none large" height="654" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/96c357de-fe05-48f3-a2d0-edeb227f50ba_1352x864-1024x654.webp" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      From&amp;nbsp;Machine learning and earthquake forecasting—next steps&amp;nbsp;by Beroza et al.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Nature Communications (CC-BY 4.0)

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;AI tools won’t necessarily detect more earthquakes than template matching. But AI-based techniques are much less compute- and labor-intensive, making them more accessible to the average research project and easier to apply in regions around the world.&lt;/p&gt;
&lt;p&gt;All in all, these machine-learning models are so good that they’ve almost completely supplanted traditional methods for detecting and phase-picking earthquakes, especially for smaller magnitudes.&lt;/p&gt;
&lt;h2 class="header-anchor-post"&gt;What does all this AI stuff do?&lt;/h2&gt;
&lt;p&gt;The holy grail of earthquake science is earthquake prediction. For instance, scientists&amp;nbsp;know that a large quake will happen near Seattle but have little ability to know whether it will happen tomorrow or in a hundred years. It would be helpful if we could predict earthquakes precisely enough to allow people in affected areas to evacuate.&lt;/p&gt;
&lt;p&gt;You might think AI tools would help predict earthquakes, but that doesn’t seem to have happened yet.&lt;/p&gt;
&lt;p&gt;The applications are more technical and less flashy, said Cornell’s Judith Hubbard.&lt;/p&gt;
&lt;p&gt;Better AI models have given seismologists much more comprehensive earthquake catalogs, which have unlocked “a lot of different techniques,” Bradley said.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;One of the coolest applications is in understanding and imaging volcanoes. Volcanic activity produces a large number of small earthquakes, whose locations help scientists understand the structure of the magma system. In a 2022&amp;nbsp;paper, John Wilding and co-authors used a large AI-generated earthquake catalog to create this incredible image of the structure of the Hawaiian volcanic system.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2120134 align-none"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="none large" height="587" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/9b4ff1f3-45ab-4851-a288-4ffb920d6a61_1250x717-1024x587.webp" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Each dot represents an individual earthquake.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Wilding et al.,&amp;nbsp;The magmatic web beneath Hawai‘i.

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;They provided direct evidence of a previously hypothesized magma connection between the deep Pāhala sill complex and Mauna Loa’s shallow volcanic structure. You can see this in the image with the arrow labeled as Pāhala-Mauna Loa seismicity band. The authors were also able to clarify the structure of the Pāhala sill complex into discrete sheets of magma. This level of detail could potentially facilitate better real-time monitoring of earthquakes and more accurate eruption forecasting.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Another promising area is lowering the cost of dealing with huge datasets.&amp;nbsp;Distributed Acoustic Sensing (DAS) is a powerful technique that uses fiber-optic cables to measure seismic activity across the entire length of the cable. A single DAS array can produce “hundreds of gigabytes of data” a day, according to Jiaxuan Li, a professor at the University of Houston. That much data can produce extremely high-resolution datasets—enough to pick out individual footsteps.&lt;/p&gt;
&lt;p&gt;AI tools make it possible to very accurately time earthquakes in DAS data. Before the introduction of AI techniques for phase picking in DAS data, Li and some of his collaborators attempted to use traditional techniques. While these “work roughly,” they weren’t accurate enough for their downstream analysis. Without AI, much of his work would have been “much harder,” he told me.&lt;/p&gt;
&lt;p&gt;Li is also optimistic that AI tools will be able to help him isolate “new types of signals” in the rich DAS data in the future.&lt;/p&gt;
&lt;h2&gt;Not all AI techniques have paid off&lt;/h2&gt;
&lt;p&gt;As in many other scientific fields, seismologists face some pressure to adopt AI methods, whether or not they are relevant to their research.&lt;/p&gt;
&lt;p&gt;“The schools want you to put the word AI in front of everything,” Byrnes said. “It’s a little out of control.”&lt;/p&gt;
&lt;p&gt;This can lead to papers that are technically sound but practically useless. Hubbard and Bradley told me that they’ve seen a lot of papers based on AI techniques that “reveal a fundamental misunderstanding of how earthquakes work.”&lt;/p&gt;
&lt;p&gt;They pointed out that graduate students can feel pressure to specialize in AI methods at the cost of learning less about the fundamentals of the scientific field. They fear that if this type of AI-driven research becomes entrenched, older methods will get “out-competed by a kind of meaninglessness.”&lt;/p&gt;
&lt;p&gt;While these are real issues, and ones Understanding AI has&amp;nbsp;reported on before, I don’t think they detract from the success of AI earthquake detection. In the last five years, an AI-based workflow has almost completely replaced one of the fundamental tasks in seismology for the better.&lt;/p&gt;
&lt;p&gt;That’s pretty cool.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Kai Williams is a reporter for Understanding AI, a Substack newsletter founded by Ars Technica alum Timothy B. Lee. His work is supported by a Tarbell Fellowship. Subscribe to Understanding AI to get more from Tim and Kai.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;








  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/science/2025/10/like-putting-on-glasses-for-the-first-time-how-ai-improves-earthquake-detection/</guid><pubDate>Fri, 10 Oct 2025 11:00:24 +0000</pubDate></item><item><title>The Download: our bodies’ memories, and Traton’s electric trucks (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/10/10/1125586/the-download-our-bodies-memories-and-tratons-electric-trucks/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;How do our bodies remember?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“Like riding a bike” is shorthand for the remarkable way that our bodies remember how to move. Most of the time when we talk about muscle memory, we’re not talking about the muscles themselves but about the memory of a coordinated movement pattern that lives in the motor neurons, which control our muscles.&lt;/p&gt;&lt;p&gt;Yet in recent years, scientists have discovered that &lt;em&gt;our muscles themselves&lt;/em&gt; have a memory for movement and exercise. And the more we move, as with riding a bike or other kinds of exercise, the more those cells begin to make a memory of that exercise. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Bonnie Tsui&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;This piece is part of MIT Technology Review Explains: our series untangling the complex, messy world of technology to help you understand what’s coming next. &lt;/strong&gt;&lt;strong&gt;You can read more from the series here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story is also from our forthcoming print issue, which is all about the body. If you haven’t already, &lt;/strong&gt;&lt;strong&gt;subscribe now&lt;/strong&gt;&lt;strong&gt; to receive future issues once they land. Plus, you'll also receive a free digital report on nuclear power.&lt;/strong&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;2025 climate tech companies to watch: Traton and its electric trucks&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Every day, trucks carry many millions of tons of cargo down roads and highways around the world. Nearly all run on diesel and make up one of the largest commercial sources of carbon emissions.&lt;/p&gt;&lt;p&gt;Traton, a subsidiary of Volkswagen, is producing zero-emission trucks that could help clean up this sector, while also investing in a Europe-wide advanced charging network so other manufacturers can more easily follow suit. Read the full story.&lt;/p&gt;&lt;p&gt;&lt;em&gt;—Amy Nordrum&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Traton is one of our 10 climate tech companies to watch—our annual list of some of the most promising climate tech firms on the planet. &lt;/strong&gt;&lt;strong&gt;Check out the rest of the list here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;This test could reveal the health of your immune system&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;We know surprisingly little about our immune health. The vast array of cells, proteins, and biomolecules that works to defend us from disease is mind-bogglingly complicated. Immunologists are still getting to grips with how it all works.&lt;/p&gt;  &lt;p&gt;Now, a new test is being developed to measure immune health, one that even gives you a score. But that’s a difficult thing to do, for several reasons. Read the full story.&lt;/p&gt; 

 &lt;p&gt;&lt;em&gt;—Jessica Hamzelou&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This article first appeared in The Checkup, MIT Technology Review’s weekly biotech newsletter. To receive it in your inbox every Thursday, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 China is cracking down on imports of Nvidia’s AI chips&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Customs officers are combing shipments looking for the company’s China-specific chips. (FT $)&lt;br /&gt;+ &lt;em&gt;US officials are investigating a firm that’s suspected of helping China sidestep export restrictions. &lt;/em&gt;(NYT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 Tesla’s ‘full self-driving’ feature is under investigation&lt;/strong&gt;&lt;br /&gt;After multiple reports of vehicles using it ran red lights. (WP $)&lt;br /&gt;+ &lt;em&gt;The company is slashing its prices to compete with Chinese giant BYD. &lt;/em&gt;(Rest of World)&lt;br /&gt;+ &lt;em&gt;Elon Musk will still receive billions, even if he fails to achieve his ambitions goals. &lt;/em&gt;(Reuters)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 A data hoarder has created a searchable database of Epstein files&lt;/strong&gt;&lt;br /&gt;Making it simple to find mentions of specific people and locations. (404 Media)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;4 OpenAI says GPT-5 is its least-biased model yet&lt;/strong&gt;&lt;br /&gt;Even when proceeding with “challenging, emotionally charged prompts.” (Axios)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 The developers behind ICE-tracking apps aren’t giving up&lt;/strong&gt;&lt;br /&gt;They’re fighting Apple’s decision to remove their creations from its app store. (Wired $)&lt;br /&gt;+ &lt;em&gt;Another effort to track ICE raids was just taken offline. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;6 The world’s biodiversity crisis is worsening&lt;br /&gt;&lt;/strong&gt;More than half of all bird species are in decline. (The Guardian)&lt;br /&gt;+ &lt;em&gt;The short, strange history of gene de-extinction. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 YouTube is extending an olive branch to banned creators&lt;br /&gt;&lt;/strong&gt;It’s overturned a lifetime ban policy to give the people behind previously-banned channels a second chance. (CNBC)&lt;br /&gt;+ &lt;em&gt;But users kicked off for copyright infringement or extremism aren’t eligible. &lt;/em&gt;(Bloomberg $)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;8 This startup wants to bring self-flying planes to our skies&amp;nbsp;&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Starting with military cargo flights. (WSJ $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 Your plumber might be using ChatGPT&lt;br /&gt;&lt;/strong&gt;They’re increasingly using the chatbot to troubleshoot on the ground. (CNN)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 Do robots really need hands?&lt;/strong&gt;&lt;br /&gt;Maybe not, but that’s not standing in the way of researchers trying to recreate them. (Fast Company $)&lt;br /&gt;+ &lt;em&gt;Will we ever trust robots? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“Social media is a complete dumpster.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Hany Farid, a professor of computer science at the University of California, Berkeley, describes the proliferation of AI slop videos infiltrating digital platforms to the New York Times.&lt;/p&gt;   
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1125588" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/image_621d18.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Who gets to decide who receives experimental medical treatments?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;There has been a trend toward lowering the bar for new medicines, and it is becoming easier for people to access treatments that might not help them—and could even harm them. Anecdotes appear to be overpowering evidence in decisions on drug approval. As a result, we’re ending up with some drugs that don’t work.&lt;/p&gt;&lt;p&gt;We urgently need to question how these decisions are made. Who should have access to experimental therapies? And who should get to decide? Such questions are especially pressing considering how quickly biotechnology is advancing. We’re not just improving on existing classes of treatments—we’re creating entirely new ones. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Jessica Hamzelou&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ I love this crowd-sourced compendium of every known Wilhelm scream in all sorts of media.&lt;br /&gt;+ Happy birthday to pocket rocket Bruno Mars, who turned 40 this week.&lt;br /&gt;+ Here’s how to visit an interstellar interloper.&lt;br /&gt;+ Bumi the penguin is having the absolute time of their life with this bubble machine 🐧&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;How do our bodies remember?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“Like riding a bike” is shorthand for the remarkable way that our bodies remember how to move. Most of the time when we talk about muscle memory, we’re not talking about the muscles themselves but about the memory of a coordinated movement pattern that lives in the motor neurons, which control our muscles.&lt;/p&gt;&lt;p&gt;Yet in recent years, scientists have discovered that &lt;em&gt;our muscles themselves&lt;/em&gt; have a memory for movement and exercise. And the more we move, as with riding a bike or other kinds of exercise, the more those cells begin to make a memory of that exercise. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Bonnie Tsui&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;This piece is part of MIT Technology Review Explains: our series untangling the complex, messy world of technology to help you understand what’s coming next. &lt;/strong&gt;&lt;strong&gt;You can read more from the series here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story is also from our forthcoming print issue, which is all about the body. If you haven’t already, &lt;/strong&gt;&lt;strong&gt;subscribe now&lt;/strong&gt;&lt;strong&gt; to receive future issues once they land. Plus, you'll also receive a free digital report on nuclear power.&lt;/strong&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;2025 climate tech companies to watch: Traton and its electric trucks&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Every day, trucks carry many millions of tons of cargo down roads and highways around the world. Nearly all run on diesel and make up one of the largest commercial sources of carbon emissions.&lt;/p&gt;&lt;p&gt;Traton, a subsidiary of Volkswagen, is producing zero-emission trucks that could help clean up this sector, while also investing in a Europe-wide advanced charging network so other manufacturers can more easily follow suit. Read the full story.&lt;/p&gt;&lt;p&gt;&lt;em&gt;—Amy Nordrum&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Traton is one of our 10 climate tech companies to watch—our annual list of some of the most promising climate tech firms on the planet. &lt;/strong&gt;&lt;strong&gt;Check out the rest of the list here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;This test could reveal the health of your immune system&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;We know surprisingly little about our immune health. The vast array of cells, proteins, and biomolecules that works to defend us from disease is mind-bogglingly complicated. Immunologists are still getting to grips with how it all works.&lt;/p&gt;  &lt;p&gt;Now, a new test is being developed to measure immune health, one that even gives you a score. But that’s a difficult thing to do, for several reasons. Read the full story.&lt;/p&gt; 

 &lt;p&gt;&lt;em&gt;—Jessica Hamzelou&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This article first appeared in The Checkup, MIT Technology Review’s weekly biotech newsletter. To receive it in your inbox every Thursday, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 China is cracking down on imports of Nvidia’s AI chips&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Customs officers are combing shipments looking for the company’s China-specific chips. (FT $)&lt;br /&gt;+ &lt;em&gt;US officials are investigating a firm that’s suspected of helping China sidestep export restrictions. &lt;/em&gt;(NYT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 Tesla’s ‘full self-driving’ feature is under investigation&lt;/strong&gt;&lt;br /&gt;After multiple reports of vehicles using it ran red lights. (WP $)&lt;br /&gt;+ &lt;em&gt;The company is slashing its prices to compete with Chinese giant BYD. &lt;/em&gt;(Rest of World)&lt;br /&gt;+ &lt;em&gt;Elon Musk will still receive billions, even if he fails to achieve his ambitions goals. &lt;/em&gt;(Reuters)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 A data hoarder has created a searchable database of Epstein files&lt;/strong&gt;&lt;br /&gt;Making it simple to find mentions of specific people and locations. (404 Media)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;4 OpenAI says GPT-5 is its least-biased model yet&lt;/strong&gt;&lt;br /&gt;Even when proceeding with “challenging, emotionally charged prompts.” (Axios)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 The developers behind ICE-tracking apps aren’t giving up&lt;/strong&gt;&lt;br /&gt;They’re fighting Apple’s decision to remove their creations from its app store. (Wired $)&lt;br /&gt;+ &lt;em&gt;Another effort to track ICE raids was just taken offline. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;6 The world’s biodiversity crisis is worsening&lt;br /&gt;&lt;/strong&gt;More than half of all bird species are in decline. (The Guardian)&lt;br /&gt;+ &lt;em&gt;The short, strange history of gene de-extinction. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 YouTube is extending an olive branch to banned creators&lt;br /&gt;&lt;/strong&gt;It’s overturned a lifetime ban policy to give the people behind previously-banned channels a second chance. (CNBC)&lt;br /&gt;+ &lt;em&gt;But users kicked off for copyright infringement or extremism aren’t eligible. &lt;/em&gt;(Bloomberg $)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;8 This startup wants to bring self-flying planes to our skies&amp;nbsp;&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Starting with military cargo flights. (WSJ $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 Your plumber might be using ChatGPT&lt;br /&gt;&lt;/strong&gt;They’re increasingly using the chatbot to troubleshoot on the ground. (CNN)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 Do robots really need hands?&lt;/strong&gt;&lt;br /&gt;Maybe not, but that’s not standing in the way of researchers trying to recreate them. (Fast Company $)&lt;br /&gt;+ &lt;em&gt;Will we ever trust robots? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“Social media is a complete dumpster.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Hany Farid, a professor of computer science at the University of California, Berkeley, describes the proliferation of AI slop videos infiltrating digital platforms to the New York Times.&lt;/p&gt;   
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1125588" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/image_621d18.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Who gets to decide who receives experimental medical treatments?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;There has been a trend toward lowering the bar for new medicines, and it is becoming easier for people to access treatments that might not help them—and could even harm them. Anecdotes appear to be overpowering evidence in decisions on drug approval. As a result, we’re ending up with some drugs that don’t work.&lt;/p&gt;&lt;p&gt;We urgently need to question how these decisions are made. Who should have access to experimental therapies? And who should get to decide? Such questions are especially pressing considering how quickly biotechnology is advancing. We’re not just improving on existing classes of treatments—we’re creating entirely new ones. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Jessica Hamzelou&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ I love this crowd-sourced compendium of every known Wilhelm scream in all sorts of media.&lt;br /&gt;+ Happy birthday to pocket rocket Bruno Mars, who turned 40 this week.&lt;br /&gt;+ Here’s how to visit an interstellar interloper.&lt;br /&gt;+ Bumi the penguin is having the absolute time of their life with this bubble machine 🐧&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/10/10/1125586/the-download-our-bodies-memories-and-tratons-electric-trucks/</guid><pubDate>Fri, 10 Oct 2025 12:10:00 +0000</pubDate></item><item><title>[NEW] Together AI's ATLAS adaptive speculator delivers 400% inference speedup by learning from workloads in real-time (AI | VentureBeat)</title><link>https://venturebeat.com/ai/together-ais-atlas-adaptive-speculator-delivers-400-inference-speedup-by</link><description>[unable to retrieve full-text content]&lt;p&gt;Enterprises expanding AI deployments are hitting an invisible performance wall. The culprit? Static speculators that can&amp;#x27;t keep up with shifting workloads.&lt;/p&gt;&lt;p&gt;Speculators are smaller AI models that work alongside large language models during inference. They draft multiple tokens ahead, which the main model then verifies in parallel. This technique (called speculative decoding) has become essential for enterprises trying to reduce inference costs and latency. Instead of generating tokens one at a time, the system can accept multiple tokens at once, dramatically improving throughput.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.together.ai/"&gt;&lt;u&gt;Together AI&lt;/u&gt;&lt;/a&gt; today announced research and a new system called ATLAS (AdapTive-LeArning Speculator System) that aims to help enterprises overcome the challenge of static speculators. The technique provides a self-learning inference optimization capability that can help to deliver up to 400% faster inference performance than a baseline level of performance available in existing inference technologies such as vLLM.. The system addresses a critical problem: as AI workloads evolve, inference speeds degrade, even with specialized speculators in place.&lt;/p&gt;&lt;p&gt;The company which&lt;a href="https://venturebeat.com/ai/ai-startup-together-raises-funding-for-open-source-ai-and-cloud-platform"&gt; &lt;u&gt;got its start&lt;/u&gt;&lt;/a&gt; in 2023, has been focused on&lt;a href="https://venturebeat.com/ai/together-ai-promises-faster-inference-and-lower-costs-with-enterprise-ai-platform-for-private-cloud"&gt; &lt;u&gt;optimizing inference&lt;/u&gt;&lt;/a&gt; on its enterprise AI platform. Earlier this year the company&lt;a href="https://venturebeat.com/ai/together-ais-305m-bet-reasoning-models-like-deepseek-r1-are-increasing-not-decreasing-gpu-demand"&gt; &lt;u&gt;raised $305 million&lt;/u&gt;&lt;/a&gt; as customer adoption and demand has grown.&lt;/p&gt;&lt;p&gt;&amp;quot;Companies we work with generally, as they scale up, they see shifting workloads, and then they don&amp;#x27;t see as much speedup from speculative execution as before,&amp;quot; Tri Dao, chief scientist at Together AI, told VentureBeat in an exclusive interview. &amp;quot;These speculators generally don&amp;#x27;t work well when their workload domain starts to shift.&amp;quot;&lt;/p&gt;&lt;h2&gt;The workload drift problem no one talks about&lt;/h2&gt;&lt;p&gt;Most speculators in production today are &amp;quot;static&amp;quot; models. They&amp;#x27;re trained once on a fixed dataset representing expected workloads, then deployed without any ability to adapt. Companies like Meta and Mistral ship pre-trained speculators alongside their main models. Inference platforms like vLLM use these static speculators to boost throughput without changing output quality.&lt;/p&gt;&lt;p&gt;But there&amp;#x27;s a catch. When an enterprise&amp;#x27;s AI usage evolves the static speculator&amp;#x27;s accuracy plummets.&lt;/p&gt;&lt;p&gt;&amp;quot;If you&amp;#x27;re a company producing coding agents, and most of your developers have been writing in Python, all of a sudden some of them switch to writing Rust or C, then you see the speed starts to go down,&amp;quot; Dao explained. &amp;quot;The speculator has a mismatch between what it was trained on versus what the actual workload is.&amp;quot;&lt;/p&gt;&lt;p&gt;This workload drift represents a hidden tax on scaling AI. Enterprises either accept degraded performance or invest in retraining custom speculators. That process captures only a snapshot in time and quickly becomes outdated.&lt;/p&gt;&lt;h2&gt;How adaptive speculators work: A dual-model approach&lt;/h2&gt;&lt;p&gt;ATLAS uses a dual-speculator architecture that combines stability with adaptation:&lt;/p&gt;&lt;p&gt;&lt;b&gt;The static speculator&lt;/b&gt; - A heavyweight model trained on broad data provides consistent baseline performance. It serves as a &amp;quot;speed floor.&amp;quot;&lt;/p&gt;&lt;p&gt;&lt;b&gt;The adaptive speculator&lt;/b&gt; - A lightweight model learns continuously from live traffic. It specializes on-the-fly to emerging domains and usage patterns.&lt;/p&gt;&lt;p&gt;&lt;b&gt;The confidence-aware controller&lt;/b&gt; - An orchestration layer dynamically chooses which speculator to use. It adjusts the speculation &amp;quot;lookahead&amp;quot; based on confidence scores.&lt;/p&gt;&lt;p&gt;&amp;quot;Before the adaptive speculator learns anything, we still have the static speculator to help provide the speed boost in the beginning,&amp;quot; Ben Athiwaratkun, staff AI scientist at Together AI explained to VentureBeat. &amp;quot;Once the adaptive speculator becomes more confident, then the speed grows over time.&amp;quot;&lt;/p&gt;&lt;p&gt;The technical innovation lies in balancing acceptance rate (how often the target model agrees with drafted tokens) and draft latency. As the adaptive model learns from traffic patterns, the controller relies more on the lightweight speculator and extends lookahead. This compounds performance gains.&lt;/p&gt;&lt;p&gt;Users don&amp;#x27;t need to tune any parameters. &amp;quot;On the user side, users don&amp;#x27;t have to turn any knobs,&amp;quot; Dao said. &amp;quot;On our side, we have turned these knobs for users to adjust in a configuration that gets good speedup.&amp;quot;&lt;/p&gt;&lt;h2&gt;Performance that rivals custom silicon&lt;/h2&gt;&lt;p&gt;Together AI&amp;#x27;s testing shows ATLAS reaching 500 tokens per second on DeepSeek-V3.1 when fully adapted. More impressively, those numbers on Nvidia B200 GPUs match or exceed specialized inference chips like&lt;a href="https://venturebeat.com/ai/groq-just-made-hugging-face-way-faster-and-its-coming-for-aws-and-google"&gt; &lt;u&gt;Groq&amp;#x27;s&lt;/u&gt;&lt;/a&gt; custom hardware.&lt;/p&gt;&lt;p&gt;&amp;quot;The software and algorithmic improvement is able to close the gap with really specialized hardware,&amp;quot; Dao said. &amp;quot;We were seeing 500 tokens per second on these huge models that are even faster than some of the customized chips.&amp;quot;&lt;/p&gt;&lt;p&gt;The 400% speedup that the company claims for inference represents the cumulative effect of Together&amp;#x27;s Turbo optimization suite. FP4 quantization delivers 80% speedup over FP8 baseline. The static Turbo Speculator adds another 80-100% gain. The adaptive system layers on top. Each optimization compounds the benefits of the others.&lt;/p&gt;&lt;p&gt;Compared to standard inference engines like&lt;a href="https://venturebeat.com/ai/how-snowflakes-open-source-text-to-sql-and-arctic-inference-models-solve-enterprise-ais-two-biggest-deployment-headaches"&gt; &lt;u&gt;vLLM&lt;/u&gt;&lt;/a&gt; or Nvidia&amp;#x27;s TensorRT-LLM, the improvement is substantial. Together AI benchmarks against the stronger baseline between the two for each workload before applying speculative optimizations.&lt;/p&gt;&lt;h2&gt;The memory-compute tradeoff explained&lt;/h2&gt;&lt;p&gt;The performance gains stem from exploiting a fundamental inefficiency in modern inference: wasted compute capacity.&lt;/p&gt;&lt;p&gt;Dao explained that typically during inference, much of the compute power is not fully utilized.&lt;/p&gt;&lt;p&gt;&amp;quot;During inference, which is actually the dominant workload nowadays, you&amp;#x27;re mostly using the memory subsystem,&amp;quot; he said.&lt;/p&gt;&lt;p&gt;Speculative decoding trades idle compute for reduced memory access. When a model generates one token at a time, it&amp;#x27;s memory-bound. The GPU sits idle while waiting for memory. But when the speculator proposes five tokens and the target model verifies them simultaneously, compute utilization spikes while memory access remains roughly constant.&lt;/p&gt;&lt;p&gt;&amp;quot;The total amount of compute to generate five tokens is the same, but you only had to access memory once, instead of five times,&amp;quot; Dao said.&lt;/p&gt;&lt;h2&gt;Think of it as intelligent caching for AI&lt;/h2&gt;&lt;p&gt;For infrastructure teams familiar with traditional database optimization, adaptive speculators function like an intelligent caching layer, but with a crucial difference.&lt;/p&gt;&lt;p&gt;Traditional caching systems like Redis or memcached require exact matches. You store the exact same query result and retrieve it when that specific query runs again. Adaptive speculators work differently.&lt;/p&gt;&lt;p&gt;&amp;quot;You can view it as an intelligent way of caching, not storing exactly, but figuring out some patterns that you see,&amp;quot; Dao explained. &amp;quot;Broadly, we&amp;#x27;re observing that you&amp;#x27;re working with similar code, or working with similar, you know, controlling compute in a similar way. We can then predict what the big model is going to say. We just get better and better at predicting that.&amp;quot;&lt;/p&gt;&lt;p&gt;Rather than storing exact responses, the system learns patterns in how the model generates tokens. It recognizes that if you&amp;#x27;re editing Python files in a specific codebase, certain token sequences become more likely. The speculator adapts to those patterns, improving its predictions over time without requiring identical inputs.&lt;/p&gt;&lt;h2&gt;Use cases: RL training and evolving workloads&lt;/h2&gt;&lt;p&gt;Two enterprise scenarios particularly benefit from adaptive speculators:&lt;/p&gt;&lt;p&gt;&lt;b&gt;Reinforcement learning training&lt;/b&gt;: Static speculators quickly fall out of alignment as the policy evolves during training. ATLAS adapts continuously to the shifting policy distribution.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Evolving workloads&lt;/b&gt;: As enterprises discover new AI use cases, workload composition shifts. &amp;quot;Maybe they started using AI for chatbots, but then they realized, hey, it can write code, so they start shifting to code,&amp;quot; Dao said. &amp;quot;Or they realize these AIs can actually call tools and control computers and do accounting and things like that.&amp;quot;&lt;/p&gt;&lt;p&gt;In a vibe-coding session, the adaptive system can specialize for the specific codebase being edited. These are files not seen during training. This further increases acceptance rates and decoding speed.&lt;/p&gt;&lt;h2&gt;What it means for enterprises and the inference ecosystem&lt;/h2&gt;&lt;p&gt;ATLAS is available now on Together AI&amp;#x27;s dedicated endpoints as part of the platform at no additional cost. The company&amp;#x27;s 800,000-plus developers (up from 450,000 in February) have access to the optimization.&lt;/p&gt;&lt;p&gt;But the broader implications extend beyond one vendor&amp;#x27;s product. The shift from static to adaptive optimization represents a fundamental rethinking of how inference platforms should work. As enterprises deploy AI across multiple domains, the industry will need to move beyond one-time trained models toward systems that learn and improve continuously.&lt;/p&gt;&lt;p&gt;Together AI has historically released some of its research techniques as open source and collaborated with projects like vLLM. While the fully integrated ATLAS system is proprietary, some of the underlying techniques may eventually influence the broader inference ecosystem. &lt;/p&gt;&lt;p&gt;For enterprises looking to lead in AI, the message is clear: adaptive algorithms on commodity hardware can match custom silicon at a fraction of the cost. As this approach matures across the industry, software optimization increasingly trumps specialized hardware.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Enterprises expanding AI deployments are hitting an invisible performance wall. The culprit? Static speculators that can&amp;#x27;t keep up with shifting workloads.&lt;/p&gt;&lt;p&gt;Speculators are smaller AI models that work alongside large language models during inference. They draft multiple tokens ahead, which the main model then verifies in parallel. This technique (called speculative decoding) has become essential for enterprises trying to reduce inference costs and latency. Instead of generating tokens one at a time, the system can accept multiple tokens at once, dramatically improving throughput.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.together.ai/"&gt;&lt;u&gt;Together AI&lt;/u&gt;&lt;/a&gt; today announced research and a new system called ATLAS (AdapTive-LeArning Speculator System) that aims to help enterprises overcome the challenge of static speculators. The technique provides a self-learning inference optimization capability that can help to deliver up to 400% faster inference performance than a baseline level of performance available in existing inference technologies such as vLLM.. The system addresses a critical problem: as AI workloads evolve, inference speeds degrade, even with specialized speculators in place.&lt;/p&gt;&lt;p&gt;The company which&lt;a href="https://venturebeat.com/ai/ai-startup-together-raises-funding-for-open-source-ai-and-cloud-platform"&gt; &lt;u&gt;got its start&lt;/u&gt;&lt;/a&gt; in 2023, has been focused on&lt;a href="https://venturebeat.com/ai/together-ai-promises-faster-inference-and-lower-costs-with-enterprise-ai-platform-for-private-cloud"&gt; &lt;u&gt;optimizing inference&lt;/u&gt;&lt;/a&gt; on its enterprise AI platform. Earlier this year the company&lt;a href="https://venturebeat.com/ai/together-ais-305m-bet-reasoning-models-like-deepseek-r1-are-increasing-not-decreasing-gpu-demand"&gt; &lt;u&gt;raised $305 million&lt;/u&gt;&lt;/a&gt; as customer adoption and demand has grown.&lt;/p&gt;&lt;p&gt;&amp;quot;Companies we work with generally, as they scale up, they see shifting workloads, and then they don&amp;#x27;t see as much speedup from speculative execution as before,&amp;quot; Tri Dao, chief scientist at Together AI, told VentureBeat in an exclusive interview. &amp;quot;These speculators generally don&amp;#x27;t work well when their workload domain starts to shift.&amp;quot;&lt;/p&gt;&lt;h2&gt;The workload drift problem no one talks about&lt;/h2&gt;&lt;p&gt;Most speculators in production today are &amp;quot;static&amp;quot; models. They&amp;#x27;re trained once on a fixed dataset representing expected workloads, then deployed without any ability to adapt. Companies like Meta and Mistral ship pre-trained speculators alongside their main models. Inference platforms like vLLM use these static speculators to boost throughput without changing output quality.&lt;/p&gt;&lt;p&gt;But there&amp;#x27;s a catch. When an enterprise&amp;#x27;s AI usage evolves the static speculator&amp;#x27;s accuracy plummets.&lt;/p&gt;&lt;p&gt;&amp;quot;If you&amp;#x27;re a company producing coding agents, and most of your developers have been writing in Python, all of a sudden some of them switch to writing Rust or C, then you see the speed starts to go down,&amp;quot; Dao explained. &amp;quot;The speculator has a mismatch between what it was trained on versus what the actual workload is.&amp;quot;&lt;/p&gt;&lt;p&gt;This workload drift represents a hidden tax on scaling AI. Enterprises either accept degraded performance or invest in retraining custom speculators. That process captures only a snapshot in time and quickly becomes outdated.&lt;/p&gt;&lt;h2&gt;How adaptive speculators work: A dual-model approach&lt;/h2&gt;&lt;p&gt;ATLAS uses a dual-speculator architecture that combines stability with adaptation:&lt;/p&gt;&lt;p&gt;&lt;b&gt;The static speculator&lt;/b&gt; - A heavyweight model trained on broad data provides consistent baseline performance. It serves as a &amp;quot;speed floor.&amp;quot;&lt;/p&gt;&lt;p&gt;&lt;b&gt;The adaptive speculator&lt;/b&gt; - A lightweight model learns continuously from live traffic. It specializes on-the-fly to emerging domains and usage patterns.&lt;/p&gt;&lt;p&gt;&lt;b&gt;The confidence-aware controller&lt;/b&gt; - An orchestration layer dynamically chooses which speculator to use. It adjusts the speculation &amp;quot;lookahead&amp;quot; based on confidence scores.&lt;/p&gt;&lt;p&gt;&amp;quot;Before the adaptive speculator learns anything, we still have the static speculator to help provide the speed boost in the beginning,&amp;quot; Ben Athiwaratkun, staff AI scientist at Together AI explained to VentureBeat. &amp;quot;Once the adaptive speculator becomes more confident, then the speed grows over time.&amp;quot;&lt;/p&gt;&lt;p&gt;The technical innovation lies in balancing acceptance rate (how often the target model agrees with drafted tokens) and draft latency. As the adaptive model learns from traffic patterns, the controller relies more on the lightweight speculator and extends lookahead. This compounds performance gains.&lt;/p&gt;&lt;p&gt;Users don&amp;#x27;t need to tune any parameters. &amp;quot;On the user side, users don&amp;#x27;t have to turn any knobs,&amp;quot; Dao said. &amp;quot;On our side, we have turned these knobs for users to adjust in a configuration that gets good speedup.&amp;quot;&lt;/p&gt;&lt;h2&gt;Performance that rivals custom silicon&lt;/h2&gt;&lt;p&gt;Together AI&amp;#x27;s testing shows ATLAS reaching 500 tokens per second on DeepSeek-V3.1 when fully adapted. More impressively, those numbers on Nvidia B200 GPUs match or exceed specialized inference chips like&lt;a href="https://venturebeat.com/ai/groq-just-made-hugging-face-way-faster-and-its-coming-for-aws-and-google"&gt; &lt;u&gt;Groq&amp;#x27;s&lt;/u&gt;&lt;/a&gt; custom hardware.&lt;/p&gt;&lt;p&gt;&amp;quot;The software and algorithmic improvement is able to close the gap with really specialized hardware,&amp;quot; Dao said. &amp;quot;We were seeing 500 tokens per second on these huge models that are even faster than some of the customized chips.&amp;quot;&lt;/p&gt;&lt;p&gt;The 400% speedup that the company claims for inference represents the cumulative effect of Together&amp;#x27;s Turbo optimization suite. FP4 quantization delivers 80% speedup over FP8 baseline. The static Turbo Speculator adds another 80-100% gain. The adaptive system layers on top. Each optimization compounds the benefits of the others.&lt;/p&gt;&lt;p&gt;Compared to standard inference engines like&lt;a href="https://venturebeat.com/ai/how-snowflakes-open-source-text-to-sql-and-arctic-inference-models-solve-enterprise-ais-two-biggest-deployment-headaches"&gt; &lt;u&gt;vLLM&lt;/u&gt;&lt;/a&gt; or Nvidia&amp;#x27;s TensorRT-LLM, the improvement is substantial. Together AI benchmarks against the stronger baseline between the two for each workload before applying speculative optimizations.&lt;/p&gt;&lt;h2&gt;The memory-compute tradeoff explained&lt;/h2&gt;&lt;p&gt;The performance gains stem from exploiting a fundamental inefficiency in modern inference: wasted compute capacity.&lt;/p&gt;&lt;p&gt;Dao explained that typically during inference, much of the compute power is not fully utilized.&lt;/p&gt;&lt;p&gt;&amp;quot;During inference, which is actually the dominant workload nowadays, you&amp;#x27;re mostly using the memory subsystem,&amp;quot; he said.&lt;/p&gt;&lt;p&gt;Speculative decoding trades idle compute for reduced memory access. When a model generates one token at a time, it&amp;#x27;s memory-bound. The GPU sits idle while waiting for memory. But when the speculator proposes five tokens and the target model verifies them simultaneously, compute utilization spikes while memory access remains roughly constant.&lt;/p&gt;&lt;p&gt;&amp;quot;The total amount of compute to generate five tokens is the same, but you only had to access memory once, instead of five times,&amp;quot; Dao said.&lt;/p&gt;&lt;h2&gt;Think of it as intelligent caching for AI&lt;/h2&gt;&lt;p&gt;For infrastructure teams familiar with traditional database optimization, adaptive speculators function like an intelligent caching layer, but with a crucial difference.&lt;/p&gt;&lt;p&gt;Traditional caching systems like Redis or memcached require exact matches. You store the exact same query result and retrieve it when that specific query runs again. Adaptive speculators work differently.&lt;/p&gt;&lt;p&gt;&amp;quot;You can view it as an intelligent way of caching, not storing exactly, but figuring out some patterns that you see,&amp;quot; Dao explained. &amp;quot;Broadly, we&amp;#x27;re observing that you&amp;#x27;re working with similar code, or working with similar, you know, controlling compute in a similar way. We can then predict what the big model is going to say. We just get better and better at predicting that.&amp;quot;&lt;/p&gt;&lt;p&gt;Rather than storing exact responses, the system learns patterns in how the model generates tokens. It recognizes that if you&amp;#x27;re editing Python files in a specific codebase, certain token sequences become more likely. The speculator adapts to those patterns, improving its predictions over time without requiring identical inputs.&lt;/p&gt;&lt;h2&gt;Use cases: RL training and evolving workloads&lt;/h2&gt;&lt;p&gt;Two enterprise scenarios particularly benefit from adaptive speculators:&lt;/p&gt;&lt;p&gt;&lt;b&gt;Reinforcement learning training&lt;/b&gt;: Static speculators quickly fall out of alignment as the policy evolves during training. ATLAS adapts continuously to the shifting policy distribution.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Evolving workloads&lt;/b&gt;: As enterprises discover new AI use cases, workload composition shifts. &amp;quot;Maybe they started using AI for chatbots, but then they realized, hey, it can write code, so they start shifting to code,&amp;quot; Dao said. &amp;quot;Or they realize these AIs can actually call tools and control computers and do accounting and things like that.&amp;quot;&lt;/p&gt;&lt;p&gt;In a vibe-coding session, the adaptive system can specialize for the specific codebase being edited. These are files not seen during training. This further increases acceptance rates and decoding speed.&lt;/p&gt;&lt;h2&gt;What it means for enterprises and the inference ecosystem&lt;/h2&gt;&lt;p&gt;ATLAS is available now on Together AI&amp;#x27;s dedicated endpoints as part of the platform at no additional cost. The company&amp;#x27;s 800,000-plus developers (up from 450,000 in February) have access to the optimization.&lt;/p&gt;&lt;p&gt;But the broader implications extend beyond one vendor&amp;#x27;s product. The shift from static to adaptive optimization represents a fundamental rethinking of how inference platforms should work. As enterprises deploy AI across multiple domains, the industry will need to move beyond one-time trained models toward systems that learn and improve continuously.&lt;/p&gt;&lt;p&gt;Together AI has historically released some of its research techniques as open source and collaborated with projects like vLLM. While the fully integrated ATLAS system is proprietary, some of the underlying techniques may eventually influence the broader inference ecosystem. &lt;/p&gt;&lt;p&gt;For enterprises looking to lead in AI, the message is clear: adaptive algorithms on commodity hardware can match custom silicon at a fraction of the cost. As this approach matures across the industry, software optimization increasingly trumps specialized hardware.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/together-ais-atlas-adaptive-speculator-delivers-400-inference-speedup-by</guid><pubDate>Fri, 10 Oct 2025 13:00:00 +0000</pubDate></item><item><title>[NEW] The world is just not quite ready for humanoids yet (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/10/the-world-is-just-not-quite-ready-for-humanoids-yet/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Humanoids.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Famed roboticist and iRobot founder Rodney Brooks has sounded the alarm on a humanoid robot investment bubble. He’s not the only one.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a recent essay, Brooks calls out the billions of venture dollars being poured into humanoid robot companies like Figure. His take: Despite the amount of money being injected into the industry, humanoids won’t be able to learn dexterity — or the fine motor movements with hands — rendering them essentially useless.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;His take might surprise some, especially those VCs investing in the sector. But not the multiple robotics-focused VCs and AI scientists who have told TechCrunch in recent months that they don’t expect to see wide adoption of humanoid robots for at least a few years — if not more than a decade.&lt;/p&gt;

&lt;h2 class="wp-block-heading has-h-2-font-size" id="h-the-issues"&gt;The issues&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Fady Saad, a general partner at robotics-focused VC Cybernetix Ventures and former co-founder of MassRobotics, told TechCrunch that beyond sending humanoids into space in place of human astronauts, he doesn’t see a huge market yet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“People who probably haven’t seen humanoids before, or haven’t kind of been closely following what’s happening, they are impressed with what’s happening now in humanoids, but we continue to be a little bit conservative and skeptical about the actual use case and the actual revenues that will be generated,” Saad said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Saad is also concerned about safety, especially when humans and humanoid robots share the same space. Safety issues could arise from humanoids and humans working closely on a factory floor, or other industrial sites. Saad says those concerns mount when humanoids enter people’s homes — a goal many humanoid companies are working toward.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If this thing falls on pets or kids, it will hurt them,” Saad said. “This is just one aspect of a big hurdle that no one is paying attention to, or very few people are paying attention to. The other thing is, how many people are comfortable with having a humanoid in their home sitting there? What if it got hacked? What if it went crazy at night and started breaking things?”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The timeline for this technology also isn’t clear — a crucial factor for VCs who have fund lifecycles and timelines to return capital to investors.&lt;/p&gt;

&lt;h2 class="wp-block-heading has-h-2-font-size" id="h-the-timeline"&gt;The timeline&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Sanja Fidler, the vice president of AI research at Nvidia, told TechCrunch in August that while it’s hard to pin the development of humanoids to an exact timeline, she compared the current swell of interest to the excitement in the early days of self-driving cars.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I mean, look at self-driving cars, in 2017 and 2016, I mean it felt tangible, right?” Fidler said at the time. “It still took them quite a few years to really scale and even now, no one really scaled to the entire world, full autonomy. It’s hard. It’s really hard to go and fully delivery on that technology.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Nvidia chief scientist Bill Dally agreed in an interview with TechCrunch. Dally and Fidler’s comments are especially notable as Nvidia is also pouring money into developing the infrastructure for humanoid companies to follow.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Seth Winterroth, a partner at Eclipse, said while it can be easy to get excited as each new technological development happens, or the latest demo drops, humanoids are incredibly complicated. He added that it will be a while before they reach their full capabilities.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s difficult to do software releases to six degrees of freedom systems; what we’re talking about with some of these humanoids is 60-plus degrees of freedom systems,” Winterroth said, regarding a robot’s ability to move on a 3D axis. “Then you need to be able to have good unit economics around that solution, such that you’ve got strong gross margin, such that you can be building an enduring business. I think we’re pretty early.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In most cases, humanoid robots aren’t ready for the world yet, either.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tesla is a great example of the struggles companies are running into. The company announced it was building its humanoid, Optimus, back in 2021. The following year, Tesla said the bot would be introduced in 2023.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That didn’t happen. When the bot was introduced in 2024 at Tesla’s “We, Robot” event, it was revealed later that the bots were largely being controlled by humans off scene. The company claims it will start selling the bots in 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Robotics startup Figure, which was valued at $39 billion in a September fundraise, has also drawn skepticism regarding how many of its humanoids the company has actually deployed, a claim the company staunchly defends.&lt;/p&gt;

&lt;h2 class="wp-block-heading has-h-2-font-size" id="h-what-is-working"&gt;What is working&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;That doesn’t mean humanoids won’t have a future market or that the technology is not worth working on.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Brooks himself said he doesn’t doubt that we will have humanoids in the future. But instead of what the market pictures when they hear humanoids, a robot with a human form, he predicts they’ll likely have wheels and other inhuman features and won’t be coming out for more than a decade.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There are startups working on the dexterity technology Brooks is skeptical humanoids will be able to reach, including Y Combinator-backed Proception and Loomia, which built a kit that can help robotics companies start to incorporate touch into their machines.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There are also numerous humanoid companies that are starting to take orders and gather interest in their robots. K-Scale Labs received more than 100 preorders for its humanoid bot in the first five days, surprising even the founders, CEO Benjamin Bolte told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hugging Face has also seen strong demand from developers for its two humanoid bots. The company opened up preorders for its smaller desktop version, the Reachy Mini, in July. The reaction was palpable. Just five days after&amp;nbsp;opening orders on its Reachy Mini robots, Hugging Face had logged $1 million worth of sales.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Humanoids.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Famed roboticist and iRobot founder Rodney Brooks has sounded the alarm on a humanoid robot investment bubble. He’s not the only one.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a recent essay, Brooks calls out the billions of venture dollars being poured into humanoid robot companies like Figure. His take: Despite the amount of money being injected into the industry, humanoids won’t be able to learn dexterity — or the fine motor movements with hands — rendering them essentially useless.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;His take might surprise some, especially those VCs investing in the sector. But not the multiple robotics-focused VCs and AI scientists who have told TechCrunch in recent months that they don’t expect to see wide adoption of humanoid robots for at least a few years — if not more than a decade.&lt;/p&gt;

&lt;h2 class="wp-block-heading has-h-2-font-size" id="h-the-issues"&gt;The issues&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Fady Saad, a general partner at robotics-focused VC Cybernetix Ventures and former co-founder of MassRobotics, told TechCrunch that beyond sending humanoids into space in place of human astronauts, he doesn’t see a huge market yet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“People who probably haven’t seen humanoids before, or haven’t kind of been closely following what’s happening, they are impressed with what’s happening now in humanoids, but we continue to be a little bit conservative and skeptical about the actual use case and the actual revenues that will be generated,” Saad said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Saad is also concerned about safety, especially when humans and humanoid robots share the same space. Safety issues could arise from humanoids and humans working closely on a factory floor, or other industrial sites. Saad says those concerns mount when humanoids enter people’s homes — a goal many humanoid companies are working toward.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If this thing falls on pets or kids, it will hurt them,” Saad said. “This is just one aspect of a big hurdle that no one is paying attention to, or very few people are paying attention to. The other thing is, how many people are comfortable with having a humanoid in their home sitting there? What if it got hacked? What if it went crazy at night and started breaking things?”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The timeline for this technology also isn’t clear — a crucial factor for VCs who have fund lifecycles and timelines to return capital to investors.&lt;/p&gt;

&lt;h2 class="wp-block-heading has-h-2-font-size" id="h-the-timeline"&gt;The timeline&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Sanja Fidler, the vice president of AI research at Nvidia, told TechCrunch in August that while it’s hard to pin the development of humanoids to an exact timeline, she compared the current swell of interest to the excitement in the early days of self-driving cars.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I mean, look at self-driving cars, in 2017 and 2016, I mean it felt tangible, right?” Fidler said at the time. “It still took them quite a few years to really scale and even now, no one really scaled to the entire world, full autonomy. It’s hard. It’s really hard to go and fully delivery on that technology.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Nvidia chief scientist Bill Dally agreed in an interview with TechCrunch. Dally and Fidler’s comments are especially notable as Nvidia is also pouring money into developing the infrastructure for humanoid companies to follow.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Seth Winterroth, a partner at Eclipse, said while it can be easy to get excited as each new technological development happens, or the latest demo drops, humanoids are incredibly complicated. He added that it will be a while before they reach their full capabilities.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s difficult to do software releases to six degrees of freedom systems; what we’re talking about with some of these humanoids is 60-plus degrees of freedom systems,” Winterroth said, regarding a robot’s ability to move on a 3D axis. “Then you need to be able to have good unit economics around that solution, such that you’ve got strong gross margin, such that you can be building an enduring business. I think we’re pretty early.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In most cases, humanoid robots aren’t ready for the world yet, either.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tesla is a great example of the struggles companies are running into. The company announced it was building its humanoid, Optimus, back in 2021. The following year, Tesla said the bot would be introduced in 2023.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That didn’t happen. When the bot was introduced in 2024 at Tesla’s “We, Robot” event, it was revealed later that the bots were largely being controlled by humans off scene. The company claims it will start selling the bots in 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Robotics startup Figure, which was valued at $39 billion in a September fundraise, has also drawn skepticism regarding how many of its humanoids the company has actually deployed, a claim the company staunchly defends.&lt;/p&gt;

&lt;h2 class="wp-block-heading has-h-2-font-size" id="h-what-is-working"&gt;What is working&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;That doesn’t mean humanoids won’t have a future market or that the technology is not worth working on.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Brooks himself said he doesn’t doubt that we will have humanoids in the future. But instead of what the market pictures when they hear humanoids, a robot with a human form, he predicts they’ll likely have wheels and other inhuman features and won’t be coming out for more than a decade.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There are startups working on the dexterity technology Brooks is skeptical humanoids will be able to reach, including Y Combinator-backed Proception and Loomia, which built a kit that can help robotics companies start to incorporate touch into their machines.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There are also numerous humanoid companies that are starting to take orders and gather interest in their robots. K-Scale Labs received more than 100 preorders for its humanoid bot in the first five days, surprising even the founders, CEO Benjamin Bolte told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hugging Face has also seen strong demand from developers for its two humanoid bots. The company opened up preorders for its smaller desktop version, the Reachy Mini, in July. The reaction was palpable. Just five days after&amp;nbsp;opening orders on its Reachy Mini robots, Hugging Face had logged $1 million worth of sales.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/10/the-world-is-just-not-quite-ready-for-humanoids-yet/</guid><pubDate>Fri, 10 Oct 2025 13:15:00 +0000</pubDate></item><item><title>[NEW] Former UK Prime Minister Rishi Sunak to advise Microsoft and Anthropic (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/10/former-uk-prime-minister-rishi-sunak-to-advise-microsoft-and-anthropic/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2021/09/Rishi-Sunak.jpg?w=799" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Rishi Sunak, who served as the United Kingdom’s prime minister from 2022 to 2024, has taken on senior advisory roles at Microsoft and Anthropic,&amp;nbsp;The Guardian reports.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Letters&amp;nbsp;from the Parliament’s office of the Advisory Committee on Business Appointments (Acoba) disclosing Sunak’s appointments revealed concerns that the ex-Conservative PM’s privileged information could “grant Microsoft an unfair advantage.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Sunak has some history with Microsoft, which has several active contracts with British government departments. In 2023, he&amp;nbsp;unveiled a £2.5 billion deal with&amp;nbsp;Microsoft&amp;nbsp;to invest in new data centers and training in the U.K.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Acoba also&amp;nbsp;noted&amp;nbsp;that “there is a reasonable concern that your appointment could be seen to offer unfair access and influence within the U.K. government…given the ongoing debate about how to best regulate AI, and at a time of intense debate and lobbying around the world on what the approach should be.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sunak said he would steer clear on advising on U.K. policy matters, stick to high-level perspectives on macro-economic and geopolitical trends, and avoid lobbying. He said he would divert his salary to the Richmond Project, a charity he founded with his wife earlier this year.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The former PM also serves as a senior advisor to investment bank Goldman Sachs and speech writer for firms like Bain Capital and Makena Capital.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sunak&amp;nbsp;isn’t&amp;nbsp;the first British politician to take on roles helping Silicon Valley tech giants navigate governmental affairs. Sunak’s senior political adviser, Liam Booth-Smith, is also on Anthropic’s payroll. And former Liberal Democrat Deputy Prime Minister Nick Clegg served as Meta’s president of global affairs until January 2025. &amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In the United States,&amp;nbsp;the revolving door between Silicon Valley and the U.S. government is&amp;nbsp;ever-active. At Meta, Clegg was replaced by Joel Kaplan, George W. Bush’s former deputy chief of staff, and Dustin Carmack, a former adviser to Florida Governor Ron DeSantis, joined the firm’s policy team in 2024. Microsoft’s current president of global affairs is Lisa Monaco, former deputy attorney general under Joe Biden.&amp;nbsp;&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2021/09/Rishi-Sunak.jpg?w=799" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Rishi Sunak, who served as the United Kingdom’s prime minister from 2022 to 2024, has taken on senior advisory roles at Microsoft and Anthropic,&amp;nbsp;The Guardian reports.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Letters&amp;nbsp;from the Parliament’s office of the Advisory Committee on Business Appointments (Acoba) disclosing Sunak’s appointments revealed concerns that the ex-Conservative PM’s privileged information could “grant Microsoft an unfair advantage.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Sunak has some history with Microsoft, which has several active contracts with British government departments. In 2023, he&amp;nbsp;unveiled a £2.5 billion deal with&amp;nbsp;Microsoft&amp;nbsp;to invest in new data centers and training in the U.K.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Acoba also&amp;nbsp;noted&amp;nbsp;that “there is a reasonable concern that your appointment could be seen to offer unfair access and influence within the U.K. government…given the ongoing debate about how to best regulate AI, and at a time of intense debate and lobbying around the world on what the approach should be.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sunak said he would steer clear on advising on U.K. policy matters, stick to high-level perspectives on macro-economic and geopolitical trends, and avoid lobbying. He said he would divert his salary to the Richmond Project, a charity he founded with his wife earlier this year.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The former PM also serves as a senior advisor to investment bank Goldman Sachs and speech writer for firms like Bain Capital and Makena Capital.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sunak&amp;nbsp;isn’t&amp;nbsp;the first British politician to take on roles helping Silicon Valley tech giants navigate governmental affairs. Sunak’s senior political adviser, Liam Booth-Smith, is also on Anthropic’s payroll. And former Liberal Democrat Deputy Prime Minister Nick Clegg served as Meta’s president of global affairs until January 2025. &amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In the United States,&amp;nbsp;the revolving door between Silicon Valley and the U.S. government is&amp;nbsp;ever-active. At Meta, Clegg was replaced by Joel Kaplan, George W. Bush’s former deputy chief of staff, and Dustin Carmack, a former adviser to Florida Governor Ron DeSantis, joined the firm’s policy team in 2024. Microsoft’s current president of global affairs is Lisa Monaco, former deputy attorney general under Joe Biden.&amp;nbsp;&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/10/former-uk-prime-minister-rishi-sunak-to-advise-microsoft-and-anthropic/</guid><pubDate>Fri, 10 Oct 2025 14:22:48 +0000</pubDate></item><item><title>[NEW] OpenAI will stop saving most ChatGPT users’ deleted chats (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/10/openai-no-longer-forced-to-save-deleted-chats-but-some-users-still-affected/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Court ends controversial order forcing OpenAI to save deleted ChatGPT logs.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="384" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1386719483-640x384.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1386719483-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          cokada | iStock / Getty Images Plus

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;OpenAI will finally stop saving most ChatGPT users' deleted and temporary chats after a court fight compelled the AI firm to retain the logs "indefinitely."&lt;/p&gt;
&lt;p&gt;The preservation order came in a lawsuit filed by The New York Times and other news plaintiffs, who alleged that user attempts to skirt paywalls with ChatGPT would most likely set their chats as temporary or delete the logs.&lt;/p&gt;
&lt;p&gt;OpenAI vowed to fight the order, defending its policies and users' privacy, but it lost. By July, news plaintiffs started digging through the logs—which only preserved ChatGPT's outputs—while a few ChatGPT users' efforts to intervene were consistently denied, as they were deemed non-parties to the lawsuit.&lt;/p&gt;
&lt;p&gt;In an order on Thursday, US Magistrate Judge Ona Wang approved a joint motion from news organizations and OpenAI to terminate the preservation order, although some ChatGPT users' deleted and temporary chats will still be monitored.&lt;/p&gt;
&lt;p&gt;Under the agreement, OpenAI was allowed to stop the controversial practice of preserving "all output log data that would otherwise be deleted" on September 26.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Moving forward, all of the deleted and temporary chats that were previously saved under the preservation order will continue to be accessible to news plaintiffs, who are looking for examples of outputs infringing their articles or attributing misinformation to their publications.&lt;/p&gt;
&lt;p&gt;Additionally, OpenAI will continue monitoring certain ChatGPT accounts, saving deleted and temporary chats of any users whose domains have been flagged by news organizations since they began searching through the data. If news plaintiffs flag additional domains during future meetings with OpenAI, more accounts could be roped in.&lt;/p&gt;
&lt;p&gt;Ars could not immediately reach OpenAI or the Times' legal team for comment.&lt;/p&gt;
&lt;p&gt;The dispute with news plaintiffs continues to heat up beyond the battle over user logs, most recently with co-defendant Microsoft pushing to keep its AI companion Copilot out of the litigation.&lt;/p&gt;
&lt;p&gt;The stakes remain high for both sides. News organizations have alleged that ChatGPT and other allegedly copyright-infringing tools threaten to replace them in their market while potentially damaging their reputations by attributing false information to them.&lt;/p&gt;
&lt;p&gt;OpenAI may be increasingly pressured to settle the lawsuit, and not by news organizations but by insurance companies that won't provide comprehensive coverage for their AI products with multiple potentially multibillion-dollar lawsuits pending.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Court ends controversial order forcing OpenAI to save deleted ChatGPT logs.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="384" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1386719483-640x384.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1386719483-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          cokada | iStock / Getty Images Plus

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;OpenAI will finally stop saving most ChatGPT users' deleted and temporary chats after a court fight compelled the AI firm to retain the logs "indefinitely."&lt;/p&gt;
&lt;p&gt;The preservation order came in a lawsuit filed by The New York Times and other news plaintiffs, who alleged that user attempts to skirt paywalls with ChatGPT would most likely set their chats as temporary or delete the logs.&lt;/p&gt;
&lt;p&gt;OpenAI vowed to fight the order, defending its policies and users' privacy, but it lost. By July, news plaintiffs started digging through the logs—which only preserved ChatGPT's outputs—while a few ChatGPT users' efforts to intervene were consistently denied, as they were deemed non-parties to the lawsuit.&lt;/p&gt;
&lt;p&gt;In an order on Thursday, US Magistrate Judge Ona Wang approved a joint motion from news organizations and OpenAI to terminate the preservation order, although some ChatGPT users' deleted and temporary chats will still be monitored.&lt;/p&gt;
&lt;p&gt;Under the agreement, OpenAI was allowed to stop the controversial practice of preserving "all output log data that would otherwise be deleted" on September 26.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Moving forward, all of the deleted and temporary chats that were previously saved under the preservation order will continue to be accessible to news plaintiffs, who are looking for examples of outputs infringing their articles or attributing misinformation to their publications.&lt;/p&gt;
&lt;p&gt;Additionally, OpenAI will continue monitoring certain ChatGPT accounts, saving deleted and temporary chats of any users whose domains have been flagged by news organizations since they began searching through the data. If news plaintiffs flag additional domains during future meetings with OpenAI, more accounts could be roped in.&lt;/p&gt;
&lt;p&gt;Ars could not immediately reach OpenAI or the Times' legal team for comment.&lt;/p&gt;
&lt;p&gt;The dispute with news plaintiffs continues to heat up beyond the battle over user logs, most recently with co-defendant Microsoft pushing to keep its AI companion Copilot out of the litigation.&lt;/p&gt;
&lt;p&gt;The stakes remain high for both sides. News organizations have alleged that ChatGPT and other allegedly copyright-infringing tools threaten to replace them in their market while potentially damaging their reputations by attributing false information to them.&lt;/p&gt;
&lt;p&gt;OpenAI may be increasingly pressured to settle the lawsuit, and not by news organizations but by insurance companies that won't provide comprehensive coverage for their AI products with multiple potentially multibillion-dollar lawsuits pending.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/10/openai-no-longer-forced-to-save-deleted-chats-but-some-users-still-affected/</guid><pubDate>Fri, 10 Oct 2025 14:58:29 +0000</pubDate></item><item><title>[NEW] You can now connect your Spotify account to ChatGPT: Here’s how to do it (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/10/you-can-now-connect-your-spotify-account-to-chatgpt-heres-how-to-do-it/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Earlier this week, OpenAI launched a way for developers to build apps within ChatGPT, allowing users to prompt the assistant to perform tasks and answer questions directly related to the participating apps.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Several companies have launched their interactive experiences, including Spotify, allowing listeners to ask ChatGPT to create playlists, recommend songs, and more.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;When you mention Spotify in your ChatGPT prompt, you’ll have the option to connect your account. A button that says “Use Spotify for this answer” will appear at the bottom of the page. Connecting your account allows the AI to access your data, such as your likes and listening history, providing relevant context that helps generate better answers.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3056403" height="357" src="https://techcrunch.com/wp-content/uploads/2025/10/chatgpt-spotify.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;ChatGPT (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;From there, you can ask it for recommendations for tracks, artists, playlists, or podcast episodes based on your tastes. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;You can be as detailed as you want in your prompts. For instance, you can create a playlist with a specific mood in mind, choose the event you want the playlist for, or request to include only your most listened-to artists, your top favorite genre, and so on. It also asks you for the preferred length, which is helpful for planning a long road trip playlist.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During our testing, we asked it to create a dog-walking playlist but all the songs needed to have “Dog” in the title. Here is what it came up with:&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3056420" height="380" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-10-at-11.16.56AM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Spotify (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Tapping a suggested track or podcast episode will open the Spotify app, allowing you to listen or watch directly from there.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;ChatGPT also has the capability to perform actions on your behalf within Spotify, including controlling playback; adding and removing items from your library; creating, editing, and following private playlists; and managing your following list.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s important to note that when you’re connecting your Spotify account, you agree to the app’s privacy policy.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This policy includes important information about data sharing and security, stating that if ChatGPT experiences a security breach, there is a risk that your data could be accessed by unauthorized parties. In addition to giving ChatGPT access to data — such as what songs and podcasts you’re playing, what is saved in your Spotify library, and who you follow — you’re also providing your IP address and approximate location.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, you can disconnect your Spotify account at any time. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, Spotify explained in a blog post that it doesn’t share content from artists and creators with OpenAI for training purposes. All music, podcasts, or any other audio or video content on the platform stays protected.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3056402" height="373" src="https://techcrunch.com/wp-content/uploads/2025/10/spotify-chatgpt-privacy.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Spotify (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The feature is available in English across 145 countries for all ChatGPT Free, Plus, and Pro users on web and mobile. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both free and premium Spotify users can use the integration; however, the company says premium users get a more tailored experience. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other companies have also integrated their apps into ChatGPT, including Booking.com, Canva, Coursera, Expedia, Figma, and Zillow. The integrations allow you to, for example, ask Coursera to teach you something or search on Zillow for apartments in your area within a specific price range.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Earlier this week, OpenAI launched a way for developers to build apps within ChatGPT, allowing users to prompt the assistant to perform tasks and answer questions directly related to the participating apps.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Several companies have launched their interactive experiences, including Spotify, allowing listeners to ask ChatGPT to create playlists, recommend songs, and more.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;When you mention Spotify in your ChatGPT prompt, you’ll have the option to connect your account. A button that says “Use Spotify for this answer” will appear at the bottom of the page. Connecting your account allows the AI to access your data, such as your likes and listening history, providing relevant context that helps generate better answers.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3056403" height="357" src="https://techcrunch.com/wp-content/uploads/2025/10/chatgpt-spotify.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;ChatGPT (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;From there, you can ask it for recommendations for tracks, artists, playlists, or podcast episodes based on your tastes. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;You can be as detailed as you want in your prompts. For instance, you can create a playlist with a specific mood in mind, choose the event you want the playlist for, or request to include only your most listened-to artists, your top favorite genre, and so on. It also asks you for the preferred length, which is helpful for planning a long road trip playlist.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During our testing, we asked it to create a dog-walking playlist but all the songs needed to have “Dog” in the title. Here is what it came up with:&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3056420" height="380" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-10-at-11.16.56AM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Spotify (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Tapping a suggested track or podcast episode will open the Spotify app, allowing you to listen or watch directly from there.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;ChatGPT also has the capability to perform actions on your behalf within Spotify, including controlling playback; adding and removing items from your library; creating, editing, and following private playlists; and managing your following list.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s important to note that when you’re connecting your Spotify account, you agree to the app’s privacy policy.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This policy includes important information about data sharing and security, stating that if ChatGPT experiences a security breach, there is a risk that your data could be accessed by unauthorized parties. In addition to giving ChatGPT access to data — such as what songs and podcasts you’re playing, what is saved in your Spotify library, and who you follow — you’re also providing your IP address and approximate location.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, you can disconnect your Spotify account at any time. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, Spotify explained in a blog post that it doesn’t share content from artists and creators with OpenAI for training purposes. All music, podcasts, or any other audio or video content on the platform stays protected.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3056402" height="373" src="https://techcrunch.com/wp-content/uploads/2025/10/spotify-chatgpt-privacy.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Spotify (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The feature is available in English across 145 countries for all ChatGPT Free, Plus, and Pro users on web and mobile. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both free and premium Spotify users can use the integration; however, the company says premium users get a more tailored experience. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other companies have also integrated their apps into ChatGPT, including Booking.com, Canva, Coursera, Expedia, Figma, and Zillow. The integrations allow you to, for example, ask Coursera to teach you something or search on Zillow for apartments in your area within a specific price range.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/10/you-can-now-connect-your-spotify-account-to-chatgpt-heres-how-to-do-it/</guid><pubDate>Fri, 10 Oct 2025 15:31:31 +0000</pubDate></item><item><title>[NEW] Instagram head Adam Mosseri pushes back on MrBeast’s AI fears but admits society will have to adjust (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/10/instagram-head-adam-mosseri-pushes-back-on-mrbeasts-ai-fears-but-admits-society-will-have-to-adjust/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/adam-mosseri-at-bloomberg-screentime.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Instagram head Adam Mosseri said AI will change who can be creative, as the new tools and technology will give people who couldn’t be creators before the ability to produce content at a certain quality and scale. However, he also admitted that bad actors will use the technology for “nefarious purposes” and that kids growing up today will have to be taught that you can’t believe something just because you saw a video of it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Meta executive shared his thoughts on how AI is impacting the creator industry at the Bloomberg Screentime conference this week. At the interview’s start, Mosseri was asked to address the recent comments from creator MrBeast (Jimmy Donaldson). On Threads, MrBeast had suggested that AI-generated videos could soon threaten creators’ livelihoods and said it was “scary times” for the industry.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Mosseri pushed back a bit at that idea, noting that most creators won’t be using AI technology to reproduce what MrBeast has historically done, with his huge sets and elaborate productions; instead, it will allow creators to do more and make better content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If you take a big step back, what the internet did, among other things, was allow almost anyone to become a publisher by reducing the cost of distributing content to essentially zero,” Mosseri explained. “And what some of these generative AI models look like they’re going to do is they’re going to reduce the cost of producing content to basically zero,” he said. (This, of course, does not reflect the true financial, environmental, and human costs of using AI, which are substantial.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, the exec suggested that there’s already a lot of “hybrid” content on today’s big social platforms, where creators are using AI in their workflow but not producing fully synthetic content. For instance, they might be using AI tools for color corrections or filters. Going forward, Mosseri said, the line between what’s real and what’s AI generated will become even more blurred.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s going to be a little bit less like, what is organic content and what is AI synthetic content, and what the percentages are. I think there’s gonna be actually more in the middle than pure synthetic content for a while,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As things change, Mosseri said Meta has some responsibility to do more in terms of identifying what content is AI generated. But he also noted that the way the company had gone about this wasn’t the “right focus” and was practically “a fool’s errand.” He was referring to how Meta had initially tried to label AI content automatically, which led to a situation where it was labeling real content as AI, because AI tools, including those from Adobe, were used as part of the process.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The executive said that the labeling system needs more work but that Meta should also provide more context that helps people make informed decisions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While he didn’t elaborate on what that newly added context would be, he may have been thinking about Meta’s Community Notes feature, which is the crowdsourced fact-checking system launched in the U.S. this year, modeled on the one X uses. Instead of turning to third-party fact checkers, Community Notes and similar systems mark content with corrections or additional context when users who often share opposing opinions agree that a fact-check or further explanation is needed. It’s likely that Meta could be weighing the use of such a system for flagging when something is AI generated but hasn’t been labeled as such.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rather than saying it was fully the platform’s responsibility to label AI content, Mosseri suggested that society itself would have to change. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“My kids are young. They’re nine, seven, and five. I need them to understand, as they grow up and they get exposed to the internet, that just because they’re seeing a video of something doesn’t mean it actually happened,” he explained. “When I grew up, and I saw a video, I could assume that that was a capture of a moment that happened in the real world,” Mosseri continued. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“What they’re going to&amp;nbsp;… need to think about who is saying it, who’s sharing it, in this case, and what are their incentives, and why might they be saying it,” he concluded. (That seems like a heavy mental load for young children, but alas.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the discussion, Mosseri also touched on other topics about the future of Instagram beyond AI, including its plans for a dedicated TV app and its newer focus on Reels and DMs as its core features (which Mosseri said just reflected user trends), and how TikTok’s changing ownership in the U.S. will impact the competitive landscape.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On the latter, he said that, ultimately, it’s better to have competition, as TikTok’s U.S. presence has forced Instagram to “do better work.” As for the TikTok deal itself, Mosseri said it’s hard to parse, but it seems like how the app has been built will not meaningfully change.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s the same app, the same ranking system, the same creators that you’re following — the same people. It’s all sort of seamless,” Mosseri said of the “new” TikTok U.S. operation. “It doesn’t seem like it’s a major change in terms of incentives,” he added. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/adam-mosseri-at-bloomberg-screentime.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Instagram head Adam Mosseri said AI will change who can be creative, as the new tools and technology will give people who couldn’t be creators before the ability to produce content at a certain quality and scale. However, he also admitted that bad actors will use the technology for “nefarious purposes” and that kids growing up today will have to be taught that you can’t believe something just because you saw a video of it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Meta executive shared his thoughts on how AI is impacting the creator industry at the Bloomberg Screentime conference this week. At the interview’s start, Mosseri was asked to address the recent comments from creator MrBeast (Jimmy Donaldson). On Threads, MrBeast had suggested that AI-generated videos could soon threaten creators’ livelihoods and said it was “scary times” for the industry.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Mosseri pushed back a bit at that idea, noting that most creators won’t be using AI technology to reproduce what MrBeast has historically done, with his huge sets and elaborate productions; instead, it will allow creators to do more and make better content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If you take a big step back, what the internet did, among other things, was allow almost anyone to become a publisher by reducing the cost of distributing content to essentially zero,” Mosseri explained. “And what some of these generative AI models look like they’re going to do is they’re going to reduce the cost of producing content to basically zero,” he said. (This, of course, does not reflect the true financial, environmental, and human costs of using AI, which are substantial.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, the exec suggested that there’s already a lot of “hybrid” content on today’s big social platforms, where creators are using AI in their workflow but not producing fully synthetic content. For instance, they might be using AI tools for color corrections or filters. Going forward, Mosseri said, the line between what’s real and what’s AI generated will become even more blurred.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s going to be a little bit less like, what is organic content and what is AI synthetic content, and what the percentages are. I think there’s gonna be actually more in the middle than pure synthetic content for a while,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As things change, Mosseri said Meta has some responsibility to do more in terms of identifying what content is AI generated. But he also noted that the way the company had gone about this wasn’t the “right focus” and was practically “a fool’s errand.” He was referring to how Meta had initially tried to label AI content automatically, which led to a situation where it was labeling real content as AI, because AI tools, including those from Adobe, were used as part of the process.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The executive said that the labeling system needs more work but that Meta should also provide more context that helps people make informed decisions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While he didn’t elaborate on what that newly added context would be, he may have been thinking about Meta’s Community Notes feature, which is the crowdsourced fact-checking system launched in the U.S. this year, modeled on the one X uses. Instead of turning to third-party fact checkers, Community Notes and similar systems mark content with corrections or additional context when users who often share opposing opinions agree that a fact-check or further explanation is needed. It’s likely that Meta could be weighing the use of such a system for flagging when something is AI generated but hasn’t been labeled as such.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rather than saying it was fully the platform’s responsibility to label AI content, Mosseri suggested that society itself would have to change. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“My kids are young. They’re nine, seven, and five. I need them to understand, as they grow up and they get exposed to the internet, that just because they’re seeing a video of something doesn’t mean it actually happened,” he explained. “When I grew up, and I saw a video, I could assume that that was a capture of a moment that happened in the real world,” Mosseri continued. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“What they’re going to&amp;nbsp;… need to think about who is saying it, who’s sharing it, in this case, and what are their incentives, and why might they be saying it,” he concluded. (That seems like a heavy mental load for young children, but alas.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the discussion, Mosseri also touched on other topics about the future of Instagram beyond AI, including its plans for a dedicated TV app and its newer focus on Reels and DMs as its core features (which Mosseri said just reflected user trends), and how TikTok’s changing ownership in the U.S. will impact the competitive landscape.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On the latter, he said that, ultimately, it’s better to have competition, as TikTok’s U.S. presence has forced Instagram to “do better work.” As for the TikTok deal itself, Mosseri said it’s hard to parse, but it seems like how the app has been built will not meaningfully change.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s the same app, the same ranking system, the same creators that you’re following — the same people. It’s all sort of seamless,” Mosseri said of the “new” TikTok U.S. operation. “It doesn’t seem like it’s a major change in terms of incentives,” he added. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/10/instagram-head-adam-mosseri-pushes-back-on-mrbeasts-ai-fears-but-admits-society-will-have-to-adjust/</guid><pubDate>Fri, 10 Oct 2025 15:55:17 +0000</pubDate></item><item><title>[NEW] Building connected data ecosystems for AI at scale (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/10/10/1124313/building-connected-data-ecosystems-for-ai-at-scale/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/iStock-853701224.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;SAP&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/iStock-853701224.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;SAP&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/10/10/1124313/building-connected-data-ecosystems-for-ai-at-scale/</guid><pubDate>Fri, 10 Oct 2025 16:11:22 +0000</pubDate></item><item><title>[NEW] Prezent raises $30 million to acquire AI services firms — starting with founder’s other company (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/10/prezent-raises-30-million-to-acquire-ai-services-firms-starting-with-founders-other-company/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Prezent, a startup that offers an AI-powered presentation builder to enterprises, said today that it has raised $30 million in funding led by Multiplier Capital, Greycroft, and Nomura Strategic Ventures, with participation from existing investors like Emergent Ventures, WestWave Capital, and Alumni Ventures.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Los Altos, California, startup, which is now valued at $400 million, has raised over $74 million to date. It plans to use the new capital largely for acquisitions.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company has also made its first acquisition, buying Prezentium, a services-led presentation company that operates in the life sciences vertical. In an unusual arrangement, Prezent founder Rahul Mishra was one of the co-founders of Prezentium. Mishra said that since the beginning of his new startup, he has been the non-operating president of Prezentium. Both startups already have an existing relationship, as Prezent uses Prezentium as a go-to-market partner. The acquisition essentially brings Mishra’s two ventures under one roof, allowing Prezent to use Prezentium’s client base to offer its own AI suite to more customers.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3056349" height="453" src="https://techcrunch.com/wp-content/uploads/2025/10/prezent_slide-library_laptop@2x.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Prezent&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;There are plenty of companies trying to create AI-powered presentation-building tools, including Presentations.ai, Lica, Gamma, and Chronicle. Notably, all these companies have Accel as a backer. While most of them focus on consumers and small business users, Prezent wants to stick to an enterprise strategy, targeting larger corporations. Previously, the company stated that it wants to expand its offerings to different verticals, including finance and manufacturing. However, Mishra noted that for now, the company wants to focus on serving clients in the life sciences and tech industries.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There are plenty of tools that are trying to make presentations pretty. We want to provide the best tools for business communications. I think presentation is one of the frontiers in businesses that is not automated yet. We want to help data scientists and designers to communicate effectively with this automation,” Mishra told TechCrunch.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3056487" height="680" src="https://techcrunch.com/wp-content/uploads/2025/10/Rajat-Head-Shot-Q3-2025.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Prezent founder Rajat Mishra&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Prezent&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Mishra, who previously worked at McKinsey, said that the company is taking a specialized approach to training AI models for presentations for each specific industry.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Prezent places a presentation engineer, who is familiar with the industry and the startup’s own offerings, in enterprises to help different people within the company get used to building presentations with AI tools.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“The reality of AI in enterprise is that while AI can do many things, it can’t teach people [how] to use AI. That is why we want to place presentation engineers in companies to help our customers adopt the product faster,” Mishra said.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3056350" height="453" src="https://techcrunch.com/wp-content/uploads/2025/10/prezent_auto-generator_laptop@2x.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Prezent&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;AI startups are looking to acquire services companies to take advantage of existing customer bases and also provide better customization to enterprise clients. Digital avatar startup D-ID acquired Berlin-based video startup Simpleshow, and Google-backed legal tech startup Lawhive acquired a U.K.-based law firm. With these acquisitions, AI startups want to combine their AI tools with the existing domain expertise and client service capabilities of services firms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Prezent wants to build more partnerships and acquire suitable companies in sectors like executive communication coaching, medical writing, and consulting firms in the communication area. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Mark Terbeek, a partner at Greycroft who has invested in multiple rounds in Prezent, said that the venture firm likes to find the areas where businesses have used costly agencies in the past to fulfill a need, and now there are AI tools available trying to do the same tasks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We felt that Rajat and Prezent were concentrating hard on solving specific customer needs of business communications. Plus, we saw software evolve quickly to adjust to the workflows of end users and help them save time,” he told TechCrunch over a call.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Going forward, Prezent wants to add better personalization to its product so the AI tool learns the style of each individual in the organization. It also wants to add multimodality capabilities to deck creations, allowing users to use text, voice, or video as input to make presentations. Like Synthesia and D-ID, Prezent wants to add digital avatars to presentations, too.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Prezent, a startup that offers an AI-powered presentation builder to enterprises, said today that it has raised $30 million in funding led by Multiplier Capital, Greycroft, and Nomura Strategic Ventures, with participation from existing investors like Emergent Ventures, WestWave Capital, and Alumni Ventures.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Los Altos, California, startup, which is now valued at $400 million, has raised over $74 million to date. It plans to use the new capital largely for acquisitions.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company has also made its first acquisition, buying Prezentium, a services-led presentation company that operates in the life sciences vertical. In an unusual arrangement, Prezent founder Rahul Mishra was one of the co-founders of Prezentium. Mishra said that since the beginning of his new startup, he has been the non-operating president of Prezentium. Both startups already have an existing relationship, as Prezent uses Prezentium as a go-to-market partner. The acquisition essentially brings Mishra’s two ventures under one roof, allowing Prezent to use Prezentium’s client base to offer its own AI suite to more customers.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3056349" height="453" src="https://techcrunch.com/wp-content/uploads/2025/10/prezent_slide-library_laptop@2x.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Prezent&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;There are plenty of companies trying to create AI-powered presentation-building tools, including Presentations.ai, Lica, Gamma, and Chronicle. Notably, all these companies have Accel as a backer. While most of them focus on consumers and small business users, Prezent wants to stick to an enterprise strategy, targeting larger corporations. Previously, the company stated that it wants to expand its offerings to different verticals, including finance and manufacturing. However, Mishra noted that for now, the company wants to focus on serving clients in the life sciences and tech industries.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There are plenty of tools that are trying to make presentations pretty. We want to provide the best tools for business communications. I think presentation is one of the frontiers in businesses that is not automated yet. We want to help data scientists and designers to communicate effectively with this automation,” Mishra told TechCrunch.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3056487" height="680" src="https://techcrunch.com/wp-content/uploads/2025/10/Rajat-Head-Shot-Q3-2025.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Prezent founder Rajat Mishra&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Prezent&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Mishra, who previously worked at McKinsey, said that the company is taking a specialized approach to training AI models for presentations for each specific industry.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Prezent places a presentation engineer, who is familiar with the industry and the startup’s own offerings, in enterprises to help different people within the company get used to building presentations with AI tools.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“The reality of AI in enterprise is that while AI can do many things, it can’t teach people [how] to use AI. That is why we want to place presentation engineers in companies to help our customers adopt the product faster,” Mishra said.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3056350" height="453" src="https://techcrunch.com/wp-content/uploads/2025/10/prezent_auto-generator_laptop@2x.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Prezent&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;AI startups are looking to acquire services companies to take advantage of existing customer bases and also provide better customization to enterprise clients. Digital avatar startup D-ID acquired Berlin-based video startup Simpleshow, and Google-backed legal tech startup Lawhive acquired a U.K.-based law firm. With these acquisitions, AI startups want to combine their AI tools with the existing domain expertise and client service capabilities of services firms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Prezent wants to build more partnerships and acquire suitable companies in sectors like executive communication coaching, medical writing, and consulting firms in the communication area. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Mark Terbeek, a partner at Greycroft who has invested in multiple rounds in Prezent, said that the venture firm likes to find the areas where businesses have used costly agencies in the past to fulfill a need, and now there are AI tools available trying to do the same tasks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We felt that Rajat and Prezent were concentrating hard on solving specific customer needs of business communications. Plus, we saw software evolve quickly to adjust to the workflows of end users and help them save time,” he told TechCrunch over a call.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Going forward, Prezent wants to add better personalization to its product so the AI tool learns the style of each individual in the organization. It also wants to add multimodality capabilities to deck creations, allowing users to use text, voice, or video as input to make presentations. Like Synthesia and D-ID, Prezent wants to add digital avatars to presentations, too.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/10/prezent-raises-30-million-to-acquire-ai-services-firms-starting-with-founders-other-company/</guid><pubDate>Fri, 10 Oct 2025 17:11:29 +0000</pubDate></item><item><title>[NEW] The billion-dollar infrastructure deals powering the AI boom (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/10/the-billion-dollar-infrastructure-deals-powering-the-ai-boom/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/08/GettyImages-1297856112.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;It takes a lot of computing power to run an AI product — and as the tech industry races to tap the power of AI models, there’s a parallel race underway to build the infrastructure that will power them. On a recent earnings call, Nvidia CEO Jensen Huang estimated that between $3 trillion and $4 trillion will be spent on AI infrastructure by the end of the decade — with much of that money coming from AI companies. Along the way, they’re placing immense strain on power grids and pushing the industry’s building capacity to its limit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Below, we’ve laid out everything we know about the biggest AI infrastructure projects, including major spending from Meta, Oracle, Microsoft, Google, and OpenAI. We’ll keep it updated as the boom continues and the numbers climb even higher.&lt;/p&gt;

&lt;h2 class="wp-block-heading has-h-3-font-size" id="h-microsoft-s-1-billion-investment-in-openai"&gt;Microsoft’s $1 billion investment in OpenAI&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;This is arguably the deal that kicked off the whole contemporary AI boom: In 2019, Microsoft made a $1 billion investment in a buzzy non-profit called OpenAI, known mostly for its association with Elon Musk. Crucially, the deal made Microsoft the exclusive cloud provider for OpenAI — and as the demands of model training became more intense, more of Microsoft’s investment started to come in the form of Azure cloud credit rather than cash. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It was a great deal for both sides: Microsoft was able to claim more Azure sales, and OpenAI got more money for its biggest single expense. In the years that followed, Microsoft would build its investment up to nearly $14 billion — a move that is set to pay off enormously when OpenAI converts into a for-profit company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The partnership between the two companies has unwound more recently. In January, OpenAI announced it would no longer be using Microsoft’s cloud exclusively, instead giving the company a right of first refusal on future infrastructure demands but pursuing others if Azure couldn’t meet their needs. More recently, Microsoft began exploring other foundation models to power its AI products, establishing even more independence from the AI giant.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s arrangement with Microsoft was so successful that it’s become a common practice for AI services to sign on with a particular cloud provider. Anthropic has received $8 billion in investment from Amazon, while making kernel-level modifications on the company’s hardware to make it better suited for AI training. Google Cloud has also signed on smaller AI companies like Lovable and Windsurf as “primary computing partners,” although those deals did not involve any investment. And even OpenAI has gone back to the well, receiving a $100 billion investment from Nvidia in September, giving it capacity to buy even more of the company’s GPUs.&lt;/p&gt;

&lt;h2 class="wp-block-heading has-h-3-font-size" id="h-the-rise-of-oracle"&gt;The rise of Oracle&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;On June 30, 2025, Oracle revealed in an SEC filing that it had signed a $30 billion cloud services deal with an unnamed partner; this is more than the company’s cloud revenues for all of the previous fiscal year. OpenAI was eventually revealed as the partner, securing Oracle a spot alongside Google as one of OpenAI’s string of post-Microsoft hosting partners. Unsurprisingly, the company’s stock went shooting up.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;A few months later, it happened again. On September 10, Oracle revealed a five-year, $300 billion deal for compute power, set to begin in 2027. Oracle’s stock climbed even higher, briefly making founder Larry Ellison the richest man in the world. The sheer scale of the deal is stunning: OpenAI does not have $300 billion to spend, so the figure presumes immense growth for both companies, and more than a little faith. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But before a single dollar is spent, the deal has already cemented Oracle as one of the leading AI infrastructure providers — and a financial force to be reckoned with.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-nvidia-s-investment-spree"&gt;Nvidia’s investment spree&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;As AI labs scramble to build infrastructure, they’re mostly all buying GPUs from one company: Nvidia. That trade has made Nvidia flush with cash — and it’s been investing that cash back into the industry in increasingly unconventional ways. In September 2025, the company bought a four percent stake in rival Intel for $5 billion — but even more surprising has been the deals with its own customers. One week after the Intel deal was revealed, the company announced a $100 billion investment in OpenAI, paid for with GPUs that would be used in OpenAI’s ongoing data center projects. Nvidia has since announced a similar deal with Elon Musk’s xAI, and OpenAI launched a separate GPU-for-stock arrangement with AMD.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If that seems circular, it should. Nvidia’s GPUs are valuable because they’re so scarce — and by trading them directly into an ever-inflating data center scheme, Nvidia is making sure they stay that way. You could say the same thing about OpenAI’s privately held stock, which are all the more valuable because they can’t be obtained through public markets. For now, OpenAI and Nvidia are riding high and nobody seems too worried — but if the momentum starts to flag, this sort of arrangement will get a lot more scrutiny.&lt;/p&gt;

&lt;h2 class="wp-block-heading has-h-3-font-size" id="h-building-tomorrow-s-hyperscale-data-centers"&gt;Building tomorrow’s hyperscale data centers&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;For companies like Meta that already have significant legacy infrastructure, the story is more complicated — although equally expensive. Mark Zuckerberg has said that Meta plans to spend $600 billion on U.S. infrastructure through the end of 2028.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In just the first half of 2025, the company spent $30 billion more than the previous year, driven largely by the company’s growing AI ambitions. Some of that spending goes toward big ticket cloud contracts, like a recent $10 billion deal with Google Cloud, but even more resources are being poured into two massive new data centers. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A new 2,250-acre site in Louisiana, dubbed Hyperion, will cost an estimated $10 billion to build out and provide an estimated 5 gigawatts of compute power. Notably, the site includes an arrangement with a local nuclear power plant to handle the increased energy load. A smaller site in Ohio, called Prometheus, is expected to come online in 2026, powered by natural gas.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That kind of buildout comes with real environmental costs. Elon Musk’s xAI built its own hybrid data center and power-generation plant in South Memphis, Tennessee. The plant has quickly become one of the county’s largest emitters of smog-producing chemicals, thanks to a string of natural gas turbines that experts say violate the Clean Air Act.&lt;/p&gt;

&lt;h2 class="wp-block-heading has-h-3-font-size" id="h-the-stargate-moonshot"&gt;The Stargate moonshot&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Just two days after his second inauguration, President Trump announced a joint venture between SoftBank, OpenAI, and Oracle, meant to spend $500 billion building AI infrastructure in the United States. Named “Stargate” after the 1994 film, the project arrived with incredible amounts of hype, with Trump calling it “the largest AI infrastructure project in history.” Sam Altman seemed to agree, saying, ​​”I think this will be the most important project of this era.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In broad strokes, the plan was for SoftBank to provide the funding, with Oracle handling the buildout with input from OpenAI. Overseeing it all was Trump, who promised to clear away any regulatory hurdles that might slow down the build. But there were doubts from the beginning, including from Elon Musk, Altman’s business rival, who claimed the project did not have the available funds.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As the hype has died down, the project has lost some momentum. In August, Bloomberg reported that the partners were failing to reach consensus. Nonetheless, the project has moved forward with the construction of eight data centers in Abilene, Texas, with construction on the final building set to be finished by the end of 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article was first published on September 22.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/08/GettyImages-1297856112.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;It takes a lot of computing power to run an AI product — and as the tech industry races to tap the power of AI models, there’s a parallel race underway to build the infrastructure that will power them. On a recent earnings call, Nvidia CEO Jensen Huang estimated that between $3 trillion and $4 trillion will be spent on AI infrastructure by the end of the decade — with much of that money coming from AI companies. Along the way, they’re placing immense strain on power grids and pushing the industry’s building capacity to its limit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Below, we’ve laid out everything we know about the biggest AI infrastructure projects, including major spending from Meta, Oracle, Microsoft, Google, and OpenAI. We’ll keep it updated as the boom continues and the numbers climb even higher.&lt;/p&gt;

&lt;h2 class="wp-block-heading has-h-3-font-size" id="h-microsoft-s-1-billion-investment-in-openai"&gt;Microsoft’s $1 billion investment in OpenAI&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;This is arguably the deal that kicked off the whole contemporary AI boom: In 2019, Microsoft made a $1 billion investment in a buzzy non-profit called OpenAI, known mostly for its association with Elon Musk. Crucially, the deal made Microsoft the exclusive cloud provider for OpenAI — and as the demands of model training became more intense, more of Microsoft’s investment started to come in the form of Azure cloud credit rather than cash. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It was a great deal for both sides: Microsoft was able to claim more Azure sales, and OpenAI got more money for its biggest single expense. In the years that followed, Microsoft would build its investment up to nearly $14 billion — a move that is set to pay off enormously when OpenAI converts into a for-profit company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The partnership between the two companies has unwound more recently. In January, OpenAI announced it would no longer be using Microsoft’s cloud exclusively, instead giving the company a right of first refusal on future infrastructure demands but pursuing others if Azure couldn’t meet their needs. More recently, Microsoft began exploring other foundation models to power its AI products, establishing even more independence from the AI giant.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s arrangement with Microsoft was so successful that it’s become a common practice for AI services to sign on with a particular cloud provider. Anthropic has received $8 billion in investment from Amazon, while making kernel-level modifications on the company’s hardware to make it better suited for AI training. Google Cloud has also signed on smaller AI companies like Lovable and Windsurf as “primary computing partners,” although those deals did not involve any investment. And even OpenAI has gone back to the well, receiving a $100 billion investment from Nvidia in September, giving it capacity to buy even more of the company’s GPUs.&lt;/p&gt;

&lt;h2 class="wp-block-heading has-h-3-font-size" id="h-the-rise-of-oracle"&gt;The rise of Oracle&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;On June 30, 2025, Oracle revealed in an SEC filing that it had signed a $30 billion cloud services deal with an unnamed partner; this is more than the company’s cloud revenues for all of the previous fiscal year. OpenAI was eventually revealed as the partner, securing Oracle a spot alongside Google as one of OpenAI’s string of post-Microsoft hosting partners. Unsurprisingly, the company’s stock went shooting up.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;A few months later, it happened again. On September 10, Oracle revealed a five-year, $300 billion deal for compute power, set to begin in 2027. Oracle’s stock climbed even higher, briefly making founder Larry Ellison the richest man in the world. The sheer scale of the deal is stunning: OpenAI does not have $300 billion to spend, so the figure presumes immense growth for both companies, and more than a little faith. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But before a single dollar is spent, the deal has already cemented Oracle as one of the leading AI infrastructure providers — and a financial force to be reckoned with.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-nvidia-s-investment-spree"&gt;Nvidia’s investment spree&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;As AI labs scramble to build infrastructure, they’re mostly all buying GPUs from one company: Nvidia. That trade has made Nvidia flush with cash — and it’s been investing that cash back into the industry in increasingly unconventional ways. In September 2025, the company bought a four percent stake in rival Intel for $5 billion — but even more surprising has been the deals with its own customers. One week after the Intel deal was revealed, the company announced a $100 billion investment in OpenAI, paid for with GPUs that would be used in OpenAI’s ongoing data center projects. Nvidia has since announced a similar deal with Elon Musk’s xAI, and OpenAI launched a separate GPU-for-stock arrangement with AMD.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If that seems circular, it should. Nvidia’s GPUs are valuable because they’re so scarce — and by trading them directly into an ever-inflating data center scheme, Nvidia is making sure they stay that way. You could say the same thing about OpenAI’s privately held stock, which are all the more valuable because they can’t be obtained through public markets. For now, OpenAI and Nvidia are riding high and nobody seems too worried — but if the momentum starts to flag, this sort of arrangement will get a lot more scrutiny.&lt;/p&gt;

&lt;h2 class="wp-block-heading has-h-3-font-size" id="h-building-tomorrow-s-hyperscale-data-centers"&gt;Building tomorrow’s hyperscale data centers&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;For companies like Meta that already have significant legacy infrastructure, the story is more complicated — although equally expensive. Mark Zuckerberg has said that Meta plans to spend $600 billion on U.S. infrastructure through the end of 2028.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In just the first half of 2025, the company spent $30 billion more than the previous year, driven largely by the company’s growing AI ambitions. Some of that spending goes toward big ticket cloud contracts, like a recent $10 billion deal with Google Cloud, but even more resources are being poured into two massive new data centers. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A new 2,250-acre site in Louisiana, dubbed Hyperion, will cost an estimated $10 billion to build out and provide an estimated 5 gigawatts of compute power. Notably, the site includes an arrangement with a local nuclear power plant to handle the increased energy load. A smaller site in Ohio, called Prometheus, is expected to come online in 2026, powered by natural gas.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That kind of buildout comes with real environmental costs. Elon Musk’s xAI built its own hybrid data center and power-generation plant in South Memphis, Tennessee. The plant has quickly become one of the county’s largest emitters of smog-producing chemicals, thanks to a string of natural gas turbines that experts say violate the Clean Air Act.&lt;/p&gt;

&lt;h2 class="wp-block-heading has-h-3-font-size" id="h-the-stargate-moonshot"&gt;The Stargate moonshot&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Just two days after his second inauguration, President Trump announced a joint venture between SoftBank, OpenAI, and Oracle, meant to spend $500 billion building AI infrastructure in the United States. Named “Stargate” after the 1994 film, the project arrived with incredible amounts of hype, with Trump calling it “the largest AI infrastructure project in history.” Sam Altman seemed to agree, saying, ​​”I think this will be the most important project of this era.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In broad strokes, the plan was for SoftBank to provide the funding, with Oracle handling the buildout with input from OpenAI. Overseeing it all was Trump, who promised to clear away any regulatory hurdles that might slow down the build. But there were doubts from the beginning, including from Elon Musk, Altman’s business rival, who claimed the project did not have the available funds.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As the hype has died down, the project has lost some momentum. In August, Bloomberg reported that the partners were failing to reach consensus. Nonetheless, the project has moved forward with the construction of eight data centers in Abilene, Texas, with construction on the final building set to be finished by the end of 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article was first published on September 22.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/10/the-billion-dollar-infrastructure-deals-powering-the-ai-boom/</guid><pubDate>Fri, 10 Oct 2025 17:55:12 +0000</pubDate></item></channel></rss>