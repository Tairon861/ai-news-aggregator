<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 10 Dec 2025 06:36:54 +0000</lastBuildDate><item><title>Amazon’s Ring rolls out controversial, AI-powered facial-recognition feature to video doorbells (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/amazons-ring-rolls-out-controversial-ai-powered-facial-recognition-feature-to-video-doorbells/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Dystopian or useful? Amazon’s Ring doorbells will now be able to identify your visitors through a new AI-powered facial-recognition feature, the company said on Tuesday. The controversial feature, dubbed “Familiar Faces,” was announced earlier this September and is now rolling out to Ring device owners in the United States.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon says the feature lets you identify the people who regularly come to your door by creating a catalog of up to 50 faces. These could include family members, friends and neighbors, delivery drivers, household staff, and others. After you label someone in the Ring app, the device will recognize them as they approach the Ring’s camera. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Then, instead of alerting you that “a person is at your door,” you’ll receive a personalized notification, like “Mom at Front Door,” the company explains in its launch announcement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature has already received pushback from consumer protection organizations, like the EFF, and a U.S. senator. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon Ring owners can use the feature to help them disable alerts they don’t want to see — like those notifications referencing their own comings and goings, for instance, the company says. And they can set these alerts on a per-face basis.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature is not enabled by default. Instead, users will need to turn it on in their app’s settings. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, faces can be named in the app directly from the Event History section or from the new Familiar Faces library. Once labeled, the face will be named in all notifications, in the app’s timeline, and in the Event History. These labels can be edited at any time, and there are tools to merge duplicates or delete faces.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon claims the face data is encrypted and never shared with others. Plus, it says unnamed faces are automatically removed after 30 days. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3074302" height="383" src="https://techcrunch.com/wp-content/uploads/2025/12/FF-Mockups.webp?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Ring&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-privacy-concerns-over-ai-facial-recognition"&gt;Privacy concerns over AI facial recognition&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Despite Amazon’s privacy assurances, the addition of the feature raises concerns.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has a history of forging partnerships with law enforcement and even once gave police and fire departments the ability to request data from the Ring Neighbors app by asking Amazon directly for people’s doorbell footage. More recently, Amazon partnered with Flock, the maker of AI-powered surveillance cameras used by police, federal law enforcement, and ICE.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Ring’s own security efforts have fallen short in the past.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ring had to pay a $5.8 million fine in 2023 after the U.S. Federal Trade Commission found that Ring employees and contractors had broad and unrestricted access to customers’ videos for years. Its Neighbors app also exposed users’ home addresses and precise locations, and users’ Ring passwords have been floating around the dark web for years.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Given Amazon’s willingness to work with law enforcement and digital surveillance providers, combined with its poor security track record, we’d suggest Ring owners, at the very least, be careful about identifying anyone using their proper name; better yet, keep the feature disabled and just look to see who it is. Not everything needs an AI upgrade.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As a result of the privacy concerns, Amazon’s Ring has already faced calls from U.S. senator Ed Markey (D-Mass.) to abandon this feature, and is facing backlash from consumer protection organizations, like the EFF. Privacy laws are preventing Amazon from launching the feature in Illinois,&amp;nbsp;Texas, and Portland, Oregon, the EFF had also noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In response to questions posed by the organization, Amazon said the users’ biometric data will be processed in the cloud and claimed it doesn’t use the data to train AI models. It also claimed it wouldn’t be able to identify all the locations where a person had been detected, from a technical standpoint, even if law enforcement requested this data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, it’s unclear why that would not be the case, given the similarity to the “Search Party” feature that looks across a neighborhood’s network of Ring cameras to find lost dogs and cats.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reached for comment, EFF’s Staff Attorney, F. Mario Trujillo, said, “Knocking on a door, or even just walking in front of it, shouldn’t require abandoning your privacy. With this feature going live, it’s more important than ever that state privacy regulators step in to investigate, protect people’s privacy, and test the strength of their biometric privacy laws.” &lt;br /&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Updated after publication with EFF comment.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Dystopian or useful? Amazon’s Ring doorbells will now be able to identify your visitors through a new AI-powered facial-recognition feature, the company said on Tuesday. The controversial feature, dubbed “Familiar Faces,” was announced earlier this September and is now rolling out to Ring device owners in the United States.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon says the feature lets you identify the people who regularly come to your door by creating a catalog of up to 50 faces. These could include family members, friends and neighbors, delivery drivers, household staff, and others. After you label someone in the Ring app, the device will recognize them as they approach the Ring’s camera. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Then, instead of alerting you that “a person is at your door,” you’ll receive a personalized notification, like “Mom at Front Door,” the company explains in its launch announcement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature has already received pushback from consumer protection organizations, like the EFF, and a U.S. senator. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon Ring owners can use the feature to help them disable alerts they don’t want to see — like those notifications referencing their own comings and goings, for instance, the company says. And they can set these alerts on a per-face basis.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature is not enabled by default. Instead, users will need to turn it on in their app’s settings. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, faces can be named in the app directly from the Event History section or from the new Familiar Faces library. Once labeled, the face will be named in all notifications, in the app’s timeline, and in the Event History. These labels can be edited at any time, and there are tools to merge duplicates or delete faces.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon claims the face data is encrypted and never shared with others. Plus, it says unnamed faces are automatically removed after 30 days. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3074302" height="383" src="https://techcrunch.com/wp-content/uploads/2025/12/FF-Mockups.webp?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Ring&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-privacy-concerns-over-ai-facial-recognition"&gt;Privacy concerns over AI facial recognition&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Despite Amazon’s privacy assurances, the addition of the feature raises concerns.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has a history of forging partnerships with law enforcement and even once gave police and fire departments the ability to request data from the Ring Neighbors app by asking Amazon directly for people’s doorbell footage. More recently, Amazon partnered with Flock, the maker of AI-powered surveillance cameras used by police, federal law enforcement, and ICE.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Ring’s own security efforts have fallen short in the past.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ring had to pay a $5.8 million fine in 2023 after the U.S. Federal Trade Commission found that Ring employees and contractors had broad and unrestricted access to customers’ videos for years. Its Neighbors app also exposed users’ home addresses and precise locations, and users’ Ring passwords have been floating around the dark web for years.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Given Amazon’s willingness to work with law enforcement and digital surveillance providers, combined with its poor security track record, we’d suggest Ring owners, at the very least, be careful about identifying anyone using their proper name; better yet, keep the feature disabled and just look to see who it is. Not everything needs an AI upgrade.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As a result of the privacy concerns, Amazon’s Ring has already faced calls from U.S. senator Ed Markey (D-Mass.) to abandon this feature, and is facing backlash from consumer protection organizations, like the EFF. Privacy laws are preventing Amazon from launching the feature in Illinois,&amp;nbsp;Texas, and Portland, Oregon, the EFF had also noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In response to questions posed by the organization, Amazon said the users’ biometric data will be processed in the cloud and claimed it doesn’t use the data to train AI models. It also claimed it wouldn’t be able to identify all the locations where a person had been detected, from a technical standpoint, even if law enforcement requested this data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, it’s unclear why that would not be the case, given the similarity to the “Search Party” feature that looks across a neighborhood’s network of Ring cameras to find lost dogs and cats.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reached for comment, EFF’s Staff Attorney, F. Mario Trujillo, said, “Knocking on a door, or even just walking in front of it, shouldn’t require abandoning your privacy. With this feature going live, it’s more important than ever that state privacy regulators step in to investigate, protect people’s privacy, and test the strength of their biometric privacy laws.” &lt;br /&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Updated after publication with EFF comment.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/amazons-ring-rolls-out-controversial-ai-powered-facial-recognition-feature-to-video-doorbells/</guid><pubDate>Tue, 09 Dec 2025 19:04:08 +0000</pubDate></item><item><title>Slack CEO Denise Dresser to join OpenAI as chief revenue officer (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/slack-ceo-denise-dresser-to-join-openai-as-chief-revenue-officer/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/54103320653_6eb7d509ce_k.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI is hiring Slack CEO Denise Dresser as its new chief revenue officer. The news was first reported by Wired, then confirmed by OpenAI in a blog post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dresser’s new role comes after more than 14 years at Salesforce, Slack’s parent company. While at Slack, Dresser oversaw the introduction of several AI features.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI says that Dresser will be responsible for the company’s revenue strategy in enterprise and customer success. That’s a pivotal role, given that the company has a rocky road ahead if it ever wants to turn a profit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re on a path to put AI tools into the hands of millions of workers, across every industry,” Fidji Simo, OpenAI’s CEO of Applications, said in a statement. “Denise has led that kind of shift before, and her experience will help us make AI useful, reliable, and accessible for businesses everywhere.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Like Dresser, Simo also joined OpenAI this year after a long track record of high-profile leadership, most recently as CEO of Instacart, which has become a close partner of OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Wired, Slack’s chief product officer, Rob Seaman, will become interim CEO of Slack.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/54103320653_6eb7d509ce_k.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI is hiring Slack CEO Denise Dresser as its new chief revenue officer. The news was first reported by Wired, then confirmed by OpenAI in a blog post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dresser’s new role comes after more than 14 years at Salesforce, Slack’s parent company. While at Slack, Dresser oversaw the introduction of several AI features.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI says that Dresser will be responsible for the company’s revenue strategy in enterprise and customer success. That’s a pivotal role, given that the company has a rocky road ahead if it ever wants to turn a profit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re on a path to put AI tools into the hands of millions of workers, across every industry,” Fidji Simo, OpenAI’s CEO of Applications, said in a statement. “Denise has led that kind of shift before, and her experience will help us make AI useful, reliable, and accessible for businesses everywhere.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Like Dresser, Simo also joined OpenAI this year after a long track record of high-profile leadership, most recently as CEO of Instacart, which has become a close partner of OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Wired, Slack’s chief product officer, Rob Seaman, will become interim CEO of Slack.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/slack-ceo-denise-dresser-to-join-openai-as-chief-revenue-officer/</guid><pubDate>Tue, 09 Dec 2025 19:41:40 +0000</pubDate></item><item><title>Mistral launches powerful Devstral 2 coding model including open source, laptop-friendly version (AI | VentureBeat)</title><link>https://venturebeat.com/ai/mistral-launches-powerful-devstral-2-coding-model-including-open-source</link><description>[unable to retrieve full-text content]&lt;p&gt;French AI startup Mistral has weathered a rocky period of public questioning over the last year to emerge, now here in December 2025, with new, crowd-pleasing models for enterprise and indie developers.&lt;/p&gt;&lt;p&gt;Just days after releasing its &lt;a href="https://venturebeat.com/ai/mistral-launches-mistral-3-a-family-of-open-models-designed-to-run-on"&gt;powerful open source, general purpose Mistral 3 LLM family&lt;/a&gt; for edge devices and local hardware, the &lt;a href="https://mistral.ai/news/devstral-2-vibe-cli"&gt;&lt;b&gt;company returned today to debut Devstral 2&lt;/b&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The release includes a new pair of models optimized for software engineering tasks — again, with one small enough to run on a single laptop, offline and privately — alongside &lt;b&gt;Mistral Vibe,&lt;/b&gt; a command-line interface (CLI) agent designed to allow developers to call the models up directly within their terminal environments. &lt;/p&gt;&lt;p&gt;The models are fast, lean, and open—at least in theory. But the real story lies not just in the benchmarks, but in how Mistral is packaging this capability: one model fully free, another conditionally so, and a terminal interface built to scale with either.&lt;/p&gt;&lt;p&gt;It’s an attempt not just to match proprietary systems like Claude and GPT-4 in performance, but to compete with them on developer experience—and to do so while holding onto the flag of open-source.&lt;/p&gt;&lt;p&gt;Both models are available now for free for a limited time &lt;a href="https://docs.mistral.ai/models/devstral-2-25-12"&gt;via Mistral’s API&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/mistralai/devstral-2"&gt;Hugging Face&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;The full Devstral 2 model is supported out-of-the-box in the community inference provider &lt;a href="https://x.com/vllm_project/status/1998428798891765926?s=20"&gt;vLLM&lt;/a&gt; and on the open source agentic coding platform &lt;a href="https://x.com/kilocode/status/1998412042357588461"&gt;Kilo Code&lt;/a&gt;.  &lt;/p&gt;&lt;h2&gt;&lt;b&gt;A Coding Model Meant to Drive&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;At the top of the announcement is Devstral 2, a 123-billion parameter dense transformer with a 256K-token context window, engineered specifically for agentic software development. &lt;/p&gt;&lt;p&gt;Mistral says the model achieves 72.2% on SWE-bench Verified, a benchmark designed to evaluate long-context software engineering tasks in real-world repositories.&lt;/p&gt;&lt;p&gt;The smaller sibling, Devstral Small 2, weighs in at 24B parameters, with the same long context window and a performance of 68.0% on SWE-bench. &lt;/p&gt;&lt;p&gt;On paper, that makes it the strongest open-weight model of its size, even outscoring many 70B-class competitors.&lt;/p&gt;&lt;p&gt;But the performance story isn’t just about raw percentages. Mistral is betting that efficient intelligence beats scale, and has made much of the fact that Devstral 2 is:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;5× smaller than DeepSeek V3.2&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;8× smaller than Kimi K2&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Yet still matches or surpasses them on key software reasoning benchmarks.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Human evaluations back this up. In side-by-side comparisons:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Devstral 2 beat DeepSeek V3.2 in 42.8% of tasks, losing only 28.6%.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Against Claude Sonnet 4.5, it lost more often (53.1%)—a reminder that while the gap is narrowing, closed models still lead in overall preference.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Still, for an open-weight model, these results place Devstral 2 at the frontier of what’s currently available to run and modify independently.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Vibe CLI: A Terminal-Native Agent&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Alongside the models, Mistral released Vibe CLI, a command-line assistant that integrates directly with Devstral models. It’s not an IDE plugin or a ChatGPT-style code explainer. It’s a native interface designed for project-wide code understanding and orchestration, built to live inside the developer’s actual workflow.&lt;/p&gt;&lt;p&gt;Vibe brings a surprising degree of intelligence to the terminal:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;It reads your file tree and Git status to understand project scope.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;It lets you reference files with @, run shell commands with !, and toggle behavior with slash commands.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;It orchestrates changes across multiple files, tracks dependencies, retries failed executions, and can even refactor at architectural scale.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Unlike most developer agents, which simulate a REPL from within a chat UI, Vibe starts with the shell and pulls intelligence in from there. It’s programmable, scriptable, and themeable. And it’s released under the Apache 2.0 license, meaning it’s truly free to use—in commercial settings, internal tools, or open-source extensions.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Licensing Structure: Open-ish — With Revenue Limitations&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;At first glance, Mistral’s licensing approach appears straightforward: the models are open-weight and publicly available. But a closer look reveals a line drawn through the middle of the release, with different rules for different users.&lt;/p&gt;&lt;p&gt;Devstral Small 2, the 24-billion parameter variant, is covered under a standard, enterprise- and developer-friendly &lt;a href="https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md"&gt;Apache 2.0 license&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;That’s a gold standard in open-source: no revenue restrictions, no fine print, no need to check with legal. Enterprises can use it in production, embed it into products, and redistribute fine-tuned versions without asking for permission.&lt;/p&gt;&lt;p&gt;Devstral 2, the flagship 123B model, is released under what Mistral calls a “&lt;a href="https://huggingface.co/mistralai/Devstral-2-123B-Instruct-2512/blob/main/LICENSE"&gt;modified MIT license&lt;/a&gt;.” That phrase sounds innocuous, but the modification introduces a critical limitation: any company making more than $20 million in monthly revenue cannot use the model at all—not even internally—without securing a separate commercial license from Mistral.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;“You are not authorized to exercise any rights under this license if the global consolidated monthly revenue of your company […] exceeds $20 million,” the license reads.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;The clause applies not only to the base model, but to derivatives, fine-tuned versions, and redistributed variants, regardless of who hosts them. In effect, it means that while the weights are “open,” their use is gated for large enterprises—unless they’re willing to engage with Mistral’s sales team or use the hosted API at metered pricing.&lt;/p&gt;&lt;p&gt;To draw an analogy: Apache 2.0 is like a public library—you walk in, borrow the book, and use it however you need. Mistral’s modified MIT license is more like a corporate co-working space that’s free for freelancers but charges rent once your company hits a certain size.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Weighing Devstral Small 2 for Enterprise Use&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;This division raises an obvious question for larger companies: can Devstral Small 2 with its more permissive and unrestricted Apache 2.0 licensing serve as a viable alternative for medium-to-large enterprises?&lt;/p&gt;&lt;p&gt;The answer depends on context. Devstral Small 2 scores 68.0% on SWE-bench, significantly ahead of many larger open models, and remains deployable on single-GPU or CPU-only setups. For teams focused on:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;internal tooling,&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;on-prem deployment,&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;low-latency edge inference,&lt;/p&gt;&lt;p&gt;…it offers a rare combination of legality, performance, and convenience.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;But the performance gap from Devstral 2 is real. For multi-agent setups, deep monorepo refactoring, or long-context code analysis, that 4-point benchmark delta may understate the actual experience difference.&lt;/p&gt;&lt;p&gt;For most enterprises, Devstral Small 2 will serve either as a low-friction way to prototype—or as a pragmatic bridge until licensing for Devstral 2 becomes feasible. It is not a drop-in replacement for the flagship, but it may be “good enough” in specific production slices, particularly when paired with Vibe CLI.&lt;/p&gt;&lt;p&gt;But because Devstral Small 2 can be run entirely offline — including on a single GPU machine or a sufficiently specced laptop — it unlocks a critical use case for developers and teams operating in tightly controlled environments. &lt;/p&gt;&lt;p&gt;Whether you’re a solo indie building tools on the go, or part of a company with strict data governance or compliance mandates, the ability to run a performant, long-context coding model without ever hitting the internet is a powerful differentiator. No cloud calls, no third-party telemetry, no risk of data leakage — just local inference with full visibility and control.&lt;/p&gt;&lt;p&gt;This matters in industries like finance, healthcare, defense, and advanced manufacturing, where data often cannot leave the network perimeter. But it’s just as useful for developers who prefer autonomy over vendor lock-in — or who want their tools to work the same on a plane, in the field, or inside an air-gapped lab. In a market where most top-tier code models are delivered as API-only SaaS products, Devstral Small 2 offers a rare level of portability, privacy, and ownership.&lt;/p&gt;&lt;p&gt;In that sense, Mistral isn’t just offering open models—they’re offering multiple paths to adoption, depending on your scale, compliance posture, and willingness to engage.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Integration, Infrastructure, and Access&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;From a technical standpoint, Mistral’s models are built for deployment. Devstral 2 requires a minimum of 4× H100-class GPUs, and is already available on build.nvidia.com. &lt;/p&gt;&lt;p&gt;Devstral Small 2 can run on a single GPU or CPU such as those in a standard laptop, making it accessible to solo developers and embedded teams alike.&lt;/p&gt;&lt;p&gt;Both models support quantized FP4 and FP8 weights, and are compatible with vLLM for scalable inference. Fine-tuning is supported out of the box.&lt;/p&gt;&lt;p&gt;API pricing—after the free introductory window—follows a token-based structure:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Devstral 2:&lt;/b&gt; $0.40 per million input tokens / $2.00 for output&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Devstral Small 2:&lt;/b&gt; $0.10 input / $0.30 output&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;That pricing sits just below OpenAI’s GPT-4 Turbo, and well below Anthropic’s Claude Sonnet at comparable performance levels.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Developer Reception: Ground-Level Buzz&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;On X (formerly Twitter), developers reacted quickly with a wave of positive reception, with Hugging Face&amp;#x27;s Head of Product &lt;a href="https://x.com/victormustar/status/1998414127400923246"&gt;Victor Mustar asking&lt;/a&gt; if the small, Apache 2.0 licensed variant was the &amp;quot;new local coding king,&amp;quot; i.e., the one developers could use to run on their laptops directly and privately, without an internet connection:&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;Another popular AI news and rumors account, TestingCatalogNews, posted that it was &amp;quot;SOTTA in coding,&amp;quot; or &amp;quot;State Of The Tiny Art&amp;quot;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;Another user, &lt;a href="https://x.com/xlr8harder/status/1998458990565396505"&gt;@xlr8harder&lt;/a&gt;, took issue with the custom licensing terms for Devstral 2, writing &amp;quot;calling the Devstral 2 license &amp;#x27;modified MIT&amp;#x27; is misleading at best. It’s a proprietary license with MIT-like attribution requirements.&amp;quot;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;While the tone was critical, it reflected some attention Mistral’s license structuring was receiving, particularly among developers familiar with open-use norms.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Strategic Context: From Codestral to Devstral and Mistral 3&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Mistral’s steady push into software development tools didn’t start with Devstral 2—it began in May 2024 with &lt;a href="https://venturebeat.com/ai/mistral-announces-codestral-its-first-programming-focused-ai-model"&gt;Codestral&lt;/a&gt;, the company’s first code-focused large language model. A 22-billion parameter system trained on more than 80 programming languages, Codestral was designed for use in developer environments ranging from basic autocompletions to full function generation. The model launched under a non-commercial license but still outperformed heavyweight competitors like CodeLlama 70B and Deepseek Coder 33B in early benchmarks such as HumanEval and RepoBench.&lt;/p&gt;&lt;p&gt;Codestral’s release marked Mistral’s first move into the competitive coding-model space, but it also established a now-familiar pattern: technically lean models with surprisingly strong results, a wide context window, and licensing choices that invited developer experimentation. Industry partners including JetBrains, LlamaIndex, and LangChain quickly began integrating the model into their workflows, citing its speed and tool compatibility as key differentiators.&lt;/p&gt;&lt;p&gt;One year later, the company followed up with &lt;a href="https://venturebeat.com/ai/mistral-ai-launches-devstral-powerful-new-open-source-swe-agent-model-that-runs-on-laptops"&gt;Devstral&lt;/a&gt;, a 24B model purpose-built for “agentic” behavior—handling long-range reasoning, file navigation, and autonomous code modification. Released in partnership with All Hands AI and licensed under Apache 2.0, Devstral was notable not just for its portability (it could run on a MacBook or RTX 4090), but for its performance: it beat out several closed models on SWE-Bench Verified, a benchmark of 500 real-world GitHub issues.&lt;/p&gt;&lt;p&gt;Then came Mistral 3, announced in December 2025 as &lt;a href="https://venturebeat.com/ai/mistral-launches-mistral-3-a-family-of-open-models-designed-to-run-on"&gt;a portfolio of 10 open-weight models&lt;/a&gt; targeting everything from drones and smartphones to cloud infrastructure. This suite included both high-end models like Mistral Large 3 (a MoE system with 41 active parameters and 256K context) and lightweight “Ministral” variants that could run on 4GB of VRAM. All were licensed under Apache 2.0, reinforcing Mistral’s commitment to flexible, edge-friendly deployment.&lt;/p&gt;&lt;p&gt;Mistral 3 positioned the company not as a direct competitor to frontier models like GPT-5 or Gemini 3, but as a developer-first platform for customized, localized AI systems. Co-founder Guillaume Lample described the vision as “distributed intelligence”—many smaller systems tuned for specific tasks and running outside centralized infrastructure. “In more than 90% of cases, a small model can do the job,” he told VentureBeat. “It doesn’t have to be a model with hundreds of billions of parameters.”&lt;/p&gt;&lt;p&gt;That broader strategy helps explain the significance of Devstral 2. It’s not a one-off release but a continuation of Mistral’s long-running commitment to code agents, local-first deployment, and open-weight availability—an ecosystem that began with Codestral, matured through Devstral, and scaled up with Mistral 3. Devstral 2, in this framing, is not just a model. It’s the next version of a playbook that’s been unfolding in public for over a year.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Final Thoughts (For Now): A Fork in the Road&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;With Devstral 2, Devstral Small 2, and Vibe CLI, Mistral AI has drawn a clear map for developers and companies alike. The tools are fast, capable, and thoughtfully integrated. But they also present &lt;b&gt;a choice&lt;/b&gt;—not just in architecture, but in how and where you’re allowed to use them.&lt;/p&gt;&lt;p&gt;If you’re an individual developer, small startup, or open-source maintainer, this is one of the most powerful AI systems you can freely run today. &lt;/p&gt;&lt;p&gt;If you’re a Fortune 500 engineering lead, you’ll need to either talk to Mistral—or settle for the smaller model and make it work.&lt;/p&gt;&lt;p&gt;In a market increasingly dominated by black-box models and SaaS lock-ins, Mistral’s offer is still a breath of fresh air. Just read the fine print before you start building.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;French AI startup Mistral has weathered a rocky period of public questioning over the last year to emerge, now here in December 2025, with new, crowd-pleasing models for enterprise and indie developers.&lt;/p&gt;&lt;p&gt;Just days after releasing its &lt;a href="https://venturebeat.com/ai/mistral-launches-mistral-3-a-family-of-open-models-designed-to-run-on"&gt;powerful open source, general purpose Mistral 3 LLM family&lt;/a&gt; for edge devices and local hardware, the &lt;a href="https://mistral.ai/news/devstral-2-vibe-cli"&gt;&lt;b&gt;company returned today to debut Devstral 2&lt;/b&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The release includes a new pair of models optimized for software engineering tasks — again, with one small enough to run on a single laptop, offline and privately — alongside &lt;b&gt;Mistral Vibe,&lt;/b&gt; a command-line interface (CLI) agent designed to allow developers to call the models up directly within their terminal environments. &lt;/p&gt;&lt;p&gt;The models are fast, lean, and open—at least in theory. But the real story lies not just in the benchmarks, but in how Mistral is packaging this capability: one model fully free, another conditionally so, and a terminal interface built to scale with either.&lt;/p&gt;&lt;p&gt;It’s an attempt not just to match proprietary systems like Claude and GPT-4 in performance, but to compete with them on developer experience—and to do so while holding onto the flag of open-source.&lt;/p&gt;&lt;p&gt;Both models are available now for free for a limited time &lt;a href="https://docs.mistral.ai/models/devstral-2-25-12"&gt;via Mistral’s API&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/mistralai/devstral-2"&gt;Hugging Face&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;The full Devstral 2 model is supported out-of-the-box in the community inference provider &lt;a href="https://x.com/vllm_project/status/1998428798891765926?s=20"&gt;vLLM&lt;/a&gt; and on the open source agentic coding platform &lt;a href="https://x.com/kilocode/status/1998412042357588461"&gt;Kilo Code&lt;/a&gt;.  &lt;/p&gt;&lt;h2&gt;&lt;b&gt;A Coding Model Meant to Drive&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;At the top of the announcement is Devstral 2, a 123-billion parameter dense transformer with a 256K-token context window, engineered specifically for agentic software development. &lt;/p&gt;&lt;p&gt;Mistral says the model achieves 72.2% on SWE-bench Verified, a benchmark designed to evaluate long-context software engineering tasks in real-world repositories.&lt;/p&gt;&lt;p&gt;The smaller sibling, Devstral Small 2, weighs in at 24B parameters, with the same long context window and a performance of 68.0% on SWE-bench. &lt;/p&gt;&lt;p&gt;On paper, that makes it the strongest open-weight model of its size, even outscoring many 70B-class competitors.&lt;/p&gt;&lt;p&gt;But the performance story isn’t just about raw percentages. Mistral is betting that efficient intelligence beats scale, and has made much of the fact that Devstral 2 is:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;5× smaller than DeepSeek V3.2&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;8× smaller than Kimi K2&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Yet still matches or surpasses them on key software reasoning benchmarks.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Human evaluations back this up. In side-by-side comparisons:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Devstral 2 beat DeepSeek V3.2 in 42.8% of tasks, losing only 28.6%.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Against Claude Sonnet 4.5, it lost more often (53.1%)—a reminder that while the gap is narrowing, closed models still lead in overall preference.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Still, for an open-weight model, these results place Devstral 2 at the frontier of what’s currently available to run and modify independently.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Vibe CLI: A Terminal-Native Agent&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Alongside the models, Mistral released Vibe CLI, a command-line assistant that integrates directly with Devstral models. It’s not an IDE plugin or a ChatGPT-style code explainer. It’s a native interface designed for project-wide code understanding and orchestration, built to live inside the developer’s actual workflow.&lt;/p&gt;&lt;p&gt;Vibe brings a surprising degree of intelligence to the terminal:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;It reads your file tree and Git status to understand project scope.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;It lets you reference files with @, run shell commands with !, and toggle behavior with slash commands.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;It orchestrates changes across multiple files, tracks dependencies, retries failed executions, and can even refactor at architectural scale.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Unlike most developer agents, which simulate a REPL from within a chat UI, Vibe starts with the shell and pulls intelligence in from there. It’s programmable, scriptable, and themeable. And it’s released under the Apache 2.0 license, meaning it’s truly free to use—in commercial settings, internal tools, or open-source extensions.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Licensing Structure: Open-ish — With Revenue Limitations&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;At first glance, Mistral’s licensing approach appears straightforward: the models are open-weight and publicly available. But a closer look reveals a line drawn through the middle of the release, with different rules for different users.&lt;/p&gt;&lt;p&gt;Devstral Small 2, the 24-billion parameter variant, is covered under a standard, enterprise- and developer-friendly &lt;a href="https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md"&gt;Apache 2.0 license&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;That’s a gold standard in open-source: no revenue restrictions, no fine print, no need to check with legal. Enterprises can use it in production, embed it into products, and redistribute fine-tuned versions without asking for permission.&lt;/p&gt;&lt;p&gt;Devstral 2, the flagship 123B model, is released under what Mistral calls a “&lt;a href="https://huggingface.co/mistralai/Devstral-2-123B-Instruct-2512/blob/main/LICENSE"&gt;modified MIT license&lt;/a&gt;.” That phrase sounds innocuous, but the modification introduces a critical limitation: any company making more than $20 million in monthly revenue cannot use the model at all—not even internally—without securing a separate commercial license from Mistral.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;“You are not authorized to exercise any rights under this license if the global consolidated monthly revenue of your company […] exceeds $20 million,” the license reads.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;The clause applies not only to the base model, but to derivatives, fine-tuned versions, and redistributed variants, regardless of who hosts them. In effect, it means that while the weights are “open,” their use is gated for large enterprises—unless they’re willing to engage with Mistral’s sales team or use the hosted API at metered pricing.&lt;/p&gt;&lt;p&gt;To draw an analogy: Apache 2.0 is like a public library—you walk in, borrow the book, and use it however you need. Mistral’s modified MIT license is more like a corporate co-working space that’s free for freelancers but charges rent once your company hits a certain size.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Weighing Devstral Small 2 for Enterprise Use&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;This division raises an obvious question for larger companies: can Devstral Small 2 with its more permissive and unrestricted Apache 2.0 licensing serve as a viable alternative for medium-to-large enterprises?&lt;/p&gt;&lt;p&gt;The answer depends on context. Devstral Small 2 scores 68.0% on SWE-bench, significantly ahead of many larger open models, and remains deployable on single-GPU or CPU-only setups. For teams focused on:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;internal tooling,&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;on-prem deployment,&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;low-latency edge inference,&lt;/p&gt;&lt;p&gt;…it offers a rare combination of legality, performance, and convenience.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;But the performance gap from Devstral 2 is real. For multi-agent setups, deep monorepo refactoring, or long-context code analysis, that 4-point benchmark delta may understate the actual experience difference.&lt;/p&gt;&lt;p&gt;For most enterprises, Devstral Small 2 will serve either as a low-friction way to prototype—or as a pragmatic bridge until licensing for Devstral 2 becomes feasible. It is not a drop-in replacement for the flagship, but it may be “good enough” in specific production slices, particularly when paired with Vibe CLI.&lt;/p&gt;&lt;p&gt;But because Devstral Small 2 can be run entirely offline — including on a single GPU machine or a sufficiently specced laptop — it unlocks a critical use case for developers and teams operating in tightly controlled environments. &lt;/p&gt;&lt;p&gt;Whether you’re a solo indie building tools on the go, or part of a company with strict data governance or compliance mandates, the ability to run a performant, long-context coding model without ever hitting the internet is a powerful differentiator. No cloud calls, no third-party telemetry, no risk of data leakage — just local inference with full visibility and control.&lt;/p&gt;&lt;p&gt;This matters in industries like finance, healthcare, defense, and advanced manufacturing, where data often cannot leave the network perimeter. But it’s just as useful for developers who prefer autonomy over vendor lock-in — or who want their tools to work the same on a plane, in the field, or inside an air-gapped lab. In a market where most top-tier code models are delivered as API-only SaaS products, Devstral Small 2 offers a rare level of portability, privacy, and ownership.&lt;/p&gt;&lt;p&gt;In that sense, Mistral isn’t just offering open models—they’re offering multiple paths to adoption, depending on your scale, compliance posture, and willingness to engage.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Integration, Infrastructure, and Access&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;From a technical standpoint, Mistral’s models are built for deployment. Devstral 2 requires a minimum of 4× H100-class GPUs, and is already available on build.nvidia.com. &lt;/p&gt;&lt;p&gt;Devstral Small 2 can run on a single GPU or CPU such as those in a standard laptop, making it accessible to solo developers and embedded teams alike.&lt;/p&gt;&lt;p&gt;Both models support quantized FP4 and FP8 weights, and are compatible with vLLM for scalable inference. Fine-tuning is supported out of the box.&lt;/p&gt;&lt;p&gt;API pricing—after the free introductory window—follows a token-based structure:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Devstral 2:&lt;/b&gt; $0.40 per million input tokens / $2.00 for output&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Devstral Small 2:&lt;/b&gt; $0.10 input / $0.30 output&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;That pricing sits just below OpenAI’s GPT-4 Turbo, and well below Anthropic’s Claude Sonnet at comparable performance levels.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Developer Reception: Ground-Level Buzz&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;On X (formerly Twitter), developers reacted quickly with a wave of positive reception, with Hugging Face&amp;#x27;s Head of Product &lt;a href="https://x.com/victormustar/status/1998414127400923246"&gt;Victor Mustar asking&lt;/a&gt; if the small, Apache 2.0 licensed variant was the &amp;quot;new local coding king,&amp;quot; i.e., the one developers could use to run on their laptops directly and privately, without an internet connection:&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;Another popular AI news and rumors account, TestingCatalogNews, posted that it was &amp;quot;SOTTA in coding,&amp;quot; or &amp;quot;State Of The Tiny Art&amp;quot;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;Another user, &lt;a href="https://x.com/xlr8harder/status/1998458990565396505"&gt;@xlr8harder&lt;/a&gt;, took issue with the custom licensing terms for Devstral 2, writing &amp;quot;calling the Devstral 2 license &amp;#x27;modified MIT&amp;#x27; is misleading at best. It’s a proprietary license with MIT-like attribution requirements.&amp;quot;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;While the tone was critical, it reflected some attention Mistral’s license structuring was receiving, particularly among developers familiar with open-use norms.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Strategic Context: From Codestral to Devstral and Mistral 3&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Mistral’s steady push into software development tools didn’t start with Devstral 2—it began in May 2024 with &lt;a href="https://venturebeat.com/ai/mistral-announces-codestral-its-first-programming-focused-ai-model"&gt;Codestral&lt;/a&gt;, the company’s first code-focused large language model. A 22-billion parameter system trained on more than 80 programming languages, Codestral was designed for use in developer environments ranging from basic autocompletions to full function generation. The model launched under a non-commercial license but still outperformed heavyweight competitors like CodeLlama 70B and Deepseek Coder 33B in early benchmarks such as HumanEval and RepoBench.&lt;/p&gt;&lt;p&gt;Codestral’s release marked Mistral’s first move into the competitive coding-model space, but it also established a now-familiar pattern: technically lean models with surprisingly strong results, a wide context window, and licensing choices that invited developer experimentation. Industry partners including JetBrains, LlamaIndex, and LangChain quickly began integrating the model into their workflows, citing its speed and tool compatibility as key differentiators.&lt;/p&gt;&lt;p&gt;One year later, the company followed up with &lt;a href="https://venturebeat.com/ai/mistral-ai-launches-devstral-powerful-new-open-source-swe-agent-model-that-runs-on-laptops"&gt;Devstral&lt;/a&gt;, a 24B model purpose-built for “agentic” behavior—handling long-range reasoning, file navigation, and autonomous code modification. Released in partnership with All Hands AI and licensed under Apache 2.0, Devstral was notable not just for its portability (it could run on a MacBook or RTX 4090), but for its performance: it beat out several closed models on SWE-Bench Verified, a benchmark of 500 real-world GitHub issues.&lt;/p&gt;&lt;p&gt;Then came Mistral 3, announced in December 2025 as &lt;a href="https://venturebeat.com/ai/mistral-launches-mistral-3-a-family-of-open-models-designed-to-run-on"&gt;a portfolio of 10 open-weight models&lt;/a&gt; targeting everything from drones and smartphones to cloud infrastructure. This suite included both high-end models like Mistral Large 3 (a MoE system with 41 active parameters and 256K context) and lightweight “Ministral” variants that could run on 4GB of VRAM. All were licensed under Apache 2.0, reinforcing Mistral’s commitment to flexible, edge-friendly deployment.&lt;/p&gt;&lt;p&gt;Mistral 3 positioned the company not as a direct competitor to frontier models like GPT-5 or Gemini 3, but as a developer-first platform for customized, localized AI systems. Co-founder Guillaume Lample described the vision as “distributed intelligence”—many smaller systems tuned for specific tasks and running outside centralized infrastructure. “In more than 90% of cases, a small model can do the job,” he told VentureBeat. “It doesn’t have to be a model with hundreds of billions of parameters.”&lt;/p&gt;&lt;p&gt;That broader strategy helps explain the significance of Devstral 2. It’s not a one-off release but a continuation of Mistral’s long-running commitment to code agents, local-first deployment, and open-weight availability—an ecosystem that began with Codestral, matured through Devstral, and scaled up with Mistral 3. Devstral 2, in this framing, is not just a model. It’s the next version of a playbook that’s been unfolding in public for over a year.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Final Thoughts (For Now): A Fork in the Road&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;With Devstral 2, Devstral Small 2, and Vibe CLI, Mistral AI has drawn a clear map for developers and companies alike. The tools are fast, capable, and thoughtfully integrated. But they also present &lt;b&gt;a choice&lt;/b&gt;—not just in architecture, but in how and where you’re allowed to use them.&lt;/p&gt;&lt;p&gt;If you’re an individual developer, small startup, or open-source maintainer, this is one of the most powerful AI systems you can freely run today. &lt;/p&gt;&lt;p&gt;If you’re a Fortune 500 engineering lead, you’ll need to either talk to Mistral—or settle for the smaller model and make it work.&lt;/p&gt;&lt;p&gt;In a market increasingly dominated by black-box models and SaaS lock-ins, Mistral’s offer is still a breath of fresh air. Just read the fine print before you start building.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/mistral-launches-powerful-devstral-2-coding-model-including-open-source</guid><pubDate>Tue, 09 Dec 2025 19:44:00 +0000</pubDate></item><item><title>Three in 10 US teens use AI chatbots every day, but safety concerns are growing (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/three-in-ten-u-s-teens-use-ai-chatbots-every-day-but-safety-concerns-are-growing/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Pew Research Center released a study on Tuesday that shows how young people are using both social media and AI chatbots.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Teen internet safety has remained a global hot topic, with Australia planning to enforce a social media ban for under-16s starting on Wednesday. The impact of social media on teen mental health has been extensively debated — some studies show how online communities can improve mental health, while other research shows the adverse effects of doomscrolling or spending too much time online. The U.S. surgeon general even called for social media platforms to put warning labels on their products last year.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Pew found that 97% of teens use the internet daily, with about 40% of respondents saying they are “almost constantly online.” While this marks a decrease from last year’s survey (46%), it’s significantly higher than the results from a decade ago, when 24% of teens said they were online almost constantly.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But as the prevalence of AI chatbots grows in the U.S., this technology has become yet another factor in the internet’s impact on American youth.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3074371" height="680" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-2.47.26-PM.png?w=619" width="619" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Pew Research Center&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;About three in 10 U.S. teens are using AI chatbots every day, the Pew study reveals, with 4% saying they use them almost constantly. Fifty-nine percent of teens say they use ChatGPT, which is more than twice as popular as the next two most-used chatbots, Google’s Gemini (23%) and Meta AI (20%). Forty-six percent of U.S. teens say that they use AI chatbots at least several times a week, while 36% report not using AI chatbots at all.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Pew’s research also details how race, age, and class impact teen chatbot use. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;About 68% of Black and Hispanic teens surveyed said they use chatbots, compared to 58% of white respondents. In particular, Black teens were about twice as likely to use Gemini and Meta AI as white teens.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“The racial and ethnic differences in teen chatbot use were striking […] but it’s tough to speculate about the reasons behind those differences,” Pew Research Associate Michelle Faverio told TechCrunch. “This pattern is consistent with other racial and ethnic differences we’ve seen in teen technology use. Black and Hispanic teens are more likely than white teens to say they’re on certain social media sites — such as TikTok, YouTube, and Instagram.”&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3074370" height="680" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-2.46.37-PM.png?w=652" width="652" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Pew Research Center&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Across all internet use, Black (55%) and Hispanic teens (52%) were around twice as likely as white teens (27%) to say that they are online “almost constantly.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Older teens (ages 15 to 17) tend to use both social media and AI chatbots more often than younger teens (ages 13 to 14). When it comes to household income, about 62% of teens living in households making more than $75,000 per year said they use ChatGPT, compared to 52% of teens below that threshold. But Character.AI usage is twice as popular (14%) in homes with incomes below $75,000.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While teenagers may start out using these tools for basic questions or homework help, their relationship to AI chatbots can become addictive and potentially harmful. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The families of at least two teens, Adam Raine and Amaurie Lacey, have sued ChatGPT maker OpenAI for its alleged role in their children’s suicides — in both cases, ChatGPT gave the teenagers detailed instructions on how to hang themselves, which were tragically effective. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;(OpenAI claims it should not be held liable for Raine’s death because the sixteen-year-old allegedly circumvented ChatGPT’s safety features and thus violated the chatbot’s terms of service; the company has yet to respond to the Lacey family’s complaint.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Character.AI, an AI role-playing platform, is also facing scrutiny for its impact on teen mental health; at least two teenagers&amp;nbsp;died by suicide after having prolonged conversations with AI chatbots. The startup ended up making the decision to stop offering its chatbots to minors, and instead launched a product called “Stories” for underage users that more closely resembles a choose-your-own-adventure game.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The experiences reflected in the lawsuits against these companies make up a small percentage of all interactions that happen on ChatGPT or Character.AI. In many cases, conversations with chatbots can be incredibly benign. According to OpenAI’s data, only 0.15% of ChatGPT’s active users have conversations about suicide each week — but on a platform with 800 million weekly active users, that small percentage reflects over one million people who discuss suicide with the chatbot per week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Even if [AI companies’] tools weren’t designed for emotional support, people are using them in that way, and that means companies do have a responsibility to adjust their models to be solving for user well-being,” Dr. Nina Vasan, a psychiatrist and director of Brainstorm: The Stanford Lab for Mental Health Innovation, told TechCrunch.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Pew Research Center released a study on Tuesday that shows how young people are using both social media and AI chatbots.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Teen internet safety has remained a global hot topic, with Australia planning to enforce a social media ban for under-16s starting on Wednesday. The impact of social media on teen mental health has been extensively debated — some studies show how online communities can improve mental health, while other research shows the adverse effects of doomscrolling or spending too much time online. The U.S. surgeon general even called for social media platforms to put warning labels on their products last year.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Pew found that 97% of teens use the internet daily, with about 40% of respondents saying they are “almost constantly online.” While this marks a decrease from last year’s survey (46%), it’s significantly higher than the results from a decade ago, when 24% of teens said they were online almost constantly.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But as the prevalence of AI chatbots grows in the U.S., this technology has become yet another factor in the internet’s impact on American youth.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3074371" height="680" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-2.47.26-PM.png?w=619" width="619" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Pew Research Center&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;About three in 10 U.S. teens are using AI chatbots every day, the Pew study reveals, with 4% saying they use them almost constantly. Fifty-nine percent of teens say they use ChatGPT, which is more than twice as popular as the next two most-used chatbots, Google’s Gemini (23%) and Meta AI (20%). Forty-six percent of U.S. teens say that they use AI chatbots at least several times a week, while 36% report not using AI chatbots at all.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Pew’s research also details how race, age, and class impact teen chatbot use. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;About 68% of Black and Hispanic teens surveyed said they use chatbots, compared to 58% of white respondents. In particular, Black teens were about twice as likely to use Gemini and Meta AI as white teens.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“The racial and ethnic differences in teen chatbot use were striking […] but it’s tough to speculate about the reasons behind those differences,” Pew Research Associate Michelle Faverio told TechCrunch. “This pattern is consistent with other racial and ethnic differences we’ve seen in teen technology use. Black and Hispanic teens are more likely than white teens to say they’re on certain social media sites — such as TikTok, YouTube, and Instagram.”&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3074370" height="680" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-2.46.37-PM.png?w=652" width="652" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Pew Research Center&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Across all internet use, Black (55%) and Hispanic teens (52%) were around twice as likely as white teens (27%) to say that they are online “almost constantly.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Older teens (ages 15 to 17) tend to use both social media and AI chatbots more often than younger teens (ages 13 to 14). When it comes to household income, about 62% of teens living in households making more than $75,000 per year said they use ChatGPT, compared to 52% of teens below that threshold. But Character.AI usage is twice as popular (14%) in homes with incomes below $75,000.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While teenagers may start out using these tools for basic questions or homework help, their relationship to AI chatbots can become addictive and potentially harmful. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The families of at least two teens, Adam Raine and Amaurie Lacey, have sued ChatGPT maker OpenAI for its alleged role in their children’s suicides — in both cases, ChatGPT gave the teenagers detailed instructions on how to hang themselves, which were tragically effective. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;(OpenAI claims it should not be held liable for Raine’s death because the sixteen-year-old allegedly circumvented ChatGPT’s safety features and thus violated the chatbot’s terms of service; the company has yet to respond to the Lacey family’s complaint.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Character.AI, an AI role-playing platform, is also facing scrutiny for its impact on teen mental health; at least two teenagers&amp;nbsp;died by suicide after having prolonged conversations with AI chatbots. The startup ended up making the decision to stop offering its chatbots to minors, and instead launched a product called “Stories” for underage users that more closely resembles a choose-your-own-adventure game.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The experiences reflected in the lawsuits against these companies make up a small percentage of all interactions that happen on ChatGPT or Character.AI. In many cases, conversations with chatbots can be incredibly benign. According to OpenAI’s data, only 0.15% of ChatGPT’s active users have conversations about suicide each week — but on a platform with 800 million weekly active users, that small percentage reflects over one million people who discuss suicide with the chatbot per week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Even if [AI companies’] tools weren’t designed for emotional support, people are using them in that way, and that means companies do have a responsibility to adjust their models to be solving for user well-being,” Dr. Nina Vasan, a psychiatrist and director of Brainstorm: The Stanford Lab for Mental Health Innovation, told TechCrunch.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/three-in-ten-u-s-teens-use-ai-chatbots-every-day-but-safety-concerns-are-growing/</guid><pubDate>Tue, 09 Dec 2025 20:00:00 +0000</pubDate></item><item><title>Apriel-1.6-15b-Thinker: Cost-efficient Frontier Multimodal Performance (Hugging Face - Blog)</title><link>https://huggingface.co/blog/ServiceNow-AI/apriel-1p6-15b-thinker</link><description>&lt;!-- HTML_TAG_START --&gt;
We release Apriel-1.6-15b-Thinker, a 15-billion parameter multimodal reasoning model in ServiceNow’s Apriel SLM series which achieves SOTA performance against models 10 times it's size. Apriel-1.6 builds on top of Apriel-1.5-15b-Thinker with an extensive focus on improving text and vision reasoning, while improving token efficiency. This version was trained on NVIDIA DGX™ Cloud with GB200 Grace™ Blackwell Superchips.
&lt;p&gt;Apriel-1.6 scores 57 on the Artificial Analysis Index, outperforming models like Gemini 2.5 Flash, Claude Haiku 4.5 and GPT OSS 20b. It obtains a score on par with Qwen3 235B A22B, while being signficantly more efficient. This new release improves or maintains task performance in comparison with the previous Apriel-1.5-15B-Thinker [1], while reducing reasoning token usage by more than 30%.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Artificial Analysis Intelligence Index (30 Nov '25)" src="https://cdn-uploads.huggingface.co/production/uploads/614cf0be2c0ca05fbc33a827/liUuJ-2ZPz_xtHEgq4wN2.png" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Mid-Training
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We follow the same overall training process used for Apriel-1.5-15B-Thinker, which includes a depth-upscaling phase followed by two Continual Pretraining (CPT) stages (detailed in [1]). The depth-upscaling corpus consists of 35% data from diverse sources, including high-quality web content, scientific and technical literature, mathematical problem sets, and programming code; 15% high-quality datasets from NVIDIA Nemotron™; and the remaining 50% pretraining-style data serving as replay.&lt;/p&gt;
&lt;p&gt;For Apriel-1.6-15B-Thinker, we expand the Stage-1 CPT mixture, which focuses on strengthening textual reasoning and image understanding, with additional text-only samples and image-text pairs. The new text data is fully synthetic, covering general reasoning, knowledge, coding, and creative writing, while the multimodal portion spans document and chart understanding, OCR, visual-reasoning tasks, and SVG/web-code synthesis.&lt;/p&gt;
&lt;p&gt;Following Stage-1, we perform a text-only CPT run at an extended 49K sequence length and then run Stage 2 to further refine the model’s visual-reasoning capabilities. This combination produced a strong base model that provided a solid foundation for subsequent post-training. Training for this mid-training pipeline required approximately 10,000 GPU hours on NVIDIA's GB200s, a small compute footprint enabled by their high throughput and aligned with our goal of building strong models with limited resources through careful data strategy and training methodology.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Post-Training
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Using the midtrained model, we perform post-training following a pipeline that consists of large scale Supervised Finetuning (SFT) and Reinforcement Learning (RL) targeting both vision and text abilities. &lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Supervised Finetuning (SFT)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Our Supervised Fine-Tuning (SFT) stage focuses on improving the reasoning quality of Apriel-1.6 by training on a meticulously curated dataset of 2.4 million high-signal text samples. Each example includes explicit, step-by-step reasoning traces, enabling the model to internalize transparent reasoning processes rather than merely reproducing final answers.&lt;/p&gt;
&lt;p&gt;To construct this dataset, we combined execution-verifiable synthetic samples for math, coding, and scientific problem-solving with a broad mix of instruction-following, conversational, API/function-calling, creative writing, safety, and other knowledge-intensive samples. Data quality was treated as a first-class priority: every sample passed through multi-stage de-duplication, content filtering, heuristic quality pruning, LLM-as-Judge validation, execution-based verification (where applicable), and strict decontamination against evaluation benchmarks.&lt;/p&gt;
&lt;p&gt;SFT was carried out in two phases, both trained at a 32K context length. In the first phase, we ran a large-scale text-only training run on the 2.4M samples for 4 epochs. Compared to Apriel-1.5-15b-Thinker, we simplified the chat template by removing redundant tags and introduced four special tokens to the tokenizer (&lt;code&gt;&amp;lt;tool_calls&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;/tool_calls&amp;gt;&lt;/code&gt;, &lt;code&gt;[BEGIN FINAL RESPONSE]&lt;/code&gt;, &lt;code&gt;&amp;lt;|end|&amp;gt;&lt;/code&gt;) for easier output parsing.&lt;/p&gt;
&lt;p&gt;The second phase was a lightweight, multimodal run trained for 3 epochs, using rejection-sampled data from Apriel-1.5-15b-Thinker to ensure the model maintained strong performance on image inputs after the introduction of these special tokens, while also preparing it for downstream RL stages.&lt;/p&gt;
&lt;p&gt;This approach provided us with a robust, high-quality SFT foundation on top of which our RL pipeline could operate effectively. The resulting model exhibits strong multimodal understanding, improved text reasoning capabilities, and enhanced agentic behavior.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Reinforcement Learning (RL)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;We adopt a multi-stage RL setup that focuses on simultaneously improving reasoning capability and efficiency.
We train the model on image domains such as visual reasoning, general visual question answering (VQA) and optical character recognition (OCR). Our training data also consists of data across different domains, such as simple questions (to encourage short, direct answers on easy queries), math (numerical reasoning), STEM (multiple-choice scientific questions), and function calling (structured tool use).&lt;/p&gt;
&lt;p&gt;Rewards are given for correctness of the response, along with penalties for undesirable behaviour, such as verbosity, incorrect formats, etc. Overall, our setup is designed to improve the model’s reasoning ability while using fewer reasoning tokens, encouraging it to avoid unnecessary intermediate steps, stop earlier when confident, and answer more directly for simpler queries.&lt;/p&gt;
&lt;p&gt;Training is done with the Group Sequence Policy Optimization loss (GSPO) [2] using the VeRL framework and rule-based verification.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Evaluation
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Text Evaluation
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;We evaluate Apriel-1.6 on various domains such as tool use, math, coding, instruction following and long context.&lt;/p&gt;

&lt;p&gt;* This score is with DCA enabled. Without this, the model scores 36.&lt;/p&gt;
&lt;p&gt;** The average score is calculated using all benchmarks except BFCL v3 Only and DeepResearchBench, since some models do not have scores for these two benchmarks.&lt;/p&gt;
&lt;p&gt;*** AA LCR score for o3-mini-high is projected score based on its AA Index score.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Image Evaluation
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;We evaluate the Apriel-1.6 model on a representative set of evaluations with the prime focus on mathematical reasoning, visual question answering, logical reasoning, STEM related tasks and chart based reasoning. All evaluations are done using VLMEvalkit. Apriel-1.6 improves on its predecessor by &lt;strong&gt;4 points&lt;/strong&gt; on the average of 13 benchmarks of the &lt;strong&gt;Image Index&lt;/strong&gt; comprising of the following benchmarks: MathVision, MathVista, MMMU (validation), MMMU-Pro (10 choice COT), MMMU-Pro (Vision only COT), MathVerse (Vision Dominant), MathVerse (Text Dominant), MMStar, BLINK, LogicVista, CharXiV (descriptive), CharXiV (reasoning), AI2D (test).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Performance on the Image Index" src="https://cdn-uploads.huggingface.co/production/uploads/65036ffdb96045f918094fd6/B5PogpJIui7vGsZ7xx-jH.png" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Cost-Efficient Frontier Performance
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Intelligence vs Total Parameters (30 Nov '25)" src="https://cdn-uploads.huggingface.co/production/uploads/614cf0be2c0ca05fbc33a827/q2Y4Xjp9Sy5dma9wFt7Ny.png" /&gt;&lt;/p&gt;
&lt;p&gt;Apriel-1.6-15B-Thinker sits in the sweet spot of the cost-efficient frontier. It delivers intelligence scores that rival or surpass much larger models while using only 15B parameters. On the chart, it’s firmly inside the &lt;em&gt;&lt;strong&gt;most attractive&lt;/strong&gt;&lt;/em&gt; quadrant, balancing efficiency with top-tier reasoning. In practice, this means Apriel-1.6-15B-Thinker offers strong performance and deep reasoning at a fraction of the compute and deployment cost of heavyweight competitors, making it an exceptionally efficient choice for the real-world, especially in enterprise applications.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Intelligence vs Output Tokens Used in Artificial Analysis Intelligence Index (30 Nov '25)" src="https://cdn-uploads.huggingface.co/production/uploads/614cf0be2c0ca05fbc33a827/QoymhuGcJf6lFAIruUQ2p.png" /&gt;&lt;/p&gt;
&lt;p&gt;Our post-training focuses heavily on improving reasoning-token efficiency. The image above showing intelligence score against token usage highlights the effectiveness of our post-training. Apriel-1.6-15B-Thinker again lands in &lt;em&gt;&lt;strong&gt;most attractive&lt;/strong&gt;&lt;/em&gt; quadrant. The model reaches a high Artificial Analysis Intelligence Index score while using far fewer tokens than many similarly capable or larger models. In comparison to Apriel-1.5-15b-Thinker [1], we reduce token usage by over 30%. &lt;/p&gt;
&lt;p&gt;Overall, Apriel-1.6 is a highly-capable reasoner, that maintains memory and efficiency characteristics required for enterprise deployment.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Acknowledgements
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We gratefully acknowledge the following people for their contributions: Varun Pandey, Shashank Maiya, Dhruv Jhamb, Massimo Caccia, Dheeraj Vattikonda, Nicolas Gontier, Patrice Bechard, Tayfun Tuna, Kavya Sriram, Denis Akhiyarov, Hari Subramani, Tara Bogavelli.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Notes and Limitations
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We are a small lab with big goals. While we are not GPU poor, our lab, in comparison has a tiny fraction of the compute available to other Frontier labs. Our goal with this work is to show that a SOTA model can be built with limited resources if you have the right data, design and solid methodology.&lt;/p&gt;
&lt;p&gt;We set out to build a small but powerful model, aiming for capabilities on par with frontier models. Developing a 15B model with this level of performance requires tradeoffs, so we prioritized getting SOTA-level performance and improving reasoning token efficiency.&lt;/p&gt;
&lt;p&gt;This model is trained to perform extensive reasoning for difficult questions and less reasoning effort for simpler questions. We are always actively working to make our models more efficient and concise in future releases.&lt;/p&gt;
&lt;p&gt;The model has a few vision-related limitations to be aware of. Complex or low-quality images can reduce OCR accuracy, dense scenes (like crowds or many similar objects) can make subtle details and counting more challenging, and highly detailed or unusually formatted charts may occasionally lead to imperfect interpretations. It may also be less precise with fine-grained visual grounding, so bounding-box predictions can sometimes be approximate or inconsistent.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		References
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;[1] Radhakrishna, S., Tiwari, A., Shukla, A., Hashemi, M., Maheshwary, R., Malay, S.K.R., Mehta, J., Pattnaik, P., Mittal, S., Slimi, K., Ogueji, K., Oladipo, A., Parikh, S., Bamgbose, O., Liang, T., Masry, A., Mahajan, K., Mudumba, S.R., Yadav, V., Madhusudhan, S.T., Scholak, T., Davasam, S., Sunkara, S. and Chapados, N., 2025. Apriel-1.5-15b-Thinker. arXiv preprint arXiv:2510.01141.&lt;/p&gt;
&lt;p&gt;[2] Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., Zhou, J. and Lin, J., 2025. Group Sequence Policy Optimization. arXiv preprint arXiv:2507.18071.&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;!-- HTML_TAG_START --&gt;
We release Apriel-1.6-15b-Thinker, a 15-billion parameter multimodal reasoning model in ServiceNow’s Apriel SLM series which achieves SOTA performance against models 10 times it's size. Apriel-1.6 builds on top of Apriel-1.5-15b-Thinker with an extensive focus on improving text and vision reasoning, while improving token efficiency. This version was trained on NVIDIA DGX™ Cloud with GB200 Grace™ Blackwell Superchips.
&lt;p&gt;Apriel-1.6 scores 57 on the Artificial Analysis Index, outperforming models like Gemini 2.5 Flash, Claude Haiku 4.5 and GPT OSS 20b. It obtains a score on par with Qwen3 235B A22B, while being signficantly more efficient. This new release improves or maintains task performance in comparison with the previous Apriel-1.5-15B-Thinker [1], while reducing reasoning token usage by more than 30%.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Artificial Analysis Intelligence Index (30 Nov '25)" src="https://cdn-uploads.huggingface.co/production/uploads/614cf0be2c0ca05fbc33a827/liUuJ-2ZPz_xtHEgq4wN2.png" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Mid-Training
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We follow the same overall training process used for Apriel-1.5-15B-Thinker, which includes a depth-upscaling phase followed by two Continual Pretraining (CPT) stages (detailed in [1]). The depth-upscaling corpus consists of 35% data from diverse sources, including high-quality web content, scientific and technical literature, mathematical problem sets, and programming code; 15% high-quality datasets from NVIDIA Nemotron™; and the remaining 50% pretraining-style data serving as replay.&lt;/p&gt;
&lt;p&gt;For Apriel-1.6-15B-Thinker, we expand the Stage-1 CPT mixture, which focuses on strengthening textual reasoning and image understanding, with additional text-only samples and image-text pairs. The new text data is fully synthetic, covering general reasoning, knowledge, coding, and creative writing, while the multimodal portion spans document and chart understanding, OCR, visual-reasoning tasks, and SVG/web-code synthesis.&lt;/p&gt;
&lt;p&gt;Following Stage-1, we perform a text-only CPT run at an extended 49K sequence length and then run Stage 2 to further refine the model’s visual-reasoning capabilities. This combination produced a strong base model that provided a solid foundation for subsequent post-training. Training for this mid-training pipeline required approximately 10,000 GPU hours on NVIDIA's GB200s, a small compute footprint enabled by their high throughput and aligned with our goal of building strong models with limited resources through careful data strategy and training methodology.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Post-Training
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Using the midtrained model, we perform post-training following a pipeline that consists of large scale Supervised Finetuning (SFT) and Reinforcement Learning (RL) targeting both vision and text abilities. &lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Supervised Finetuning (SFT)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Our Supervised Fine-Tuning (SFT) stage focuses on improving the reasoning quality of Apriel-1.6 by training on a meticulously curated dataset of 2.4 million high-signal text samples. Each example includes explicit, step-by-step reasoning traces, enabling the model to internalize transparent reasoning processes rather than merely reproducing final answers.&lt;/p&gt;
&lt;p&gt;To construct this dataset, we combined execution-verifiable synthetic samples for math, coding, and scientific problem-solving with a broad mix of instruction-following, conversational, API/function-calling, creative writing, safety, and other knowledge-intensive samples. Data quality was treated as a first-class priority: every sample passed through multi-stage de-duplication, content filtering, heuristic quality pruning, LLM-as-Judge validation, execution-based verification (where applicable), and strict decontamination against evaluation benchmarks.&lt;/p&gt;
&lt;p&gt;SFT was carried out in two phases, both trained at a 32K context length. In the first phase, we ran a large-scale text-only training run on the 2.4M samples for 4 epochs. Compared to Apriel-1.5-15b-Thinker, we simplified the chat template by removing redundant tags and introduced four special tokens to the tokenizer (&lt;code&gt;&amp;lt;tool_calls&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;/tool_calls&amp;gt;&lt;/code&gt;, &lt;code&gt;[BEGIN FINAL RESPONSE]&lt;/code&gt;, &lt;code&gt;&amp;lt;|end|&amp;gt;&lt;/code&gt;) for easier output parsing.&lt;/p&gt;
&lt;p&gt;The second phase was a lightweight, multimodal run trained for 3 epochs, using rejection-sampled data from Apriel-1.5-15b-Thinker to ensure the model maintained strong performance on image inputs after the introduction of these special tokens, while also preparing it for downstream RL stages.&lt;/p&gt;
&lt;p&gt;This approach provided us with a robust, high-quality SFT foundation on top of which our RL pipeline could operate effectively. The resulting model exhibits strong multimodal understanding, improved text reasoning capabilities, and enhanced agentic behavior.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Reinforcement Learning (RL)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;We adopt a multi-stage RL setup that focuses on simultaneously improving reasoning capability and efficiency.
We train the model on image domains such as visual reasoning, general visual question answering (VQA) and optical character recognition (OCR). Our training data also consists of data across different domains, such as simple questions (to encourage short, direct answers on easy queries), math (numerical reasoning), STEM (multiple-choice scientific questions), and function calling (structured tool use).&lt;/p&gt;
&lt;p&gt;Rewards are given for correctness of the response, along with penalties for undesirable behaviour, such as verbosity, incorrect formats, etc. Overall, our setup is designed to improve the model’s reasoning ability while using fewer reasoning tokens, encouraging it to avoid unnecessary intermediate steps, stop earlier when confident, and answer more directly for simpler queries.&lt;/p&gt;
&lt;p&gt;Training is done with the Group Sequence Policy Optimization loss (GSPO) [2] using the VeRL framework and rule-based verification.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Evaluation
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Text Evaluation
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;We evaluate Apriel-1.6 on various domains such as tool use, math, coding, instruction following and long context.&lt;/p&gt;

&lt;p&gt;* This score is with DCA enabled. Without this, the model scores 36.&lt;/p&gt;
&lt;p&gt;** The average score is calculated using all benchmarks except BFCL v3 Only and DeepResearchBench, since some models do not have scores for these two benchmarks.&lt;/p&gt;
&lt;p&gt;*** AA LCR score for o3-mini-high is projected score based on its AA Index score.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Image Evaluation
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;We evaluate the Apriel-1.6 model on a representative set of evaluations with the prime focus on mathematical reasoning, visual question answering, logical reasoning, STEM related tasks and chart based reasoning. All evaluations are done using VLMEvalkit. Apriel-1.6 improves on its predecessor by &lt;strong&gt;4 points&lt;/strong&gt; on the average of 13 benchmarks of the &lt;strong&gt;Image Index&lt;/strong&gt; comprising of the following benchmarks: MathVision, MathVista, MMMU (validation), MMMU-Pro (10 choice COT), MMMU-Pro (Vision only COT), MathVerse (Vision Dominant), MathVerse (Text Dominant), MMStar, BLINK, LogicVista, CharXiV (descriptive), CharXiV (reasoning), AI2D (test).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Performance on the Image Index" src="https://cdn-uploads.huggingface.co/production/uploads/65036ffdb96045f918094fd6/B5PogpJIui7vGsZ7xx-jH.png" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Cost-Efficient Frontier Performance
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Intelligence vs Total Parameters (30 Nov '25)" src="https://cdn-uploads.huggingface.co/production/uploads/614cf0be2c0ca05fbc33a827/q2Y4Xjp9Sy5dma9wFt7Ny.png" /&gt;&lt;/p&gt;
&lt;p&gt;Apriel-1.6-15B-Thinker sits in the sweet spot of the cost-efficient frontier. It delivers intelligence scores that rival or surpass much larger models while using only 15B parameters. On the chart, it’s firmly inside the &lt;em&gt;&lt;strong&gt;most attractive&lt;/strong&gt;&lt;/em&gt; quadrant, balancing efficiency with top-tier reasoning. In practice, this means Apriel-1.6-15B-Thinker offers strong performance and deep reasoning at a fraction of the compute and deployment cost of heavyweight competitors, making it an exceptionally efficient choice for the real-world, especially in enterprise applications.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Intelligence vs Output Tokens Used in Artificial Analysis Intelligence Index (30 Nov '25)" src="https://cdn-uploads.huggingface.co/production/uploads/614cf0be2c0ca05fbc33a827/QoymhuGcJf6lFAIruUQ2p.png" /&gt;&lt;/p&gt;
&lt;p&gt;Our post-training focuses heavily on improving reasoning-token efficiency. The image above showing intelligence score against token usage highlights the effectiveness of our post-training. Apriel-1.6-15B-Thinker again lands in &lt;em&gt;&lt;strong&gt;most attractive&lt;/strong&gt;&lt;/em&gt; quadrant. The model reaches a high Artificial Analysis Intelligence Index score while using far fewer tokens than many similarly capable or larger models. In comparison to Apriel-1.5-15b-Thinker [1], we reduce token usage by over 30%. &lt;/p&gt;
&lt;p&gt;Overall, Apriel-1.6 is a highly-capable reasoner, that maintains memory and efficiency characteristics required for enterprise deployment.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Acknowledgements
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We gratefully acknowledge the following people for their contributions: Varun Pandey, Shashank Maiya, Dhruv Jhamb, Massimo Caccia, Dheeraj Vattikonda, Nicolas Gontier, Patrice Bechard, Tayfun Tuna, Kavya Sriram, Denis Akhiyarov, Hari Subramani, Tara Bogavelli.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Notes and Limitations
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We are a small lab with big goals. While we are not GPU poor, our lab, in comparison has a tiny fraction of the compute available to other Frontier labs. Our goal with this work is to show that a SOTA model can be built with limited resources if you have the right data, design and solid methodology.&lt;/p&gt;
&lt;p&gt;We set out to build a small but powerful model, aiming for capabilities on par with frontier models. Developing a 15B model with this level of performance requires tradeoffs, so we prioritized getting SOTA-level performance and improving reasoning token efficiency.&lt;/p&gt;
&lt;p&gt;This model is trained to perform extensive reasoning for difficult questions and less reasoning effort for simpler questions. We are always actively working to make our models more efficient and concise in future releases.&lt;/p&gt;
&lt;p&gt;The model has a few vision-related limitations to be aware of. Complex or low-quality images can reduce OCR accuracy, dense scenes (like crowds or many similar objects) can make subtle details and counting more challenging, and highly detailed or unusually formatted charts may occasionally lead to imperfect interpretations. It may also be less precise with fine-grained visual grounding, so bounding-box predictions can sometimes be approximate or inconsistent.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		References
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;[1] Radhakrishna, S., Tiwari, A., Shukla, A., Hashemi, M., Maheshwary, R., Malay, S.K.R., Mehta, J., Pattnaik, P., Mittal, S., Slimi, K., Ogueji, K., Oladipo, A., Parikh, S., Bamgbose, O., Liang, T., Masry, A., Mahajan, K., Mudumba, S.R., Yadav, V., Madhusudhan, S.T., Scholak, T., Davasam, S., Sunkara, S. and Chapados, N., 2025. Apriel-1.5-15b-Thinker. arXiv preprint arXiv:2510.01141.&lt;/p&gt;
&lt;p&gt;[2] Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., Zhou, J. and Lin, J., 2025. Group Sequence Policy Optimization. arXiv preprint arXiv:2507.18071.&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/ServiceNow-AI/apriel-1p6-15b-thinker</guid><pubDate>Tue, 09 Dec 2025 20:06:56 +0000</pubDate></item><item><title>Rivian is building its own AI assistant (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/rivian-is-building-its-own-ai-assistant/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/250522-ANASTASIA-BENSON-GOOGLE-MAPS-AEB08056-FINAL.jpg?resize=1200,790" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Rivian has spent nearly two years building its own AI assistant, an effort that remains separate from its multibillion-dollar technology joint venture with Volkswagen, TechCrunch has learned.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rivian hasn’t revealed when it will put the AI assistant in consumers hands. However, in an interview earlier this year, Rivian’s software chief Wassym Bensaid told TechCrunch it was targeting the end of the year. The company will likely share more during its upcoming AI &amp;amp; Autonomy Day, which will be livestreamed starting at 9 a.m. PT December 11.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Rivian’s plans are reflective of the moment as the pace of development from foundational AI companies — the tech giants and startups like Anthropic, Google, Microsoft, Meta, and OpenAI that are building the core models and infrastructure — accelerates and industries scramble to keep up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But as Bensaid noted to TechCrunch earlier this year, this isn’t some slapdash effort to stay on trend. Nor is it simply a chatbot thrown into the infotainment system. The company has put considerable thought, resources, and time into the product, Bensaid said, noting that it’s designed to be integrated with all vehicle controls.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company started with an underlying philosophy to build an overall architecture that is model and platform agnostic, according to Bensaid. The Rivian AI assistant team, which is based out of the company’s Palo Alto office, soon realized effort and attention should also be directed toward developing the software layers that help coordinate various workflows as well as the control logic that resolves conflicts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“And that’s the in-vehicle platform we have built,” Bensaid said. “We use what the industry loves to now call an agentic framework; but we thought about that architecture since very early so that we can interface with different models.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The in-house AI assistant program is consistent with Rivian’s push to become more vertically integrated. In 2024, Rivian overhauled its flagship R1T truck and R1S SUV, changing everything from the battery pack and suspension system to the electrical architecture, sensor stack, and software user interface. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The company has also put considerable resources toward developing and improving its own software stack, which includes everything related to real-time operating systems (RTOS) that manage the car, such as thermal dynamics, ADAS, and safety systems, as well as another layer related to the infotainment system.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bensaid didn’t provide detailed information about the AI assistant, but he did say it includes a mix of models that handle specific tasks. The result is a hybrid software stack that combines edge AI, where tasks are handled on the device, and cloud AI, in which large models that require more compute are handled by remote servers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This should mean a flexible, customized AI assistant that splits the workload between the edge and cloud.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Rivian developed much of the AI software stack in-house, including its own custom models and the “orchestration layer,” the conductor or traffic cop of sorts that makes sure the various AI models work together. Rivian tapped other companies for specific agentic AI functions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The mission is to develop an AI assistant that increases customer trust and engagement, Bensaid said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For now, the AI assistant is staying within Rivian. The company’s joint venture with Volkswagen is focused on software, but not an AI assistant or anything to do with automated driving.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The technology joint venture with Volkswagen, which was announced in 2024 and is worth up to $5.8 billion, is centered on the underlying electrical architecture and zonal compute, and infotainment. The joint venture officially kicked off in November 2024 and is expected to supply the electrical architecture and software for Volkswagen Group as early as 2027.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Autonomy and AI are separate for now, but “it doesn’t mean that it may not be in the future,” Bensaid said.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/250522-ANASTASIA-BENSON-GOOGLE-MAPS-AEB08056-FINAL.jpg?resize=1200,790" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Rivian has spent nearly two years building its own AI assistant, an effort that remains separate from its multibillion-dollar technology joint venture with Volkswagen, TechCrunch has learned.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rivian hasn’t revealed when it will put the AI assistant in consumers hands. However, in an interview earlier this year, Rivian’s software chief Wassym Bensaid told TechCrunch it was targeting the end of the year. The company will likely share more during its upcoming AI &amp;amp; Autonomy Day, which will be livestreamed starting at 9 a.m. PT December 11.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Rivian’s plans are reflective of the moment as the pace of development from foundational AI companies — the tech giants and startups like Anthropic, Google, Microsoft, Meta, and OpenAI that are building the core models and infrastructure — accelerates and industries scramble to keep up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But as Bensaid noted to TechCrunch earlier this year, this isn’t some slapdash effort to stay on trend. Nor is it simply a chatbot thrown into the infotainment system. The company has put considerable thought, resources, and time into the product, Bensaid said, noting that it’s designed to be integrated with all vehicle controls.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company started with an underlying philosophy to build an overall architecture that is model and platform agnostic, according to Bensaid. The Rivian AI assistant team, which is based out of the company’s Palo Alto office, soon realized effort and attention should also be directed toward developing the software layers that help coordinate various workflows as well as the control logic that resolves conflicts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“And that’s the in-vehicle platform we have built,” Bensaid said. “We use what the industry loves to now call an agentic framework; but we thought about that architecture since very early so that we can interface with different models.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The in-house AI assistant program is consistent with Rivian’s push to become more vertically integrated. In 2024, Rivian overhauled its flagship R1T truck and R1S SUV, changing everything from the battery pack and suspension system to the electrical architecture, sensor stack, and software user interface. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The company has also put considerable resources toward developing and improving its own software stack, which includes everything related to real-time operating systems (RTOS) that manage the car, such as thermal dynamics, ADAS, and safety systems, as well as another layer related to the infotainment system.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bensaid didn’t provide detailed information about the AI assistant, but he did say it includes a mix of models that handle specific tasks. The result is a hybrid software stack that combines edge AI, where tasks are handled on the device, and cloud AI, in which large models that require more compute are handled by remote servers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This should mean a flexible, customized AI assistant that splits the workload between the edge and cloud.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Rivian developed much of the AI software stack in-house, including its own custom models and the “orchestration layer,” the conductor or traffic cop of sorts that makes sure the various AI models work together. Rivian tapped other companies for specific agentic AI functions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The mission is to develop an AI assistant that increases customer trust and engagement, Bensaid said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For now, the AI assistant is staying within Rivian. The company’s joint venture with Volkswagen is focused on software, but not an AI assistant or anything to do with automated driving.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The technology joint venture with Volkswagen, which was announced in 2024 and is worth up to $5.8 billion, is centered on the underlying electrical architecture and zonal compute, and infotainment. The joint venture officially kicked off in November 2024 and is expected to supply the electrical architecture and software for Volkswagen Group as early as 2027.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Autonomy and AI are separate for now, but “it doesn’t mean that it may not be in the future,” Bensaid said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/rivian-is-building-its-own-ai-assistant/</guid><pubDate>Tue, 09 Dec 2025 20:11:13 +0000</pubDate></item><item><title>Why Cursor’s CEO believes OpenAI, Anthropic competition won’t crush his startup (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/why-cursors-ceo-believes-openai-anthropic-competition-wont-crush-his-startup/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Anysphere-Cursor-Michael-Truell-.png?resize=1200,745" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anysphere, the company that makes AI coding assistant darling Cursor, isn’t thinking about an IPO any time soon, its co-founder CEO Michael Truell said onstage Monday at Fortune’s AI Brainstorm conference.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After reaching $1 billion in annualized revenue in November and raising $2.3 billion at a $29.3 billion valuation last month, Truell said his company is instead focused on building out more features.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For instance, he noted that Cursor’s homegrown LLMs were geared to support specific products. Cursor also confirmed the existence of those models in November when it said in a blog post, “Our in-house models now generate more code than almost any other LLMs in the world.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His comments about the models came up at Fortune’s event when the founder was asked how he plans to compete with the LLM makers that he relies on when the major ones — OpenAI, Anthropic — have their own AI coding offerings.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Truell likened their coding products to “a concept car,” whereas his product is a production automobile.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It would be like taking an engine and a concept car around it instead of a whole end-to-end car that was manufactured,” Truell said. “What we do is we take the best intelligence that the market has to offer from many different providers. And we also do our own product-specific models in places. We take that, we build it together and integrate it, then also build the best tool and end UX for working with AI.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cursor’s dependence on its competitors — and its need to build its own LLMs — has been a subject of speculation among VCs in Silicon Valley since earlier this year when OpenAI reportedly looked at Anysphere as an acquisition target. Anysphere turned the idea down. (This was around the same time that Windsurf’s OpenAI deal also didn’t materialize, with the founder eventually joining Google.)&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The issue, investors told TechCrunch, was that AI coding editors were losing money thanks to high costs they paid to the model makers. In Cursor’s case, instead of selling, it adjusted pricing to a usage model in July, directly passing along the API fees that model makers charge to its users. This change from an all-inclusive subscription fee (and the surprise big bills some customers faced) caused an uproar among some of its users.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Monday, when asked about the pricing kerfuffle, Truell said, “When we started Cursor, you would turn to Cursor for a quick JavaScript question and now you’re turning to it to do hours of work for you. So the pricing model had to shift for us and others in the space. That means shifting more towards a consumption model,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Truell added that one of the tools the company is working on is cloud-computing-like cost-management tools, which lets enterprises monitor their total usage and keep tabs on the bills their engineers are running up.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We have a whole team internally dedicated to enterprise engineering and building things like spend controls and billing groups and visibility,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, he said Cursor is focused on two major areas for the next year. One is handling more complex agentic functions. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We want you to take end-to-end tasks, ones that are concise to specify but then are really hard to do, and have them entirely be done by Cursor. An example is a bug fix,” Truell explained. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He particularly wants Cursor to be able to fix the kinds of bugs that might be easy to describe but take “weeks of someone’s time, thousands of times running the code” to handle. “We want Cursor to do that, end-to-end,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The other area he named, but didn’t explain with much detail, was the idea of “thinking about teams as the atomic unit that we serve,” he said. This must be in contrast to serving individual coders, and a hint to how well its enterprise business is going. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to cost-monitoring features, Truell said he wants Cursor to handle more parts of the software development life cycle outside of writing code. He pointed to Cursor’s code review product as an example, which he said is being used by some customers to analyze every pull request, be it written by AI or human. (A pull request is when a programmer submits code for review before it is merged into the main project.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“So you’ll see us start to help teams more as a whole,” with more features like that, he promised.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, big competitors are also all gearing up for the complex-task agentic world. Amazon just released a coding tool it promises can already run for days on end. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Just this week, the AI power players, including Anthropic, OpenAI, Microsoft, AWS, and many others, launched a new consortium under the Linux Foundation to develop open source agentic interoperability standards. They even contributed some of their key projects, like Anthropic’s wildly popular Model Context Protocol (MCP).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His plans for the year likely won’t put Anysphere firmly ahead of Cursor’s main model-maker competitors. They should, however, keep the company in the race.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Anysphere-Cursor-Michael-Truell-.png?resize=1200,745" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anysphere, the company that makes AI coding assistant darling Cursor, isn’t thinking about an IPO any time soon, its co-founder CEO Michael Truell said onstage Monday at Fortune’s AI Brainstorm conference.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After reaching $1 billion in annualized revenue in November and raising $2.3 billion at a $29.3 billion valuation last month, Truell said his company is instead focused on building out more features.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For instance, he noted that Cursor’s homegrown LLMs were geared to support specific products. Cursor also confirmed the existence of those models in November when it said in a blog post, “Our in-house models now generate more code than almost any other LLMs in the world.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His comments about the models came up at Fortune’s event when the founder was asked how he plans to compete with the LLM makers that he relies on when the major ones — OpenAI, Anthropic — have their own AI coding offerings.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Truell likened their coding products to “a concept car,” whereas his product is a production automobile.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It would be like taking an engine and a concept car around it instead of a whole end-to-end car that was manufactured,” Truell said. “What we do is we take the best intelligence that the market has to offer from many different providers. And we also do our own product-specific models in places. We take that, we build it together and integrate it, then also build the best tool and end UX for working with AI.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cursor’s dependence on its competitors — and its need to build its own LLMs — has been a subject of speculation among VCs in Silicon Valley since earlier this year when OpenAI reportedly looked at Anysphere as an acquisition target. Anysphere turned the idea down. (This was around the same time that Windsurf’s OpenAI deal also didn’t materialize, with the founder eventually joining Google.)&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The issue, investors told TechCrunch, was that AI coding editors were losing money thanks to high costs they paid to the model makers. In Cursor’s case, instead of selling, it adjusted pricing to a usage model in July, directly passing along the API fees that model makers charge to its users. This change from an all-inclusive subscription fee (and the surprise big bills some customers faced) caused an uproar among some of its users.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Monday, when asked about the pricing kerfuffle, Truell said, “When we started Cursor, you would turn to Cursor for a quick JavaScript question and now you’re turning to it to do hours of work for you. So the pricing model had to shift for us and others in the space. That means shifting more towards a consumption model,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Truell added that one of the tools the company is working on is cloud-computing-like cost-management tools, which lets enterprises monitor their total usage and keep tabs on the bills their engineers are running up.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We have a whole team internally dedicated to enterprise engineering and building things like spend controls and billing groups and visibility,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, he said Cursor is focused on two major areas for the next year. One is handling more complex agentic functions. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We want you to take end-to-end tasks, ones that are concise to specify but then are really hard to do, and have them entirely be done by Cursor. An example is a bug fix,” Truell explained. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He particularly wants Cursor to be able to fix the kinds of bugs that might be easy to describe but take “weeks of someone’s time, thousands of times running the code” to handle. “We want Cursor to do that, end-to-end,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The other area he named, but didn’t explain with much detail, was the idea of “thinking about teams as the atomic unit that we serve,” he said. This must be in contrast to serving individual coders, and a hint to how well its enterprise business is going. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to cost-monitoring features, Truell said he wants Cursor to handle more parts of the software development life cycle outside of writing code. He pointed to Cursor’s code review product as an example, which he said is being used by some customers to analyze every pull request, be it written by AI or human. (A pull request is when a programmer submits code for review before it is merged into the main project.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“So you’ll see us start to help teams more as a whole,” with more features like that, he promised.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, big competitors are also all gearing up for the complex-task agentic world. Amazon just released a coding tool it promises can already run for days on end. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Just this week, the AI power players, including Anthropic, OpenAI, Microsoft, AWS, and many others, launched a new consortium under the Linux Foundation to develop open source agentic interoperability standards. They even contributed some of their key projects, like Anthropic’s wildly popular Model Context Protocol (MCP).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His plans for the year likely won’t put Anysphere firmly ahead of Cursor’s main model-maker competitors. They should, however, keep the company in the race.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/why-cursors-ceo-believes-openai-anthropic-competition-wont-crush-his-startup/</guid><pubDate>Tue, 09 Dec 2025 20:59:21 +0000</pubDate></item><item><title>Big Tech joins forces with Linux Foundation to standardize AI agents (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/12/big-tech-joins-forces-with-linux-foundation-to-standardize-ai-agents/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The Agentic AI Foundation launches to support MCP, AGENTS.md, and goose.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2024/12/GettyImages-1327016094-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/12/GettyImages-1327016094-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      It's thinking, but not in words.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Big Tech has spent the past year telling us we’re living in the era of AI agents, but most of what we’ve been promised is still theoretical. As companies race to turn fantasy into reality, they’ve developed a collection of tools to guide the development of generative AI. A cadre of major players in the AI race, including Anthropic, Block, and OpenAI, has come together to promote interoperability with the newly formed Agentic AI Foundation (AAIF). This move elevates a handful of popular technologies and could make them a de facto standard for AI development going forward.&lt;/p&gt;
&lt;p&gt;The development path for agentic AI models is cloudy to say the least, but companies have invested so heavily in creating these systems that some tools have percolated to the surface. The AAIF, which is part of the nonprofit Linux Foundation, has been launched to govern the development of three key AI technologies: Model Context Protocol (MCP), goose, and AGENTS.md.&lt;/p&gt;
&lt;p&gt;MCP is probably the most well-known of the trio, having been open-sourced by Anthropic a year ago. The goal of MCP is to link AI agents to data sources in a standardized way—Anthropic (and now the AAIF) is fond of calling MCP a “USB-C port for AI.” Rather than creating custom integrations for every different database or cloud storage platform, MCP allows developers to quickly and easily connect to any MCP-compliant server.&lt;/p&gt;
&lt;p&gt;Since its release, MCP has been widely used across the AI industry. Google announced at I/O 2025 that it was adding support for MCP in its dev tools, and many of its products have since added MCP servers to make data more accessible to agents. OpenAI also adopted MCP just a few months after it was released.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2131122 align-none"&gt;
    &lt;div&gt;
                        &lt;img alt="mcp simple diagram" class="none medium" height="250" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/mcp-simple-diagram-640x250.png" width="640" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Anthropic

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Expanding use of MCP might help users customize their AI experience. For instance, the new Pebble Index 01 ring uses a local LLM that can act on your voice notes, and it supports MCP for user customization.&lt;/p&gt;
&lt;p&gt;Local AI models have to make some sacrifices compared to bigger cloud-based models, but MCP can fill in the functionality gaps. “A lot of tasks on productivity and content are fully doable on the edge,” Qualcomm head of AI products, Vinesh Sukumar, tells Ars. “With MCP, you have a handshake with multiple cloud service providers for any kind of complex task to be completed.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The Model Context Protocol is the most well-established of the AAIF’s new charges. Goose, which was contributed to the project by Square owner Block, launched in early 2025. This is a customizable open source agent for coding. It’s designed to run locally or in the cloud and can use any LLM you choose. It also has built-in support for MCP.&lt;/p&gt;
&lt;p&gt;Meanwhile, AGENTS.md comes from OpenAI, and it’s also a very recent arrival in the AI sphere. OpenAI announced the tool this past August, and now it’s also part of the AAIF. AGENTS.md is essentially a markdown-based readme for AI coding agents to guide their behavior in more predictable ways.&lt;/p&gt;
&lt;h2&gt;Moving fast&lt;/h2&gt;
&lt;p&gt;Think about the timeline here. The world in which tech companies operate has changed considerably in a short time as everyone rushes to stuff gen AI into every product and process. And no one knows who is on the right track—maybe no one!&lt;/p&gt;
&lt;p&gt;Against that backdrop, big tech has seemingly decided to standardize. Even for MCP, the most widely supported of these tools, there’s still considerable flux in how basic technologies like OAuth will be handled.&lt;/p&gt;
&lt;p&gt;The Linux Foundation has spun up numerous projects to support neutral and interoperable development of key technologies. For example, it formed the Cloud Native Computing Foundation (CNCF) in 2015 to support Google’s open Kubernetes cluster manager, but the project has since integrated a few dozen cloud computing tools. Certification and training for these tools help keep the lights on at the foundation, but Kubernetes was already a proven technology when Google released it widely. All these AI technologies are popular right now, sure, but is MCP or AGENTS.md going to be important in the long term?&lt;/p&gt;
&lt;p&gt;Regardless, everyone in the AI industry seems to be on board. In addition to the companies adding their tools to the project, the AAIF has support from Amazon, Google, Cloudflare, Microsoft, and others. The Linux Foundation says it intends to shepherd these key technologies forward in the name of openness, but it may end up collecting a lot of nascent AI tools at this rate.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The Agentic AI Foundation launches to support MCP, AGENTS.md, and goose.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2024/12/GettyImages-1327016094-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/12/GettyImages-1327016094-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      It's thinking, but not in words.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Big Tech has spent the past year telling us we’re living in the era of AI agents, but most of what we’ve been promised is still theoretical. As companies race to turn fantasy into reality, they’ve developed a collection of tools to guide the development of generative AI. A cadre of major players in the AI race, including Anthropic, Block, and OpenAI, has come together to promote interoperability with the newly formed Agentic AI Foundation (AAIF). This move elevates a handful of popular technologies and could make them a de facto standard for AI development going forward.&lt;/p&gt;
&lt;p&gt;The development path for agentic AI models is cloudy to say the least, but companies have invested so heavily in creating these systems that some tools have percolated to the surface. The AAIF, which is part of the nonprofit Linux Foundation, has been launched to govern the development of three key AI technologies: Model Context Protocol (MCP), goose, and AGENTS.md.&lt;/p&gt;
&lt;p&gt;MCP is probably the most well-known of the trio, having been open-sourced by Anthropic a year ago. The goal of MCP is to link AI agents to data sources in a standardized way—Anthropic (and now the AAIF) is fond of calling MCP a “USB-C port for AI.” Rather than creating custom integrations for every different database or cloud storage platform, MCP allows developers to quickly and easily connect to any MCP-compliant server.&lt;/p&gt;
&lt;p&gt;Since its release, MCP has been widely used across the AI industry. Google announced at I/O 2025 that it was adding support for MCP in its dev tools, and many of its products have since added MCP servers to make data more accessible to agents. OpenAI also adopted MCP just a few months after it was released.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2131122 align-none"&gt;
    &lt;div&gt;
                        &lt;img alt="mcp simple diagram" class="none medium" height="250" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/mcp-simple-diagram-640x250.png" width="640" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Anthropic

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Expanding use of MCP might help users customize their AI experience. For instance, the new Pebble Index 01 ring uses a local LLM that can act on your voice notes, and it supports MCP for user customization.&lt;/p&gt;
&lt;p&gt;Local AI models have to make some sacrifices compared to bigger cloud-based models, but MCP can fill in the functionality gaps. “A lot of tasks on productivity and content are fully doable on the edge,” Qualcomm head of AI products, Vinesh Sukumar, tells Ars. “With MCP, you have a handshake with multiple cloud service providers for any kind of complex task to be completed.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The Model Context Protocol is the most well-established of the AAIF’s new charges. Goose, which was contributed to the project by Square owner Block, launched in early 2025. This is a customizable open source agent for coding. It’s designed to run locally or in the cloud and can use any LLM you choose. It also has built-in support for MCP.&lt;/p&gt;
&lt;p&gt;Meanwhile, AGENTS.md comes from OpenAI, and it’s also a very recent arrival in the AI sphere. OpenAI announced the tool this past August, and now it’s also part of the AAIF. AGENTS.md is essentially a markdown-based readme for AI coding agents to guide their behavior in more predictable ways.&lt;/p&gt;
&lt;h2&gt;Moving fast&lt;/h2&gt;
&lt;p&gt;Think about the timeline here. The world in which tech companies operate has changed considerably in a short time as everyone rushes to stuff gen AI into every product and process. And no one knows who is on the right track—maybe no one!&lt;/p&gt;
&lt;p&gt;Against that backdrop, big tech has seemingly decided to standardize. Even for MCP, the most widely supported of these tools, there’s still considerable flux in how basic technologies like OAuth will be handled.&lt;/p&gt;
&lt;p&gt;The Linux Foundation has spun up numerous projects to support neutral and interoperable development of key technologies. For example, it formed the Cloud Native Computing Foundation (CNCF) in 2015 to support Google’s open Kubernetes cluster manager, but the project has since integrated a few dozen cloud computing tools. Certification and training for these tools help keep the lights on at the foundation, but Kubernetes was already a proven technology when Google released it widely. All these AI technologies are popular right now, sure, but is MCP or AGENTS.md going to be important in the long term?&lt;/p&gt;
&lt;p&gt;Regardless, everyone in the AI industry seems to be on board. In addition to the companies adding their tools to the project, the AAIF has support from Amazon, Google, Cloudflare, Microsoft, and others. The Linux Foundation says it intends to shepherd these key technologies forward in the name of openness, but it may end up collecting a lot of nascent AI tools at this rate.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/12/big-tech-joins-forces-with-linux-foundation-to-standardize-ai-agents/</guid><pubDate>Tue, 09 Dec 2025 21:08:11 +0000</pubDate></item><item><title>B Capital founding partner Kabir Narang leaves to launch new investment platform (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/b-capital-founding-partner-kabir-narang-leaves-to-launch-new-investment-platform/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/kabir-narang-b-capital-GettyImages-1027568616.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Kabir Narang, a founding general partner at B Capital and an early backer of several Indian startups, has left the global venture firm, TechCrunch has learned and confirmed with the company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Narang is laying the groundwork for a new investment platform slated for 2026 that will focus on “compounding at the intersection of technology, AI, and global capital flows,” per a note shared with founders and reviewed by TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;After joining B Capital in March 2017, Narang co-led the firm’s Asia strategy from Singapore and chaired its global investment committee. During his tenure, he backed Indian startups such as Meesho, Khatabook, CredAvenue, Bounce, and Bizongo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are living through one of the most profound technological revolutions in history, and one of the toughest tests of investor discipline,” Narang wrote. “AI scales thought itself, compressing the gap between idea and output. The founders who pair that speed with pricing power and improving unit economics will define the next generation of enduring value.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside developing the new investment platform, Narang told founders that he is taking 1% to 2% personal stakes in companies he believes can “compound intelligently.” This suggests he plans to stay active in early-stage investing while setting up a broader vehicle.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;B Capital confirmed Narang’s exit to TechCrunch and noted that Eduardo Saverin, Karan Mohla, and Howard Morgan would manage its Asia portfolio alongside the existing team in South and Southeast Asia.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“After more than eight years with the firm, Kabir Narang, who focused on later stage growth investing efforts in Asia, has left his role to pursue other opportunities,” a B Capital spokesperson said. “We are grateful for his contributions, and we wish him continued success in the future.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Founded in 2015 by Facebook co-founder Eduardo Saverin and former Bain Capital executive Raj Ganguly, B Capital is a multi-stage investor focused on technology, healthcare, and resilience tech. The San Francisco–based firm manages more than $9 billion across nine offices in the U.S. and Asia. Through a partnership with Boston Consulting Group, B Capital also provides portfolio companies with strategic and operational support.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Before joining B Capital, Narang spent nearly nine years at Fidelity-backed Eight Roads Ventures India, where he was a managing director.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“B Capital remains deeply committed to our strategy in Asia and our broader global platform,” the B Capital spokesperson said. “With strong leadership and an experienced team across the region, we are well-positioned to capitalize on the next wave of innovation and continue backing category-defining companies across our core markets.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Narang did not respond to a request for comment.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/kabir-narang-b-capital-GettyImages-1027568616.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Kabir Narang, a founding general partner at B Capital and an early backer of several Indian startups, has left the global venture firm, TechCrunch has learned and confirmed with the company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Narang is laying the groundwork for a new investment platform slated for 2026 that will focus on “compounding at the intersection of technology, AI, and global capital flows,” per a note shared with founders and reviewed by TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;After joining B Capital in March 2017, Narang co-led the firm’s Asia strategy from Singapore and chaired its global investment committee. During his tenure, he backed Indian startups such as Meesho, Khatabook, CredAvenue, Bounce, and Bizongo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are living through one of the most profound technological revolutions in history, and one of the toughest tests of investor discipline,” Narang wrote. “AI scales thought itself, compressing the gap between idea and output. The founders who pair that speed with pricing power and improving unit economics will define the next generation of enduring value.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside developing the new investment platform, Narang told founders that he is taking 1% to 2% personal stakes in companies he believes can “compound intelligently.” This suggests he plans to stay active in early-stage investing while setting up a broader vehicle.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;B Capital confirmed Narang’s exit to TechCrunch and noted that Eduardo Saverin, Karan Mohla, and Howard Morgan would manage its Asia portfolio alongside the existing team in South and Southeast Asia.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“After more than eight years with the firm, Kabir Narang, who focused on later stage growth investing efforts in Asia, has left his role to pursue other opportunities,” a B Capital spokesperson said. “We are grateful for his contributions, and we wish him continued success in the future.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Founded in 2015 by Facebook co-founder Eduardo Saverin and former Bain Capital executive Raj Ganguly, B Capital is a multi-stage investor focused on technology, healthcare, and resilience tech. The San Francisco–based firm manages more than $9 billion across nine offices in the U.S. and Asia. Through a partnership with Boston Consulting Group, B Capital also provides portfolio companies with strategic and operational support.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Before joining B Capital, Narang spent nearly nine years at Fidelity-backed Eight Roads Ventures India, where he was a managing director.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“B Capital remains deeply committed to our strategy in Asia and our broader global platform,” the B Capital spokesperson said. “With strong leadership and an experienced team across the region, we are well-positioned to capitalize on the next wave of innovation and continue backing category-defining companies across our core markets.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Narang did not respond to a request for comment.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/b-capital-founding-partner-kabir-narang-leaves-to-launch-new-investment-platform/</guid><pubDate>Tue, 09 Dec 2025 21:13:19 +0000</pubDate></item><item><title>Max Hodak is more worried about Twitter than brain-computer interface hacking (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/podcast/max-hodak-is-more-worried-about-twitter-than-brain-computer-interface-hacking/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-05-at-2.45.47-AM.png?resize=1200,852" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;This week on StrictlyVC Download, Connie Loizos speaks with Science Corp. founder Max Hodak to discuss how brain-computer interfaces are arriving faster than anyone realizes. The Neuralink co-founder and former president shares how his company recently achieved what may be the biggest breakthrough in vision restoration in decades, enabling 80% of blind patients to read again with a tiny retinal implant smaller than a grain of rice. In this conversation, the two also explore the near-term commercial path for BCIs through medical applications, the long-term potential for cognitive enhancement, and “binding” multiple brains together, and why Science, which has so far raised $260 million from investors, is keen to generate revenue while it invests in its future products. Not last, Hodak addresses the practical and ethical questions around hacking, enhancement, and why he thinks it may well be possible in the not-too-distant future to “move consciousness” outside of the body.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;StrictlyVC Download posts every Tuesday. Subscribe on &lt;/em&gt;&lt;strong&gt;&lt;em&gt;Apple&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;, &lt;/em&gt;&lt;strong&gt;&lt;em&gt;Spotify&lt;/em&gt;&lt;/strong&gt;, &lt;em&gt;or &lt;/em&gt;&lt;strong&gt;&lt;em&gt;wherever you listen to podcasts&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;to be alerted when new episodes drop.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-05-at-2.45.47-AM.png?resize=1200,852" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;This week on StrictlyVC Download, Connie Loizos speaks with Science Corp. founder Max Hodak to discuss how brain-computer interfaces are arriving faster than anyone realizes. The Neuralink co-founder and former president shares how his company recently achieved what may be the biggest breakthrough in vision restoration in decades, enabling 80% of blind patients to read again with a tiny retinal implant smaller than a grain of rice. In this conversation, the two also explore the near-term commercial path for BCIs through medical applications, the long-term potential for cognitive enhancement, and “binding” multiple brains together, and why Science, which has so far raised $260 million from investors, is keen to generate revenue while it invests in its future products. Not last, Hodak addresses the practical and ethical questions around hacking, enhancement, and why he thinks it may well be possible in the not-too-distant future to “move consciousness” outside of the body.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;StrictlyVC Download posts every Tuesday. Subscribe on &lt;/em&gt;&lt;strong&gt;&lt;em&gt;Apple&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;, &lt;/em&gt;&lt;strong&gt;&lt;em&gt;Spotify&lt;/em&gt;&lt;/strong&gt;, &lt;em&gt;or &lt;/em&gt;&lt;strong&gt;&lt;em&gt;wherever you listen to podcasts&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;to be alerted when new episodes drop.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/podcast/max-hodak-is-more-worried-about-twitter-than-brain-computer-interface-hacking/</guid><pubDate>Tue, 09 Dec 2025 21:56:34 +0000</pubDate></item><item><title>Cashew Research is going after the $90B market research industry with AI (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/cashew-research-is-going-after-the-90b-market-research-industry-with-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Cashew-photo.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Market research is a $90 billion industry that helps brands figure out how to best present themselves to potential customers. But that market insight isn’t cheap, nor is it quick. Cashew Research wants to change that using AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Calgary, Alberta-based Cashew uses AI to develop market research plans and surveys for brands based on what information they are looking for — like what their brand recognition is for a specific population or how a marketing tagline resonates with customers. Cashew then sends the survey to real people and uses AI to summarize and digest the findings.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Cashew was one of the 200 startups chosen for TechCrunch’s Startup Battlefield competition in 2025 and won the Enterprise Stage pitch competition at TechCrunch Disrupt.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You can use an LLM to try to do deep research and get answers to your questions, or you could use a firm that’s going to be really expensive,” Addy Graves, co-founder and CEO of Cashew, told TechCrunch in describing the current market research industry. “Now there’s Cashew that exists in the middle. It creates custom, fresh data to answer your question instead of you just using an LLM that’s surfacing the same recycled pool of data that everybody’s finding on the internet.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Graves has more than a decade of market research experience. The original idea for Cashew was sparked by an issue she ran into frequently: Clients wanted full research projects — with real-world data from humans — done within a few days.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For years, shortening that timeline while still producing the same quality of research results wasn’t possible, Graves said, because the technology to speed up the process wasn’t ready yet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“That was definitely the aha moment,” Graves said. “And it wasn’t until the onset of AI that we were actually able to automate these processes that we use as researchers, best practices, these data science-backed methodologies, as well as the formatting of reports that we know that everybody wants.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Bringing automation to the process also brings the cost down, which makes Cashew an option for small and medium-size brands that wouldn’t have been able to afford to work with a traditional market research firm, Graves added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Graves founded Cashew in 2023 alongside Rose Wong, chief operating officer, with an initial focus on consumer packaged goods, specifically food and beverage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Graves said she thinks Cashew can stand out in the increasingly crowded AI marketing tools category because it isn’t fully automated. Each Cashew client gets fresh human data with each project, which requires market research expertise, Graves said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Cashew’s competitive advantage may only grow as the company matures. The company takes all of the real-world data it collects from its clients’ projects, anonymizes it, and puts it in a database, which can help add additional proprietary data to future research projects, too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has raised C$1.5 million in pre-seed funding and is gearing up to launch its seed round in early 2026 with the hope of raising up to $5 million, Graves said. That capital will be put toward continuing to develop the product’s tech.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Graves said the company’s two main areas of focus heading into next year are increasing the company’s presence in the U.S. and also working to build up its B2B business.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The people who are already buying research, that’s already a massive category, but that doesn’t even include all the people that could be buying research but just can’t afford it or can’t do it right now because they don’t have the timelines,” Graves said. “We’re actually creating this new category for marketers to gain access to answers to these questions that they’ve had.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Cashew-photo.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Market research is a $90 billion industry that helps brands figure out how to best present themselves to potential customers. But that market insight isn’t cheap, nor is it quick. Cashew Research wants to change that using AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Calgary, Alberta-based Cashew uses AI to develop market research plans and surveys for brands based on what information they are looking for — like what their brand recognition is for a specific population or how a marketing tagline resonates with customers. Cashew then sends the survey to real people and uses AI to summarize and digest the findings.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Cashew was one of the 200 startups chosen for TechCrunch’s Startup Battlefield competition in 2025 and won the Enterprise Stage pitch competition at TechCrunch Disrupt.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You can use an LLM to try to do deep research and get answers to your questions, or you could use a firm that’s going to be really expensive,” Addy Graves, co-founder and CEO of Cashew, told TechCrunch in describing the current market research industry. “Now there’s Cashew that exists in the middle. It creates custom, fresh data to answer your question instead of you just using an LLM that’s surfacing the same recycled pool of data that everybody’s finding on the internet.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Graves has more than a decade of market research experience. The original idea for Cashew was sparked by an issue she ran into frequently: Clients wanted full research projects — with real-world data from humans — done within a few days.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For years, shortening that timeline while still producing the same quality of research results wasn’t possible, Graves said, because the technology to speed up the process wasn’t ready yet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“That was definitely the aha moment,” Graves said. “And it wasn’t until the onset of AI that we were actually able to automate these processes that we use as researchers, best practices, these data science-backed methodologies, as well as the formatting of reports that we know that everybody wants.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Bringing automation to the process also brings the cost down, which makes Cashew an option for small and medium-size brands that wouldn’t have been able to afford to work with a traditional market research firm, Graves added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Graves founded Cashew in 2023 alongside Rose Wong, chief operating officer, with an initial focus on consumer packaged goods, specifically food and beverage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Graves said she thinks Cashew can stand out in the increasingly crowded AI marketing tools category because it isn’t fully automated. Each Cashew client gets fresh human data with each project, which requires market research expertise, Graves said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Cashew’s competitive advantage may only grow as the company matures. The company takes all of the real-world data it collects from its clients’ projects, anonymizes it, and puts it in a database, which can help add additional proprietary data to future research projects, too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has raised C$1.5 million in pre-seed funding and is gearing up to launch its seed round in early 2026 with the hope of raising up to $5 million, Graves said. That capital will be put toward continuing to develop the product’s tech.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Graves said the company’s two main areas of focus heading into next year are increasing the company’s presence in the U.S. and also working to build up its B2B business.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The people who are already buying research, that’s already a massive category, but that doesn’t even include all the people that could be buying research but just can’t afford it or can’t do it right now because they don’t have the timelines,” Graves said. “We’re actually creating this new category for marketers to gain access to answers to these questions that they’ve had.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/cashew-research-is-going-after-the-90b-market-research-industry-with-ai/</guid><pubDate>Tue, 09 Dec 2025 22:20:39 +0000</pubDate></item><item><title>Unconventional AI confirms its massive  $475M seed round (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/unconventional-ai-confirms-its-massive-475m-seed-round/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/04/Naveen-Rao-headshot.png?resize=1200,1000" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Naveen Rao, the former head of AI at Databricks, has raised $475 million in seed capital at a $4.5 billion valuation for his new startup, Unconventional AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The round was led by Andreessen Horowitz and Lightspeed Ventures, with participation from Lux Capital and DCVC. The funding is a first installment toward the goal of up to $1 billion for the round, Rao told Bloomberg.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;TechCrunch was first to report, back in October, that Unconventional AI was seeking this mega funding for Rao’s new startup, although the final valuation is marginally lower than the $5 billion sources told us he was seeking. If he does eventually raise as much as $1 billion, we’ll see how that impacts his company’s value.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unconventional AI is set on building a new, energy-efficient computer for AI. Rao previously wrote on X that his goal is to create a computer that is “as efficient as biology.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Databricks acquired Rao’s previous startup,&amp;nbsp;MosaicML in 2023, for $1.3 billion. Prior to MosaicML, Rao co-founded the machine learning platform&amp;nbsp;Nervana Systems, which Intel Corp. acquired in 2016 for reportedly more than $400 million.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/04/Naveen-Rao-headshot.png?resize=1200,1000" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Naveen Rao, the former head of AI at Databricks, has raised $475 million in seed capital at a $4.5 billion valuation for his new startup, Unconventional AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The round was led by Andreessen Horowitz and Lightspeed Ventures, with participation from Lux Capital and DCVC. The funding is a first installment toward the goal of up to $1 billion for the round, Rao told Bloomberg.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;TechCrunch was first to report, back in October, that Unconventional AI was seeking this mega funding for Rao’s new startup, although the final valuation is marginally lower than the $5 billion sources told us he was seeking. If he does eventually raise as much as $1 billion, we’ll see how that impacts his company’s value.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unconventional AI is set on building a new, energy-efficient computer for AI. Rao previously wrote on X that his goal is to create a computer that is “as efficient as biology.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Databricks acquired Rao’s previous startup,&amp;nbsp;MosaicML in 2023, for $1.3 billion. Prior to MosaicML, Rao co-founded the machine learning platform&amp;nbsp;Nervana Systems, which Intel Corp. acquired in 2016 for reportedly more than $400 million.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/unconventional-ai-confirms-its-massive-475m-seed-round/</guid><pubDate>Wed, 10 Dec 2025 00:24:16 +0000</pubDate></item><item><title>CoreWeave CEO defends AI circular deals as ‘working together’ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/coreweave-ceo-defends-ai-circular-deals-as-working-together/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/Mike-Intrator-Headshot-e1750958788389.jpg?w=898" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;It’s been quite the year for CoreWeave. In March, the AI cloud infrastructure provider went public in one of the biggest and most anticipated IPOs of the year that didn’t live up to its hype.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another setback took place in October, when a planned acquisition of the cloud provider’s business partner, Core Scientific, faltered due to skepticism from the acquisition target’s shareholders.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In the meantime, the firm has acquired a number of different companies, its stock has gone up and down, and it’s been both criticized and lauded for its role in the booming AI data center market.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In an interview at the Fortune Brainstorm AI summit in San Francisco on Tuesday, CoreWeave’s co-founder and CEO, Michael Intrator, defended his company’s performance from critics, noting that it was in the midst of creating a “new business model” for how cloud computing can be built and run. Their collection of Nvidia GPUs is so valuable, they borrow against it to help finance their business. The executive seemed to imply: If you’re charting a new path, you’re destined to encounter some road bumps along the way.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think people are myopic a lot of times,” Intrator said when questioned about his company’s occasionally unstable stock price. “Yes, it is seesawing,” he admitted, while noting that the CoreWeave IPO took place not long before President Trump’s tariffs went into effect — a notably uncertain moment for the overall economy.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We came out into one of the most challenging environments, right around Liberation Day and, in spite of the incredible headwinds, were able to launch a successful IPO,” the CEO told Brainstorm editorial director Andrew Nusca. “I couldn’t be prouder of what the company has accomplished,” he added.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CoreWeave’s stock may have debuted amid the economic doldrums of March but its price has gone on quite the journey since then. It debuted at $40 and, over the past eight months, has climbed to well over $150, but currently rests at around $90. Its more wary critics have compared it to a meme stock due to its penchant for going up and down.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Some of the uncertainty around CoreWeave’s stock has been credited to the company’s hefty level of debt. Not long after CoreWeave announced a deal on Monday to issue even more debt to finance its data center buildout, its stock dropped some 8%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Intrator seems to see his company as a disruptor, one whose unconventional tactics may take some getting used to. “When you introduce a new model, when you introduce a new way of doing business, when you disrupt what has been a static environment, it’s going to take some people some time,” he said during his appearance Tuesday.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CoreWeave actually started its corporate life as a crypto miner but in short order built itself into a pivotal provider of “AI infrastructure” to some of the tech industry’s most major players. In that role, it provides GPUs to AI developers and has made major partnerships with Microsoft, OpenAI, Nvidia, Meta, and other tech titans.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Another topic broached Tuesday was the notion of “circularity” within the AI industry. “Circular” business deals, in which a small number of powerful AI companies invest in one another, have frequently been criticized and have raised questions about the industry’s long-term economic stability. Perhaps not surprisingly, since Nvidia is one of its investors and its supplier of GPUs, Intrator swatted away such concerns. “Companies are trying to address a violent change in supply and demand,” he said. “You do that by working together.”&lt;br /&gt;&amp;nbsp;&lt;br /&gt;Since the IPO, CoreWeave has continued to make efforts to expand its business. After it acquired Weights &amp;amp; Biases, an AI developer platform, in March, it went on to acquire OpenPipe, a startup that helps companies create and deploy AI agents through reinforcement learning. In October, it also made deals to acquire Marimo (the creator of an open source notebook) and Monolith, another AI company. It also recently announced an expansion of its cloud partnership with OpenAI and said it has plans to move into the federal market, where it wants to provide cloud infrastructure to U.S. government agencies and the defense industrial base.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/Mike-Intrator-Headshot-e1750958788389.jpg?w=898" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;It’s been quite the year for CoreWeave. In March, the AI cloud infrastructure provider went public in one of the biggest and most anticipated IPOs of the year that didn’t live up to its hype.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another setback took place in October, when a planned acquisition of the cloud provider’s business partner, Core Scientific, faltered due to skepticism from the acquisition target’s shareholders.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In the meantime, the firm has acquired a number of different companies, its stock has gone up and down, and it’s been both criticized and lauded for its role in the booming AI data center market.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In an interview at the Fortune Brainstorm AI summit in San Francisco on Tuesday, CoreWeave’s co-founder and CEO, Michael Intrator, defended his company’s performance from critics, noting that it was in the midst of creating a “new business model” for how cloud computing can be built and run. Their collection of Nvidia GPUs is so valuable, they borrow against it to help finance their business. The executive seemed to imply: If you’re charting a new path, you’re destined to encounter some road bumps along the way.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think people are myopic a lot of times,” Intrator said when questioned about his company’s occasionally unstable stock price. “Yes, it is seesawing,” he admitted, while noting that the CoreWeave IPO took place not long before President Trump’s tariffs went into effect — a notably uncertain moment for the overall economy.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We came out into one of the most challenging environments, right around Liberation Day and, in spite of the incredible headwinds, were able to launch a successful IPO,” the CEO told Brainstorm editorial director Andrew Nusca. “I couldn’t be prouder of what the company has accomplished,” he added.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CoreWeave’s stock may have debuted amid the economic doldrums of March but its price has gone on quite the journey since then. It debuted at $40 and, over the past eight months, has climbed to well over $150, but currently rests at around $90. Its more wary critics have compared it to a meme stock due to its penchant for going up and down.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Some of the uncertainty around CoreWeave’s stock has been credited to the company’s hefty level of debt. Not long after CoreWeave announced a deal on Monday to issue even more debt to finance its data center buildout, its stock dropped some 8%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Intrator seems to see his company as a disruptor, one whose unconventional tactics may take some getting used to. “When you introduce a new model, when you introduce a new way of doing business, when you disrupt what has been a static environment, it’s going to take some people some time,” he said during his appearance Tuesday.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CoreWeave actually started its corporate life as a crypto miner but in short order built itself into a pivotal provider of “AI infrastructure” to some of the tech industry’s most major players. In that role, it provides GPUs to AI developers and has made major partnerships with Microsoft, OpenAI, Nvidia, Meta, and other tech titans.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Another topic broached Tuesday was the notion of “circularity” within the AI industry. “Circular” business deals, in which a small number of powerful AI companies invest in one another, have frequently been criticized and have raised questions about the industry’s long-term economic stability. Perhaps not surprisingly, since Nvidia is one of its investors and its supplier of GPUs, Intrator swatted away such concerns. “Companies are trying to address a violent change in supply and demand,” he said. “You do that by working together.”&lt;br /&gt;&amp;nbsp;&lt;br /&gt;Since the IPO, CoreWeave has continued to make efforts to expand its business. After it acquired Weights &amp;amp; Biases, an AI developer platform, in March, it went on to acquire OpenPipe, a startup that helps companies create and deploy AI agents through reinforcement learning. In October, it also made deals to acquire Marimo (the creator of an open source notebook) and Monolith, another AI company. It also recently announced an expansion of its cloud partnership with OpenAI and said it has plans to move into the federal market, where it wants to provide cloud infrastructure to U.S. government agencies and the defense industrial base.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/coreweave-ceo-defends-ai-circular-deals-as-working-together/</guid><pubDate>Wed, 10 Dec 2025 00:49:25 +0000</pubDate></item><item><title>The AI that scored 95% — until consultants learned it was AI (AI | VentureBeat)</title><link>https://venturebeat.com/ai/the-ai-that-scored-95-until-consultants-learned-it-was-ai</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;i&gt;Presented by SAP&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;When SAP ran a quiet internal experiment to gauge consultant attitudes toward AI, the results were striking. Five teams were asked to validate answers to more than 1,000 business requirements completed by SAP’s AI co-pilot, Joule for Consultants — a workload that would normally take several weeks.&lt;/p&gt;&lt;p&gt;Four teams were told the analysis had been completed by junior interns fresh out of school. They reviewed the material, found it impressive, and rated the work about 95% accurate.&lt;/p&gt;&lt;p&gt;The fifth team was told the very same answers had come from AI.&lt;/p&gt;&lt;p&gt;They rejected almost everything.&lt;/p&gt;&lt;p&gt;Only when asked to validate each answer one by one did they discover that the AI was, in fact, highly accurate — surfacing detailed insights the consultants had initially dismissed. The overall accuracy? Again, about 95%.&lt;/p&gt;&lt;p&gt;“The lesson learned here is that we need to be very cautious as we introduce AI — especially in how we communicate with senior consultants about its possibilities and how to integrate it into their workflows,” says Guillermo B. Vazquez Mendez, chief architect, RI business transformation and architecture, SAP America Inc.&lt;/p&gt;&lt;p&gt;The experiment has since become a revealing starting point for SAP’s push toward the consultant of 2030: a practitioner who is deeply human, enabled by AI, and no longer weighed down by the technical grunt work of the past.&lt;/p&gt;&lt;h3&gt;Overcoming AI skepticism&lt;/h3&gt;&lt;p&gt;Resistance isn’t surprising, Vazquez notes. Consultants with two or three decades of experience carry enormous institutional knowledge — and an understandable degree of caution.&lt;/p&gt;&lt;p&gt;But AI copilots like Joule for Consultants are not replacing expertise. They’re amplifying it.&lt;/p&gt;&lt;p&gt;“What Joule really does is make their very expensive time far more effective,” Vazquez says. “It removes the clerical work, so they can focus on turning out high-quality answers in a fraction of the time.”&lt;/p&gt;&lt;p&gt;He emphasizes this message constantly: “AI is not replacing you. It’s a tool for you. Human oversight is always required. But now, instead of spending your time looking for documentation, you’re gaining significant time and boosting the effectiveness and detail of your answers.”&lt;/p&gt;&lt;h3&gt;The consultant time-shift: from tech execution to business insight&lt;/h3&gt;&lt;p&gt;Historically, consultants spent about 80% of their time understanding technical systems — how processes run, how data flows, how functions execute. Customers, by contrast, spend 80% of their time focused on their business.&lt;/p&gt;&lt;p&gt;That mismatch is exactly where Joule steps in.&lt;/p&gt;&lt;p&gt;“There’s a gap there — and the bridge is AI,” Vazquez says. “It flips the time equation, enabling consultants to invest more of their energy in understanding the customer’s industry and business goals. AI takes on the heavy technical lift, so consultants can focus on driving the right business outcomes.”&lt;/p&gt;&lt;h3&gt;Bringing new consultants up to speed&lt;/h3&gt;&lt;p&gt;AI is also transforming how new hires learn.&lt;/p&gt;&lt;p&gt;“We’re excited to see Joule acting as a bridge between senior consultants, who are adapting more slowly, and interns and new consultants who are already technically savvy,” Vazquez says.&lt;/p&gt;&lt;p&gt;Junior consultants ramp up faster because Joule helps them operate independently. Seniors, meanwhile, engage where their insight matters most.&lt;/p&gt;&lt;p&gt;This is also where many consultants learn the fundamentals of today’s AI copilots. Much of the work depends on prompt engineering — for instance, instructing Joule to act as a senior chief technology architect specializing in finance and SAP S/4HANA 2023, then asking it to analyze business requirements and deliver the output as tables or PowerPoint slides.&lt;/p&gt;&lt;p&gt;Once they grasp how to frame prompts, consultants consistently get higher-quality, more structured answers.&lt;/p&gt;&lt;p&gt;New architects are also able to communicate more clearly with their more experienced counterparts. They know what they don’t know and can ask targeted questions, which makes mentorship far smoother. It’s created a real synergy, Vazquez adds — senior consultants see how quickly new hires are adapting and learning with AI, and that momentum encourages them to keep pace and adopt the technology themselves.&lt;/p&gt;&lt;h3&gt;Looking ahead to the future of AI copilots&lt;/h3&gt;&lt;p&gt;“We’re still in the baby steps of AI — we’re toddlers,” Vazquez says. “Right now, copilots depend on prompt engineering to get good answers. The better you prompt, the better the answer you get.”&lt;/p&gt;&lt;p&gt;But that represents only the earliest phase of what these systems will eventually do. As copilots mature, they’ll move beyond responding to prompts and start interpreting entire business processes — understanding the sequence of steps, identifying where human intervention is needed, and spotting where an AI agent could take over. That shift is what leads directly into agentic AI.&lt;/p&gt;&lt;p&gt;SAP’s depth of process knowledge is what makes that evolution possible. The company has mapped more than 3,500 business processes across industries — a repository Vazquez calls “some of the most valuable, rigorously tested processes developed in the last 50 years.” Every day, SAP systems support roughly $7.3 trillion in global commerce, giving these emerging AI agents a rich foundation to navigate and reason over.&lt;/p&gt;&lt;p&gt;“With that level of process insight and data, we can take a real leap forward,” he says, “equipping our consultants with agentic AI that can solve complex challenges and push us toward increasingly autonomous systems.”&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;
&lt;i&gt;Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact &lt;/i&gt;&lt;a href="mailto:sales@venturebeat.com"&gt;&lt;i&gt;&lt;u&gt;sales@venturebeat.com&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;i&gt;Presented by SAP&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;When SAP ran a quiet internal experiment to gauge consultant attitudes toward AI, the results were striking. Five teams were asked to validate answers to more than 1,000 business requirements completed by SAP’s AI co-pilot, Joule for Consultants — a workload that would normally take several weeks.&lt;/p&gt;&lt;p&gt;Four teams were told the analysis had been completed by junior interns fresh out of school. They reviewed the material, found it impressive, and rated the work about 95% accurate.&lt;/p&gt;&lt;p&gt;The fifth team was told the very same answers had come from AI.&lt;/p&gt;&lt;p&gt;They rejected almost everything.&lt;/p&gt;&lt;p&gt;Only when asked to validate each answer one by one did they discover that the AI was, in fact, highly accurate — surfacing detailed insights the consultants had initially dismissed. The overall accuracy? Again, about 95%.&lt;/p&gt;&lt;p&gt;“The lesson learned here is that we need to be very cautious as we introduce AI — especially in how we communicate with senior consultants about its possibilities and how to integrate it into their workflows,” says Guillermo B. Vazquez Mendez, chief architect, RI business transformation and architecture, SAP America Inc.&lt;/p&gt;&lt;p&gt;The experiment has since become a revealing starting point for SAP’s push toward the consultant of 2030: a practitioner who is deeply human, enabled by AI, and no longer weighed down by the technical grunt work of the past.&lt;/p&gt;&lt;h3&gt;Overcoming AI skepticism&lt;/h3&gt;&lt;p&gt;Resistance isn’t surprising, Vazquez notes. Consultants with two or three decades of experience carry enormous institutional knowledge — and an understandable degree of caution.&lt;/p&gt;&lt;p&gt;But AI copilots like Joule for Consultants are not replacing expertise. They’re amplifying it.&lt;/p&gt;&lt;p&gt;“What Joule really does is make their very expensive time far more effective,” Vazquez says. “It removes the clerical work, so they can focus on turning out high-quality answers in a fraction of the time.”&lt;/p&gt;&lt;p&gt;He emphasizes this message constantly: “AI is not replacing you. It’s a tool for you. Human oversight is always required. But now, instead of spending your time looking for documentation, you’re gaining significant time and boosting the effectiveness and detail of your answers.”&lt;/p&gt;&lt;h3&gt;The consultant time-shift: from tech execution to business insight&lt;/h3&gt;&lt;p&gt;Historically, consultants spent about 80% of their time understanding technical systems — how processes run, how data flows, how functions execute. Customers, by contrast, spend 80% of their time focused on their business.&lt;/p&gt;&lt;p&gt;That mismatch is exactly where Joule steps in.&lt;/p&gt;&lt;p&gt;“There’s a gap there — and the bridge is AI,” Vazquez says. “It flips the time equation, enabling consultants to invest more of their energy in understanding the customer’s industry and business goals. AI takes on the heavy technical lift, so consultants can focus on driving the right business outcomes.”&lt;/p&gt;&lt;h3&gt;Bringing new consultants up to speed&lt;/h3&gt;&lt;p&gt;AI is also transforming how new hires learn.&lt;/p&gt;&lt;p&gt;“We’re excited to see Joule acting as a bridge between senior consultants, who are adapting more slowly, and interns and new consultants who are already technically savvy,” Vazquez says.&lt;/p&gt;&lt;p&gt;Junior consultants ramp up faster because Joule helps them operate independently. Seniors, meanwhile, engage where their insight matters most.&lt;/p&gt;&lt;p&gt;This is also where many consultants learn the fundamentals of today’s AI copilots. Much of the work depends on prompt engineering — for instance, instructing Joule to act as a senior chief technology architect specializing in finance and SAP S/4HANA 2023, then asking it to analyze business requirements and deliver the output as tables or PowerPoint slides.&lt;/p&gt;&lt;p&gt;Once they grasp how to frame prompts, consultants consistently get higher-quality, more structured answers.&lt;/p&gt;&lt;p&gt;New architects are also able to communicate more clearly with their more experienced counterparts. They know what they don’t know and can ask targeted questions, which makes mentorship far smoother. It’s created a real synergy, Vazquez adds — senior consultants see how quickly new hires are adapting and learning with AI, and that momentum encourages them to keep pace and adopt the technology themselves.&lt;/p&gt;&lt;h3&gt;Looking ahead to the future of AI copilots&lt;/h3&gt;&lt;p&gt;“We’re still in the baby steps of AI — we’re toddlers,” Vazquez says. “Right now, copilots depend on prompt engineering to get good answers. The better you prompt, the better the answer you get.”&lt;/p&gt;&lt;p&gt;But that represents only the earliest phase of what these systems will eventually do. As copilots mature, they’ll move beyond responding to prompts and start interpreting entire business processes — understanding the sequence of steps, identifying where human intervention is needed, and spotting where an AI agent could take over. That shift is what leads directly into agentic AI.&lt;/p&gt;&lt;p&gt;SAP’s depth of process knowledge is what makes that evolution possible. The company has mapped more than 3,500 business processes across industries — a repository Vazquez calls “some of the most valuable, rigorously tested processes developed in the last 50 years.” Every day, SAP systems support roughly $7.3 trillion in global commerce, giving these emerging AI agents a rich foundation to navigate and reason over.&lt;/p&gt;&lt;p&gt;“With that level of process insight and data, we can take a real leap forward,” he says, “equipping our consultants with agentic AI that can solve complex challenges and push us toward increasingly autonomous systems.”&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;
&lt;i&gt;Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact &lt;/i&gt;&lt;a href="mailto:sales@venturebeat.com"&gt;&lt;i&gt;&lt;u&gt;sales@venturebeat.com&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/the-ai-that-scored-95-until-consultants-learned-it-was-ai</guid><pubDate>Wed, 10 Dec 2025 15:00:00 +0000</pubDate></item></channel></rss>