<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 16 Sep 2025 12:42:44 +0000</lastBuildDate><item><title> ()</title><link>https://venturebeat.com/category/ai/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://venturebeat.com/category/ai/feed/</guid></item><item><title>[NEW] The looming crackdown on AI companionship (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/09/16/1123614/the-looming-crackdown-on-ai-companionship/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/GettyImages-901654862.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;As long as there has been AI, there have been people sounding alarms about what it might do to us: rogue superintelligence, mass unemployment, or environmental ruin from data center sprawl. But this week showed that another threat entirely—that of kids forming unhealthy bonds with AI—is the one pulling AI safety out of the academic fringe and into regulators’ crosshairs.&lt;/p&gt;  &lt;p&gt;This has been bubbling for a while. Two high-profile lawsuits filed in the last year, against Character.AI and OpenAI, allege that companion-like behavior in their models contributed to the suicides of two teenagers. A study by US nonprofit Common Sense Media, published in July, found that 72% of teenagers have used AI for companionship. Stories in reputable outlets about “AI psychosis” have highlighted how endless conversations with chatbots can lead people down delusional spirals.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;It’s hard to overstate the impact of these stories. To the public, they are proof that AI is not merely imperfect, but a technology that’s more harmful than helpful. If you doubted that this outrage would be taken seriously by regulators and companies, three things happened this week that might change your mind.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;A California law passes the legislature&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;On Thursday, the California state legislature passed a first-of-its-kind bill. It would require AI companies to include reminders for users they know to be minors that responses are AI generated. Companies would also need to have a protocol for addressing suicide and self-harm and provide annual reports on instances of suicidal ideation in users’ conversations with their chatbots. It was led by Democratic state senator Steve Padilla, passed with heavy bipartisan support, and now awaits Governor Gavin Newsom’s signature.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;There are reasons to be skeptical of the bill’s impact. It doesn’t specify efforts companies should take to identify which users are minors, and lots of AI companies already include referrals to crisis providers when someone is talking about suicide. (In the case of Adam Raine, one of the teenagers whose survivors are suing, his conversations with ChatGPT before his death included this type of information, but the chatbot allegedly went on to give advice related to suicide anyway.)&lt;/p&gt;  &lt;p&gt;Still, it is undoubtedly the most significant of the efforts to rein in companion-like behaviors in AI models, which are in the works in other states too. If the bill becomes law, it would strike a blow to the position OpenAI has taken, which is that “America leads best with clear, nationwide rules, not a patchwork of state or local regulations,” as the company’s chief global affairs officer, Chris Lehane, wrote on LinkedIn last week.&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;The Federal Trade Commission takes aim&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The very same day, the Federal Trade Commission announced an inquiry into seven companies, seeking information about how they develop companion-like characters, monetize engagement, measure and test the impact of their chatbots, and more. The companies are Google, Instagram, Meta, OpenAI, Snap, X, and Character Technologies, the maker of Character.AI.&lt;/p&gt;  &lt;p&gt;The White House now wields immense, and potentially illegal, political influence over the agency. In March, President Trump fired its lone Democratic commissioner, Rebecca Slaughter. In July, a federal judge ruled that firing illegal, but last week the US Supreme Court temporarily permitted the firing.&lt;/p&gt;  &lt;p&gt;“Protecting kids online is a top priority for the Trump-Vance FTC, and so is fostering innovation in critical sectors of our economy,” said FTC chairman Andrew Ferguson in a press release about the inquiry.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Right now, it’s just that—an inquiry—but the process might (depending on how public the FTC makes its findings) reveal the inner workings of how the companies build their AI companions to keep users coming back again and again.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Sam Altman on suicide cases&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;&lt;em&gt;Also&lt;/em&gt; on the same day (a busy day for AI news), Tucker Carlson published an hour-long interview with OpenAI’s CEO, Sam Altman. It covers a lot of ground—Altman’s battle with Elon Musk, OpenAI’s military customers, conspiracy theories about the death of a former employee—but it also includes the most candid comments Altman’s made so far about the cases of suicide following conversations with AI.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Altman talked about “the tension between user freedom and privacy and protecting vulnerable users” in cases like these. But then he offered up something I hadn’t heard before.&lt;/p&gt;  &lt;p&gt;“I think it’d be very reasonable for us to say that in cases of young people talking about suicide seriously, where we cannot get in touch with parents, we do call the authorities,” he said. “That would be a change.”&lt;/p&gt;  &lt;p&gt;So where does all this go next? For now, it’s clear that—at least in the case of children harmed by AI companionship—companies’ familiar playbook won’t hold. They can no longer deflect responsibility by leaning on privacy, personalization, or “user choice.” Pressure to take a harder line is mounting from state laws, regulators, and an outraged public.&lt;/p&gt; 

 &lt;p&gt;But what will that look like? Politically, the left and right are now paying attention to AI’s harm to children, but their solutions differ. On the right, the proposed solution aligns with the wave of internet age-verification laws that have now been passed in over 20 states. These are meant to shield kids from adult content while defending “family values.” On the left, it’s the revival of stalled ambitions to hold Big Tech accountable through antitrust and consumer-protection powers.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Consensus on the problem is easier than agreement on the cure. As it stands, it looks likely we’ll end up with exactly the patchwork of state and local regulations that OpenAI (and plenty of others) have lobbied against.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;For now, it’s down to companies to decide where to draw the lines. They’re having to decide things like: Should chatbots cut off conversations when users spiral toward self-harm, or would that leave some people worse off? Should they be licensed and regulated like therapists, or treated as entertainment products with warnings? The uncertainty stems from a basic contradiction: Companies have built chatbots to act like caring humans, but they’ve postponed developing the standards and accountability we demand of real caregivers. The clock is now running out.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,&amp;nbsp;sign up here.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/GettyImages-901654862.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;As long as there has been AI, there have been people sounding alarms about what it might do to us: rogue superintelligence, mass unemployment, or environmental ruin from data center sprawl. But this week showed that another threat entirely—that of kids forming unhealthy bonds with AI—is the one pulling AI safety out of the academic fringe and into regulators’ crosshairs.&lt;/p&gt;  &lt;p&gt;This has been bubbling for a while. Two high-profile lawsuits filed in the last year, against Character.AI and OpenAI, allege that companion-like behavior in their models contributed to the suicides of two teenagers. A study by US nonprofit Common Sense Media, published in July, found that 72% of teenagers have used AI for companionship. Stories in reputable outlets about “AI psychosis” have highlighted how endless conversations with chatbots can lead people down delusional spirals.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;It’s hard to overstate the impact of these stories. To the public, they are proof that AI is not merely imperfect, but a technology that’s more harmful than helpful. If you doubted that this outrage would be taken seriously by regulators and companies, three things happened this week that might change your mind.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;A California law passes the legislature&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;On Thursday, the California state legislature passed a first-of-its-kind bill. It would require AI companies to include reminders for users they know to be minors that responses are AI generated. Companies would also need to have a protocol for addressing suicide and self-harm and provide annual reports on instances of suicidal ideation in users’ conversations with their chatbots. It was led by Democratic state senator Steve Padilla, passed with heavy bipartisan support, and now awaits Governor Gavin Newsom’s signature.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;There are reasons to be skeptical of the bill’s impact. It doesn’t specify efforts companies should take to identify which users are minors, and lots of AI companies already include referrals to crisis providers when someone is talking about suicide. (In the case of Adam Raine, one of the teenagers whose survivors are suing, his conversations with ChatGPT before his death included this type of information, but the chatbot allegedly went on to give advice related to suicide anyway.)&lt;/p&gt;  &lt;p&gt;Still, it is undoubtedly the most significant of the efforts to rein in companion-like behaviors in AI models, which are in the works in other states too. If the bill becomes law, it would strike a blow to the position OpenAI has taken, which is that “America leads best with clear, nationwide rules, not a patchwork of state or local regulations,” as the company’s chief global affairs officer, Chris Lehane, wrote on LinkedIn last week.&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;The Federal Trade Commission takes aim&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The very same day, the Federal Trade Commission announced an inquiry into seven companies, seeking information about how they develop companion-like characters, monetize engagement, measure and test the impact of their chatbots, and more. The companies are Google, Instagram, Meta, OpenAI, Snap, X, and Character Technologies, the maker of Character.AI.&lt;/p&gt;  &lt;p&gt;The White House now wields immense, and potentially illegal, political influence over the agency. In March, President Trump fired its lone Democratic commissioner, Rebecca Slaughter. In July, a federal judge ruled that firing illegal, but last week the US Supreme Court temporarily permitted the firing.&lt;/p&gt;  &lt;p&gt;“Protecting kids online is a top priority for the Trump-Vance FTC, and so is fostering innovation in critical sectors of our economy,” said FTC chairman Andrew Ferguson in a press release about the inquiry.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Right now, it’s just that—an inquiry—but the process might (depending on how public the FTC makes its findings) reveal the inner workings of how the companies build their AI companions to keep users coming back again and again.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Sam Altman on suicide cases&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;&lt;em&gt;Also&lt;/em&gt; on the same day (a busy day for AI news), Tucker Carlson published an hour-long interview with OpenAI’s CEO, Sam Altman. It covers a lot of ground—Altman’s battle with Elon Musk, OpenAI’s military customers, conspiracy theories about the death of a former employee—but it also includes the most candid comments Altman’s made so far about the cases of suicide following conversations with AI.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Altman talked about “the tension between user freedom and privacy and protecting vulnerable users” in cases like these. But then he offered up something I hadn’t heard before.&lt;/p&gt;  &lt;p&gt;“I think it’d be very reasonable for us to say that in cases of young people talking about suicide seriously, where we cannot get in touch with parents, we do call the authorities,” he said. “That would be a change.”&lt;/p&gt;  &lt;p&gt;So where does all this go next? For now, it’s clear that—at least in the case of children harmed by AI companionship—companies’ familiar playbook won’t hold. They can no longer deflect responsibility by leaning on privacy, personalization, or “user choice.” Pressure to take a harder line is mounting from state laws, regulators, and an outraged public.&lt;/p&gt; 

 &lt;p&gt;But what will that look like? Politically, the left and right are now paying attention to AI’s harm to children, but their solutions differ. On the right, the proposed solution aligns with the wave of internet age-verification laws that have now been passed in over 20 states. These are meant to shield kids from adult content while defending “family values.” On the left, it’s the revival of stalled ambitions to hold Big Tech accountable through antitrust and consumer-protection powers.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Consensus on the problem is easier than agreement on the cure. As it stands, it looks likely we’ll end up with exactly the patchwork of state and local regulations that OpenAI (and plenty of others) have lobbied against.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;For now, it’s down to companies to decide where to draw the lines. They’re having to decide things like: Should chatbots cut off conversations when users spiral toward self-harm, or would that leave some people worse off? Should they be licensed and regulated like therapists, or treated as entertainment products with warnings? The uncertainty stems from a basic contradiction: Companies have built chatbots to act like caring humans, but they’ve postponed developing the standards and accountability we demand of real caregivers. The clock is now running out.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,&amp;nbsp;sign up here.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/09/16/1123614/the-looming-crackdown-on-ai-companionship/</guid><pubDate>Tue, 16 Sep 2025 09:00:00 +0000</pubDate></item><item><title>[NEW] Millions turn to AI chatbots for spiritual guidance and confession (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/09/millions-turn-to-ai-chatbots-for-spiritual-guidance-and-confession/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Bible Chat hits 30 million downloads as users seek algorithmic absolution.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="3D-generated image of a crowd of people watching a humanoid robot on a screen." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/robot_church-640x360.jpg" width="640" /&gt;
                  &lt;img alt="3D-generated image of a crowd of people watching a humanoid robot on a screen." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/robot_church-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          gremlin via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Sunday, The New York Times reported that tens of millions of people are confessing secrets to AI chatbots trained on religious texts, with apps like Bible Chat reaching over 30 million downloads and Catholic app Hallow briefly topping Netflix, Instagram, and TikTok in Apple's App Store. In China, people are using DeepSeek to try to decode their fortunes. In her report, Lauren Jackson examined "faith tech" apps that cost users up to $70 annually, with some platforms claiming to channel divine communication directly.&lt;/p&gt;
&lt;p&gt;Some of the apps address what creators describe as an accessibility problem. "You don't want to disturb your pastor at three in the morning," Krista Rogers, a 61-year-old Ohio resident, told the Times about using the YouVersion Bible app and ChatGPT for spiritual questions.&lt;/p&gt;
&lt;p&gt;The report also examines platforms that go beyond simple scriptural guidance. While a service like ChatwithGod operates as a "spiritual advisor," its conversational nature is convincing enough that users often question whether they are speaking directly with a divine being. As its chief executive told the Times, the most frequent question from users is, "Is this actually God I am talking to?"&lt;/p&gt;
&lt;p&gt;The answer, of course, is no. These chatbots operate like other large language models—they generate statistically plausible text based on patterns in training data, not divine words from the heavens. When trained on religious texts, they produce responses that sound spiritually informed but can potentially mislead people with erroneous information or reassurance. Unlike human spiritual advisors, chatbots cannot have your best interests in mind because they don't have a mind: Chatbots are neither people nor supernatural beings.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The fusion of AI and religion isn't entirely new. In 2023, we reported on an experimental ChatGPT-powered church service at St. Paul's Church in Fürth, Germany, where over 300 attendees watched computer-generated avatars deliver a 40-minute sermon. Jonas Simmerlein, the theologian behind that event, framed it as learning to deal with AI's increasing presence in all aspects of life. But while that service was an intentional experiment with congregants aware they were hearing machine-generated text, today's faith tech apps blur the line between human spiritual guidance and algorithmic pattern matching, with millions of users potentially unaware of the distinction.&lt;/p&gt;
&lt;h2&gt;Theological yes-men&lt;/h2&gt;
&lt;p&gt;Many of these spiritual-flavored chatbots run on the same AI language models that run apps like ChatGPT and Gemini. While some companies train these models on religious texts and consult theologians about fine-tuning responses, it's widely known (and even admitted by the companies that make them) that these AI models trend toward outputs that validate users' feelings and ideas. If content-filtering safeguards fail, the tendency to affirm all ideas can lead to dangerous situations for vulnerable users.&lt;/p&gt;
&lt;p&gt;"They're generally affirming. They are generally 'yes men,'" Ryan Beck, chief technology officer at Pray.com, told the Times. While this tendency, called "sycophancy" in the AI industry, has led to life-threatening problems with some users, Beck sees affirmation as beneficial. "Who doesn't need a little affirmation in their life?"&lt;/p&gt;
&lt;p&gt;This validation tendency creates theological complications. Traditional faith practices often involve challenging believers to confront uncomfortable truths, but chatbots avoid this spiritual friction. Heidi Campbell, a Texas A&amp;amp;M professor who studies technology and religion, told The Times that chatbots "tell us what we want to hear," and "it's not using spiritual discernment, it is using data and patterns."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Privacy concerns compound these issues. "I wonder if there isn't a larger danger in pouring your heart out to a chatbot," Catholic priest Fr. Mike Schmitz told The Times. "Is it at some point going to become accessible to other people?" Users share intimate spiritual moments that now exist as data points in corporate servers.&lt;/p&gt;
&lt;p&gt;Some users prefer the chatbots' non-judgmental responses to human religious communities. Delphine Collins, a 43-year-old Detroit preschool teacher, told the Times she found more support on Bible Chat than at her church after sharing her health struggles. "People stopped talking to me. It was horrible."&lt;/p&gt;
&lt;p&gt;App creators maintain that their products supplement rather than replace human spiritual connection, and the apps arrive as approximately 40 million people have left US churches in recent decades. "They aren't going to church like they used to," Beck said. "But it's not that they're less inclined to find spiritual nourishment. It's just that they do it through different modes."&lt;/p&gt;
&lt;p&gt;Different modes indeed. What faith-seeking users may not realize is that each chatbot response emerges fresh from the prompt you provide, with no permanent thread connecting one instance to the next beyond a rolling history of the present conversation and what might be stored as a "memory" in a separate system. When a religious chatbot says, "I'll pray for you," the simulated "I" making that promise ceases to exist the moment the response completes. There's no persistent identity to provide ongoing spiritual guidance, and no memory of your spiritual journey beyond what gets fed back into the prompt with every query.&lt;/p&gt;
&lt;p&gt;But this is spirituality we're talking about, and despite technical realities, many people will believe that the chatbots can give them divine guidance. In matters of faith, contradictory evidence rarely shakes a strong belief once it takes hold, whether that faith is placed in the divine or in what are essentially voices emanating from a roll of loaded dice. For many, there may not be much difference.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Bible Chat hits 30 million downloads as users seek algorithmic absolution.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="3D-generated image of a crowd of people watching a humanoid robot on a screen." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/robot_church-640x360.jpg" width="640" /&gt;
                  &lt;img alt="3D-generated image of a crowd of people watching a humanoid robot on a screen." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/robot_church-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          gremlin via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Sunday, The New York Times reported that tens of millions of people are confessing secrets to AI chatbots trained on religious texts, with apps like Bible Chat reaching over 30 million downloads and Catholic app Hallow briefly topping Netflix, Instagram, and TikTok in Apple's App Store. In China, people are using DeepSeek to try to decode their fortunes. In her report, Lauren Jackson examined "faith tech" apps that cost users up to $70 annually, with some platforms claiming to channel divine communication directly.&lt;/p&gt;
&lt;p&gt;Some of the apps address what creators describe as an accessibility problem. "You don't want to disturb your pastor at three in the morning," Krista Rogers, a 61-year-old Ohio resident, told the Times about using the YouVersion Bible app and ChatGPT for spiritual questions.&lt;/p&gt;
&lt;p&gt;The report also examines platforms that go beyond simple scriptural guidance. While a service like ChatwithGod operates as a "spiritual advisor," its conversational nature is convincing enough that users often question whether they are speaking directly with a divine being. As its chief executive told the Times, the most frequent question from users is, "Is this actually God I am talking to?"&lt;/p&gt;
&lt;p&gt;The answer, of course, is no. These chatbots operate like other large language models—they generate statistically plausible text based on patterns in training data, not divine words from the heavens. When trained on religious texts, they produce responses that sound spiritually informed but can potentially mislead people with erroneous information or reassurance. Unlike human spiritual advisors, chatbots cannot have your best interests in mind because they don't have a mind: Chatbots are neither people nor supernatural beings.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The fusion of AI and religion isn't entirely new. In 2023, we reported on an experimental ChatGPT-powered church service at St. Paul's Church in Fürth, Germany, where over 300 attendees watched computer-generated avatars deliver a 40-minute sermon. Jonas Simmerlein, the theologian behind that event, framed it as learning to deal with AI's increasing presence in all aspects of life. But while that service was an intentional experiment with congregants aware they were hearing machine-generated text, today's faith tech apps blur the line between human spiritual guidance and algorithmic pattern matching, with millions of users potentially unaware of the distinction.&lt;/p&gt;
&lt;h2&gt;Theological yes-men&lt;/h2&gt;
&lt;p&gt;Many of these spiritual-flavored chatbots run on the same AI language models that run apps like ChatGPT and Gemini. While some companies train these models on religious texts and consult theologians about fine-tuning responses, it's widely known (and even admitted by the companies that make them) that these AI models trend toward outputs that validate users' feelings and ideas. If content-filtering safeguards fail, the tendency to affirm all ideas can lead to dangerous situations for vulnerable users.&lt;/p&gt;
&lt;p&gt;"They're generally affirming. They are generally 'yes men,'" Ryan Beck, chief technology officer at Pray.com, told the Times. While this tendency, called "sycophancy" in the AI industry, has led to life-threatening problems with some users, Beck sees affirmation as beneficial. "Who doesn't need a little affirmation in their life?"&lt;/p&gt;
&lt;p&gt;This validation tendency creates theological complications. Traditional faith practices often involve challenging believers to confront uncomfortable truths, but chatbots avoid this spiritual friction. Heidi Campbell, a Texas A&amp;amp;M professor who studies technology and religion, told The Times that chatbots "tell us what we want to hear," and "it's not using spiritual discernment, it is using data and patterns."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Privacy concerns compound these issues. "I wonder if there isn't a larger danger in pouring your heart out to a chatbot," Catholic priest Fr. Mike Schmitz told The Times. "Is it at some point going to become accessible to other people?" Users share intimate spiritual moments that now exist as data points in corporate servers.&lt;/p&gt;
&lt;p&gt;Some users prefer the chatbots' non-judgmental responses to human religious communities. Delphine Collins, a 43-year-old Detroit preschool teacher, told the Times she found more support on Bible Chat than at her church after sharing her health struggles. "People stopped talking to me. It was horrible."&lt;/p&gt;
&lt;p&gt;App creators maintain that their products supplement rather than replace human spiritual connection, and the apps arrive as approximately 40 million people have left US churches in recent decades. "They aren't going to church like they used to," Beck said. "But it's not that they're less inclined to find spiritual nourishment. It's just that they do it through different modes."&lt;/p&gt;
&lt;p&gt;Different modes indeed. What faith-seeking users may not realize is that each chatbot response emerges fresh from the prompt you provide, with no permanent thread connecting one instance to the next beyond a rolling history of the present conversation and what might be stored as a "memory" in a separate system. When a religious chatbot says, "I'll pray for you," the simulated "I" making that promise ceases to exist the moment the response completes. There's no persistent identity to provide ongoing spiritual guidance, and no memory of your spiritual journey beyond what gets fed back into the prompt with every query.&lt;/p&gt;
&lt;p&gt;But this is spirituality we're talking about, and despite technical realities, many people will believe that the chatbots can give them divine guidance. In matters of faith, contradictory evidence rarely shakes a strong belief once it takes hold, whether that faith is placed in the divine or in what are essentially voices emanating from a roll of loaded dice. For many, there may not be much difference.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/09/millions-turn-to-ai-chatbots-for-spiritual-guidance-and-confession/</guid><pubDate>Tue, 16 Sep 2025 11:15:32 +0000</pubDate></item><item><title>[NEW] Y Combinator-backed Rulebase wants to be the AI coworker for fintech (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/16/y-combinator-backed-rulebase-wants-to-be-the-ai-coworker-for-fintech/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Y Combinator-alum Rulebase is betting that the next wave of automation in financial services won’t be about flashy AI interfaces, but the unglamorous back-office tasks like compliance.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup, founded by Gideon Ebose and Chidi Williams, two Nigerian engineers who met in London, just raised a $2.1 million pre-seed round led by Bowery Capital, with participation from Y Combinator, Commerce Ventures, Transpose Platform VC, alongside several angels.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Financial services firms spend enormous amounts of effort on support tickets, resolving disputes, ensuring quality assurance, and regulatory compliance. Rulebase’s software, which it calls an agent coworker, replaces much of the manual grunt work in these tasks. Its AI agent can evaluate customer interactions, flag regulatory risks, and trigger the right follow-ups across tools like Zendesk, Jira, and Slack without losing the human-in-the-loop oversight that financial firms demand.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our ‘Coworker’ tool integrates across platforms and collaborates with human agents and back-office teams to fully manage the dispute lifecycle while saving time, reducing errors, and maintaining compliance,” said CTO Williams. Currently, the year-old startup is already deployed at customers like U.S. business banking platform Rho and an unnamed Fortune 50 financial institution.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rulebase wasn’t the founders’ first swing. Ebose, a former product lead at Microsoft and Williams, a former backend engineer at Goldman Sachs, built several products together like an AI customer feedback tool before eventually settling on Rulebase. The idea came about after seeing how inefficient back-office operations were in small and large financial institutions, especially when it came to regulatory workflows.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup currently focuses on workflows triggered by customer service interactions, with its first wedge around quality assurance. QA analysts in traditional financial institutions typically manually review 3–5% of support interactions to ensure reps follow compliance protocols.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-wp-embed is-provider-techcrunch wp-block-embed-techcrunch"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Rulebase now evaluates 100% of such interactions, cutting costs by up to 70%, the founders say. In the case of Rho, for instance, Rulebase has helped cut escalations by up to 30%.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“We automate workflows that start with a customer interaction, areas we’re already great at handling end-to-end,” CEO Ebose said in an interview with TechCrunch. “While much of that is QA, compliance, and disputes tied to customer calls and messages, long-term our goal is to take on as many manual back-office tasks as possible by pulling these fragmented steps and tabs into one coordinated workflow.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new funding will help it double down on engineering and eventually add new features to AI Coworker like fraud investigation, audit preparation, and regulatory reporting.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rulebase is focused on financial services for now because automation demands precision. “You need to understand MasterCard’s rules, CFPB timelines. That depth of domain knowledge is our moat,” Ebose said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;They company is targeting business banks, neobanks, and card issuers across Africa, Europe, and the U.S. But the roadmap could eventually include adjacent verticals like insurance, where similar workflows exist.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3046286" height="515" src="https://techcrunch.com/wp-content/uploads/2025/09/IMG_2147.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Screenshot&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Revenue is growing fast, with “double-digit” month-over-month growth since joining Y Combinator’s Fall 2024 batch, the founders say. Rulebase’s business model is usage-based, charging per interaction reviewed or workflow automated.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As one of the few African founders to get into YC building AI tools, Ebose and Williams’ advice to founders trying to get accepted into the global accelerator is to think globally from day one.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re in a moment where small teams can deliver more value, more quickly, than ever before, so limiting yourself to ‘X for Y’ or a narrow vertical feels like a missed opportunity,” Williams remarked. “With AI, it feels obvious that you have to go after something massive. Anything less than the most ambitious version of your idea likely won’t cut it,” said Williams, who before Rulebase built Buzz, an early open-source speech-to-text tool with over 300,000 downloads and over 12,000 GitHub stars.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Y Combinator-alum Rulebase is betting that the next wave of automation in financial services won’t be about flashy AI interfaces, but the unglamorous back-office tasks like compliance.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup, founded by Gideon Ebose and Chidi Williams, two Nigerian engineers who met in London, just raised a $2.1 million pre-seed round led by Bowery Capital, with participation from Y Combinator, Commerce Ventures, Transpose Platform VC, alongside several angels.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Financial services firms spend enormous amounts of effort on support tickets, resolving disputes, ensuring quality assurance, and regulatory compliance. Rulebase’s software, which it calls an agent coworker, replaces much of the manual grunt work in these tasks. Its AI agent can evaluate customer interactions, flag regulatory risks, and trigger the right follow-ups across tools like Zendesk, Jira, and Slack without losing the human-in-the-loop oversight that financial firms demand.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our ‘Coworker’ tool integrates across platforms and collaborates with human agents and back-office teams to fully manage the dispute lifecycle while saving time, reducing errors, and maintaining compliance,” said CTO Williams. Currently, the year-old startup is already deployed at customers like U.S. business banking platform Rho and an unnamed Fortune 50 financial institution.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rulebase wasn’t the founders’ first swing. Ebose, a former product lead at Microsoft and Williams, a former backend engineer at Goldman Sachs, built several products together like an AI customer feedback tool before eventually settling on Rulebase. The idea came about after seeing how inefficient back-office operations were in small and large financial institutions, especially when it came to regulatory workflows.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup currently focuses on workflows triggered by customer service interactions, with its first wedge around quality assurance. QA analysts in traditional financial institutions typically manually review 3–5% of support interactions to ensure reps follow compliance protocols.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-wp-embed is-provider-techcrunch wp-block-embed-techcrunch"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Rulebase now evaluates 100% of such interactions, cutting costs by up to 70%, the founders say. In the case of Rho, for instance, Rulebase has helped cut escalations by up to 30%.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“We automate workflows that start with a customer interaction, areas we’re already great at handling end-to-end,” CEO Ebose said in an interview with TechCrunch. “While much of that is QA, compliance, and disputes tied to customer calls and messages, long-term our goal is to take on as many manual back-office tasks as possible by pulling these fragmented steps and tabs into one coordinated workflow.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new funding will help it double down on engineering and eventually add new features to AI Coworker like fraud investigation, audit preparation, and regulatory reporting.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rulebase is focused on financial services for now because automation demands precision. “You need to understand MasterCard’s rules, CFPB timelines. That depth of domain knowledge is our moat,” Ebose said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;They company is targeting business banks, neobanks, and card issuers across Africa, Europe, and the U.S. But the roadmap could eventually include adjacent verticals like insurance, where similar workflows exist.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3046286" height="515" src="https://techcrunch.com/wp-content/uploads/2025/09/IMG_2147.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Screenshot&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Revenue is growing fast, with “double-digit” month-over-month growth since joining Y Combinator’s Fall 2024 batch, the founders say. Rulebase’s business model is usage-based, charging per interaction reviewed or workflow automated.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As one of the few African founders to get into YC building AI tools, Ebose and Williams’ advice to founders trying to get accepted into the global accelerator is to think globally from day one.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re in a moment where small teams can deliver more value, more quickly, than ever before, so limiting yourself to ‘X for Y’ or a narrow vertical feels like a missed opportunity,” Williams remarked. “With AI, it feels obvious that you have to go after something massive. Anything less than the most ambitious version of your idea likely won’t cut it,” said Williams, who before Rulebase built Buzz, an early open-source speech-to-text tool with over 300,000 downloads and over 12,000 GitHub stars.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/16/y-combinator-backed-rulebase-wants-to-be-the-ai-coworker-for-fintech/</guid><pubDate>Tue, 16 Sep 2025 12:03:04 +0000</pubDate></item><item><title>[NEW] The Download: regulators are coming for AI companions, and meet our Innovator of 2025 (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/09/16/1123695/the-download-regulators-are-coming-for-ai-companions-and-meet-our-innovator-of-2025/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The looming crackdown on AI companionship&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;As long as there has been AI, there have been people sounding alarms about what it might do to us: rogue superintelligence, mass unemployment, or environmental ruin. But another threat entirely—that of kids forming unhealthy bonds with AI—is pulling AI safety out of the academic fringe and into regulators’ crosshairs.&lt;/p&gt;  &lt;p&gt;This has been bubbling for a while. Two high-profile lawsuits filed in the last year, against Character.AI and OpenAI, allege that their models contributed to the suicides of two teenagers. A study published in July, found that 72% of teenagers have used AI for companionship. And stories about “AI psychosis” have highlighted how endless conversations with chatbots can lead people down delusional spirals.&lt;/p&gt; 
 &lt;p&gt;It’s hard to overstate the impact of these stories. To the public, they are proof that AI is not merely imperfect, but harmful. If you doubted that this outrage would be taken seriously by regulators and companies, three things happened this week that might change your mind.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—James O’Donnell&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;If you’re interested in reading more about AI companionship, why not check out:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;+ AI companions are the final stage of digital addiction—and lawmakers are taking aim. Read the full story.&lt;/p&gt;&lt;p&gt;+ Chatbots are rapidly changing how we connect to each other—and ourselves. We’re never going back. Read the full story.&lt;/p&gt;  &lt;p&gt;+ Why GPT-4o’s sudden shutdown last month left people grieving. Read the full story.&lt;/p&gt;&lt;p&gt;+ An AI chatbot told a user how to kill himself—but the company doesn’t want to “censor” it.&lt;/p&gt;&lt;p&gt;+ OpenAI has released its first research into how using ChatGPT affects people’s emotional well-being. But there’s still a lot we don’t know.&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Meet the designer of the world’s fastest whole-genome sequencing method&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Every year, MIT Technology Review selects one individual whose work we admire to recognize as Innovator of the Year. For 2025, we chose Sneha Goenka, who designed the computations behind the world’s fastest whole-genome sequencing method. Thanks to her work, physicians can now sequence a patient’s genome and diagnose a genetic condition in less than eight hours—an achievement that could transform medical care.&lt;/p&gt;  &lt;p&gt;Register here to join an exclusive subscriber-only Roundtable conversation with Goenka, Leilani Battle, assistant professor at the University of Washington, and our editor in chief Mat Honan at 1pm ET on Tuesday September 23.&lt;/p&gt;   

 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Childhood vaccination rates are falling across the US&lt;/strong&gt;&lt;br /&gt;Much of the country no longer has the means to stop the spread of deadly disease. (NBC News)&lt;br /&gt;+ &lt;em&gt;Take a look at the factors driving vaccine hesitancy. &lt;/em&gt;(WP $)&lt;br /&gt;+ &lt;em&gt;RFK Jr is appointing more vaccine skeptics to the CDC advisory panel&lt;/em&gt;. (Ars Technica)&lt;br /&gt;+ &lt;em&gt;Why US federal health agencies are abandoning mRNA vaccines. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 The US and China have reached a TikTok deal&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Beijing says the spin-off version sold to US investors will still use ByteDance’s algorithm. (FT $)&lt;br /&gt;+ &lt;em&gt;But further details are still pretty scarce. &lt;/em&gt;(WP $)&lt;br /&gt;+ &lt;em&gt;The deal may have been fueled by China’s desire for Trump to visit the country. &lt;/em&gt;(WSJ $)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;3 OpenAI is releasing a version of GPT-5 optimized for agentic coding&lt;br /&gt;It’s a direct rival to Anthropic’s Claude Code and Microsoft’s GitHub Copilot. (TechCrunch)&lt;br /&gt;+ &lt;em&gt;OpenAI says it’s been trained on real-world engineering tasks. &lt;/em&gt;(VentureBeat)&lt;br /&gt;+ &lt;em&gt;The second wave of AI coding is here. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;4&lt;/strong&gt; &lt;strong&gt;The FTC is investigating Ticketmaster’s bot-fighting measures&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;It’s probing whether the platform is doing enough to prevent illegal automated reselling. (Bloomberg $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 Google has created a new privacy-preserving LLM&lt;br /&gt;&lt;/strong&gt;VaultGemma uses a technique called differential privacy to reduce the amount of data AI holds onto. (Ars Technica)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 Space tech firms are fighting it out for NATO contracts&lt;br /&gt;&lt;/strong&gt;Militaries are willing to branch out and strike deals with commercial vendors. (FT $)&lt;br /&gt;+ &lt;em&gt;Why Trump’s “golden dome” missile defense idea is another ripped straight from the movies. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Facebook users are receiving their Cambridge Analytica payouts&lt;/strong&gt;&lt;br /&gt;Don’t spend it all at once! (The Verge)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;8 The future of supercomputing could hinge on moon mining missions&lt;br /&gt;&lt;/strong&gt;Companies are rushing to buy the moon’s resources before mining has even begun. (WP $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 What it’s like living with an AI toy&lt;/strong&gt;&lt;br /&gt;Featuring unsettling conversations galore. (The Guardian)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10 Anthropic’s staff are obsessed with an albino alligator 🐊&lt;/strong&gt;&lt;br /&gt;As luck would have it, he just happens to be called Claude. (WSJ $)&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p class="has-large-font-size"&gt;&lt;strong&gt;“It’s going to mean more infections, more hospitalizations, more disability and more death.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Demetre Daskalakis, former director of the CDC's National Center for Immunization and Respiratory Diseases, explains the probable outcomes of America’s current vaccine policy jumble, the BBC reports.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; 
 &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1123697" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/image_fa8fb9.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Robots are bringing new life to extinct species&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In the last few years, paleontologists have developed a new trick for turning back time and studying prehistoric animals: building experimental robotic models of them.&lt;/p&gt;&lt;p&gt;In the absence of a living specimen, scientists say, an ambling, flying, swimming, or slithering automaton is the next best thing for studying the behavior of extinct organisms. Here are four examples of robots that are shedding light on creatures of yore.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Shi En Kim&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ New York City is full of natural life, if you know where to look.&lt;br /&gt;+ This photo of Jim Morrison enjoying a beer for breakfast is the epitome of rock ‘n’ roll.&lt;br /&gt;+ How to age like a champion athlete.&lt;br /&gt;+ Would you dare drive the world’s most narrow car?&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The looming crackdown on AI companionship&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;As long as there has been AI, there have been people sounding alarms about what it might do to us: rogue superintelligence, mass unemployment, or environmental ruin. But another threat entirely—that of kids forming unhealthy bonds with AI—is pulling AI safety out of the academic fringe and into regulators’ crosshairs.&lt;/p&gt;  &lt;p&gt;This has been bubbling for a while. Two high-profile lawsuits filed in the last year, against Character.AI and OpenAI, allege that their models contributed to the suicides of two teenagers. A study published in July, found that 72% of teenagers have used AI for companionship. And stories about “AI psychosis” have highlighted how endless conversations with chatbots can lead people down delusional spirals.&lt;/p&gt; 
 &lt;p&gt;It’s hard to overstate the impact of these stories. To the public, they are proof that AI is not merely imperfect, but harmful. If you doubted that this outrage would be taken seriously by regulators and companies, three things happened this week that might change your mind.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—James O’Donnell&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;If you’re interested in reading more about AI companionship, why not check out:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;+ AI companions are the final stage of digital addiction—and lawmakers are taking aim. Read the full story.&lt;/p&gt;&lt;p&gt;+ Chatbots are rapidly changing how we connect to each other—and ourselves. We’re never going back. Read the full story.&lt;/p&gt;  &lt;p&gt;+ Why GPT-4o’s sudden shutdown last month left people grieving. Read the full story.&lt;/p&gt;&lt;p&gt;+ An AI chatbot told a user how to kill himself—but the company doesn’t want to “censor” it.&lt;/p&gt;&lt;p&gt;+ OpenAI has released its first research into how using ChatGPT affects people’s emotional well-being. But there’s still a lot we don’t know.&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Meet the designer of the world’s fastest whole-genome sequencing method&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Every year, MIT Technology Review selects one individual whose work we admire to recognize as Innovator of the Year. For 2025, we chose Sneha Goenka, who designed the computations behind the world’s fastest whole-genome sequencing method. Thanks to her work, physicians can now sequence a patient’s genome and diagnose a genetic condition in less than eight hours—an achievement that could transform medical care.&lt;/p&gt;  &lt;p&gt;Register here to join an exclusive subscriber-only Roundtable conversation with Goenka, Leilani Battle, assistant professor at the University of Washington, and our editor in chief Mat Honan at 1pm ET on Tuesday September 23.&lt;/p&gt;   

 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Childhood vaccination rates are falling across the US&lt;/strong&gt;&lt;br /&gt;Much of the country no longer has the means to stop the spread of deadly disease. (NBC News)&lt;br /&gt;+ &lt;em&gt;Take a look at the factors driving vaccine hesitancy. &lt;/em&gt;(WP $)&lt;br /&gt;+ &lt;em&gt;RFK Jr is appointing more vaccine skeptics to the CDC advisory panel&lt;/em&gt;. (Ars Technica)&lt;br /&gt;+ &lt;em&gt;Why US federal health agencies are abandoning mRNA vaccines. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 The US and China have reached a TikTok deal&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Beijing says the spin-off version sold to US investors will still use ByteDance’s algorithm. (FT $)&lt;br /&gt;+ &lt;em&gt;But further details are still pretty scarce. &lt;/em&gt;(WP $)&lt;br /&gt;+ &lt;em&gt;The deal may have been fueled by China’s desire for Trump to visit the country. &lt;/em&gt;(WSJ $)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;3 OpenAI is releasing a version of GPT-5 optimized for agentic coding&lt;br /&gt;It’s a direct rival to Anthropic’s Claude Code and Microsoft’s GitHub Copilot. (TechCrunch)&lt;br /&gt;+ &lt;em&gt;OpenAI says it’s been trained on real-world engineering tasks. &lt;/em&gt;(VentureBeat)&lt;br /&gt;+ &lt;em&gt;The second wave of AI coding is here. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;4&lt;/strong&gt; &lt;strong&gt;The FTC is investigating Ticketmaster’s bot-fighting measures&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;It’s probing whether the platform is doing enough to prevent illegal automated reselling. (Bloomberg $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 Google has created a new privacy-preserving LLM&lt;br /&gt;&lt;/strong&gt;VaultGemma uses a technique called differential privacy to reduce the amount of data AI holds onto. (Ars Technica)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 Space tech firms are fighting it out for NATO contracts&lt;br /&gt;&lt;/strong&gt;Militaries are willing to branch out and strike deals with commercial vendors. (FT $)&lt;br /&gt;+ &lt;em&gt;Why Trump’s “golden dome” missile defense idea is another ripped straight from the movies. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Facebook users are receiving their Cambridge Analytica payouts&lt;/strong&gt;&lt;br /&gt;Don’t spend it all at once! (The Verge)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;8 The future of supercomputing could hinge on moon mining missions&lt;br /&gt;&lt;/strong&gt;Companies are rushing to buy the moon’s resources before mining has even begun. (WP $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 What it’s like living with an AI toy&lt;/strong&gt;&lt;br /&gt;Featuring unsettling conversations galore. (The Guardian)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10 Anthropic’s staff are obsessed with an albino alligator 🐊&lt;/strong&gt;&lt;br /&gt;As luck would have it, he just happens to be called Claude. (WSJ $)&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p class="has-large-font-size"&gt;&lt;strong&gt;“It’s going to mean more infections, more hospitalizations, more disability and more death.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Demetre Daskalakis, former director of the CDC's National Center for Immunization and Respiratory Diseases, explains the probable outcomes of America’s current vaccine policy jumble, the BBC reports.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; 
 &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1123697" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/image_fa8fb9.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Robots are bringing new life to extinct species&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In the last few years, paleontologists have developed a new trick for turning back time and studying prehistoric animals: building experimental robotic models of them.&lt;/p&gt;&lt;p&gt;In the absence of a living specimen, scientists say, an ambling, flying, swimming, or slithering automaton is the next best thing for studying the behavior of extinct organisms. Here are four examples of robots that are shedding light on creatures of yore.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Shi En Kim&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ New York City is full of natural life, if you know where to look.&lt;br /&gt;+ This photo of Jim Morrison enjoying a beer for breakfast is the epitome of rock ‘n’ roll.&lt;br /&gt;+ How to age like a champion athlete.&lt;br /&gt;+ Would you dare drive the world’s most narrow car?&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/09/16/1123695/the-download-regulators-are-coming-for-ai-companions-and-meet-our-innovator-of-2025/</guid><pubDate>Tue, 16 Sep 2025 12:10:00 +0000</pubDate></item></channel></rss>