<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 02 Dec 2025 06:36:03 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>DeepSeek just dropped two insanely powerful AI models that rival GPT-5 and they're totally free (AI | VentureBeat)</title><link>https://venturebeat.com/ai/deepseek-just-dropped-two-insanely-powerful-ai-models-that-rival-gpt-5-and</link><description>[unable to retrieve full-text content]&lt;p&gt;Chinese artificial intelligence startup &lt;a href="https://www.deepseek.com/"&gt;&lt;u&gt;DeepSeek&lt;/u&gt;&lt;/a&gt; released two powerful new AI models on Sunday that the company claims match or exceed the capabilities of OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-gpt-5/"&gt;&lt;u&gt;GPT-5&lt;/u&gt;&lt;/a&gt; and Google&amp;#x27;s &lt;a href="https://blog.google/products/gemini/gemini-3/"&gt;&lt;u&gt;Gemini-3.0-Pro&lt;/u&gt;&lt;/a&gt; — a development that could reshape the competitive landscape between American tech giants and their Chinese challengers.&lt;/p&gt;&lt;p&gt;The Hangzhou-based company launched &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;&lt;u&gt;DeepSeek-V3.2&lt;/u&gt;&lt;/a&gt;, designed as an everyday reasoning assistant, alongside DeepSeek-V3.2-Speciale, a high-powered variant that achieved gold-medal performance in four elite international competitions: the 2025 International Mathematical Olympiad, the International Olympiad in Informatics, the ICPC World Finals, and the China Mathematical Olympiad.&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;The release carries profound implications for American technology leadership. DeepSeek has once again demonstrated that it can produce frontier AI systems despite U.S. export controls that &lt;a href="https://www.reuters.com/world/china/trump-says-nvidias-blackwell-ai-chip-not-other-people-2025-11-03/"&gt;&lt;u&gt;restrict China&amp;#x27;s access to advanced Nvidia chips&lt;/u&gt;&lt;/a&gt; — and it has done so while making its models freely available under an open-source MIT license.&lt;/p&gt;&lt;p&gt;&amp;quot;People thought DeepSeek gave a one-time breakthrough but we came back much bigger,&amp;quot; wrote &lt;a href="https://x.com/ChenHuiOG"&gt;&lt;u&gt;Chen Fang&lt;/u&gt;&lt;/a&gt;, who identified himself as a contributor to the project, on X (formerly Twitter). The release drew swift reactions online, with one user declaring: &amp;quot;&lt;a href="https://x.com/iampritamj/status/1995511358142701612"&gt;&lt;u&gt;Rest in peace, ChatGPT&lt;/u&gt;&lt;/a&gt;.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How DeepSeek&amp;#x27;s sparse attention breakthrough slashes computing costs&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;At the heart of the new release lies &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;&lt;u&gt;DeepSeek Sparse Attention&lt;/u&gt;&lt;/a&gt;, or DSA — a novel architectural innovation that dramatically reduces the computational burden of running AI models on long documents and complex tasks.&lt;/p&gt;&lt;p&gt;Traditional AI attention mechanisms, the core technology allowing language models to understand context, scale poorly as input length increases. Processing a document twice as long typically requires four times the computation. DeepSeek&amp;#x27;s approach breaks this constraint using what the company calls a &amp;quot;lightning indexer&amp;quot; that identifies only the most relevant portions of context for each query, ignoring the rest.&lt;/p&gt;&lt;p&gt;According to &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2/blob/main/assets/paper.pdf"&gt;&lt;u&gt;DeepSeek&amp;#x27;s technical report&lt;/u&gt;&lt;/a&gt;, DSA reduces inference costs by roughly half compared to previous models when processing long sequences. The architecture &amp;quot;substantially reduces computational complexity while preserving model performance,&amp;quot; the report states.&lt;/p&gt;&lt;p&gt;Processing 128,000 tokens — roughly equivalent to a 300-page book — now costs approximately $0.70 per million tokens for decoding, compared to $2.40 for the previous &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus"&gt;&lt;u&gt;V3.1-Terminus model&lt;/u&gt;&lt;/a&gt;. That represents a 70% reduction in inference costs.&lt;/p&gt;&lt;p&gt;The 685-billion-parameter models support context windows of 128,000 tokens, making them suitable for analyzing lengthy documents, codebases, and research papers. DeepSeek&amp;#x27;s &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2/blob/main/assets/paper.pdf"&gt;&lt;u&gt;technical report&lt;/u&gt;&lt;/a&gt; notes that independent evaluations on long-context benchmarks show V3.2 performing on par with or better than its predecessor &amp;quot;despite incorporating a sparse attention mechanism.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The benchmark results that put DeepSeek in the same league as GPT-5&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;DeepSeek&amp;#x27;s claims of parity with America&amp;#x27;s leading AI systems rest on extensive testing across mathematics, coding, and reasoning tasks — and the numbers are striking.&lt;/p&gt;&lt;p&gt;On &lt;a href="https://maa.org/aime-thresholds-are-available/"&gt;&lt;u&gt;AIME 2025&lt;/u&gt;&lt;/a&gt;, a prestigious American mathematics competition, &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale"&gt;&lt;u&gt;DeepSeek-V3.2-Speciale&lt;/u&gt;&lt;/a&gt; achieved a 96.0% pass rate, compared to 94.6% for GPT-5-High and 95.0% for Gemini-3.0-Pro. On the &lt;a href="https://www.hmmt.org/"&gt;&lt;u&gt;Harvard-MIT Mathematics Tournament&lt;/u&gt;&lt;/a&gt;, the Speciale variant scored 99.2%, surpassing Gemini&amp;#x27;s 97.5%.&lt;/p&gt;&lt;p&gt;The standard &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;&lt;u&gt;V3.2 model&lt;/u&gt;&lt;/a&gt;, optimized for everyday use, scored 93.1% on AIME and 92.5% on HMMT — marginally below frontier models but achieved with substantially fewer computational resources.&lt;/p&gt;&lt;p&gt;Most striking are the competition results. &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale"&gt;&lt;u&gt;DeepSeek-V3.2-Speciale&lt;/u&gt;&lt;/a&gt; scored 35 out of 42 points on the &lt;a href="https://www.imo-official.org/year_info.aspx?year=2025"&gt;&lt;u&gt;2025 International Mathematical Olympiad&lt;/u&gt;&lt;/a&gt;, earning gold-medal status. At the &lt;a href="https://ioinformatics.org/"&gt;&lt;u&gt;International Olympiad in Informatics&lt;/u&gt;&lt;/a&gt;, it scored 492 out of 600 points — also gold, ranking 10th overall. The model solved 10 of 12 problems at the &lt;a href="https://worldfinals.icpc.global/2025/"&gt;&lt;u&gt;ICPC World Finals&lt;/u&gt;&lt;/a&gt;, placing second.&lt;/p&gt;&lt;p&gt;These results came without internet access or tools during testing. DeepSeek&amp;#x27;s report states that &amp;quot;testing strictly adheres to the contest&amp;#x27;s time and attempt limits.&amp;quot;&lt;/p&gt;&lt;p&gt;On coding benchmarks, &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;&lt;u&gt;DeepSeek-V3.2&lt;/u&gt;&lt;/a&gt; resolved 73.1% of real-world software bugs on &lt;a href="https://www.swebench.com/"&gt;&lt;u&gt;SWE-Verified&lt;/u&gt;&lt;/a&gt;, competitive with GPT-5-High at 74.9%. On &lt;a href="https://www.tbench.ai/"&gt;&lt;u&gt;Terminal Bench 2.0&lt;/u&gt;&lt;/a&gt;, measuring complex coding workflows, DeepSeek scored 46.4%—well above GPT-5-High&amp;#x27;s 35.2%.&lt;/p&gt;&lt;p&gt;The company acknowledges limitations. &amp;quot;Token efficiency remains a challenge,&amp;quot; the technical report states, noting that DeepSeek &amp;quot;typically requires longer generation trajectories&amp;quot; to match Gemini-3.0-Pro&amp;#x27;s output quality.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why teaching AI to think while using tools changes everything&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Beyond raw reasoning, &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;&lt;u&gt;DeepSeek-V3.2&lt;/u&gt;&lt;/a&gt; introduces &amp;quot;thinking in tool-use&amp;quot; — the ability to reason through problems while simultaneously executing code, searching the web, and manipulating files.&lt;/p&gt;&lt;p&gt;Previous AI models faced a frustrating limitation: each time they called an external tool, they lost their train of thought and had to restart reasoning from scratch. DeepSeek&amp;#x27;s architecture preserves the reasoning trace across multiple tool calls, enabling fluid multi-step problem solving.&lt;/p&gt;&lt;p&gt;To train this capability, the company built a massive synthetic data pipeline generating over 1,800 distinct task environments and 85,000 complex instructions. These included challenges like multi-day trip planning with budget constraints, software bug fixes across eight programming languages, and web-based research requiring dozens of searches.&lt;/p&gt;&lt;p&gt;The technical report describes one example: planning a three-day trip from Hangzhou with constraints on hotel prices, restaurant ratings, and attraction costs that vary based on accommodation choices. Such tasks are &amp;quot;hard to solve but easy to verify,&amp;quot; making them ideal for training AI agents.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.deepseek.com/"&gt;&lt;u&gt;DeepSeek&lt;/u&gt;&lt;/a&gt; employed real-world tools during training — actual web search APIs, coding environments, and Jupyter notebooks — while generating synthetic prompts to ensure diversity. The result is a model that generalizes to unseen tools and environments, a critical capability for real-world deployment.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;DeepSeek&amp;#x27;s open-source gambit could upend the AI industry&amp;#x27;s business model&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Unlike OpenAI and Anthropic, which guard their most powerful models as proprietary assets, DeepSeek has released both &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;&lt;u&gt;V3.2&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale"&gt;&lt;u&gt;V3.2-Speciale&lt;/u&gt;&lt;/a&gt; under the MIT license — one of the most permissive open-source frameworks available.&lt;/p&gt;&lt;p&gt;Any developer, researcher, or company can download, modify, and deploy the 685-billion-parameter models without restriction. Full model weights, training code, and documentation are &lt;a href="https://huggingface.co/deepseek-ai"&gt;&lt;u&gt;available on Hugging Face&lt;/u&gt;&lt;/a&gt;, the leading platform for AI model sharing.&lt;/p&gt;&lt;p&gt;The strategic implications are significant. By making frontier-capable models freely available, DeepSeek undermines competitors charging premium API prices. The Hugging Face model card notes that DeepSeek has provided Python scripts and test cases &amp;quot;demonstrating how to encode messages in OpenAI-compatible format&amp;quot; — making migration from competing services straightforward.&lt;/p&gt;&lt;p&gt;For enterprise customers, the value proposition is compelling: frontier performance at dramatically lower cost, with deployment flexibility. But data residency concerns and regulatory uncertainty may limit adoption in sensitive applications — particularly given DeepSeek&amp;#x27;s Chinese origins.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Regulatory walls are rising against DeepSeek in Europe and America&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;DeepSeek&amp;#x27;s global expansion faces mounting resistance. In June, Berlin&amp;#x27;s data protection commissioner Meike Kamp declared that DeepSeek&amp;#x27;s transfer of German user data to China is &amp;quot;&lt;a href="https://www.cnbc.com/2025/06/27/germany-tells-apple-google-to-block-deepseek-ai-app.html#:~:text=Berlin&amp;#x27;s%20data%20protection%20commissioner%20Meike,under%20EU%20data%20protection%20rules."&gt;&lt;u&gt;unlawful&lt;/u&gt;&lt;/a&gt;&amp;quot; under EU rules, asking Apple and Google to consider blocking the app.&lt;/p&gt;&lt;p&gt;The German authority expressed concern that &amp;quot;Chinese authorities have extensive access rights to personal data within the sphere of influence of Chinese companies.&amp;quot; Italy ordered DeepSeek to &lt;a href="https://www.reuters.com/technology/artificial-intelligence/italys-privacy-watchdog-blocks-chinese-ai-app-deepseek-2025-01-30/"&gt;&lt;u&gt;block its app&lt;/u&gt;&lt;/a&gt; in February. U.S. lawmakers have moved to &lt;a href="https://www.washingtonpost.com/technology/2025/10/30/tp-link-proposed-ban-commerce-department/"&gt;&lt;u&gt;ban the service&lt;/u&gt;&lt;/a&gt; from government devices, citing national security concerns.&lt;/p&gt;&lt;p&gt;Questions also persist about U.S. export controls designed to limit China&amp;#x27;s AI capabilities. In August, DeepSeek hinted that China would soon have &amp;quot;&lt;a href="https://www.cnbc.com/2025/08/22/deepseek-hints-latest-model-supported-by-chinas-next-generation-homegrown-ai-chips.html"&gt;&lt;u&gt;next generation&lt;/u&gt;&lt;/a&gt;&amp;quot; domestically built chips to support its models. The company indicated its systems work with Chinese-made chips from &lt;a href="https://www.huawei.com/en/"&gt;&lt;u&gt;Huawei&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://www.cambricon.com/"&gt;&lt;u&gt;Cambricon&lt;/u&gt;&lt;/a&gt; without additional setup.&lt;/p&gt;&lt;p&gt;DeepSeek&amp;#x27;s original V3 model was reportedly trained on roughly 2,000 older &lt;a href="https://www.reuters.com/technology/nvidia-tweaks-flagship-h100-chip-export-china-h800-2023-03-21/"&gt;&lt;u&gt;Nvidia H800 chips&lt;/u&gt;&lt;/a&gt; — hardware since restricted for China export. The company has not disclosed what powered V3.2 training, but its continued advancement suggests export controls alone cannot halt Chinese AI progress.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What DeepSeek&amp;#x27;s release means for the future of AI competition&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The release arrives at a pivotal moment. After years of massive investment, some analysts question whether an AI bubble is forming. DeepSeek&amp;#x27;s ability to match American frontier models at a fraction of the cost challenges assumptions that AI leadership requires enormous capital expenditure.&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2/blob/main/assets/paper.pdf"&gt;&lt;u&gt;technical report&lt;/u&gt;&lt;/a&gt; reveals that post-training investment now exceeds 10% of pre-training costs — a substantial allocation credited for reasoning improvements. But DeepSeek acknowledges gaps: &amp;quot;The breadth of world knowledge in DeepSeek-V3.2 still lags behind leading proprietary models,&amp;quot; the report states. The company plans to address this by scaling pre-training compute.&lt;/p&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale"&gt;&lt;u&gt;DeepSeek-V3.2-Speciale&lt;/u&gt;&lt;/a&gt; remains available through a temporary API until December 15, when its capabilities will merge into the standard release. The Speciale variant is designed exclusively for deep reasoning and does not support tool calling — a limitation the standard model addresses.&lt;/p&gt;&lt;p&gt;For now, the AI race between the United States and China has entered a new phase. DeepSeek&amp;#x27;s release demonstrates that open-source models can achieve frontier performance, that efficiency innovations can slash costs dramatically, and that the most powerful AI systems may soon be freely available to anyone with an internet connection.&lt;/p&gt;&lt;p&gt;As one commenter on X observed: &amp;quot;Deepseek just casually breaking those historic benchmarks set by Gemini is bonkers.&amp;quot;&lt;/p&gt;&lt;p&gt;The question is no longer whether Chinese AI can compete with Silicon Valley. It&amp;#x27;s whether American companies can maintain their lead when their Chinese rival gives comparable technology away for free.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Chinese artificial intelligence startup &lt;a href="https://www.deepseek.com/"&gt;&lt;u&gt;DeepSeek&lt;/u&gt;&lt;/a&gt; released two powerful new AI models on Sunday that the company claims match or exceed the capabilities of OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-gpt-5/"&gt;&lt;u&gt;GPT-5&lt;/u&gt;&lt;/a&gt; and Google&amp;#x27;s &lt;a href="https://blog.google/products/gemini/gemini-3/"&gt;&lt;u&gt;Gemini-3.0-Pro&lt;/u&gt;&lt;/a&gt; — a development that could reshape the competitive landscape between American tech giants and their Chinese challengers.&lt;/p&gt;&lt;p&gt;The Hangzhou-based company launched &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;&lt;u&gt;DeepSeek-V3.2&lt;/u&gt;&lt;/a&gt;, designed as an everyday reasoning assistant, alongside DeepSeek-V3.2-Speciale, a high-powered variant that achieved gold-medal performance in four elite international competitions: the 2025 International Mathematical Olympiad, the International Olympiad in Informatics, the ICPC World Finals, and the China Mathematical Olympiad.&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;The release carries profound implications for American technology leadership. DeepSeek has once again demonstrated that it can produce frontier AI systems despite U.S. export controls that &lt;a href="https://www.reuters.com/world/china/trump-says-nvidias-blackwell-ai-chip-not-other-people-2025-11-03/"&gt;&lt;u&gt;restrict China&amp;#x27;s access to advanced Nvidia chips&lt;/u&gt;&lt;/a&gt; — and it has done so while making its models freely available under an open-source MIT license.&lt;/p&gt;&lt;p&gt;&amp;quot;People thought DeepSeek gave a one-time breakthrough but we came back much bigger,&amp;quot; wrote &lt;a href="https://x.com/ChenHuiOG"&gt;&lt;u&gt;Chen Fang&lt;/u&gt;&lt;/a&gt;, who identified himself as a contributor to the project, on X (formerly Twitter). The release drew swift reactions online, with one user declaring: &amp;quot;&lt;a href="https://x.com/iampritamj/status/1995511358142701612"&gt;&lt;u&gt;Rest in peace, ChatGPT&lt;/u&gt;&lt;/a&gt;.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How DeepSeek&amp;#x27;s sparse attention breakthrough slashes computing costs&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;At the heart of the new release lies &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;&lt;u&gt;DeepSeek Sparse Attention&lt;/u&gt;&lt;/a&gt;, or DSA — a novel architectural innovation that dramatically reduces the computational burden of running AI models on long documents and complex tasks.&lt;/p&gt;&lt;p&gt;Traditional AI attention mechanisms, the core technology allowing language models to understand context, scale poorly as input length increases. Processing a document twice as long typically requires four times the computation. DeepSeek&amp;#x27;s approach breaks this constraint using what the company calls a &amp;quot;lightning indexer&amp;quot; that identifies only the most relevant portions of context for each query, ignoring the rest.&lt;/p&gt;&lt;p&gt;According to &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2/blob/main/assets/paper.pdf"&gt;&lt;u&gt;DeepSeek&amp;#x27;s technical report&lt;/u&gt;&lt;/a&gt;, DSA reduces inference costs by roughly half compared to previous models when processing long sequences. The architecture &amp;quot;substantially reduces computational complexity while preserving model performance,&amp;quot; the report states.&lt;/p&gt;&lt;p&gt;Processing 128,000 tokens — roughly equivalent to a 300-page book — now costs approximately $0.70 per million tokens for decoding, compared to $2.40 for the previous &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus"&gt;&lt;u&gt;V3.1-Terminus model&lt;/u&gt;&lt;/a&gt;. That represents a 70% reduction in inference costs.&lt;/p&gt;&lt;p&gt;The 685-billion-parameter models support context windows of 128,000 tokens, making them suitable for analyzing lengthy documents, codebases, and research papers. DeepSeek&amp;#x27;s &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2/blob/main/assets/paper.pdf"&gt;&lt;u&gt;technical report&lt;/u&gt;&lt;/a&gt; notes that independent evaluations on long-context benchmarks show V3.2 performing on par with or better than its predecessor &amp;quot;despite incorporating a sparse attention mechanism.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The benchmark results that put DeepSeek in the same league as GPT-5&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;DeepSeek&amp;#x27;s claims of parity with America&amp;#x27;s leading AI systems rest on extensive testing across mathematics, coding, and reasoning tasks — and the numbers are striking.&lt;/p&gt;&lt;p&gt;On &lt;a href="https://maa.org/aime-thresholds-are-available/"&gt;&lt;u&gt;AIME 2025&lt;/u&gt;&lt;/a&gt;, a prestigious American mathematics competition, &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale"&gt;&lt;u&gt;DeepSeek-V3.2-Speciale&lt;/u&gt;&lt;/a&gt; achieved a 96.0% pass rate, compared to 94.6% for GPT-5-High and 95.0% for Gemini-3.0-Pro. On the &lt;a href="https://www.hmmt.org/"&gt;&lt;u&gt;Harvard-MIT Mathematics Tournament&lt;/u&gt;&lt;/a&gt;, the Speciale variant scored 99.2%, surpassing Gemini&amp;#x27;s 97.5%.&lt;/p&gt;&lt;p&gt;The standard &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;&lt;u&gt;V3.2 model&lt;/u&gt;&lt;/a&gt;, optimized for everyday use, scored 93.1% on AIME and 92.5% on HMMT — marginally below frontier models but achieved with substantially fewer computational resources.&lt;/p&gt;&lt;p&gt;Most striking are the competition results. &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale"&gt;&lt;u&gt;DeepSeek-V3.2-Speciale&lt;/u&gt;&lt;/a&gt; scored 35 out of 42 points on the &lt;a href="https://www.imo-official.org/year_info.aspx?year=2025"&gt;&lt;u&gt;2025 International Mathematical Olympiad&lt;/u&gt;&lt;/a&gt;, earning gold-medal status. At the &lt;a href="https://ioinformatics.org/"&gt;&lt;u&gt;International Olympiad in Informatics&lt;/u&gt;&lt;/a&gt;, it scored 492 out of 600 points — also gold, ranking 10th overall. The model solved 10 of 12 problems at the &lt;a href="https://worldfinals.icpc.global/2025/"&gt;&lt;u&gt;ICPC World Finals&lt;/u&gt;&lt;/a&gt;, placing second.&lt;/p&gt;&lt;p&gt;These results came without internet access or tools during testing. DeepSeek&amp;#x27;s report states that &amp;quot;testing strictly adheres to the contest&amp;#x27;s time and attempt limits.&amp;quot;&lt;/p&gt;&lt;p&gt;On coding benchmarks, &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;&lt;u&gt;DeepSeek-V3.2&lt;/u&gt;&lt;/a&gt; resolved 73.1% of real-world software bugs on &lt;a href="https://www.swebench.com/"&gt;&lt;u&gt;SWE-Verified&lt;/u&gt;&lt;/a&gt;, competitive with GPT-5-High at 74.9%. On &lt;a href="https://www.tbench.ai/"&gt;&lt;u&gt;Terminal Bench 2.0&lt;/u&gt;&lt;/a&gt;, measuring complex coding workflows, DeepSeek scored 46.4%—well above GPT-5-High&amp;#x27;s 35.2%.&lt;/p&gt;&lt;p&gt;The company acknowledges limitations. &amp;quot;Token efficiency remains a challenge,&amp;quot; the technical report states, noting that DeepSeek &amp;quot;typically requires longer generation trajectories&amp;quot; to match Gemini-3.0-Pro&amp;#x27;s output quality.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why teaching AI to think while using tools changes everything&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Beyond raw reasoning, &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;&lt;u&gt;DeepSeek-V3.2&lt;/u&gt;&lt;/a&gt; introduces &amp;quot;thinking in tool-use&amp;quot; — the ability to reason through problems while simultaneously executing code, searching the web, and manipulating files.&lt;/p&gt;&lt;p&gt;Previous AI models faced a frustrating limitation: each time they called an external tool, they lost their train of thought and had to restart reasoning from scratch. DeepSeek&amp;#x27;s architecture preserves the reasoning trace across multiple tool calls, enabling fluid multi-step problem solving.&lt;/p&gt;&lt;p&gt;To train this capability, the company built a massive synthetic data pipeline generating over 1,800 distinct task environments and 85,000 complex instructions. These included challenges like multi-day trip planning with budget constraints, software bug fixes across eight programming languages, and web-based research requiring dozens of searches.&lt;/p&gt;&lt;p&gt;The technical report describes one example: planning a three-day trip from Hangzhou with constraints on hotel prices, restaurant ratings, and attraction costs that vary based on accommodation choices. Such tasks are &amp;quot;hard to solve but easy to verify,&amp;quot; making them ideal for training AI agents.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.deepseek.com/"&gt;&lt;u&gt;DeepSeek&lt;/u&gt;&lt;/a&gt; employed real-world tools during training — actual web search APIs, coding environments, and Jupyter notebooks — while generating synthetic prompts to ensure diversity. The result is a model that generalizes to unseen tools and environments, a critical capability for real-world deployment.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;DeepSeek&amp;#x27;s open-source gambit could upend the AI industry&amp;#x27;s business model&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Unlike OpenAI and Anthropic, which guard their most powerful models as proprietary assets, DeepSeek has released both &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;&lt;u&gt;V3.2&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale"&gt;&lt;u&gt;V3.2-Speciale&lt;/u&gt;&lt;/a&gt; under the MIT license — one of the most permissive open-source frameworks available.&lt;/p&gt;&lt;p&gt;Any developer, researcher, or company can download, modify, and deploy the 685-billion-parameter models without restriction. Full model weights, training code, and documentation are &lt;a href="https://huggingface.co/deepseek-ai"&gt;&lt;u&gt;available on Hugging Face&lt;/u&gt;&lt;/a&gt;, the leading platform for AI model sharing.&lt;/p&gt;&lt;p&gt;The strategic implications are significant. By making frontier-capable models freely available, DeepSeek undermines competitors charging premium API prices. The Hugging Face model card notes that DeepSeek has provided Python scripts and test cases &amp;quot;demonstrating how to encode messages in OpenAI-compatible format&amp;quot; — making migration from competing services straightforward.&lt;/p&gt;&lt;p&gt;For enterprise customers, the value proposition is compelling: frontier performance at dramatically lower cost, with deployment flexibility. But data residency concerns and regulatory uncertainty may limit adoption in sensitive applications — particularly given DeepSeek&amp;#x27;s Chinese origins.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Regulatory walls are rising against DeepSeek in Europe and America&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;DeepSeek&amp;#x27;s global expansion faces mounting resistance. In June, Berlin&amp;#x27;s data protection commissioner Meike Kamp declared that DeepSeek&amp;#x27;s transfer of German user data to China is &amp;quot;&lt;a href="https://www.cnbc.com/2025/06/27/germany-tells-apple-google-to-block-deepseek-ai-app.html#:~:text=Berlin&amp;#x27;s%20data%20protection%20commissioner%20Meike,under%20EU%20data%20protection%20rules."&gt;&lt;u&gt;unlawful&lt;/u&gt;&lt;/a&gt;&amp;quot; under EU rules, asking Apple and Google to consider blocking the app.&lt;/p&gt;&lt;p&gt;The German authority expressed concern that &amp;quot;Chinese authorities have extensive access rights to personal data within the sphere of influence of Chinese companies.&amp;quot; Italy ordered DeepSeek to &lt;a href="https://www.reuters.com/technology/artificial-intelligence/italys-privacy-watchdog-blocks-chinese-ai-app-deepseek-2025-01-30/"&gt;&lt;u&gt;block its app&lt;/u&gt;&lt;/a&gt; in February. U.S. lawmakers have moved to &lt;a href="https://www.washingtonpost.com/technology/2025/10/30/tp-link-proposed-ban-commerce-department/"&gt;&lt;u&gt;ban the service&lt;/u&gt;&lt;/a&gt; from government devices, citing national security concerns.&lt;/p&gt;&lt;p&gt;Questions also persist about U.S. export controls designed to limit China&amp;#x27;s AI capabilities. In August, DeepSeek hinted that China would soon have &amp;quot;&lt;a href="https://www.cnbc.com/2025/08/22/deepseek-hints-latest-model-supported-by-chinas-next-generation-homegrown-ai-chips.html"&gt;&lt;u&gt;next generation&lt;/u&gt;&lt;/a&gt;&amp;quot; domestically built chips to support its models. The company indicated its systems work with Chinese-made chips from &lt;a href="https://www.huawei.com/en/"&gt;&lt;u&gt;Huawei&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://www.cambricon.com/"&gt;&lt;u&gt;Cambricon&lt;/u&gt;&lt;/a&gt; without additional setup.&lt;/p&gt;&lt;p&gt;DeepSeek&amp;#x27;s original V3 model was reportedly trained on roughly 2,000 older &lt;a href="https://www.reuters.com/technology/nvidia-tweaks-flagship-h100-chip-export-china-h800-2023-03-21/"&gt;&lt;u&gt;Nvidia H800 chips&lt;/u&gt;&lt;/a&gt; — hardware since restricted for China export. The company has not disclosed what powered V3.2 training, but its continued advancement suggests export controls alone cannot halt Chinese AI progress.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What DeepSeek&amp;#x27;s release means for the future of AI competition&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The release arrives at a pivotal moment. After years of massive investment, some analysts question whether an AI bubble is forming. DeepSeek&amp;#x27;s ability to match American frontier models at a fraction of the cost challenges assumptions that AI leadership requires enormous capital expenditure.&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2/blob/main/assets/paper.pdf"&gt;&lt;u&gt;technical report&lt;/u&gt;&lt;/a&gt; reveals that post-training investment now exceeds 10% of pre-training costs — a substantial allocation credited for reasoning improvements. But DeepSeek acknowledges gaps: &amp;quot;The breadth of world knowledge in DeepSeek-V3.2 still lags behind leading proprietary models,&amp;quot; the report states. The company plans to address this by scaling pre-training compute.&lt;/p&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale"&gt;&lt;u&gt;DeepSeek-V3.2-Speciale&lt;/u&gt;&lt;/a&gt; remains available through a temporary API until December 15, when its capabilities will merge into the standard release. The Speciale variant is designed exclusively for deep reasoning and does not support tool calling — a limitation the standard model addresses.&lt;/p&gt;&lt;p&gt;For now, the AI race between the United States and China has entered a new phase. DeepSeek&amp;#x27;s release demonstrates that open-source models can achieve frontier performance, that efficiency innovations can slash costs dramatically, and that the most powerful AI systems may soon be freely available to anyone with an internet connection.&lt;/p&gt;&lt;p&gt;As one commenter on X observed: &amp;quot;Deepseek just casually breaking those historic benchmarks set by Gemini is bonkers.&amp;quot;&lt;/p&gt;&lt;p&gt;The question is no longer whether Chinese AI can compete with Silicon Valley. It&amp;#x27;s whether American companies can maintain their lead when their Chinese rival gives comparable technology away for free.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/deepseek-just-dropped-two-insanely-powerful-ai-models-that-rival-gpt-5-and</guid><pubDate>Mon, 01 Dec 2025 18:45:00 +0000</pubDate></item><item><title>Data center energy demand forecasted to soar nearly 300% through 2035 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/01/data-center-energy-demand-forecasted-to-soar-nearly-300-through-2035/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Planned data center construction shows no signs of fading, with new additions to require 2.7x — nearly triple — the sector’s current demand for electricity over the next decade, according to a new report from BloombergNEF.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By 2035, data centers will draw 106 gigawatts, up sharply from the 40 gigawatts they use today. Much of that growth will occur in more rural areas as facilities grow in size and as sites near urban areas become scarce, BloombergNEF said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Driving part of the growth is the sheer scale of planned data centers. Today, only 10% of data centers draw more than 50 megawatts of electricity, but over the next decade, the average new facility will draw well over 100 megawatts. The biggest sites help skew the data: Nearly a quarter will be larger than 500 megawatts, and a few will exceed 1 gigawatt.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="A chart illustrating data center electricity use through 2032." class="wp-image-3071402" height="432" src="https://techcrunch.com/wp-content/uploads/2025/12/BloombergNEF-data-center-forecast-12-1-2025.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Planned data centers are significantly larger than those currently in operation&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;BloombergNEF&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;At the same time, the utilization rate for all data centers is expected to grow from 59% to 69% as AI training and inference grows to nearly 40% of total data center compute.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In some ways, the findings in the new report aren’t surprising. AI companies have been racing to build more powerful data centers, helping to drive global investment in the facilities up to $580 billion this year. That’s more than the world spends finding new supplies of oil.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, the new report shows just how quickly the landscape is changing. It is a sharp revision upwards from a document the group published in April. The upswing was driven by a surge in new projects that have been announced since then. “With an average seven-year timeline for projects to come online, developments in earlier stages affect the tail end of our forecast the most,” the new report said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Early-stage projects have more than doubled between early 2024 and early 2025, though those are distinct from projects that have been committed or are currently under construction.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Much of that new capacity is being planned for Virginia, Pennsylvania, Ohio, Illinois, and New Jersey. They lie within a region known to industry experts at the PJM Interconnection, a regional transmission organization that’s tasked with operating the electrical grid in those states and others, including Delaware, West Virginia, and parts of Kentucky and North Carolina. Texas’s Ercot grid will see a large number of additions, too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The report arrives as the PJM Interconnection is under scrutiny from its independent monitor, Monitoring Analytics. The group filed a complaint with the Federal Energy Regulatory Commission (FERC) saying that PJM has the authority to authorize new data center connections only when its grid has adequate capacity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As part of its obligation to maintain reliability, PJM has the authority to require large new data center loads to wait to be added to the system until the loads can be served reliably,” Monitoring Analytics wrote. “PJM has the authority to create a load queue.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;What’s more, data centers are responsible for today’s high electricity prices within the region, the organization said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“PJM’s failure to clarify and enforce its existing rules and to protect reliable and affordable service in PJM is unjust and unreasonable,” it said.&amp;nbsp;&lt;/p&gt;



&lt;!-- Add a placeholder for the Twitch embed --&gt;


&lt;!-- Load the Twitch embed script --&gt;

&lt;!-- Create a Twitch.Player object. This will render within the placeholder div --&gt;


&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. &lt;strong&gt;This &lt;em&gt;stream&lt;/em&gt; is brought to you in partnership with AWS.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Planned data center construction shows no signs of fading, with new additions to require 2.7x — nearly triple — the sector’s current demand for electricity over the next decade, according to a new report from BloombergNEF.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By 2035, data centers will draw 106 gigawatts, up sharply from the 40 gigawatts they use today. Much of that growth will occur in more rural areas as facilities grow in size and as sites near urban areas become scarce, BloombergNEF said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Driving part of the growth is the sheer scale of planned data centers. Today, only 10% of data centers draw more than 50 megawatts of electricity, but over the next decade, the average new facility will draw well over 100 megawatts. The biggest sites help skew the data: Nearly a quarter will be larger than 500 megawatts, and a few will exceed 1 gigawatt.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="A chart illustrating data center electricity use through 2032." class="wp-image-3071402" height="432" src="https://techcrunch.com/wp-content/uploads/2025/12/BloombergNEF-data-center-forecast-12-1-2025.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Planned data centers are significantly larger than those currently in operation&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;BloombergNEF&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;At the same time, the utilization rate for all data centers is expected to grow from 59% to 69% as AI training and inference grows to nearly 40% of total data center compute.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In some ways, the findings in the new report aren’t surprising. AI companies have been racing to build more powerful data centers, helping to drive global investment in the facilities up to $580 billion this year. That’s more than the world spends finding new supplies of oil.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, the new report shows just how quickly the landscape is changing. It is a sharp revision upwards from a document the group published in April. The upswing was driven by a surge in new projects that have been announced since then. “With an average seven-year timeline for projects to come online, developments in earlier stages affect the tail end of our forecast the most,” the new report said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Early-stage projects have more than doubled between early 2024 and early 2025, though those are distinct from projects that have been committed or are currently under construction.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Much of that new capacity is being planned for Virginia, Pennsylvania, Ohio, Illinois, and New Jersey. They lie within a region known to industry experts at the PJM Interconnection, a regional transmission organization that’s tasked with operating the electrical grid in those states and others, including Delaware, West Virginia, and parts of Kentucky and North Carolina. Texas’s Ercot grid will see a large number of additions, too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The report arrives as the PJM Interconnection is under scrutiny from its independent monitor, Monitoring Analytics. The group filed a complaint with the Federal Energy Regulatory Commission (FERC) saying that PJM has the authority to authorize new data center connections only when its grid has adequate capacity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As part of its obligation to maintain reliability, PJM has the authority to require large new data center loads to wait to be added to the system until the loads can be served reliably,” Monitoring Analytics wrote. “PJM has the authority to create a load queue.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;What’s more, data centers are responsible for today’s high electricity prices within the region, the organization said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“PJM’s failure to clarify and enforce its existing rules and to protect reliable and affordable service in PJM is unjust and unreasonable,” it said.&amp;nbsp;&lt;/p&gt;



&lt;!-- Add a placeholder for the Twitch embed --&gt;


&lt;!-- Load the Twitch embed script --&gt;

&lt;!-- Create a Twitch.Player object. This will render within the placeholder div --&gt;


&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. &lt;strong&gt;This &lt;em&gt;stream&lt;/em&gt; is brought to you in partnership with AWS.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/01/data-center-energy-demand-forecasted-to-soar-nearly-300-through-2035/</guid><pubDate>Mon, 01 Dec 2025 19:08:42 +0000</pubDate></item><item><title>Construction workers are cashing in on the AI boom (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/01/construction-workers-are-cashing-in-on-the-ai-boom/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/GettyImages-2152222109.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The AI boom is proving to be a windfall for construction workers building the massive data centers that power it all. According to The Wall Street Journal, workers moving into data-center construction are seeing pay jumps of 25% to 30% compared to their previous jobs — and in some cases, much more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Among them is DeMond Chambliss, who traded his small drywall business in Columbus, Ohio, for a supervisor role overseeing 200 workers at a data center site. He now makes over $100,000 annually. “I pinch myself going to work every day,” the 51-year-old tells the Journal. In Oregon, electrical safety specialist Marc Benner pulls in $225,000 a year, while electrician Andrew Mason makes over $200,000 managing workers at six Northern Virginia data centers.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Journal reports this isn’t just about higher base pay. Companies are sweetening the pot with perks like heated break tents, free lunches, daily incentive bonuses, and even remote project management positions. One construction site offers workers $100 in daily incentive pay, which can add up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The surge comes as tech giants like Amazon, Google, and Microsoft race to build hundreds of new data centers, colliding with an industry-wide shortage of roughly 439,000 skilled workers, according to the Associated Builders and Contractors trade group. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/GettyImages-2152222109.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The AI boom is proving to be a windfall for construction workers building the massive data centers that power it all. According to The Wall Street Journal, workers moving into data-center construction are seeing pay jumps of 25% to 30% compared to their previous jobs — and in some cases, much more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Among them is DeMond Chambliss, who traded his small drywall business in Columbus, Ohio, for a supervisor role overseeing 200 workers at a data center site. He now makes over $100,000 annually. “I pinch myself going to work every day,” the 51-year-old tells the Journal. In Oregon, electrical safety specialist Marc Benner pulls in $225,000 a year, while electrician Andrew Mason makes over $200,000 managing workers at six Northern Virginia data centers.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Journal reports this isn’t just about higher base pay. Companies are sweetening the pot with perks like heated break tents, free lunches, daily incentive bonuses, and even remote project management positions. One construction site offers workers $100 in daily incentive pay, which can add up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The surge comes as tech giants like Amazon, Google, and Microsoft race to build hundreds of new data centers, colliding with an industry-wide shortage of roughly 439,000 skilled workers, according to the Associated Builders and Contractors trade group. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/01/construction-workers-are-cashing-in-on-the-ai-boom/</guid><pubDate>Mon, 01 Dec 2025 19:13:50 +0000</pubDate></item><item><title>Ideas: Community building, machine learning, and the future of AI (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/podcast/ideas-community-building-machine-learning-and-the-future-of-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/11/HannaJenn-Ideas_TW_LI_FB_1200x627-1.jpg" /&gt;&lt;/div&gt;&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;/figure&gt;






&lt;p&gt;Behind every emerging technology is a great idea propelling it forward. In the Microsoft Research Podcast series &lt;em&gt;Ideas&lt;/em&gt;, members of the research community at Microsoft discuss the beliefs that animate their research, the experiences and thinkers that inform it, and the positive human impact it targets.&lt;/p&gt;



&lt;p&gt;In 2006, three PhD students organized the Women in Machine Learning Workshop, or&amp;nbsp;&lt;em&gt;WiML&lt;/em&gt;, to provide a space for women in ML to connect and share their research. The event has been held every year since,&amp;nbsp;growing in size&amp;nbsp;and mission.&lt;/p&gt;



&lt;p&gt;In this episode, two of the WiML cofounders, Jenn Wortman Vaughan, a Microsoft senior principal research manager, and Hanna Wallach, a Microsoft vice president and distinguished scientist, reflect on the 20th workshop. They discuss WiML’s journey from a potential one-off event to a nonprofit supporting women and nonbinary individuals worldwide; their friendship and collaborations, including their contributions to defining responsible AI at Microsoft; and the advice they’d give their younger selves.&lt;/p&gt;



&lt;div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow"&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h2 class="wp-block-heading h5" id="learn-more-1"&gt;Learn more:&lt;/h2&gt;








&lt;/div&gt;



&lt;section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast"&gt;
	
&lt;/section&gt;


&lt;div class="wp-block-msr-show-more"&gt;
	&lt;div class="bg-neutral-100 p-5"&gt;
		&lt;div class="show-more-show-less"&gt;
			&lt;div&gt;
				&lt;span&gt;
					

&lt;h2 class="wp-block-heading" id="transcript"&gt;Transcript&lt;/h2&gt;



&lt;p&gt;[MUSIC]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SERIES INTRODUCTION:&lt;/strong&gt; You’re listening to &lt;em&gt;Ideas&lt;/em&gt;, a Microsoft Research Podcast that dives deep into the world of technology research and the profound questions behind the code. In this series, we’ll explore the technologies that are shaping our future and the big ideas that propel them forward.&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;JENN WORTMAN VAUGHAN:&lt;/strong&gt; Hello, and welcome. I’m Jenn Wortman Vaughan. This week, machine learning researchers around the world will be attending the annual Conference on Neural Information Processing Systems, or NeurIPS. I am especially excited about NeurIPS this year because of a co-located event, the 20th annual workshop for Women in Machine Learning&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, or WiML, which I am going to be attending both as a mentor and as a keynote speaker.&lt;/p&gt;



&lt;p&gt;So to celebrate 20 years of WiML, I’m here today with my long-term collaborator, colleague, close friend, &lt;em&gt;and&lt;/em&gt; my cofounder of the workshop for Women in Machine Learning, Hanna Wallach.&lt;/p&gt;



&lt;p&gt;You know, you and I have known each other for a very long time at this point. And in many ways, we followed very parallel and often intersecting paths before we both ended up here working in responsible AI at Microsoft. So I thought it might be fun to kick off this podcast with a bit of the story of our interleaving trajectories.&lt;/p&gt;



&lt;p&gt;So let’s start way back 20 years ago, around the time we first had the idea for WiML. Where were you, and what were you up to?&lt;/p&gt;



				&lt;/span&gt;
				&lt;span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1"&gt;
					



&lt;p&gt;&lt;strong&gt;HANNA WALLACH:&lt;/strong&gt; Yeah, so I was a PhD student at the University of Cambridge, and I was working with the late David MacKay. I was focusing on machine learning for analyzing text, and at that point in time, I’d actually just begun working on Bayesian latent variable models for text analysis, and my research was really focusing on trying to combine ideas from &lt;em&gt;n&lt;/em&gt;-gram language modeling with statistical topic modeling in order to come up with models that just did a better job at modeling text.&lt;/p&gt;



&lt;p&gt;I was also doing this super-weird two-country thing. So I was doing my PhD at Cambridge, but at the end of the first year of my PhD, I spent three months as a visiting graduate student at the University of Pennsylvania, and I loved it, so much so that at the end of the three months I said, can I extend for a full year? Cambridge said yes; Penn said yes. So I did that and actually ended up then extending another year and then another year and another year and so on and so forth.&lt;/p&gt;



&lt;p&gt;But during my first full year at Penn, that was when I met &lt;em&gt;you&lt;/em&gt;, and it was at the visiting students weekend, and I had been told by the faculty in the department that I had to work really hard on recruiting you. I had no idea that that was actually going to be the start of a 20-plus-year friendship.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, I still remember that visiting weekend very well. I actually met you; I met my husband, Jeff; and I met my PhD advisor, Michael Kearns, all on the same day at that visiting student weekend. So I didn’t know it at the time, but it was a very big day for me.&lt;/p&gt;



&lt;p&gt;So around that time when I started my PhD at Penn, I was working in machine learning theory and algorithmic economics. So even then, you know, just like I am now, I was interested in the intersection of people and AI systems. But since my training was in theory, my “people” tended to be these mathematically ideal people with these well-defined preferences and beliefs who behaved in very well-defined ways.&lt;/p&gt;



&lt;p&gt;Working in learning theory like this was appealing to me because it was very neat and precise. There was just none of the mess of the real world. You could just write down your model, which contained all of your assumptions, and everything else that followed from there was in some sense objective.&lt;/p&gt;



&lt;p&gt;So I was really enjoying this work, and I was also so excited to have you around the department at the time. You know, honestly, I also loved Penn. It was just such a great environment. I was just actually back there a few weeks ago, visiting to give a talk. I had an amazing time. But it was, I will say, very male dominated in the computer science department at the time. In my incoming class of PhD students, we had 20 incoming PhDs, and I was the only woman there. But we managed to build a community. We had our weekly ladies brunch, which I loved, and things like that really kept me going during my PhD.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah, I loved that ladies brunch. That made a huge difference to me and, kind of, kept me going through the PhD, as well.&lt;/p&gt;



&lt;p&gt;And, like you, I’d always been interested in people. And during the course of my PhD, I realized that I wasn’t interested in analyzing text for the sake of text, right. I was interested because text is one of these ways that people communicate with each other. You know, people don’t write text for the sake of writing text. They write it because they’re trying to convey something. And it was really &lt;em&gt;that&lt;/em&gt; that I was interested in. It was these, kind of, social aspects of text that I found super interesting.&lt;/p&gt;



&lt;p&gt;So coming out of the PhD, I then got a postdoc job focused on analyzing texts as part of these, sort of, broader social processes. From there, I ended up getting a faculty job, also at UMass, as one of four founding members of UMass’s Computational Social Science Institute&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. So there was me in computer science, then there was another assistant professor in statistics, another in political science, and another in sociology. And in many ways, this was my dream job. I was being paid to develop and use machine learning methods to study social processes and answer questions that social scientists wanted to study. It was pretty awesome. You, I think, started a faculty position at the same time, right?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah. So I also did a postdoc. First, I spent a year as a postdoc at Harvard, which was super fun. And then I started a tenure track position in computer science at UCLA in 2010.&lt;/p&gt;



&lt;p&gt;Again, you know, it was a very male-dominated environment. My department was mostly men. But maybe even more importantly than this, I just didn’t really have a network there. You know, it was lonely. One exception to this was Mihaela van der Schaar. She was at UCLA at the time, though not in my department, and she, kind of, took me under her wing. So I’m very grateful that I had that support. But overall, this position just wasn’t a great fit for me, and I was under more stress then than I think I have been at any other point in my life that I could really remember.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah. So at that point, then, you ended up transitioning to Microsoft Research, right?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Why did you end up choosing MSR [Microsoft Research]?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, so this was back in 2012. MSR had just opened up this &lt;em&gt;new&lt;/em&gt; New York City lab at the time, and working in this lab was basically my dream job. I think I actually tried to apply before they had even officially opened the lab, like when I just heard it was happening.&lt;/p&gt;



&lt;p&gt;So this lab focused in three areas at the time. It focused in machine learning, algorithmic economics, and computational social science. And my research at the time cut across all three of these areas. So it felt just like this perfect opportunity to work in the space where my work would fit in so well and be really appreciated.&lt;/p&gt;



&lt;p&gt;The algorithmic economics group at the time actually was working on building prediction markets to aggregate information about future events, and they were already, in doing this, building on top of some of my theoretical research, which was just super cool to see. So that was exciting. And I already knew a couple of people here. I knew John Langford and Dave Pennock, who was in the economics group at the time, because I’d done an internship actually with the two of them at Yahoo Research before they came to Microsoft. And I was really excited to come back and work with them again, as well.&lt;/p&gt;



&lt;p&gt;You know, even here at the time that I joined the lab, it was 13 men and me. So once again, not great numbers. And I think that in some ways this was especially hard on me because I was just naturally, like, a very shy person and I hadn’t really built up the confidence that I should have at that point in my career. But on the other hand, I found the research fit just so spot-on that I couldn’t say no. And I suspect that this is something that you understand yourself because you actually came and joined me here in the New York lab a year or two later. So why did you make this switch?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah, so I anticipated that I was going to love my faculty job. It was focusing on all this stuff that I was so excited about. And much to my surprise, though, I kind of didn’t. And it wasn’t like there was any one particular thing that I didn’t like. It was more of a mixture of things. I did love my research, though. That was pretty clear to me. &lt;em&gt;But&lt;/em&gt; I wasn’t happy. So I spent a summer talking to as many people as possible in all different kinds of jobs, really just with the goal of figuring out what their day-to-day lives looked like. You were one of the people I spoke to, but I spoke to a ton of other people, as well.&lt;/p&gt;



&lt;p&gt;And from doing that, at the end of that summer, I ended up deciding to apply to industry jobs, and I applied to a bunch of places and got a bunch of offers. But I ended up deciding to join Microsoft Research New York City because of all the places I was considering going, they were the only place that said, “We love your research. We love what you do. Do you want to come here and do that same research?”&lt;/p&gt;



&lt;p&gt;And that was really appealing to me because I loved my research. Of course, I wanted to come there and do my same research and especially with all of these amazing people like you, Duncan Watts, who’d for many years been somebody I’d really looked up to. He was there, as well, at that point in time. There was this real focus on computational social science but with a little bit more of an industry perspective. There were also these amazing machine learning researchers. Just for many of the same reasons as you, I was just really excited to join that lab and particularly excited to be working in the same organization as you again.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, I’m happy to take at least a little bit of the credit for …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Oh yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; … recruiting you to Microsoft here many years ago.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Oh yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah. I was really excited to have you join, too, though I think the timing actually worked out so that I missed your first couple of months because I was on maternity leave with my first daughter at the time. I should say I’ve got two daughters, and I’m very proud to share in the context of this podcast that they’re both very interested in math and reading, as well.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah, they’re both great.&lt;/p&gt;



&lt;p&gt;Um, so then we ended up working in the same place. But despite that, it still took us several years to end up actually collaborating on research. Do you remember how we ended up working together?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah. So I used to tell this story a lot. Actually, I was at this panel on AI in society back in, I think, it was probably 2016. It was taking place in DC. And someone on this panel made this statement that soon our AI systems are just going to be so good that all of the uncertainty is going to be taken out of our decision-making, and something about this statement just, like, really set me off. I got so mad about it because I thought it was just …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; I remember.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; … such an irresponsible thing to be saying. So I came back to New York, and I think I was ranting to you about this in the lab, and this conversation ended up getting us started on this whole longer discussion about the importance of communicating uncertainty and about explaining the assumptions that are behind the predictions that you’re making and all of this.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; So this was something … I was really excited about this because this was something that had really been drummed into me for years as a Bayesian. So Bayesian statistics, which forms a lot of the foundation of the type of machine learning that I was doing, is all about explicitly stating assumptions and quantifying uncertainty. So I just felt super strongly about this stuff.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah. So somehow all of these discussions we were having led us to read up on this literature that was coming out of the machine learning community on interpretability at the time. There are a bunch of these papers coming out that were making claims about models being interpretable without stopping to define &lt;em&gt;who&lt;/em&gt; they were interpretable to or for what purpose. Never &lt;em&gt;actually&lt;/em&gt; taking these models and putting them down in front of real people. And we wanted to do something about this. So we started running controlled experiments with &lt;em&gt;real&lt;/em&gt; people and found that we often can’t trust our intuition about what makes a model interpretable.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah, one of the things that came up a lot in that work was, sort of, how to measure these squishy abstract human concepts, like &lt;em&gt;interpretability&lt;/em&gt;, that are really hard to define, let alone quantify and measure and stuff like that.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Absolutely. So I think one of the first things that we really struggled with in this line of work was what it even means to be &lt;em&gt;interpretable&lt;/em&gt; or &lt;em&gt;intelligible&lt;/em&gt; or any of these terms that were getting thrown around at the time.&lt;/p&gt;



&lt;p&gt;Um, we ended up doing some research, which is still one of my favorite papers, …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Me, too.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; … with our colleagues Forough Poursabzi, Jake Hofman, and Dan Goldstein. And in this work, we found it really useful to think about interpretability as a latent property that can be, kind of, influenced by different properties of a model or system’s design. So things like the number of features the model has or whether the model’s linear or even things like the user interface of the model.&lt;/p&gt;



&lt;p&gt;This was kind of a gateway project for me in the sense that it’s one of the first projects that I got really excited about that was more of a human-computer interaction, or HCI, project rather than a theory project like I’d been working on in the past. And it just set off this huge spark of excitement in me. It felt to me at the time more important than other things that I was doing, and I just wanted to do more and more of this work.&lt;/p&gt;



&lt;p&gt;I would say the other project that had a really similar effect on me, which we also worked on together right around the same time, was our work with Ken Holstein mapping out challenges that industry practitioners were facing in the space of AI fairness.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Oh yeah. OK, yep. That project, that was so fun, and I learned so much from it. If I recall correctly, we originally hired Ken, who I think was an HCI PhD student at CMU at the time, as an intern …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; … to work with us on creating, sort of, user experiences for fairness tools like the Fairlearn toolkit&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. And we started that project—so that was in collaboration with Miro Dudík and Hal Daumé—we started that project by having Ken talk to a whole bunch of practitioners at Microsoft but at other organizations, as well, to get a sense for how they were and weren’t using fairness toolkits like Fairlearn.&lt;/p&gt;



&lt;p&gt;And I want to point out that at that point in time, the academic research community was super focused on all of these, like, simple quantitative metrics for assessing the fairness in the context of predictions and predictive machine learning models with this, kind of, understanding that these tools could then be built to help practitioners assess the fairness of their predictive models and maybe even make fairer predictions. And so that’s the kind of stuff that this Fairlearn toolkit was originally developed to do. So we ended up asking all of these practitioners originally just as, sort of, the precursor to what we thought we were going to end up doing with this project.&lt;/p&gt;



&lt;p&gt;We also asked these practitioners about their current practices and challenges around fairness in their work and about their additional needs for support. So where did they feel like they had the right tools and processes and practices and where did they feel like they were missing stuff. And this was really eye-opening because what we found was so different than what we were expecting. And there’s two things that really stood out to us.&lt;/p&gt;



&lt;p&gt;So the first thing was that we found a much, much wider range of applications beyond prediction. So we’d come into this assuming that all these practitioners were doing stuff with predictive machine learning models, but in fact, we were finding they were doing all kinds of stuff. There was a bunch of unsupervised stuff; there was a bunch of, you know, language-based stuff—all of this kind of thing. And in hindsight, that probably doesn’t sound very surprising nowadays because of the rise of generative AI, and really the entire machine learning and AI field is much less focused on prediction in that, kind of, narrow, kind of, classification-regression kind of way. But at the time, this was really surprising, especially in light of the academic literature’s focus on predictions when thinking about fairness.&lt;/p&gt;



&lt;p&gt;The second thing that we found was that practitioners often struggled to use existing fairness research, in part because these quantitative metrics that were all the rage at that point in time, just weren’t really amenable to the types of real-world complex scenarios that these practitioners were facing. And there was a bunch of different reasons for this, but one of the things that really stood out to us was that this wasn’t so much about the underlying models and stuff like that, but it was actually that there were a variety of data challenges involved here around things like data collection, collection of sensitive attributes, which you need in order to actually use these fairness metrics.&lt;/p&gt;



&lt;p&gt;So putting all this together, the upshot of all this was that we never did what we originally set out to do with that [LAUGHS], that internship project. We … because we uncovered this really large gap between research and practice, we ended up publishing this paper that characterized this gap and then surfaced important directions for future research. The other thing that the paper did was emphasize the importance of doing this kind of qualitative work to actually understand what’s happening in practice rather than just making assumptions about what practitioners are and aren’t doing.&lt;/p&gt;



&lt;p&gt;The other thing that came out of it, of course, was that the four of us—so you, me, Miro and Hal—learned a ton about HCI and about qualitative research from Ken, which was just, uh, so fun.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, and I started to be confronted with the fact that I could no longer reasonably ignore all of these messes of the real world because, you know, in some ways, responsible AI is really all about the messes.&lt;/p&gt;



&lt;p&gt;So I think this project was really a big shift for both of us. And in some ways, working on this and the interpretability work really led us to be active in these early efforts that were happening within Microsoft in the responsible AI space. Um, the research that we were doing was feeding directly into company policy, and it felt like it was just, like, a huge place where we could have some impact. So it was very exciting.&lt;/p&gt;



&lt;p&gt;So switching gears a bit. Hanna, do you remember how we first got the idea for WiML?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yes, I do. So we were at NeurIPS. This was back in 2005. It was a … so NeurIPS was a very different conference back then. Now it’s like tens of thousands of people. It’s held in a massive convention center. Yes, there are researchers there, but there’s a variety of people from across the tech industry who attend, but that is &lt;em&gt;not&lt;/em&gt; what it was like back then.&lt;/p&gt;



&lt;p&gt;So in around … in 2005, it was more like 600 people thereabouts in total[1], and the main conference would be held every year in Vancouver, and then everybody at the conference would pile onto these buses, and we would all head up to Whistler for the workshops.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; So super different to what’s happening nowadays. It was my third time. I think that’s right. I think it was my third time attending the conference. But it was my first time sharing a hotel room with other women. And I remember up at the workshops, up in Whistler, there were five of us sitting around in a hotel room, and we were talking about how amazing it was that there were five of us sitting around talking, &lt;em&gt;women&lt;/em&gt;. And we, kind of, couldn’t believe there were five of us. We’re all PhD students at the time. And so we decided to make this list, and we started trying to figure out who the other women in machine learning were. And we came up with about 10 names, and we were kind of amazed that there were even 10 women in machine learning. We thought this was a &lt;em&gt;huge&lt;/em&gt; number. We were very excited. And we started talking about how it might be really fun to just bring them all together sometime.&lt;/p&gt;



&lt;p&gt;So we returned from NeurIPS, and you and I ended up getting lunch to strategize. I still remember walking out of the department together to go get lunch and you were walking ahead of me. I can visualize the coat you were wearing as you were walking in front of me. And so we strategized a bit and ended up deciding, along with one of the other women, Lisa Wainer, to submit a proposal to the Grace Hopper conference for a session in which women in machine learning would give short talks about their research.&lt;/p&gt;



&lt;p&gt;We reached out to the 10 names that we had written down in the hotel room and through that process actually ended up finding out about more women in machine learning and eventually had something like 25 women listed on the final proposal. I think there’s an email somewhere where one or another of us is saying to the other one, “Oh my gosh! I can’t believe there are so many women in machine learning.”&lt;/p&gt;



&lt;p&gt;So we submitted this proposal, and ultimately, the proposal was rejected by the Grace Hopper conference. But we were so excited about the idea and just really invested in it by that point that we decided to hold our own co-located event the day before the Grace Hopper conference. And I’ve got to say, you know, 20 years later, I don’t know &lt;em&gt;what&lt;/em&gt; we were thinking. Like, that was a bold move on the part of three PhD students. And it turned out to be a huge amount of work that we had to do entirely ourselves, as well.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; We had no idea what we were doing. But the Grace Hopper folks very nicely connected us with the venue that the conference was going to be held at, and somehow, we managed to pull it off. Ultimately, that first workshop had around 100 women, and there was this … rather than just, like, a single short session, which is what we’d originally had in mind, we had this full day’s worth of talks. I actually have the booklet of abstracts from all of those talks at my desk in the office. I still have that today. And it was just an amazing experience.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, it was. And, you know, you mentioned how bold we were. I just, I really don’t think that any of us at the time realized how bold we were being here, getting this workshop rejected and then saying, you know, &lt;em&gt;no&lt;/em&gt;, we think this is important. We’re going to do it anyway. &lt;em&gt;On our own. As grad students.&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;So I’ve already talked a little bit about some of the spaces that I was in throughout my career where there just weren’t a lot of women around in the room with me. How had you experienced a lack of community or network of women in machine learning before the founding of WiML? And, you know, why do you think it’s important to have that kind of community?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; So I felt it in a number of different ways. I think I mentioned a few minutes ago that, like, it was my third time at NeurIPS but my first time sharing a hotel room with another woman. But there were many places over the years where I’d felt this.&lt;/p&gt;



&lt;p&gt;So first, as an undergraduate. Then, I did a lot of free and open-source software development, and I was pretty involved in stuff to do with the Debian Linux distribution. And back then, the percentage of women involved in free and open-source software development was about 1 percent, 1.5%&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, and the percentage involved actually in Debian was even less than that. So that had led me and some others to start this Debian Women Project&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. And then, again, of course, I faced this in machine learning.&lt;/p&gt;



&lt;p&gt;I just didn’t know that many other women in machine learning. I didn’t … there weren’t a large number of senior women, for example, to look up to as role models. There weren’t a large number of female PhD students. And this, kind of, made me sad because I was really excited about machine learning, and I hoped to spend my entire career in it. But because I didn’t see so many other women around, particularly more senior women, that really made me question whether that would even be possible, and I just didn’t know.&lt;/p&gt;



&lt;p&gt;Um, I think, you know, thinking about this, and I’ve obviously reflected on this a lot over the years, but I think having a diverse community in any area, be it free and open-source software development, be it machine learning, any of these kinds of things, is just so important for so many reasons. And some of those reasons are little things like finding people that you would feel comfortable sharing a hotel room with.&lt;/p&gt;



&lt;p&gt;But many of these things are bigger things that can then have, like, even, kind of, knock-on cumulative effects. Like feeling valued in the community, feeling welcome in the community, having role models, being able to, sort of, see people and say, “Oh, I want to be kind of like that person when I grow up; I could do this.” And then even just representation of different perspectives in the work itself is so important.&lt;/p&gt;



&lt;p&gt;The flip side of that is that there are a whole bunch of things that can go wrong if you don’t have a diverse community. You can end up with gatekeeping, with toxic or unsafe cultures, obviously attrition as people just leave these kinds of spaces because they feel that they’re not welcome there and won’t be valued there. And then to that point of having representation of different perspectives, with a really homogeneous community, you can end up with, kind of, blind spots around the technology itself, which can then lead to harms.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; 100%. So did you ever imagine during&amp;nbsp;all of&amp;nbsp;this that WiML would still be around 20 years later and we would be sitting here on a podcast talking about this?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; [LAUGHS] No, absolutely not. I didn’t even think that WiML would necessarily be around for a second year. I thought it was probably going to be, like, a one-off event. And I certainly don’t think that I thought that I would still be involved in the machine learning community 20 years later, as well. So very unexpected.&lt;/p&gt;



&lt;p&gt;I’ve got a question for you, though. What do you remember most about that first workshop?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; I remember a lot of things. I remember that, you know, when we were planning this, we always really wanted the focus to be the research. And, you know, if you think back to what this first workshop looked like, it was a lot of us just giving talks or presenting posters about our own research to other people.&lt;/p&gt;



&lt;p&gt;And, you know, I remember thinking at the poster session, like, the vibe was just so much different and better, healthier really than other poster sessions I had been to. Everyone was so supportive and encouraging, but it really was all about the research. I also remember being blown away just walking into that conference room in the morning and seeing all of these women gathered in one place and knowing that somehow, we had actually made this happen.&lt;/p&gt;



&lt;p&gt;Um, I remember we also faced some challenges with the workshop early on. What are the challenges that stand out to you most?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah, so a lot of people really got it, right. And they were super supportive. So, for example, folks at Penn totally got it, and they actually funded a bunch of that first workshop. But others in the community didn’t get it and didn’t see the point, didn’t see why it was necessary.&lt;/p&gt;



&lt;p&gt;I remember having dinner with one machine learning researcher and him telling me that he didn’t think this kind of workshop was necessary because women’s experiences were no different to men’s experiences. And then later on in the conversation, he talked about—like, you know, this is, like, an hour and a half later or something—he talked about how he and a friend of his had gone to the bar at an all-women’s college and he’d felt so awkward and out of place. And I ended up pointing out to him [LAUGHS] that he just, kind of, explained to himself why we needed WiML. So, yeah, there were some people who didn’t get it, and it took a lot of, sort of, talking to people and, kind of, explaining.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Another challenge was figuring out how to fund it in an ongoing manner once we decided that we wanted to do this more than once.&lt;/p&gt;



&lt;p&gt;So as I said, Penn funded a lot of that first workshop, but that wasn’t a sustainable model, and it wasn’t going to be realistic for Penn to keep funding it. So in the end, we worked with Amy Greenwald to obtain a National Science Foundation grant that would cover a lot of costs, and we also received donations from other organizations.&lt;/p&gt;



&lt;p&gt;Um, a third challenge was figuring out where to hold the workshop given that we did want that focus to be on research. So the first two times, we held the workshop at the Grace Hopper conference, but we started to feel that that wasn’t really the right venue given that we wanted that focus to be on research. So we ended up moving it to NeurIPS, and this had a bunch of benefits, some of which I don’t think we’d even fully thought through when we made that decision.&lt;/p&gt;



&lt;p&gt;So one of the benefits was that attendees’ WiML travel funding—so we would give them this travel funding to enable them to pay the cost of attending WiML, stay in hotel rooms, all this kind of stuff—this would actually enable them to attend NeurIPS, as well, if we co-located with NeurIPS.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Another main benefit was that we held WiML on the day before NeurIPS. So then throughout the rest of the conference, WiML attendees would see familiar faces throughout the crowd and wouldn’t necessarily feel so alone.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; So you’re talking about these challenges. How have these challenges changed over time? Or, you know, more broadly, can you talk about how the workshop and Women in Machine Learning as an organization as a whole, kind of, evolved over the years? I know that you served a term as the WiML president.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah. So it’s changed a lot. So first, obviously, most importantly, it evolved from being, kind of, this one-off event where we were just seeing what would happen to being really a robust organization. And the first step in that was creating the WiML board. And, as you just said, I served as the first president of that.&lt;/p&gt;



&lt;p&gt;But there have been a bunch of other steps since then. And one of the things I want to flag about the WiML board was that this was really important because the board members could focus on the long-term health of the organization and these, sort of, like, you know, things that spanned multiple years, like how to get sustainable funding sources, this kind of thing, versus the actual workshop organizers, who would focus on things like running the call for submissions and stuff like that. And being able to separate those roles made it really just reduce the burden on the workshop organizers meant that we could take this, kind of, longer-term perspective.&lt;/p&gt;



&lt;p&gt;Another really important step was becoming, &lt;em&gt;officially&lt;/em&gt; becoming a non-profit. So that happened a few years ago. And again, it was the natural thing to do at that point in time and just another step towards creating this, sort of, durable, robust organization.&lt;/p&gt;



&lt;p&gt;But it’s really taken on a life of its own. I’m honestly not super actively involved nowadays, which I think is fantastic. The organization doesn’t need me. That’s great. It’s also wild to me that because it’s been around for 20 years at this point that there are women in the field who don’t know what it’s like to &lt;em&gt;not&lt;/em&gt; have WiML.&lt;/p&gt;



&lt;p&gt;So a bunch of other affinity groups got created. So Timnit Gebru cofounded Black in AI when she was actually a postdoc at Microsoft Research New York City. So you and I got to actually see the founding of that affinity group up close. And then now there are a ton of other affinity groups. So there’s LatinX in AI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;; there’s Queer in AI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Muslims in ML&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Indigenous in AI and ML&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, New In ML&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, just to name a few.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, and all of these are growing, too, every year.&lt;/p&gt;



&lt;p&gt;You know, this year, WiML had over 400 submissions. They accepted 250 to be presented. It’s amazing.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; That’s wild.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, yep. And there’s going to be a WiML presence this year actually at all three of the NeurIPS venues. So there’s going to be a presence in Mexico City, in Copenhagen, and, of course, in San Diego for the main workshop. So it’s pretty great.&lt;/p&gt;



&lt;p&gt;And, you know, on top of that, I think the organization now, as you were saying, is able to do so much more than just the workshop alone. So for instance, WiML now runs this worldwide mentorship program for women and nonbinary individuals in machine learning, where they’re matched with a mentor and they can participate in these one-to-one mentoring meetings and seminars and panel discussions, which happens all throughout the year. I think they have about 50 mentors signing up each year, but I’m sure they could always use more. Um, so it’s just really amazing to look back and see how much the WiML community has done and how much it’s grown.&lt;/p&gt;



&lt;p&gt;And, you know, on the one hand, I think that honestly, like, founding WiML was one of the things that I’ve done over the course of my career, if not &lt;em&gt;the&lt;/em&gt; thing, that I am most proud of …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Oh yeah, me, too.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; … to this day, but at the same time, like, we can’t take credit for any of this. It’s, like, a community effort.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; No.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; It’s the community that has really kept us going …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yes.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; … for the last 20 years,&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yes.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; … so it’s great. Going to stop gushing now, but it’s amazing.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; And it’s not just WiML that’s changed over the years. The entire industry has changed a ton, as well.&lt;/p&gt;



&lt;p&gt;How has your research evolved as a result of these changes to the entire field of AI and machine learning and also from your own change from academia to industry?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; It’s a great question. You know, we’ve touched on this a little bit, but our research paths really evolved differently but ended up in these very similar places where we’re working on responsible AI, we’re advocating for interdisciplinary approaches, incorporating techniques from HCI, and so on. And I think that part of this was because of shifts of the community and also what’s happening in industry. Working in responsible AI in industry, there’s definitely not ever a shortage of interesting problems to solve, right.&lt;/p&gt;



&lt;p&gt;And I think that for both of us, our research interests in recent years really have been driven by these really practical challenges that we’re seeing. We were both involved early on in defining what responsible AI means within Microsoft, shaping our internal Responsible AI Standard&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. I led this internal companywide working group on AI transparency, which was focused both on model interpretability like we were talking about earlier but also other forms of transparency like data sheets for datasets and the transparency notes that Microsoft now releases with all of our products. And at the same time, you are leading this internal working group on fairness.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah, taking on that internal working group was, kind of, a big transition point in my career. You know, when I joined Microsoft, I was focusing on computational social science and I was also entirely doing research and wasn’t really that involved in stuff in the rest of the company.&lt;/p&gt;



&lt;p&gt;Then at the end of my first year at Microsoft, I attended the first Fairness, Accountability, and Transparency in Machine Learning workshop&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, which was co-located with NeurIPS. It was one of the NeurIPS workshops. And I got really excited about that and thought, great, I’m going to spend like 20% of my time, maybe one day a week, doing research on topics in the space of fairness and accountability and transparency. Um, that is not what ended up happening.&lt;/p&gt;



&lt;p&gt;Over the next couple of years, I ended up doing more and more research on responsible AI, you know, as you said, on topics to do with fairness, to do with interpretability. And then in early 2018, I was asked to co-chair this internal working group on fairness, and that was the point where I started getting much more involved in responsible AI stuff across Microsoft, so outside of just Microsoft Research.&lt;/p&gt;



&lt;p&gt;And this was really exciting to me because responsible AI was so new, which meant that research had a really big role to play. It wasn’t like this was kind of an established area where folks in engineering and policy knew exactly what they were doing. And so that meant that I got to branch out from this very, sort of, research-focused work into much more applied work in collaboration with folks from policy, from engineering, and so on.&lt;/p&gt;



&lt;p&gt;Now, in fact, as well as being a researcher, I actually run a small applied science team, the Sociotechnical Alignment Center, or STAC for short, within Microsoft Research that focuses specifically on bridging research and practice in responsible AI.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah. Do you think that your involvement in WiML has played a role in this work?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yes, definitely. [LAUGHS] Yeah, without a doubt. So particularly when working on topics related to fairness, I’ve ended up focusing a bunch on stuff to do with marginalized groups as part of my responsible AI work.&lt;/p&gt;



&lt;p&gt;So there’s been this, sort of, you know, focus on marginalized groups, particularly women, in the context of machine learning and with my WiML, kind of, work and then in my research work thinking about fairness, as well.&lt;/p&gt;



&lt;p&gt;The other way that WiML has really, sort of, affected what I do is that I work with a much more varied group of people nowadays than I did back when I was just focusing on, kind of, machine learning, computational social science, and stuff like that. And many of my collaborators are people that I’ve met through WiML over the years.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; And, of course, there has been another big shift within industry recently, which is just all the excitement around generative AI. Can you say a bit about how that has changed your research?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; OK, yeah. So this is another big one. There are so many ways that this changed my work. One of the biggest ways, though, is that generative AI systems are now everywhere. They’re being used all over the place for all kinds of things. And, you know, you see all these news headlines about GenAI systems, you know, diagnosing illnesses, solving math problems, and writing code, stuff like that. And also headlines about various different risks that can occur when you’re using generative AI. So fabricating facts, memorizing copyrighted data, generating harmful content, you know, these kinds of things. And with all this attention, it’s really natural to ask, what is the evidence behind these claims? So where is this evidence coming from, and should we trust it?&lt;/p&gt;



&lt;p&gt;It turns out that much of the evidence comes from GenAI evaluations that involve measuring the capabilities, the behaviors, and the impacts of GenAI systems, but the current evaluation practices that are often used in the space don’t really have as much scientific rigor as we would like, and that’s, kind of, a problem.&lt;/p&gt;



&lt;p&gt;So one of the biggest challenges is that the concepts of interest when people are, sort of, doing these GenAI evaluations—so things like diagnostic ability, memorization, harmful content, concepts like that—are much more abstract than the concepts like prediction accuracy that underpinned machine learning evaluations before the generative AI era.&lt;/p&gt;



&lt;p&gt;And when we look at these new concepts that we need to be able to focus on in order to evaluate GenAI systems, we see that they’re actually much more reminiscent of these abstract contested concepts—these, kind of, fuzzy, squishy concepts—that are studied in the social sciences. So things like democracy and political science or personality traits and psychometrics. So there’s really that, sort of, connection there to these, kind of, squishier things.&lt;/p&gt;



&lt;p&gt;So when I was focusing primarily on computational social science, most of my work was focused on developing machine learning methods to help social scientists measure abstract contested concepts. So then when GenAI started to be a big thing and I saw all of these evaluative claims involving measurements of abstract concepts, it seemed super clear to me that if we were going to actually be able to make meaningful claims about what AI can do and can’t do, we’re going to need to take a different approach to GenAI evaluation.&lt;/p&gt;



&lt;p&gt;And so I ended up, sort of, drawing on my computational social science work around measurement and I started advocating for adopting a variant of the framework that social scientists use for measuring abstract contested concepts. And my reason for doing this was that I believed—I &lt;em&gt;still&lt;/em&gt; believe—that this is an important way to improve the scientific rigor of GenAI evaluations.&lt;/p&gt;



&lt;p&gt;You know all of this, of course, because you and I, along with a &lt;em&gt;bunch&lt;/em&gt; of other collaborators at Microsoft Research and Stanford and the University of Michigan published a position paper on this framework entitled “Evaluating GenAI Systems is a Social Science Measurement Challenge” at ICML [International Conference on Machine Learning] this past summer.&lt;/p&gt;



&lt;p&gt;What are you excited about at the moment?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, so lately, I have been spending a lot of time thinking about AI and critical thought: how can we design AI systems to support appropriate reliance, preserve human agency, and really encourage critical engagement on the part of the human, right?&lt;/p&gt;



&lt;p&gt;So this is an area where I think we actually have a huge opportunity, but there are also huge risks. If I think about my most optimistic possible vision of the future of AI —which is not something that is easy for me to do, as I’m not a natural optimist, as you know—it would be a future in which AI helps people grow and flourish, in which it, kind of, enriches our own &lt;em&gt;human&lt;/em&gt; capabilities. It deepens our own &lt;em&gt;human&lt;/em&gt; thinking and safeguards our own agency.&lt;/p&gt;



&lt;p&gt;So in this future, you know, we could build AI systems that actually help us brainstorm and learn new knowledge and skills, both in formal educational settings and in our day-to-day work, as well. But I think we’re not going to achieve this future by default. It’s something that we really need to design for if we want to get there.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; You mentioned that there are risks. What are the risks that you can see here?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, there’s so much at stake here. You know, in the short term, there are things like overreliance—depending on the output of an AI system even when the system’s wrong. This is something that I’ve worked on a bunch myself. There’s a risk of loss of agency or the ability to make and execute independent decisions and to ensure that our outcomes of AI systems are aligned with personal or professional values of the humans who are using those systems. This is something that I’ve been looking at recently in the context of AI tools for journalism&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. There’s diminished innovation, by which I mean a loss of creativity or diversity of ideas.&lt;/p&gt;



&lt;p&gt;You know, longer term, we risk atrophied skills—people just losing or simply never developing helpful skills for their career or their life because of prolonged use of AI systems. The famous example that people often bring up here is pilots losing the ability to perform certain actions in flight because of dependence on autopilot systems. And I think we’re already starting to see the same sort of thing happen across all sorts of fields because of AI.&lt;/p&gt;



&lt;p&gt;And, you know, finally, another risk that I’ll mention that seems to resonate with a lot of folks I talk to is what I would just call loss of joy, right. What happens when we are delegating to AI systems the parts of our activities that we really take pleasure and find this satisfaction in doing ourselves.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; So then as a community, what should we be doing if we’re worried about these risks?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, I mean, I think this is going to have to be a big community effort if we want to achieve this. This is a big goal. But there are a few places I think we especially need work.&lt;/p&gt;



&lt;p&gt;So I think we need generalized principles and practices for AI system builders for how they can build AI systems in ways that promote human agency and encourage critical thought. We also need principles and practices for system &lt;em&gt;users&lt;/em&gt;. So how do we teach the general population to use AI in ways that amplify their skills and capabilities and help them learn new things?&lt;/p&gt;



&lt;p&gt;And then, you know, close to your heart, I’m sure, I think that we need more work on measurement and evaluation, right. We are once again back to these squishy human properties.&lt;/p&gt;



&lt;p&gt;You know, I mentioned I’ve done some work on overreliance in generative AI systems, and I started there because on the grand scale of risks here, overreliance is something that is relatively easy to measure, at least in the short term. But how do we start thinking about measuring people’s critical thinking when using AI across all sorts of contexts and at scale and over long-time horizons, right? How do we measure the, sort of, longitudinal effect of AI systems just on our critical thought as a population?&lt;/p&gt;



&lt;p&gt;And by the way, if anyone listening is going to be at the WiML workshop, I’ll actually be giving a keynote on this topic. And this is something I’m just incredibly excited about because first, I’m incredibly excited about this topic, but also, in the whole 20 years of WiML, I’ve given opening remarks and similar several times, but this is actually the very first time that I will be talking about my own research there. So this is like my dream. I’m thrilled that this is happening.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; That’s awesome. Oh, that’s so exciting. Excellent.&lt;/p&gt;



&lt;p&gt;So one last question for you. If you could go back and talk to yourself 20 years ago and give yourself some advice, what would you say?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, OK, I’ve thought about this one a bit over the past week, and there are three things here I want to mention.&lt;/p&gt;



&lt;p&gt;So first, I would tell myself to be brave about speaking up. You know I’m about as introverted as it gets and I’m naturally very shy, and this has always held me back. It still holds me back now. It was really embarrassingly late in my career that I decided to do something about this and start to develop strategies to help myself speak up more. And eventually, it started to grow into something that’s a little bit more natural.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; What kind of, um, what kind of strategies?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, so you know, one example is I use a lot of notes. For this podcast, I have a lot of notes here. I’m a big notes person, and things like that really help me.&lt;/p&gt;



&lt;p&gt;The second thing that I would tell myself is to, you know, work on the problems that you really want to see solved. As researchers, we have this amazing freedom to choose our own direction. And early on, you know, a lot of the problems that I worked on were problems that I really enjoyed thinking about on a day-to-day basis. It was a lot of fun. They were like little math puzzles to me. But I often found that, you know, when I would be at conferences and people would ask me about my work, I didn’t really want to talk about these problems. I just in some sense, you know, I had fun doing it, but I didn’t really care. I wasn’t passionate about it. I didn’t care that I had solved the problem.&lt;/p&gt;



&lt;p&gt;And so once, many years ago now, when I was thinking about my research agenda, I got some good advice from our former lab director, Jennifer Chayes, who suggested that I go through my recent projects and sort them into projects where I really &lt;em&gt;liked&lt;/em&gt; working on them—it was a fun experience day-to-day—and projects that I liked talking about after the fact and, kind of, felt good about the results and then see where the overlap is. And this is something that, like, it kind of sounds, kind of, obvious when I say it now, but at the time, it was really eye-opening for me.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; That’s so cool. And now I, kind of, want to do that with all of my projects, particularly at the moment. I actually just took five months, as you know, five months off of work for parental leave because I just had a baby. And so I’m, sort of, taking a big, kind of, inventory of everything as I get back into all of this now, and I love this idea. I think this is really cool.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; It’s changed really my whole approach to research. Like, you know, we were talking about this, but most of the work I do now is more HCI than machine learning because I found that the problems that really motivate me, that I want to be talking to people about at conferences, are the &lt;em&gt;people&lt;/em&gt; problems.&lt;/p&gt;



&lt;p&gt;The third piece of advice I would give myself is that you should bring more people into your work, right.&lt;/p&gt;



&lt;p&gt;So there’s this kind of vision on the outside of research being this solo endeavor, and it can feel so competitive at times, right. We all feel this. But time and time again, I’ve seen that the best research comes from collaborations and from bringing people together with diverse perspectives who can challenge each other in a way that is respectful but makes the work better.&lt;/p&gt;



&lt;p&gt;Is there advice that you would give to your former self of 20 years ago?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah. OK. So I’ve also been thinking about this a bunch over the past week. There’s actually a lot of advice I think I would give my former self, [LAUGHS] but there are three things that I keep coming back to.&lt;/p&gt;



&lt;p&gt;OK, so first—and this is similar to your second point—push for doing the work that you find to be most fulfilling even if that means taking a nontraditional path. So in my case, I’ve always been interested in the social sciences. Back when I was a student, you know, even when I was a PhD student, doing research that combined computer science &lt;em&gt;and&lt;/em&gt; the social sciences just wasn’t really a thing. And so as a result, it would have been really easy for me to just be like, “Oh well, I guess that isn’t possible. I’ll just focus on traditional computer science problems.”&lt;/p&gt;



&lt;p&gt;But that’s not what I ended up doing. Instead, and often in ways that made my career, kind of, harder than it probably would have been otherwise, I ended up pushing. I kept pushing, and in fact, I keep pushing, even nowadays, to bring these things together—computer science and the social sciences—in an interdisciplinary fashion. And this hasn’t been easy. But cumulatively, the effect has been that I’ve been able to do much more impactful work than I think I would have been able to do otherwise, and the work I’ve done, I’ve just enjoyed so much more than would otherwise have been the case.&lt;/p&gt;



&lt;p&gt;OK, so second, be brave and share your work. So this is actually advice for my current self and my former self, as this is something that I definitely still struggle with.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; As do I, you know, and actually, I think it’s funny to hear you say this because I would say that you are much better at this than I am.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; I still, I think I have a lot of work to do on this one. Yeah, it’s hard. It’s really hard.&lt;/p&gt;



&lt;p&gt;As you know, I am a perfectionist, and this is good in some ways, but this is also bad in other ways. And one way in which this is bad is that I tend to be really anxious about sharing and publicizing my work, especially when I feel it’s not perfect.&lt;/p&gt;



&lt;p&gt;So as an example, I wrote this massive tutorial on computational social science for ICML in 2015, but I never put the slides … and I wrote a whole script for it … I never put the slides or the script online as a resource for others because I felt it needed more work. And I actually went back and looked at it earlier this year, when we were working on the ICML paper, and I was stunned because it’s great. Why didn’t I put this online? All these things that I thought were problems 10 years ago, no, they’re not a big deal. I should have just shared it.&lt;/p&gt;



&lt;p&gt;As another example, STAC, my applied science team, was using LLMs as part of our approach to GenAI evaluation back in 2022, way before the sort of “LLM-as-a-judge” paradigm was widespread. But I was really worried that others would think negatively of us for doing this, so we didn’t share that much about what we were doing, and I regret that because we missed out on an opportunity to kick off an industrywide discussion about this, sort of, LLM-as-a-judge paradigm.&lt;/p&gt;



&lt;p&gt;OK, so then my third point is that the social side of research is just as valuable as the technical side. And by this, I’m actually not talking about social science and computer science. I actually mean that the &lt;em&gt;how&lt;/em&gt; of doing research, including &lt;em&gt;who&lt;/em&gt; you talk to, &lt;em&gt;who&lt;/em&gt; you collaborate with, and &lt;em&gt;how&lt;/em&gt; you approach those interactions, is just as important as the research itself.&lt;/p&gt;



&lt;p&gt;As a PhD student, I felt really bad about spending time socializing with other researchers, especially at conferences, because I thought that I was supposed to be listening to talks, reading papers, and discussing technical topics with researchers and not socializing. But in hindsight, I think that was wrong. Many of those social connections have ended up being incredibly valuable to my research, both because I’ve ended up collaborating with and in some cases even hiring the people who I first got to know socially …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; … but also because the friendships that I’ve built, like our friendship, for example, have served as a crucial support network over the years, especially when things have felt particularly challenging.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, absolutely. I agree with all of that so much.&lt;/p&gt;



&lt;p&gt;And with that, I will say thank you so much for doing this podcast with me today.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Thank &lt;em&gt;you&lt;/em&gt;.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; It was a lot of fun to reflect on the last 20 years of WiML, but also the last 20 years of our careers and friendship and all of this, so it’s great, and I never would have agreed to do this if it had been with anyone but you.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Likewise. [LAUGHS]&lt;/p&gt;



&lt;p&gt;So thank you, everybody, for listening to us, and hopefully some of you will join for the 20th annual workshop for Women in Machine Learning&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, which is taking place on Dec. 2. And of course, Jenn and I will both be there in person. We’ll also be at NeurIPS afterwards. So feel free to reach out to us if you want to chat with us or to learn more about anything that we covered here today.&lt;/p&gt;



&lt;p&gt;[MUSIC]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;OUTRO:&lt;/strong&gt; You’ve been listening to &lt;em&gt;Ideas&lt;/em&gt;, a Microsoft Research Podcast. Find more episodes of the podcast at aka.ms/researchpodcast&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&lt;/p&gt;

				&lt;/span&gt;
			&lt;/div&gt;
			&lt;button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button"&gt;
				Show more			&lt;/button&gt;
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;p&gt;[1] Wallach later clarified that the number of registrants for the 2005 Conference on Neural Information Processing Systems was around 900.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/11/HannaJenn-Ideas_TW_LI_FB_1200x627-1.jpg" /&gt;&lt;/div&gt;&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;/figure&gt;






&lt;p&gt;Behind every emerging technology is a great idea propelling it forward. In the Microsoft Research Podcast series &lt;em&gt;Ideas&lt;/em&gt;, members of the research community at Microsoft discuss the beliefs that animate their research, the experiences and thinkers that inform it, and the positive human impact it targets.&lt;/p&gt;



&lt;p&gt;In 2006, three PhD students organized the Women in Machine Learning Workshop, or&amp;nbsp;&lt;em&gt;WiML&lt;/em&gt;, to provide a space for women in ML to connect and share their research. The event has been held every year since,&amp;nbsp;growing in size&amp;nbsp;and mission.&lt;/p&gt;



&lt;p&gt;In this episode, two of the WiML cofounders, Jenn Wortman Vaughan, a Microsoft senior principal research manager, and Hanna Wallach, a Microsoft vice president and distinguished scientist, reflect on the 20th workshop. They discuss WiML’s journey from a potential one-off event to a nonprofit supporting women and nonbinary individuals worldwide; their friendship and collaborations, including their contributions to defining responsible AI at Microsoft; and the advice they’d give their younger selves.&lt;/p&gt;



&lt;div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow"&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h2 class="wp-block-heading h5" id="learn-more-1"&gt;Learn more:&lt;/h2&gt;








&lt;/div&gt;



&lt;section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast"&gt;
	
&lt;/section&gt;


&lt;div class="wp-block-msr-show-more"&gt;
	&lt;div class="bg-neutral-100 p-5"&gt;
		&lt;div class="show-more-show-less"&gt;
			&lt;div&gt;
				&lt;span&gt;
					

&lt;h2 class="wp-block-heading" id="transcript"&gt;Transcript&lt;/h2&gt;



&lt;p&gt;[MUSIC]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SERIES INTRODUCTION:&lt;/strong&gt; You’re listening to &lt;em&gt;Ideas&lt;/em&gt;, a Microsoft Research Podcast that dives deep into the world of technology research and the profound questions behind the code. In this series, we’ll explore the technologies that are shaping our future and the big ideas that propel them forward.&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;JENN WORTMAN VAUGHAN:&lt;/strong&gt; Hello, and welcome. I’m Jenn Wortman Vaughan. This week, machine learning researchers around the world will be attending the annual Conference on Neural Information Processing Systems, or NeurIPS. I am especially excited about NeurIPS this year because of a co-located event, the 20th annual workshop for Women in Machine Learning&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, or WiML, which I am going to be attending both as a mentor and as a keynote speaker.&lt;/p&gt;



&lt;p&gt;So to celebrate 20 years of WiML, I’m here today with my long-term collaborator, colleague, close friend, &lt;em&gt;and&lt;/em&gt; my cofounder of the workshop for Women in Machine Learning, Hanna Wallach.&lt;/p&gt;



&lt;p&gt;You know, you and I have known each other for a very long time at this point. And in many ways, we followed very parallel and often intersecting paths before we both ended up here working in responsible AI at Microsoft. So I thought it might be fun to kick off this podcast with a bit of the story of our interleaving trajectories.&lt;/p&gt;



&lt;p&gt;So let’s start way back 20 years ago, around the time we first had the idea for WiML. Where were you, and what were you up to?&lt;/p&gt;



				&lt;/span&gt;
				&lt;span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1"&gt;
					



&lt;p&gt;&lt;strong&gt;HANNA WALLACH:&lt;/strong&gt; Yeah, so I was a PhD student at the University of Cambridge, and I was working with the late David MacKay. I was focusing on machine learning for analyzing text, and at that point in time, I’d actually just begun working on Bayesian latent variable models for text analysis, and my research was really focusing on trying to combine ideas from &lt;em&gt;n&lt;/em&gt;-gram language modeling with statistical topic modeling in order to come up with models that just did a better job at modeling text.&lt;/p&gt;



&lt;p&gt;I was also doing this super-weird two-country thing. So I was doing my PhD at Cambridge, but at the end of the first year of my PhD, I spent three months as a visiting graduate student at the University of Pennsylvania, and I loved it, so much so that at the end of the three months I said, can I extend for a full year? Cambridge said yes; Penn said yes. So I did that and actually ended up then extending another year and then another year and another year and so on and so forth.&lt;/p&gt;



&lt;p&gt;But during my first full year at Penn, that was when I met &lt;em&gt;you&lt;/em&gt;, and it was at the visiting students weekend, and I had been told by the faculty in the department that I had to work really hard on recruiting you. I had no idea that that was actually going to be the start of a 20-plus-year friendship.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, I still remember that visiting weekend very well. I actually met you; I met my husband, Jeff; and I met my PhD advisor, Michael Kearns, all on the same day at that visiting student weekend. So I didn’t know it at the time, but it was a very big day for me.&lt;/p&gt;



&lt;p&gt;So around that time when I started my PhD at Penn, I was working in machine learning theory and algorithmic economics. So even then, you know, just like I am now, I was interested in the intersection of people and AI systems. But since my training was in theory, my “people” tended to be these mathematically ideal people with these well-defined preferences and beliefs who behaved in very well-defined ways.&lt;/p&gt;



&lt;p&gt;Working in learning theory like this was appealing to me because it was very neat and precise. There was just none of the mess of the real world. You could just write down your model, which contained all of your assumptions, and everything else that followed from there was in some sense objective.&lt;/p&gt;



&lt;p&gt;So I was really enjoying this work, and I was also so excited to have you around the department at the time. You know, honestly, I also loved Penn. It was just such a great environment. I was just actually back there a few weeks ago, visiting to give a talk. I had an amazing time. But it was, I will say, very male dominated in the computer science department at the time. In my incoming class of PhD students, we had 20 incoming PhDs, and I was the only woman there. But we managed to build a community. We had our weekly ladies brunch, which I loved, and things like that really kept me going during my PhD.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah, I loved that ladies brunch. That made a huge difference to me and, kind of, kept me going through the PhD, as well.&lt;/p&gt;



&lt;p&gt;And, like you, I’d always been interested in people. And during the course of my PhD, I realized that I wasn’t interested in analyzing text for the sake of text, right. I was interested because text is one of these ways that people communicate with each other. You know, people don’t write text for the sake of writing text. They write it because they’re trying to convey something. And it was really &lt;em&gt;that&lt;/em&gt; that I was interested in. It was these, kind of, social aspects of text that I found super interesting.&lt;/p&gt;



&lt;p&gt;So coming out of the PhD, I then got a postdoc job focused on analyzing texts as part of these, sort of, broader social processes. From there, I ended up getting a faculty job, also at UMass, as one of four founding members of UMass’s Computational Social Science Institute&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. So there was me in computer science, then there was another assistant professor in statistics, another in political science, and another in sociology. And in many ways, this was my dream job. I was being paid to develop and use machine learning methods to study social processes and answer questions that social scientists wanted to study. It was pretty awesome. You, I think, started a faculty position at the same time, right?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah. So I also did a postdoc. First, I spent a year as a postdoc at Harvard, which was super fun. And then I started a tenure track position in computer science at UCLA in 2010.&lt;/p&gt;



&lt;p&gt;Again, you know, it was a very male-dominated environment. My department was mostly men. But maybe even more importantly than this, I just didn’t really have a network there. You know, it was lonely. One exception to this was Mihaela van der Schaar. She was at UCLA at the time, though not in my department, and she, kind of, took me under her wing. So I’m very grateful that I had that support. But overall, this position just wasn’t a great fit for me, and I was under more stress then than I think I have been at any other point in my life that I could really remember.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah. So at that point, then, you ended up transitioning to Microsoft Research, right?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Why did you end up choosing MSR [Microsoft Research]?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, so this was back in 2012. MSR had just opened up this &lt;em&gt;new&lt;/em&gt; New York City lab at the time, and working in this lab was basically my dream job. I think I actually tried to apply before they had even officially opened the lab, like when I just heard it was happening.&lt;/p&gt;



&lt;p&gt;So this lab focused in three areas at the time. It focused in machine learning, algorithmic economics, and computational social science. And my research at the time cut across all three of these areas. So it felt just like this perfect opportunity to work in the space where my work would fit in so well and be really appreciated.&lt;/p&gt;



&lt;p&gt;The algorithmic economics group at the time actually was working on building prediction markets to aggregate information about future events, and they were already, in doing this, building on top of some of my theoretical research, which was just super cool to see. So that was exciting. And I already knew a couple of people here. I knew John Langford and Dave Pennock, who was in the economics group at the time, because I’d done an internship actually with the two of them at Yahoo Research before they came to Microsoft. And I was really excited to come back and work with them again, as well.&lt;/p&gt;



&lt;p&gt;You know, even here at the time that I joined the lab, it was 13 men and me. So once again, not great numbers. And I think that in some ways this was especially hard on me because I was just naturally, like, a very shy person and I hadn’t really built up the confidence that I should have at that point in my career. But on the other hand, I found the research fit just so spot-on that I couldn’t say no. And I suspect that this is something that you understand yourself because you actually came and joined me here in the New York lab a year or two later. So why did you make this switch?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah, so I anticipated that I was going to love my faculty job. It was focusing on all this stuff that I was so excited about. And much to my surprise, though, I kind of didn’t. And it wasn’t like there was any one particular thing that I didn’t like. It was more of a mixture of things. I did love my research, though. That was pretty clear to me. &lt;em&gt;But&lt;/em&gt; I wasn’t happy. So I spent a summer talking to as many people as possible in all different kinds of jobs, really just with the goal of figuring out what their day-to-day lives looked like. You were one of the people I spoke to, but I spoke to a ton of other people, as well.&lt;/p&gt;



&lt;p&gt;And from doing that, at the end of that summer, I ended up deciding to apply to industry jobs, and I applied to a bunch of places and got a bunch of offers. But I ended up deciding to join Microsoft Research New York City because of all the places I was considering going, they were the only place that said, “We love your research. We love what you do. Do you want to come here and do that same research?”&lt;/p&gt;



&lt;p&gt;And that was really appealing to me because I loved my research. Of course, I wanted to come there and do my same research and especially with all of these amazing people like you, Duncan Watts, who’d for many years been somebody I’d really looked up to. He was there, as well, at that point in time. There was this real focus on computational social science but with a little bit more of an industry perspective. There were also these amazing machine learning researchers. Just for many of the same reasons as you, I was just really excited to join that lab and particularly excited to be working in the same organization as you again.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, I’m happy to take at least a little bit of the credit for …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Oh yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; … recruiting you to Microsoft here many years ago.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Oh yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah. I was really excited to have you join, too, though I think the timing actually worked out so that I missed your first couple of months because I was on maternity leave with my first daughter at the time. I should say I’ve got two daughters, and I’m very proud to share in the context of this podcast that they’re both very interested in math and reading, as well.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah, they’re both great.&lt;/p&gt;



&lt;p&gt;Um, so then we ended up working in the same place. But despite that, it still took us several years to end up actually collaborating on research. Do you remember how we ended up working together?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah. So I used to tell this story a lot. Actually, I was at this panel on AI in society back in, I think, it was probably 2016. It was taking place in DC. And someone on this panel made this statement that soon our AI systems are just going to be so good that all of the uncertainty is going to be taken out of our decision-making, and something about this statement just, like, really set me off. I got so mad about it because I thought it was just …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; I remember.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; … such an irresponsible thing to be saying. So I came back to New York, and I think I was ranting to you about this in the lab, and this conversation ended up getting us started on this whole longer discussion about the importance of communicating uncertainty and about explaining the assumptions that are behind the predictions that you’re making and all of this.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; So this was something … I was really excited about this because this was something that had really been drummed into me for years as a Bayesian. So Bayesian statistics, which forms a lot of the foundation of the type of machine learning that I was doing, is all about explicitly stating assumptions and quantifying uncertainty. So I just felt super strongly about this stuff.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah. So somehow all of these discussions we were having led us to read up on this literature that was coming out of the machine learning community on interpretability at the time. There are a bunch of these papers coming out that were making claims about models being interpretable without stopping to define &lt;em&gt;who&lt;/em&gt; they were interpretable to or for what purpose. Never &lt;em&gt;actually&lt;/em&gt; taking these models and putting them down in front of real people. And we wanted to do something about this. So we started running controlled experiments with &lt;em&gt;real&lt;/em&gt; people and found that we often can’t trust our intuition about what makes a model interpretable.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah, one of the things that came up a lot in that work was, sort of, how to measure these squishy abstract human concepts, like &lt;em&gt;interpretability&lt;/em&gt;, that are really hard to define, let alone quantify and measure and stuff like that.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Absolutely. So I think one of the first things that we really struggled with in this line of work was what it even means to be &lt;em&gt;interpretable&lt;/em&gt; or &lt;em&gt;intelligible&lt;/em&gt; or any of these terms that were getting thrown around at the time.&lt;/p&gt;



&lt;p&gt;Um, we ended up doing some research, which is still one of my favorite papers, …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Me, too.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; … with our colleagues Forough Poursabzi, Jake Hofman, and Dan Goldstein. And in this work, we found it really useful to think about interpretability as a latent property that can be, kind of, influenced by different properties of a model or system’s design. So things like the number of features the model has or whether the model’s linear or even things like the user interface of the model.&lt;/p&gt;



&lt;p&gt;This was kind of a gateway project for me in the sense that it’s one of the first projects that I got really excited about that was more of a human-computer interaction, or HCI, project rather than a theory project like I’d been working on in the past. And it just set off this huge spark of excitement in me. It felt to me at the time more important than other things that I was doing, and I just wanted to do more and more of this work.&lt;/p&gt;



&lt;p&gt;I would say the other project that had a really similar effect on me, which we also worked on together right around the same time, was our work with Ken Holstein mapping out challenges that industry practitioners were facing in the space of AI fairness.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Oh yeah. OK, yep. That project, that was so fun, and I learned so much from it. If I recall correctly, we originally hired Ken, who I think was an HCI PhD student at CMU at the time, as an intern …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; … to work with us on creating, sort of, user experiences for fairness tools like the Fairlearn toolkit&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. And we started that project—so that was in collaboration with Miro Dudík and Hal Daumé—we started that project by having Ken talk to a whole bunch of practitioners at Microsoft but at other organizations, as well, to get a sense for how they were and weren’t using fairness toolkits like Fairlearn.&lt;/p&gt;



&lt;p&gt;And I want to point out that at that point in time, the academic research community was super focused on all of these, like, simple quantitative metrics for assessing the fairness in the context of predictions and predictive machine learning models with this, kind of, understanding that these tools could then be built to help practitioners assess the fairness of their predictive models and maybe even make fairer predictions. And so that’s the kind of stuff that this Fairlearn toolkit was originally developed to do. So we ended up asking all of these practitioners originally just as, sort of, the precursor to what we thought we were going to end up doing with this project.&lt;/p&gt;



&lt;p&gt;We also asked these practitioners about their current practices and challenges around fairness in their work and about their additional needs for support. So where did they feel like they had the right tools and processes and practices and where did they feel like they were missing stuff. And this was really eye-opening because what we found was so different than what we were expecting. And there’s two things that really stood out to us.&lt;/p&gt;



&lt;p&gt;So the first thing was that we found a much, much wider range of applications beyond prediction. So we’d come into this assuming that all these practitioners were doing stuff with predictive machine learning models, but in fact, we were finding they were doing all kinds of stuff. There was a bunch of unsupervised stuff; there was a bunch of, you know, language-based stuff—all of this kind of thing. And in hindsight, that probably doesn’t sound very surprising nowadays because of the rise of generative AI, and really the entire machine learning and AI field is much less focused on prediction in that, kind of, narrow, kind of, classification-regression kind of way. But at the time, this was really surprising, especially in light of the academic literature’s focus on predictions when thinking about fairness.&lt;/p&gt;



&lt;p&gt;The second thing that we found was that practitioners often struggled to use existing fairness research, in part because these quantitative metrics that were all the rage at that point in time, just weren’t really amenable to the types of real-world complex scenarios that these practitioners were facing. And there was a bunch of different reasons for this, but one of the things that really stood out to us was that this wasn’t so much about the underlying models and stuff like that, but it was actually that there were a variety of data challenges involved here around things like data collection, collection of sensitive attributes, which you need in order to actually use these fairness metrics.&lt;/p&gt;



&lt;p&gt;So putting all this together, the upshot of all this was that we never did what we originally set out to do with that [LAUGHS], that internship project. We … because we uncovered this really large gap between research and practice, we ended up publishing this paper that characterized this gap and then surfaced important directions for future research. The other thing that the paper did was emphasize the importance of doing this kind of qualitative work to actually understand what’s happening in practice rather than just making assumptions about what practitioners are and aren’t doing.&lt;/p&gt;



&lt;p&gt;The other thing that came out of it, of course, was that the four of us—so you, me, Miro and Hal—learned a ton about HCI and about qualitative research from Ken, which was just, uh, so fun.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, and I started to be confronted with the fact that I could no longer reasonably ignore all of these messes of the real world because, you know, in some ways, responsible AI is really all about the messes.&lt;/p&gt;



&lt;p&gt;So I think this project was really a big shift for both of us. And in some ways, working on this and the interpretability work really led us to be active in these early efforts that were happening within Microsoft in the responsible AI space. Um, the research that we were doing was feeding directly into company policy, and it felt like it was just, like, a huge place where we could have some impact. So it was very exciting.&lt;/p&gt;



&lt;p&gt;So switching gears a bit. Hanna, do you remember how we first got the idea for WiML?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yes, I do. So we were at NeurIPS. This was back in 2005. It was a … so NeurIPS was a very different conference back then. Now it’s like tens of thousands of people. It’s held in a massive convention center. Yes, there are researchers there, but there’s a variety of people from across the tech industry who attend, but that is &lt;em&gt;not&lt;/em&gt; what it was like back then.&lt;/p&gt;



&lt;p&gt;So in around … in 2005, it was more like 600 people thereabouts in total[1], and the main conference would be held every year in Vancouver, and then everybody at the conference would pile onto these buses, and we would all head up to Whistler for the workshops.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; So super different to what’s happening nowadays. It was my third time. I think that’s right. I think it was my third time attending the conference. But it was my first time sharing a hotel room with other women. And I remember up at the workshops, up in Whistler, there were five of us sitting around in a hotel room, and we were talking about how amazing it was that there were five of us sitting around talking, &lt;em&gt;women&lt;/em&gt;. And we, kind of, couldn’t believe there were five of us. We’re all PhD students at the time. And so we decided to make this list, and we started trying to figure out who the other women in machine learning were. And we came up with about 10 names, and we were kind of amazed that there were even 10 women in machine learning. We thought this was a &lt;em&gt;huge&lt;/em&gt; number. We were very excited. And we started talking about how it might be really fun to just bring them all together sometime.&lt;/p&gt;



&lt;p&gt;So we returned from NeurIPS, and you and I ended up getting lunch to strategize. I still remember walking out of the department together to go get lunch and you were walking ahead of me. I can visualize the coat you were wearing as you were walking in front of me. And so we strategized a bit and ended up deciding, along with one of the other women, Lisa Wainer, to submit a proposal to the Grace Hopper conference for a session in which women in machine learning would give short talks about their research.&lt;/p&gt;



&lt;p&gt;We reached out to the 10 names that we had written down in the hotel room and through that process actually ended up finding out about more women in machine learning and eventually had something like 25 women listed on the final proposal. I think there’s an email somewhere where one or another of us is saying to the other one, “Oh my gosh! I can’t believe there are so many women in machine learning.”&lt;/p&gt;



&lt;p&gt;So we submitted this proposal, and ultimately, the proposal was rejected by the Grace Hopper conference. But we were so excited about the idea and just really invested in it by that point that we decided to hold our own co-located event the day before the Grace Hopper conference. And I’ve got to say, you know, 20 years later, I don’t know &lt;em&gt;what&lt;/em&gt; we were thinking. Like, that was a bold move on the part of three PhD students. And it turned out to be a huge amount of work that we had to do entirely ourselves, as well.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; We had no idea what we were doing. But the Grace Hopper folks very nicely connected us with the venue that the conference was going to be held at, and somehow, we managed to pull it off. Ultimately, that first workshop had around 100 women, and there was this … rather than just, like, a single short session, which is what we’d originally had in mind, we had this full day’s worth of talks. I actually have the booklet of abstracts from all of those talks at my desk in the office. I still have that today. And it was just an amazing experience.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, it was. And, you know, you mentioned how bold we were. I just, I really don’t think that any of us at the time realized how bold we were being here, getting this workshop rejected and then saying, you know, &lt;em&gt;no&lt;/em&gt;, we think this is important. We’re going to do it anyway. &lt;em&gt;On our own. As grad students.&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;So I’ve already talked a little bit about some of the spaces that I was in throughout my career where there just weren’t a lot of women around in the room with me. How had you experienced a lack of community or network of women in machine learning before the founding of WiML? And, you know, why do you think it’s important to have that kind of community?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; So I felt it in a number of different ways. I think I mentioned a few minutes ago that, like, it was my third time at NeurIPS but my first time sharing a hotel room with another woman. But there were many places over the years where I’d felt this.&lt;/p&gt;



&lt;p&gt;So first, as an undergraduate. Then, I did a lot of free and open-source software development, and I was pretty involved in stuff to do with the Debian Linux distribution. And back then, the percentage of women involved in free and open-source software development was about 1 percent, 1.5%&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, and the percentage involved actually in Debian was even less than that. So that had led me and some others to start this Debian Women Project&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. And then, again, of course, I faced this in machine learning.&lt;/p&gt;



&lt;p&gt;I just didn’t know that many other women in machine learning. I didn’t … there weren’t a large number of senior women, for example, to look up to as role models. There weren’t a large number of female PhD students. And this, kind of, made me sad because I was really excited about machine learning, and I hoped to spend my entire career in it. But because I didn’t see so many other women around, particularly more senior women, that really made me question whether that would even be possible, and I just didn’t know.&lt;/p&gt;



&lt;p&gt;Um, I think, you know, thinking about this, and I’ve obviously reflected on this a lot over the years, but I think having a diverse community in any area, be it free and open-source software development, be it machine learning, any of these kinds of things, is just so important for so many reasons. And some of those reasons are little things like finding people that you would feel comfortable sharing a hotel room with.&lt;/p&gt;



&lt;p&gt;But many of these things are bigger things that can then have, like, even, kind of, knock-on cumulative effects. Like feeling valued in the community, feeling welcome in the community, having role models, being able to, sort of, see people and say, “Oh, I want to be kind of like that person when I grow up; I could do this.” And then even just representation of different perspectives in the work itself is so important.&lt;/p&gt;



&lt;p&gt;The flip side of that is that there are a whole bunch of things that can go wrong if you don’t have a diverse community. You can end up with gatekeeping, with toxic or unsafe cultures, obviously attrition as people just leave these kinds of spaces because they feel that they’re not welcome there and won’t be valued there. And then to that point of having representation of different perspectives, with a really homogeneous community, you can end up with, kind of, blind spots around the technology itself, which can then lead to harms.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; 100%. So did you ever imagine during&amp;nbsp;all of&amp;nbsp;this that WiML would still be around 20 years later and we would be sitting here on a podcast talking about this?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; [LAUGHS] No, absolutely not. I didn’t even think that WiML would necessarily be around for a second year. I thought it was probably going to be, like, a one-off event. And I certainly don’t think that I thought that I would still be involved in the machine learning community 20 years later, as well. So very unexpected.&lt;/p&gt;



&lt;p&gt;I’ve got a question for you, though. What do you remember most about that first workshop?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; I remember a lot of things. I remember that, you know, when we were planning this, we always really wanted the focus to be the research. And, you know, if you think back to what this first workshop looked like, it was a lot of us just giving talks or presenting posters about our own research to other people.&lt;/p&gt;



&lt;p&gt;And, you know, I remember thinking at the poster session, like, the vibe was just so much different and better, healthier really than other poster sessions I had been to. Everyone was so supportive and encouraging, but it really was all about the research. I also remember being blown away just walking into that conference room in the morning and seeing all of these women gathered in one place and knowing that somehow, we had actually made this happen.&lt;/p&gt;



&lt;p&gt;Um, I remember we also faced some challenges with the workshop early on. What are the challenges that stand out to you most?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah, so a lot of people really got it, right. And they were super supportive. So, for example, folks at Penn totally got it, and they actually funded a bunch of that first workshop. But others in the community didn’t get it and didn’t see the point, didn’t see why it was necessary.&lt;/p&gt;



&lt;p&gt;I remember having dinner with one machine learning researcher and him telling me that he didn’t think this kind of workshop was necessary because women’s experiences were no different to men’s experiences. And then later on in the conversation, he talked about—like, you know, this is, like, an hour and a half later or something—he talked about how he and a friend of his had gone to the bar at an all-women’s college and he’d felt so awkward and out of place. And I ended up pointing out to him [LAUGHS] that he just, kind of, explained to himself why we needed WiML. So, yeah, there were some people who didn’t get it, and it took a lot of, sort of, talking to people and, kind of, explaining.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Another challenge was figuring out how to fund it in an ongoing manner once we decided that we wanted to do this more than once.&lt;/p&gt;



&lt;p&gt;So as I said, Penn funded a lot of that first workshop, but that wasn’t a sustainable model, and it wasn’t going to be realistic for Penn to keep funding it. So in the end, we worked with Amy Greenwald to obtain a National Science Foundation grant that would cover a lot of costs, and we also received donations from other organizations.&lt;/p&gt;



&lt;p&gt;Um, a third challenge was figuring out where to hold the workshop given that we did want that focus to be on research. So the first two times, we held the workshop at the Grace Hopper conference, but we started to feel that that wasn’t really the right venue given that we wanted that focus to be on research. So we ended up moving it to NeurIPS, and this had a bunch of benefits, some of which I don’t think we’d even fully thought through when we made that decision.&lt;/p&gt;



&lt;p&gt;So one of the benefits was that attendees’ WiML travel funding—so we would give them this travel funding to enable them to pay the cost of attending WiML, stay in hotel rooms, all this kind of stuff—this would actually enable them to attend NeurIPS, as well, if we co-located with NeurIPS.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Another main benefit was that we held WiML on the day before NeurIPS. So then throughout the rest of the conference, WiML attendees would see familiar faces throughout the crowd and wouldn’t necessarily feel so alone.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; So you’re talking about these challenges. How have these challenges changed over time? Or, you know, more broadly, can you talk about how the workshop and Women in Machine Learning as an organization as a whole, kind of, evolved over the years? I know that you served a term as the WiML president.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah. So it’s changed a lot. So first, obviously, most importantly, it evolved from being, kind of, this one-off event where we were just seeing what would happen to being really a robust organization. And the first step in that was creating the WiML board. And, as you just said, I served as the first president of that.&lt;/p&gt;



&lt;p&gt;But there have been a bunch of other steps since then. And one of the things I want to flag about the WiML board was that this was really important because the board members could focus on the long-term health of the organization and these, sort of, like, you know, things that spanned multiple years, like how to get sustainable funding sources, this kind of thing, versus the actual workshop organizers, who would focus on things like running the call for submissions and stuff like that. And being able to separate those roles made it really just reduce the burden on the workshop organizers meant that we could take this, kind of, longer-term perspective.&lt;/p&gt;



&lt;p&gt;Another really important step was becoming, &lt;em&gt;officially&lt;/em&gt; becoming a non-profit. So that happened a few years ago. And again, it was the natural thing to do at that point in time and just another step towards creating this, sort of, durable, robust organization.&lt;/p&gt;



&lt;p&gt;But it’s really taken on a life of its own. I’m honestly not super actively involved nowadays, which I think is fantastic. The organization doesn’t need me. That’s great. It’s also wild to me that because it’s been around for 20 years at this point that there are women in the field who don’t know what it’s like to &lt;em&gt;not&lt;/em&gt; have WiML.&lt;/p&gt;



&lt;p&gt;So a bunch of other affinity groups got created. So Timnit Gebru cofounded Black in AI when she was actually a postdoc at Microsoft Research New York City. So you and I got to actually see the founding of that affinity group up close. And then now there are a ton of other affinity groups. So there’s LatinX in AI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;; there’s Queer in AI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Muslims in ML&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Indigenous in AI and ML&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, New In ML&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, just to name a few.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, and all of these are growing, too, every year.&lt;/p&gt;



&lt;p&gt;You know, this year, WiML had over 400 submissions. They accepted 250 to be presented. It’s amazing.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; That’s wild.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, yep. And there’s going to be a WiML presence this year actually at all three of the NeurIPS venues. So there’s going to be a presence in Mexico City, in Copenhagen, and, of course, in San Diego for the main workshop. So it’s pretty great.&lt;/p&gt;



&lt;p&gt;And, you know, on top of that, I think the organization now, as you were saying, is able to do so much more than just the workshop alone. So for instance, WiML now runs this worldwide mentorship program for women and nonbinary individuals in machine learning, where they’re matched with a mentor and they can participate in these one-to-one mentoring meetings and seminars and panel discussions, which happens all throughout the year. I think they have about 50 mentors signing up each year, but I’m sure they could always use more. Um, so it’s just really amazing to look back and see how much the WiML community has done and how much it’s grown.&lt;/p&gt;



&lt;p&gt;And, you know, on the one hand, I think that honestly, like, founding WiML was one of the things that I’ve done over the course of my career, if not &lt;em&gt;the&lt;/em&gt; thing, that I am most proud of …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Oh yeah, me, too.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; … to this day, but at the same time, like, we can’t take credit for any of this. It’s, like, a community effort.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; No.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; It’s the community that has really kept us going …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yes.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; … for the last 20 years,&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yes.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; … so it’s great. Going to stop gushing now, but it’s amazing.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; And it’s not just WiML that’s changed over the years. The entire industry has changed a ton, as well.&lt;/p&gt;



&lt;p&gt;How has your research evolved as a result of these changes to the entire field of AI and machine learning and also from your own change from academia to industry?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; It’s a great question. You know, we’ve touched on this a little bit, but our research paths really evolved differently but ended up in these very similar places where we’re working on responsible AI, we’re advocating for interdisciplinary approaches, incorporating techniques from HCI, and so on. And I think that part of this was because of shifts of the community and also what’s happening in industry. Working in responsible AI in industry, there’s definitely not ever a shortage of interesting problems to solve, right.&lt;/p&gt;



&lt;p&gt;And I think that for both of us, our research interests in recent years really have been driven by these really practical challenges that we’re seeing. We were both involved early on in defining what responsible AI means within Microsoft, shaping our internal Responsible AI Standard&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. I led this internal companywide working group on AI transparency, which was focused both on model interpretability like we were talking about earlier but also other forms of transparency like data sheets for datasets and the transparency notes that Microsoft now releases with all of our products. And at the same time, you are leading this internal working group on fairness.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah, taking on that internal working group was, kind of, a big transition point in my career. You know, when I joined Microsoft, I was focusing on computational social science and I was also entirely doing research and wasn’t really that involved in stuff in the rest of the company.&lt;/p&gt;



&lt;p&gt;Then at the end of my first year at Microsoft, I attended the first Fairness, Accountability, and Transparency in Machine Learning workshop&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, which was co-located with NeurIPS. It was one of the NeurIPS workshops. And I got really excited about that and thought, great, I’m going to spend like 20% of my time, maybe one day a week, doing research on topics in the space of fairness and accountability and transparency. Um, that is not what ended up happening.&lt;/p&gt;



&lt;p&gt;Over the next couple of years, I ended up doing more and more research on responsible AI, you know, as you said, on topics to do with fairness, to do with interpretability. And then in early 2018, I was asked to co-chair this internal working group on fairness, and that was the point where I started getting much more involved in responsible AI stuff across Microsoft, so outside of just Microsoft Research.&lt;/p&gt;



&lt;p&gt;And this was really exciting to me because responsible AI was so new, which meant that research had a really big role to play. It wasn’t like this was kind of an established area where folks in engineering and policy knew exactly what they were doing. And so that meant that I got to branch out from this very, sort of, research-focused work into much more applied work in collaboration with folks from policy, from engineering, and so on.&lt;/p&gt;



&lt;p&gt;Now, in fact, as well as being a researcher, I actually run a small applied science team, the Sociotechnical Alignment Center, or STAC for short, within Microsoft Research that focuses specifically on bridging research and practice in responsible AI.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah. Do you think that your involvement in WiML has played a role in this work?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yes, definitely. [LAUGHS] Yeah, without a doubt. So particularly when working on topics related to fairness, I’ve ended up focusing a bunch on stuff to do with marginalized groups as part of my responsible AI work.&lt;/p&gt;



&lt;p&gt;So there’s been this, sort of, you know, focus on marginalized groups, particularly women, in the context of machine learning and with my WiML, kind of, work and then in my research work thinking about fairness, as well.&lt;/p&gt;



&lt;p&gt;The other way that WiML has really, sort of, affected what I do is that I work with a much more varied group of people nowadays than I did back when I was just focusing on, kind of, machine learning, computational social science, and stuff like that. And many of my collaborators are people that I’ve met through WiML over the years.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; And, of course, there has been another big shift within industry recently, which is just all the excitement around generative AI. Can you say a bit about how that has changed your research?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; OK, yeah. So this is another big one. There are so many ways that this changed my work. One of the biggest ways, though, is that generative AI systems are now everywhere. They’re being used all over the place for all kinds of things. And, you know, you see all these news headlines about GenAI systems, you know, diagnosing illnesses, solving math problems, and writing code, stuff like that. And also headlines about various different risks that can occur when you’re using generative AI. So fabricating facts, memorizing copyrighted data, generating harmful content, you know, these kinds of things. And with all this attention, it’s really natural to ask, what is the evidence behind these claims? So where is this evidence coming from, and should we trust it?&lt;/p&gt;



&lt;p&gt;It turns out that much of the evidence comes from GenAI evaluations that involve measuring the capabilities, the behaviors, and the impacts of GenAI systems, but the current evaluation practices that are often used in the space don’t really have as much scientific rigor as we would like, and that’s, kind of, a problem.&lt;/p&gt;



&lt;p&gt;So one of the biggest challenges is that the concepts of interest when people are, sort of, doing these GenAI evaluations—so things like diagnostic ability, memorization, harmful content, concepts like that—are much more abstract than the concepts like prediction accuracy that underpinned machine learning evaluations before the generative AI era.&lt;/p&gt;



&lt;p&gt;And when we look at these new concepts that we need to be able to focus on in order to evaluate GenAI systems, we see that they’re actually much more reminiscent of these abstract contested concepts—these, kind of, fuzzy, squishy concepts—that are studied in the social sciences. So things like democracy and political science or personality traits and psychometrics. So there’s really that, sort of, connection there to these, kind of, squishier things.&lt;/p&gt;



&lt;p&gt;So when I was focusing primarily on computational social science, most of my work was focused on developing machine learning methods to help social scientists measure abstract contested concepts. So then when GenAI started to be a big thing and I saw all of these evaluative claims involving measurements of abstract concepts, it seemed super clear to me that if we were going to actually be able to make meaningful claims about what AI can do and can’t do, we’re going to need to take a different approach to GenAI evaluation.&lt;/p&gt;



&lt;p&gt;And so I ended up, sort of, drawing on my computational social science work around measurement and I started advocating for adopting a variant of the framework that social scientists use for measuring abstract contested concepts. And my reason for doing this was that I believed—I &lt;em&gt;still&lt;/em&gt; believe—that this is an important way to improve the scientific rigor of GenAI evaluations.&lt;/p&gt;



&lt;p&gt;You know all of this, of course, because you and I, along with a &lt;em&gt;bunch&lt;/em&gt; of other collaborators at Microsoft Research and Stanford and the University of Michigan published a position paper on this framework entitled “Evaluating GenAI Systems is a Social Science Measurement Challenge” at ICML [International Conference on Machine Learning] this past summer.&lt;/p&gt;



&lt;p&gt;What are you excited about at the moment?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, so lately, I have been spending a lot of time thinking about AI and critical thought: how can we design AI systems to support appropriate reliance, preserve human agency, and really encourage critical engagement on the part of the human, right?&lt;/p&gt;



&lt;p&gt;So this is an area where I think we actually have a huge opportunity, but there are also huge risks. If I think about my most optimistic possible vision of the future of AI —which is not something that is easy for me to do, as I’m not a natural optimist, as you know—it would be a future in which AI helps people grow and flourish, in which it, kind of, enriches our own &lt;em&gt;human&lt;/em&gt; capabilities. It deepens our own &lt;em&gt;human&lt;/em&gt; thinking and safeguards our own agency.&lt;/p&gt;



&lt;p&gt;So in this future, you know, we could build AI systems that actually help us brainstorm and learn new knowledge and skills, both in formal educational settings and in our day-to-day work, as well. But I think we’re not going to achieve this future by default. It’s something that we really need to design for if we want to get there.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; You mentioned that there are risks. What are the risks that you can see here?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, there’s so much at stake here. You know, in the short term, there are things like overreliance—depending on the output of an AI system even when the system’s wrong. This is something that I’ve worked on a bunch myself. There’s a risk of loss of agency or the ability to make and execute independent decisions and to ensure that our outcomes of AI systems are aligned with personal or professional values of the humans who are using those systems. This is something that I’ve been looking at recently in the context of AI tools for journalism&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. There’s diminished innovation, by which I mean a loss of creativity or diversity of ideas.&lt;/p&gt;



&lt;p&gt;You know, longer term, we risk atrophied skills—people just losing or simply never developing helpful skills for their career or their life because of prolonged use of AI systems. The famous example that people often bring up here is pilots losing the ability to perform certain actions in flight because of dependence on autopilot systems. And I think we’re already starting to see the same sort of thing happen across all sorts of fields because of AI.&lt;/p&gt;



&lt;p&gt;And, you know, finally, another risk that I’ll mention that seems to resonate with a lot of folks I talk to is what I would just call loss of joy, right. What happens when we are delegating to AI systems the parts of our activities that we really take pleasure and find this satisfaction in doing ourselves.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; So then as a community, what should we be doing if we’re worried about these risks?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, I mean, I think this is going to have to be a big community effort if we want to achieve this. This is a big goal. But there are a few places I think we especially need work.&lt;/p&gt;



&lt;p&gt;So I think we need generalized principles and practices for AI system builders for how they can build AI systems in ways that promote human agency and encourage critical thought. We also need principles and practices for system &lt;em&gt;users&lt;/em&gt;. So how do we teach the general population to use AI in ways that amplify their skills and capabilities and help them learn new things?&lt;/p&gt;



&lt;p&gt;And then, you know, close to your heart, I’m sure, I think that we need more work on measurement and evaluation, right. We are once again back to these squishy human properties.&lt;/p&gt;



&lt;p&gt;You know, I mentioned I’ve done some work on overreliance in generative AI systems, and I started there because on the grand scale of risks here, overreliance is something that is relatively easy to measure, at least in the short term. But how do we start thinking about measuring people’s critical thinking when using AI across all sorts of contexts and at scale and over long-time horizons, right? How do we measure the, sort of, longitudinal effect of AI systems just on our critical thought as a population?&lt;/p&gt;



&lt;p&gt;And by the way, if anyone listening is going to be at the WiML workshop, I’ll actually be giving a keynote on this topic. And this is something I’m just incredibly excited about because first, I’m incredibly excited about this topic, but also, in the whole 20 years of WiML, I’ve given opening remarks and similar several times, but this is actually the very first time that I will be talking about my own research there. So this is like my dream. I’m thrilled that this is happening.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; That’s awesome. Oh, that’s so exciting. Excellent.&lt;/p&gt;



&lt;p&gt;So one last question for you. If you could go back and talk to yourself 20 years ago and give yourself some advice, what would you say?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, OK, I’ve thought about this one a bit over the past week, and there are three things here I want to mention.&lt;/p&gt;



&lt;p&gt;So first, I would tell myself to be brave about speaking up. You know I’m about as introverted as it gets and I’m naturally very shy, and this has always held me back. It still holds me back now. It was really embarrassingly late in my career that I decided to do something about this and start to develop strategies to help myself speak up more. And eventually, it started to grow into something that’s a little bit more natural.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; What kind of, um, what kind of strategies?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, so you know, one example is I use a lot of notes. For this podcast, I have a lot of notes here. I’m a big notes person, and things like that really help me.&lt;/p&gt;



&lt;p&gt;The second thing that I would tell myself is to, you know, work on the problems that you really want to see solved. As researchers, we have this amazing freedom to choose our own direction. And early on, you know, a lot of the problems that I worked on were problems that I really enjoyed thinking about on a day-to-day basis. It was a lot of fun. They were like little math puzzles to me. But I often found that, you know, when I would be at conferences and people would ask me about my work, I didn’t really want to talk about these problems. I just in some sense, you know, I had fun doing it, but I didn’t really care. I wasn’t passionate about it. I didn’t care that I had solved the problem.&lt;/p&gt;



&lt;p&gt;And so once, many years ago now, when I was thinking about my research agenda, I got some good advice from our former lab director, Jennifer Chayes, who suggested that I go through my recent projects and sort them into projects where I really &lt;em&gt;liked&lt;/em&gt; working on them—it was a fun experience day-to-day—and projects that I liked talking about after the fact and, kind of, felt good about the results and then see where the overlap is. And this is something that, like, it kind of sounds, kind of, obvious when I say it now, but at the time, it was really eye-opening for me.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; That’s so cool. And now I, kind of, want to do that with all of my projects, particularly at the moment. I actually just took five months, as you know, five months off of work for parental leave because I just had a baby. And so I’m, sort of, taking a big, kind of, inventory of everything as I get back into all of this now, and I love this idea. I think this is really cool.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; It’s changed really my whole approach to research. Like, you know, we were talking about this, but most of the work I do now is more HCI than machine learning because I found that the problems that really motivate me, that I want to be talking to people about at conferences, are the &lt;em&gt;people&lt;/em&gt; problems.&lt;/p&gt;



&lt;p&gt;The third piece of advice I would give myself is that you should bring more people into your work, right.&lt;/p&gt;



&lt;p&gt;So there’s this kind of vision on the outside of research being this solo endeavor, and it can feel so competitive at times, right. We all feel this. But time and time again, I’ve seen that the best research comes from collaborations and from bringing people together with diverse perspectives who can challenge each other in a way that is respectful but makes the work better.&lt;/p&gt;



&lt;p&gt;Is there advice that you would give to your former self of 20 years ago?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah. OK. So I’ve also been thinking about this a bunch over the past week. There’s actually a lot of advice I think I would give my former self, [LAUGHS] but there are three things that I keep coming back to.&lt;/p&gt;



&lt;p&gt;OK, so first—and this is similar to your second point—push for doing the work that you find to be most fulfilling even if that means taking a nontraditional path. So in my case, I’ve always been interested in the social sciences. Back when I was a student, you know, even when I was a PhD student, doing research that combined computer science &lt;em&gt;and&lt;/em&gt; the social sciences just wasn’t really a thing. And so as a result, it would have been really easy for me to just be like, “Oh well, I guess that isn’t possible. I’ll just focus on traditional computer science problems.”&lt;/p&gt;



&lt;p&gt;But that’s not what I ended up doing. Instead, and often in ways that made my career, kind of, harder than it probably would have been otherwise, I ended up pushing. I kept pushing, and in fact, I keep pushing, even nowadays, to bring these things together—computer science and the social sciences—in an interdisciplinary fashion. And this hasn’t been easy. But cumulatively, the effect has been that I’ve been able to do much more impactful work than I think I would have been able to do otherwise, and the work I’ve done, I’ve just enjoyed so much more than would otherwise have been the case.&lt;/p&gt;



&lt;p&gt;OK, so second, be brave and share your work. So this is actually advice for my current self and my former self, as this is something that I definitely still struggle with.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; As do I, you know, and actually, I think it’s funny to hear you say this because I would say that you are much better at this than I am.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; I still, I think I have a lot of work to do on this one. Yeah, it’s hard. It’s really hard.&lt;/p&gt;



&lt;p&gt;As you know, I am a perfectionist, and this is good in some ways, but this is also bad in other ways. And one way in which this is bad is that I tend to be really anxious about sharing and publicizing my work, especially when I feel it’s not perfect.&lt;/p&gt;



&lt;p&gt;So as an example, I wrote this massive tutorial on computational social science for ICML in 2015, but I never put the slides … and I wrote a whole script for it … I never put the slides or the script online as a resource for others because I felt it needed more work. And I actually went back and looked at it earlier this year, when we were working on the ICML paper, and I was stunned because it’s great. Why didn’t I put this online? All these things that I thought were problems 10 years ago, no, they’re not a big deal. I should have just shared it.&lt;/p&gt;



&lt;p&gt;As another example, STAC, my applied science team, was using LLMs as part of our approach to GenAI evaluation back in 2022, way before the sort of “LLM-as-a-judge” paradigm was widespread. But I was really worried that others would think negatively of us for doing this, so we didn’t share that much about what we were doing, and I regret that because we missed out on an opportunity to kick off an industrywide discussion about this, sort of, LLM-as-a-judge paradigm.&lt;/p&gt;



&lt;p&gt;OK, so then my third point is that the social side of research is just as valuable as the technical side. And by this, I’m actually not talking about social science and computer science. I actually mean that the &lt;em&gt;how&lt;/em&gt; of doing research, including &lt;em&gt;who&lt;/em&gt; you talk to, &lt;em&gt;who&lt;/em&gt; you collaborate with, and &lt;em&gt;how&lt;/em&gt; you approach those interactions, is just as important as the research itself.&lt;/p&gt;



&lt;p&gt;As a PhD student, I felt really bad about spending time socializing with other researchers, especially at conferences, because I thought that I was supposed to be listening to talks, reading papers, and discussing technical topics with researchers and not socializing. But in hindsight, I think that was wrong. Many of those social connections have ended up being incredibly valuable to my research, both because I’ve ended up collaborating with and in some cases even hiring the people who I first got to know socially …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; … but also because the friendships that I’ve built, like our friendship, for example, have served as a crucial support network over the years, especially when things have felt particularly challenging.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, absolutely. I agree with all of that so much.&lt;/p&gt;



&lt;p&gt;And with that, I will say thank you so much for doing this podcast with me today.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Thank &lt;em&gt;you&lt;/em&gt;.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; It was a lot of fun to reflect on the last 20 years of WiML, but also the last 20 years of our careers and friendship and all of this, so it’s great, and I never would have agreed to do this if it had been with anyone but you.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Likewise. [LAUGHS]&lt;/p&gt;



&lt;p&gt;So thank you, everybody, for listening to us, and hopefully some of you will join for the 20th annual workshop for Women in Machine Learning&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, which is taking place on Dec. 2. And of course, Jenn and I will both be there in person. We’ll also be at NeurIPS afterwards. So feel free to reach out to us if you want to chat with us or to learn more about anything that we covered here today.&lt;/p&gt;



&lt;p&gt;[MUSIC]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;OUTRO:&lt;/strong&gt; You’ve been listening to &lt;em&gt;Ideas&lt;/em&gt;, a Microsoft Research Podcast. Find more episodes of the podcast at aka.ms/researchpodcast&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&lt;/p&gt;

				&lt;/span&gt;
			&lt;/div&gt;
			&lt;button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button"&gt;
				Show more			&lt;/button&gt;
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;p&gt;[1] Wallach later clarified that the number of registrants for the 2005 Conference on Neural Information Processing Systems was around 900.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/podcast/ideas-community-building-machine-learning-and-the-future-of-ai/</guid><pubDate>Mon, 01 Dec 2025 19:18:20 +0000</pubDate></item><item><title>Driving American battery innovation forward (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/driving-american-battery-innovation-forward-1201</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/MITei-Kurt-Kelty.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Advancements in battery innovation are transforming both mobility and energy systems alike, according to Kurt Kelty, vice president of battery, propulsion, and sustainability at General Motors (GM). At the MIT Energy Initiative (MITEI) Fall Colloquium, Kelty explored how GM is bringing next-generation battery technologies from lab to commercialization, driving American battery innovation forward. The colloquium is part of the ongoing MITEI Presents: Advancing the Energy Transition speaker series.&lt;/p&gt;&lt;p&gt;At GM, Kelty’s team is primarily focused on three things: first, improving affordability to get more electric vehicles (EVs) on the road. “How do you drive down the cost?” Kelty asked the audience. “It's the batteries. The batteries make up about 30 percent of the cost of the vehicle.” Second, his team strives to improve battery performance, including charging speed and energy density. Third, they are working on localizing the supply chain. “We've got to build up our resilience and our independence here in North America, so we're not relying on materials coming from China,” Kelty explained.&lt;/p&gt;&lt;p&gt;To aid their efforts, resources are being poured into the virtualization space, significantly cutting down on time dedicated to research and development. Now, Kelty’s team can do modeling up front using artificial intelligence, reducing what previously would have taken months to a couple of days.&lt;/p&gt;&lt;p&gt;“If you want to modify … the nickel content ever so slightly, we can very quickly model: ‘OK, how’s that going to affect the energy density? The safety? How’s that going to affect the charge capability?’” said Kelty. “We can look at that at the cell level, then the pack level, then the vehicle level.”&lt;/p&gt;&lt;p&gt;Kelty revealed that they have found a solution that addresses affordability, accessibility, and commercialization: lithium manganese-rich (LMR) batteries. Previously, the industry looked to reduce costs by lowering the amount of cobalt in batteries by adding greater amounts of nickel. These high-nickel batteries are in most cars on the road in the United States due to their high range. LMR batteries, though, take things a step further by reducing the amount of nickel and adding more manganese, which drives the cost of batteries down even further while maintaining range.&lt;/p&gt;&lt;p&gt;Lithium-iron-phosphate (LFP) batteries are the chemistry of choice in China, known for low cost, high cycle life, and high safety. With LMR batteries, the cost is comparable to LFP with a range that is closer to high-nickel. “That’s what’s really a breakthrough,” said Kelty.&lt;/p&gt;&lt;p&gt;LMR batteries are not new, but there have been challenges to adopting them, according to Kelty. “People knew about it, but they didn’t know how to commercialize it. They didn’t know how to make it work in an EV,” he explained. Now that GM has figured out commercialization, they will be the first to market these batteries in their EVs in 2028.&lt;/p&gt;&lt;p&gt;Kelty also expressed excitement over the use of vehicle-to-grid technologies in the future. Using a bidirectional charger with a two-way flow of energy, EVs could charge, but also send power from their batteries back to the electrical grid. This would allow customers to charge “their vehicles at night when the electricity prices are really low, and they can discharge it during the day when electricity rates are really high,” he said.&lt;/p&gt;&lt;p&gt;In addition to working in the transportation sector, GM is exploring ways to extend their battery expertise into applications in grid-scale energy storage. “It’s a big market right now, but it’s growing very quickly because of the data center growth,” said Kelty.&lt;/p&gt;&lt;p&gt;When looking to the future of battery manufacturing and EVs in the United States, Kelty remains optimistic: “we’ve got the technology here to make it happen. We’ve always had the innovation here. Now, we’re getting more and more of the manufacturing. We’re getting that all together. We’ve got just tremendous opportunity here that I’m hopeful we’re going to be able to take advantage of and really build a massive battery industry here.”&lt;/p&gt;&lt;p&gt;This speaker series highlights energy experts and leaders at the forefront of the scientific, technological, and policy solutions needed to transform our energy systems. Visit&amp;nbsp;MITEI’s Events page for more information on this and additional events.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/MITei-Kurt-Kelty.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Advancements in battery innovation are transforming both mobility and energy systems alike, according to Kurt Kelty, vice president of battery, propulsion, and sustainability at General Motors (GM). At the MIT Energy Initiative (MITEI) Fall Colloquium, Kelty explored how GM is bringing next-generation battery technologies from lab to commercialization, driving American battery innovation forward. The colloquium is part of the ongoing MITEI Presents: Advancing the Energy Transition speaker series.&lt;/p&gt;&lt;p&gt;At GM, Kelty’s team is primarily focused on three things: first, improving affordability to get more electric vehicles (EVs) on the road. “How do you drive down the cost?” Kelty asked the audience. “It's the batteries. The batteries make up about 30 percent of the cost of the vehicle.” Second, his team strives to improve battery performance, including charging speed and energy density. Third, they are working on localizing the supply chain. “We've got to build up our resilience and our independence here in North America, so we're not relying on materials coming from China,” Kelty explained.&lt;/p&gt;&lt;p&gt;To aid their efforts, resources are being poured into the virtualization space, significantly cutting down on time dedicated to research and development. Now, Kelty’s team can do modeling up front using artificial intelligence, reducing what previously would have taken months to a couple of days.&lt;/p&gt;&lt;p&gt;“If you want to modify … the nickel content ever so slightly, we can very quickly model: ‘OK, how’s that going to affect the energy density? The safety? How’s that going to affect the charge capability?’” said Kelty. “We can look at that at the cell level, then the pack level, then the vehicle level.”&lt;/p&gt;&lt;p&gt;Kelty revealed that they have found a solution that addresses affordability, accessibility, and commercialization: lithium manganese-rich (LMR) batteries. Previously, the industry looked to reduce costs by lowering the amount of cobalt in batteries by adding greater amounts of nickel. These high-nickel batteries are in most cars on the road in the United States due to their high range. LMR batteries, though, take things a step further by reducing the amount of nickel and adding more manganese, which drives the cost of batteries down even further while maintaining range.&lt;/p&gt;&lt;p&gt;Lithium-iron-phosphate (LFP) batteries are the chemistry of choice in China, known for low cost, high cycle life, and high safety. With LMR batteries, the cost is comparable to LFP with a range that is closer to high-nickel. “That’s what’s really a breakthrough,” said Kelty.&lt;/p&gt;&lt;p&gt;LMR batteries are not new, but there have been challenges to adopting them, according to Kelty. “People knew about it, but they didn’t know how to commercialize it. They didn’t know how to make it work in an EV,” he explained. Now that GM has figured out commercialization, they will be the first to market these batteries in their EVs in 2028.&lt;/p&gt;&lt;p&gt;Kelty also expressed excitement over the use of vehicle-to-grid technologies in the future. Using a bidirectional charger with a two-way flow of energy, EVs could charge, but also send power from their batteries back to the electrical grid. This would allow customers to charge “their vehicles at night when the electricity prices are really low, and they can discharge it during the day when electricity rates are really high,” he said.&lt;/p&gt;&lt;p&gt;In addition to working in the transportation sector, GM is exploring ways to extend their battery expertise into applications in grid-scale energy storage. “It’s a big market right now, but it’s growing very quickly because of the data center growth,” said Kelty.&lt;/p&gt;&lt;p&gt;When looking to the future of battery manufacturing and EVs in the United States, Kelty remains optimistic: “we’ve got the technology here to make it happen. We’ve always had the innovation here. Now, we’re getting more and more of the manufacturing. We’re getting that all together. We’ve got just tremendous opportunity here that I’m hopeful we’re going to be able to take advantage of and really build a massive battery industry here.”&lt;/p&gt;&lt;p&gt;This speaker series highlights energy experts and leaders at the forefront of the scientific, technological, and policy solutions needed to transform our energy systems. Visit&amp;nbsp;MITEI’s Events page for more information on this and additional events.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/driving-american-battery-innovation-forward-1201</guid><pubDate>Mon, 01 Dec 2025 20:50:00 +0000</pubDate></item><item><title>AWS re:Invent 2025: How to watch and follow along live (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/01/aws-reinvent-2025-how-to-watch-and-follow-along-live/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/12/IMG_3817.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon Web Services’ big annual event, re:Invent 2025, is getting into full swing in Las Vegas this week. Last year’s event was largely focused on their AI efforts, including new foundation models, services tackling AI hallucinations, and new security measures. And this year is likely to follow suit, based on what Amazon has announced so far and the lineup of speakers and programming that you can explore in more detail below.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The event formally kicks off tomorrow, December 2, at 9 a.m. PT. You can expect editorial coverage of the keynotes, interviews with AWS execs, and a roundup of news as the week’s programming rolls out, all of which you’ll be able to find on our site or by just keeping tabs on our AWS re:Invent tag.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AWS is partnering with companies like TechCrunch to highlight a sponsored showcase of their AWS OnAir programming in particular, which kicked off today to preview the event and highlight some early reveals:&lt;/p&gt;



&lt;!-- Add a placeholder for the Twitch embed --&gt;





&lt;!-- Load the Twitch embed script --&gt;



&lt;!-- Create a Twitch.Player object. This will render within the placeholder div --&gt;



&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s become the norm for events like re:Invent to broadcast keynotes and select programming — we just did something similar for TechCrunch Disrupt 2025 — and you can check out a breadth of streams with no ticket necessary below:&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-aws-re-invent-keynote-addresses"&gt;AWS re:Invent keynote addresses&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;This year’s re:Invent features five keynotes from Tuesday through Thursday, with, of course, a clear focus on AI. You can follow along with each of those keynotes via the scheduled livestreams below. Or you can just go to Fortnite (yes, really) and watch their five keynotes live there.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-aws-ceo-matt-garman-december-2-at-8-a-m-pt"&gt;AWS CEO Matt Garman: December 2 at 8 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-vp-of-agentic-ai-swami-sivasubramanian-december-3-at-8-30-a-m-pt"&gt;AWS VP of Agentic AI Swami Sivasubramanian: December 3 at 8:30 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-vp-of-global-specialists-and-partners-aws-dr-ruba-borno-december-3-at-3-p-m-pt"&gt;VP of Global Specialists and Partners, AWS Dr. Ruba Borno: December 3 at 3 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-svp-of-utility-computing-peter-desantis-vp-of-compute-and-machine-learning-services-aws-december-4-at-9-a-m-pt"&gt;SVP of Utility Computing Peter DeSantis, VP of Compute and Machine Learning Services, AWS: December 4 at 9 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-cto-of-amazon-com-dr-werner-vogels-december-4-at-3-30-p-m-pt"&gt;CTO of Amazon.com Dr. Werner Vogels: December 4 at 3:30 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-aws-re-invent-partner-showcases"&gt;AWS re:Invent partner showcases&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;A series of industry-specific partner showcases will also be livestreamed for those not on the ground in Las Vegas, with a series of streams that extend throughout the run of the event.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h3 class="wp-block-heading" id="h-aws-security-day-one-december-2-at-10-a-m-pt"&gt;AWS Security, Day One: December 2 at 10 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-ai-day-one-december-2-at-2-p-m-pt"&gt;AWS AI, Day One: December 2 at 2 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-ai-day-two-december-3-at-10-a-m-pt"&gt;AWS AI, Day Two: December 3 at 10 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-security-day-two-december-3-at-4-30-p-m-pt"&gt;AWS Security, Day Two: December 3 at 4:30 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-security-day-three-december-4-at-10-00-a-m-pt"&gt;AWS Security, Day Three: December 4 at 10:00 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-ai-day-three-december-4-at-11-35-a-m-pt"&gt;AWS AI, Day Three: December 4 at 11:35 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-industries-december-4-at-1-40-p-m-pt"&gt;AWS Industries: December 4 at 1:40 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/12/IMG_3817.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon Web Services’ big annual event, re:Invent 2025, is getting into full swing in Las Vegas this week. Last year’s event was largely focused on their AI efforts, including new foundation models, services tackling AI hallucinations, and new security measures. And this year is likely to follow suit, based on what Amazon has announced so far and the lineup of speakers and programming that you can explore in more detail below.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The event formally kicks off tomorrow, December 2, at 9 a.m. PT. You can expect editorial coverage of the keynotes, interviews with AWS execs, and a roundup of news as the week’s programming rolls out, all of which you’ll be able to find on our site or by just keeping tabs on our AWS re:Invent tag.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AWS is partnering with companies like TechCrunch to highlight a sponsored showcase of their AWS OnAir programming in particular, which kicked off today to preview the event and highlight some early reveals:&lt;/p&gt;



&lt;!-- Add a placeholder for the Twitch embed --&gt;





&lt;!-- Load the Twitch embed script --&gt;



&lt;!-- Create a Twitch.Player object. This will render within the placeholder div --&gt;



&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s become the norm for events like re:Invent to broadcast keynotes and select programming — we just did something similar for TechCrunch Disrupt 2025 — and you can check out a breadth of streams with no ticket necessary below:&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-aws-re-invent-keynote-addresses"&gt;AWS re:Invent keynote addresses&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;This year’s re:Invent features five keynotes from Tuesday through Thursday, with, of course, a clear focus on AI. You can follow along with each of those keynotes via the scheduled livestreams below. Or you can just go to Fortnite (yes, really) and watch their five keynotes live there.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-aws-ceo-matt-garman-december-2-at-8-a-m-pt"&gt;AWS CEO Matt Garman: December 2 at 8 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-vp-of-agentic-ai-swami-sivasubramanian-december-3-at-8-30-a-m-pt"&gt;AWS VP of Agentic AI Swami Sivasubramanian: December 3 at 8:30 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-vp-of-global-specialists-and-partners-aws-dr-ruba-borno-december-3-at-3-p-m-pt"&gt;VP of Global Specialists and Partners, AWS Dr. Ruba Borno: December 3 at 3 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-svp-of-utility-computing-peter-desantis-vp-of-compute-and-machine-learning-services-aws-december-4-at-9-a-m-pt"&gt;SVP of Utility Computing Peter DeSantis, VP of Compute and Machine Learning Services, AWS: December 4 at 9 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-cto-of-amazon-com-dr-werner-vogels-december-4-at-3-30-p-m-pt"&gt;CTO of Amazon.com Dr. Werner Vogels: December 4 at 3:30 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-aws-re-invent-partner-showcases"&gt;AWS re:Invent partner showcases&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;A series of industry-specific partner showcases will also be livestreamed for those not on the ground in Las Vegas, with a series of streams that extend throughout the run of the event.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h3 class="wp-block-heading" id="h-aws-security-day-one-december-2-at-10-a-m-pt"&gt;AWS Security, Day One: December 2 at 10 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-ai-day-one-december-2-at-2-p-m-pt"&gt;AWS AI, Day One: December 2 at 2 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-ai-day-two-december-3-at-10-a-m-pt"&gt;AWS AI, Day Two: December 3 at 10 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-security-day-two-december-3-at-4-30-p-m-pt"&gt;AWS Security, Day Two: December 3 at 4:30 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-security-day-three-december-4-at-10-00-a-m-pt"&gt;AWS Security, Day Three: December 4 at 10:00 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-ai-day-three-december-4-at-11-35-a-m-pt"&gt;AWS AI, Day Three: December 4 at 11:35 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-industries-december-4-at-1-40-p-m-pt"&gt;AWS Industries: December 4 at 1:40 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/01/aws-reinvent-2025-how-to-watch-and-follow-along-live/</guid><pubDate>Mon, 01 Dec 2025 20:50:00 +0000</pubDate></item><item><title>Nvidia announces new open AI models and tools for autonomous driving research (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/01/nvidia-announces-new-open-ai-models-and-tools-for-autonomous-driving-research/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/GettyImages-2216028442.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nvidia announced new infrastructure and AI models on Monday as it works to build the backbone technology for physical AI, including robots and autonomous vehicles that can perceive and interact with the real world.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The semiconductor giant announced Alpamayo-R1, an open reasoning vision language model for autonomous driving research at the NeurIPS AI conference in San Diego, California. The company claims this is the first vision language action model focused on autonomous driving. Visual language models can process both text and images together, allowing vehicles to “see” their surroundings and make decisions based on what they perceive.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This new model is based on Nvidia’s Cosmos-Reason model, a reasoning model that thinks through decisions before it responds. Nvidia initially released the Cosmos model family in January 2025. Additional models were released in August.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Technology like the Alpamayo-R1 is critical for companies looking to reach level 4 autonomous driving, which means full autonomy in a defined area and under specific circumstances, Nvidia said in a blog post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia hopes that this type of reasoning model will give autonomous vehicles the “common sense” to better approach nuanced driving decisions like humans do.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This new model is available on GitHub and Hugging Face.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside the new vision model, Nvidia also uploaded new step-by-step guides, inference resources, and post-training workflows to GitHub — collectively called the Cosmos Cookbook — to help developers better use and train Cosmos models for their specific use cases. The guide covers data curation, synthetic data generation, and model evaluation.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;These announcements come as the company is pushing full-speed into physical AI as a new avenue for its advanced AI GPUs. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia’s co-founder and CEO Jensen Huang has repeatedly said that the next wave of AI is physical AI. Bill Dally, Nvidia’s chief scientist, echoed that sentiment in a conversation with TechCrunch over the summer, emphasizing physical AI in robotics.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think eventually robots are going to be a huge player in the world and we want to basically be making the brains of all the robots,” Dally said at the time. “To do that, we need to start developing the key technologies.”&lt;/p&gt;



&lt;!--&amp;lt;!— Add a placeholder for the Twitch embed --&gt;--&amp;gt;


&lt;!-- Load the Twitch embed script --&gt;

&lt;!-- Create a Twitch.Player object. This will render within the placeholder div --&gt;


&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. &lt;strong&gt;This video is brought to you in partnership with AWS.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/GettyImages-2216028442.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nvidia announced new infrastructure and AI models on Monday as it works to build the backbone technology for physical AI, including robots and autonomous vehicles that can perceive and interact with the real world.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The semiconductor giant announced Alpamayo-R1, an open reasoning vision language model for autonomous driving research at the NeurIPS AI conference in San Diego, California. The company claims this is the first vision language action model focused on autonomous driving. Visual language models can process both text and images together, allowing vehicles to “see” their surroundings and make decisions based on what they perceive.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This new model is based on Nvidia’s Cosmos-Reason model, a reasoning model that thinks through decisions before it responds. Nvidia initially released the Cosmos model family in January 2025. Additional models were released in August.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Technology like the Alpamayo-R1 is critical for companies looking to reach level 4 autonomous driving, which means full autonomy in a defined area and under specific circumstances, Nvidia said in a blog post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia hopes that this type of reasoning model will give autonomous vehicles the “common sense” to better approach nuanced driving decisions like humans do.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This new model is available on GitHub and Hugging Face.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside the new vision model, Nvidia also uploaded new step-by-step guides, inference resources, and post-training workflows to GitHub — collectively called the Cosmos Cookbook — to help developers better use and train Cosmos models for their specific use cases. The guide covers data curation, synthetic data generation, and model evaluation.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;These announcements come as the company is pushing full-speed into physical AI as a new avenue for its advanced AI GPUs. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia’s co-founder and CEO Jensen Huang has repeatedly said that the next wave of AI is physical AI. Bill Dally, Nvidia’s chief scientist, echoed that sentiment in a conversation with TechCrunch over the summer, emphasizing physical AI in robotics.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think eventually robots are going to be a huge player in the world and we want to basically be making the brains of all the robots,” Dally said at the time. “To do that, we need to start developing the key technologies.”&lt;/p&gt;



&lt;!--&amp;lt;!— Add a placeholder for the Twitch embed --&gt;--&amp;gt;


&lt;!-- Load the Twitch embed script --&gt;

&lt;!-- Create a Twitch.Player object. This will render within the placeholder div --&gt;


&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. &lt;strong&gt;This video is brought to you in partnership with AWS.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/01/nvidia-announces-new-open-ai-models-and-tools-for-autonomous-driving-research/</guid><pubDate>Mon, 01 Dec 2025 21:00:22 +0000</pubDate></item><item><title>MIT Sea Grant students explore the intersection of technology and offshore aquaculture in Norway (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/aquaculture-shock-ai-and-autonomy-aquaculture-1201</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/Farm-Sveinung-Beckett-MIT-Aquaculture-00.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;Norway is the world’s largest producer of farmed Atlantic salmon and a top exporter of seafood, while the United States remains the largest importer of these products, according to the Food and Agriculture Organization. Two MIT students recently traveled to Trondheim, Norway to explore the cutting-edge technologies being developed and deployed in offshore aquaculture.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Beckett Devoe, a senior in artificial intelligence and decision-making, and Tony Tang, a junior in mechanical engineering, first worked with MIT Sea Grant through the Undergraduate Research Opportunities Program (UROP). They contributed to projects focusing on wave generator design and machine learning applications for analyzing oyster larvae health in hatcheries. While near-shore aquaculture is a well-established industry across Massachusetts and the United States, open-ocean farming is still a nascent field here, facing unique and complex challenges.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;To help better understand this emerging industry, MIT Sea Grant created a collaborative initiative, AquaCulture Shock, with funding from an Aquaculture Technologies and Education Travel Grant through the National Sea Grant College Program. Collaborating with the MIT-Scandinavia MISTI (MIT International Science and Technology Initiatives) program, MIT Sea Grant matched Devoe and Tang with aquaculture-related summer internships at SINTEF Ocean, one of the largest research institutes in Europe.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“The opportunity to work on this hands-on aquaculture project, under a world-renowned research institution, in an area of the world known for its innovation in marine technology — this is what MISTI is all about,” says Madeline Smith, managing director for MIT-Scandinavia. “Not only are students gaining valuable experience in their fields of study, but they’re developing cultural understanding and skills that equip them to be future global leaders.” Both students worked within SINTEF Ocean’s Aquaculture Robotics and Autonomous Systems Laboratory (ACE-Robotic Lab), a facility designed to develop and test new aquaculture technologies.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“Norway has this unique geography where it has all of these fjords,” says Sveinung Ohrem, research manager for the Aquaculture Robotics and Automation Group at SINTEF Ocean. “So you have a lot of sheltered waters, which makes it ideal to do sea-based aquaculture.” He estimates that there are about a thousand fish farms along Norway’s coast, and walks through some of the tools being used in the industry: decision-making systems to gather and visualize data for the farmers and operators; robots for inspection and cleaning; environmental sensors to measure oxygen, temperature, and currents; echosounders that send out acoustic signals to track where the fish are; and cameras to help estimate biomass and fine-tune feeding. “Feeding is a huge challenge,” he notes. “Feed is the largest cost, by far, so optimizing feeding leads to a very significant decrease in your cost.”&lt;/p&gt;&lt;p dir="ltr"&gt;During the internship, Devoe focused on a project that uses AI for fish feeding optimization. “I try to look at the different features of the farm — so maybe how big the fish are, or how cold the water is ... and use that to try to give the farmers an optimal feeding amount for the best outcomes, while also saving money on feed,” he explains. “It was good to learn some more machine learning techniques and just get better at that on a real-world project.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;In the same lab, Tang worked on the simulation of an underwater vehicle-manipulator system to navigate farms and repair damage on cage nets with a robotic arm. Ohrem says there are thousands of aquaculture robots operating in Norway today. “The scale is huge,” he says. “You can’t have 8,000 people controlling 8,000 robots — that’s not economically or practically feasible. So the level of autonomy in all of these robots needs to be increased.”&lt;/p&gt;&lt;p dir="ltr"&gt;The collaboration between MIT and SINTEF Ocean began in 2023 when MIT Sea Grant hosted Eleni Kelasidi, a visiting research scientist from the ACE-Robotic Lab. Kelasidi collaborated with MIT Sea Grant director Michael Triantafyllou and professor of mechanical engineering Themistoklis Sapsis developing controllers, models, and underwater vehicles for aquaculture, while also investigating fish-machine interactions.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“We have had a long and fruitful collaboration with the Norwegian University of Science and Technology (NTNU) and SINTEF, which continues with important efforts such as the aquaculture project with Dr. Kelasidi,” Triantafyllou says. “Norway is at the forefront of offshore aquaculture and MIT Sea Grant is investing in this field, so we anticipate great results from the collaboration.”&lt;/p&gt;&lt;p dir="ltr"&gt;Kelasidi, who is now a professor at NTNU, also leads the Field Robotics Lab, focusing on developing resilient robotic systems to operate in very complex and harsh environments. “Aquaculture is one of the most challenging field domains we can demonstrate any autonomous solutions, because everything is moving,” she says. Kelasidi describes aquaculture as a deeply interdisciplinary field, requiring more students with backgrounds both in biology and technology. “We cannot develop technologies that are applied for industries where we don’t have biological components,” she explains, “and then apply them somewhere where we have a live fish or other live organisms.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Ohrem affirms that maintaining fish welfare is the primary driver for researchers and companies operating in aquaculture, especially as the industry continues to grow. “So the big question is,” he says, “how can you ensure that?” SINTEF Ocean has four research licenses for farming fish, which they operate through a collaboration with SalMar, the second-largest salmon farmer in the world. The students had the opportunity to visit one of the industrial-scale farms, Singsholmen, on the island of Hitra. The farm has 10 large, round net pens about 50 meters across that extend deep below the surface, each holding up to 200,000 salmon. “I got to physically touch the nets and see how the [robotic] arm might be able to fix the net,” says Tang.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Kelasidi emphasizes that the information gained in the field cannot be learned from the office or lab. “That opens up and makes you realize, what is the scale of the challenges, or the scale of the facilities,” she says. She also highlights the importance of international and institutional collaboration to advance this field of research and develop more resilient robotic systems. “We need to try to target that problem, and let’s solve it together.”&lt;/p&gt;&lt;p dir="ltr"&gt;MIT Sea Grant and the MIT-Scandinavia MISTI program are currently recruiting a new cohort of four MIT students to intern in Norway this summer with institutes advancing offshore farming technologies, including NTNU’s Field Robotics Lab in Trondheim. Students interested in autonomy, deep learning, simulation modeling, underwater robotic systems, and other aquaculture-related areas are encouraged to reach out to Lily Keyes at MIT Sea Grant.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/Farm-Sveinung-Beckett-MIT-Aquaculture-00.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;Norway is the world’s largest producer of farmed Atlantic salmon and a top exporter of seafood, while the United States remains the largest importer of these products, according to the Food and Agriculture Organization. Two MIT students recently traveled to Trondheim, Norway to explore the cutting-edge technologies being developed and deployed in offshore aquaculture.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Beckett Devoe, a senior in artificial intelligence and decision-making, and Tony Tang, a junior in mechanical engineering, first worked with MIT Sea Grant through the Undergraduate Research Opportunities Program (UROP). They contributed to projects focusing on wave generator design and machine learning applications for analyzing oyster larvae health in hatcheries. While near-shore aquaculture is a well-established industry across Massachusetts and the United States, open-ocean farming is still a nascent field here, facing unique and complex challenges.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;To help better understand this emerging industry, MIT Sea Grant created a collaborative initiative, AquaCulture Shock, with funding from an Aquaculture Technologies and Education Travel Grant through the National Sea Grant College Program. Collaborating with the MIT-Scandinavia MISTI (MIT International Science and Technology Initiatives) program, MIT Sea Grant matched Devoe and Tang with aquaculture-related summer internships at SINTEF Ocean, one of the largest research institutes in Europe.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“The opportunity to work on this hands-on aquaculture project, under a world-renowned research institution, in an area of the world known for its innovation in marine technology — this is what MISTI is all about,” says Madeline Smith, managing director for MIT-Scandinavia. “Not only are students gaining valuable experience in their fields of study, but they’re developing cultural understanding and skills that equip them to be future global leaders.” Both students worked within SINTEF Ocean’s Aquaculture Robotics and Autonomous Systems Laboratory (ACE-Robotic Lab), a facility designed to develop and test new aquaculture technologies.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“Norway has this unique geography where it has all of these fjords,” says Sveinung Ohrem, research manager for the Aquaculture Robotics and Automation Group at SINTEF Ocean. “So you have a lot of sheltered waters, which makes it ideal to do sea-based aquaculture.” He estimates that there are about a thousand fish farms along Norway’s coast, and walks through some of the tools being used in the industry: decision-making systems to gather and visualize data for the farmers and operators; robots for inspection and cleaning; environmental sensors to measure oxygen, temperature, and currents; echosounders that send out acoustic signals to track where the fish are; and cameras to help estimate biomass and fine-tune feeding. “Feeding is a huge challenge,” he notes. “Feed is the largest cost, by far, so optimizing feeding leads to a very significant decrease in your cost.”&lt;/p&gt;&lt;p dir="ltr"&gt;During the internship, Devoe focused on a project that uses AI for fish feeding optimization. “I try to look at the different features of the farm — so maybe how big the fish are, or how cold the water is ... and use that to try to give the farmers an optimal feeding amount for the best outcomes, while also saving money on feed,” he explains. “It was good to learn some more machine learning techniques and just get better at that on a real-world project.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;In the same lab, Tang worked on the simulation of an underwater vehicle-manipulator system to navigate farms and repair damage on cage nets with a robotic arm. Ohrem says there are thousands of aquaculture robots operating in Norway today. “The scale is huge,” he says. “You can’t have 8,000 people controlling 8,000 robots — that’s not economically or practically feasible. So the level of autonomy in all of these robots needs to be increased.”&lt;/p&gt;&lt;p dir="ltr"&gt;The collaboration between MIT and SINTEF Ocean began in 2023 when MIT Sea Grant hosted Eleni Kelasidi, a visiting research scientist from the ACE-Robotic Lab. Kelasidi collaborated with MIT Sea Grant director Michael Triantafyllou and professor of mechanical engineering Themistoklis Sapsis developing controllers, models, and underwater vehicles for aquaculture, while also investigating fish-machine interactions.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“We have had a long and fruitful collaboration with the Norwegian University of Science and Technology (NTNU) and SINTEF, which continues with important efforts such as the aquaculture project with Dr. Kelasidi,” Triantafyllou says. “Norway is at the forefront of offshore aquaculture and MIT Sea Grant is investing in this field, so we anticipate great results from the collaboration.”&lt;/p&gt;&lt;p dir="ltr"&gt;Kelasidi, who is now a professor at NTNU, also leads the Field Robotics Lab, focusing on developing resilient robotic systems to operate in very complex and harsh environments. “Aquaculture is one of the most challenging field domains we can demonstrate any autonomous solutions, because everything is moving,” she says. Kelasidi describes aquaculture as a deeply interdisciplinary field, requiring more students with backgrounds both in biology and technology. “We cannot develop technologies that are applied for industries where we don’t have biological components,” she explains, “and then apply them somewhere where we have a live fish or other live organisms.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Ohrem affirms that maintaining fish welfare is the primary driver for researchers and companies operating in aquaculture, especially as the industry continues to grow. “So the big question is,” he says, “how can you ensure that?” SINTEF Ocean has four research licenses for farming fish, which they operate through a collaboration with SalMar, the second-largest salmon farmer in the world. The students had the opportunity to visit one of the industrial-scale farms, Singsholmen, on the island of Hitra. The farm has 10 large, round net pens about 50 meters across that extend deep below the surface, each holding up to 200,000 salmon. “I got to physically touch the nets and see how the [robotic] arm might be able to fix the net,” says Tang.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Kelasidi emphasizes that the information gained in the field cannot be learned from the office or lab. “That opens up and makes you realize, what is the scale of the challenges, or the scale of the facilities,” she says. She also highlights the importance of international and institutional collaboration to advance this field of research and develop more resilient robotic systems. “We need to try to target that problem, and let’s solve it together.”&lt;/p&gt;&lt;p dir="ltr"&gt;MIT Sea Grant and the MIT-Scandinavia MISTI program are currently recruiting a new cohort of four MIT students to intern in Norway this summer with institutes advancing offshore farming technologies, including NTNU’s Field Robotics Lab in Trondheim. Students interested in autonomy, deep learning, simulation modeling, underwater robotic systems, and other aquaculture-related areas are encouraged to reach out to Lily Keyes at MIT Sea Grant.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/aquaculture-shock-ai-and-autonomy-aquaculture-1201</guid><pubDate>Mon, 01 Dec 2025 21:25:00 +0000</pubDate></item><item><title>OpenAI desperate to avoid explaining why it deleted pirated book datasets (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/12/openai-desperate-to-avoid-explaining-why-it-deleted-pirated-book-datasets/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI risks increased fines after deleting pirated books datasets.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="412" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1227510667-640x412.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1227510667-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          wenmei Zhou | DigitalVision Vectors

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;OpenAI may soon be forced to explain why it deleted a pair of controversial datasets composed of pirated books, and the stakes could not be higher.&lt;/p&gt;
&lt;p&gt;At the heart of a class-action lawsuit from authors alleging that ChatGPT was illegally trained on their works, OpenAI’s decision to delete the datasets could end up being a deciding factor that gives the authors the win.&lt;/p&gt;
&lt;p&gt;It’s undisputed that OpenAI deleted the datasets, known as “Books 1” and “Books 2,” prior to ChatGPT’s release in 2022. Created by former OpenAI employees in 2021, the datasets were built by scraping the open web and seizing the bulk of its data from a shadow library called Library Genesis (LibGen).&lt;/p&gt;
&lt;p&gt;As OpenAI tells it, the datasets fell out of use within that same year, prompting an internal decision to delete them.&lt;/p&gt;
&lt;p&gt;But the authors suspect there’s more to the story than that. They noted that OpenAI appeared to flip-flop by retracting its claim that the datasets’ “non-use” was a reason for deletion, then later claiming that all reasons for deletion, including “non-use,” should be shielded under attorney-client privilege.&lt;/p&gt;
&lt;p&gt;To the authors, it seemed like OpenAI was quickly backtracking after the court granted the authors’ discovery requests to review OpenAI’s internal messages on the firm’s “non-use.”&lt;/p&gt;
&lt;p&gt;In fact, OpenAI’s reversal only made authors more eager to see how OpenAI discussed “non-use,” and now they may get to find out all the reasons why OpenAI deleted the datasets.&lt;/p&gt;
&lt;p&gt;Last week, US district judge Ona Wang ordered OpenAI to share all communications with in-house lawyers about deleting the datasets, as well as “all internal references to LibGen that OpenAI has redacted or withheld on the basis of attorney-client privilege.”&lt;/p&gt;
&lt;p&gt;According to Wang, OpenAI slipped up by arguing that “non-use” was not a “reason” for deleting the datasets, while simultaneously claiming that it should also be deemed a “reason” considered privileged.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Either way, the judge ruled that OpenAI couldn’t block discovery on “non-use” just by deleting a few words from prior filings that had been on the docket for more than a year.&lt;/p&gt;
&lt;p&gt;“OpenAI has gone back-and-forth on whether ‘non-use’ as a ‘reason’ for the deletion of Books1 and Books2 is privileged at all,” Wang wrote. “OpenAI cannot state a ‘reason’ (which implies it is not privileged) and then later assert that the ‘reason’ is privileged to avoid discovery.”&lt;/p&gt;
&lt;p&gt;Additionally, OpenAI’s claim that all reasons for deleting the datasets are privileged “strains credulity,” she concluded, ordering OpenAI to produce a wide range of potentially revealing internal messages by December 8. OpenAI must also make its in-house lawyers available for deposition by December 19.&lt;/p&gt;
&lt;p&gt;OpenAI has argued that it never flip-flopped or retracted anything. It simply used vague phrasing that led to confusion over whether any of the reasons for deleting the datasets were considered non-privileged. But Wang didn’t buy into that, concluding that “even if a ‘reason’ like ‘non-use’ could be privileged, OpenAI has waived privilege by making a moving target of its privilege assertions.”&lt;/p&gt;
&lt;p&gt;Asked for comment, OpenAI told Ars that “we disagree with the ruling and intend to appeal.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;OpenAI’s “flip-flop” may cost it the win&lt;/h2&gt;
&lt;p&gt;So far, OpenAI has avoided disclosing its rationale, claiming that all the reasons it had for deleting the datasets are privileged. In-house lawyers weighed in on the decision to delete and were even copied on a Slack channel initially called “excise-libgen.”&lt;/p&gt;
&lt;p&gt;But Wang reviewed those Slack messages and found that “the vast majority of these communications were not privileged because they were ‘plainly devoid of any request for legal advice and counsel [did] not once weigh in.'”&lt;/p&gt;
&lt;p&gt;In a particularly non-privileged batch of messages, one OpenAI lawyer, Jason Kwon, only weighed in once, the judge noted, to recommend the channel name be changed to “project-clear.” Wang reminded OpenAI that “the entirety of the Slack channel and all messages contained therein is not privileged simply because it was created at the direction of an attorney and/or the fact that a lawyer was copied on the communications.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The authors believe that exposing OpenAI’s rationale may help prove that the ChatGPT maker willfully infringed on copyrights when pirating the book data. As Wang explained, OpenAI’s retraction risked putting the AI firm’s “good faith and state of mind at issue,” which could increase fines in a loss.&lt;/p&gt;
&lt;p&gt;“In a copyright case, a court can increase the award of statutory damages up to $150,000 per infringed work if the infringement was willful, meaning the defendant ‘was actually aware of the infringing activity’ or the ‘defendant’s actions were the result of reckless disregard for, or willful blindness to, the copyright holder’s rights,'” Wang wrote.&lt;/p&gt;
&lt;p&gt;In a court transcript, a lawyer representing some of the authors suing OpenAI, Christopher Young, noted that OpenAI could be in trouble if evidence showed that it decided against using the datasets for later models due to legal risks. He also suggested that OpenAI could be using the datasets under different names to mask further infringement.&lt;/p&gt;
&lt;h2&gt;Judge calls out OpenAI for twisting fair use ruling&lt;/h2&gt;
&lt;p&gt;Wang also found it contradictory that OpenAI continued to argue in a recent filing that it acted in good faith, while “artfully” removing “its good faith affirmative defense and key words such as ‘innocent,’ ‘reasonably believed,’ and ‘good faith.'” These changes only strengthened discovery requests to explore authors’ willfulness theory, Wang wrote, noting the sought-after internal messages would now be critical for the court’s review.&lt;/p&gt;
&lt;p&gt;“A jury is entitled to know the basis for OpenAI’s purported good faith,” Wang wrote.&lt;/p&gt;
&lt;p&gt;The judge appeared particularly frustrated by OpenAI seemingly twisting the Anthropic ruling to defend against the authors’ request to learn more about the deletion of the datasets.&lt;/p&gt;
&lt;p&gt;In a footnote, Wang called out OpenAI for “bizarrely” citing an Anthropic ruling that “grossly” misrepresented Judge William Alsup’s decision by claiming that he found that “downloading pirated copies of books is lawful as long as they are subsequently used for training an LLM.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Instead, Alsup wrote that he doubted that “any accused infringer could ever meet its burden of explaining why downloading source copies from pirate sites that it could have purchased or otherwise accessed lawfully was itself reasonably necessary to any subsequent fair use.”&lt;/p&gt;
&lt;p&gt;If anything, Wang wrote, OpenAI’s decision to pirate book data—then delete it—seemed “to fall squarely into the category of activities proscribed by” Alsup. For emphasis, she quoted Alsup’s order, which said, “such piracy of otherwise available copies is inherently, irredeemably infringing even if the pirated copies are immediately used for the transformative use and immediately discarded.”&lt;/p&gt;
&lt;p&gt;For the authors, getting hold of OpenAI’s privileged communications could tip the scales in their favor, the Hollywood Reporter suggested. Some authors believe the key to winning could be testimony from Anthropic CEO Dario Amodei, who is accused of creating the controversial datasets while he was still at OpenAI. The authors think Amodei also possesses information on the destruction of the datasets, court filings show.&lt;/p&gt;
&lt;p&gt;OpenAI tried to fight the authors’ motion to depose Amodei, but a judge sided with the authors in March, compelling Amodei to answer their biggest questions on his involvement.&lt;/p&gt;
&lt;p&gt;Whether Amodei’s testimony is a bombshell remains to be seen, but it’s clear that OpenAI may struggle to overcome claims of willful infringement. Wang noted there is a “fundamental conflict” in circumstances “where a party asserts a good faith defense based on advice of counsel but then blocks inquiry into their state of mind by asserting attorney-client privilege,” suggesting that OpenAI may have substantially weakened its defense.&lt;/p&gt;
&lt;p&gt;The outcome of the dispute over the deletions could influence OpenAI’s calculus on whether it should ultimately settle the lawsuit. Ahead of the Anthropic settlement—the largest publicly reported copyright class action settlement in history—authors suing pointed to evidence that Anthropic became “not so gung ho about” training on pirated books “for legal reasons.” That seems to be the type of smoking-gun evidence that authors hope will emerge from OpenAI’s withheld Slack messages.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI risks increased fines after deleting pirated books datasets.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="412" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1227510667-640x412.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1227510667-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          wenmei Zhou | DigitalVision Vectors

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;OpenAI may soon be forced to explain why it deleted a pair of controversial datasets composed of pirated books, and the stakes could not be higher.&lt;/p&gt;
&lt;p&gt;At the heart of a class-action lawsuit from authors alleging that ChatGPT was illegally trained on their works, OpenAI’s decision to delete the datasets could end up being a deciding factor that gives the authors the win.&lt;/p&gt;
&lt;p&gt;It’s undisputed that OpenAI deleted the datasets, known as “Books 1” and “Books 2,” prior to ChatGPT’s release in 2022. Created by former OpenAI employees in 2021, the datasets were built by scraping the open web and seizing the bulk of its data from a shadow library called Library Genesis (LibGen).&lt;/p&gt;
&lt;p&gt;As OpenAI tells it, the datasets fell out of use within that same year, prompting an internal decision to delete them.&lt;/p&gt;
&lt;p&gt;But the authors suspect there’s more to the story than that. They noted that OpenAI appeared to flip-flop by retracting its claim that the datasets’ “non-use” was a reason for deletion, then later claiming that all reasons for deletion, including “non-use,” should be shielded under attorney-client privilege.&lt;/p&gt;
&lt;p&gt;To the authors, it seemed like OpenAI was quickly backtracking after the court granted the authors’ discovery requests to review OpenAI’s internal messages on the firm’s “non-use.”&lt;/p&gt;
&lt;p&gt;In fact, OpenAI’s reversal only made authors more eager to see how OpenAI discussed “non-use,” and now they may get to find out all the reasons why OpenAI deleted the datasets.&lt;/p&gt;
&lt;p&gt;Last week, US district judge Ona Wang ordered OpenAI to share all communications with in-house lawyers about deleting the datasets, as well as “all internal references to LibGen that OpenAI has redacted or withheld on the basis of attorney-client privilege.”&lt;/p&gt;
&lt;p&gt;According to Wang, OpenAI slipped up by arguing that “non-use” was not a “reason” for deleting the datasets, while simultaneously claiming that it should also be deemed a “reason” considered privileged.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Either way, the judge ruled that OpenAI couldn’t block discovery on “non-use” just by deleting a few words from prior filings that had been on the docket for more than a year.&lt;/p&gt;
&lt;p&gt;“OpenAI has gone back-and-forth on whether ‘non-use’ as a ‘reason’ for the deletion of Books1 and Books2 is privileged at all,” Wang wrote. “OpenAI cannot state a ‘reason’ (which implies it is not privileged) and then later assert that the ‘reason’ is privileged to avoid discovery.”&lt;/p&gt;
&lt;p&gt;Additionally, OpenAI’s claim that all reasons for deleting the datasets are privileged “strains credulity,” she concluded, ordering OpenAI to produce a wide range of potentially revealing internal messages by December 8. OpenAI must also make its in-house lawyers available for deposition by December 19.&lt;/p&gt;
&lt;p&gt;OpenAI has argued that it never flip-flopped or retracted anything. It simply used vague phrasing that led to confusion over whether any of the reasons for deleting the datasets were considered non-privileged. But Wang didn’t buy into that, concluding that “even if a ‘reason’ like ‘non-use’ could be privileged, OpenAI has waived privilege by making a moving target of its privilege assertions.”&lt;/p&gt;
&lt;p&gt;Asked for comment, OpenAI told Ars that “we disagree with the ruling and intend to appeal.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;OpenAI’s “flip-flop” may cost it the win&lt;/h2&gt;
&lt;p&gt;So far, OpenAI has avoided disclosing its rationale, claiming that all the reasons it had for deleting the datasets are privileged. In-house lawyers weighed in on the decision to delete and were even copied on a Slack channel initially called “excise-libgen.”&lt;/p&gt;
&lt;p&gt;But Wang reviewed those Slack messages and found that “the vast majority of these communications were not privileged because they were ‘plainly devoid of any request for legal advice and counsel [did] not once weigh in.'”&lt;/p&gt;
&lt;p&gt;In a particularly non-privileged batch of messages, one OpenAI lawyer, Jason Kwon, only weighed in once, the judge noted, to recommend the channel name be changed to “project-clear.” Wang reminded OpenAI that “the entirety of the Slack channel and all messages contained therein is not privileged simply because it was created at the direction of an attorney and/or the fact that a lawyer was copied on the communications.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The authors believe that exposing OpenAI’s rationale may help prove that the ChatGPT maker willfully infringed on copyrights when pirating the book data. As Wang explained, OpenAI’s retraction risked putting the AI firm’s “good faith and state of mind at issue,” which could increase fines in a loss.&lt;/p&gt;
&lt;p&gt;“In a copyright case, a court can increase the award of statutory damages up to $150,000 per infringed work if the infringement was willful, meaning the defendant ‘was actually aware of the infringing activity’ or the ‘defendant’s actions were the result of reckless disregard for, or willful blindness to, the copyright holder’s rights,'” Wang wrote.&lt;/p&gt;
&lt;p&gt;In a court transcript, a lawyer representing some of the authors suing OpenAI, Christopher Young, noted that OpenAI could be in trouble if evidence showed that it decided against using the datasets for later models due to legal risks. He also suggested that OpenAI could be using the datasets under different names to mask further infringement.&lt;/p&gt;
&lt;h2&gt;Judge calls out OpenAI for twisting fair use ruling&lt;/h2&gt;
&lt;p&gt;Wang also found it contradictory that OpenAI continued to argue in a recent filing that it acted in good faith, while “artfully” removing “its good faith affirmative defense and key words such as ‘innocent,’ ‘reasonably believed,’ and ‘good faith.'” These changes only strengthened discovery requests to explore authors’ willfulness theory, Wang wrote, noting the sought-after internal messages would now be critical for the court’s review.&lt;/p&gt;
&lt;p&gt;“A jury is entitled to know the basis for OpenAI’s purported good faith,” Wang wrote.&lt;/p&gt;
&lt;p&gt;The judge appeared particularly frustrated by OpenAI seemingly twisting the Anthropic ruling to defend against the authors’ request to learn more about the deletion of the datasets.&lt;/p&gt;
&lt;p&gt;In a footnote, Wang called out OpenAI for “bizarrely” citing an Anthropic ruling that “grossly” misrepresented Judge William Alsup’s decision by claiming that he found that “downloading pirated copies of books is lawful as long as they are subsequently used for training an LLM.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Instead, Alsup wrote that he doubted that “any accused infringer could ever meet its burden of explaining why downloading source copies from pirate sites that it could have purchased or otherwise accessed lawfully was itself reasonably necessary to any subsequent fair use.”&lt;/p&gt;
&lt;p&gt;If anything, Wang wrote, OpenAI’s decision to pirate book data—then delete it—seemed “to fall squarely into the category of activities proscribed by” Alsup. For emphasis, she quoted Alsup’s order, which said, “such piracy of otherwise available copies is inherently, irredeemably infringing even if the pirated copies are immediately used for the transformative use and immediately discarded.”&lt;/p&gt;
&lt;p&gt;For the authors, getting hold of OpenAI’s privileged communications could tip the scales in their favor, the Hollywood Reporter suggested. Some authors believe the key to winning could be testimony from Anthropic CEO Dario Amodei, who is accused of creating the controversial datasets while he was still at OpenAI. The authors think Amodei also possesses information on the destruction of the datasets, court filings show.&lt;/p&gt;
&lt;p&gt;OpenAI tried to fight the authors’ motion to depose Amodei, but a judge sided with the authors in March, compelling Amodei to answer their biggest questions on his involvement.&lt;/p&gt;
&lt;p&gt;Whether Amodei’s testimony is a bombshell remains to be seen, but it’s clear that OpenAI may struggle to overcome claims of willful infringement. Wang noted there is a “fundamental conflict” in circumstances “where a party asserts a good faith defense based on advice of counsel but then blocks inquiry into their state of mind by asserting attorney-client privilege,” suggesting that OpenAI may have substantially weakened its defense.&lt;/p&gt;
&lt;p&gt;The outcome of the dispute over the deletions could influence OpenAI’s calculus on whether it should ultimately settle the lawsuit. Ahead of the Anthropic settlement—the largest publicly reported copyright class action settlement in history—authors suing pointed to evidence that Anthropic became “not so gung ho about” training on pirated books “for legal reasons.” That seems to be the type of smoking-gun evidence that authors hope will emerge from OpenAI’s withheld Slack messages.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/12/openai-desperate-to-avoid-explaining-why-it-deleted-pirated-book-datasets/</guid><pubDate>Mon, 01 Dec 2025 22:16:28 +0000</pubDate></item><item><title>One of Google’s biggest AI advantages is what it already knows about you (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/01/one-of-googles-biggest-ai-advantages-is-what-it-already-knows-about-you/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/pluribus.jpeg?resize=1200,803" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A Google Search exec said that one of the company’s biggest opportunities in AI lies in its ability to get to know the user better and personalize its responses. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The promise is AI that’s uniquely helpful because it knows you. But the risk is AI that feels more like surveillance than service.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In a recent episode of the Limitless podcast, Robby Stein, VP of Product for Google Search, explained that Google’s AI tends to field more queries that are advice-seeking or those where the user is looking for recommendations — and these types of questions are more likely to benefit from more subjective responses.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We think there’s a huge opportunity for our AI to know you better and then be uniquely helpful because of that knowledge,” Stein said in the interview. “And one of the things we talked about at [Google’s developer conference] I/O was how the AI can get a better understanding of you through connected services like Gmail.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google has been integrating AI into its apps for some time, starting back when Gemini was still known as Bard. More recently, it began pulling personal data into another AI product, Gemini Deep Research. And Gemini is now infused into Google Workspace apps like Gmail, Calendar and Drive.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But as Google integrates more personal data into its AI — spanning your emails, documents, photos, location history, and browsing behavior — the line between a helpful assistant and an intrusive one becomes increasingly blurred. And unlike opt-in services, avoiding Google’s data collection may become harder as AI becomes central to its products. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google’s pitch is that this deep personalization makes the AI far more useful. The idea is that Google’s AI technology could learn from the user’s interactions across Google’s various services, then use that understanding to make more personalized recommendations. For instance, if it learned that a user likes particular products or brands, the AI responses might favor those in its recommendations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That, Stein said, would be “much more useful” than just showing users a more generic list of the best-selling products in a given category. “That is, I think, very much the vision — of building something that can be really knowledgeable for you, specifically.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This idea isn’t all that different from how the “Others” in the hit Apple TV show “Pluribus” have gobbled up the world’s knowledge, including intimate details about individuals. When the system interacts with the show’s protagonist, Carol, it uses that data to personalize everything: cooking her favorite meals, adopting a familiar face to handle its communications with her, and otherwise anticipating her needs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Carol doesn’t find the personalized responses kind; she finds them invasive. She never consented to sharing her data with the hivemind, yet it knows her better than she’d like.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Similarly, it seems that avoiding Google’s data-gobbling ways will get increasingly difficult in the AI era, and if Google doesn’t get the balance right, the results could feel more creepy than useful.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;(To be clear: Google &lt;em&gt;does&lt;/em&gt; let you control the apps Gemini uses to make its AI more knowledgeable about you specifically — it’s under “Connected Apps” in Gemini’s settings.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If you do share app data with Gemini, Google says it will save and use that data according to the Gemini privacy policy. And that policy reminds users that human reviewers may read some of their data and not to “enter confidential information that you wouldn’t want a reviewer to see or Google to use to improve its services.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But as more data gets ingested into Google’s own hivemind, it’s easy to see how AI could make data privacy more of a gray area.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google, however, believes it has a solution of sorts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Stein says that Google will indicate when its AI responses are personalized.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think people want to intuitively understand when they’re being personalized — when information is made for them, versus when [it’s] something that everyone would see if they were to ask this question,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Stein noted, too, that Google could send a push notification to users when a product they had been considering after several days of online research becomes available or is on sale.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“There are all these ways that Google now, across modes, across kind of different aspects of your life, [is] being incredibly helpful to you…” he said. “And I think that’s more of how I think of the future of search than any one specific feature or single form factor.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/pluribus.jpeg?resize=1200,803" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A Google Search exec said that one of the company’s biggest opportunities in AI lies in its ability to get to know the user better and personalize its responses. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The promise is AI that’s uniquely helpful because it knows you. But the risk is AI that feels more like surveillance than service.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In a recent episode of the Limitless podcast, Robby Stein, VP of Product for Google Search, explained that Google’s AI tends to field more queries that are advice-seeking or those where the user is looking for recommendations — and these types of questions are more likely to benefit from more subjective responses.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We think there’s a huge opportunity for our AI to know you better and then be uniquely helpful because of that knowledge,” Stein said in the interview. “And one of the things we talked about at [Google’s developer conference] I/O was how the AI can get a better understanding of you through connected services like Gmail.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google has been integrating AI into its apps for some time, starting back when Gemini was still known as Bard. More recently, it began pulling personal data into another AI product, Gemini Deep Research. And Gemini is now infused into Google Workspace apps like Gmail, Calendar and Drive.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But as Google integrates more personal data into its AI — spanning your emails, documents, photos, location history, and browsing behavior — the line between a helpful assistant and an intrusive one becomes increasingly blurred. And unlike opt-in services, avoiding Google’s data collection may become harder as AI becomes central to its products. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google’s pitch is that this deep personalization makes the AI far more useful. The idea is that Google’s AI technology could learn from the user’s interactions across Google’s various services, then use that understanding to make more personalized recommendations. For instance, if it learned that a user likes particular products or brands, the AI responses might favor those in its recommendations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That, Stein said, would be “much more useful” than just showing users a more generic list of the best-selling products in a given category. “That is, I think, very much the vision — of building something that can be really knowledgeable for you, specifically.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This idea isn’t all that different from how the “Others” in the hit Apple TV show “Pluribus” have gobbled up the world’s knowledge, including intimate details about individuals. When the system interacts with the show’s protagonist, Carol, it uses that data to personalize everything: cooking her favorite meals, adopting a familiar face to handle its communications with her, and otherwise anticipating her needs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Carol doesn’t find the personalized responses kind; she finds them invasive. She never consented to sharing her data with the hivemind, yet it knows her better than she’d like.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Similarly, it seems that avoiding Google’s data-gobbling ways will get increasingly difficult in the AI era, and if Google doesn’t get the balance right, the results could feel more creepy than useful.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;(To be clear: Google &lt;em&gt;does&lt;/em&gt; let you control the apps Gemini uses to make its AI more knowledgeable about you specifically — it’s under “Connected Apps” in Gemini’s settings.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If you do share app data with Gemini, Google says it will save and use that data according to the Gemini privacy policy. And that policy reminds users that human reviewers may read some of their data and not to “enter confidential information that you wouldn’t want a reviewer to see or Google to use to improve its services.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But as more data gets ingested into Google’s own hivemind, it’s easy to see how AI could make data privacy more of a gray area.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google, however, believes it has a solution of sorts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Stein says that Google will indicate when its AI responses are personalized.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think people want to intuitively understand when they’re being personalized — when information is made for them, versus when [it’s] something that everyone would see if they were to ask this question,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Stein noted, too, that Google could send a push notification to users when a product they had been considering after several days of online research becomes available or is on sale.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“There are all these ways that Google now, across modes, across kind of different aspects of your life, [is] being incredibly helpful to you…” he said. “And I think that’s more of how I think of the future of search than any one specific feature or single form factor.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/01/one-of-googles-biggest-ai-advantages-is-what-it-already-knows-about-you/</guid><pubDate>Tue, 02 Dec 2025 00:17:52 +0000</pubDate></item><item><title>Apple just named a new AI chief with Google and Microsoft expertise, as John Giannandrea steps down (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/01/apple-just-named-a-new-ai-chief-with-google-and-microsoft-expertise-as-john-giannandrea-steps-down/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2017/09/tcdisrupt_sf17_johngiannadrea-3043.jpg?resize=1200,869" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;In a carefully worded announcement on Monday, Apple said John Giannandrea, who has been the company’s AI chief since 2018, is “stepping down” to, well, not work at Apple anymore. He’ll stick around through spring as an adviser.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His replacement is Amar Subramanya, a highly regarded Microsoft executive who spent 16 years at Google, most recently leading engineering for the Gemini Assistant. It’s a savvy hire, given that Subramanya knows the competition intimately.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The move is being characterized as a shake-up. It was seemingly inevitable in retrospect. Apple Intelligence, the company’s answer to the ChatGPT moment, has been stumbling since its October 2024 launch. Reviews have ranged from “underwhelming” to outright alarmed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Its first months were some of the roughest. A notification summary feature meant to condense multiple alerts into digestible snippets generated a series of embarrassing, untrue headlines in late 2024 and early 2025. Among other missteps, the BBC complained twice after Apple Intelligence falsely reported that Luigi Mangione, the man accused of killing UnitedHealthcare CEO Brian Thompson, had shot himself (he hadn’t) and that a darts player, Luke Littler, won a championship before the final even began.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Then there was Siri’s promised overhaul, which became a black eye for Apple. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A Bloomberg investigation published in May revealed the depths of Apple’s AI struggles. For instance, when Craig Federighi, Apple’s software chief, tested the new Siri on his own phone just weeks before its planned launch in April, he was dismayed to find that many of the features the company had been touting didn’t work. The launch was delayed indefinitely, triggering class-action lawsuits from iPhone 16 buyers who’d been promised an AI-powered assistant.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By that point, Giannandrea had already been sidelined, according to Bloomberg. The news organization reported that Tim Cook had stripped Siri from Giannandrea’s oversight entirely back in March, handing it to Vision Pro creator Mike Rockwell. Apple removed its secretive robotics division from Giannandrea’s control, too.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Bloomberg’s investigation painted a picture of organizational dysfunction, with weak communication between AI and marketing teams, budget misalignments, and a leadership crisis severe enough that some employees had taken to mockingly calling Giannandrea’s group “AI/MLess.” The report also documented an exodus of AI researchers to competitors, including OpenAI, Google, and Meta.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple is reportedly now leaning on Google’s Gemini to power the next version of Siri, an astonishing and, presumably, humbling twist considering the intense rivalry between the two companies that dates back more than 15 years, across mobile operating systems, app stores, browsers, maps, cloud services, smart home devices, and now AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Giannandrea came to Apple from Google, where he ran Machine Intelligence and Search. At Apple, he oversaw the AI strategy, machine learning infrastructure, and Siri development.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Now Subramanya inherits those responsibilities, reporting to Federighi with a clear mandate to help Apple catch up in AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s an interesting moment for the company. While competitors have been pouring billions of dollars into massive AI data centers, Apple has focused on processing AI tasks directly on users’ devices using its custom Apple Silicon chips, a privacy-first approach that avoids collecting user data. (When more complex requests require cloud processing, Apple routes them through Private Cloud Compute, servers that promise to process data temporarily and delete it immediately.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether that philosophy pays off or whether it has permanently left Apple behind is an outstanding question. Apple’s approach comes with clear trade-offs. Among them, on-device models are smaller and less capable than the massive models running in competitors’ data centers, and Apple’s reluctance to collect user data has left its researchers training models on licensed and synthetic data rather than the giant troves of real-world information that fuel its rivals’ systems.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2017/09/tcdisrupt_sf17_johngiannadrea-3043.jpg?resize=1200,869" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;In a carefully worded announcement on Monday, Apple said John Giannandrea, who has been the company’s AI chief since 2018, is “stepping down” to, well, not work at Apple anymore. He’ll stick around through spring as an adviser.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His replacement is Amar Subramanya, a highly regarded Microsoft executive who spent 16 years at Google, most recently leading engineering for the Gemini Assistant. It’s a savvy hire, given that Subramanya knows the competition intimately.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The move is being characterized as a shake-up. It was seemingly inevitable in retrospect. Apple Intelligence, the company’s answer to the ChatGPT moment, has been stumbling since its October 2024 launch. Reviews have ranged from “underwhelming” to outright alarmed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Its first months were some of the roughest. A notification summary feature meant to condense multiple alerts into digestible snippets generated a series of embarrassing, untrue headlines in late 2024 and early 2025. Among other missteps, the BBC complained twice after Apple Intelligence falsely reported that Luigi Mangione, the man accused of killing UnitedHealthcare CEO Brian Thompson, had shot himself (he hadn’t) and that a darts player, Luke Littler, won a championship before the final even began.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Then there was Siri’s promised overhaul, which became a black eye for Apple. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A Bloomberg investigation published in May revealed the depths of Apple’s AI struggles. For instance, when Craig Federighi, Apple’s software chief, tested the new Siri on his own phone just weeks before its planned launch in April, he was dismayed to find that many of the features the company had been touting didn’t work. The launch was delayed indefinitely, triggering class-action lawsuits from iPhone 16 buyers who’d been promised an AI-powered assistant.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By that point, Giannandrea had already been sidelined, according to Bloomberg. The news organization reported that Tim Cook had stripped Siri from Giannandrea’s oversight entirely back in March, handing it to Vision Pro creator Mike Rockwell. Apple removed its secretive robotics division from Giannandrea’s control, too.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Bloomberg’s investigation painted a picture of organizational dysfunction, with weak communication between AI and marketing teams, budget misalignments, and a leadership crisis severe enough that some employees had taken to mockingly calling Giannandrea’s group “AI/MLess.” The report also documented an exodus of AI researchers to competitors, including OpenAI, Google, and Meta.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple is reportedly now leaning on Google’s Gemini to power the next version of Siri, an astonishing and, presumably, humbling twist considering the intense rivalry between the two companies that dates back more than 15 years, across mobile operating systems, app stores, browsers, maps, cloud services, smart home devices, and now AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Giannandrea came to Apple from Google, where he ran Machine Intelligence and Search. At Apple, he oversaw the AI strategy, machine learning infrastructure, and Siri development.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Now Subramanya inherits those responsibilities, reporting to Federighi with a clear mandate to help Apple catch up in AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s an interesting moment for the company. While competitors have been pouring billions of dollars into massive AI data centers, Apple has focused on processing AI tasks directly on users’ devices using its custom Apple Silicon chips, a privacy-first approach that avoids collecting user data. (When more complex requests require cloud processing, Apple routes them through Private Cloud Compute, servers that promise to process data temporarily and delete it immediately.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether that philosophy pays off or whether it has permanently left Apple behind is an outstanding question. Apple’s approach comes with clear trade-offs. Among them, on-device models are smaller and less capable than the massive models running in competitors’ data centers, and Apple’s reluctance to collect user data has left its researchers training models on licensed and synthetic data rather than the giant troves of real-world information that fuel its rivals’ systems.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/01/apple-just-named-a-new-ai-chief-with-google-and-microsoft-expertise-as-john-giannandrea-steps-down/</guid><pubDate>Tue, 02 Dec 2025 01:34:46 +0000</pubDate></item><item><title>[NEW] Arcee aims to reboot U.S. open source AI with new Trinity models released under Apache 2.0 (AI | VentureBeat)</title><link>https://venturebeat.com/ai/arcee-aims-to-reboot-u-s-open-source-ai-with-new-trinity-models-released</link><description>[unable to retrieve full-text content]&lt;p&gt;For much of 2025, the frontier of open-weight language models has been defined not in Silicon Valley or New York City, but in Beijing and Hangzhou.&lt;/p&gt;&lt;p&gt;Chinese research labs including Alibaba&amp;#x27;s &lt;a href="https://venturebeat.com/ai/its-qwens-summer-new-open-source-qwen3-235b-a22b-thinking-2507-tops-openai-gemini-reasoning-models-on-key-benchmarks"&gt;Qwen&lt;/a&gt;, &lt;a href="https://venturebeat.com/ai/deepseek-just-dropped-two-insanely-powerful-ai-models-that-rival-gpt-5-and"&gt;DeepSeek&lt;/a&gt;, &lt;a href="https://venturebeat.com/ai/moonshots-kimi-k2-thinking-emerges-as-leading-open-source-ai-outperforming"&gt;Moonshot&lt;/a&gt; and &lt;a href="https://venturebeat.com/ai/baidus-new-ernie-4-5-model-is-open-for-enterprise-use-with-apache-2-0"&gt;Baidu&lt;/a&gt; have rapidly set the pace in developing large-scale, open Mixture-of-Experts (MoE) models — often with permissive licenses and leading benchmark performance. While OpenAI fielded its own open source, general purpose LLM this summer as well — &lt;a href="https://venturebeat.com/ai/openai-returns-to-open-source-roots-with-new-models-gpt-oss-120b-and-gpt-oss-20b"&gt;gpt-oss-20B and 120B&lt;/a&gt; — the uptake has been &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mdsjn2/unbelievable_china_dominates_top_10_opensource/"&gt;slowed by so many equally or better performing alternatives. &lt;/a&gt;&lt;/p&gt;&lt;p&gt;Now, one small U.S. company is pushing back.&lt;/p&gt;&lt;p&gt;Today, &lt;a href="https://x.com/arcee_ai/status/1995600354374025395"&gt;Arcee AI announced&lt;/a&gt; the release of Trinity Mini and Trinity Nano Preview, the first two models in its new “Trinity” family—an open-weight MoE model suite fully trained in the United States. &lt;/p&gt;&lt;p&gt;Users can try the former directly for themselves in a chatbot format on Acree&amp;#x27;s new website, &lt;a href="https://chat.arcee.ai/"&gt;chat.arcee.ai&lt;/a&gt;, and developers can download the code for both models on &lt;a href="https://huggingface.co/collections/arcee-ai/trinity"&gt;Hugging Face&lt;/a&gt; and run it themselves, as well as modify them&lt;!-- --&gt;/fine-tune&lt;!-- --&gt; to their liking — all for free under an enterprise-friendly Apache 2.0 license.  &lt;/p&gt;&lt;p&gt;While small compared to the largest frontier models, these releases represent a rare attempt by a U.S. startup to build end-to-end open-weight models at scale—trained from scratch, on American infrastructure, using a U.S.-curated dataset pipeline.&lt;/p&gt;&lt;p&gt;&amp;quot;I&amp;#x27;m experiencing a combination of extreme pride in my team and crippling exhaustion, so I&amp;#x27;m struggling to put into words just how excited I am to have these models out,&amp;quot; wrote Arcee Chief Technology Officer (CTO) Lucas Atkins in &lt;a href="https://x.com/latkins/status/1995592666164363335?s=20"&gt;a post on the social network X (formerly Twitter)&lt;/a&gt;. &amp;quot;Especially Mini.&amp;quot;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;A third model, Trinity Large, is already in training: a 420B parameter model with 13B active parameters per token, scheduled to launch in January 2026.&lt;/p&gt;&lt;p&gt;“We want to add something that has been missing in that picture,” Atkins wrote in the &lt;a href="https://www.arcee.ai/blog/the-trinity-manifesto"&gt;Trinity launch manifesto&lt;/a&gt; published on Arcee&amp;#x27;s website. “A serious open weight model family trained end to end in America… that businesses and developers can actually own.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;From Small Models to Scaled Ambition&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The Trinity project marks a turning point for Arcee AI, which until now has been known for its compact, enterprise-focused models. The company has raised $29.5 million in funding to date, including a &lt;a href="https://venturebeat.com/ai/small-language-models-rising-as-arcee-ai-lands-24m-series-a"&gt;$24 million Series A&lt;/a&gt; in 2024 led by Emergence Capital, and its previous releases include &lt;a href="https://venturebeat.com/ai/arcee-opens-up-new-enterprise-focused-customizable-ai-model-afm-4-5b-trained-on-clean-rigorously-filtered-data"&gt;AFM-4.5B&lt;/a&gt;, a compact instruct-tuned model released in mid-2025, and &lt;a href="https://venturebeat.com/ai/arcee-ai-unveils-supernova-a-customizable-instruction-adherent-model-for-enterprises"&gt;SuperNova&lt;/a&gt;, an earlier 70B-parameter instruction-following model designed for in-VPC enterprise deployment. &lt;/p&gt;&lt;p&gt;Both were aimed at solving regulatory and cost issues plaguing proprietary LLM adoption in the enterprise.&lt;/p&gt;&lt;p&gt;With Trinity, Arcee is aiming higher: not just instruction tuning or post-training, but full-stack pretraining of open-weight foundation models—built for long-context reasoning, synthetic data adaptation, and future integration with live retraining systems.&lt;/p&gt;&lt;p&gt;Originally conceived as a stepping stone to Trinity Large, both Mini and Nano emerged from early experimentation with sparse modeling and quickly became production targets themselves.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Technical Highlights&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Trinity Mini is a 26B parameter model with 3B active per token, designed for high-throughput reasoning, function calling, and tool use. Trinity Nano Preview is a 6B parameter model with roughly 800M active non-embedding parameters—a more experimental, chat-focused model with a stronger personality, but lower reasoning robustness. &lt;/p&gt;&lt;p&gt;Both models use Arcee’s new Attention-First Mixture-of-Experts (AFMoE) architecture, a custom MoE design blending global sparsity, local/global attention, and gated attention techniques.&lt;/p&gt;&lt;p&gt;Inspired by recent advances from DeepSeek and Qwen, AFMoE departs from traditional MoE by tightly integrating sparse expert routing with an enhanced attention stack — including grouped-query attention, gated attention, and a local/global pattern that improves long-context reasoning. &lt;/p&gt;&lt;p&gt;Think of a typical MoE model like a call center with 128 specialized agents (called “experts”) — but only a few are consulted for each call, depending on the question. This saves time and energy, since not every expert needs to weigh in.&lt;/p&gt;&lt;p&gt;What makes AFMoE different is how it decides which agents to call and how it blends their answers. Most MoE models use a standard approach that picks experts based on a simple ranking. &lt;/p&gt;&lt;p&gt;AFMoE, by contrast, uses a smoother method (called sigmoid routing) that’s more like adjusting a volume dial than flipping a switch — letting the model blend multiple perspectives more gracefully.&lt;/p&gt;&lt;p&gt;The “attention-first” part means the model focuses heavily on how it pays attention to different parts of the conversation. Imagine reading a novel and remembering some parts more clearly than others based on importance, recency, or emotional impact — that’s attention. AFMoE improves this by combining local attention (focusing on what was just said) with global attention (remembering key points from earlier), using a rhythm that keeps things balanced.&lt;/p&gt;&lt;p&gt;Finally, AFMoE introduces something called gated attention, which acts like a volume control on each attention output — helping the model emphasize or dampen different pieces of information as needed, like adjusting how much you care about each voice in a group discussion.&lt;/p&gt;&lt;p&gt;All of this is designed to make the model more stable during training and more efficient at scale — so it can understand longer conversations, reason more clearly, and run faster without needing massive computing resources.&lt;/p&gt;&lt;p&gt;Unlike many existing MoE implementations, AFMoE emphasizes stability at depth and training efficiency, using techniques like sigmoid-based routing without auxiliary loss, and depth-scaled normalization to support scaling without divergence.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Model Capabilities&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Trinity Mini adopts an MoE architecture with 128 experts, 8 active per token, and 1 always-on shared expert. Context windows reach up to 131,072 tokens, depending on provider. &lt;/p&gt;&lt;p&gt;Benchmarks show Trinity Mini performing competitively with larger models across reasoning tasks, including outperforming gpt-oss on the SimpleQA benchmark (tests factual recall and whether the model admits uncertainty), MMLU (Zero shot, measuring broad academic knowledge and reasoning across many subjects without examples), and BFCL V3 (evaluates multi-step function calling and real-world tool use):&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;MMLU (zero-shot):&lt;/b&gt; 84.95&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Math-500:&lt;/b&gt; 92.10&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GPQA-Diamond:&lt;/b&gt; 58.55&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;BFCL V3:&lt;/b&gt; 59.67&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Latency and throughput numbers across providers like Together and Clarifai show 200+ tokens per second throughput with sub-three-second E2E latency—making Trinity Mini viable for interactive applications and agent pipelines.&lt;/p&gt;&lt;p&gt;Trinity Nano, while smaller and not as stable on edge cases, demonstrates sparse MoE architecture viability at under 1B active parameters per token. &lt;/p&gt;&lt;h3&gt;&lt;b&gt;Access, Pricing, and Ecosystem Integration&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Both Trinity models are released under the permissive, enterprise-friendly, &lt;b&gt;Apache 2.0 license&lt;/b&gt;, allowing unrestricted commercial and research use. Trinity Mini is available via:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://huggingface.co/arcee-ai"&gt;Hugging Face&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://openrouter.ai/arcee-ai/trinity-mini"&gt;OpenRouter&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://chat.arcee.ai/"&gt;chat.arcee.ai&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;API pricing for Trinity Mini via &lt;a href="https://openrouter.ai/arcee-ai/trinity-mini"&gt;OpenRouter&lt;/a&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;$0.045 per million input tokens&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;$0.15 per million output tokens&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A free tier is available for a limited time on OpenRouter&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The model is already integrated into apps including Benchable.ai, Open WebUI, and SillyTavern. It&amp;#x27;s supported in Hugging Face Transformers, VLLM, LM Studio, and llama.cpp.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Data Without Compromise: DatologyAI’s Role&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Central to Arcee’s approach is control over training data—a sharp contrast to many open models trained on web-scraped or legally ambiguous datasets. That’s where &lt;a href="https://www.datologyai.com/"&gt;DatologyAI&lt;/a&gt;, a data curation startup co-founded by former Meta and DeepMind researcher Ari Morcos, plays a critical role.&lt;/p&gt;&lt;p&gt;DatologyAI’s platform automates data filtering, deduplication, and quality enhancement across modalities, ensuring Arcee’s training corpus avoids the pitfalls of noisy, biased, or copyright-risk content. &lt;/p&gt;&lt;p&gt;For Trinity, DatologyAI helped construct a 10 trillion token curriculum organized into three phases: 7T general data, 1.8T high-quality text, and 1.2T STEM-heavy material, including math and code.&lt;/p&gt;&lt;p&gt;This is the same partnership that powered Arcee’s AFM-4.5B—but scaled significantly in both size and complexity. According to Arcee, it was Datology’s filtering and data-ranking tools that allowed Trinity to scale cleanly while improving performance on tasks like mathematics, QA, and agent tool use.&lt;/p&gt;&lt;p&gt;Datology’s contribution also extends into synthetic data generation. For Trinity Large, the company has produced over 10 trillion synthetic tokens—paired with 10T curated web tokens—to form a 20T-token training corpus for the full-scale model now in progress.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Building the Infrastructure to Compete: Prime Intellect&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Arcee’s ability to execute full-scale training in the U.S. is also thanks to its infrastructure partner, &lt;a href="https://www.primeintellect.ai/"&gt;Prime Intellect&lt;/a&gt;. The startup, founded in early 2024, began with a mission to democratize access to AI compute by building a decentralized GPU marketplace and training stack.&lt;/p&gt;&lt;p&gt;While Prime Intellect made headlines with its distributed training of INTELLECT-1—a 10B parameter model trained across contributors in five countries—its more recent work, including the 106B INTELLECT-3, acknowledges the tradeoffs of scale: distributed training works, but for 100B+ models, centralized infrastructure is still more efficient.&lt;/p&gt;&lt;p&gt;For Trinity Mini and Nano, Prime Intellect supplied the orchestration stack, modified TorchTitan runtime, and physical compute environment: 512 H200 GPUs in a custom bf16 pipeline, running high-efficiency HSDP parallelism. It is also hosting the 2048 B300 GPU cluster used to train Trinity Large.&lt;/p&gt;&lt;p&gt;The collaboration shows the difference between branding and execution. While Prime Intellect’s long-term goal remains decentralized compute, its short-term value for Arcee lies in efficient, transparent training infrastructure—infrastructure that remains under U.S. jurisdiction, with known provenance and security controls.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;A Strategic Bet on Model Sovereignty&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Arcee&amp;#x27;s push into full pretraining reflects a broader thesis: that the future of enterprise AI will depend on owning the training loop—not just fine-tuning. As systems evolve to adapt from live usage and interact with tools autonomously, compliance and control over training objectives will matter as much as performance.&lt;/p&gt;&lt;p&gt;“As applications get more ambitious, the boundary between ‘model’ and ‘product’ keeps moving,” Atkins noted in Arcee&amp;#x27;s Trinity manifesto. “To build that kind of software you need to control the weights and the training pipeline, not only the instruction layer.”&lt;/p&gt;&lt;p&gt;This framing sets Trinity apart from other open-weight efforts. Rather than patching someone else’s base model, Arcee has built its own—from data to deployment, infrastructure to optimizer—alongside partners who share that vision of openness and sovereignty.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Looking Ahead: Trinity Large&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Training is currently underway for Trinity Large, Arcee’s 420B parameter MoE model, using the same afmoe architecture scaled to a larger expert set. &lt;/p&gt;&lt;p&gt;The dataset includes 20T tokens, split evenly between synthetic data from DatologyAI and curated wb data.&lt;/p&gt;&lt;p&gt;The model is expected to launch next month in January 2026, with a full technical report to follow shortly thereafter.&lt;/p&gt;&lt;p&gt;If successful, it would make Trinity Large one of the only fully open-weight, U.S.-trained frontier-scale models—positioning Arcee as a serious player in the open ecosystem at a time when most American LLM efforts are either closed or based on non-U.S. foundations.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;A recommitment to U.S. open source&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;In a landscape where the most ambitious open-weight models are increasingly shaped by Chinese research labs, Arcee’s Trinity launch signals a rare shift in direction: an attempt to reclaim ground for transparent, U.S.-controlled model development. &lt;/p&gt;&lt;p&gt;Backed by specialized partners in data and infrastructure, and built from scratch for long-term adaptability, Trinity is a bold statement about the future of U.S. AI development, showing that small, lesser-known companies can still push the boundaries and innovate in an open fashion even as the industry is increasingly productized and commodtized. &lt;/p&gt;&lt;p&gt;What remains to be seen is whether Trinity Large can match the capabilities of its better-funded peers. But with Mini and Nano already in use, and a strong architectural foundation in place, Arcee may already be proving its central thesis: that model sovereignty, not just model size, will define the next era of AI.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;For much of 2025, the frontier of open-weight language models has been defined not in Silicon Valley or New York City, but in Beijing and Hangzhou.&lt;/p&gt;&lt;p&gt;Chinese research labs including Alibaba&amp;#x27;s &lt;a href="https://venturebeat.com/ai/its-qwens-summer-new-open-source-qwen3-235b-a22b-thinking-2507-tops-openai-gemini-reasoning-models-on-key-benchmarks"&gt;Qwen&lt;/a&gt;, &lt;a href="https://venturebeat.com/ai/deepseek-just-dropped-two-insanely-powerful-ai-models-that-rival-gpt-5-and"&gt;DeepSeek&lt;/a&gt;, &lt;a href="https://venturebeat.com/ai/moonshots-kimi-k2-thinking-emerges-as-leading-open-source-ai-outperforming"&gt;Moonshot&lt;/a&gt; and &lt;a href="https://venturebeat.com/ai/baidus-new-ernie-4-5-model-is-open-for-enterprise-use-with-apache-2-0"&gt;Baidu&lt;/a&gt; have rapidly set the pace in developing large-scale, open Mixture-of-Experts (MoE) models — often with permissive licenses and leading benchmark performance. While OpenAI fielded its own open source, general purpose LLM this summer as well — &lt;a href="https://venturebeat.com/ai/openai-returns-to-open-source-roots-with-new-models-gpt-oss-120b-and-gpt-oss-20b"&gt;gpt-oss-20B and 120B&lt;/a&gt; — the uptake has been &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mdsjn2/unbelievable_china_dominates_top_10_opensource/"&gt;slowed by so many equally or better performing alternatives. &lt;/a&gt;&lt;/p&gt;&lt;p&gt;Now, one small U.S. company is pushing back.&lt;/p&gt;&lt;p&gt;Today, &lt;a href="https://x.com/arcee_ai/status/1995600354374025395"&gt;Arcee AI announced&lt;/a&gt; the release of Trinity Mini and Trinity Nano Preview, the first two models in its new “Trinity” family—an open-weight MoE model suite fully trained in the United States. &lt;/p&gt;&lt;p&gt;Users can try the former directly for themselves in a chatbot format on Acree&amp;#x27;s new website, &lt;a href="https://chat.arcee.ai/"&gt;chat.arcee.ai&lt;/a&gt;, and developers can download the code for both models on &lt;a href="https://huggingface.co/collections/arcee-ai/trinity"&gt;Hugging Face&lt;/a&gt; and run it themselves, as well as modify them&lt;!-- --&gt;/fine-tune&lt;!-- --&gt; to their liking — all for free under an enterprise-friendly Apache 2.0 license.  &lt;/p&gt;&lt;p&gt;While small compared to the largest frontier models, these releases represent a rare attempt by a U.S. startup to build end-to-end open-weight models at scale—trained from scratch, on American infrastructure, using a U.S.-curated dataset pipeline.&lt;/p&gt;&lt;p&gt;&amp;quot;I&amp;#x27;m experiencing a combination of extreme pride in my team and crippling exhaustion, so I&amp;#x27;m struggling to put into words just how excited I am to have these models out,&amp;quot; wrote Arcee Chief Technology Officer (CTO) Lucas Atkins in &lt;a href="https://x.com/latkins/status/1995592666164363335?s=20"&gt;a post on the social network X (formerly Twitter)&lt;/a&gt;. &amp;quot;Especially Mini.&amp;quot;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;A third model, Trinity Large, is already in training: a 420B parameter model with 13B active parameters per token, scheduled to launch in January 2026.&lt;/p&gt;&lt;p&gt;“We want to add something that has been missing in that picture,” Atkins wrote in the &lt;a href="https://www.arcee.ai/blog/the-trinity-manifesto"&gt;Trinity launch manifesto&lt;/a&gt; published on Arcee&amp;#x27;s website. “A serious open weight model family trained end to end in America… that businesses and developers can actually own.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;From Small Models to Scaled Ambition&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The Trinity project marks a turning point for Arcee AI, which until now has been known for its compact, enterprise-focused models. The company has raised $29.5 million in funding to date, including a &lt;a href="https://venturebeat.com/ai/small-language-models-rising-as-arcee-ai-lands-24m-series-a"&gt;$24 million Series A&lt;/a&gt; in 2024 led by Emergence Capital, and its previous releases include &lt;a href="https://venturebeat.com/ai/arcee-opens-up-new-enterprise-focused-customizable-ai-model-afm-4-5b-trained-on-clean-rigorously-filtered-data"&gt;AFM-4.5B&lt;/a&gt;, a compact instruct-tuned model released in mid-2025, and &lt;a href="https://venturebeat.com/ai/arcee-ai-unveils-supernova-a-customizable-instruction-adherent-model-for-enterprises"&gt;SuperNova&lt;/a&gt;, an earlier 70B-parameter instruction-following model designed for in-VPC enterprise deployment. &lt;/p&gt;&lt;p&gt;Both were aimed at solving regulatory and cost issues plaguing proprietary LLM adoption in the enterprise.&lt;/p&gt;&lt;p&gt;With Trinity, Arcee is aiming higher: not just instruction tuning or post-training, but full-stack pretraining of open-weight foundation models—built for long-context reasoning, synthetic data adaptation, and future integration with live retraining systems.&lt;/p&gt;&lt;p&gt;Originally conceived as a stepping stone to Trinity Large, both Mini and Nano emerged from early experimentation with sparse modeling and quickly became production targets themselves.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Technical Highlights&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Trinity Mini is a 26B parameter model with 3B active per token, designed for high-throughput reasoning, function calling, and tool use. Trinity Nano Preview is a 6B parameter model with roughly 800M active non-embedding parameters—a more experimental, chat-focused model with a stronger personality, but lower reasoning robustness. &lt;/p&gt;&lt;p&gt;Both models use Arcee’s new Attention-First Mixture-of-Experts (AFMoE) architecture, a custom MoE design blending global sparsity, local/global attention, and gated attention techniques.&lt;/p&gt;&lt;p&gt;Inspired by recent advances from DeepSeek and Qwen, AFMoE departs from traditional MoE by tightly integrating sparse expert routing with an enhanced attention stack — including grouped-query attention, gated attention, and a local/global pattern that improves long-context reasoning. &lt;/p&gt;&lt;p&gt;Think of a typical MoE model like a call center with 128 specialized agents (called “experts”) — but only a few are consulted for each call, depending on the question. This saves time and energy, since not every expert needs to weigh in.&lt;/p&gt;&lt;p&gt;What makes AFMoE different is how it decides which agents to call and how it blends their answers. Most MoE models use a standard approach that picks experts based on a simple ranking. &lt;/p&gt;&lt;p&gt;AFMoE, by contrast, uses a smoother method (called sigmoid routing) that’s more like adjusting a volume dial than flipping a switch — letting the model blend multiple perspectives more gracefully.&lt;/p&gt;&lt;p&gt;The “attention-first” part means the model focuses heavily on how it pays attention to different parts of the conversation. Imagine reading a novel and remembering some parts more clearly than others based on importance, recency, or emotional impact — that’s attention. AFMoE improves this by combining local attention (focusing on what was just said) with global attention (remembering key points from earlier), using a rhythm that keeps things balanced.&lt;/p&gt;&lt;p&gt;Finally, AFMoE introduces something called gated attention, which acts like a volume control on each attention output — helping the model emphasize or dampen different pieces of information as needed, like adjusting how much you care about each voice in a group discussion.&lt;/p&gt;&lt;p&gt;All of this is designed to make the model more stable during training and more efficient at scale — so it can understand longer conversations, reason more clearly, and run faster without needing massive computing resources.&lt;/p&gt;&lt;p&gt;Unlike many existing MoE implementations, AFMoE emphasizes stability at depth and training efficiency, using techniques like sigmoid-based routing without auxiliary loss, and depth-scaled normalization to support scaling without divergence.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Model Capabilities&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Trinity Mini adopts an MoE architecture with 128 experts, 8 active per token, and 1 always-on shared expert. Context windows reach up to 131,072 tokens, depending on provider. &lt;/p&gt;&lt;p&gt;Benchmarks show Trinity Mini performing competitively with larger models across reasoning tasks, including outperforming gpt-oss on the SimpleQA benchmark (tests factual recall and whether the model admits uncertainty), MMLU (Zero shot, measuring broad academic knowledge and reasoning across many subjects without examples), and BFCL V3 (evaluates multi-step function calling and real-world tool use):&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;MMLU (zero-shot):&lt;/b&gt; 84.95&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Math-500:&lt;/b&gt; 92.10&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GPQA-Diamond:&lt;/b&gt; 58.55&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;BFCL V3:&lt;/b&gt; 59.67&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Latency and throughput numbers across providers like Together and Clarifai show 200+ tokens per second throughput with sub-three-second E2E latency—making Trinity Mini viable for interactive applications and agent pipelines.&lt;/p&gt;&lt;p&gt;Trinity Nano, while smaller and not as stable on edge cases, demonstrates sparse MoE architecture viability at under 1B active parameters per token. &lt;/p&gt;&lt;h3&gt;&lt;b&gt;Access, Pricing, and Ecosystem Integration&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Both Trinity models are released under the permissive, enterprise-friendly, &lt;b&gt;Apache 2.0 license&lt;/b&gt;, allowing unrestricted commercial and research use. Trinity Mini is available via:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://huggingface.co/arcee-ai"&gt;Hugging Face&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://openrouter.ai/arcee-ai/trinity-mini"&gt;OpenRouter&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://chat.arcee.ai/"&gt;chat.arcee.ai&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;API pricing for Trinity Mini via &lt;a href="https://openrouter.ai/arcee-ai/trinity-mini"&gt;OpenRouter&lt;/a&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;$0.045 per million input tokens&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;$0.15 per million output tokens&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A free tier is available for a limited time on OpenRouter&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The model is already integrated into apps including Benchable.ai, Open WebUI, and SillyTavern. It&amp;#x27;s supported in Hugging Face Transformers, VLLM, LM Studio, and llama.cpp.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Data Without Compromise: DatologyAI’s Role&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Central to Arcee’s approach is control over training data—a sharp contrast to many open models trained on web-scraped or legally ambiguous datasets. That’s where &lt;a href="https://www.datologyai.com/"&gt;DatologyAI&lt;/a&gt;, a data curation startup co-founded by former Meta and DeepMind researcher Ari Morcos, plays a critical role.&lt;/p&gt;&lt;p&gt;DatologyAI’s platform automates data filtering, deduplication, and quality enhancement across modalities, ensuring Arcee’s training corpus avoids the pitfalls of noisy, biased, or copyright-risk content. &lt;/p&gt;&lt;p&gt;For Trinity, DatologyAI helped construct a 10 trillion token curriculum organized into three phases: 7T general data, 1.8T high-quality text, and 1.2T STEM-heavy material, including math and code.&lt;/p&gt;&lt;p&gt;This is the same partnership that powered Arcee’s AFM-4.5B—but scaled significantly in both size and complexity. According to Arcee, it was Datology’s filtering and data-ranking tools that allowed Trinity to scale cleanly while improving performance on tasks like mathematics, QA, and agent tool use.&lt;/p&gt;&lt;p&gt;Datology’s contribution also extends into synthetic data generation. For Trinity Large, the company has produced over 10 trillion synthetic tokens—paired with 10T curated web tokens—to form a 20T-token training corpus for the full-scale model now in progress.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Building the Infrastructure to Compete: Prime Intellect&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Arcee’s ability to execute full-scale training in the U.S. is also thanks to its infrastructure partner, &lt;a href="https://www.primeintellect.ai/"&gt;Prime Intellect&lt;/a&gt;. The startup, founded in early 2024, began with a mission to democratize access to AI compute by building a decentralized GPU marketplace and training stack.&lt;/p&gt;&lt;p&gt;While Prime Intellect made headlines with its distributed training of INTELLECT-1—a 10B parameter model trained across contributors in five countries—its more recent work, including the 106B INTELLECT-3, acknowledges the tradeoffs of scale: distributed training works, but for 100B+ models, centralized infrastructure is still more efficient.&lt;/p&gt;&lt;p&gt;For Trinity Mini and Nano, Prime Intellect supplied the orchestration stack, modified TorchTitan runtime, and physical compute environment: 512 H200 GPUs in a custom bf16 pipeline, running high-efficiency HSDP parallelism. It is also hosting the 2048 B300 GPU cluster used to train Trinity Large.&lt;/p&gt;&lt;p&gt;The collaboration shows the difference between branding and execution. While Prime Intellect’s long-term goal remains decentralized compute, its short-term value for Arcee lies in efficient, transparent training infrastructure—infrastructure that remains under U.S. jurisdiction, with known provenance and security controls.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;A Strategic Bet on Model Sovereignty&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Arcee&amp;#x27;s push into full pretraining reflects a broader thesis: that the future of enterprise AI will depend on owning the training loop—not just fine-tuning. As systems evolve to adapt from live usage and interact with tools autonomously, compliance and control over training objectives will matter as much as performance.&lt;/p&gt;&lt;p&gt;“As applications get more ambitious, the boundary between ‘model’ and ‘product’ keeps moving,” Atkins noted in Arcee&amp;#x27;s Trinity manifesto. “To build that kind of software you need to control the weights and the training pipeline, not only the instruction layer.”&lt;/p&gt;&lt;p&gt;This framing sets Trinity apart from other open-weight efforts. Rather than patching someone else’s base model, Arcee has built its own—from data to deployment, infrastructure to optimizer—alongside partners who share that vision of openness and sovereignty.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Looking Ahead: Trinity Large&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Training is currently underway for Trinity Large, Arcee’s 420B parameter MoE model, using the same afmoe architecture scaled to a larger expert set. &lt;/p&gt;&lt;p&gt;The dataset includes 20T tokens, split evenly between synthetic data from DatologyAI and curated wb data.&lt;/p&gt;&lt;p&gt;The model is expected to launch next month in January 2026, with a full technical report to follow shortly thereafter.&lt;/p&gt;&lt;p&gt;If successful, it would make Trinity Large one of the only fully open-weight, U.S.-trained frontier-scale models—positioning Arcee as a serious player in the open ecosystem at a time when most American LLM efforts are either closed or based on non-U.S. foundations.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;A recommitment to U.S. open source&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;In a landscape where the most ambitious open-weight models are increasingly shaped by Chinese research labs, Arcee’s Trinity launch signals a rare shift in direction: an attempt to reclaim ground for transparent, U.S.-controlled model development. &lt;/p&gt;&lt;p&gt;Backed by specialized partners in data and infrastructure, and built from scratch for long-term adaptability, Trinity is a bold statement about the future of U.S. AI development, showing that small, lesser-known companies can still push the boundaries and innovate in an open fashion even as the industry is increasingly productized and commodtized. &lt;/p&gt;&lt;p&gt;What remains to be seen is whether Trinity Large can match the capabilities of its better-funded peers. But with Mini and Nano already in use, and a strong architectural foundation in place, Arcee may already be proving its central thesis: that model sovereignty, not just model size, will define the next era of AI.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/arcee-aims-to-reboot-u-s-open-source-ai-with-new-trinity-models-released</guid><pubDate>Tue, 02 Dec 2025 03:53:00 +0000</pubDate></item><item><title>[NEW] What does it mean when Uncle Sam is one of your biggest shareholders? Chip startup xLight is about to find out (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/01/what-does-it-mean-when-uncle-sam-is-one-of-your-biggest-shareholders-chip-startup-xlight-is-about-to-find-out/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/04/Screen-Shot-2019-04-09-at-12.38.58-PM.png?resize=1200,731" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Trump administration has agreed to inject up to $150 million into xLight, a semiconductor startup developing advanced chip-making technology, marking the third time the U.S. government has taken an equity position in a private startup and further expanding a controversial strategy that has put Washington on the cap tables of American companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Wall Street Journal reported Monday that the Commerce Department will provide the funding to xLight in exchange for an equity stake that will likely make the government the startup’s largest shareholder. The deal uses funding from the 2022 Chips and Science Act and represents the first Chips Act award in President Trump’s second term, though it remains preliminary and subject to change.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Previous government equity investments under the Trump administration include publicly traded companies Intel, MP Materials, Lithium Americas, and Trilogy Metals. Two rare earths startups also secured funding in exchange for equity from the Commerce Department last month.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;You can imagine how this is all going over in Silicon Valley, where the libertarian ethos runs deep. At TechCrunch’s signature Disrupt event back in October, Sequoia Capital’s Roelof Botha jokingly offered what might be the understatement of the year when asked about the trend: “[Some] of the most dangerous words in the world are: ‘I’m from the government, and I’m here to help.’”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other VCs have similarly expressed concerns, if quietly, about what it means when their portfolio companies are suddenly competing against startups backed by the U.S. Treasury, or even when they find themselves sitting across the table from government representatives at board meetings.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The four-year-old, Palo Alto, California, company at the center of this particular experiment is trying to do something genuinely audacious in semiconductor manufacturing. XLight wants to build particle accelerator-powered lasers — machines the size of a football field, mind you — that would create more powerful and precise light sources for making chips.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If it works, it could challenge the near-total dominance of ASML, the Dutch giant that has been publicly traded since 1995 and currently enjoys an absolute monopoly on extreme ultraviolet lithography machines. (Its shares have surged 48.6% this year.)&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The CEO of xLight is Nicholas Kelez, a quantum computing and government labs veteran who presumably knows his way around a particle accelerator. Helping this venture as executive chairman is Pat Gelsinger, the former Intel CEO who was shown the door late last year after his ambitious manufacturing revival plans failed to materialize.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I wasn’t done yet,” Gelsinger — who is also a general partner at Playground Global, which led the startup’s $40 million funding round this summer — told the Journal, adding that the effort is “deeply personal” to him.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Indeed, xLight doesn’t just want to compete with ASML but to go much further. While ASML’s machines work at wavelengths around 13.5 nanometers, xLight is targeting 2 nanometers. Gelsinger claims the technology could boost wafer processing efficiency by 30% to 40% while using far less energy.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As it happens, both Kelez and Gelsinger will be holding forth at TechCrunch’s StrictlyVC event on Wednesday night in Palo Alto, where the government’s backing will no doubt come up. (You can still nab a seat here.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Commerce Secretary Howard Lutnick, for his part, insists this is all in service of national security and technological leadership, saying the partnership could “fundamentally rewrite the limits of chipmaking.” Critics will continue to question whether taxpayer-funded equity stakes represent visionary industrial policy or state capitalism with a patriotic sheen, though even skeptics acknowledge the geopolitical reality.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At least Botha, who described himself at Disrupt as a “sort of libertarian, free market thinker by nature,” conceded that industrial policy has its place when national interests demand it. “The only reason the U.S. is resorting to this is because we have other nation states with whom we compete who are using industrial policy to further their industries that are strategic and maybe adverse to the U.S. in long-term interests.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/04/Screen-Shot-2019-04-09-at-12.38.58-PM.png?resize=1200,731" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Trump administration has agreed to inject up to $150 million into xLight, a semiconductor startup developing advanced chip-making technology, marking the third time the U.S. government has taken an equity position in a private startup and further expanding a controversial strategy that has put Washington on the cap tables of American companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Wall Street Journal reported Monday that the Commerce Department will provide the funding to xLight in exchange for an equity stake that will likely make the government the startup’s largest shareholder. The deal uses funding from the 2022 Chips and Science Act and represents the first Chips Act award in President Trump’s second term, though it remains preliminary and subject to change.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Previous government equity investments under the Trump administration include publicly traded companies Intel, MP Materials, Lithium Americas, and Trilogy Metals. Two rare earths startups also secured funding in exchange for equity from the Commerce Department last month.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;You can imagine how this is all going over in Silicon Valley, where the libertarian ethos runs deep. At TechCrunch’s signature Disrupt event back in October, Sequoia Capital’s Roelof Botha jokingly offered what might be the understatement of the year when asked about the trend: “[Some] of the most dangerous words in the world are: ‘I’m from the government, and I’m here to help.’”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other VCs have similarly expressed concerns, if quietly, about what it means when their portfolio companies are suddenly competing against startups backed by the U.S. Treasury, or even when they find themselves sitting across the table from government representatives at board meetings.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The four-year-old, Palo Alto, California, company at the center of this particular experiment is trying to do something genuinely audacious in semiconductor manufacturing. XLight wants to build particle accelerator-powered lasers — machines the size of a football field, mind you — that would create more powerful and precise light sources for making chips.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If it works, it could challenge the near-total dominance of ASML, the Dutch giant that has been publicly traded since 1995 and currently enjoys an absolute monopoly on extreme ultraviolet lithography machines. (Its shares have surged 48.6% this year.)&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The CEO of xLight is Nicholas Kelez, a quantum computing and government labs veteran who presumably knows his way around a particle accelerator. Helping this venture as executive chairman is Pat Gelsinger, the former Intel CEO who was shown the door late last year after his ambitious manufacturing revival plans failed to materialize.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I wasn’t done yet,” Gelsinger — who is also a general partner at Playground Global, which led the startup’s $40 million funding round this summer — told the Journal, adding that the effort is “deeply personal” to him.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Indeed, xLight doesn’t just want to compete with ASML but to go much further. While ASML’s machines work at wavelengths around 13.5 nanometers, xLight is targeting 2 nanometers. Gelsinger claims the technology could boost wafer processing efficiency by 30% to 40% while using far less energy.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As it happens, both Kelez and Gelsinger will be holding forth at TechCrunch’s StrictlyVC event on Wednesday night in Palo Alto, where the government’s backing will no doubt come up. (You can still nab a seat here.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Commerce Secretary Howard Lutnick, for his part, insists this is all in service of national security and technological leadership, saying the partnership could “fundamentally rewrite the limits of chipmaking.” Critics will continue to question whether taxpayer-funded equity stakes represent visionary industrial policy or state capitalism with a patriotic sheen, though even skeptics acknowledge the geopolitical reality.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At least Botha, who described himself at Disrupt as a “sort of libertarian, free market thinker by nature,” conceded that industrial policy has its place when national interests demand it. “The only reason the U.S. is resorting to this is because we have other nation states with whom we compete who are using industrial policy to further their industries that are strategic and maybe adverse to the U.S. in long-term interests.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/01/what-does-it-mean-when-uncle-sam-is-one-of-your-biggest-shareholders-chip-startup-xlight-is-about-to-find-out/</guid><pubDate>Tue, 02 Dec 2025 04:03:25 +0000</pubDate></item></channel></rss>