<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 09 Dec 2025 18:31:26 +0000</lastBuildDate><item><title>[NEW] Brand-context AI: The missing requirement for marketing AI (AI | VentureBeat)</title><link>https://venturebeat.com/ai/brand-context-ai-the-missing-requirement-for-marketing-ai</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;i&gt;Presented by BlueOcean&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;AI has become a central part of how marketing teams work, but the results often fall short. Models can generate content at scale and summarize information in seconds, yet the outputs are not always aligned with the brand, the audience, or the company’s strategic goals. The problem is not capability. The problem is the absence of context.&lt;/p&gt;&lt;p&gt;The bottleneck is no longer computational power. It is contextual intelligence.&lt;/p&gt;&lt;p&gt;Generative AI is powerful, but it doesn’t understand the nuances of the business it supports. It doesn’t have the context for why customers choose one brand over another or what creates competitive advantage. Without that grounding, AI operates as a fast executor rather than a strategic partner. It produces more, but it does not always help teams make better decisions.&lt;/p&gt;&lt;p&gt;This becomes even more visible inside complex marketing organizations where insights live in different corners of the business and rarely come together in a unified way.&lt;/p&gt;&lt;p&gt;As Grant McDougall, CEO of &lt;a href="https://www.blueocean.ai/?utm_source=VentureBeat&amp;amp;utm_medium=article&amp;amp;utm_campaign=q42025&amp;amp;utm_content=aicontext"&gt;BlueOcean&lt;/a&gt;, explains, “Inside large marketing organizations, the data is vertical. Digital has theirs, loyalty has theirs, content has theirs, media has theirs. But CMOs think horizontally. They need to combine customer insight, competitive movement, creative performance, and sales signals into one coherent view. Connecting that data fundamentally changes how decisions get made.”&lt;/p&gt;&lt;p&gt;This shift from vertical data to horizontal intelligence reflects a new phase in AI adoption. The emphasis is shifting from output volume to decision quality. Marketers are recognizing that the future of AI is intelligence that understands who you are as a company and why you matter to your customers.&lt;/p&gt;&lt;p&gt;In BlueOcean’s work with global brands across technology, healthcare, and consumer industries, including Amazon, Cisco, SAP, and Intel, the same pattern appears. Teams move faster and make better decisions when AI is grounded in structured brand and competitive context.&lt;/p&gt;&lt;h3&gt;Why context is becoming the critical ingredient&lt;/h3&gt;&lt;p&gt;Large language models excel at producing language. They do not inherently understand brand, meaning, or intention. This is why generic prompts often lead to generic outputs. The model executes based on statistical prediction, not strategic nuance.&lt;/p&gt;&lt;p&gt;Context changes that. When AI systems are supplied with structured inputs about brand strategy, audience insight, and creative intent, the output becomes sharper and more reliable. Recommendations become more specific. Creative stays on brief. The AI begins to act less like a content generator and more like a partner that understands the boundaries and goals of the business.&lt;/p&gt;&lt;p&gt;This shift mirrors a key theme from BlueOcean’s recent report, &lt;a href="https://www.blueocean.ai/blog/from-execution-to-intelligence-how-cmos-can-build-context-aware-ai-workflows"&gt;&lt;i&gt;Building Marketing Intelligence: The CMO Blueprint for Context-Aware AI&lt;/i&gt;&lt;/a&gt;. The report explains that AI is most effective when it is grounded in a clear frame of reference. CMOs who design these context-aware workflows see better performance, stronger creative, and more reliable decision-making.&lt;/p&gt;&lt;p&gt;For a deeper exploration of these principles, the full report is available &lt;a href="https://www.blueocean.ai/blog/from-execution-to-intelligence-how-cmos-can-build-context-aware-ai-workflows"&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;The industry’s pivot: From execution to understanding&lt;/h3&gt;&lt;p&gt;Many teams remain in an experimentation phase with AI. They test tools, run pilots, and explore new workflows. This creates productivity gains but not intelligence. Without shared context, every team uses AI differently, and the result is fragmentation.&lt;/p&gt;&lt;p&gt;The companies making the clearest progress treat context as a shared layer across workflows. When teams pull from the same brand strategy, insights, and creative guidance, AI becomes more predictable and more valuable. It supports decisions rather than contradicting them. This becomes especially effective when the context includes external signals such as shifts in sentiment, competitor movement, content performance, and broader category trends.&lt;/p&gt;&lt;p&gt;Brand-context AI connects brand identity, customer sentiment, competitive movement, and creative performance in a single environment. It strengthens workflows in practical ways: briefs become more strategic, content reviews more accurate, and insights faster because the system synthesizes patterns teams once assembled manually.&lt;/p&gt;&lt;p&gt;Across enterprise teams supported by BlueOcean, this shift consistently unlocks clarity. AI becomes a contributor to strategic understanding rather than a generator of disconnected output. With shared context in place, teams make more confident, coherent, and aligned decisions.&lt;/p&gt;&lt;h3&gt;Structured context: What it actually includes&lt;/h3&gt;&lt;p&gt;Structured context is the intelligence marketers already curate to understand how their brand shows up in the world. It brings together the narrative elements that shape the brand’s voice, the customer motivations that influence messaging, the competitive signals unfolding in the market, and the creative patterns that have historically performed. It also includes the external brand signals teams monitor every day: sentiment shifts, content dynamics, press and social movement, and how competitors position themselves across channels.&lt;/p&gt;&lt;p&gt;When this information is organized into a coherent frame, AI can interpret direction and creative choices with the same clarity strategists use. The value does not come from giving AI more data; it comes from giving it structure so it can reason through decisions the way marketers already do.&lt;/p&gt;&lt;h3&gt;The new division of labor between humans and AI&lt;/h3&gt;&lt;p&gt;The strongest AI-enabled marketing teams have one thing in common. They are clear about what humans own and what AI owns. Humans define purpose, strategy, and creative judgment. They understand emotion, cultural nuance, competitive meaning, and brand intent.&lt;/p&gt;&lt;p&gt;AI delivers speed, scale, and precision. It excels at synthesizing information, producing iterations, and following structured instruction.&lt;/p&gt;&lt;p&gt;“AI works best when it is given clear boundaries and clear intent,” says McDougall. “Humans set the direction led by creativity and imagination. AI executes with precision. That partnership is where the real value emerges.”&lt;/p&gt;&lt;p&gt;The systems that perform best are the ones guided by human-defined boundaries and human-led strategy. AI provides scale, but people provide meaning.&lt;/p&gt;&lt;p&gt;CMOs are recognizing that governing context is becoming a leadership responsibility. They already own brand, messaging, and customer insight. Extending this ownership into AI systems ensures the brand shows up consistently across every touchpoint, whether a human or a model produced the work.&lt;/p&gt;&lt;h3&gt;A practical example of context in action&lt;/h3&gt;&lt;p&gt;Consider a team preparing a global campaign. Without context, an AI system might generate copy that sounds polished but generic. It may overlook claims the brand can make, reference benefits competitors own, or ignore differentiators that matter most.  It may even amplify a competitor’s message simply because that language appears frequently in public data.&lt;/p&gt;&lt;p&gt;With structured context, the experience changes. The model understands the audience, the brand tone, the competitive landscape, and the objective. It knows which competitors are gaining attention, which messages resonate in the market, and where the brand has permission to play. It can propose angles that strengthen positioning rather than dilute it. It can generate variations that stay on brief and avoid competitor-owned territory.&lt;/p&gt;&lt;p&gt;BlueOcean has observed this shift inside enterprise teams including Amazon, Intel, and SAP, where structured brand and competitive context has improved alignment and reduced drift at scale.&lt;/p&gt;&lt;p&gt;Creative, brand, and competitive signals are no longer separate inputs. When they are connected and contextualized, AI begins supporting decision-making in a meaningful way. The technology stops producing output for its own sake and starts helping marketers understand where the brand stands and what actions will grow it.&lt;/p&gt;&lt;h3&gt;What comes next&lt;/h3&gt;&lt;p&gt;A new phase of AI is beginning. AI agents are evolving from task assistants to systems that collaborate across tools and workflows. As these systems become more capable, context will determine whether they behave unpredictably or perform as trusted extensions of the team.&lt;/p&gt;&lt;p&gt;Brand-context AI provides a path forward. It gives AI systems the structure they need to operate consistently. It supports the teams responsible for protecting brand integrity. In practice, these agents can already assemble context-aware creative briefs, review content for competitive and brand alignment, monitor shifts in category messaging, and synthesize insights across products or markets. It creates intelligence that adapts rather than overwhelms.&lt;/p&gt;&lt;p&gt;In the coming years, success will not come from producing more content, but from producing content anchored in brand context, the kind that sharpens decisions, strengthens positioning, and drives long-term growth.&lt;/p&gt;&lt;p&gt;The companies that build on context today will define the generative enterprise of tomorrow. BlueOcean is helping leading enterprises shape the next generation of context-aware AI systems.&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact &lt;/i&gt;&lt;a href="mailto:sales@venturebeat.com"&gt;&lt;i&gt;&lt;u&gt;sales@venturebeat.com&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;i&gt;Presented by BlueOcean&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;AI has become a central part of how marketing teams work, but the results often fall short. Models can generate content at scale and summarize information in seconds, yet the outputs are not always aligned with the brand, the audience, or the company’s strategic goals. The problem is not capability. The problem is the absence of context.&lt;/p&gt;&lt;p&gt;The bottleneck is no longer computational power. It is contextual intelligence.&lt;/p&gt;&lt;p&gt;Generative AI is powerful, but it doesn’t understand the nuances of the business it supports. It doesn’t have the context for why customers choose one brand over another or what creates competitive advantage. Without that grounding, AI operates as a fast executor rather than a strategic partner. It produces more, but it does not always help teams make better decisions.&lt;/p&gt;&lt;p&gt;This becomes even more visible inside complex marketing organizations where insights live in different corners of the business and rarely come together in a unified way.&lt;/p&gt;&lt;p&gt;As Grant McDougall, CEO of &lt;a href="https://www.blueocean.ai/?utm_source=VentureBeat&amp;amp;utm_medium=article&amp;amp;utm_campaign=q42025&amp;amp;utm_content=aicontext"&gt;BlueOcean&lt;/a&gt;, explains, “Inside large marketing organizations, the data is vertical. Digital has theirs, loyalty has theirs, content has theirs, media has theirs. But CMOs think horizontally. They need to combine customer insight, competitive movement, creative performance, and sales signals into one coherent view. Connecting that data fundamentally changes how decisions get made.”&lt;/p&gt;&lt;p&gt;This shift from vertical data to horizontal intelligence reflects a new phase in AI adoption. The emphasis is shifting from output volume to decision quality. Marketers are recognizing that the future of AI is intelligence that understands who you are as a company and why you matter to your customers.&lt;/p&gt;&lt;p&gt;In BlueOcean’s work with global brands across technology, healthcare, and consumer industries, including Amazon, Cisco, SAP, and Intel, the same pattern appears. Teams move faster and make better decisions when AI is grounded in structured brand and competitive context.&lt;/p&gt;&lt;h3&gt;Why context is becoming the critical ingredient&lt;/h3&gt;&lt;p&gt;Large language models excel at producing language. They do not inherently understand brand, meaning, or intention. This is why generic prompts often lead to generic outputs. The model executes based on statistical prediction, not strategic nuance.&lt;/p&gt;&lt;p&gt;Context changes that. When AI systems are supplied with structured inputs about brand strategy, audience insight, and creative intent, the output becomes sharper and more reliable. Recommendations become more specific. Creative stays on brief. The AI begins to act less like a content generator and more like a partner that understands the boundaries and goals of the business.&lt;/p&gt;&lt;p&gt;This shift mirrors a key theme from BlueOcean’s recent report, &lt;a href="https://www.blueocean.ai/blog/from-execution-to-intelligence-how-cmos-can-build-context-aware-ai-workflows"&gt;&lt;i&gt;Building Marketing Intelligence: The CMO Blueprint for Context-Aware AI&lt;/i&gt;&lt;/a&gt;. The report explains that AI is most effective when it is grounded in a clear frame of reference. CMOs who design these context-aware workflows see better performance, stronger creative, and more reliable decision-making.&lt;/p&gt;&lt;p&gt;For a deeper exploration of these principles, the full report is available &lt;a href="https://www.blueocean.ai/blog/from-execution-to-intelligence-how-cmos-can-build-context-aware-ai-workflows"&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;The industry’s pivot: From execution to understanding&lt;/h3&gt;&lt;p&gt;Many teams remain in an experimentation phase with AI. They test tools, run pilots, and explore new workflows. This creates productivity gains but not intelligence. Without shared context, every team uses AI differently, and the result is fragmentation.&lt;/p&gt;&lt;p&gt;The companies making the clearest progress treat context as a shared layer across workflows. When teams pull from the same brand strategy, insights, and creative guidance, AI becomes more predictable and more valuable. It supports decisions rather than contradicting them. This becomes especially effective when the context includes external signals such as shifts in sentiment, competitor movement, content performance, and broader category trends.&lt;/p&gt;&lt;p&gt;Brand-context AI connects brand identity, customer sentiment, competitive movement, and creative performance in a single environment. It strengthens workflows in practical ways: briefs become more strategic, content reviews more accurate, and insights faster because the system synthesizes patterns teams once assembled manually.&lt;/p&gt;&lt;p&gt;Across enterprise teams supported by BlueOcean, this shift consistently unlocks clarity. AI becomes a contributor to strategic understanding rather than a generator of disconnected output. With shared context in place, teams make more confident, coherent, and aligned decisions.&lt;/p&gt;&lt;h3&gt;Structured context: What it actually includes&lt;/h3&gt;&lt;p&gt;Structured context is the intelligence marketers already curate to understand how their brand shows up in the world. It brings together the narrative elements that shape the brand’s voice, the customer motivations that influence messaging, the competitive signals unfolding in the market, and the creative patterns that have historically performed. It also includes the external brand signals teams monitor every day: sentiment shifts, content dynamics, press and social movement, and how competitors position themselves across channels.&lt;/p&gt;&lt;p&gt;When this information is organized into a coherent frame, AI can interpret direction and creative choices with the same clarity strategists use. The value does not come from giving AI more data; it comes from giving it structure so it can reason through decisions the way marketers already do.&lt;/p&gt;&lt;h3&gt;The new division of labor between humans and AI&lt;/h3&gt;&lt;p&gt;The strongest AI-enabled marketing teams have one thing in common. They are clear about what humans own and what AI owns. Humans define purpose, strategy, and creative judgment. They understand emotion, cultural nuance, competitive meaning, and brand intent.&lt;/p&gt;&lt;p&gt;AI delivers speed, scale, and precision. It excels at synthesizing information, producing iterations, and following structured instruction.&lt;/p&gt;&lt;p&gt;“AI works best when it is given clear boundaries and clear intent,” says McDougall. “Humans set the direction led by creativity and imagination. AI executes with precision. That partnership is where the real value emerges.”&lt;/p&gt;&lt;p&gt;The systems that perform best are the ones guided by human-defined boundaries and human-led strategy. AI provides scale, but people provide meaning.&lt;/p&gt;&lt;p&gt;CMOs are recognizing that governing context is becoming a leadership responsibility. They already own brand, messaging, and customer insight. Extending this ownership into AI systems ensures the brand shows up consistently across every touchpoint, whether a human or a model produced the work.&lt;/p&gt;&lt;h3&gt;A practical example of context in action&lt;/h3&gt;&lt;p&gt;Consider a team preparing a global campaign. Without context, an AI system might generate copy that sounds polished but generic. It may overlook claims the brand can make, reference benefits competitors own, or ignore differentiators that matter most.  It may even amplify a competitor’s message simply because that language appears frequently in public data.&lt;/p&gt;&lt;p&gt;With structured context, the experience changes. The model understands the audience, the brand tone, the competitive landscape, and the objective. It knows which competitors are gaining attention, which messages resonate in the market, and where the brand has permission to play. It can propose angles that strengthen positioning rather than dilute it. It can generate variations that stay on brief and avoid competitor-owned territory.&lt;/p&gt;&lt;p&gt;BlueOcean has observed this shift inside enterprise teams including Amazon, Intel, and SAP, where structured brand and competitive context has improved alignment and reduced drift at scale.&lt;/p&gt;&lt;p&gt;Creative, brand, and competitive signals are no longer separate inputs. When they are connected and contextualized, AI begins supporting decision-making in a meaningful way. The technology stops producing output for its own sake and starts helping marketers understand where the brand stands and what actions will grow it.&lt;/p&gt;&lt;h3&gt;What comes next&lt;/h3&gt;&lt;p&gt;A new phase of AI is beginning. AI agents are evolving from task assistants to systems that collaborate across tools and workflows. As these systems become more capable, context will determine whether they behave unpredictably or perform as trusted extensions of the team.&lt;/p&gt;&lt;p&gt;Brand-context AI provides a path forward. It gives AI systems the structure they need to operate consistently. It supports the teams responsible for protecting brand integrity. In practice, these agents can already assemble context-aware creative briefs, review content for competitive and brand alignment, monitor shifts in category messaging, and synthesize insights across products or markets. It creates intelligence that adapts rather than overwhelms.&lt;/p&gt;&lt;p&gt;In the coming years, success will not come from producing more content, but from producing content anchored in brand context, the kind that sharpens decisions, strengthens positioning, and drives long-term growth.&lt;/p&gt;&lt;p&gt;The companies that build on context today will define the generative enterprise of tomorrow. BlueOcean is helping leading enterprises shape the next generation of context-aware AI systems.&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact &lt;/i&gt;&lt;a href="mailto:sales@venturebeat.com"&gt;&lt;i&gt;&lt;u&gt;sales@venturebeat.com&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/brand-context-ai-the-missing-requirement-for-marketing-ai</guid><pubDate>Tue, 09 Dec 2025 08:00:00 +0000</pubDate></item><item><title>Newsweek: Building AI-resilience for the next era of information (AI News)</title><link>https://www.artificialintelligence-news.com/news/newsweek-building-ai-resilience-for-the-next-era-of-information/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/pexels-markusspiske-330771-scaled.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Artificial intelligence is transforming the way information is created, summarised, and delivered. For publishers, the shift is already visible. Search engines provide AI-generated overviews, users get answers without clicking, and content is scraped by large language models that train on decades of journalism.&lt;/p&gt;&lt;p&gt;In this environment one question remains: How does a publisher survive when the traditional rules of distribution fall apart? Dev Pragad, the CEO of Newsweek, is offering one of the clearest answers.&lt;/p&gt;&lt;p&gt;Pragad’s strategy begins with an acknowledgement of reality. In his view, publishers need to accept the search-driven traffic model that defined the digital era is no longer dependable. AI-powered answer engines are restructuring the way users interact with information. A user might ask a question, receive a summary generated by an LLM, and never visit the publisher’s website. Page views become unpredictable, programmatic advertising becomes unstable, and legacy structures become vulnerable.&lt;/p&gt;&lt;p&gt;Rather than respond with fear, Dev Pragad has taken a proactive approach grounded in three core areas.&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Redesign the brand so that it remains visually strong in any context.&lt;/li&gt;&lt;li&gt;Diversify revenue so the business is not tied to a single distribution mechanism.&lt;/li&gt;&lt;li&gt;Expand those content formats that are less dependent on search engines and more aligned with the new habits of audiences.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In September 2025 Newsweek unveiled its redesigned identity under the tagline ‘A World Drawn Closer’. This redesign, created with 2×4, introduced a refined wordmark, a bold ‘N’ icon, and a unified visual system used for print, digital, video and international editions. For the AI era such a coherence matters. An AI summary might reference Newsweek visually, a feed might show a thumbnail with minimal space, and a social clip might require brand clarity in a fraction of a second.&lt;/p&gt;&lt;p&gt;The new design prepares Newsweek for the new reality by making the brand easy to identify.&lt;/p&gt;&lt;p&gt;The editorial shift under Dev Pragad is also significant. &lt;em&gt;Newsmakers&lt;/em&gt;, the series that features cultural leaders (Spike Lee, Liam Neeson, and Clark Hunt, for example), is available free on YouTube and digital platforms.&lt;/p&gt;&lt;p&gt;The decision to make the series accessible at no cost is strategic. Video that travels across platforms is harder for AI summaries to replace. It is more immersive, and it reaches audiences directly, plus it builds brand equity and cultural relevance beyond search traffic.&lt;/p&gt;&lt;p&gt;In interviews Pragad has said Newsmakers represents the future of journalism, blending storytelling, accessibility and platform fluency. Each episode is supported by a companion article and a collectable cover, creating a cross media footprint that is not reliant on one format or algorithm.&lt;/p&gt;&lt;p&gt;In addition to editorial innovation, Newsweek is evolving its business architecture to withstand AI driven disruption. While digital advertising remains part of the company’s revenue model, Pragad has expanded the title into events, direct advertising relationships, data driven rankings, and verticals such as healthcare. This approach creates multiple revenue streams that do not depend on unpredictable traffic patterns.&lt;/p&gt;&lt;p&gt;Another factor shaping Newsweek’s AI strategy is the way large language models scrape content. Newsweek monitors this activity through systems like TollBit which track bot behaviour and provide insight into how often AI engines attempt to access the site. Pragad has turned down licensing deals that undervalued the worth of Newsweek’s archives and has advocated for fair compensation for the use of publisher content. He believes publishers must negotiate collectively and maintain leverage rather than rush into agreements that minimise the value of their intellectual property.&lt;/p&gt;&lt;p&gt;The redesign is also in response to the challenge of brand recognition in a world dominated by fast-moving feeds and AI-driven surfaces. Clear typography, concise visual hierarchy, and a distinct colour palette support recognition across AI-generated snippets, smart devices, social networks, and search previews. This is a design built for the realities of the modern information economy.&lt;/p&gt;&lt;p&gt;Newsweek’s growth reflects the strength of these choices. The publication has been recognised as one of the fastest-rising digital news destinations in the US, and global audience numbers continue to climb. Although the company continues to evolve its revenue structure, its editorial mission remains grounded in fairness and trust. The new tagline reflects that commitment. Journalism brings the world closer when it is clear, accessible, and human-centred.&lt;/p&gt;&lt;p&gt;The AI revolution has placed publishers in a difficult position, yet it has also opened an opportunity. Those willing to rethink design, editorial formats, AI licensing, distribution, and revenue have the chance to define what comes next. Under Dev Pragad Newsweek is doing exactly that. The company is no longer relying on assumptions about how audiences discover information. It’s building a future in which journalism can coexist with AI, not be erased by it.&lt;/p&gt;&lt;p&gt;Dev Pragad has created a blueprint that demonstrates how a legacy publisher can reinvent itself for the AI age. Through design clarity, accessible cultural storytelling, diversified business models, and a firm stance on content value, he is positioning Newsweek not only to survive, but to lead in a world where information flows faster and more unpredictably than before. The result is a modern media entity built for a new era of intelligence, creativity, and connection.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/pexels-markusspiske-330771-scaled.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Artificial intelligence is transforming the way information is created, summarised, and delivered. For publishers, the shift is already visible. Search engines provide AI-generated overviews, users get answers without clicking, and content is scraped by large language models that train on decades of journalism.&lt;/p&gt;&lt;p&gt;In this environment one question remains: How does a publisher survive when the traditional rules of distribution fall apart? Dev Pragad, the CEO of Newsweek, is offering one of the clearest answers.&lt;/p&gt;&lt;p&gt;Pragad’s strategy begins with an acknowledgement of reality. In his view, publishers need to accept the search-driven traffic model that defined the digital era is no longer dependable. AI-powered answer engines are restructuring the way users interact with information. A user might ask a question, receive a summary generated by an LLM, and never visit the publisher’s website. Page views become unpredictable, programmatic advertising becomes unstable, and legacy structures become vulnerable.&lt;/p&gt;&lt;p&gt;Rather than respond with fear, Dev Pragad has taken a proactive approach grounded in three core areas.&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Redesign the brand so that it remains visually strong in any context.&lt;/li&gt;&lt;li&gt;Diversify revenue so the business is not tied to a single distribution mechanism.&lt;/li&gt;&lt;li&gt;Expand those content formats that are less dependent on search engines and more aligned with the new habits of audiences.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In September 2025 Newsweek unveiled its redesigned identity under the tagline ‘A World Drawn Closer’. This redesign, created with 2×4, introduced a refined wordmark, a bold ‘N’ icon, and a unified visual system used for print, digital, video and international editions. For the AI era such a coherence matters. An AI summary might reference Newsweek visually, a feed might show a thumbnail with minimal space, and a social clip might require brand clarity in a fraction of a second.&lt;/p&gt;&lt;p&gt;The new design prepares Newsweek for the new reality by making the brand easy to identify.&lt;/p&gt;&lt;p&gt;The editorial shift under Dev Pragad is also significant. &lt;em&gt;Newsmakers&lt;/em&gt;, the series that features cultural leaders (Spike Lee, Liam Neeson, and Clark Hunt, for example), is available free on YouTube and digital platforms.&lt;/p&gt;&lt;p&gt;The decision to make the series accessible at no cost is strategic. Video that travels across platforms is harder for AI summaries to replace. It is more immersive, and it reaches audiences directly, plus it builds brand equity and cultural relevance beyond search traffic.&lt;/p&gt;&lt;p&gt;In interviews Pragad has said Newsmakers represents the future of journalism, blending storytelling, accessibility and platform fluency. Each episode is supported by a companion article and a collectable cover, creating a cross media footprint that is not reliant on one format or algorithm.&lt;/p&gt;&lt;p&gt;In addition to editorial innovation, Newsweek is evolving its business architecture to withstand AI driven disruption. While digital advertising remains part of the company’s revenue model, Pragad has expanded the title into events, direct advertising relationships, data driven rankings, and verticals such as healthcare. This approach creates multiple revenue streams that do not depend on unpredictable traffic patterns.&lt;/p&gt;&lt;p&gt;Another factor shaping Newsweek’s AI strategy is the way large language models scrape content. Newsweek monitors this activity through systems like TollBit which track bot behaviour and provide insight into how often AI engines attempt to access the site. Pragad has turned down licensing deals that undervalued the worth of Newsweek’s archives and has advocated for fair compensation for the use of publisher content. He believes publishers must negotiate collectively and maintain leverage rather than rush into agreements that minimise the value of their intellectual property.&lt;/p&gt;&lt;p&gt;The redesign is also in response to the challenge of brand recognition in a world dominated by fast-moving feeds and AI-driven surfaces. Clear typography, concise visual hierarchy, and a distinct colour palette support recognition across AI-generated snippets, smart devices, social networks, and search previews. This is a design built for the realities of the modern information economy.&lt;/p&gt;&lt;p&gt;Newsweek’s growth reflects the strength of these choices. The publication has been recognised as one of the fastest-rising digital news destinations in the US, and global audience numbers continue to climb. Although the company continues to evolve its revenue structure, its editorial mission remains grounded in fairness and trust. The new tagline reflects that commitment. Journalism brings the world closer when it is clear, accessible, and human-centred.&lt;/p&gt;&lt;p&gt;The AI revolution has placed publishers in a difficult position, yet it has also opened an opportunity. Those willing to rethink design, editorial formats, AI licensing, distribution, and revenue have the chance to define what comes next. Under Dev Pragad Newsweek is doing exactly that. The company is no longer relying on assumptions about how audiences discover information. It’s building a future in which journalism can coexist with AI, not be erased by it.&lt;/p&gt;&lt;p&gt;Dev Pragad has created a blueprint that demonstrates how a legacy publisher can reinvent itself for the AI age. Through design clarity, accessible cultural storytelling, diversified business models, and a firm stance on content value, he is positioning Newsweek not only to survive, but to lead in a world where information flows faster and more unpredictably than before. The result is a modern media entity built for a new era of intelligence, creativity, and connection.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/newsweek-building-ai-resilience-for-the-next-era-of-information/</guid><pubDate>Tue, 09 Dec 2025 08:29:21 +0000</pubDate></item><item><title>How people really use AI: The surprising truth from analysing billions of interactions (AI News)</title><link>https://www.artificialintelligence-news.com/news/how-people-really-use-ai-the-surprising-truth-from-analysing-billions-of-interactions/</link><description>&lt;p&gt;For the past year, we’ve been told that artificial intelligence is&amp;nbsp;revolutionising&amp;nbsp;productivity—helping us write emails, generate code, and summarise documents. But what if the reality of how people actually use AI is completely different from what we’ve been led to believe?&lt;/p&gt;&lt;p&gt;A data-driven&amp;nbsp;study&amp;nbsp;by OpenRouter has just pulled back the curtain on real-world AI usage by&amp;nbsp;analysing&amp;nbsp;over 100 trillion tokens—essentially billions upon billions of conversations and interactions with large language models like ChatGPT, Claude, and dozens of others. The findings challenge many assumptions about the AI revolution.&lt;/p&gt;&lt;p&gt;​​OpenRouter is a multi-model AI inference platform that routes requests across more than 300 models from over 60 providers—from OpenAI and Anthropic to open-source alternatives like DeepSeek and Meta’s LLaMA.&amp;nbsp;&lt;/p&gt;&lt;p&gt;With over 50% of its usage originating outside the United States and serving millions of developers globally, the platform offers a unique cross-section of how AI is actually deployed across different geographies, use cases, and user types.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Importantly, the study&amp;nbsp;analysed&amp;nbsp;metadata from billions of interactions without accessing the actual text of conversations, preserving user privacy while revealing behavioural patterns.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-111217" height="677" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-3.36.21-PM-1024x677.png" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Open-source AI models have grown to capture approximately one-third of total usage by late 2025, with notable spikes following major releases.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-the-roleplay-revolution-nobody-saw-coming"&gt;The roleplay revolution nobody saw coming&lt;/h3&gt;&lt;p&gt;Perhaps the most surprising discovery: more than half of all open-source AI model usage isn’t for productivity at all. It’s for roleplay and creative storytelling.&lt;/p&gt;&lt;p&gt;Yes, you read that right. While tech executives tout AI’s potential to transform business, users are spending the majority of their time engaging in character-driven conversations, interactive fiction, and gaming scenarios.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Over 50% of open-source model interactions fall into this category, dwarfing even programming assistance.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-111218" height="576" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-3.38.37-PM-1024x576.png" width="1024" /&gt;&lt;/figure&gt;&lt;p&gt;“This counters an assumption that LLMs are mostly used for writing code, emails, or summaries,” the report states. “In reality, many users engage with these models for companionship or exploration.”&lt;/p&gt;&lt;p&gt;This isn’t just casual chatting. The data shows users treat AI models as structured roleplaying engines, with 60% of roleplay tokens falling under specific gaming scenarios and creative writing contexts. It’s a massive, largely invisible use case that’s reshaping how AI companies think about their products.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-programming-s-meteoric-rise"&gt;Programming’s meteoric rise&lt;/h3&gt;&lt;p&gt;While roleplay dominates open-source usage, programming has become the fastest-growing category across all AI models. At the start of 2025, coding-related queries accounted for just 11% of total AI usage. By the end of the year, that figure had exploded to over 50%.&lt;/p&gt;&lt;p&gt;This growth reflects AI’s deepening integration into software development. Average prompt lengths for programming tasks have grown fourfold, from around 1,500 tokens to over 6,000, with some code-related requests exceeding 20,000 tokens—roughly equivalent to feeding an entire codebase into an AI model for analysis.&lt;/p&gt;&lt;p&gt;For context, programming queries now generate some of the longest and most complex interactions in the entire AI ecosystem. Developers aren’t just asking for simple code snippets anymore; they’re conducting sophisticated debugging sessions, architectural reviews, and multi-step problem solving.&lt;/p&gt;&lt;p&gt;Anthropic’s Claude models dominate this space, capturing over 60% of programming-related usage for most of 2025, though competition is intensifying as Google, OpenAI, and open-source alternatives gain ground.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-111219" height="474" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-3.39.57-PM-1024x474.png" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Programming-related queries exploded from 11% of total AI usage in early 2025 to over 50% by year’s end.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-the-chinese-ai-surge"&gt;The Chinese AI surge&lt;/h3&gt;&lt;p&gt;Another major revelation: Chinese AI models now account for approximately 30% of global usage—nearly triple their 13% share at the start of 2025.&lt;/p&gt;&lt;p&gt;Models from DeepSeek, Qwen (Alibaba), and Moonshot AI have rapidly gained traction, with DeepSeek alone processing 14.37 trillion tokens during the study period. This represents a fundamental shift in the global AI landscape, where Western companies no longer hold unchallenged dominance.&lt;/p&gt;&lt;p&gt;Simplified Chinese is now the second-most common language for AI interactions globally at 5% of total usage, behind only English at 83%. Asia’s overall share of AI spending more than doubled from 13% to 31%, with Singapore emerging as the second-largest country by usage after the United States.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-111220" height="665" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-3.43.29-PM-1024x665.png" width="1024" /&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-the-rise-of-agentic-ai"&gt;The rise of “Agentic” AI&lt;/h3&gt;&lt;p&gt;The study introduces a concept that will define AI’s next phase: agentic inference. This means AI models are no longer just answering single questions—they’re executing multi-step tasks, calling external tools, and reasoning across extended conversations.&lt;/p&gt;&lt;p&gt;The share of AI interactions classified as “reasoning-optimised” jumped from nearly zero in early 2025 to over 50% by year’s end. This reflects a fundamental shift from AI as a text generator to AI as an autonomous agent capable of planning and execution.&lt;/p&gt;&lt;p&gt;“The median LLM request is no longer a simple question or isolated instruction,” the researchers explain. “Instead, it is part of a structured, agent-like loop, invoking external tools, reasoning over state, and persisting across longer contexts.”&lt;/p&gt;&lt;p&gt;Think of it this way: instead of asking AI to “write a function,” you’re now asking it to “debug this codebase, identify the performance bottleneck, and implement a solution”—and it can actually do it.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-glass-slipper-effect"&gt;The “Glass Slipper Effect”&lt;/h3&gt;&lt;p&gt;One of the study’s most fascinating insights relates to user retention. Researchers discovered what they call the Cinderella “Glass Slipper” effect—a phenomenon where AI models that are “first to solve” a critical problem create lasting user loyalty.&lt;/p&gt;&lt;p&gt;When a newly released model perfectly matches a previously unmet need—the metaphorical “glass slipper”—those early users stick around far longer than later adopters. For example, the June 2025 cohort of Google’s Gemini 2.5 Pro retained approximately 40% of users at month five, substantially higher than later cohorts.&lt;/p&gt;&lt;p&gt;This challenges conventional wisdom about AI competition. Being first matters, but specifically being first to solve a high-value problem creates a durable competitive advantage. Users embed these models into their workflows, making switching costly both technically and behaviorally.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-cost-doesn-t-matter-as-much-as-you-d-think"&gt;Cost doesn’t matter (as much as you’d think)&lt;/h3&gt;&lt;p&gt;Perhaps counterintuitively, the study reveals that AI usage is relatively price-inelastic. A 10% decrease in price corresponds to only about a 0.5-0.7% increase in usage.&lt;/p&gt;&lt;p&gt;Premium models from Anthropic and OpenAI command $2-35 per million tokens while maintaining high usage, while budget options like DeepSeek and Google’s Gemini Flash achieve similar scale at under $0.40 per million tokens. Both coexist successfully.&lt;/p&gt;&lt;p&gt;“The LLM market does not seem to behave like a commodity just yet,” the report concludes. “Users balance cost with reasoning quality, reliability, and breadth of capability.”&lt;/p&gt;&lt;p&gt;This means AI hasn’t become a race to the bottom on pricing. Quality, reliability, and capability still command premiums—at least for now.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-this-means-going-forward"&gt;What this means going forward&lt;/h3&gt;&lt;p&gt;The OpenRouter study paints a picture of real-world AI usage that’s far more nuanced than industry narratives suggest. Yes, AI is transforming programming and professional work. But it’s also creating entirely new categories of human-computer interaction through roleplay and creative applications.&lt;/p&gt;&lt;p&gt;The market is diversifying geographically, with China emerging as a major force. The technology is evolving from simple text generation to complex, multi-step reasoning. And user loyalty depends less on being first to market than on being first to truly solve a problem.&lt;/p&gt;&lt;p&gt;As the report notes, “ways in which people use LLMs do not always align with expectations and vary significantly country by country, state by state, use case by use case.”&lt;/p&gt;&lt;p&gt;Understanding these real-world patterns—not just benchmark scores or marketing claims—will be crucial as AI becomes further embedded in daily life. The gap between how we think AI is used and how it’s actually used is wider than most realise. This study helps close that gap.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;Deep Cogito v2: Open-source AI that hones its reasoning skills&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110949" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-18.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;For the past year, we’ve been told that artificial intelligence is&amp;nbsp;revolutionising&amp;nbsp;productivity—helping us write emails, generate code, and summarise documents. But what if the reality of how people actually use AI is completely different from what we’ve been led to believe?&lt;/p&gt;&lt;p&gt;A data-driven&amp;nbsp;study&amp;nbsp;by OpenRouter has just pulled back the curtain on real-world AI usage by&amp;nbsp;analysing&amp;nbsp;over 100 trillion tokens—essentially billions upon billions of conversations and interactions with large language models like ChatGPT, Claude, and dozens of others. The findings challenge many assumptions about the AI revolution.&lt;/p&gt;&lt;p&gt;​​OpenRouter is a multi-model AI inference platform that routes requests across more than 300 models from over 60 providers—from OpenAI and Anthropic to open-source alternatives like DeepSeek and Meta’s LLaMA.&amp;nbsp;&lt;/p&gt;&lt;p&gt;With over 50% of its usage originating outside the United States and serving millions of developers globally, the platform offers a unique cross-section of how AI is actually deployed across different geographies, use cases, and user types.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Importantly, the study&amp;nbsp;analysed&amp;nbsp;metadata from billions of interactions without accessing the actual text of conversations, preserving user privacy while revealing behavioural patterns.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-111217" height="677" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-3.36.21-PM-1024x677.png" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Open-source AI models have grown to capture approximately one-third of total usage by late 2025, with notable spikes following major releases.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-the-roleplay-revolution-nobody-saw-coming"&gt;The roleplay revolution nobody saw coming&lt;/h3&gt;&lt;p&gt;Perhaps the most surprising discovery: more than half of all open-source AI model usage isn’t for productivity at all. It’s for roleplay and creative storytelling.&lt;/p&gt;&lt;p&gt;Yes, you read that right. While tech executives tout AI’s potential to transform business, users are spending the majority of their time engaging in character-driven conversations, interactive fiction, and gaming scenarios.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Over 50% of open-source model interactions fall into this category, dwarfing even programming assistance.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-111218" height="576" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-3.38.37-PM-1024x576.png" width="1024" /&gt;&lt;/figure&gt;&lt;p&gt;“This counters an assumption that LLMs are mostly used for writing code, emails, or summaries,” the report states. “In reality, many users engage with these models for companionship or exploration.”&lt;/p&gt;&lt;p&gt;This isn’t just casual chatting. The data shows users treat AI models as structured roleplaying engines, with 60% of roleplay tokens falling under specific gaming scenarios and creative writing contexts. It’s a massive, largely invisible use case that’s reshaping how AI companies think about their products.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-programming-s-meteoric-rise"&gt;Programming’s meteoric rise&lt;/h3&gt;&lt;p&gt;While roleplay dominates open-source usage, programming has become the fastest-growing category across all AI models. At the start of 2025, coding-related queries accounted for just 11% of total AI usage. By the end of the year, that figure had exploded to over 50%.&lt;/p&gt;&lt;p&gt;This growth reflects AI’s deepening integration into software development. Average prompt lengths for programming tasks have grown fourfold, from around 1,500 tokens to over 6,000, with some code-related requests exceeding 20,000 tokens—roughly equivalent to feeding an entire codebase into an AI model for analysis.&lt;/p&gt;&lt;p&gt;For context, programming queries now generate some of the longest and most complex interactions in the entire AI ecosystem. Developers aren’t just asking for simple code snippets anymore; they’re conducting sophisticated debugging sessions, architectural reviews, and multi-step problem solving.&lt;/p&gt;&lt;p&gt;Anthropic’s Claude models dominate this space, capturing over 60% of programming-related usage for most of 2025, though competition is intensifying as Google, OpenAI, and open-source alternatives gain ground.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-111219" height="474" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-3.39.57-PM-1024x474.png" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Programming-related queries exploded from 11% of total AI usage in early 2025 to over 50% by year’s end.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-the-chinese-ai-surge"&gt;The Chinese AI surge&lt;/h3&gt;&lt;p&gt;Another major revelation: Chinese AI models now account for approximately 30% of global usage—nearly triple their 13% share at the start of 2025.&lt;/p&gt;&lt;p&gt;Models from DeepSeek, Qwen (Alibaba), and Moonshot AI have rapidly gained traction, with DeepSeek alone processing 14.37 trillion tokens during the study period. This represents a fundamental shift in the global AI landscape, where Western companies no longer hold unchallenged dominance.&lt;/p&gt;&lt;p&gt;Simplified Chinese is now the second-most common language for AI interactions globally at 5% of total usage, behind only English at 83%. Asia’s overall share of AI spending more than doubled from 13% to 31%, with Singapore emerging as the second-largest country by usage after the United States.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-111220" height="665" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-3.43.29-PM-1024x665.png" width="1024" /&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-the-rise-of-agentic-ai"&gt;The rise of “Agentic” AI&lt;/h3&gt;&lt;p&gt;The study introduces a concept that will define AI’s next phase: agentic inference. This means AI models are no longer just answering single questions—they’re executing multi-step tasks, calling external tools, and reasoning across extended conversations.&lt;/p&gt;&lt;p&gt;The share of AI interactions classified as “reasoning-optimised” jumped from nearly zero in early 2025 to over 50% by year’s end. This reflects a fundamental shift from AI as a text generator to AI as an autonomous agent capable of planning and execution.&lt;/p&gt;&lt;p&gt;“The median LLM request is no longer a simple question or isolated instruction,” the researchers explain. “Instead, it is part of a structured, agent-like loop, invoking external tools, reasoning over state, and persisting across longer contexts.”&lt;/p&gt;&lt;p&gt;Think of it this way: instead of asking AI to “write a function,” you’re now asking it to “debug this codebase, identify the performance bottleneck, and implement a solution”—and it can actually do it.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-glass-slipper-effect"&gt;The “Glass Slipper Effect”&lt;/h3&gt;&lt;p&gt;One of the study’s most fascinating insights relates to user retention. Researchers discovered what they call the Cinderella “Glass Slipper” effect—a phenomenon where AI models that are “first to solve” a critical problem create lasting user loyalty.&lt;/p&gt;&lt;p&gt;When a newly released model perfectly matches a previously unmet need—the metaphorical “glass slipper”—those early users stick around far longer than later adopters. For example, the June 2025 cohort of Google’s Gemini 2.5 Pro retained approximately 40% of users at month five, substantially higher than later cohorts.&lt;/p&gt;&lt;p&gt;This challenges conventional wisdom about AI competition. Being first matters, but specifically being first to solve a high-value problem creates a durable competitive advantage. Users embed these models into their workflows, making switching costly both technically and behaviorally.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-cost-doesn-t-matter-as-much-as-you-d-think"&gt;Cost doesn’t matter (as much as you’d think)&lt;/h3&gt;&lt;p&gt;Perhaps counterintuitively, the study reveals that AI usage is relatively price-inelastic. A 10% decrease in price corresponds to only about a 0.5-0.7% increase in usage.&lt;/p&gt;&lt;p&gt;Premium models from Anthropic and OpenAI command $2-35 per million tokens while maintaining high usage, while budget options like DeepSeek and Google’s Gemini Flash achieve similar scale at under $0.40 per million tokens. Both coexist successfully.&lt;/p&gt;&lt;p&gt;“The LLM market does not seem to behave like a commodity just yet,” the report concludes. “Users balance cost with reasoning quality, reliability, and breadth of capability.”&lt;/p&gt;&lt;p&gt;This means AI hasn’t become a race to the bottom on pricing. Quality, reliability, and capability still command premiums—at least for now.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-this-means-going-forward"&gt;What this means going forward&lt;/h3&gt;&lt;p&gt;The OpenRouter study paints a picture of real-world AI usage that’s far more nuanced than industry narratives suggest. Yes, AI is transforming programming and professional work. But it’s also creating entirely new categories of human-computer interaction through roleplay and creative applications.&lt;/p&gt;&lt;p&gt;The market is diversifying geographically, with China emerging as a major force. The technology is evolving from simple text generation to complex, multi-step reasoning. And user loyalty depends less on being first to market than on being first to truly solve a problem.&lt;/p&gt;&lt;p&gt;As the report notes, “ways in which people use LLMs do not always align with expectations and vary significantly country by country, state by state, use case by use case.”&lt;/p&gt;&lt;p&gt;Understanding these real-world patterns—not just benchmark scores or marketing claims—will be crucial as AI becomes further embedded in daily life. The gap between how we think AI is used and how it’s actually used is wider than most realise. This study helps close that gap.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;Deep Cogito v2: Open-source AI that hones its reasoning skills&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110949" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-18.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/how-people-really-use-ai-the-surprising-truth-from-analysing-billions-of-interactions/</guid><pubDate>Tue, 09 Dec 2025 09:00:00 +0000</pubDate></item><item><title>[NEW] The Download: a peek at AI’s future (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/12/09/1129029/the-download-a-peek-at-ais-future/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The State of AI: A vision of the world in 2030&amp;nbsp;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;There are huge gulfs of opinion when it comes to predicting the near-future impacts of generative AI. In one camp there are those who predict that over the next decade the impact of AI will exceed that of the Industrial Revolution—a 150-year period of economic and social upheaval so great that we still live in the world it wrought.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;At the other end of the scale we have team ‘Normal Technology’: experts who push back not only on these sorts of predictions but on their foundational worldview. That’s not how technology works, they argue.&lt;/p&gt; 
 &lt;p&gt;Advances at the cutting edge may come thick and fast, but change across the wider economy, and society as a whole, moves at human speed. Widespread adoption of new technologies can be slow; acceptance slower. AI will be no different. What should we make of these extremes?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Read the full conversation between MIT Technology Review’s senior AI editor Will Douglas Heaven and Tim Bradshaw, FT global tech correspondent, about where AI will go next, and what our world will look like in the next five years.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;This is the final edition of The State of AI, a collaboration between the Financial Times and MIT Technology Review.&amp;nbsp;Read the rest of the series, and if you want to keep up-to-date with what’s going on in the world of AI,&amp;nbsp;sign up&amp;nbsp;to receive our free Algorithm newsletter every Monday.&lt;/strong&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;How AI is changing the economy&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;There's a lot at stake when it comes to understanding how AI is changing the economy at large. What's the right outlook to have? Join Mat Honan, editor in chief, David Rotman, editor at large, and Richard Waters, FT columnist, at 1pm ET today to hear them discuss what's happening across industries and the market.&amp;nbsp;Sign up now&amp;nbsp;to be part of this exclusive subscriber-only event.&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Trump says he’ll sign an order blocking states from regulating AI&lt;/strong&gt;&lt;br /&gt;But he’s facing a lot of pushback, including from members of his own party. (CNN)&lt;br /&gt;+&lt;em&gt;&amp;nbsp;The whole debacle can be traced back to congressional inaction.&amp;nbsp;&lt;/em&gt;(Semafor)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 Google’s new smart glasses are getting rave reviews&amp;nbsp;&lt;/strong&gt;👓&lt;br /&gt;You’ll be able to get your hands on a pair in 2026. Watch out, Apple and Meta. (Tech Radar)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3 Trump gave the go-ahead for Nvidia to sell powerful AI chips to China&lt;/strong&gt;&lt;br /&gt;The US gets a 25% cut of the sales—but what does it lose longer-term? (WP&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;And how much could China stand to gain?&lt;/em&gt;&amp;nbsp;(NYT&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;How a top Chinese AI model overcame US sanctions.&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 America’s data center backlash is here&lt;/strong&gt;&lt;br /&gt;Republican and Democrat alike, local residents are sick of rapidly rising power bills. (Vox&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;More than 200 environmental groups are demanding a US-wide moratorium on new data centers.&lt;/em&gt;&amp;nbsp;(The Guardian)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;The data center boom in the desert.&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;5 A quarter of teens are turning to AI chatbots for mental health support&lt;/strong&gt;&lt;br /&gt;Given the lack of real-world help, can you really blame them? (The Guardian)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Therapists are secretly using ChatGPT. Clients are triggered&lt;/em&gt;. (MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;6 ICEBlock is suing the US government over its App Store removal&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Its creator is arguing that the Department of Justice’s demands to Apple violated his First Amendment rights. (404 Media)&lt;br /&gt;+&lt;em&gt;&amp;nbsp;It’s one of a number of ICE-tracking initiatives to be pulled by tech platforms this year.&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;7 This band quit Spotify, but it’s been replaced by AI knockoffs&lt;/strong&gt;&lt;br /&gt;The platform seems to be struggling against the tide of slop. (Futurism)&amp;nbsp;&lt;br /&gt;+&amp;nbsp;&lt;em&gt;AI is coming for music, too.&amp;nbsp;&lt;/em&gt;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;8 Think you’re immune to online ads? Think again&lt;/strong&gt;&lt;br /&gt;If you’re scrolling on social media, you’re being sold to. Relentlessly. (The Verge&amp;nbsp;$)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;9 People really do not like Microsoft Copilot&lt;/strong&gt;&lt;br /&gt;It’s like Clippy all over again, except it’s even less avoidable. (Quartz&amp;nbsp;$)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10 The longest solar eclipse for 100 years is coming&lt;/strong&gt;&lt;br /&gt;And we’ll only have to wait until 2027 to see it! (Wired&amp;nbsp;$)&lt;/p&gt; 

   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;&lt;strong&gt;“Governments and MPs are shooting themselves in the foot by pandering to tech giants, because that just tells young people that they don’t care about our future.”&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Adele Zeynep Walton, founding member of online safety campaign group Ctrl+Alt+Reclaim, tells&amp;nbsp;The Guardian&amp;nbsp;why young activists are taking matters into their own hands.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="fleet of ships at sea" class="wp-image-1108743" src="https://wp.technologyreview.com/wp-content/uploads/2024/12/Armada-high-res.jpg" /&gt;&lt;div class="image-credit"&gt;COURTESY OF OCEANBIRD&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;&lt;strong&gt;Inside the long quest to advance Chinese writing technology&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Every second of every day, someone is typing in Chinese. Though the mechanics look a little different from typing in English—people usually type the pronunciation of a character and then pick it out of a selection that pops up, autocomplete-style—it’s hard to think of anything more quotidian. The software that allows this exists beneath the awareness of pretty much everyone who uses it. It’s just there.&lt;/p&gt;&lt;p&gt;What’s largely been forgotten is that a large cast of eccentrics and linguists, engineers and polymaths, spent much of the 20th century torturing themselves over how Chinese was ever going to move away from the ink brush to any other medium.&amp;nbsp;Read the full story.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;—Veronique Greenwood&lt;/em&gt;&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Pantone chose a ‘calming’&amp;nbsp;shade of white&amp;nbsp;for its Color of 2026… and people are fuming.&amp;nbsp;&lt;br /&gt;+ Ozempic needles on the Christmas tree, anyone?&amp;nbsp;Here’s why&amp;nbsp;we’re going crazy for weird baubles.&amp;nbsp;&lt;br /&gt;+ Can relate to&amp;nbsp;this baby seal&amp;nbsp;for instinctively heading to the nearest pub.&lt;br /&gt;+ Thrilled to see One Battle After Another get so many Golden Globes&amp;nbsp;nominations.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The State of AI: A vision of the world in 2030&amp;nbsp;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;There are huge gulfs of opinion when it comes to predicting the near-future impacts of generative AI. In one camp there are those who predict that over the next decade the impact of AI will exceed that of the Industrial Revolution—a 150-year period of economic and social upheaval so great that we still live in the world it wrought.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;At the other end of the scale we have team ‘Normal Technology’: experts who push back not only on these sorts of predictions but on their foundational worldview. That’s not how technology works, they argue.&lt;/p&gt; 
 &lt;p&gt;Advances at the cutting edge may come thick and fast, but change across the wider economy, and society as a whole, moves at human speed. Widespread adoption of new technologies can be slow; acceptance slower. AI will be no different. What should we make of these extremes?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Read the full conversation between MIT Technology Review’s senior AI editor Will Douglas Heaven and Tim Bradshaw, FT global tech correspondent, about where AI will go next, and what our world will look like in the next five years.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;This is the final edition of The State of AI, a collaboration between the Financial Times and MIT Technology Review.&amp;nbsp;Read the rest of the series, and if you want to keep up-to-date with what’s going on in the world of AI,&amp;nbsp;sign up&amp;nbsp;to receive our free Algorithm newsletter every Monday.&lt;/strong&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;How AI is changing the economy&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;There's a lot at stake when it comes to understanding how AI is changing the economy at large. What's the right outlook to have? Join Mat Honan, editor in chief, David Rotman, editor at large, and Richard Waters, FT columnist, at 1pm ET today to hear them discuss what's happening across industries and the market.&amp;nbsp;Sign up now&amp;nbsp;to be part of this exclusive subscriber-only event.&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Trump says he’ll sign an order blocking states from regulating AI&lt;/strong&gt;&lt;br /&gt;But he’s facing a lot of pushback, including from members of his own party. (CNN)&lt;br /&gt;+&lt;em&gt;&amp;nbsp;The whole debacle can be traced back to congressional inaction.&amp;nbsp;&lt;/em&gt;(Semafor)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 Google’s new smart glasses are getting rave reviews&amp;nbsp;&lt;/strong&gt;👓&lt;br /&gt;You’ll be able to get your hands on a pair in 2026. Watch out, Apple and Meta. (Tech Radar)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3 Trump gave the go-ahead for Nvidia to sell powerful AI chips to China&lt;/strong&gt;&lt;br /&gt;The US gets a 25% cut of the sales—but what does it lose longer-term? (WP&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;And how much could China stand to gain?&lt;/em&gt;&amp;nbsp;(NYT&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;How a top Chinese AI model overcame US sanctions.&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 America’s data center backlash is here&lt;/strong&gt;&lt;br /&gt;Republican and Democrat alike, local residents are sick of rapidly rising power bills. (Vox&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;More than 200 environmental groups are demanding a US-wide moratorium on new data centers.&lt;/em&gt;&amp;nbsp;(The Guardian)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;The data center boom in the desert.&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;5 A quarter of teens are turning to AI chatbots for mental health support&lt;/strong&gt;&lt;br /&gt;Given the lack of real-world help, can you really blame them? (The Guardian)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Therapists are secretly using ChatGPT. Clients are triggered&lt;/em&gt;. (MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;6 ICEBlock is suing the US government over its App Store removal&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Its creator is arguing that the Department of Justice’s demands to Apple violated his First Amendment rights. (404 Media)&lt;br /&gt;+&lt;em&gt;&amp;nbsp;It’s one of a number of ICE-tracking initiatives to be pulled by tech platforms this year.&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;7 This band quit Spotify, but it’s been replaced by AI knockoffs&lt;/strong&gt;&lt;br /&gt;The platform seems to be struggling against the tide of slop. (Futurism)&amp;nbsp;&lt;br /&gt;+&amp;nbsp;&lt;em&gt;AI is coming for music, too.&amp;nbsp;&lt;/em&gt;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;8 Think you’re immune to online ads? Think again&lt;/strong&gt;&lt;br /&gt;If you’re scrolling on social media, you’re being sold to. Relentlessly. (The Verge&amp;nbsp;$)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;9 People really do not like Microsoft Copilot&lt;/strong&gt;&lt;br /&gt;It’s like Clippy all over again, except it’s even less avoidable. (Quartz&amp;nbsp;$)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10 The longest solar eclipse for 100 years is coming&lt;/strong&gt;&lt;br /&gt;And we’ll only have to wait until 2027 to see it! (Wired&amp;nbsp;$)&lt;/p&gt; 

   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;&lt;strong&gt;“Governments and MPs are shooting themselves in the foot by pandering to tech giants, because that just tells young people that they don’t care about our future.”&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Adele Zeynep Walton, founding member of online safety campaign group Ctrl+Alt+Reclaim, tells&amp;nbsp;The Guardian&amp;nbsp;why young activists are taking matters into their own hands.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="fleet of ships at sea" class="wp-image-1108743" src="https://wp.technologyreview.com/wp-content/uploads/2024/12/Armada-high-res.jpg" /&gt;&lt;div class="image-credit"&gt;COURTESY OF OCEANBIRD&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;&lt;strong&gt;Inside the long quest to advance Chinese writing technology&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Every second of every day, someone is typing in Chinese. Though the mechanics look a little different from typing in English—people usually type the pronunciation of a character and then pick it out of a selection that pops up, autocomplete-style—it’s hard to think of anything more quotidian. The software that allows this exists beneath the awareness of pretty much everyone who uses it. It’s just there.&lt;/p&gt;&lt;p&gt;What’s largely been forgotten is that a large cast of eccentrics and linguists, engineers and polymaths, spent much of the 20th century torturing themselves over how Chinese was ever going to move away from the ink brush to any other medium.&amp;nbsp;Read the full story.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;—Veronique Greenwood&lt;/em&gt;&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Pantone chose a ‘calming’&amp;nbsp;shade of white&amp;nbsp;for its Color of 2026… and people are fuming.&amp;nbsp;&lt;br /&gt;+ Ozempic needles on the Christmas tree, anyone?&amp;nbsp;Here’s why&amp;nbsp;we’re going crazy for weird baubles.&amp;nbsp;&lt;br /&gt;+ Can relate to&amp;nbsp;this baby seal&amp;nbsp;for instinctively heading to the nearest pub.&lt;br /&gt;+ Thrilled to see One Battle After Another get so many Golden Globes&amp;nbsp;nominations.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/12/09/1129029/the-download-a-peek-at-ais-future/</guid><pubDate>Tue, 09 Dec 2025 13:10:00 +0000</pubDate></item><item><title>[NEW] Empromptu raises $2M pre-seed to help enterprises build AI apps (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/empromptu-raises-2m-pre-seed-to-help-enterprises-build-ai-apps/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/06/GettyImages-1410815886.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Shanea Leven says she learned two important lessons when building her first company,&amp;nbsp;CodeSee. The first lesson was knowing the difference between what businesses need versus what sounds visionary; the second was that the fundamentals always apply, even with new technologies such as AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Security, compliance, reliability, quality, those things don’t just go away for enterprise applications,” she said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;After CodeSee was acquired in 2024, Leven decided that she wanted to build a product that would let business owners, even those without technical backgrounds, build AI applications. She teamed up with AI researcher Sean Robinson, and last October, the two launched&amp;nbsp;Empromptu,&amp;nbsp;an AI service that businesses can use to build AI applications.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Empromptu claims all a user has to do is tell the platform’s AI chatbot what they want — like a new classification app or a generative recommendation app — and the tool will go ahead and build it. It also provides LLM tools to help users if they want to fine-tune any results and lets companies add AI features to their own existing code bases.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Leven doesn’t consider it a vibe-coding platform, though she does look to compete with companies like Replit and Lovable.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Vibe coding is excellent for quick experiments, but Empromptu is what turns those experiments into real software,” she said. Empromptu, she continued, “turns ideas into production features with built-in evaluation, governance, and self-improvement. You ship to real customers, with real data and complete control. If vibe coding is the brainstorm, Empromptu is the build.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Tuesday,&amp;nbsp;the company&amp;nbsp;said it had raised $2 million in a pre-seed funding round led by Precursor Ventures.&amp;nbsp;Zeal Capital, Alumni Ventures, FoundersEdge, and South Loop also participated.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Leven said the fresh capital will be used for hiring staff and developing new proprietary technology. It also announced three new features, including the ability to create custom data models and infinite memory. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is hoping to target businesses launching in regulated industries or “deeply complex” areas that involve capturing data and creating applications — software that services hotels, for example.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Overall, Leven hopes that founders feel their businesses can be transformed without having to learn the technical skills to take advantage of the AI revolution.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“It’s just like any other skill,” Leven said. “And the beauty of this skill is that AI can help you learn it along the way.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This piece was updated to clarify what Empromptu does. &lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/06/GettyImages-1410815886.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Shanea Leven says she learned two important lessons when building her first company,&amp;nbsp;CodeSee. The first lesson was knowing the difference between what businesses need versus what sounds visionary; the second was that the fundamentals always apply, even with new technologies such as AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Security, compliance, reliability, quality, those things don’t just go away for enterprise applications,” she said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;After CodeSee was acquired in 2024, Leven decided that she wanted to build a product that would let business owners, even those without technical backgrounds, build AI applications. She teamed up with AI researcher Sean Robinson, and last October, the two launched&amp;nbsp;Empromptu,&amp;nbsp;an AI service that businesses can use to build AI applications.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Empromptu claims all a user has to do is tell the platform’s AI chatbot what they want — like a new classification app or a generative recommendation app — and the tool will go ahead and build it. It also provides LLM tools to help users if they want to fine-tune any results and lets companies add AI features to their own existing code bases.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Leven doesn’t consider it a vibe-coding platform, though she does look to compete with companies like Replit and Lovable.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Vibe coding is excellent for quick experiments, but Empromptu is what turns those experiments into real software,” she said. Empromptu, she continued, “turns ideas into production features with built-in evaluation, governance, and self-improvement. You ship to real customers, with real data and complete control. If vibe coding is the brainstorm, Empromptu is the build.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Tuesday,&amp;nbsp;the company&amp;nbsp;said it had raised $2 million in a pre-seed funding round led by Precursor Ventures.&amp;nbsp;Zeal Capital, Alumni Ventures, FoundersEdge, and South Loop also participated.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Leven said the fresh capital will be used for hiring staff and developing new proprietary technology. It also announced three new features, including the ability to create custom data models and infinite memory. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is hoping to target businesses launching in regulated industries or “deeply complex” areas that involve capturing data and creating applications — software that services hotels, for example.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Overall, Leven hopes that founders feel their businesses can be transformed without having to learn the technical skills to take advantage of the AI revolution.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“It’s just like any other skill,” Leven said. “And the beauty of this skill is that AI can help you learn it along the way.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This piece was updated to clarify what Empromptu does. &lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/empromptu-raises-2m-pre-seed-to-help-enterprises-build-ai-apps/</guid><pubDate>Tue, 09 Dec 2025 13:51:22 +0000</pubDate></item><item><title>[NEW] Mistral AI surfs vibe-coding tailwinds with new coding models (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/mistral-ai-surfs-vibe-coding-tailwinds-with-new-coding-models/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/GettyImages-2147859992-e1713960898378.webp?resize=1200,676" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;French AI startup Mistral today launched Devstral 2, a new generation of its AI model designed for coding, as the company seeks to catch up to bigger AI labs like Anthropic and other coding-focused LLMs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This announcement follows the recent launch of the Mistral 3 family of open-weight models and confirms Mistral’s intent to close in on its bigger and better-funded AI rivals.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The unicorn is also jumping into the vibe-coding race, which has fueled the rise of companies like Cursor and Supabase, with Mistral Vibe, a new command-line interface (CLI) aimed at facilitating code automation through natural language, with tools for file manipulation, code searching, version control, and command execution.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mistral AI is betting on the added value of context awareness, which is particularly relevant in business use cases. Similar to its AI assistant, Le Chat, which can remember previous conversations with users and use that context to guide its answers, Vibe CLI features persistent history and can scan file structures and Git statuses to build context to inform its behavior.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This focus on production-grade workflows also explains why Devstral 2 is relatively demanding, requiring at least four H100 GPUs or equivalent for deployment, and weighing 123 billion parameters. However, the model is also available in a smaller size with Devstral Small, which, at 24 billion parameters, makes it deployable locally on consumer hardware.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The models differ in their open source licensing — Devstral 2 ships under a modified MIT license, while Devstral Small uses Apache 2.0.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;They also differ in pricing. Devstral 2 is currently free to use via the company’s API. After the free period, the API pricing will cost $0.40/$2.00 per million tokens (input/output) for Devstral 2, and $0.10/$0.30 for Devstral Small.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Mistral has partnered with agent tools Kilo Code and Cline to release Devstral 2 to users, while Mistral Vibe CLI is available as an extension in Zed for use inside the IDE.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Europe’s champion AI lab, Mistral is currently valued at €11.7 billion (approximately $13.8 billion) following a Series C funding round led by Dutch semiconductor company ASML, which invested €1.3 billion (approximately $1.5 billion) in September.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/GettyImages-2147859992-e1713960898378.webp?resize=1200,676" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;French AI startup Mistral today launched Devstral 2, a new generation of its AI model designed for coding, as the company seeks to catch up to bigger AI labs like Anthropic and other coding-focused LLMs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This announcement follows the recent launch of the Mistral 3 family of open-weight models and confirms Mistral’s intent to close in on its bigger and better-funded AI rivals.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The unicorn is also jumping into the vibe-coding race, which has fueled the rise of companies like Cursor and Supabase, with Mistral Vibe, a new command-line interface (CLI) aimed at facilitating code automation through natural language, with tools for file manipulation, code searching, version control, and command execution.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mistral AI is betting on the added value of context awareness, which is particularly relevant in business use cases. Similar to its AI assistant, Le Chat, which can remember previous conversations with users and use that context to guide its answers, Vibe CLI features persistent history and can scan file structures and Git statuses to build context to inform its behavior.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This focus on production-grade workflows also explains why Devstral 2 is relatively demanding, requiring at least four H100 GPUs or equivalent for deployment, and weighing 123 billion parameters. However, the model is also available in a smaller size with Devstral Small, which, at 24 billion parameters, makes it deployable locally on consumer hardware.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The models differ in their open source licensing — Devstral 2 ships under a modified MIT license, while Devstral Small uses Apache 2.0.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;They also differ in pricing. Devstral 2 is currently free to use via the company’s API. After the free period, the API pricing will cost $0.40/$2.00 per million tokens (input/output) for Devstral 2, and $0.10/$0.30 for Devstral Small.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Mistral has partnered with agent tools Kilo Code and Cline to release Devstral 2 to users, while Mistral Vibe CLI is available as an extension in Zed for use inside the IDE.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Europe’s champion AI lab, Mistral is currently valued at €11.7 billion (approximately $13.8 billion) following a Series C funding round led by Dutch semiconductor company ASML, which invested €1.3 billion (approximately $1.5 billion) in September.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/mistral-ai-surfs-vibe-coding-tailwinds-with-new-coding-models/</guid><pubDate>Tue, 09 Dec 2025 14:45:00 +0000</pubDate></item><item><title>[NEW] OpenAI targets AI skills gap with new certification standards (AI News)</title><link>https://www.artificialintelligence-news.com/news/openai-targets-ai-skills-gap-with-new-certification-standards/</link><description>&lt;p&gt;Adoption of generative AI has outpaced workforce capability, prompting OpenAI to target the skills gap with new certification standards.&lt;/p&gt;&lt;p&gt;While it’s safe to say OpenAI’s tools have reached mass adoption, organisations struggle to convert this usage into reliable output. To address this, OpenAI has announced ‘AI Foundations,’ a structured initiative designed to standardise how employees learn and apply the technology.&lt;/p&gt;&lt;p&gt;OpenAI’s initiative marks a necessary evolution in the vendor ecosystem; indicating a departure from the “move fast” phase of experimental deployment toward a focus on verifiable competence. OpenAI explicitly states its intention to certify 10 million Americans by 2030.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-workers-and-employers-have-an-incentive-to-close-the-ai-skills-gap"&gt;Workers and employers have an incentive to close the AI skills gap&lt;/h3&gt;&lt;p&gt;The economic case for AI training and certification is rooted in wage and productivity data. Workers possessing AI skills earn approximately 50 percent more than those without them. However, CIOs often find that productivity gains on paper fail to materialise in practice. OpenAI notes that gains “only materialise when people have the skills to use the technology.”&lt;/p&gt;&lt;p&gt;Without guidance, widespread access can create operational risk. OpenAI admits the technology is “disruptive, leaving many people unsure which skills matter most.” By defining a standard curriculum, OpenAI aims to help organisations capture the efficiency gains promised by their software investments.&lt;/p&gt;&lt;p&gt;The delivery method for AI Foundations differs from traditional corporate LMS (Learning Management System) modules. The course sits directly inside ChatGPT, allowing the platform to act as “tutor, the practice space, and the feedback loop” simultaneously. This integration allows learners to execute real tasks and receive context-aware corrections to help close the AI skills gap, rather than just watching passive video content.&lt;/p&gt;&lt;p&gt;Completing the programme yields a badge verifying “job-ready AI skills”. This credential serves as a stepping stone toward a full OpenAI Certification. To ensure these badges carry weight in the labour market, OpenAI has engaged Coursera, ETS, and Credly by Pearson to validate the psychometric rigour and design of the assessments.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-operational-pilots-for-the-ai-certification-and-improving-the-hiring-pipeline"&gt;Operational pilots for the AI certification and improving the hiring pipeline&lt;/h3&gt;&lt;p&gt;A consortium of large-scale employers and public-sector bodies will test the curriculum before a wider rollout. Pilot partners include Walmart, John Deere, Lowe’s, Boston Consulting Group, Russell Reynolds Associates, Upwork, Elevance Health, and Accenture. The Office of the Governor of Delaware is also participating, which shows interest from state-level administration.&lt;/p&gt;&lt;p&gt;These partners span industries with heavy operational footprints (including retail, agriculture, and healthcare) suggesting the training targets core business functions rather than just technical roles. OpenAI plans to use the next few months to refine the course based on data from these pilots to ensure that it can effectively close the AI skills gap.&lt;/p&gt;&lt;p&gt;OpenAI’s initiative extends into recruitment. The company is developing an ‘OpenAI Jobs Platform’ to connect certified workers with employers. Partnerships with Indeed and Upwork support this mechanism, aiming to make it easier for businesses to identify candidates with verified technical expertise.&lt;/p&gt;&lt;p&gt;For hiring managers, this offers a potential solution to the difficulty of vetting AI literacy. A standardised AI certification could reduce the reliance on self-reported skills, providing “portable evidence” of a candidate’s development.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-academic-alignment-to-seed-future-ai-talent"&gt;Academic alignment to seed future AI talent&lt;/h3&gt;&lt;p&gt;While the enterprise focus is immediate, OpenAI is also seeding the future talent pipeline. A ‘ChatGPT Foundations for Teachers’ course has launched on Coursera. With three in five teachers already using AI tools to save time and personalise materials, this stream aims to formalise existing habits.&lt;/p&gt;&lt;p&gt;Simultaneously, pilots with Arizona State University and the California State University system are creating pathways for students to certify their skills before entering the job market. This ensures that the next wave of graduates arrives with the “job-ready” verification that enterprise employers are beginning to demand.&lt;/p&gt;&lt;p&gt;Organisations must now decide whether to rely on vendor-supplied certification or continue developing proprietary training. The involvement of firms like Boston Consulting Group and Accenture implies that major players see value in a standardised external benchmark.&lt;/p&gt;&lt;p&gt;As OpenAI moves to certify millions of people and close the AI skills gap, the certification badge may become a baseline expectation for knowledge workers much like office suite proficiency in previous decades.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Instacart pilots agentic commerce by embedding in ChatGPT&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110949" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-18.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Adoption of generative AI has outpaced workforce capability, prompting OpenAI to target the skills gap with new certification standards.&lt;/p&gt;&lt;p&gt;While it’s safe to say OpenAI’s tools have reached mass adoption, organisations struggle to convert this usage into reliable output. To address this, OpenAI has announced ‘AI Foundations,’ a structured initiative designed to standardise how employees learn and apply the technology.&lt;/p&gt;&lt;p&gt;OpenAI’s initiative marks a necessary evolution in the vendor ecosystem; indicating a departure from the “move fast” phase of experimental deployment toward a focus on verifiable competence. OpenAI explicitly states its intention to certify 10 million Americans by 2030.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-workers-and-employers-have-an-incentive-to-close-the-ai-skills-gap"&gt;Workers and employers have an incentive to close the AI skills gap&lt;/h3&gt;&lt;p&gt;The economic case for AI training and certification is rooted in wage and productivity data. Workers possessing AI skills earn approximately 50 percent more than those without them. However, CIOs often find that productivity gains on paper fail to materialise in practice. OpenAI notes that gains “only materialise when people have the skills to use the technology.”&lt;/p&gt;&lt;p&gt;Without guidance, widespread access can create operational risk. OpenAI admits the technology is “disruptive, leaving many people unsure which skills matter most.” By defining a standard curriculum, OpenAI aims to help organisations capture the efficiency gains promised by their software investments.&lt;/p&gt;&lt;p&gt;The delivery method for AI Foundations differs from traditional corporate LMS (Learning Management System) modules. The course sits directly inside ChatGPT, allowing the platform to act as “tutor, the practice space, and the feedback loop” simultaneously. This integration allows learners to execute real tasks and receive context-aware corrections to help close the AI skills gap, rather than just watching passive video content.&lt;/p&gt;&lt;p&gt;Completing the programme yields a badge verifying “job-ready AI skills”. This credential serves as a stepping stone toward a full OpenAI Certification. To ensure these badges carry weight in the labour market, OpenAI has engaged Coursera, ETS, and Credly by Pearson to validate the psychometric rigour and design of the assessments.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-operational-pilots-for-the-ai-certification-and-improving-the-hiring-pipeline"&gt;Operational pilots for the AI certification and improving the hiring pipeline&lt;/h3&gt;&lt;p&gt;A consortium of large-scale employers and public-sector bodies will test the curriculum before a wider rollout. Pilot partners include Walmart, John Deere, Lowe’s, Boston Consulting Group, Russell Reynolds Associates, Upwork, Elevance Health, and Accenture. The Office of the Governor of Delaware is also participating, which shows interest from state-level administration.&lt;/p&gt;&lt;p&gt;These partners span industries with heavy operational footprints (including retail, agriculture, and healthcare) suggesting the training targets core business functions rather than just technical roles. OpenAI plans to use the next few months to refine the course based on data from these pilots to ensure that it can effectively close the AI skills gap.&lt;/p&gt;&lt;p&gt;OpenAI’s initiative extends into recruitment. The company is developing an ‘OpenAI Jobs Platform’ to connect certified workers with employers. Partnerships with Indeed and Upwork support this mechanism, aiming to make it easier for businesses to identify candidates with verified technical expertise.&lt;/p&gt;&lt;p&gt;For hiring managers, this offers a potential solution to the difficulty of vetting AI literacy. A standardised AI certification could reduce the reliance on self-reported skills, providing “portable evidence” of a candidate’s development.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-academic-alignment-to-seed-future-ai-talent"&gt;Academic alignment to seed future AI talent&lt;/h3&gt;&lt;p&gt;While the enterprise focus is immediate, OpenAI is also seeding the future talent pipeline. A ‘ChatGPT Foundations for Teachers’ course has launched on Coursera. With three in five teachers already using AI tools to save time and personalise materials, this stream aims to formalise existing habits.&lt;/p&gt;&lt;p&gt;Simultaneously, pilots with Arizona State University and the California State University system are creating pathways for students to certify their skills before entering the job market. This ensures that the next wave of graduates arrives with the “job-ready” verification that enterprise employers are beginning to demand.&lt;/p&gt;&lt;p&gt;Organisations must now decide whether to rely on vendor-supplied certification or continue developing proprietary training. The involvement of firms like Boston Consulting Group and Accenture implies that major players see value in a standardised external benchmark.&lt;/p&gt;&lt;p&gt;As OpenAI moves to certify millions of people and close the AI skills gap, the certification badge may become a baseline expectation for knowledge workers much like office suite proficiency in previous decades.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Instacart pilots agentic commerce by embedding in ChatGPT&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110949" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-18.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/openai-targets-ai-skills-gap-with-new-certification-standards/</guid><pubDate>Tue, 09 Dec 2025 14:45:06 +0000</pubDate></item><item><title>[NEW] Pebble’s founder introduces a $75 AI smart ring for recording brief notes with a press of a button (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/pebbles-founder-introduces-a-75-ai-smart-ring-for-recording-brief-notes-with-a-press-of-a-button/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After rebooting the Pebble smartwatch brand, founder Eric Migicovsky is expanding his company’s device lineup with a new smart wearable: an AI-powered smart ring known as Index 01. Named for the finger where the ring is meant to be worn, the new $75 ring is not meant to be a competitor to always-on, always-listening AI devices, like the AI pendant Friend, but instead offers a way to record quick notes and reminders with a press of a button on the ring’s side.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI only comes into play via the open source, speech-to-text, and AI models that run locally on your smartphone through the Pebble mobile app. That is, if the ring’s button is not being pressed, it’s not recording. (And this is a press-and-&lt;em&gt;hold&lt;/em&gt; gesture, too, which means you can’t start the ring’s recording and then let go to surreptitiously record a conversation.)&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3073863" height="680" src="https://techcrunch.com/wp-content/uploads/2025/12/Index01-polished-gold-rocks.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Core Devices LLC&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;You can wear the stainless steel ring while in the shower, washing hands, doing dishes, or in the rain, but you have to take it off for other water-related activities, like swimming. At launch, it’s water-resistant to 1 meter.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The ring is also not a fitness tracker or sleep monitor. It doesn’t record details about your heart rate or health. And it’s not there to be your AI friend. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I’m not trying to build some AI assistant thing,” Migicovsky told TechCrunch in an interview. “I build things that solve one main problem, and they solve it really well,” he explains. “I think of [the ring] as external memory for my brain&amp;nbsp;… That’s what this is. It’s always with you.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, the ring has been designed to be highly reliable and privacy-preserving, he says, as all your thoughts are stored on your phone, not in the cloud. There is no subscription. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Migicovsky’s ring enters a growing market for voice-note wearables. Last month, Sandbar, a New York-based startup founded by former Meta employees, unveiled its Stream Ring, which also lets users record thoughts via a touch-activated microphone. However, unlike Index 01’s no-subscription model, Sandbar’s $249 ring offers both a free tier with limited AI interactions and a $10-per-month Stream Pro subscription for unlimited chats and early access to features. The Stream Ring is expected to ship next summer.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Migicovsky has been wearing his own ring for three months now and says he cannot imagine going back to a world where he doesn’t always have a memory device with him.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The problem is that, during the day, I get ideas or I remember something, and if I don’t write it down that second, I forget it,” he says. The ring solves this problem, he adds, without becoming another device you need to charge. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The battery lasts for years,” Migicovsky claims. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The ring is said to support roughly 12 to 14 hours of recording. On average, the founder says he uses it 10 to 20 times per day to record 3- to 6-second thoughts. At that rate, he’ll get about two years of usage. When the ring’s battery dies, you can ship it back to the company for recycling.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;When using the Index, you can record up to five minutes of audio, which can be saved to the ring and synced to your phone later. This makes sense for recording briefer, personal thoughts and notes, even when you don’t have your phone handy, but it wouldn’t work for recording a longer chat, like a presentation, meeting, or in-person interview of some kind.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3073862" height="680" src="https://techcrunch.com/wp-content/uploads/2025/12/Index01-trio-flat.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Core Devices LLC&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The ring also supports more than 100 languages and has a bit of on-device memory for times when you’re not in Bluetooth range of your device, where the recording is ultimately saved and transcribed. (The raw audio is retained, too, in case the speech-to-text is garbled due to loud background noise.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If you own a Pebble smartwatch or one from another brand, your recorded thought can even appear on the watch’s screen so you can verify it’s correct. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The ring works with Pebble’s mobile app, which offers notes and reminders but can optionally integrate with your phone’s calendaring system, too, or other apps, like Notion. And the ring’s software is open source, which makes it hackable by the community, the founder points out.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Because of its open nature, the ring’s button is already programmable. In addition to the press-and-hold gesture, you can program the ring to do other things with a single or double press, like play or pause your music or control the shutter on your phone’s camera. You could use it to send a message through the universal chat app Beeper, which Migicovsky also created, or you could add your own voice actions via MCP.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3073865" height="624" src="https://techcrunch.com/wp-content/uploads/2025/12/Index01-matte-black-hero.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Core Devices LLC&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-a-new-approach-to-hardware"&gt;A new approach to hardware &lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Migicovsky acknowledges that hardware can be difficult to get right, as Pebble’s exit to Fitbit showed. (Fitbit was later acquired by Google in 2021.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I didn’t earn any money during Pebble — we exited, but it was not a great exit,” Migicovsky admits.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This year, however, he decided to reboot the Pebble project after Google open sourced PebbleOS, which opened the door to new hardware.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;With his new company, Core Devices, Migicovsky plans to do things differently. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, the founder doesn’t regret his previous choices, he clarifies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I wouldn’t have gone back and changed anything. I loved what we built. I loved what we did. I love the company that we built, but it’s not the only way to build a company,” he told TechCrunch. “And speaking as an ex-YC partner, there’s a time and a place for building a venture-backed startup. Some companies are phenomenal when they raise money and build a big team, and I tried that&amp;nbsp;… What I’m doing now is trying an alternative path, which is [to] start from profitability,” he says.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new company is a small team of five, self-funded, and focused on sustainability.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So far, Core Devices has shipped the Pebble 2 Duo smartwatch with a black-and-white display. Its first run sold out, and the company is now preparing to ship the upgraded version, the Pebble Time 2. The newer device, which has seen 25,000 preorders, is a stainless steel watch with a larger, color e-ink screen.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As for the Index 01, the ring’s preorder offer ends in March 2026. After that, the price increases to $99. It currently comes in silver, polished gold, and matte black and works with iOS and Android devices. Customers can select from eight ring sizes, ranging from 6 to 13.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After rebooting the Pebble smartwatch brand, founder Eric Migicovsky is expanding his company’s device lineup with a new smart wearable: an AI-powered smart ring known as Index 01. Named for the finger where the ring is meant to be worn, the new $75 ring is not meant to be a competitor to always-on, always-listening AI devices, like the AI pendant Friend, but instead offers a way to record quick notes and reminders with a press of a button on the ring’s side.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI only comes into play via the open source, speech-to-text, and AI models that run locally on your smartphone through the Pebble mobile app. That is, if the ring’s button is not being pressed, it’s not recording. (And this is a press-and-&lt;em&gt;hold&lt;/em&gt; gesture, too, which means you can’t start the ring’s recording and then let go to surreptitiously record a conversation.)&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3073863" height="680" src="https://techcrunch.com/wp-content/uploads/2025/12/Index01-polished-gold-rocks.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Core Devices LLC&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;You can wear the stainless steel ring while in the shower, washing hands, doing dishes, or in the rain, but you have to take it off for other water-related activities, like swimming. At launch, it’s water-resistant to 1 meter.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The ring is also not a fitness tracker or sleep monitor. It doesn’t record details about your heart rate or health. And it’s not there to be your AI friend. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I’m not trying to build some AI assistant thing,” Migicovsky told TechCrunch in an interview. “I build things that solve one main problem, and they solve it really well,” he explains. “I think of [the ring] as external memory for my brain&amp;nbsp;… That’s what this is. It’s always with you.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, the ring has been designed to be highly reliable and privacy-preserving, he says, as all your thoughts are stored on your phone, not in the cloud. There is no subscription. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Migicovsky’s ring enters a growing market for voice-note wearables. Last month, Sandbar, a New York-based startup founded by former Meta employees, unveiled its Stream Ring, which also lets users record thoughts via a touch-activated microphone. However, unlike Index 01’s no-subscription model, Sandbar’s $249 ring offers both a free tier with limited AI interactions and a $10-per-month Stream Pro subscription for unlimited chats and early access to features. The Stream Ring is expected to ship next summer.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Migicovsky has been wearing his own ring for three months now and says he cannot imagine going back to a world where he doesn’t always have a memory device with him.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The problem is that, during the day, I get ideas or I remember something, and if I don’t write it down that second, I forget it,” he says. The ring solves this problem, he adds, without becoming another device you need to charge. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The battery lasts for years,” Migicovsky claims. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The ring is said to support roughly 12 to 14 hours of recording. On average, the founder says he uses it 10 to 20 times per day to record 3- to 6-second thoughts. At that rate, he’ll get about two years of usage. When the ring’s battery dies, you can ship it back to the company for recycling.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;When using the Index, you can record up to five minutes of audio, which can be saved to the ring and synced to your phone later. This makes sense for recording briefer, personal thoughts and notes, even when you don’t have your phone handy, but it wouldn’t work for recording a longer chat, like a presentation, meeting, or in-person interview of some kind.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3073862" height="680" src="https://techcrunch.com/wp-content/uploads/2025/12/Index01-trio-flat.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Core Devices LLC&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The ring also supports more than 100 languages and has a bit of on-device memory for times when you’re not in Bluetooth range of your device, where the recording is ultimately saved and transcribed. (The raw audio is retained, too, in case the speech-to-text is garbled due to loud background noise.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If you own a Pebble smartwatch or one from another brand, your recorded thought can even appear on the watch’s screen so you can verify it’s correct. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The ring works with Pebble’s mobile app, which offers notes and reminders but can optionally integrate with your phone’s calendaring system, too, or other apps, like Notion. And the ring’s software is open source, which makes it hackable by the community, the founder points out.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Because of its open nature, the ring’s button is already programmable. In addition to the press-and-hold gesture, you can program the ring to do other things with a single or double press, like play or pause your music or control the shutter on your phone’s camera. You could use it to send a message through the universal chat app Beeper, which Migicovsky also created, or you could add your own voice actions via MCP.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3073865" height="624" src="https://techcrunch.com/wp-content/uploads/2025/12/Index01-matte-black-hero.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Core Devices LLC&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-a-new-approach-to-hardware"&gt;A new approach to hardware &lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Migicovsky acknowledges that hardware can be difficult to get right, as Pebble’s exit to Fitbit showed. (Fitbit was later acquired by Google in 2021.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I didn’t earn any money during Pebble — we exited, but it was not a great exit,” Migicovsky admits.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This year, however, he decided to reboot the Pebble project after Google open sourced PebbleOS, which opened the door to new hardware.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;With his new company, Core Devices, Migicovsky plans to do things differently. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, the founder doesn’t regret his previous choices, he clarifies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I wouldn’t have gone back and changed anything. I loved what we built. I loved what we did. I love the company that we built, but it’s not the only way to build a company,” he told TechCrunch. “And speaking as an ex-YC partner, there’s a time and a place for building a venture-backed startup. Some companies are phenomenal when they raise money and build a big team, and I tried that&amp;nbsp;… What I’m doing now is trying an alternative path, which is [to] start from profitability,” he says.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new company is a small team of five, self-funded, and focused on sustainability.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So far, Core Devices has shipped the Pebble 2 Duo smartwatch with a black-and-white display. Its first run sold out, and the company is now preparing to ship the upgraded version, the Pebble Time 2. The newer device, which has seen 25,000 preorders, is a stainless steel watch with a larger, color e-ink screen.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As for the Index 01, the ring’s preorder offer ends in March 2026. After that, the price increases to $99. It currently comes in silver, polished gold, and matte black and works with iOS and Android devices. Customers can select from eight ring sizes, ranging from 6 to 13.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/pebbles-founder-introduces-a-75-ai-smart-ring-for-recording-brief-notes-with-a-press-of-a-button/</guid><pubDate>Tue, 09 Dec 2025 15:00:00 +0000</pubDate></item><item><title>[NEW] Pebble maker announces Index 01, a smart-ish ring for under $100 (AI – Ars Technica)</title><link>https://arstechnica.com/gadgets/2025/12/resurrected-pebble-maker-announces-a-kind-of-smart-ring-for-capturing-audio-notes/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The Pebble Index 01 isn’t quite a smart ring, but it can do some smart things.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Index 01 rock background" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Index01-polished-silver-rocks-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Index 01 rock background" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Index01-polished-silver-rocks-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Core Devices

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Nearly a decade after Pebble’s nascent smartwatch empire crumbled, the brand is staging a comeback with new wearables. The Pebble Core Duo 2 and Core Time 2 are a natural evolution of the company’s low-power smartwatch designs, but its next wearable is something different. The Index 01 is a ring, but you probably shouldn’t call it a smart ring. The Index does just one thing—capture voice notes—but the firm says it does that one thing extremely well.&lt;/p&gt;
&lt;p&gt;Most of today’s smart rings offer users the ability to track health stats, along with various minor smartphone integrations. With all the sensors and data collection, these devices can cost as much as a smartwatch and require frequent charging. The Index 01 doesn’t do any of that. It contains a Bluetooth radio, a microphone, a hearing aid battery, and a physical button. You press the button, record your note, and that’s it. The company says the Index 01 will run for years on a charge and will cost just $75 during the preorder period. After that, it will go up to $99.&lt;/p&gt;
&lt;p&gt;Core Devices, the new home of Pebble, says the Index is designed to be worn on your index finger (get it?), where you can easily mash the device’s button with your thumb. Unlike recording notes with a phone or smartwatch, you don’t need both hands to create voice notes with the Index.&lt;/p&gt;
&lt;p&gt;The ring’s lone physical control is tactile, ensuring you’ll know when it’s activated and recording. When you’re done talking, just release the button. If that button is not depressed, the ring won’t record audio for any reason. The company apparently worked to ensure this process is 100 percent reliable—it only does one thing, so it really has to do it well.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2130868 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Index 01 holding bag" class="fullwidth full" height="1080" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Index01-lifestyle.png" width="1920" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The ring is designed to be worn on the index finger so the button can be pressed with your thumb.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Core Devices

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;A smart ring usually needs to be recharged every few days, but you will never recharge the Index. The idea is that since you never have to take it off to charge, using the Index 01 “becomes muscle memory.” The integrated battery will power the device for 12–14 total hours of recording. The designers estimate that to be roughly two years of usage if you record 10 to 20 short voice notes per day. And what happens when the battery runs out? You just send the ring back to be recycled.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;A different approach to AI&lt;/h2&gt;
&lt;p&gt;There is a little more to the Index than meets the eye. The ring makes use of generative AI in a way that might have tempted most companies in 2025 to shout about it from the rooftops. However, Pebble isn’t looking to sell you an AI subscription or feed on your personal data.&lt;/p&gt;
&lt;p&gt;After you record a voice note, it’s beamed over Bluetooth to your phone (Android or iOS), and it stays there. The recording is converted to text and fed into a large language model (LLM) that runs locally on your device to take actions. The speech-to-text process and LLM operate in the open source Pebble app, and no data from your notes is sent to the Internet. However, there is an optional online backup service for your recordings.&lt;/p&gt;
&lt;p&gt;While the company is anxious to talk about the ironclad reliability of voice notes on the Index 01, there’s no such guarantee with an LLM. A model small enough to run on your phone has to focus on specific functionality rather than doing everything like a big cloud-based AI. So the Index will only support a few actions out of the box. Here’s the full list.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create or add to notes&lt;/li&gt;
&lt;li&gt;Set reminder&lt;/li&gt;
&lt;li&gt;Create alarm&lt;/li&gt;
&lt;li&gt;Create timer&lt;/li&gt;
&lt;li&gt;Play/pause/skip music track (via button press)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If that’s not enough, you’re in luck. The Index 01 is also designed to be hacking-friendly. The audio and transcribed text is yours to do with as you please. You can route it to a different app via a webhook, and the LLM supports model context protocol (MCP), so you can add new functionality that also runs locally. The AI model will also be released as an open source project.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2130869 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="1080" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Index01-trio-together.jpg" width="1920" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Core Devices

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The Index 01 comes in polished silver, polished gold, and matte black colorways and US sizes 6 through 13. Preorders start today at the $75 price. Worldwide shipping will begin in March 2026, at which time the price will go up to $99.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The Pebble Index 01 isn’t quite a smart ring, but it can do some smart things.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Index 01 rock background" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Index01-polished-silver-rocks-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Index 01 rock background" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Index01-polished-silver-rocks-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Core Devices

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Nearly a decade after Pebble’s nascent smartwatch empire crumbled, the brand is staging a comeback with new wearables. The Pebble Core Duo 2 and Core Time 2 are a natural evolution of the company’s low-power smartwatch designs, but its next wearable is something different. The Index 01 is a ring, but you probably shouldn’t call it a smart ring. The Index does just one thing—capture voice notes—but the firm says it does that one thing extremely well.&lt;/p&gt;
&lt;p&gt;Most of today’s smart rings offer users the ability to track health stats, along with various minor smartphone integrations. With all the sensors and data collection, these devices can cost as much as a smartwatch and require frequent charging. The Index 01 doesn’t do any of that. It contains a Bluetooth radio, a microphone, a hearing aid battery, and a physical button. You press the button, record your note, and that’s it. The company says the Index 01 will run for years on a charge and will cost just $75 during the preorder period. After that, it will go up to $99.&lt;/p&gt;
&lt;p&gt;Core Devices, the new home of Pebble, says the Index is designed to be worn on your index finger (get it?), where you can easily mash the device’s button with your thumb. Unlike recording notes with a phone or smartwatch, you don’t need both hands to create voice notes with the Index.&lt;/p&gt;
&lt;p&gt;The ring’s lone physical control is tactile, ensuring you’ll know when it’s activated and recording. When you’re done talking, just release the button. If that button is not depressed, the ring won’t record audio for any reason. The company apparently worked to ensure this process is 100 percent reliable—it only does one thing, so it really has to do it well.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2130868 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Index 01 holding bag" class="fullwidth full" height="1080" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Index01-lifestyle.png" width="1920" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The ring is designed to be worn on the index finger so the button can be pressed with your thumb.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Core Devices

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;A smart ring usually needs to be recharged every few days, but you will never recharge the Index. The idea is that since you never have to take it off to charge, using the Index 01 “becomes muscle memory.” The integrated battery will power the device for 12–14 total hours of recording. The designers estimate that to be roughly two years of usage if you record 10 to 20 short voice notes per day. And what happens when the battery runs out? You just send the ring back to be recycled.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;A different approach to AI&lt;/h2&gt;
&lt;p&gt;There is a little more to the Index than meets the eye. The ring makes use of generative AI in a way that might have tempted most companies in 2025 to shout about it from the rooftops. However, Pebble isn’t looking to sell you an AI subscription or feed on your personal data.&lt;/p&gt;
&lt;p&gt;After you record a voice note, it’s beamed over Bluetooth to your phone (Android or iOS), and it stays there. The recording is converted to text and fed into a large language model (LLM) that runs locally on your device to take actions. The speech-to-text process and LLM operate in the open source Pebble app, and no data from your notes is sent to the Internet. However, there is an optional online backup service for your recordings.&lt;/p&gt;
&lt;p&gt;While the company is anxious to talk about the ironclad reliability of voice notes on the Index 01, there’s no such guarantee with an LLM. A model small enough to run on your phone has to focus on specific functionality rather than doing everything like a big cloud-based AI. So the Index will only support a few actions out of the box. Here’s the full list.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create or add to notes&lt;/li&gt;
&lt;li&gt;Set reminder&lt;/li&gt;
&lt;li&gt;Create alarm&lt;/li&gt;
&lt;li&gt;Create timer&lt;/li&gt;
&lt;li&gt;Play/pause/skip music track (via button press)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If that’s not enough, you’re in luck. The Index 01 is also designed to be hacking-friendly. The audio and transcribed text is yours to do with as you please. You can route it to a different app via a webhook, and the LLM supports model context protocol (MCP), so you can add new functionality that also runs locally. The AI model will also be released as an open source project.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2130869 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="1080" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Index01-trio-together.jpg" width="1920" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Core Devices

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The Index 01 comes in polished silver, polished gold, and matte black colorways and US sizes 6 through 13. Preorders start today at the $75 price. Worldwide shipping will begin in March 2026, at which time the price will go up to $99.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/gadgets/2025/12/resurrected-pebble-maker-announces-a-kind-of-smart-ring-for-capturing-audio-notes/</guid><pubDate>Tue, 09 Dec 2025 15:00:56 +0000</pubDate></item><item><title>[NEW] Anthropic and Accenture sign multi-year AI strategic partnership (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/anthropic-and-accenture-sign-multi-year-ai-strategic-partnership/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/Anthropic-economic-futures-program.png?resize=1200,667" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI research lab Anthropic continues to cement its stake as the predominant AI player in the enterprise space.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Tuesday, Anthropic announced a multi-year partnership with professional services firm Accenture. Financial terms of the deal were not disclosed; however, The Wall Street Journal reported that the deal is for three years.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Accenture confirmed the deal is for three years but declined to comment on financials. TechCrunch reached out to Anthropic for more information. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The two companies are forming the Accenture Anthropic Business Group. This will include formal Claude training for Accenture’s 30,000 employees. Anthropic’s Claude Code coding tools will be available for Accenture’s tens of thousands of developers. They are also launching a joint initiative to help chief investment officers track their return on investment for AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This announcement comes as Anthropic’s market share within enterprise continues to grow.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A new report from Menlo Ventures shows that Anthropic holds 40% of the market share within enterprise and 54% of the market share when it comes to coding. This marks a bump from Menlo’s previous survey this summer, where Anthropic held 32% of the enterprise market share.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic announced a $200 million deal with cloud data company Snowflake last week. The company also announced sizable and similar AI partnerships with both Deloitte and IBM in October.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/Anthropic-economic-futures-program.png?resize=1200,667" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI research lab Anthropic continues to cement its stake as the predominant AI player in the enterprise space.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Tuesday, Anthropic announced a multi-year partnership with professional services firm Accenture. Financial terms of the deal were not disclosed; however, The Wall Street Journal reported that the deal is for three years.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Accenture confirmed the deal is for three years but declined to comment on financials. TechCrunch reached out to Anthropic for more information. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The two companies are forming the Accenture Anthropic Business Group. This will include formal Claude training for Accenture’s 30,000 employees. Anthropic’s Claude Code coding tools will be available for Accenture’s tens of thousands of developers. They are also launching a joint initiative to help chief investment officers track their return on investment for AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This announcement comes as Anthropic’s market share within enterprise continues to grow.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A new report from Menlo Ventures shows that Anthropic holds 40% of the market share within enterprise and 54% of the market share when it comes to coding. This marks a bump from Menlo’s previous survey this summer, where Anthropic held 32% of the enterprise market share.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic announced a $200 million deal with cloud data company Snowflake last week. The company also announced sizable and similar AI partnerships with both Deloitte and IBM in October.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/anthropic-and-accenture-sign-multi-year-ai-strategic-partnership/</guid><pubDate>Tue, 09 Dec 2025 15:02:35 +0000</pubDate></item><item><title>[NEW] GigaTIME: Scaling tumor microenvironment modeling using virtual population generated by multimodal AI (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/gigatime-scaling-tumor-microenvironment-modeling-using-virtual-population-generated-by-multimodal-ai/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Illustration of a person kneeling and looking through a large telescope on a textured pink surface. The telescope is aimed at a dark sky filled with colorful circular icons, each containing abstract shapes. The telescope itself has circuit-like patterns inside." class="wp-image-1156682" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/11/LANDSCAPE-INMUNE-MAPS_holasoyka.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;The convergence of digital transformation and the GenAI revolution creates an unprecedented opportunity for accelerating progress in precision health. Precision immunotherapy is a poster child for this transformation. Emerging technologies such as multiplex immunofluorescence (mIF) can assess internal states of individual cells along with their spatial locations, which is critical for deciphering how tumors interact with the immune system. The resulting insights, often referred to as the “grammar” of the tumor microenvironment, can help predict whether a tumor will respond to immunotherapy. If it is unlikely to respond, these insights can also inform strategies to reprogram the tumor from “cold” to “hot,” increasing its susceptibility to treatment.&lt;/p&gt;



&lt;p&gt;This is exciting, but progress is hindered by the high cost and limited scalability of current technology. For example, obtaining mIF data of a couple dozen protein channels for a tissue sample can cost thousands of dollars, and even the most advanced labs can barely scale it to a tiny fraction of their available tissue samples.&lt;/p&gt;



&lt;p&gt;In our paper published in Cell on&amp;nbsp;December&amp;nbsp;9, “Multimodal AI generates virtual population for tumor microenvironment modeling&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;,” we present&amp;nbsp;GigaTIME&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, a&amp;nbsp;multimodal AI&amp;nbsp;model&amp;nbsp;for translating routinely available hematoxylin and eosin (H&amp;amp;E)&amp;nbsp;pathology slides&amp;nbsp;to virtual&amp;nbsp;mIF&amp;nbsp;images. Developed in collaboration with Providence and the University of Washington, GigaTIME was trained on a Providence dataset of 40 million cells with paired H&amp;amp;E and mIF images across 21 protein channels. We applied&amp;nbsp;GigaTIME&amp;nbsp;to&amp;nbsp;14,256&amp;nbsp;cancer&amp;nbsp;patients from 51 hospitals and over a thousand clinics within the Providence system. This effort generated a virtual population of around 300,000 mIF images spanning 24 cancer types and 306 cancer subtypes. This virtual population uncovered&amp;nbsp;1,234 statistically significant&amp;nbsp;associations linking&amp;nbsp;mIF&amp;nbsp;protein activations with key clinical attributes such as biomarkers, staging,&amp;nbsp;and patient&amp;nbsp;survival.&amp;nbsp;Independent external validation&amp;nbsp;on&amp;nbsp;10,200&amp;nbsp;Cancer Genome Atlas (TCGA) patients further corroborated our findings.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To our knowledge, this is the first population-scale study of tumor immune microenvironment (TIME) based on spatial proteomics. Such studies were previously infeasible due to the scarcity of mIF data. By translating readily available H&amp;amp;E pathology slides into high-resolution virtual mIF data, GigaTIME provides a novel research framework for exploring precision immuno-oncology through population-scale TIME analysis and discovery. We have made our GigaTIME model publicly available at Microsoft Foundry Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and on Hugging Face&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; to help accelerate clinical research in precision oncology.&lt;/p&gt;



&lt;p&gt;“GigaTIME&amp;nbsp;is about unlocking insights that were previously out of reach,” explained Carlo Bifulco, MD, chief medical officer of Providence Genomics and medical director of cancer genomics and precision oncology at the Providence Cancer Institute. “By analyzing the tumor microenvironment of thousands of patients,&amp;nbsp;GigaTIME&amp;nbsp;has the potential to accelerate discoveries that will shape the future of precision oncology and improve patient outcomes.”&amp;nbsp;&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;PODCAST SERIES&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/h2&gt;
				
								&lt;p class="large" id="ai-testing-and-evaluation-learnings-from-science-and-industry"&gt;Discover how Microsoft is learning from other domains to advance evaluation and testing as a pillar of AI governance.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="gigatime-generates-a-virtual-population-for-tumor-microenvironment-modeling"&gt;GigaTIME generates a virtual population for tumor microenvironment modeling&lt;/h2&gt;



&lt;p&gt;Digital pathology transforms a microscopy slide of stained tumor tissue into a high-resolution digital image, revealing details of cell morphology such as nucleus and cytoplasm. Such a slide only costs $5 to $10 per image and has become routinely available in cancer care. It is well known that H&amp;amp;E-based cell morphology contains information about the cellular states. Last year, we released GigaPath, the first digital pathology foundation model for scaling transformer architectures to gigapixel H&amp;amp;E slides. Afterward, researchers at Mount Sinai Hospital and Memorial Sloan Kettering Cancer Center showed in a global prospective trial that it can reliably predict a key biomarker from H&amp;amp;E slides for precision oncology triaging. However, such prior works are generally limited to average biomarker status across the entire tissue. GigaTIME thus represents a major step forward by learning to predict spatially resolved, single-cell states essential for tumor microenvironment modeling. In turn, this enables us to generate a virtual population of mIF images for large-scale TIME analysis (Figure 1).&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1. GigaTIME enables population-scale tumor immune microenvironment (TIME) analysis. A, GigaTIME inputs a hematoxylin and eosin (H&amp;amp;E) whole-slide image and outputs multiplex immunofluorescence (mIF) across 21 protein channels. By applying GigaTIME to 14,256 patients, we generated a virtual population with mIF information, leading to population-scale discovery on clinical biomarkers and patient stratification, with independent validation on TCGA. B, Circular plot visualizing a TIME spectrum encompassing the GigaTIME-translated virtual mIF activation scores across different protein channels at the population scale, where each channel is represented as an individual circular bar chart segment. The inner circle encodes OncoTree, which classifies 14,256 patients into 306 subtypes across 24 cancer types. The outer circle groups these activations by cancer types, allowing visual comparison across major categories. C, Scatter plot comparing the subtype-level GigaTIME-translated virtual mIF activations between TCGA and Providence virtual populations. Each dot denotes the average activation score of a protein channel among all tumors of a cancer subtype." class="wp-image-1153563" height="1518" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/GigaTIME_Figure1.png" width="1265" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. GigaTIME enables population-scale tumor immune microenvironment (TIME) analysis. A, GigaTIME inputs a hematoxylin and eosin (H&amp;amp;E) whole-slide image and outputs multiplex immunofluorescence (mIF) across 21 protein channels. By applying GigaTIME to 14,256 patients, we generated a virtual population with mIF information, leading to population-scale discovery on clinical biomarkers and patient stratification, with independent validation on TCGA. B, Circular plot visualizing a TIME spectrum encompassing the GigaTIME-translated virtual mIF activation scores across different protein channels at the population scale, where each channel is represented as an individual circular bar chart segment. The inner circle encodes OncoTree, which classifies 14,256 patients into 306 subtypes across 24 cancer types. The outer circle groups these activations by cancer types, allowing visual comparison across major categories. C, Scatter plot comparing the subtype-level GigaTIME-translated virtual mIF activations between TCGA and Providence virtual populations. Each dot denotes the average activation score of a protein channel among all tumors of a cancer subtype.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="gigatime-learns-a-multimodal-ai-model-to-translate-pathology-slides-into-spatial-proteomics-images-bridging-cell-morphology-and-cell-states"&gt;GigaTIME learns a multimodal AI model to translate pathology slides into spatial proteomics images, bridging cell morphology and cell states&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 2. GigaTIME enables translation from hematoxylin and eosin (H&amp;amp;E) to multiplex immunofluorescence (mIF) images. A,B, Bar plot comparing GigaTIME and CycleGAN on the translation performance in terms of Dice score (A) and Pearson correlation (B). C, Scatter plots comparing the activation density of the translated mIF and the ground truth mIF across four channels. D, Qualitative results for a sample H&amp;amp;E whole-slide image from our held-out test set with zoomed-in visualizations of the measured mIF and GigaTIME-translated mIF for DAPI, PD-L1, and CD68 channels." class="wp-image-1153564" height="1418" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/GigaTIME_Figure2.png" width="1241" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. GigaTIME enables translation from hematoxylin and eosin (H&amp;amp;E) to multiplex immunofluorescence (mIF) images. A,B, Bar plot comparing GigaTIME and CycleGAN on the translation performance in terms of Dice score (A) and Pearson correlation (B). C, Scatter plots comparing the activation density of the translated mIF and the ground truth mIF across four channels. D, Qualitative results for a sample H&amp;amp;E whole-slide image from our held-out test set with zoomed-in visualizations of the measured mIF and GigaTIME-translated mIF for DAPI, PD-L1, and CD68 channels.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;GigaTIME learned a cross-modal AI translator from digital pathology to spatial multiplex proteomics by training on 40 million cells with paired H&amp;amp;E slides and mIF images from Providence. To our knowledge, this is the first large-scale study exploring multimodal AI for scaling virtual mIF generation. The high-quality paired data enabled much more accurate cross-modal translation compared to prior state-of-the-art methods (Figure 2).&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="virtual-population-enables-population-scale-discovery-of-associations-between-cell-states-and-key-biomarkers"&gt;Virtual population enables population-scale discovery of associations between cell states and key biomarkers&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 3. GigaTIME identifies novel TIME protein vs biomarker associations at pan-cancer, cancer type, cancer subtype levels. A, GigaTIME generates a virtual population of 14,256 with virtual mIF by translating available H&amp;amp;E images to mIF images, enabling pan-cancer, cancer type, and cancer subtype levels of biomedical discovery. B-G, Correlation analysis between protein channels in virtual mIF and patient biomarkers reveal TIME protein-biomarker associations at pan-cancer level (B), cancer-type level (C-E), and cancer-subtype level (F,G). Circle size denotes significance strength. Circle color denotes the directionality in which the correlation occurs. Channel color denotes high, medium, and low confidence based on pearson correlations evaluated using test set. H, A case study showcasing the activation maps across different virtual mIF channels for a H&amp;amp;E slide in our virtual population, and virtual mIF of sample patches from this slide." class="wp-image-1153565" height="1747" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/GigaTIME_Figure3.png" width="1257" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 3. GigaTIME identifies novel TIME protein vs biomarker associations at pan-cancer, cancer type, cancer subtype levels. A, GigaTIME generates a virtual population of 14,256 with virtual mIF by translating available H&amp;amp;E images to mIF images, enabling pan-cancer, cancer type, and cancer subtype levels of biomedical discovery. B-G, Correlation analysis between protein channels in virtual mIF and patient biomarkers reveal TIME protein-biomarker associations at pan-cancer level (B), cancer-type level (C-E), and cancer-subtype level (F,G). Circle size denotes significance strength. Circle color denotes the directionality in which the correlation occurs. Channel color denotes high, medium, and low confidence based on pearson correlations evaluated using test set. H, A case study showcasing the activation maps across different virtual mIF channels for a H&amp;amp;E slide in our virtual population, and virtual mIF of sample patches from this slide.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;By applying GigaTIME to Providence real-world data, we generated a virtual population of 14,256 patients with virtual mIF and key clinical attributes. After correcting for multiple hypothesis testing, we have identified 1,234 statistically significant associations between tumor immune cell states (CD138, CD20, CD4) and clinical biomarkers (tumor mutation burden, KRAS, KMT2D), from pan-cancer to cancer subtypes (Figure 3). Many of these findings are supported by existing literature. For example, MSI high and TMB high associated with increased activations of TIME-related channels such as CD138. Additionally, the virtual population also uncovered previously unknown associations, such as pan-cancer associations between immune activations and key tumor biomarkers, such as the tumor suppressor KMT2D and the oncogene KRAS).&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="virtual-population-enables-population-scale-discovery-of-tumor-immune-signatures-for-patient-stratification"&gt;Virtual population enables population-scale discovery of tumor immune signatures for patient stratification&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 4. GigaTIME enables effective patient stratification across pathological stages and survival groups. A-C, Correlation analysis between virtual mIF and pathological stages at pan-cancer level (A), cancer-type level (B), and cancer-subtype level (C). Circle size denotes significance strength. Circle color denotes the directionality in which the correlation happens. Channel color denotes high, medium, and low confidence based on pearson correlations evaluated using test set. D-F, Survival analysis on lung cancer by using virtual CD3, virtual CD8, and virtual GigaTIME signature (all 21 GigaTIME protein channels) to stratify patients at pan-cancer level (D) and cancer-type level: lung (E), brain (F). G, Bar plot comparing pan-cancer patient stratification performance in terms of survival log rank p-values among virtual GigaTIME signature and individual virtual protein channels." class="wp-image-1153566" height="1733" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/GigaTIME_Figure4.png" width="1654" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 4. GigaTIME enables effective patient stratification across pathological stages and survival groups. A-C, Correlation analysis between virtual mIF and pathological stages at pan-cancer level (A), cancer-type level (B), and cancer-subtype level (C). Circle size denotes significance strength. Circle color denotes the directionality in which the correlation happens. Channel color denotes high, medium, and low confidence based on pearson correlations evaluated using test set. D-F, Survival analysis on lung cancer by using virtual CD3, virtual CD8, and virtual GigaTIME signature (all 21 GigaTIME protein channels) to stratify patients at pan-cancer level (D) and cancer-type level: lung (E), brain (F). G, Bar plot comparing pan-cancer patient stratification performance in terms of survival log rank p-values among virtual GigaTIME signature and individual virtual protein channels.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The virtual population also uncovered&amp;nbsp;GigaTIME&amp;nbsp;signatures for&amp;nbsp;effective&amp;nbsp;patient stratification&amp;nbsp;across staging and survival profiles&amp;nbsp;(Figure 4), from pan-cancer to cancer subtypes.&amp;nbsp;Prior studies have&amp;nbsp;explored patient stratification based on&amp;nbsp;individual&amp;nbsp;immune&amp;nbsp;proteins&amp;nbsp;such as CD3 and CD8. We found that GigaTIME-simulated CD3 and CD8 are similarly effective. Moreover, the combined GigaTIME signature across all 21 protein channels attained even better patient stratification compared to individual channels.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="virtual-population-uncovers-interesting-spatial-and-combinatorial-interactions"&gt;Virtual population uncovers interesting spatial and combinatorial interactions&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 5. GigaTIME uncovers interesting spatial and combinatorial virtual mIF patterns. A,B,C Bar plots comparing virtual mIF activation density with spatial metrics on identifying TIME protein-biomarker correlations. We investigated three spatial metrics based on entropy (A), signal-to-noise ratio (SNR) (B), and sharpness (C). D,E, Bar plots comparing single-channel and combinatorial-channel (using the OR logical operation) in biomarker associations for two GigaTIME virtual protein pairs: CD138/CD68 (D) and PD-L1/Caspase 3 (E), demonstrating substantially improved associations for the combination. F, Case studies visualizing the virtual mIF activation maps of individual channels (CD138, CD68; PD-L1, Caspase 3) and their combinations." class="wp-image-1153567" height="1606" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/GigaTIME_Figure5.png" width="1249" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 5. GigaTIME uncovers interesting spatial and combinatorial virtual mIF patterns. A,B,C Bar plots comparing virtual mIF activation density with spatial metrics on identifying TIME protein-biomarker correlations. We investigated three spatial metrics based on entropy (A), signal-to-noise ratio (SNR) (B), and sharpness (C). D,E, Bar plots comparing single-channel and combinatorial-channel (using the OR logical operation) in biomarker associations for two GigaTIME virtual protein pairs: CD138/CD68 (D) and PD-L1/Caspase 3 (E), demonstrating substantially improved associations for the combination. F, Case studies visualizing the virtual mIF activation maps of individual channels (CD138, CD68; PD-L1, Caspase 3) and their combinations.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The virtual population uncovered interesting non-linear interactions across the GigaTIME virtual protein channels, revealing associations with spatial features such as sharpness and entropy, as well as with key clinical biomarkers like APC and KMT2D (Figure 6). Such combinatorial studies were previously out of reach given the scarcity of mIF data.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="independent-external-validation-on-tcga"&gt;Independent external validation on TCGA&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 6. Independent validation on a virtual population from TCGA. A, Grid charts showing significantly correlated pan-cancer GigaTIME protein-biomarker pairs in Providence (left), TCGA (middle), and both (right). B, Grid charts showing significantly correlated GigaTIME protein-biomarker pairs for lung cancer in Providence and TCGA. C, Grid chart showing significantly correlated GigaTIME protein-biomarker pairs for LUAD in Providence. Channel color denotes high, medium, and low confidence based on pearson correlations evaluated using test set. D, Case studies with visualizations of H&amp;amp;E slides and the corresponding virtual mIF activations for the pair of a GigaTIME protein channel and a biomarker (mutated/non-mutated), where the patient with the given mutation demonstrates much higher activation scores for that GigaTIME protein channel." class="wp-image-1153568" height="2560" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/GigaTIME_Figure6-1-scaled.png" width="1977" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 6. Independent validation on a virtual population from TCGA. A, Grid charts showing significantly correlated pan-cancer GigaTIME protein-biomarker pairs in Providence (left), TCGA (middle), and both (right). B, Grid charts showing significantly correlated GigaTIME protein-biomarker pairs for lung cancer in Providence and TCGA. C, Grid chart showing significantly correlated GigaTIME protein-biomarker pairs for LUAD in Providence. Channel color denotes high, medium, and low confidence based on pearson correlations evaluated using test set. D, Case studies with visualizations of H&amp;amp;E slides and the corresponding virtual mIF activations for the pair of a GigaTIME protein channel and a biomarker (mutated/non-mutated), where the patient with the given mutation demonstrates much higher activation scores for that GigaTIME protein channel.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;We conducted an independent external validation by applying GigaTIME to 10,200 patients in The Cancer Genome Atlas (TCGA) dataset and studied associations between GigaTIME-simulated virtual mIF and clinical biomarkers available in TCGA. We observed significant concordance across the virtual populations from Providence and TCGA, with a Spearman correlation of 0.88 for virtual protein activations across cancer subtypes. The two populations also uncovered a significant overlap of associations between GigaTIME-simulated protein activations and clinical biomarkers (Fisher’s exact test p  2 × 10−9). On the other hand, the Providence virtual population yielded 33% more significant associations than TCGA, highlighting the value of large and diverse real-world data for clinical discovery.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="gigatime-is-a-promising-step-toward-the-moonshot-of-virtual-patient"&gt;GigaTIME is a promising step toward the moonshot of “virtual patient”&lt;/h2&gt;



&lt;p&gt;By learning to translate across modalities, GigaTIME is a promising step toward “learning the language of patients” for the ultimate goal of developing a “virtual patient”, a high-fidelity digital twin that could one day accurately forecast disease progression and counterfactual treatment response. By converting routinely available cell morphology data into otherwise scarce high-resolution cell states signals, GigaTIME demonstrated the potential in harnessing multimodal AI to scale real-world evidence (RWE) generation.&lt;/p&gt;



&lt;p&gt;Going forward, growth opportunities abound. GigaTIME can be extended to handle more spatial modalities and cell-state channels. It can be integrated into advanced multimodal frameworks such as LLaVA-Med to facilitate conversational image analysis by “talking to the data.” To facilitate research in tumor microenvironment modeling, we have made GigaTIME open-source&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; on Foundry Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and Hugging Face&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;p&gt;GigaTIME is a joint work with Providence and the University of Washington’s Paul G. Allen School of Computer Science &amp;amp; Engineering. It reflects Microsoft’s larger commitment to advancing multimodal generative AI for precision health&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, with other exciting progress such as GigaPath, BiomedCLIP, LLaVA-Rad&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, BiomedJourney, BiomedParse, TrialScope, Curiosity.&lt;/p&gt;







&lt;p&gt;Paper co-authors: &lt;em&gt;Jeya Maria Jose Valanarasu, Hanwen Xu, Naoto Usuyama, Chanwoo Kim, Cliff Wong, Peniel Argaw, Racheli Ben Shimol, Angela Crabtree, Kevin Matlock, Alexandra Q. Bartlett, Jaspreet Bagga, Yu Gu, Sheng Zhang, Tristan Naumann, Bernard A. Fox, Bill Wright, Ari Robicsek, Brian Piening, Carlo Bifulco, Sheng Wang, Hoifung Poon&lt;/em&gt;&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Illustration of a person kneeling and looking through a large telescope on a textured pink surface. The telescope is aimed at a dark sky filled with colorful circular icons, each containing abstract shapes. The telescope itself has circuit-like patterns inside." class="wp-image-1156682" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/11/LANDSCAPE-INMUNE-MAPS_holasoyka.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;The convergence of digital transformation and the GenAI revolution creates an unprecedented opportunity for accelerating progress in precision health. Precision immunotherapy is a poster child for this transformation. Emerging technologies such as multiplex immunofluorescence (mIF) can assess internal states of individual cells along with their spatial locations, which is critical for deciphering how tumors interact with the immune system. The resulting insights, often referred to as the “grammar” of the tumor microenvironment, can help predict whether a tumor will respond to immunotherapy. If it is unlikely to respond, these insights can also inform strategies to reprogram the tumor from “cold” to “hot,” increasing its susceptibility to treatment.&lt;/p&gt;



&lt;p&gt;This is exciting, but progress is hindered by the high cost and limited scalability of current technology. For example, obtaining mIF data of a couple dozen protein channels for a tissue sample can cost thousands of dollars, and even the most advanced labs can barely scale it to a tiny fraction of their available tissue samples.&lt;/p&gt;



&lt;p&gt;In our paper published in Cell on&amp;nbsp;December&amp;nbsp;9, “Multimodal AI generates virtual population for tumor microenvironment modeling&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;,” we present&amp;nbsp;GigaTIME&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, a&amp;nbsp;multimodal AI&amp;nbsp;model&amp;nbsp;for translating routinely available hematoxylin and eosin (H&amp;amp;E)&amp;nbsp;pathology slides&amp;nbsp;to virtual&amp;nbsp;mIF&amp;nbsp;images. Developed in collaboration with Providence and the University of Washington, GigaTIME was trained on a Providence dataset of 40 million cells with paired H&amp;amp;E and mIF images across 21 protein channels. We applied&amp;nbsp;GigaTIME&amp;nbsp;to&amp;nbsp;14,256&amp;nbsp;cancer&amp;nbsp;patients from 51 hospitals and over a thousand clinics within the Providence system. This effort generated a virtual population of around 300,000 mIF images spanning 24 cancer types and 306 cancer subtypes. This virtual population uncovered&amp;nbsp;1,234 statistically significant&amp;nbsp;associations linking&amp;nbsp;mIF&amp;nbsp;protein activations with key clinical attributes such as biomarkers, staging,&amp;nbsp;and patient&amp;nbsp;survival.&amp;nbsp;Independent external validation&amp;nbsp;on&amp;nbsp;10,200&amp;nbsp;Cancer Genome Atlas (TCGA) patients further corroborated our findings.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To our knowledge, this is the first population-scale study of tumor immune microenvironment (TIME) based on spatial proteomics. Such studies were previously infeasible due to the scarcity of mIF data. By translating readily available H&amp;amp;E pathology slides into high-resolution virtual mIF data, GigaTIME provides a novel research framework for exploring precision immuno-oncology through population-scale TIME analysis and discovery. We have made our GigaTIME model publicly available at Microsoft Foundry Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and on Hugging Face&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; to help accelerate clinical research in precision oncology.&lt;/p&gt;



&lt;p&gt;“GigaTIME&amp;nbsp;is about unlocking insights that were previously out of reach,” explained Carlo Bifulco, MD, chief medical officer of Providence Genomics and medical director of cancer genomics and precision oncology at the Providence Cancer Institute. “By analyzing the tumor microenvironment of thousands of patients,&amp;nbsp;GigaTIME&amp;nbsp;has the potential to accelerate discoveries that will shape the future of precision oncology and improve patient outcomes.”&amp;nbsp;&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;PODCAST SERIES&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/h2&gt;
				
								&lt;p class="large" id="ai-testing-and-evaluation-learnings-from-science-and-industry"&gt;Discover how Microsoft is learning from other domains to advance evaluation and testing as a pillar of AI governance.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="gigatime-generates-a-virtual-population-for-tumor-microenvironment-modeling"&gt;GigaTIME generates a virtual population for tumor microenvironment modeling&lt;/h2&gt;



&lt;p&gt;Digital pathology transforms a microscopy slide of stained tumor tissue into a high-resolution digital image, revealing details of cell morphology such as nucleus and cytoplasm. Such a slide only costs $5 to $10 per image and has become routinely available in cancer care. It is well known that H&amp;amp;E-based cell morphology contains information about the cellular states. Last year, we released GigaPath, the first digital pathology foundation model for scaling transformer architectures to gigapixel H&amp;amp;E slides. Afterward, researchers at Mount Sinai Hospital and Memorial Sloan Kettering Cancer Center showed in a global prospective trial that it can reliably predict a key biomarker from H&amp;amp;E slides for precision oncology triaging. However, such prior works are generally limited to average biomarker status across the entire tissue. GigaTIME thus represents a major step forward by learning to predict spatially resolved, single-cell states essential for tumor microenvironment modeling. In turn, this enables us to generate a virtual population of mIF images for large-scale TIME analysis (Figure 1).&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1. GigaTIME enables population-scale tumor immune microenvironment (TIME) analysis. A, GigaTIME inputs a hematoxylin and eosin (H&amp;amp;E) whole-slide image and outputs multiplex immunofluorescence (mIF) across 21 protein channels. By applying GigaTIME to 14,256 patients, we generated a virtual population with mIF information, leading to population-scale discovery on clinical biomarkers and patient stratification, with independent validation on TCGA. B, Circular plot visualizing a TIME spectrum encompassing the GigaTIME-translated virtual mIF activation scores across different protein channels at the population scale, where each channel is represented as an individual circular bar chart segment. The inner circle encodes OncoTree, which classifies 14,256 patients into 306 subtypes across 24 cancer types. The outer circle groups these activations by cancer types, allowing visual comparison across major categories. C, Scatter plot comparing the subtype-level GigaTIME-translated virtual mIF activations between TCGA and Providence virtual populations. Each dot denotes the average activation score of a protein channel among all tumors of a cancer subtype." class="wp-image-1153563" height="1518" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/GigaTIME_Figure1.png" width="1265" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. GigaTIME enables population-scale tumor immune microenvironment (TIME) analysis. A, GigaTIME inputs a hematoxylin and eosin (H&amp;amp;E) whole-slide image and outputs multiplex immunofluorescence (mIF) across 21 protein channels. By applying GigaTIME to 14,256 patients, we generated a virtual population with mIF information, leading to population-scale discovery on clinical biomarkers and patient stratification, with independent validation on TCGA. B, Circular plot visualizing a TIME spectrum encompassing the GigaTIME-translated virtual mIF activation scores across different protein channels at the population scale, where each channel is represented as an individual circular bar chart segment. The inner circle encodes OncoTree, which classifies 14,256 patients into 306 subtypes across 24 cancer types. The outer circle groups these activations by cancer types, allowing visual comparison across major categories. C, Scatter plot comparing the subtype-level GigaTIME-translated virtual mIF activations between TCGA and Providence virtual populations. Each dot denotes the average activation score of a protein channel among all tumors of a cancer subtype.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="gigatime-learns-a-multimodal-ai-model-to-translate-pathology-slides-into-spatial-proteomics-images-bridging-cell-morphology-and-cell-states"&gt;GigaTIME learns a multimodal AI model to translate pathology slides into spatial proteomics images, bridging cell morphology and cell states&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 2. GigaTIME enables translation from hematoxylin and eosin (H&amp;amp;E) to multiplex immunofluorescence (mIF) images. A,B, Bar plot comparing GigaTIME and CycleGAN on the translation performance in terms of Dice score (A) and Pearson correlation (B). C, Scatter plots comparing the activation density of the translated mIF and the ground truth mIF across four channels. D, Qualitative results for a sample H&amp;amp;E whole-slide image from our held-out test set with zoomed-in visualizations of the measured mIF and GigaTIME-translated mIF for DAPI, PD-L1, and CD68 channels." class="wp-image-1153564" height="1418" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/GigaTIME_Figure2.png" width="1241" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. GigaTIME enables translation from hematoxylin and eosin (H&amp;amp;E) to multiplex immunofluorescence (mIF) images. A,B, Bar plot comparing GigaTIME and CycleGAN on the translation performance in terms of Dice score (A) and Pearson correlation (B). C, Scatter plots comparing the activation density of the translated mIF and the ground truth mIF across four channels. D, Qualitative results for a sample H&amp;amp;E whole-slide image from our held-out test set with zoomed-in visualizations of the measured mIF and GigaTIME-translated mIF for DAPI, PD-L1, and CD68 channels.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;GigaTIME learned a cross-modal AI translator from digital pathology to spatial multiplex proteomics by training on 40 million cells with paired H&amp;amp;E slides and mIF images from Providence. To our knowledge, this is the first large-scale study exploring multimodal AI for scaling virtual mIF generation. The high-quality paired data enabled much more accurate cross-modal translation compared to prior state-of-the-art methods (Figure 2).&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="virtual-population-enables-population-scale-discovery-of-associations-between-cell-states-and-key-biomarkers"&gt;Virtual population enables population-scale discovery of associations between cell states and key biomarkers&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 3. GigaTIME identifies novel TIME protein vs biomarker associations at pan-cancer, cancer type, cancer subtype levels. A, GigaTIME generates a virtual population of 14,256 with virtual mIF by translating available H&amp;amp;E images to mIF images, enabling pan-cancer, cancer type, and cancer subtype levels of biomedical discovery. B-G, Correlation analysis between protein channels in virtual mIF and patient biomarkers reveal TIME protein-biomarker associations at pan-cancer level (B), cancer-type level (C-E), and cancer-subtype level (F,G). Circle size denotes significance strength. Circle color denotes the directionality in which the correlation occurs. Channel color denotes high, medium, and low confidence based on pearson correlations evaluated using test set. H, A case study showcasing the activation maps across different virtual mIF channels for a H&amp;amp;E slide in our virtual population, and virtual mIF of sample patches from this slide." class="wp-image-1153565" height="1747" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/GigaTIME_Figure3.png" width="1257" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 3. GigaTIME identifies novel TIME protein vs biomarker associations at pan-cancer, cancer type, cancer subtype levels. A, GigaTIME generates a virtual population of 14,256 with virtual mIF by translating available H&amp;amp;E images to mIF images, enabling pan-cancer, cancer type, and cancer subtype levels of biomedical discovery. B-G, Correlation analysis between protein channels in virtual mIF and patient biomarkers reveal TIME protein-biomarker associations at pan-cancer level (B), cancer-type level (C-E), and cancer-subtype level (F,G). Circle size denotes significance strength. Circle color denotes the directionality in which the correlation occurs. Channel color denotes high, medium, and low confidence based on pearson correlations evaluated using test set. H, A case study showcasing the activation maps across different virtual mIF channels for a H&amp;amp;E slide in our virtual population, and virtual mIF of sample patches from this slide.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;By applying GigaTIME to Providence real-world data, we generated a virtual population of 14,256 patients with virtual mIF and key clinical attributes. After correcting for multiple hypothesis testing, we have identified 1,234 statistically significant associations between tumor immune cell states (CD138, CD20, CD4) and clinical biomarkers (tumor mutation burden, KRAS, KMT2D), from pan-cancer to cancer subtypes (Figure 3). Many of these findings are supported by existing literature. For example, MSI high and TMB high associated with increased activations of TIME-related channels such as CD138. Additionally, the virtual population also uncovered previously unknown associations, such as pan-cancer associations between immune activations and key tumor biomarkers, such as the tumor suppressor KMT2D and the oncogene KRAS).&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="virtual-population-enables-population-scale-discovery-of-tumor-immune-signatures-for-patient-stratification"&gt;Virtual population enables population-scale discovery of tumor immune signatures for patient stratification&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 4. GigaTIME enables effective patient stratification across pathological stages and survival groups. A-C, Correlation analysis between virtual mIF and pathological stages at pan-cancer level (A), cancer-type level (B), and cancer-subtype level (C). Circle size denotes significance strength. Circle color denotes the directionality in which the correlation happens. Channel color denotes high, medium, and low confidence based on pearson correlations evaluated using test set. D-F, Survival analysis on lung cancer by using virtual CD3, virtual CD8, and virtual GigaTIME signature (all 21 GigaTIME protein channels) to stratify patients at pan-cancer level (D) and cancer-type level: lung (E), brain (F). G, Bar plot comparing pan-cancer patient stratification performance in terms of survival log rank p-values among virtual GigaTIME signature and individual virtual protein channels." class="wp-image-1153566" height="1733" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/GigaTIME_Figure4.png" width="1654" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 4. GigaTIME enables effective patient stratification across pathological stages and survival groups. A-C, Correlation analysis between virtual mIF and pathological stages at pan-cancer level (A), cancer-type level (B), and cancer-subtype level (C). Circle size denotes significance strength. Circle color denotes the directionality in which the correlation happens. Channel color denotes high, medium, and low confidence based on pearson correlations evaluated using test set. D-F, Survival analysis on lung cancer by using virtual CD3, virtual CD8, and virtual GigaTIME signature (all 21 GigaTIME protein channels) to stratify patients at pan-cancer level (D) and cancer-type level: lung (E), brain (F). G, Bar plot comparing pan-cancer patient stratification performance in terms of survival log rank p-values among virtual GigaTIME signature and individual virtual protein channels.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The virtual population also uncovered&amp;nbsp;GigaTIME&amp;nbsp;signatures for&amp;nbsp;effective&amp;nbsp;patient stratification&amp;nbsp;across staging and survival profiles&amp;nbsp;(Figure 4), from pan-cancer to cancer subtypes.&amp;nbsp;Prior studies have&amp;nbsp;explored patient stratification based on&amp;nbsp;individual&amp;nbsp;immune&amp;nbsp;proteins&amp;nbsp;such as CD3 and CD8. We found that GigaTIME-simulated CD3 and CD8 are similarly effective. Moreover, the combined GigaTIME signature across all 21 protein channels attained even better patient stratification compared to individual channels.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="virtual-population-uncovers-interesting-spatial-and-combinatorial-interactions"&gt;Virtual population uncovers interesting spatial and combinatorial interactions&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 5. GigaTIME uncovers interesting spatial and combinatorial virtual mIF patterns. A,B,C Bar plots comparing virtual mIF activation density with spatial metrics on identifying TIME protein-biomarker correlations. We investigated three spatial metrics based on entropy (A), signal-to-noise ratio (SNR) (B), and sharpness (C). D,E, Bar plots comparing single-channel and combinatorial-channel (using the OR logical operation) in biomarker associations for two GigaTIME virtual protein pairs: CD138/CD68 (D) and PD-L1/Caspase 3 (E), demonstrating substantially improved associations for the combination. F, Case studies visualizing the virtual mIF activation maps of individual channels (CD138, CD68; PD-L1, Caspase 3) and their combinations." class="wp-image-1153567" height="1606" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/GigaTIME_Figure5.png" width="1249" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 5. GigaTIME uncovers interesting spatial and combinatorial virtual mIF patterns. A,B,C Bar plots comparing virtual mIF activation density with spatial metrics on identifying TIME protein-biomarker correlations. We investigated three spatial metrics based on entropy (A), signal-to-noise ratio (SNR) (B), and sharpness (C). D,E, Bar plots comparing single-channel and combinatorial-channel (using the OR logical operation) in biomarker associations for two GigaTIME virtual protein pairs: CD138/CD68 (D) and PD-L1/Caspase 3 (E), demonstrating substantially improved associations for the combination. F, Case studies visualizing the virtual mIF activation maps of individual channels (CD138, CD68; PD-L1, Caspase 3) and their combinations.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The virtual population uncovered interesting non-linear interactions across the GigaTIME virtual protein channels, revealing associations with spatial features such as sharpness and entropy, as well as with key clinical biomarkers like APC and KMT2D (Figure 6). Such combinatorial studies were previously out of reach given the scarcity of mIF data.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="independent-external-validation-on-tcga"&gt;Independent external validation on TCGA&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 6. Independent validation on a virtual population from TCGA. A, Grid charts showing significantly correlated pan-cancer GigaTIME protein-biomarker pairs in Providence (left), TCGA (middle), and both (right). B, Grid charts showing significantly correlated GigaTIME protein-biomarker pairs for lung cancer in Providence and TCGA. C, Grid chart showing significantly correlated GigaTIME protein-biomarker pairs for LUAD in Providence. Channel color denotes high, medium, and low confidence based on pearson correlations evaluated using test set. D, Case studies with visualizations of H&amp;amp;E slides and the corresponding virtual mIF activations for the pair of a GigaTIME protein channel and a biomarker (mutated/non-mutated), where the patient with the given mutation demonstrates much higher activation scores for that GigaTIME protein channel." class="wp-image-1153568" height="2560" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/GigaTIME_Figure6-1-scaled.png" width="1977" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 6. Independent validation on a virtual population from TCGA. A, Grid charts showing significantly correlated pan-cancer GigaTIME protein-biomarker pairs in Providence (left), TCGA (middle), and both (right). B, Grid charts showing significantly correlated GigaTIME protein-biomarker pairs for lung cancer in Providence and TCGA. C, Grid chart showing significantly correlated GigaTIME protein-biomarker pairs for LUAD in Providence. Channel color denotes high, medium, and low confidence based on pearson correlations evaluated using test set. D, Case studies with visualizations of H&amp;amp;E slides and the corresponding virtual mIF activations for the pair of a GigaTIME protein channel and a biomarker (mutated/non-mutated), where the patient with the given mutation demonstrates much higher activation scores for that GigaTIME protein channel.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;We conducted an independent external validation by applying GigaTIME to 10,200 patients in The Cancer Genome Atlas (TCGA) dataset and studied associations between GigaTIME-simulated virtual mIF and clinical biomarkers available in TCGA. We observed significant concordance across the virtual populations from Providence and TCGA, with a Spearman correlation of 0.88 for virtual protein activations across cancer subtypes. The two populations also uncovered a significant overlap of associations between GigaTIME-simulated protein activations and clinical biomarkers (Fisher’s exact test p  2 × 10−9). On the other hand, the Providence virtual population yielded 33% more significant associations than TCGA, highlighting the value of large and diverse real-world data for clinical discovery.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="gigatime-is-a-promising-step-toward-the-moonshot-of-virtual-patient"&gt;GigaTIME is a promising step toward the moonshot of “virtual patient”&lt;/h2&gt;



&lt;p&gt;By learning to translate across modalities, GigaTIME is a promising step toward “learning the language of patients” for the ultimate goal of developing a “virtual patient”, a high-fidelity digital twin that could one day accurately forecast disease progression and counterfactual treatment response. By converting routinely available cell morphology data into otherwise scarce high-resolution cell states signals, GigaTIME demonstrated the potential in harnessing multimodal AI to scale real-world evidence (RWE) generation.&lt;/p&gt;



&lt;p&gt;Going forward, growth opportunities abound. GigaTIME can be extended to handle more spatial modalities and cell-state channels. It can be integrated into advanced multimodal frameworks such as LLaVA-Med to facilitate conversational image analysis by “talking to the data.” To facilitate research in tumor microenvironment modeling, we have made GigaTIME open-source&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; on Foundry Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and Hugging Face&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;p&gt;GigaTIME is a joint work with Providence and the University of Washington’s Paul G. Allen School of Computer Science &amp;amp; Engineering. It reflects Microsoft’s larger commitment to advancing multimodal generative AI for precision health&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, with other exciting progress such as GigaPath, BiomedCLIP, LLaVA-Rad&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, BiomedJourney, BiomedParse, TrialScope, Curiosity.&lt;/p&gt;







&lt;p&gt;Paper co-authors: &lt;em&gt;Jeya Maria Jose Valanarasu, Hanwen Xu, Naoto Usuyama, Chanwoo Kim, Cliff Wong, Peniel Argaw, Racheli Ben Shimol, Angela Crabtree, Kevin Matlock, Alexandra Q. Bartlett, Jaspreet Bagga, Yu Gu, Sheng Zhang, Tristan Naumann, Bernard A. Fox, Bill Wright, Ari Robicsek, Brian Piening, Carlo Bifulco, Sheng Wang, Hoifung Poon&lt;/em&gt;&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/gigatime-scaling-tumor-microenvironment-modeling-using-virtual-population-generated-by-multimodal-ai/</guid><pubDate>Tue, 09 Dec 2025 16:00:00 +0000</pubDate></item><item><title>[NEW] India proposes charging OpenAI, Google for training AI on copyrighted content (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/india-proposes-charging-openai-google-for-training-ai-on-copyrighted-content/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/06/india-ai.jpg?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;India has proposed a mandatory royalty system for AI companies that train their models on copyrighted content&amp;nbsp;—&amp;nbsp;a move that could reshape how OpenAI and Google operate in what has already become one of their most important and fastest-growing markets globally.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Tuesday, India’s Department for Promotion of Industry and Internal Trade released a proposed framework that would give AI companies access to all copyrighted works for training in exchange for paying royalties to a new collecting body composed of rights-holding organizations, with payments then distributed to creators. The proposal argues that this “mandatory blanket license” would lower compliance costs for AI firms while ensuring that writers, musicians, artists, and other rights holders are compensated when their work is scraped to train commercial models.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;India’s proposal comes amid mounting concerns in global markets over how AI companies train their models on copyrighted material, a practice that has triggered lawsuits from authors, news organizations, artists, and other rights holders in the U.S. and Europe. Courts and regulators are still weighing whether such training qualifies as fair use, leaving AI firms operating under legal uncertainty and allowing them to rapidly expand their business without clear regulations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unlike the U.S. and the European Union, where policymakers are debating transparency obligations and fair-use boundaries, India is proposing one of the most interventionist approaches yet by giving AI companies automatic access to copyrighted material in exchange for mandatory payment. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The eight-member committee, formed by the Indian government in late April, argues the system would avoid years of legal uncertainty while ensuring creators are compensated from the outset.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Defending the system, the committee says in a 125-page submission (PDF) that a blanket license “aims to provide an easy access to content for AI developers… reduce transaction costs… [and] ensure fair compensation for rightsholders,” calling it the least burdensome way to manage large-scale AI training. The submission adds that the single collecting body would function as a “single window,” eliminating the need for individual negotiations and enabling royalties to flow to both registered and unregistered creators.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The committee also points to India’s growing importance as a market for GenAI tools. Citing OpenAI CEO Sam Altman’s remark that India is the company’s second-largest market after the U.S. and “may well become our largest,” it argues that because AI firms derive significant revenue from Indian users while relying on Indian creators’ work to train their models, a portion of that value should flow back to those creators. That, it says, is part of the rationale for establishing a “balanced framework” that guarantees compensation.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;India’s proposal lands amid intensifying legal battles worldwide over whether AI companies can lawfully use copyrighted material to train their models. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In India, news agency ANI sued OpenAI in the Delhi High Court, arguing its articles were used without permission — a case that has prompted the court to examine whether AI training is itself an act of reproduction or protected by “fair dealing.” Courts in the U.S. and Europe are confronting similar disputes, with creators alleging that tech companies have built their models on unlicensed content.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-ai-proposal-sees-pushback-and-dissent"&gt;AI proposal sees pushback and dissent &lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Not everyone is convinced by the Indian government’s proposed model, though. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Nasscom, the industry body representing technology firms, including Google and Microsoft, filed a formal dissent arguing that India should instead adopt a broad text-and-data-mining exception that would allow AI developers to train on copyrighted content as long as the material is lawfully accessed. It warned that a mandatory licensing regime could slow innovation and said rightsholders who object should be allowed to opt out rather than force companies to pay for all training data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Business Software Alliance, which represents global tech firms, including Adobe, Amazon Web Services, and Microsoft, pressed the Indian government to avoid a purely licensing-based regime. It urged India to introduce an explicit text-and-data-mining exception, arguing that “relying solely on direct or statutory licensing for AI training data may be impractical and may not yield the best outcomes.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Limiting AI models to smaller sets of licensed or public-domain material, BSA warned, could reduce model quality and “increase the risk that outputs simply reflect trends and biases of the limited training data sets,” adding that a clear TDM exception would better balance innovation and rights holders’ interests.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The committee did not consider both a broad text-and-data-mining exception and an opt-out model, arguing that such systems either undermine copyright protections or are impossible to enforce. Instead, it proposed a “hybrid model” that would grant AI firms automatic access to all lawfully available copyrighted works while requiring them to pay royalties into the central collecting body that distributes the proceeds to creators.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Indian government has now opened the proposal for public consultation, giving companies and other stakeholders 30 days to submit their comments. After reviewing the feedback, the committee will finalize its recommendations before the framework is taken up by the government.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI and Google did not respond to requests for comments.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/06/india-ai.jpg?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;India has proposed a mandatory royalty system for AI companies that train their models on copyrighted content&amp;nbsp;—&amp;nbsp;a move that could reshape how OpenAI and Google operate in what has already become one of their most important and fastest-growing markets globally.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Tuesday, India’s Department for Promotion of Industry and Internal Trade released a proposed framework that would give AI companies access to all copyrighted works for training in exchange for paying royalties to a new collecting body composed of rights-holding organizations, with payments then distributed to creators. The proposal argues that this “mandatory blanket license” would lower compliance costs for AI firms while ensuring that writers, musicians, artists, and other rights holders are compensated when their work is scraped to train commercial models.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;India’s proposal comes amid mounting concerns in global markets over how AI companies train their models on copyrighted material, a practice that has triggered lawsuits from authors, news organizations, artists, and other rights holders in the U.S. and Europe. Courts and regulators are still weighing whether such training qualifies as fair use, leaving AI firms operating under legal uncertainty and allowing them to rapidly expand their business without clear regulations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unlike the U.S. and the European Union, where policymakers are debating transparency obligations and fair-use boundaries, India is proposing one of the most interventionist approaches yet by giving AI companies automatic access to copyrighted material in exchange for mandatory payment. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The eight-member committee, formed by the Indian government in late April, argues the system would avoid years of legal uncertainty while ensuring creators are compensated from the outset.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Defending the system, the committee says in a 125-page submission (PDF) that a blanket license “aims to provide an easy access to content for AI developers… reduce transaction costs… [and] ensure fair compensation for rightsholders,” calling it the least burdensome way to manage large-scale AI training. The submission adds that the single collecting body would function as a “single window,” eliminating the need for individual negotiations and enabling royalties to flow to both registered and unregistered creators.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The committee also points to India’s growing importance as a market for GenAI tools. Citing OpenAI CEO Sam Altman’s remark that India is the company’s second-largest market after the U.S. and “may well become our largest,” it argues that because AI firms derive significant revenue from Indian users while relying on Indian creators’ work to train their models, a portion of that value should flow back to those creators. That, it says, is part of the rationale for establishing a “balanced framework” that guarantees compensation.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;India’s proposal lands amid intensifying legal battles worldwide over whether AI companies can lawfully use copyrighted material to train their models. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In India, news agency ANI sued OpenAI in the Delhi High Court, arguing its articles were used without permission — a case that has prompted the court to examine whether AI training is itself an act of reproduction or protected by “fair dealing.” Courts in the U.S. and Europe are confronting similar disputes, with creators alleging that tech companies have built their models on unlicensed content.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-ai-proposal-sees-pushback-and-dissent"&gt;AI proposal sees pushback and dissent &lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Not everyone is convinced by the Indian government’s proposed model, though. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Nasscom, the industry body representing technology firms, including Google and Microsoft, filed a formal dissent arguing that India should instead adopt a broad text-and-data-mining exception that would allow AI developers to train on copyrighted content as long as the material is lawfully accessed. It warned that a mandatory licensing regime could slow innovation and said rightsholders who object should be allowed to opt out rather than force companies to pay for all training data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Business Software Alliance, which represents global tech firms, including Adobe, Amazon Web Services, and Microsoft, pressed the Indian government to avoid a purely licensing-based regime. It urged India to introduce an explicit text-and-data-mining exception, arguing that “relying solely on direct or statutory licensing for AI training data may be impractical and may not yield the best outcomes.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Limiting AI models to smaller sets of licensed or public-domain material, BSA warned, could reduce model quality and “increase the risk that outputs simply reflect trends and biases of the limited training data sets,” adding that a clear TDM exception would better balance innovation and rights holders’ interests.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The committee did not consider both a broad text-and-data-mining exception and an opt-out model, arguing that such systems either undermine copyright protections or are impossible to enforce. Instead, it proposed a “hybrid model” that would grant AI firms automatic access to all lawfully available copyrighted works while requiring them to pay royalties into the central collecting body that distributes the proceeds to creators.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Indian government has now opened the proposal for public consultation, giving companies and other stakeholders 30 days to submit their comments. After reviewing the feedback, the committee will finalize its recommendations before the framework is taken up by the government.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI and Google did not respond to requests for comments.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/india-proposes-charging-openai-google-for-training-ai-on-copyrighted-content/</guid><pubDate>Tue, 09 Dec 2025 16:09:17 +0000</pubDate></item><item><title>[NEW] Microsoft to invest $17.5B in India by 2029 as AI race accelerates (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/microsoft-to-invest-17-5b-in-india-by-2029-as-ai-race-accelerates/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2020/01/GettyImages-1173107665-1.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft plans to invest $17.5 billion in India over the next four years, expanding its AI and cloud footprint in the South Asian nation, whose vast online and smartphone user base is turning it into a critical battleground for global tech companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Announced on Tuesday, the investment — Microsoft’s largest in Asia — will fund new data centers, AI infrastructure, and skilling programs from 2026 to 2029, building on the $3 billion the company committed in India in January.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Microsoft’s move comes as major U.S. tech companies ramp up spending on data centers and AI compute worldwide, with India emerging as a strategic prize thanks to its fast-growing developer base and one of the world’s largest pools of internet and smartphone users. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The latest push also puts pressure on rivals such as Google, Amazon, and OpenAI, which are growing their presence in India to tap demand for cloud services and AI tools from businesses, startups, and government agencies. Moreover, the investment aligns with New Delhi’s push to accelerate digital infrastructure and AI adoption across sectors, as India looks to position itself as a global technology hub while addressing concerns around data governance and equitable access.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The announcement comes during Microsoft CEO Satya Nadella’s visit to India and follows his meeting with Prime Minister Narendra Modi on Tuesday, ahead of a keynote in New Delhi on Wednesday. The Redmond-based company also said it will open a new data center region in Hyderabad by mid-2026, its largest in India, comprising three availability zones — a footprint the company described as roughly the size of two Eden Gardens stadiums. Microsoft said it will continue expanding its three existing data-center regions in Chennai, Hyderabad, and Pune.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As part of the push, Microsoft also announced it will work with the Ministry of Labour and Employment to integrate advanced AI capabilities into two of its flagship digital public platforms — e-Shram and the National Career Service — to offer AI-driven services to more than 310 million informal workers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The two Indian government platforms will use Microsoft’s Azure OpenAI Service to provide multilingual access, AI-assisted job matching, predictive analytics on skill and demand trends, automated résumé creation, and personalized pathways, the company said.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft also said it is rolling out new sovereign cloud options for Indian customers, including a Sovereign Public Cloud now available across its India regions and a Sovereign Private Cloud powered by Azure Local for both connected and air-gapped operations. The offerings would help enterprises meet regulatory and data-residency requirements and support high-performance workloads with access to the latest Nvidia GPUs and Microsoft 365 services, the company noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, Microsoft said its skilling efforts are also accelerating, noting that through its “ADVANTA(I)GE India” initiative, it has trained 5.6 million people since January — well ahead of its goal of training 10 million by 2030 — with the programs enabling more than 125,000 individuals to secure jobs or start businesses. The company is doubling its earlier commitment and now aims to equip 20 million Indians with basic AI skills by 2030, working with government agencies, industry partners, and digital public platforms to broaden access to training.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-india-attracts-global-tech-firms-in-the-ai-era"&gt;India attracts global tech firms in the AI era&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft’s investment commitment comes just months after Google announced a $15 billion plan to build an AI hub and data-center infrastructure in India — the company’s largest investment in the country and one that follows its earlier $10 billion pledge in 2020.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In the recent months, India has emerged as an especially attractive market for global tech companies as they look for regions to expand their AI footprint, drawn by the country’s vast base of internet subscribers, hundreds of millions of smartphone users, a fast-growing startup ecosystem, and the Indian government’s aggressive digitization agenda — all of which promise both consumer scale and enterprise demand. The push has accelerated this year, with OpenAI and Anthropic setting up offices in India, and Google and Perplexity striking partnerships with major telecom operators Reliance Jio and Bharti Airtel, respectively, to deepen their reach in the market.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, even as global tech firms ramp up investment, hyperscalers are expected to face significant constraints in India, where data-center expansion is challenged by patchy power availability, high energy costs, and water scarcity in several regions — factors that could slow the build-out of AI infrastructure and raise operating expenses for cloud providers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nonetheless, the Indian government has been pushing aggressively to draw more big-tech investment, framing large-scale data-center and AI projects as central to its economic and digital-public-infrastructure ambitions. Despite the constraints, New Delhi has rolled out incentives for AI and semiconductor projects, eased some regulatory hurdles, and encouraged partnerships with domestic telecom and IT firms to anchor more of the global AI value chain in India.&lt;/p&gt;

&lt;figure class="wp-block-embed aligncenter is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;When it comes to AI, the world is optimistic about India! &lt;/p&gt;&lt;p&gt;Had a very productive discussion with Mr. Satya Nadella. Happy to see India being the place where Microsoft will make its largest-ever investment in Asia. &lt;/p&gt;&lt;p&gt;The youth of India will harness this opportunity to innovate… https://t.co/fMFcGQ8ctK&lt;/p&gt;— Narendra Modi (@narendramodi) December 9, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“Microsoft has been part of India’s fabric for more than three decades,” said Puneet Chandok, president, Microsoft India and South Asia, in a prepared statement. “As the nation moves confidently into its AI-first future, we are proud to stand as a trusted partner in advancing the infrastructure, innovation, and opportunity that can power a billion dreams.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft already employs more than 22,000 people across Bengaluru, Hyderabad, Pune, Gurugram, Noida, and other cities, including engineering teams that build AI products such as Copilot Studio, Azure AI Search, AI agents, speech and translation tools, and Azure Machine Learning for global markets, while also supporting the company’s domestic operations.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2020/01/GettyImages-1173107665-1.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft plans to invest $17.5 billion in India over the next four years, expanding its AI and cloud footprint in the South Asian nation, whose vast online and smartphone user base is turning it into a critical battleground for global tech companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Announced on Tuesday, the investment — Microsoft’s largest in Asia — will fund new data centers, AI infrastructure, and skilling programs from 2026 to 2029, building on the $3 billion the company committed in India in January.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Microsoft’s move comes as major U.S. tech companies ramp up spending on data centers and AI compute worldwide, with India emerging as a strategic prize thanks to its fast-growing developer base and one of the world’s largest pools of internet and smartphone users. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The latest push also puts pressure on rivals such as Google, Amazon, and OpenAI, which are growing their presence in India to tap demand for cloud services and AI tools from businesses, startups, and government agencies. Moreover, the investment aligns with New Delhi’s push to accelerate digital infrastructure and AI adoption across sectors, as India looks to position itself as a global technology hub while addressing concerns around data governance and equitable access.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The announcement comes during Microsoft CEO Satya Nadella’s visit to India and follows his meeting with Prime Minister Narendra Modi on Tuesday, ahead of a keynote in New Delhi on Wednesday. The Redmond-based company also said it will open a new data center region in Hyderabad by mid-2026, its largest in India, comprising three availability zones — a footprint the company described as roughly the size of two Eden Gardens stadiums. Microsoft said it will continue expanding its three existing data-center regions in Chennai, Hyderabad, and Pune.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As part of the push, Microsoft also announced it will work with the Ministry of Labour and Employment to integrate advanced AI capabilities into two of its flagship digital public platforms — e-Shram and the National Career Service — to offer AI-driven services to more than 310 million informal workers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The two Indian government platforms will use Microsoft’s Azure OpenAI Service to provide multilingual access, AI-assisted job matching, predictive analytics on skill and demand trends, automated résumé creation, and personalized pathways, the company said.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft also said it is rolling out new sovereign cloud options for Indian customers, including a Sovereign Public Cloud now available across its India regions and a Sovereign Private Cloud powered by Azure Local for both connected and air-gapped operations. The offerings would help enterprises meet regulatory and data-residency requirements and support high-performance workloads with access to the latest Nvidia GPUs and Microsoft 365 services, the company noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, Microsoft said its skilling efforts are also accelerating, noting that through its “ADVANTA(I)GE India” initiative, it has trained 5.6 million people since January — well ahead of its goal of training 10 million by 2030 — with the programs enabling more than 125,000 individuals to secure jobs or start businesses. The company is doubling its earlier commitment and now aims to equip 20 million Indians with basic AI skills by 2030, working with government agencies, industry partners, and digital public platforms to broaden access to training.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-india-attracts-global-tech-firms-in-the-ai-era"&gt;India attracts global tech firms in the AI era&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft’s investment commitment comes just months after Google announced a $15 billion plan to build an AI hub and data-center infrastructure in India — the company’s largest investment in the country and one that follows its earlier $10 billion pledge in 2020.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In the recent months, India has emerged as an especially attractive market for global tech companies as they look for regions to expand their AI footprint, drawn by the country’s vast base of internet subscribers, hundreds of millions of smartphone users, a fast-growing startup ecosystem, and the Indian government’s aggressive digitization agenda — all of which promise both consumer scale and enterprise demand. The push has accelerated this year, with OpenAI and Anthropic setting up offices in India, and Google and Perplexity striking partnerships with major telecom operators Reliance Jio and Bharti Airtel, respectively, to deepen their reach in the market.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, even as global tech firms ramp up investment, hyperscalers are expected to face significant constraints in India, where data-center expansion is challenged by patchy power availability, high energy costs, and water scarcity in several regions — factors that could slow the build-out of AI infrastructure and raise operating expenses for cloud providers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nonetheless, the Indian government has been pushing aggressively to draw more big-tech investment, framing large-scale data-center and AI projects as central to its economic and digital-public-infrastructure ambitions. Despite the constraints, New Delhi has rolled out incentives for AI and semiconductor projects, eased some regulatory hurdles, and encouraged partnerships with domestic telecom and IT firms to anchor more of the global AI value chain in India.&lt;/p&gt;

&lt;figure class="wp-block-embed aligncenter is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;When it comes to AI, the world is optimistic about India! &lt;/p&gt;&lt;p&gt;Had a very productive discussion with Mr. Satya Nadella. Happy to see India being the place where Microsoft will make its largest-ever investment in Asia. &lt;/p&gt;&lt;p&gt;The youth of India will harness this opportunity to innovate… https://t.co/fMFcGQ8ctK&lt;/p&gt;— Narendra Modi (@narendramodi) December 9, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“Microsoft has been part of India’s fabric for more than three decades,” said Puneet Chandok, president, Microsoft India and South Asia, in a prepared statement. “As the nation moves confidently into its AI-first future, we are proud to stand as a trusted partner in advancing the infrastructure, innovation, and opportunity that can power a billion dreams.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft already employs more than 22,000 people across Bengaluru, Hyderabad, Pune, Gurugram, Noida, and other cities, including engineering teams that build AI products such as Copilot Studio, Azure AI Search, AI agents, speech and translation tools, and Azure Machine Learning for global markets, while also supporting the company’s domestic operations.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/microsoft-to-invest-17-5b-in-india-by-2029-as-ai-race-accelerates/</guid><pubDate>Tue, 09 Dec 2025 16:20:12 +0000</pubDate></item><item><title>[NEW] Accenture and Anthropic partner to boost enterprise AI integration (AI News)</title><link>https://www.artificialintelligence-news.com/news/accenture-anthropic-partner-boost-enterprise-ai-integration/</link><description>&lt;p&gt;Accenture and Anthropic are setting out to boost enterprise AI integration with a newly-expanded partnership.&lt;/p&gt;&lt;p&gt;While 2024 was defined by corporate curiosity regarding Large Language Models (LLMs), the current mandate for business leaders is operationalising these tools to achieve a return on investment.&lt;/p&gt;&lt;p&gt;The new Accenture Anthropic Business Group combines Anthropic’s model capabilities with Accenture’s implementation machinery to industrialise the deployment of generative AI across regulated sectors.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-industrialising-the-developer-workflow"&gt;Industrialising the developer workflow&lt;/h3&gt;&lt;p&gt;A primary component of this collaboration focuses on software engineering. Coding assistance is often seen as the path of least resistance for AI adoption, yet integrating these tools into existing CI/CD pipelines remains complex.&lt;/p&gt;&lt;p&gt;Accenture is positioning itself as a primary partner for Claude Code, Anthropic’s coding tool, which the company claims now holds over half of the AI coding market. The consultancy plans to train approximately 30,000 of its own professionals on Claude, creating one of the largest global ecosystems of practitioners familiar with the tool.&lt;/p&gt;&lt;p&gt;The promise of deeper enterprise integration of AI coding tools is a complete restructuring of the development hierarchy. The joint offering suggests that junior developers can utilise these tools to produce senior-level code and complete integration tasks more quickly to reduce onboarding times from months to weeks. Senior developers can then concentrate on high-value architecture, validation, and oversight.&lt;/p&gt;&lt;p&gt;Dario Amodei, CEO and Co-Founder of Anthropic, said: “AI is changing how almost everyone works, and enterprises need both cutting-edge AI and trusted expertise to deploy it at scale. Accenture brings deep enterprise transformation experience, and Anthropic brings the most capable models.&lt;/p&gt;&lt;p&gt;“Our new partnership means that tens of thousands of Accenture developers will be using Claude Code, making this our largest ever deployment—and the new Accenture Anthropic Business Group will help enterprise clients use our smartest AI models to make major productivity gains.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-justifying-ai-inference-costs-and-removing-deployment-barriers"&gt;Justifying AI inference costs and removing deployment barriers&lt;/h3&gt;&lt;p&gt;A persistent friction point for enterprise leaders seeking deeper AI integration is justifying the ongoing cost of inference against actual business value. To counter this, the partnership is launching a specific product designed to help CIOs measure value and drive adoption across engineering organisations.&lt;/p&gt;&lt;p&gt;This offering attempts to provide a structured path for software design and maintenance, moving beyond the ad-hoc usage of coding assistants. It combines Claude Code with a framework for quantifying productivity gains and workflow redesigns tailored for AI-first development teams.&lt;/p&gt;&lt;p&gt;For the enterprise, the goal is to translate individual developer efficiency into broader company impact; such as shorter development cycles and faster time-to-market for new products.&lt;/p&gt;&lt;p&gt;However, the most substantial barrier to AI adoption in the Global 2000 remains compliance. Sectors such as financial services, healthcare, and the public sector face strict governance requirements that often stall AI initiatives.&lt;/p&gt;&lt;p&gt;Accenture and Anthropic are developing industry-specific enterprise AI solutions to address these deployment challenges. In financial services, for instance, the focus is on automating compliance workflows and processing complex documents with the precision required for high-stakes decisions.&lt;/p&gt;&lt;p&gt;Health and life sciences firms face a parallel demand. Here, the partnership aims to leverage Claude’s analytical capabilities to query proprietary datasets and streamline clinical trial processing. For the public sector, the utility lies in AI agents that assist citizens in navigating government services while adhering to statutory data privacy requirements.&lt;/p&gt;&lt;p&gt;Julie Sweet, Chair and CEO of Accenture, commented: “With the powerful combination of Anthropic’s Claude capabilities and Accenture’s AI expertise and industry and function domain knowledge, organisations can embed AI everywhere responsibly and at speed – from software development to customer experience – to drive innovation, unlock new sources of growth, and build their confidence to lead in the age of AI.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-how-accenture-and-anthropic-are-mitigating-risks-to-support-enterprise-ai-integration"&gt;How Accenture and Anthropic are mitigating risks to support enterprise AI integration&lt;/h3&gt;&lt;p&gt;To mitigate the risks associated with deploying non-deterministic models, the partnership emphasises “responsible AI.” This involves combining Anthropic’s “constitutional AI” principles – which embed safety rules directly into the model – with Accenture’s governance expertise.&lt;/p&gt;&lt;p&gt;Practical implementation will occur through Accenture’s network of Innovation Hubs, which will serve as controlled environments or “sandboxes”. These hubs allow clients to prototype and validate solutions without exposing production systems or sensitive data to risk. The companies also plan to co-invest in a ‘Claude Center of Excellence’ to design bespoke AI offerings tailored to specific industry needs.&lt;/p&gt;&lt;p&gt;This expanded partnership with Accenture follows Anthropic reporting a growth in its enterprise AI market share from 24 percent to 40 percent. For Accenture, establishing a dedicated business group with specific go-to-market focus reflects a long-term commitment to the platform.&lt;/p&gt;&lt;p&gt;The era of standalone AI pilots is fading. The next phase for enterprise AI integration demands tight coupling between model capabilities, workforce training, and rigorous value measurement.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;OpenAI targets AI skills gap with new certification standards&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-111183" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/image-3.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Accenture and Anthropic are setting out to boost enterprise AI integration with a newly-expanded partnership.&lt;/p&gt;&lt;p&gt;While 2024 was defined by corporate curiosity regarding Large Language Models (LLMs), the current mandate for business leaders is operationalising these tools to achieve a return on investment.&lt;/p&gt;&lt;p&gt;The new Accenture Anthropic Business Group combines Anthropic’s model capabilities with Accenture’s implementation machinery to industrialise the deployment of generative AI across regulated sectors.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-industrialising-the-developer-workflow"&gt;Industrialising the developer workflow&lt;/h3&gt;&lt;p&gt;A primary component of this collaboration focuses on software engineering. Coding assistance is often seen as the path of least resistance for AI adoption, yet integrating these tools into existing CI/CD pipelines remains complex.&lt;/p&gt;&lt;p&gt;Accenture is positioning itself as a primary partner for Claude Code, Anthropic’s coding tool, which the company claims now holds over half of the AI coding market. The consultancy plans to train approximately 30,000 of its own professionals on Claude, creating one of the largest global ecosystems of practitioners familiar with the tool.&lt;/p&gt;&lt;p&gt;The promise of deeper enterprise integration of AI coding tools is a complete restructuring of the development hierarchy. The joint offering suggests that junior developers can utilise these tools to produce senior-level code and complete integration tasks more quickly to reduce onboarding times from months to weeks. Senior developers can then concentrate on high-value architecture, validation, and oversight.&lt;/p&gt;&lt;p&gt;Dario Amodei, CEO and Co-Founder of Anthropic, said: “AI is changing how almost everyone works, and enterprises need both cutting-edge AI and trusted expertise to deploy it at scale. Accenture brings deep enterprise transformation experience, and Anthropic brings the most capable models.&lt;/p&gt;&lt;p&gt;“Our new partnership means that tens of thousands of Accenture developers will be using Claude Code, making this our largest ever deployment—and the new Accenture Anthropic Business Group will help enterprise clients use our smartest AI models to make major productivity gains.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-justifying-ai-inference-costs-and-removing-deployment-barriers"&gt;Justifying AI inference costs and removing deployment barriers&lt;/h3&gt;&lt;p&gt;A persistent friction point for enterprise leaders seeking deeper AI integration is justifying the ongoing cost of inference against actual business value. To counter this, the partnership is launching a specific product designed to help CIOs measure value and drive adoption across engineering organisations.&lt;/p&gt;&lt;p&gt;This offering attempts to provide a structured path for software design and maintenance, moving beyond the ad-hoc usage of coding assistants. It combines Claude Code with a framework for quantifying productivity gains and workflow redesigns tailored for AI-first development teams.&lt;/p&gt;&lt;p&gt;For the enterprise, the goal is to translate individual developer efficiency into broader company impact; such as shorter development cycles and faster time-to-market for new products.&lt;/p&gt;&lt;p&gt;However, the most substantial barrier to AI adoption in the Global 2000 remains compliance. Sectors such as financial services, healthcare, and the public sector face strict governance requirements that often stall AI initiatives.&lt;/p&gt;&lt;p&gt;Accenture and Anthropic are developing industry-specific enterprise AI solutions to address these deployment challenges. In financial services, for instance, the focus is on automating compliance workflows and processing complex documents with the precision required for high-stakes decisions.&lt;/p&gt;&lt;p&gt;Health and life sciences firms face a parallel demand. Here, the partnership aims to leverage Claude’s analytical capabilities to query proprietary datasets and streamline clinical trial processing. For the public sector, the utility lies in AI agents that assist citizens in navigating government services while adhering to statutory data privacy requirements.&lt;/p&gt;&lt;p&gt;Julie Sweet, Chair and CEO of Accenture, commented: “With the powerful combination of Anthropic’s Claude capabilities and Accenture’s AI expertise and industry and function domain knowledge, organisations can embed AI everywhere responsibly and at speed – from software development to customer experience – to drive innovation, unlock new sources of growth, and build their confidence to lead in the age of AI.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-how-accenture-and-anthropic-are-mitigating-risks-to-support-enterprise-ai-integration"&gt;How Accenture and Anthropic are mitigating risks to support enterprise AI integration&lt;/h3&gt;&lt;p&gt;To mitigate the risks associated with deploying non-deterministic models, the partnership emphasises “responsible AI.” This involves combining Anthropic’s “constitutional AI” principles – which embed safety rules directly into the model – with Accenture’s governance expertise.&lt;/p&gt;&lt;p&gt;Practical implementation will occur through Accenture’s network of Innovation Hubs, which will serve as controlled environments or “sandboxes”. These hubs allow clients to prototype and validate solutions without exposing production systems or sensitive data to risk. The companies also plan to co-invest in a ‘Claude Center of Excellence’ to design bespoke AI offerings tailored to specific industry needs.&lt;/p&gt;&lt;p&gt;This expanded partnership with Accenture follows Anthropic reporting a growth in its enterprise AI market share from 24 percent to 40 percent. For Accenture, establishing a dedicated business group with specific go-to-market focus reflects a long-term commitment to the platform.&lt;/p&gt;&lt;p&gt;The era of standalone AI pilots is fading. The next phase for enterprise AI integration demands tight coupling between model capabilities, workforce training, and rigorous value measurement.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;OpenAI targets AI skills gap with new certification standards&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-111183" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/image-3.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/accenture-anthropic-partner-boost-enterprise-ai-integration/</guid><pubDate>Tue, 09 Dec 2025 16:20:18 +0000</pubDate></item><item><title>[NEW] EU launches antitrust probe into Google’s AI search tools (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/eu-launches-antitrust-probe-into-googles-ai-search-tools/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/GettyImages-2206888090.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Even as Big Tech and American tech elites criticize how the European Union is implementing rules to regulate tech and AI on the continent, the bloc isn’t letting competition concerns slide. The European Commission has launched an investigation into whether Google may have breached EU’s competition laws by using content from websites without compensating owners to generate answers for its AI summaries that appear above search results.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The EC also will look at how AI summaries use videos from YouTube to generate answers. The investigation will examine if Google is harming competition in the AI market by granting itself access to websites’ content, and imposing “unfair terms and conditions on publishers and content creators.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The Commission will investigate to what extent the generation of AI Overviews and AI Mode by Google is based on web publishers’ content without appropriate compensation for that, and without the possibility for publishers to refuse without losing access to Google Search,” the bloc’s executive arm wrote in a statement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google’s AI Overview and AI Mode are the two chief products being investigated here, and the EC highlights that the tech giant doesn’t leave websites or content producers with much choice since it directs a majority of web traffic, doesn’t pay them for using their content, and doesn’t allow YouTube uploads if you don’t let Google use that data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The EU is also concerned over the fact that Google doesn’t allow rival AI companies to use YouTube content to train their own AI models. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This complaint risks stifling innovation in a market that is more competitive than ever,” a Google spokesperson said in an emailed statement. “Europeans deserve to benefit from the latest technologies and we will continue to work closely with the news and creative industries as they transition to the AI era.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The investigation comes at a time when companies developing AI models and content are being sued for copyright infringement by publishers and websites. AI search tool Perplexity, for one, has been sued by several  outlets, including The New York Times, Chicago Tribune, News Corp, New York Post, Merriam-Webster, Nikkei, and Reddit.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The EU’s investigation differs, however, because in many cases, these media companies are suing as a way to negotiate content-licensing deals with AI firms in hopes of compensating creators and being paid for their content. The EU is seeking to level the playing field for AI companies that compete with Google, which according to some reports, benefits from its reach by being able to train its AI models on much more of the internet than its rivals.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Under consistent and widespread criticism of its AI regulation, however, the EU is considering simplifying its AI rules, and has proposed to delay the implementation of rules for the use of AI in high-risk applications.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/GettyImages-2206888090.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Even as Big Tech and American tech elites criticize how the European Union is implementing rules to regulate tech and AI on the continent, the bloc isn’t letting competition concerns slide. The European Commission has launched an investigation into whether Google may have breached EU’s competition laws by using content from websites without compensating owners to generate answers for its AI summaries that appear above search results.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The EC also will look at how AI summaries use videos from YouTube to generate answers. The investigation will examine if Google is harming competition in the AI market by granting itself access to websites’ content, and imposing “unfair terms and conditions on publishers and content creators.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The Commission will investigate to what extent the generation of AI Overviews and AI Mode by Google is based on web publishers’ content without appropriate compensation for that, and without the possibility for publishers to refuse without losing access to Google Search,” the bloc’s executive arm wrote in a statement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google’s AI Overview and AI Mode are the two chief products being investigated here, and the EC highlights that the tech giant doesn’t leave websites or content producers with much choice since it directs a majority of web traffic, doesn’t pay them for using their content, and doesn’t allow YouTube uploads if you don’t let Google use that data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The EU is also concerned over the fact that Google doesn’t allow rival AI companies to use YouTube content to train their own AI models. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This complaint risks stifling innovation in a market that is more competitive than ever,” a Google spokesperson said in an emailed statement. “Europeans deserve to benefit from the latest technologies and we will continue to work closely with the news and creative industries as they transition to the AI era.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The investigation comes at a time when companies developing AI models and content are being sued for copyright infringement by publishers and websites. AI search tool Perplexity, for one, has been sued by several  outlets, including The New York Times, Chicago Tribune, News Corp, New York Post, Merriam-Webster, Nikkei, and Reddit.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The EU’s investigation differs, however, because in many cases, these media companies are suing as a way to negotiate content-licensing deals with AI firms in hopes of compensating creators and being paid for their content. The EU is seeking to level the playing field for AI companies that compete with Google, which according to some reports, benefits from its reach by being able to train its AI models on much more of the internet than its rivals.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Under consistent and widespread criticism of its AI regulation, however, the EU is considering simplifying its AI rules, and has proposed to delay the implementation of rules for the use of AI in high-risk applications.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/eu-launches-antitrust-probe-into-googles-ai-search-tools/</guid><pubDate>Tue, 09 Dec 2025 16:29:49 +0000</pubDate></item><item><title>[NEW] Google’s first AI glasses expected next year (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/googles-first-ai-glasses-expected-next-year/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/glasses_2745f1.jpg?resize=1200,716" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google will launch its first AI glasses in 2026, according to a company blog post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At Google’s I/O event in May, the company announced partnerships with Gentle Monster and Warby Parker to create consumer wearables based on Android XR, the operating system that powers Samsung’s Galaxy XR headset.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;But you can’t wear a bulky headset while out in the real world, which makes smart glasses appealing as a less obtrusive smart wearable.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“For AI and XR to be truly helpful, the hardware needs to fit seamlessly into your life and match your personal style,” Google writes. “We want to give you the freedom to choose the right balance of weight, style and immersion for your needs.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is working on various types of AI-powered glasses — one model is designed for screen-free assistance, using built-in speakers, microphones, and cameras to allow the user to communicate with Gemini and take photos. The other model has an in-lens display — which is only visible to the person wearing the glasses — that can show turn-by-turn directions or closed captioning. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google also shared a preview of the wired XR glasses from Xreal called Project Aura. This model situates itself between a bulky headset and an unobtrusive pair of glasses. Beyond just an in-lens display, the Project Aura glasses can function as an extended workplace or entertainment device, allowing the user to use Google’s suite of products or stream video as they would in a more advanced headset.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Meta has gotten off to an early lead in smart glasses development, Google now joins Apple and Snap among the companies expected to challenge Meta with their own hardware next year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta’s smart glasses have caught on in part thanks to its partnership with Ray-Ban, and it sells these products in retail stores. Google’s partnership with Warby Parker seems like it will follow a similar strategy, committing $75 million thus far to support the eyewear company’s product development and commercialization costs. If Warby Parker meets certain milestones, Google will commit an additional $75 million and take an equity stake in the brand.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/glasses_2745f1.jpg?resize=1200,716" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google will launch its first AI glasses in 2026, according to a company blog post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At Google’s I/O event in May, the company announced partnerships with Gentle Monster and Warby Parker to create consumer wearables based on Android XR, the operating system that powers Samsung’s Galaxy XR headset.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;But you can’t wear a bulky headset while out in the real world, which makes smart glasses appealing as a less obtrusive smart wearable.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“For AI and XR to be truly helpful, the hardware needs to fit seamlessly into your life and match your personal style,” Google writes. “We want to give you the freedom to choose the right balance of weight, style and immersion for your needs.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is working on various types of AI-powered glasses — one model is designed for screen-free assistance, using built-in speakers, microphones, and cameras to allow the user to communicate with Gemini and take photos. The other model has an in-lens display — which is only visible to the person wearing the glasses — that can show turn-by-turn directions or closed captioning. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google also shared a preview of the wired XR glasses from Xreal called Project Aura. This model situates itself between a bulky headset and an unobtrusive pair of glasses. Beyond just an in-lens display, the Project Aura glasses can function as an extended workplace or entertainment device, allowing the user to use Google’s suite of products or stream video as they would in a more advanced headset.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Meta has gotten off to an early lead in smart glasses development, Google now joins Apple and Snap among the companies expected to challenge Meta with their own hardware next year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta’s smart glasses have caught on in part thanks to its partnership with Ray-Ban, and it sells these products in retail stores. Google’s partnership with Warby Parker seems like it will follow a similar strategy, committing $75 million thus far to support the eyewear company’s product development and commercialization costs. If Warby Parker meets certain milestones, Google will commit an additional $75 million and take an equity stake in the brand.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/googles-first-ai-glasses-expected-next-year/</guid><pubDate>Tue, 09 Dec 2025 16:30:37 +0000</pubDate></item><item><title>[NEW] Amazon adds delivery tracking, last-minute adds, gift ideas to Alexa+ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/amazon-adds-delivery-tracking-last-minute-adds-gift-ideas-to-alexa/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon has long had to deal with the reality that owners of its Echo smart speakers weren’t using its voice-controlled assistant, Alexa, to buy things, as it had hoped. The tech giant hasn’t given up on that dream yet: Its AI assistant, Alexa+, is getting new shopping-enabled features in the U.S. and Canada from today, including a shopping hub, tools to add items to recent orders, and personalized recommendations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has already been working on features that make Alexa+ more of a shopping assistant with abilities like automated deal tracking and automatic purchases. The former feature lets you set Alexa to alert you when items in your cart or list drop below a certain price point. If you also have automatic purchases set up, Alexa can immediately order an item when it reaches that target price.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Now, Amazon says it’s turning its Echo devices with a screen — the Echo Show 15 and 21 — into a shopping hub with an interface that it’s calling the “Shopping Essentials” experience. From this dashboard, Amazon shoppers can track deliveries in real time, see information about recent orders, get reminders about household essentials they need to reorder, and view their shopping list and saved items.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Echo Show" class="wp-image-2918385" height="382" src="https://techcrunch.com/wp-content/uploads/2024/11/amazon-echo-show-15-21.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The screen would also let shoppers tap to see more products and add items directly to their cart and then check out.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To access the new experience, Amazon users can say “Alexa, where’s my stuff?” or “Open Shopping Essentials.” Soon, a shopping widget will be available to add to the Echo device’s home screen, too. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another new feature rolling out now will allow Alexa device owners to add items to an upcoming delivery at any time until the item leaves the warehouse. This builds on a similar feature recently added to Amazon’s retail website and app, so it isn’t necessarily an Alexa+ exclusive, but the feature wasn’t live on Alexa devices so far.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alexa+ is also gaining the ability to recommend gifts. You’ll be able to describe who you’re shopping for, or the occasion, and Alexa+ will display product suggestions on the screen, organized into categories.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon says Alexa+ is now available to “tens of millions” of customers, and these new features are live for users in the U.S. and Canada. While not everyone has been happy with Alexa+, the company says the percentage of users who downgraded back to the AI-free interface remains in the “very low single digits.”&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon has long had to deal with the reality that owners of its Echo smart speakers weren’t using its voice-controlled assistant, Alexa, to buy things, as it had hoped. The tech giant hasn’t given up on that dream yet: Its AI assistant, Alexa+, is getting new shopping-enabled features in the U.S. and Canada from today, including a shopping hub, tools to add items to recent orders, and personalized recommendations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has already been working on features that make Alexa+ more of a shopping assistant with abilities like automated deal tracking and automatic purchases. The former feature lets you set Alexa to alert you when items in your cart or list drop below a certain price point. If you also have automatic purchases set up, Alexa can immediately order an item when it reaches that target price.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Now, Amazon says it’s turning its Echo devices with a screen — the Echo Show 15 and 21 — into a shopping hub with an interface that it’s calling the “Shopping Essentials” experience. From this dashboard, Amazon shoppers can track deliveries in real time, see information about recent orders, get reminders about household essentials they need to reorder, and view their shopping list and saved items.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Echo Show" class="wp-image-2918385" height="382" src="https://techcrunch.com/wp-content/uploads/2024/11/amazon-echo-show-15-21.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The screen would also let shoppers tap to see more products and add items directly to their cart and then check out.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To access the new experience, Amazon users can say “Alexa, where’s my stuff?” or “Open Shopping Essentials.” Soon, a shopping widget will be available to add to the Echo device’s home screen, too. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another new feature rolling out now will allow Alexa device owners to add items to an upcoming delivery at any time until the item leaves the warehouse. This builds on a similar feature recently added to Amazon’s retail website and app, so it isn’t necessarily an Alexa+ exclusive, but the feature wasn’t live on Alexa devices so far.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alexa+ is also gaining the ability to recommend gifts. You’ll be able to describe who you’re shopping for, or the occasion, and Alexa+ will display product suggestions on the screen, organized into categories.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon says Alexa+ is now available to “tens of millions” of customers, and these new features are live for users in the U.S. and Canada. While not everyone has been happy with Alexa+, the company says the percentage of users who downgraded back to the AI-free interface remains in the “very low single digits.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/amazon-adds-delivery-tracking-last-minute-adds-gift-ideas-to-alexa/</guid><pubDate>Tue, 09 Dec 2025 16:42:57 +0000</pubDate></item><item><title>[NEW] OpenAI, Anthropic and Block join new Linux Foundation effort to standardize the AI agent era (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/openai-anthropic-and-block-join-new-linux-foundation-effort-to-standardize-the-ai-agent-era/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-12.11.34-PM.png?resize=1200,673" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As AI moves beyond chatbots and towards systems that can take actions, the Linux Foundation is launching a new group dedicated to keeping AI agents from splintering into a mess of incompatible, locked-down products.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The group, dubbed the Agentic AI Foundation (AAIF), will act as a neutral home for open-source projects related to AI agents. Anchoring the AAIF at launch are donations from Anthropic, Block, and OpenAI.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Anthropic is donating its MCP (Model Context Protocol), a standard way to connect models and agents to tools and data; Block is contributing Goose, its open-source agent framework; and OpenAI is bringing AGENTS.md to the table, its simple instruction file developers can add to a repository to tell AI coding tools how to behave. You can think of these tools as the basic plumbing of the agent era.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other members in the AAIF include&lt;strong&gt; &lt;/strong&gt;AWS, Bloomberg, Cloudflare, and Google, signaling an industry-level push for shared guardrails so that AI agents can be trustworthy at scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In OpenAI engineer Nick Cooper’s view, protocols are essentially a shared language that lets different agents and systems work together without every developer reinventing integrations from scratch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We need multiple [protocols] to negotiate, communicate, and work together to deliver value for people, and that sort of openness and communication is why it’s not ever going to be one provider, one host, one company,” Cooper told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jim Zemlin, executive director of the Linux Foundation, put it more bluntly in conversations around the launch: the goal is to avoid a future of “closed wall” proprietary stacks, where tool connections, agent behavior, and orchestration are locked behind a handful of platforms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“By bringing these projects together under the AAIF, we are now able to coordinate interoperability, safety patterns, and best practices specifically for AI agents,” Zemlin said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Block – the fintech company behind Square and Cash App – isn’t known for AI infrastructure, but it’s making an openness play with Goose. AI Tech Lead Brad Axen frames it as proof that open alternatives can match proprietary agents at scale, with thousands of engineers using it weekly for coding, data analysis, and documentation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Open-sourcing Goose serves a dual purpose for Block.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Getting it out into the world gives us a place for other people to come help us make it better,” Axen told TechCrunch. “We have a lot of contributors from open source, and everything they do to improve it comes back to our company.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, donating Goose to the Linux Foundation gives Block access to community stress-tests while positioning it as a working example of AAIF’s vision – an agent framework designed to plug into shared building blocks like MCP and AGENTS.md.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic is making a similar move at the protocol layer, handing MCP to the Linux Foundation. The goal: make MCP the neutral infrastructure connecting AI models to tools, data, and applications without endless one-off adapters.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The main goal is to have enough adoption in the world that it’s the de facto standard,” MCP co-creator David Soria Parra told TechCrunch. “We’re all better off if we have an open integration center where you can build something once as a developer and use it across any client.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Donating MCP to AAIF signals that the protocol won’t be controlled by a single vendor.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That governance point is central to why the Linux Foundation created a new umbrella at all. The organization already hosts major AI and developer infrastructure projects – everything from PyTorch and Ray to Kubernetes – but says AAIF is specifically aimed at agent standards and orchestration, including shared safety patterns and interoperability.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AAIF’s structure is funded through a “directed fund,” meaning companies can contribute money through membership dues. But Zemlin of the Linux Foundation argues that funding doesn’t equal control: project roadmaps are set by technical steering committees, and no single member gets unilateral say over direction.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, the big question is whether AAIF becomes real infrastructure or just another industry logo alliance.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“An early indicator of success, in addition to adoption of these standards, would be the development and implementation of shared standards being used by vendor agents around the world,” Zemlin said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For OpenAI’s Cooper, success would look like an evolution of standards: “I don’t want it to be a stagnant thing. I don’t want these protocols to be part of this foundation, and that’s where they sat for two years. They should evolve and continually accept further input.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There’s also a more subtle consequence: even with open governance, one company’s implementation could become the default simply because it ships fastest or gains the most usage. Zemlin says that’s not necessarily a bad thing, though. He points to open-source history – like Kubernetes “winning” the container race – as evidence that “dominance emerges from merit and not vendor control.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For developers and enterprises, the short-term appeal is clear: less time building custom connectors, more predictable agent behavior across codebases, and simpler deployment in security-conscious environments.&amp;nbsp;&lt;br /&gt;The larger vision is more ambitious: if tools like MCP, AGENTS.md, and Goose become standard infrastructure, the agent landscape could shift from closed platforms to an open, mix-and-match software world reminiscent of the interoperable systems that built the modern web.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-12.11.34-PM.png?resize=1200,673" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As AI moves beyond chatbots and towards systems that can take actions, the Linux Foundation is launching a new group dedicated to keeping AI agents from splintering into a mess of incompatible, locked-down products.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The group, dubbed the Agentic AI Foundation (AAIF), will act as a neutral home for open-source projects related to AI agents. Anchoring the AAIF at launch are donations from Anthropic, Block, and OpenAI.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Anthropic is donating its MCP (Model Context Protocol), a standard way to connect models and agents to tools and data; Block is contributing Goose, its open-source agent framework; and OpenAI is bringing AGENTS.md to the table, its simple instruction file developers can add to a repository to tell AI coding tools how to behave. You can think of these tools as the basic plumbing of the agent era.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other members in the AAIF include&lt;strong&gt; &lt;/strong&gt;AWS, Bloomberg, Cloudflare, and Google, signaling an industry-level push for shared guardrails so that AI agents can be trustworthy at scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In OpenAI engineer Nick Cooper’s view, protocols are essentially a shared language that lets different agents and systems work together without every developer reinventing integrations from scratch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We need multiple [protocols] to negotiate, communicate, and work together to deliver value for people, and that sort of openness and communication is why it’s not ever going to be one provider, one host, one company,” Cooper told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jim Zemlin, executive director of the Linux Foundation, put it more bluntly in conversations around the launch: the goal is to avoid a future of “closed wall” proprietary stacks, where tool connections, agent behavior, and orchestration are locked behind a handful of platforms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“By bringing these projects together under the AAIF, we are now able to coordinate interoperability, safety patterns, and best practices specifically for AI agents,” Zemlin said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Block – the fintech company behind Square and Cash App – isn’t known for AI infrastructure, but it’s making an openness play with Goose. AI Tech Lead Brad Axen frames it as proof that open alternatives can match proprietary agents at scale, with thousands of engineers using it weekly for coding, data analysis, and documentation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Open-sourcing Goose serves a dual purpose for Block.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Getting it out into the world gives us a place for other people to come help us make it better,” Axen told TechCrunch. “We have a lot of contributors from open source, and everything they do to improve it comes back to our company.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, donating Goose to the Linux Foundation gives Block access to community stress-tests while positioning it as a working example of AAIF’s vision – an agent framework designed to plug into shared building blocks like MCP and AGENTS.md.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic is making a similar move at the protocol layer, handing MCP to the Linux Foundation. The goal: make MCP the neutral infrastructure connecting AI models to tools, data, and applications without endless one-off adapters.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The main goal is to have enough adoption in the world that it’s the de facto standard,” MCP co-creator David Soria Parra told TechCrunch. “We’re all better off if we have an open integration center where you can build something once as a developer and use it across any client.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Donating MCP to AAIF signals that the protocol won’t be controlled by a single vendor.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That governance point is central to why the Linux Foundation created a new umbrella at all. The organization already hosts major AI and developer infrastructure projects – everything from PyTorch and Ray to Kubernetes – but says AAIF is specifically aimed at agent standards and orchestration, including shared safety patterns and interoperability.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AAIF’s structure is funded through a “directed fund,” meaning companies can contribute money through membership dues. But Zemlin of the Linux Foundation argues that funding doesn’t equal control: project roadmaps are set by technical steering committees, and no single member gets unilateral say over direction.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, the big question is whether AAIF becomes real infrastructure or just another industry logo alliance.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“An early indicator of success, in addition to adoption of these standards, would be the development and implementation of shared standards being used by vendor agents around the world,” Zemlin said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For OpenAI’s Cooper, success would look like an evolution of standards: “I don’t want it to be a stagnant thing. I don’t want these protocols to be part of this foundation, and that’s where they sat for two years. They should evolve and continually accept further input.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There’s also a more subtle consequence: even with open governance, one company’s implementation could become the default simply because it ships fastest or gains the most usage. Zemlin says that’s not necessarily a bad thing, though. He points to open-source history – like Kubernetes “winning” the container race – as evidence that “dominance emerges from merit and not vendor control.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For developers and enterprises, the short-term appeal is clear: less time building custom connectors, more predictable agent behavior across codebases, and simpler deployment in security-conscious environments.&amp;nbsp;&lt;br /&gt;The larger vision is more ambitious: if tools like MCP, AGENTS.md, and Goose become standard infrastructure, the agent landscape could shift from closed platforms to an open, mix-and-match software world reminiscent of the interoperable systems that built the modern web.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/openai-anthropic-and-block-join-new-linux-foundation-effort-to-standardize-the-ai-agent-era/</guid><pubDate>Tue, 09 Dec 2025 17:28:36 +0000</pubDate></item></channel></rss>