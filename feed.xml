<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 11 Nov 2025 12:45:55 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>Understanding the nuances of human-like intelligence (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/understanding-nuances-human-intelligence-phillip-isola-1111</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/MIT-Phillip-Isola-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;What can we learn about human intelligence by studying how machines ‚Äúthink?‚Äù Can we better understand ourselves if we better understand the artificial intelligence systems that are becoming a more significant part of our everyday lives?&lt;/p&gt;&lt;p&gt;These questions may be deeply philosophical, but for Phillip Isola, finding the answers is as much about computation as it is about cogitation.&lt;/p&gt;&lt;p&gt;Isola, the newly tenured associate professor in the Department of Electrical Engineering and Computer Science (EECS), studies the fundamental mechanisms involved in human-like intelligence from a computational perspective.&lt;/p&gt;&lt;p&gt;While understanding intelligence is the overarching goal, his work focuses mainly on computer vision and machine learning. Isola is particularly interested in exploring how intelligence emerges in AI models, how these models learn to represent the world around them, and what their ‚Äúbrains‚Äù share with the brains of their human creators.&lt;/p&gt;&lt;p&gt;‚ÄúI see all the different kinds of intelligence as having a lot of commonalities, and I‚Äôd like to understand those commonalities. What is it that all animals, humans, and AIs have in common?‚Äù says Isola, who is also a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL).&lt;/p&gt;&lt;p&gt;To Isola, a better scientific understanding of the intelligence that AI agents possess will help the world integrate them safely and effectively into society, maximizing their potential to benefit humanity.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Asking questions&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Isola began pondering scientific questions at a young age.&lt;/p&gt;&lt;p&gt;While growing up in San Francisco, he and his father frequently went hiking along the northern California coastline or camping around Point Reyes and in the hills of Marin County.&lt;/p&gt;&lt;p&gt;He was fascinated by geological processes and often wondered what made the natural world work. In school, Isola was driven by an insatiable curiosity, and while he gravitated toward technical subjects like math and science, there was no limit to what he wanted to learn.&lt;/p&gt;&lt;p&gt;Not entirely sure what to study as an undergraduate at Yale University, Isola dabbled until he came upon cognitive sciences.&lt;/p&gt;&lt;p&gt;‚ÄúMy earlier interest had been with nature ‚Äî how the world works. But then I realized that the brain was even more interesting, and more complex than even the formation of the planets. Now, I wanted to know what makes us tick,‚Äù he says.&lt;/p&gt;&lt;p&gt;As a first-year student, he started working in the lab of his cognitive sciences professor and soon-to-be mentor, Brian Scholl, a member of the Yale Department of Psychology. He remained in that lab throughout his time as an undergraduate.&lt;/p&gt;&lt;p&gt;After spending a gap year working with some childhood friends at an indie video game company, Isola was ready to dive back into the complex world of the human brain. He enrolled in the graduate program in brain and cognitive sciences at MIT.&lt;/p&gt;&lt;p&gt;‚ÄúGrad school was where I felt like I finally found my place. I had a lot of great experiences at Yale and in other phases of my life, but when I got to MIT, I realized this was the work I really loved and these are the people who think similarly to me,‚Äù he says.&lt;/p&gt;&lt;p&gt;Isola credits his PhD advisor, Ted Adelson, the John and Dorothy Wilson Professor of Vision Science, as a major influence on his future path. He was inspired by Adelson‚Äôs focus on understanding fundamental principles, rather than only chasing new engineering benchmarks, which are formalized tests used to measure the performance of a system.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A computational perspective&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;At MIT, Isola‚Äôs research drifted toward computer science and artificial intelligence.&lt;/p&gt;&lt;p&gt;‚ÄúI still loved all those questions from cognitive sciences, but I felt I could make more progress on some of those questions if I came at it from a purely computational perspective,‚Äù he says.&lt;/p&gt;&lt;p&gt;His thesis was focused on perceptual grouping, which involves the mechanisms people and machines use to organize discrete parts of an image as a single, coherent object.&lt;/p&gt;&lt;p&gt;If machines can learn perceptual groupings on their own, that could enable AI systems to recognize objects without human intervention. This type of self-supervised learning has applications in areas such autonomous vehicles, medical imaging, robotics, and automatic language translation.&lt;/p&gt;&lt;p&gt;After graduating from MIT, Isola completed a postdoc at the University of California at Berkeley so he could broaden his perspectives by working in a lab solely focused on computer science.&lt;/p&gt;&lt;p&gt;‚ÄúThat experience helped my work become a lot more impactful because I learned to balance understanding fundamental, abstract principles of intelligence with the pursuit of some more concrete benchmarks,‚Äù Isola recalls.&lt;/p&gt;&lt;p&gt;At Berkeley, he developed image-to-image translation frameworks, an early form of generative AI model that could turn a sketch into a photographic image, for instance, or turn a black-and-white photo into a color one.&lt;/p&gt;&lt;p&gt;He entered the academic job market and accepted a faculty position at MIT, but Isola deferred for a year to work at a then-small startup called OpenAI.&lt;/p&gt;&lt;p&gt;‚ÄúIt was a nonprofit, and I liked the idealistic mission at that time. They were really good at reinforcement learning, and I thought that seemed like an important topic to learn more about,‚Äù he says.&lt;/p&gt;&lt;p&gt;He enjoyed working in a lab with so much scientific freedom, but after a year Isola was ready to return to MIT and start his own research group.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Studying human-like intelligence&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Running a research lab instantly appealed to him.&lt;/p&gt;&lt;p&gt;‚ÄúI really love the early stage of an idea. I feel like I am a sort of startup incubator where I am constantly able to do new things and learn new things,‚Äù he says.&lt;/p&gt;&lt;p&gt;Building on his interest in cognitive sciences and desire to understand the human brain, his group studies the fundamental computations involved in the human-like intelligence that emerges in machines.&lt;/p&gt;&lt;p&gt;One primary focus is representation learning, or the ability of humans and machines to represent and perceive the sensory world around them.&lt;/p&gt;&lt;p&gt;In recent work, he and his collaborators observed that the many varied types of machine-learning models, from LLMs to computer vision models to audio models, seem to represent the world in similar ways.&lt;/p&gt;&lt;p&gt;These models are designed to do vastly different tasks, but there are many similarities in their architectures. And as they get bigger and are trained on more data, their internal structures become more alike.&lt;/p&gt;&lt;p&gt;This led Isola and his team to introduce the Platonic Representation Hypothesis (drawing its name from the Greek philosopher Plato) which says that the representations all these models learn are converging toward a shared, underlying representation of reality.&lt;/p&gt;&lt;p&gt;‚ÄúLanguage, images, sound ‚Äî all of these are different shadows on the wall from which you can infer that there is some kind of underlying physical process ‚Äî some kind of causal reality ‚Äî out there. If you train models on all these different types of data, they should converge on that world model in the end,‚Äù Isola says.&lt;/p&gt;&lt;p&gt;A related area his team studies is self-supervised learning. This involves the ways in which AI models learn to group related pixels in an image or words in a sentence without having labeled examples to learn from.&lt;/p&gt;&lt;p&gt;Because data are expensive and labels are limited, using only labeled data to train models could hold back the capabilities of AI systems. With self-supervised learning, the goal is to develop models that can come up with an accurate internal representation of the world on their own.&lt;/p&gt;&lt;p&gt;‚ÄúIf you can come up with a good representation of the world, that should make subsequent problem solving easier,‚Äù he explains.&lt;/p&gt;&lt;p&gt;The focus of Isola‚Äôs research is more about finding something new and surprising than about building complex systems that can outdo the latest machine-learning benchmarks.&lt;/p&gt;&lt;p&gt;While this approach has yielded much success in uncovering innovative techniques and architectures, it means the work sometimes lacks a concrete end goal, which can lead to challenges.&lt;/p&gt;&lt;p&gt;For instance, keeping a team aligned and the funding flowing can be difficult when the lab is focused on searching for unexpected results, he says.&lt;/p&gt;&lt;p&gt;‚ÄúIn a sense, we are always working in the dark. It is high-risk and high-reward work. Every once in while, we find some kernel of truth that is new and surprising,‚Äù he says.&lt;/p&gt;&lt;p&gt;In addition to pursuing knowledge, Isola is passionate about imparting knowledge to the next generation of scientists and engineers. Among his favorite courses to teach is 6.7960 (Deep Learning), which he and several other MIT faculty members launched four years ago.&lt;/p&gt;&lt;p&gt;The class has seen exponential growth, from 30 students in its initial offering to more than 700 this fall.&lt;/p&gt;&lt;p&gt;And while the popularity of AI means there is no shortage of interested students, the speed at which the field moves can make it difficult to separate the hype from truly significant advances.&lt;/p&gt;&lt;p&gt;‚ÄúI tell the students they have to take everything we say in the class with a grain of salt. Maybe in a few years we‚Äôll tell them something different. We are really on the edge of knowledge with this course,‚Äù he says.&lt;/p&gt;&lt;p&gt;But Isola also emphasizes to students that, for all the hype surrounding the latest AI models, intelligent machines are far simpler than most people suspect.&lt;/p&gt;&lt;p&gt;‚ÄúHuman ingenuity, creativity, and emotions ‚Äî many people believe these can never be modeled. That might turn out to be true, but I think intelligence is fairly simple once we understand it,‚Äù he says.&lt;/p&gt;&lt;p&gt;Even though his current work focuses on deep-learning models, Isola is still fascinated by the complexity of the human brain and continues to collaborate with researchers who study cognitive sciences.&lt;/p&gt;&lt;p&gt;All the while, he has remained captivated by the beauty of the natural world that inspired his first interest in science.&lt;/p&gt;&lt;p&gt;Although he has less time for hobbies these days, Isola enjoys hiking and backpacking in the mountains or on Cape Cod, skiing and kayaking, or finding scenic places to spend time when he travels for scientific conferences.&lt;/p&gt;&lt;p&gt;And while he looks forward to exploring new questions in his lab at MIT, Isola can‚Äôt help but contemplate how the role of intelligent machines might change the course of his work.&lt;/p&gt;&lt;p&gt;He believes that artificial general intelligence (AGI), or the point where machines can learn and apply their knowledge as well as humans can, is not that far off.&lt;/p&gt;&lt;p&gt;‚ÄúI don‚Äôt think AIs will just do everything for us and we‚Äôll go and enjoy life at the beach. I think there is going to be this coexistence between smart machines and humans who still have a lot of agency and control. Now, I‚Äôm thinking about the interesting questions and applications once that happens. How can I help the world in this post-AGI future? I don‚Äôt have any answers yet, but it‚Äôs on my mind,‚Äù he says.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/MIT-Phillip-Isola-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;What can we learn about human intelligence by studying how machines ‚Äúthink?‚Äù Can we better understand ourselves if we better understand the artificial intelligence systems that are becoming a more significant part of our everyday lives?&lt;/p&gt;&lt;p&gt;These questions may be deeply philosophical, but for Phillip Isola, finding the answers is as much about computation as it is about cogitation.&lt;/p&gt;&lt;p&gt;Isola, the newly tenured associate professor in the Department of Electrical Engineering and Computer Science (EECS), studies the fundamental mechanisms involved in human-like intelligence from a computational perspective.&lt;/p&gt;&lt;p&gt;While understanding intelligence is the overarching goal, his work focuses mainly on computer vision and machine learning. Isola is particularly interested in exploring how intelligence emerges in AI models, how these models learn to represent the world around them, and what their ‚Äúbrains‚Äù share with the brains of their human creators.&lt;/p&gt;&lt;p&gt;‚ÄúI see all the different kinds of intelligence as having a lot of commonalities, and I‚Äôd like to understand those commonalities. What is it that all animals, humans, and AIs have in common?‚Äù says Isola, who is also a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL).&lt;/p&gt;&lt;p&gt;To Isola, a better scientific understanding of the intelligence that AI agents possess will help the world integrate them safely and effectively into society, maximizing their potential to benefit humanity.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Asking questions&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Isola began pondering scientific questions at a young age.&lt;/p&gt;&lt;p&gt;While growing up in San Francisco, he and his father frequently went hiking along the northern California coastline or camping around Point Reyes and in the hills of Marin County.&lt;/p&gt;&lt;p&gt;He was fascinated by geological processes and often wondered what made the natural world work. In school, Isola was driven by an insatiable curiosity, and while he gravitated toward technical subjects like math and science, there was no limit to what he wanted to learn.&lt;/p&gt;&lt;p&gt;Not entirely sure what to study as an undergraduate at Yale University, Isola dabbled until he came upon cognitive sciences.&lt;/p&gt;&lt;p&gt;‚ÄúMy earlier interest had been with nature ‚Äî how the world works. But then I realized that the brain was even more interesting, and more complex than even the formation of the planets. Now, I wanted to know what makes us tick,‚Äù he says.&lt;/p&gt;&lt;p&gt;As a first-year student, he started working in the lab of his cognitive sciences professor and soon-to-be mentor, Brian Scholl, a member of the Yale Department of Psychology. He remained in that lab throughout his time as an undergraduate.&lt;/p&gt;&lt;p&gt;After spending a gap year working with some childhood friends at an indie video game company, Isola was ready to dive back into the complex world of the human brain. He enrolled in the graduate program in brain and cognitive sciences at MIT.&lt;/p&gt;&lt;p&gt;‚ÄúGrad school was where I felt like I finally found my place. I had a lot of great experiences at Yale and in other phases of my life, but when I got to MIT, I realized this was the work I really loved and these are the people who think similarly to me,‚Äù he says.&lt;/p&gt;&lt;p&gt;Isola credits his PhD advisor, Ted Adelson, the John and Dorothy Wilson Professor of Vision Science, as a major influence on his future path. He was inspired by Adelson‚Äôs focus on understanding fundamental principles, rather than only chasing new engineering benchmarks, which are formalized tests used to measure the performance of a system.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A computational perspective&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;At MIT, Isola‚Äôs research drifted toward computer science and artificial intelligence.&lt;/p&gt;&lt;p&gt;‚ÄúI still loved all those questions from cognitive sciences, but I felt I could make more progress on some of those questions if I came at it from a purely computational perspective,‚Äù he says.&lt;/p&gt;&lt;p&gt;His thesis was focused on perceptual grouping, which involves the mechanisms people and machines use to organize discrete parts of an image as a single, coherent object.&lt;/p&gt;&lt;p&gt;If machines can learn perceptual groupings on their own, that could enable AI systems to recognize objects without human intervention. This type of self-supervised learning has applications in areas such autonomous vehicles, medical imaging, robotics, and automatic language translation.&lt;/p&gt;&lt;p&gt;After graduating from MIT, Isola completed a postdoc at the University of California at Berkeley so he could broaden his perspectives by working in a lab solely focused on computer science.&lt;/p&gt;&lt;p&gt;‚ÄúThat experience helped my work become a lot more impactful because I learned to balance understanding fundamental, abstract principles of intelligence with the pursuit of some more concrete benchmarks,‚Äù Isola recalls.&lt;/p&gt;&lt;p&gt;At Berkeley, he developed image-to-image translation frameworks, an early form of generative AI model that could turn a sketch into a photographic image, for instance, or turn a black-and-white photo into a color one.&lt;/p&gt;&lt;p&gt;He entered the academic job market and accepted a faculty position at MIT, but Isola deferred for a year to work at a then-small startup called OpenAI.&lt;/p&gt;&lt;p&gt;‚ÄúIt was a nonprofit, and I liked the idealistic mission at that time. They were really good at reinforcement learning, and I thought that seemed like an important topic to learn more about,‚Äù he says.&lt;/p&gt;&lt;p&gt;He enjoyed working in a lab with so much scientific freedom, but after a year Isola was ready to return to MIT and start his own research group.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Studying human-like intelligence&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Running a research lab instantly appealed to him.&lt;/p&gt;&lt;p&gt;‚ÄúI really love the early stage of an idea. I feel like I am a sort of startup incubator where I am constantly able to do new things and learn new things,‚Äù he says.&lt;/p&gt;&lt;p&gt;Building on his interest in cognitive sciences and desire to understand the human brain, his group studies the fundamental computations involved in the human-like intelligence that emerges in machines.&lt;/p&gt;&lt;p&gt;One primary focus is representation learning, or the ability of humans and machines to represent and perceive the sensory world around them.&lt;/p&gt;&lt;p&gt;In recent work, he and his collaborators observed that the many varied types of machine-learning models, from LLMs to computer vision models to audio models, seem to represent the world in similar ways.&lt;/p&gt;&lt;p&gt;These models are designed to do vastly different tasks, but there are many similarities in their architectures. And as they get bigger and are trained on more data, their internal structures become more alike.&lt;/p&gt;&lt;p&gt;This led Isola and his team to introduce the Platonic Representation Hypothesis (drawing its name from the Greek philosopher Plato) which says that the representations all these models learn are converging toward a shared, underlying representation of reality.&lt;/p&gt;&lt;p&gt;‚ÄúLanguage, images, sound ‚Äî all of these are different shadows on the wall from which you can infer that there is some kind of underlying physical process ‚Äî some kind of causal reality ‚Äî out there. If you train models on all these different types of data, they should converge on that world model in the end,‚Äù Isola says.&lt;/p&gt;&lt;p&gt;A related area his team studies is self-supervised learning. This involves the ways in which AI models learn to group related pixels in an image or words in a sentence without having labeled examples to learn from.&lt;/p&gt;&lt;p&gt;Because data are expensive and labels are limited, using only labeled data to train models could hold back the capabilities of AI systems. With self-supervised learning, the goal is to develop models that can come up with an accurate internal representation of the world on their own.&lt;/p&gt;&lt;p&gt;‚ÄúIf you can come up with a good representation of the world, that should make subsequent problem solving easier,‚Äù he explains.&lt;/p&gt;&lt;p&gt;The focus of Isola‚Äôs research is more about finding something new and surprising than about building complex systems that can outdo the latest machine-learning benchmarks.&lt;/p&gt;&lt;p&gt;While this approach has yielded much success in uncovering innovative techniques and architectures, it means the work sometimes lacks a concrete end goal, which can lead to challenges.&lt;/p&gt;&lt;p&gt;For instance, keeping a team aligned and the funding flowing can be difficult when the lab is focused on searching for unexpected results, he says.&lt;/p&gt;&lt;p&gt;‚ÄúIn a sense, we are always working in the dark. It is high-risk and high-reward work. Every once in while, we find some kernel of truth that is new and surprising,‚Äù he says.&lt;/p&gt;&lt;p&gt;In addition to pursuing knowledge, Isola is passionate about imparting knowledge to the next generation of scientists and engineers. Among his favorite courses to teach is 6.7960 (Deep Learning), which he and several other MIT faculty members launched four years ago.&lt;/p&gt;&lt;p&gt;The class has seen exponential growth, from 30 students in its initial offering to more than 700 this fall.&lt;/p&gt;&lt;p&gt;And while the popularity of AI means there is no shortage of interested students, the speed at which the field moves can make it difficult to separate the hype from truly significant advances.&lt;/p&gt;&lt;p&gt;‚ÄúI tell the students they have to take everything we say in the class with a grain of salt. Maybe in a few years we‚Äôll tell them something different. We are really on the edge of knowledge with this course,‚Äù he says.&lt;/p&gt;&lt;p&gt;But Isola also emphasizes to students that, for all the hype surrounding the latest AI models, intelligent machines are far simpler than most people suspect.&lt;/p&gt;&lt;p&gt;‚ÄúHuman ingenuity, creativity, and emotions ‚Äî many people believe these can never be modeled. That might turn out to be true, but I think intelligence is fairly simple once we understand it,‚Äù he says.&lt;/p&gt;&lt;p&gt;Even though his current work focuses on deep-learning models, Isola is still fascinated by the complexity of the human brain and continues to collaborate with researchers who study cognitive sciences.&lt;/p&gt;&lt;p&gt;All the while, he has remained captivated by the beauty of the natural world that inspired his first interest in science.&lt;/p&gt;&lt;p&gt;Although he has less time for hobbies these days, Isola enjoys hiking and backpacking in the mountains or on Cape Cod, skiing and kayaking, or finding scenic places to spend time when he travels for scientific conferences.&lt;/p&gt;&lt;p&gt;And while he looks forward to exploring new questions in his lab at MIT, Isola can‚Äôt help but contemplate how the role of intelligent machines might change the course of his work.&lt;/p&gt;&lt;p&gt;He believes that artificial general intelligence (AGI), or the point where machines can learn and apply their knowledge as well as humans can, is not that far off.&lt;/p&gt;&lt;p&gt;‚ÄúI don‚Äôt think AIs will just do everything for us and we‚Äôll go and enjoy life at the beach. I think there is going to be this coexistence between smart machines and humans who still have a lot of agency and control. Now, I‚Äôm thinking about the interesting questions and applications once that happens. How can I help the world in this post-AGI future? I don‚Äôt have any answers yet, but it‚Äôs on my mind,‚Äù he says.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/understanding-nuances-human-intelligence-phillip-isola-1111</guid><pubDate>Tue, 11 Nov 2025 05:00:00 +0000</pubDate></item><item><title>[NEW] Chinese AI startup Moonshot outperforms GPT-5 and Claude Sonnet 4.5: What you need to know (AI News)</title><link>https://www.artificialintelligence-news.com/news/moonshot-ai-gpt-5-claude-comparison-china-breakthrough/</link><description>&lt;p&gt;A Chinese AI startup, Moonshot, has disrupted expectations in artificial intelligence development after its Kimi K2 Thinking model surpassed OpenAI‚Äôs GPT-5 and Anthropic‚Äôs Claude Sonnet 4.5 across multiple performance benchmarks, sparking renewed debate about whether America‚Äôs AI dominance is being challenged by cost-efficient Chinese innovation.&lt;/p&gt;&lt;p&gt;Beijing-based Moonshot AI, valued at US$3.3 billion and backed by tech giants Alibaba Group Holding and Tencent Holdings, released the open-source Kimi K2 Thinking model on November 6, achieving what industry observers are calling another ‚ÄúDeepSeek moment‚Äù ‚Äì a reference to the Hangzhou-based startup‚Äôs earlier disruption of AI cost assumptions.&lt;/p&gt;&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;&lt;blockquote class="cmplz-placeholder-element twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;üöÄ Hello, Kimi K2 Thinking!&lt;br /&gt;The Open-Source Thinking Agent Model is here.&lt;/p&gt;&lt;p&gt;üîπ SOTA on HLE (44.9%) and BrowseComp (60.2%)&lt;br /&gt;üîπ Executes up to 200 ‚Äì 300 sequential tool calls without human interference&lt;br /&gt;üîπ Excels in reasoning, agentic search, and coding&lt;br /&gt;üîπ 256K context window&lt;/p&gt;&lt;p&gt;Built‚Ä¶ pic.twitter.com/lZCNBIgbV2&lt;/p&gt;‚Äî Kimi.ai (@Kimi_Moonshot) November 6, 2025&lt;/blockquote&gt;&lt;/div&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-performance-metrics-challenge-us-models"&gt;Performance metrics challenge US models&lt;/h3&gt;&lt;p&gt;According to the company‚Äôs GitHub blog&amp;nbsp;post, Kimi K2 Thinking scored 44.9% on Humanity‚Äôs Last Exam, a large language model benchmark consisting of 2,500 questions across a broad range of subjects, exceeding GPT-5‚Äôs 41.7%.&lt;/p&gt;&lt;p&gt;The model also achieved 60.2% on the BrowseComp benchmark, which evaluates web browsing proficiency and information-seeking persistence of large language model agents, and scored 56.3% to lead in the Seal-0 benchmark designed to challenge search-augmented models on real-world research queries.&lt;/p&gt;&lt;p&gt;&lt;em&gt;VentureBeat&lt;/em&gt;&amp;nbsp;reported&amp;nbsp;that the fully open-weight release meeting or exceeding GPT-5‚Äôs scores marks a turning point where the gap between closed frontier systems and publicly available models has effectively collapsed for high-end reasoning and coding.&lt;/p&gt;&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;&lt;blockquote class="cmplz-placeholder-element twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Kimi K2 Thinking is the new leading open weights model: it demonstrates particular strength in agentic contexts but is very verbose, generating the most tokens of any model in completing our Intelligence Index evals@Kimi_Moonshot's Kimi K2 Thinking achieves a 67 in the‚Ä¶ pic.twitter.com/m6SvpW7iif&lt;/p&gt;‚Äî Artificial Analysis (@ArtificialAnlys) November 7, 2025&lt;/blockquote&gt;&lt;/div&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-cost-efficiency-raises-nbsp-questions"&gt;Cost efficiency raises&amp;nbsp;questions&lt;/h3&gt;&lt;p&gt;The popularity of the model grew after CNBC reported its training cost was merely US$4.6 million, though Moonshot AI did not comment on the cost.&amp;nbsp;According to calculations by the&amp;nbsp;&lt;em&gt;South China Morning Post&lt;/em&gt;,&amp;nbsp;the cost of&amp;nbsp;Kimi K2 Thinking‚Äôs application programming interface was&amp;nbsp;six&amp;nbsp;to 10 times cheaper than&amp;nbsp;that&amp;nbsp;of OpenAI and Anthropic‚Äôs models.&lt;/p&gt;&lt;p&gt;The model uses a Mixture-of-Experts architecture with&amp;nbsp;one&amp;nbsp;trillion total parameters, of which 32 billion are activated per inference, and was trained&amp;nbsp;using&amp;nbsp;INT4 quantisation to achieve roughly&amp;nbsp;two times&amp;nbsp;generation speed&amp;nbsp;improvement&amp;nbsp;while maintaining state-of-the-art performance.&lt;/p&gt;&lt;p&gt;Thomas Wolf, co-founder of Hugging Face,&amp;nbsp;commented&amp;nbsp;on X that Kimi K2 Thinking was another case of an open-source model passing a closed-source model, asking, ‚ÄúIs this another DeepSeek moment? Should we expect [one] every couple of months now?‚Äù&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-technical-capabilities-and-limitations"&gt;Technical capabilities and limitations&lt;/h3&gt;&lt;p&gt;Moonshot AI researchers&amp;nbsp;said&amp;nbsp;Kimi K2 Thinking set ‚Äúnew records across benchmarks that assess reasoning, coding and agent capabilities‚Äù. The model can execute up to 200-300 sequential tool calls without human interference, reasoning coherently across hundreds of steps to solve complex problems.&lt;/p&gt;&lt;p&gt;Independent testing by consultancy Artificial Analysis placed Kimi K2 on top of its Tau-2 Bench Telecom agentic benchmark with 93% accuracy, which was&amp;nbsp;described&amp;nbsp;as the highest score it has independently measured.&lt;/p&gt;&lt;p&gt;However, Nathan Lambert, a researcher at the Allen Institute for AI, suggested there‚Äôs still a time lag of approximately four to six months in raw performance between the best closed and open models, though he&amp;nbsp;acknowledged&amp;nbsp;that Chinese labs are closing in and performing very strongly on key benchmarks.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-market-implications-and-competitive-pressure"&gt;Market implications and competitive pressure&lt;/h3&gt;&lt;p&gt;Zhang Ruiwang, a Beijing-based information technology system architect, said the trend was for Chinese companies to keep costs down, explaining, ‚ÄúThe overall performance of Chinese models still lags behind top US models, so they have to compete in the realms of cost-effectiveness to have a way out‚Äù.&lt;/p&gt;&lt;p&gt;Zhang Yi, chief analyst at consultancy iiMedia, said the training costs of Chinese AI models were seeing a ‚Äúcliff-like drop‚Äù driven by innovation in model architecture and training technique, and input of quality training data, marking a shift away from the heaping of computing resources in the early days.&lt;/p&gt;&lt;p&gt;The model was released under a Modified MIT License that grants full commercial and derivative rights, with one restriction: deployers serving over 100 million monthly active users or&amp;nbsp;generating&amp;nbsp;over US$20 million per month in revenue must prominently display ‚ÄúKimi K2‚Äù on the product‚Äôs user interface.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-industry-response-and-future-outlook"&gt;Industry response and future outlook&lt;/h3&gt;&lt;p&gt;Deedy Das, a partner at early-stage venture capital firm Menlo Ventures, wrote in a post on X that ‚ÄúToday is a turning point in AI. A Chinese open-source model is #1. Seminal moment in AI‚Äù.&lt;/p&gt;&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;&lt;blockquote class="cmplz-placeholder-element twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;üö® Today is a turning point in AI. A Chinese open source model is #1.&lt;/p&gt;&lt;p&gt;Kimi K2 Thinking scored 51% in Humanity's Last Exam, higher than GPT-5 and every other model. $0.6/M in, $2.5/M output.&lt;/p&gt;&lt;p&gt;The best at writing, and does 15tps on two Mac M3 Ultras!&lt;/p&gt;&lt;p&gt;Seminal moment in AI.&lt;/p&gt;&lt;p&gt;Try it‚Ä¶ pic.twitter.com/fmxlxpCGbE&lt;/p&gt;‚Äî Deedy (@deedydas) November 7, 2025&lt;/blockquote&gt;&lt;/div&gt;&lt;/figure&gt;&lt;p&gt;Nathan Lambert wrote in a Substack article that the success of Chinese open-source AI developers, including Moonshot AI and DeepSeek, showed how they ‚Äúmade the closed labs sweat,‚Äù adding ‚ÄúThere‚Äôs serious pricing pressure and expectations that [the US developers] need to manage‚Äù.&lt;/p&gt;&lt;p&gt;The release positions Moonshot AI alongside other Chinese AI companies like DeepSeek, Qwen, and Baichuan that are increasingly challenging the narrative of American AI supremacy through cost-efficient innovation and open-source development strategies.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Whether this represents a sustainable competitive advantage or a temporary convergence in capabilities remains to be seen as both US and Chinese companies continue advancing their models.&lt;/p&gt;&lt;p&gt;the public nature of the statements, and the market‚Äôs reaction, suggest substantive discussions may soon be underway.&lt;/p&gt;&lt;p&gt;The AI chip landscape is entering a period of flux. Organisations should maintain flexibility in their infrastructure strategy and monitor how partnerships like Tesla-Intel might reshape the competitive dynamics of AI hardware manufacturing.&lt;/p&gt;&lt;p&gt;The decisions made today about chip manufacturing partnerships could determine which organisations have access to cost-effective, high-performance AI infrastructure in the coming years.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Photo by Moonshot AI)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also:&lt;/strong&gt; DeepSeek disruption: Chinese AI innovation narrows global technology divide&lt;/p&gt;&lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. This comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;A Chinese AI startup, Moonshot, has disrupted expectations in artificial intelligence development after its Kimi K2 Thinking model surpassed OpenAI‚Äôs GPT-5 and Anthropic‚Äôs Claude Sonnet 4.5 across multiple performance benchmarks, sparking renewed debate about whether America‚Äôs AI dominance is being challenged by cost-efficient Chinese innovation.&lt;/p&gt;&lt;p&gt;Beijing-based Moonshot AI, valued at US$3.3 billion and backed by tech giants Alibaba Group Holding and Tencent Holdings, released the open-source Kimi K2 Thinking model on November 6, achieving what industry observers are calling another ‚ÄúDeepSeek moment‚Äù ‚Äì a reference to the Hangzhou-based startup‚Äôs earlier disruption of AI cost assumptions.&lt;/p&gt;&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;&lt;blockquote class="cmplz-placeholder-element twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;üöÄ Hello, Kimi K2 Thinking!&lt;br /&gt;The Open-Source Thinking Agent Model is here.&lt;/p&gt;&lt;p&gt;üîπ SOTA on HLE (44.9%) and BrowseComp (60.2%)&lt;br /&gt;üîπ Executes up to 200 ‚Äì 300 sequential tool calls without human interference&lt;br /&gt;üîπ Excels in reasoning, agentic search, and coding&lt;br /&gt;üîπ 256K context window&lt;/p&gt;&lt;p&gt;Built‚Ä¶ pic.twitter.com/lZCNBIgbV2&lt;/p&gt;‚Äî Kimi.ai (@Kimi_Moonshot) November 6, 2025&lt;/blockquote&gt;&lt;/div&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-performance-metrics-challenge-us-models"&gt;Performance metrics challenge US models&lt;/h3&gt;&lt;p&gt;According to the company‚Äôs GitHub blog&amp;nbsp;post, Kimi K2 Thinking scored 44.9% on Humanity‚Äôs Last Exam, a large language model benchmark consisting of 2,500 questions across a broad range of subjects, exceeding GPT-5‚Äôs 41.7%.&lt;/p&gt;&lt;p&gt;The model also achieved 60.2% on the BrowseComp benchmark, which evaluates web browsing proficiency and information-seeking persistence of large language model agents, and scored 56.3% to lead in the Seal-0 benchmark designed to challenge search-augmented models on real-world research queries.&lt;/p&gt;&lt;p&gt;&lt;em&gt;VentureBeat&lt;/em&gt;&amp;nbsp;reported&amp;nbsp;that the fully open-weight release meeting or exceeding GPT-5‚Äôs scores marks a turning point where the gap between closed frontier systems and publicly available models has effectively collapsed for high-end reasoning and coding.&lt;/p&gt;&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;&lt;blockquote class="cmplz-placeholder-element twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Kimi K2 Thinking is the new leading open weights model: it demonstrates particular strength in agentic contexts but is very verbose, generating the most tokens of any model in completing our Intelligence Index evals@Kimi_Moonshot's Kimi K2 Thinking achieves a 67 in the‚Ä¶ pic.twitter.com/m6SvpW7iif&lt;/p&gt;‚Äî Artificial Analysis (@ArtificialAnlys) November 7, 2025&lt;/blockquote&gt;&lt;/div&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-cost-efficiency-raises-nbsp-questions"&gt;Cost efficiency raises&amp;nbsp;questions&lt;/h3&gt;&lt;p&gt;The popularity of the model grew after CNBC reported its training cost was merely US$4.6 million, though Moonshot AI did not comment on the cost.&amp;nbsp;According to calculations by the&amp;nbsp;&lt;em&gt;South China Morning Post&lt;/em&gt;,&amp;nbsp;the cost of&amp;nbsp;Kimi K2 Thinking‚Äôs application programming interface was&amp;nbsp;six&amp;nbsp;to 10 times cheaper than&amp;nbsp;that&amp;nbsp;of OpenAI and Anthropic‚Äôs models.&lt;/p&gt;&lt;p&gt;The model uses a Mixture-of-Experts architecture with&amp;nbsp;one&amp;nbsp;trillion total parameters, of which 32 billion are activated per inference, and was trained&amp;nbsp;using&amp;nbsp;INT4 quantisation to achieve roughly&amp;nbsp;two times&amp;nbsp;generation speed&amp;nbsp;improvement&amp;nbsp;while maintaining state-of-the-art performance.&lt;/p&gt;&lt;p&gt;Thomas Wolf, co-founder of Hugging Face,&amp;nbsp;commented&amp;nbsp;on X that Kimi K2 Thinking was another case of an open-source model passing a closed-source model, asking, ‚ÄúIs this another DeepSeek moment? Should we expect [one] every couple of months now?‚Äù&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-technical-capabilities-and-limitations"&gt;Technical capabilities and limitations&lt;/h3&gt;&lt;p&gt;Moonshot AI researchers&amp;nbsp;said&amp;nbsp;Kimi K2 Thinking set ‚Äúnew records across benchmarks that assess reasoning, coding and agent capabilities‚Äù. The model can execute up to 200-300 sequential tool calls without human interference, reasoning coherently across hundreds of steps to solve complex problems.&lt;/p&gt;&lt;p&gt;Independent testing by consultancy Artificial Analysis placed Kimi K2 on top of its Tau-2 Bench Telecom agentic benchmark with 93% accuracy, which was&amp;nbsp;described&amp;nbsp;as the highest score it has independently measured.&lt;/p&gt;&lt;p&gt;However, Nathan Lambert, a researcher at the Allen Institute for AI, suggested there‚Äôs still a time lag of approximately four to six months in raw performance between the best closed and open models, though he&amp;nbsp;acknowledged&amp;nbsp;that Chinese labs are closing in and performing very strongly on key benchmarks.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-market-implications-and-competitive-pressure"&gt;Market implications and competitive pressure&lt;/h3&gt;&lt;p&gt;Zhang Ruiwang, a Beijing-based information technology system architect, said the trend was for Chinese companies to keep costs down, explaining, ‚ÄúThe overall performance of Chinese models still lags behind top US models, so they have to compete in the realms of cost-effectiveness to have a way out‚Äù.&lt;/p&gt;&lt;p&gt;Zhang Yi, chief analyst at consultancy iiMedia, said the training costs of Chinese AI models were seeing a ‚Äúcliff-like drop‚Äù driven by innovation in model architecture and training technique, and input of quality training data, marking a shift away from the heaping of computing resources in the early days.&lt;/p&gt;&lt;p&gt;The model was released under a Modified MIT License that grants full commercial and derivative rights, with one restriction: deployers serving over 100 million monthly active users or&amp;nbsp;generating&amp;nbsp;over US$20 million per month in revenue must prominently display ‚ÄúKimi K2‚Äù on the product‚Äôs user interface.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-industry-response-and-future-outlook"&gt;Industry response and future outlook&lt;/h3&gt;&lt;p&gt;Deedy Das, a partner at early-stage venture capital firm Menlo Ventures, wrote in a post on X that ‚ÄúToday is a turning point in AI. A Chinese open-source model is #1. Seminal moment in AI‚Äù.&lt;/p&gt;&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;&lt;blockquote class="cmplz-placeholder-element twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;üö® Today is a turning point in AI. A Chinese open source model is #1.&lt;/p&gt;&lt;p&gt;Kimi K2 Thinking scored 51% in Humanity's Last Exam, higher than GPT-5 and every other model. $0.6/M in, $2.5/M output.&lt;/p&gt;&lt;p&gt;The best at writing, and does 15tps on two Mac M3 Ultras!&lt;/p&gt;&lt;p&gt;Seminal moment in AI.&lt;/p&gt;&lt;p&gt;Try it‚Ä¶ pic.twitter.com/fmxlxpCGbE&lt;/p&gt;‚Äî Deedy (@deedydas) November 7, 2025&lt;/blockquote&gt;&lt;/div&gt;&lt;/figure&gt;&lt;p&gt;Nathan Lambert wrote in a Substack article that the success of Chinese open-source AI developers, including Moonshot AI and DeepSeek, showed how they ‚Äúmade the closed labs sweat,‚Äù adding ‚ÄúThere‚Äôs serious pricing pressure and expectations that [the US developers] need to manage‚Äù.&lt;/p&gt;&lt;p&gt;The release positions Moonshot AI alongside other Chinese AI companies like DeepSeek, Qwen, and Baichuan that are increasingly challenging the narrative of American AI supremacy through cost-efficient innovation and open-source development strategies.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Whether this represents a sustainable competitive advantage or a temporary convergence in capabilities remains to be seen as both US and Chinese companies continue advancing their models.&lt;/p&gt;&lt;p&gt;the public nature of the statements, and the market‚Äôs reaction, suggest substantive discussions may soon be underway.&lt;/p&gt;&lt;p&gt;The AI chip landscape is entering a period of flux. Organisations should maintain flexibility in their infrastructure strategy and monitor how partnerships like Tesla-Intel might reshape the competitive dynamics of AI hardware manufacturing.&lt;/p&gt;&lt;p&gt;The decisions made today about chip manufacturing partnerships could determine which organisations have access to cost-effective, high-performance AI infrastructure in the coming years.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Photo by Moonshot AI)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also:&lt;/strong&gt; DeepSeek disruption: Chinese AI innovation narrows global technology divide&lt;/p&gt;&lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. This comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/moonshot-ai-gpt-5-claude-comparison-china-breakthrough/</guid><pubDate>Tue, 11 Nov 2025 09:00:00 +0000</pubDate></item></channel></rss>