<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 16 Oct 2025 01:40:47 +0000</lastBuildDate><item><title>3 days left: Save up to $624 on your TechCrunch Disrupt 2025 Pass before prices rise (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/15/3-days-left-save-up-to-624-on-your-techcrunch-disrupt-2025-pass-before-prices-rise/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The final countdown of the final flash sale is here. Only three days remain to save up to $624 on your &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; pass before prices increase on Friday, October 17 at 11:59 p.m. PT.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;From October 27–29 at San Francisco’s Moscone West, Disrupt 2025 will bring together 10,000+ founders, investors, and operators for three days of ideas, dealmaking, and discovery across more than 200 sessions, 250+ speakers, and 300+ exhibiting startups building what’s next.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Register now to save up to $624&lt;/strong&gt; on your pass before this week ends. Got a group? Save up to 30% on &lt;strong&gt;group passes&lt;/strong&gt;. Bringing a plus-one? Get 50% off the second pass.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 3 days left" class="wp-image-3011291" height="383" src="https://techcrunch.com/wp-content/uploads/2025/05/TC25_3Days-16X9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-where-innovation-and-opportunity-collide"&gt;Where innovation and opportunity collide&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Disrupt is where the global startup ecosystem comes to connect, learn, and move faster. Discover the trends shaping AI, fintech, climate tech, mobility, and frontier industries, and meet the people leading those transformations.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2024 Breakout Session" class="wp-image-2985187" height="454" src="https://techcrunch.com/wp-content/uploads/2025/03/Disrupt-2024-Breakout.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Startup Battlefield 200&lt;/strong&gt; returns with the top 20 early-stage startups pitching live for $100,000 equity-free funding on the main stage.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Unparalleled networking&lt;/strong&gt; opportunities through curated meetups and impromptu run-ins.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Expert-led programming&lt;/strong&gt; designed to deliver actionable insights for founders, investors, and operators alike.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 class="wp-block-heading" id="h-featured-speakers"&gt;Featured speakers&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Sarah Guo and Elad Gil" class="wp-image-2660351" height="453" src="https://techcrunch.com/wp-content/uploads/2024/02/53486897769_be737acd80_k.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Join the industry’s most influential voices, including:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Chris Barman&lt;/strong&gt;, CEO,&amp;nbsp;Slate Auto&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Baiju Bhatt&lt;/strong&gt;, founder, Aetherflux&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Roelof Botha&lt;/strong&gt;, managing partner,&amp;nbsp;Sequoia Capital&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Phoebe Gates&lt;/strong&gt;&amp;nbsp;and&amp;nbsp;&lt;strong&gt;Sophia Kianni&lt;/strong&gt;, co-founders,&amp;nbsp;Phia&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Elad Gil&lt;/strong&gt;, CEO, Gil &amp;amp; Co.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Vinod Khosla&lt;/strong&gt;, founder,&amp;nbsp;Khosla Ventures&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Tekedra Mawakana&lt;/strong&gt;, co-CEO,&amp;nbsp;Waymo&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Bridgit Mendler&lt;/strong&gt;, co-founder, Northwood Space&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Kevin Scott&lt;/strong&gt;, CTO,&amp;nbsp;Microsoft&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Elizabeth Stone&lt;/strong&gt;, CTO,&amp;nbsp;Netflix&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Anatoly Yakovenko&lt;/strong&gt;, co-founder, Solana, and CEO,&amp;nbsp;Solana Labs&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Check out the &lt;strong&gt;250+ top voices&lt;/strong&gt; joining the Disrupt lineup&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-1707092" height="453" src="https://techcrunch.com/wp-content/uploads/2018/09/disruptsf18_brynn_putnam_mirror-1618.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Steve Jennings / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-passes-for-everyone-in-tech"&gt;Passes for everyone in tech&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Disrupt is &lt;em&gt;the&lt;/em&gt; tech epicenter of the year. Anybody and everybody who is building, investing, operating, or simply has a passion for tech, there’s&amp;nbsp;&lt;strong&gt;a pass designed for you&lt;/strong&gt;, your plus-one,&amp;nbsp;or&amp;nbsp;&lt;strong&gt;your team&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-founder-pass-build-smarter-scale-faster"&gt;Founder Pass: Build smarter, scale faster&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;The &lt;strong&gt;Founder Pass&lt;/strong&gt; is built for entrepreneurs ready to grow.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Access the &lt;strong&gt;Deal Flow Cafe&lt;/strong&gt; for networking and investor meetups.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Learn from &lt;strong&gt;Startup Battlefield 200&lt;/strong&gt; finalists and alumni who’ve gone on to raise millions.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Attend tactical sessions on &lt;strong&gt;fundraising, product strategy, and scaling&lt;/strong&gt;.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Connect directly with mentors, partners, and journalists who can amplify your story.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;Whether you’re pre-seed or scaling globally, the &lt;strong&gt;Founder Pass&lt;/strong&gt; is your ticket to meaningful progress.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-investor-pass-source-smarter-connect-faster"&gt;Investor Pass: Source smarter, connect faster&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;The &lt;strong&gt;Investor Pass&lt;/strong&gt; is designed for dealmakers and venture leaders.&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Gain &lt;strong&gt;exclusive access to Startup Battlefield 200&lt;/strong&gt; and the founders behind tomorrow’s category leaders.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Attend &lt;strong&gt;private investor-only receptions&lt;/strong&gt; and networking events.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Get &lt;strong&gt;market insights&lt;/strong&gt; from VCs, LPs, and strategics across emerging tech sectors in an investor-exclusive StrictlyVC session.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Build relationships with the operators and innovators defining the next generation of startups.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;For angels, venture firms, and CVCs, the &lt;strong&gt;Investor Pass&lt;/strong&gt; delivers high-value connections and actionable insight in one place.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-the-clock-is-ticking-3-days-until-the-final-flash-sale-disappears"&gt;The clock is ticking: 3 days until the final flash sale disappears&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;This is your final chance to get into one of the biggest tech conferences of the year at a discount. &lt;strong&gt;Register now and save up to $624&lt;/strong&gt; before prices rise on Friday, October 17 at 11:59 p.m. PT. Bringing your team? Save 15% to 30% on &lt;strong&gt;group passes&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt" class="wp-image-1301789" height="453" src="https://techcrunch.com/wp-content/uploads/2016/04/17205300128_99b3d607eb_o.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Noam Galai&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The final countdown of the final flash sale is here. Only three days remain to save up to $624 on your &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; pass before prices increase on Friday, October 17 at 11:59 p.m. PT.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;From October 27–29 at San Francisco’s Moscone West, Disrupt 2025 will bring together 10,000+ founders, investors, and operators for three days of ideas, dealmaking, and discovery across more than 200 sessions, 250+ speakers, and 300+ exhibiting startups building what’s next.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Register now to save up to $624&lt;/strong&gt; on your pass before this week ends. Got a group? Save up to 30% on &lt;strong&gt;group passes&lt;/strong&gt;. Bringing a plus-one? Get 50% off the second pass.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 3 days left" class="wp-image-3011291" height="383" src="https://techcrunch.com/wp-content/uploads/2025/05/TC25_3Days-16X9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-where-innovation-and-opportunity-collide"&gt;Where innovation and opportunity collide&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Disrupt is where the global startup ecosystem comes to connect, learn, and move faster. Discover the trends shaping AI, fintech, climate tech, mobility, and frontier industries, and meet the people leading those transformations.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2024 Breakout Session" class="wp-image-2985187" height="454" src="https://techcrunch.com/wp-content/uploads/2025/03/Disrupt-2024-Breakout.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Startup Battlefield 200&lt;/strong&gt; returns with the top 20 early-stage startups pitching live for $100,000 equity-free funding on the main stage.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Unparalleled networking&lt;/strong&gt; opportunities through curated meetups and impromptu run-ins.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Expert-led programming&lt;/strong&gt; designed to deliver actionable insights for founders, investors, and operators alike.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 class="wp-block-heading" id="h-featured-speakers"&gt;Featured speakers&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Sarah Guo and Elad Gil" class="wp-image-2660351" height="453" src="https://techcrunch.com/wp-content/uploads/2024/02/53486897769_be737acd80_k.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Join the industry’s most influential voices, including:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Chris Barman&lt;/strong&gt;, CEO,&amp;nbsp;Slate Auto&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Baiju Bhatt&lt;/strong&gt;, founder, Aetherflux&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Roelof Botha&lt;/strong&gt;, managing partner,&amp;nbsp;Sequoia Capital&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Phoebe Gates&lt;/strong&gt;&amp;nbsp;and&amp;nbsp;&lt;strong&gt;Sophia Kianni&lt;/strong&gt;, co-founders,&amp;nbsp;Phia&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Elad Gil&lt;/strong&gt;, CEO, Gil &amp;amp; Co.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Vinod Khosla&lt;/strong&gt;, founder,&amp;nbsp;Khosla Ventures&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Tekedra Mawakana&lt;/strong&gt;, co-CEO,&amp;nbsp;Waymo&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Bridgit Mendler&lt;/strong&gt;, co-founder, Northwood Space&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Kevin Scott&lt;/strong&gt;, CTO,&amp;nbsp;Microsoft&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Elizabeth Stone&lt;/strong&gt;, CTO,&amp;nbsp;Netflix&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Anatoly Yakovenko&lt;/strong&gt;, co-founder, Solana, and CEO,&amp;nbsp;Solana Labs&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Check out the &lt;strong&gt;250+ top voices&lt;/strong&gt; joining the Disrupt lineup&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-1707092" height="453" src="https://techcrunch.com/wp-content/uploads/2018/09/disruptsf18_brynn_putnam_mirror-1618.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Steve Jennings / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-passes-for-everyone-in-tech"&gt;Passes for everyone in tech&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Disrupt is &lt;em&gt;the&lt;/em&gt; tech epicenter of the year. Anybody and everybody who is building, investing, operating, or simply has a passion for tech, there’s&amp;nbsp;&lt;strong&gt;a pass designed for you&lt;/strong&gt;, your plus-one,&amp;nbsp;or&amp;nbsp;&lt;strong&gt;your team&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-founder-pass-build-smarter-scale-faster"&gt;Founder Pass: Build smarter, scale faster&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;The &lt;strong&gt;Founder Pass&lt;/strong&gt; is built for entrepreneurs ready to grow.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Access the &lt;strong&gt;Deal Flow Cafe&lt;/strong&gt; for networking and investor meetups.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Learn from &lt;strong&gt;Startup Battlefield 200&lt;/strong&gt; finalists and alumni who’ve gone on to raise millions.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Attend tactical sessions on &lt;strong&gt;fundraising, product strategy, and scaling&lt;/strong&gt;.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Connect directly with mentors, partners, and journalists who can amplify your story.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;Whether you’re pre-seed or scaling globally, the &lt;strong&gt;Founder Pass&lt;/strong&gt; is your ticket to meaningful progress.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-investor-pass-source-smarter-connect-faster"&gt;Investor Pass: Source smarter, connect faster&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;The &lt;strong&gt;Investor Pass&lt;/strong&gt; is designed for dealmakers and venture leaders.&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Gain &lt;strong&gt;exclusive access to Startup Battlefield 200&lt;/strong&gt; and the founders behind tomorrow’s category leaders.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Attend &lt;strong&gt;private investor-only receptions&lt;/strong&gt; and networking events.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Get &lt;strong&gt;market insights&lt;/strong&gt; from VCs, LPs, and strategics across emerging tech sectors in an investor-exclusive StrictlyVC session.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Build relationships with the operators and innovators defining the next generation of startups.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;For angels, venture firms, and CVCs, the &lt;strong&gt;Investor Pass&lt;/strong&gt; delivers high-value connections and actionable insight in one place.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-the-clock-is-ticking-3-days-until-the-final-flash-sale-disappears"&gt;The clock is ticking: 3 days until the final flash sale disappears&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;This is your final chance to get into one of the biggest tech conferences of the year at a discount. &lt;strong&gt;Register now and save up to $624&lt;/strong&gt; before prices rise on Friday, October 17 at 11:59 p.m. PT. Bringing your team? Save 15% to 30% on &lt;strong&gt;group passes&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt" class="wp-image-1301789" height="453" src="https://techcrunch.com/wp-content/uploads/2016/04/17205300128_99b3d607eb_o.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Noam Galai&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/15/3-days-left-save-up-to-624-on-your-techcrunch-disrupt-2025-pass-before-prices-rise/</guid><pubDate>Wed, 15 Oct 2025 14:00:00 +0000</pubDate></item><item><title>You can now text Spotify’s AI DJ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/15/you-can-now-text-spotifys-ai-dj/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/DJ-Header-2-2048x782-1.png?resize=1200,458" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Spotify on Wednesday upgraded its AI DJ feature — available to Premium subscribers — with a handful of new features, including the ability to send in your requests by typing, not just using voice commands.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature works with both English and Spanish requests, as Spotify’s Spanish-language DJ, called DJ Livi, now accepts music requests.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The AI DJ feature was updated earlier this year to accept voice requests, instead of only playing tunes Spotify thinks that you’ll like. However, that feature was only available to the English-language AI DJ until today.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Texting with an AI assistant is now a common behavior, thanks to the rising popularity of AI chatbots like ChatGPT and Gemini. These services allow for multi-modal inputs, meaning you can either talk, text, or upload images and files as part of your request. As more people have gotten used to shifting back and forth between input methods, Apple rolled out a version of its Siri assistant that can also be reached by text.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a natural next step for Spotify’s AI feature to accept text input, as well, given that users are often engaged with the streaming service when out and about, commuting, or in a quiet space where they don’t want to disturb others by issuing voice commands.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to the new texting feature, Spotify says that the AI DJ will also now offer personalized prompt suggestions to help inspire you if you’re not sure what you want to listen to next.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To access the DJ, you simply search for the term “DJ” on Spotify and then press play to start your curated selection of music. If you want to change the music, you can tap the DJ button in the bottom-right of the screen, then offer your suggestion via voice or text.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Spotify notes the DJ can handle requests that combine genre, mood, artist, or activity. The feature is currently live in English and Spanish in over 60 markets worldwide.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/DJ-Header-2-2048x782-1.png?resize=1200,458" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Spotify on Wednesday upgraded its AI DJ feature — available to Premium subscribers — with a handful of new features, including the ability to send in your requests by typing, not just using voice commands.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature works with both English and Spanish requests, as Spotify’s Spanish-language DJ, called DJ Livi, now accepts music requests.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The AI DJ feature was updated earlier this year to accept voice requests, instead of only playing tunes Spotify thinks that you’ll like. However, that feature was only available to the English-language AI DJ until today.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Texting with an AI assistant is now a common behavior, thanks to the rising popularity of AI chatbots like ChatGPT and Gemini. These services allow for multi-modal inputs, meaning you can either talk, text, or upload images and files as part of your request. As more people have gotten used to shifting back and forth between input methods, Apple rolled out a version of its Siri assistant that can also be reached by text.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a natural next step for Spotify’s AI feature to accept text input, as well, given that users are often engaged with the streaming service when out and about, commuting, or in a quiet space where they don’t want to disturb others by issuing voice commands.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to the new texting feature, Spotify says that the AI DJ will also now offer personalized prompt suggestions to help inspire you if you’re not sure what you want to listen to next.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To access the DJ, you simply search for the term “DJ” on Spotify and then press play to start your curated selection of music. If you want to change the music, you can tap the DJ button in the bottom-right of the screen, then offer your suggestion via voice or text.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Spotify notes the DJ can handle requests that combine genre, mood, artist, or activity. The feature is currently live in English and Spanish in over 60 markets worldwide.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/15/you-can-now-text-spotifys-ai-dj/</guid><pubDate>Wed, 15 Oct 2025 14:17:28 +0000</pubDate></item><item><title>Less than 3 days remain to secure your exhibit table at TechCrunch Disrupt 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/15/less-than-3-days-remain-to-secure-your-exhibit-table-at-techcrunch-disrupt-2025/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; runs October 27-29 at San Francisco’s Moscone West, and the Expo Hall is filling up fast. With just three days left to book, &lt;em&gt;now&lt;/em&gt; is the moment to &lt;strong&gt;reserve your exhibit table&lt;/strong&gt; before a competitor takes it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is your opportunity to showcase your innovation to investors, founders, media, and tech leaders who are actively seeking the next big idea, investment, or solution. The bustling Expo Hall is where all of Disrupt’s foot traffic converges — and that’s exactly where you’ll be.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Register now&lt;/strong&gt; before October 17, at 11:59 p.m. PT, and learn more about how an exhibit table boosts your brand.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt Expo Hall" class="wp-image-2571166" height="383" src="https://techcrunch.com/wp-content/uploads/2023/07/expo_hall.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Eric Slomonson, The Photo Group&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-exclusive-perks-for-exhibitors"&gt;Exclusive perks for exhibitors&lt;/h2&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Lead generation&lt;/strong&gt; through the Disrupt mobile app to connect with qualified attendees instantly&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Complimentary&lt;/strong&gt; partner Wi-Fi network to keep your team connected&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Silver Tier sponsor recognition for &lt;strong&gt;maximum visibility&lt;/strong&gt;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Access to the &lt;strong&gt;TechCrunch Disrupt press list&lt;/strong&gt; to amplify your story&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;10 comped passes&lt;/strong&gt; for your team to network, attend sessions, and meet key players&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Branding&lt;/strong&gt; on the website, app, on-site signage, and more to elevate your presence&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 class="wp-block-heading" id="h-don-t-miss-your-brand-spotlight"&gt;Don’t miss your brand spotlight&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;TechCrunch Disrupt&lt;/strong&gt; is where startups launch, deals are made, and industry-defining conversations happen. Tables are going fast, and the spot you want might not be available for long. &lt;strong&gt;Reserve your table now&lt;/strong&gt; before the October 17 booking deadline.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Early Stage 2024 Fidelity exhibit" class="wp-image-2987335" height="453" src="https://techcrunch.com/wp-content/uploads/2025/03/Early-Stage-2024-Fidelity-exhibit.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Halo Creative&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; runs October 27-29 at San Francisco’s Moscone West, and the Expo Hall is filling up fast. With just three days left to book, &lt;em&gt;now&lt;/em&gt; is the moment to &lt;strong&gt;reserve your exhibit table&lt;/strong&gt; before a competitor takes it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is your opportunity to showcase your innovation to investors, founders, media, and tech leaders who are actively seeking the next big idea, investment, or solution. The bustling Expo Hall is where all of Disrupt’s foot traffic converges — and that’s exactly where you’ll be.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Register now&lt;/strong&gt; before October 17, at 11:59 p.m. PT, and learn more about how an exhibit table boosts your brand.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt Expo Hall" class="wp-image-2571166" height="383" src="https://techcrunch.com/wp-content/uploads/2023/07/expo_hall.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Eric Slomonson, The Photo Group&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-exclusive-perks-for-exhibitors"&gt;Exclusive perks for exhibitors&lt;/h2&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Lead generation&lt;/strong&gt; through the Disrupt mobile app to connect with qualified attendees instantly&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Complimentary&lt;/strong&gt; partner Wi-Fi network to keep your team connected&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Silver Tier sponsor recognition for &lt;strong&gt;maximum visibility&lt;/strong&gt;&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Access to the &lt;strong&gt;TechCrunch Disrupt press list&lt;/strong&gt; to amplify your story&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;10 comped passes&lt;/strong&gt; for your team to network, attend sessions, and meet key players&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Branding&lt;/strong&gt; on the website, app, on-site signage, and more to elevate your presence&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 class="wp-block-heading" id="h-don-t-miss-your-brand-spotlight"&gt;Don’t miss your brand spotlight&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;TechCrunch Disrupt&lt;/strong&gt; is where startups launch, deals are made, and industry-defining conversations happen. Tables are going fast, and the spot you want might not be available for long. &lt;strong&gt;Reserve your table now&lt;/strong&gt; before the October 17 booking deadline.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Early Stage 2024 Fidelity exhibit" class="wp-image-2987335" height="453" src="https://techcrunch.com/wp-content/uploads/2025/03/Early-Stage-2024-Fidelity-exhibit.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Halo Creative&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/15/less-than-3-days-remain-to-secure-your-exhibit-table-at-techcrunch-disrupt-2025/</guid><pubDate>Wed, 15 Oct 2025 14:30:00 +0000</pubDate></item><item><title>ChatGPT erotica coming soon with age verification, CEO says (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/10/chatgpt-will-soon-allow-erotic-chats-for-verified-adults-only/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Sam Altman claims new tools can detect mental distress while relaxing limits for adults.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2023/12/openai-math-apples-300x169.jpg" width="300" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2023/12/openai-math-apples-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson | Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Tuesday, OpenAI CEO Sam Altman announced that the company will allow verified adult users to have erotic conversations with ChatGPT starting in December. The change represents a shift in how OpenAI approaches content restrictions, which the company had loosened in February but then dramatically tightened&amp;nbsp;after an August lawsuit from parents of a teen who died by suicide after allegedly receiving encouragement from ChatGPT.&lt;/p&gt;
&lt;p&gt;"In December, as we roll out age-gating more fully and as part of our 'treat adult users like adults' principle, we will allow even more, like erotica for verified adults," Altman wrote in his post on X (formerly Twitter). The announcement follows OpenAI's recent hint that it would allow developers to create "mature" ChatGPT applications once the company implements appropriate age verification and controls.&lt;/p&gt;
&lt;p&gt;Altman explained that OpenAI had made ChatGPT "pretty restrictive to make sure we were being careful with mental health issues" but acknowledged this approach made the chatbot "less useful/enjoyable to many users who had no mental health problems." The CEO said the company now has new tools to better detect when users are experiencing mental distress, allowing OpenAI to relax restrictions in most cases.&lt;/p&gt;
&lt;p&gt;Striking the right balance between freedom for adults and safety for users has been a difficult balancing act for OpenAI, which has vacillated between permissive and restrictive chat content controls over the past year.&lt;/p&gt;
&lt;p&gt;In February, the company updated its Model Spec to allow erotica in "appropriate contexts." But a March update made GPT-4o so agreeable that users complained about its "relentlessly positive tone." By August, Ars reported on cases where ChatGPT's sycophantic behavior had validated users' false beliefs to the point of causing mental health crises, and news of the aforementioned suicide lawsuit hit not long after.&lt;/p&gt;
&lt;p&gt;Aside from adjusting the behavioral outputs for its previous GPT-40 AI language model, new model changes have also created some turmoil among users. Since the launch of GPT-5 in early August, some users have been complaining that the new model feels less engaging than its predecessor, prompting OpenAI to bring back the older model as an option. Altman said the upcoming release will allow users to choose whether they want ChatGPT to "respond in a very human-like way, or use a ton of emoji, or act like a friend."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The December rollout will implement age verification for adult content, which OpenAI has not yet detailed technically. This represents a more explicit approach than the February policy change, which allowed erotica in certain contexts but lacked age-gating infrastructure.&lt;/p&gt;
&lt;h2&gt;Mental health concerns remain&lt;/h2&gt;
&lt;p&gt;Over time, as OpenAI allowed ChatGPT to express more humanlike simulated personality through revised system instructions and fine-tuning as a response to user feedback, ChatGPT has become more like a companion to some people than a work assistant. But dealing with the unexpected impacts of a reported 700 million users relying emotionally on largely unregulated and untested technology has been difficult for OpenAI, and the company has been forced to rapidly develop new safety initiatives and oversight bodies.&lt;/p&gt;
&lt;p&gt;OpenAI recently formed a council on "wellbeing and AI" to help guide the company's response to sensitive scenarios involving users in distress. The council includes eight researchers and experts who study how technology and AI affect mental health. However, as we previously reported, the council does not include any suicide prevention experts, despite recent calls from that community for OpenAI to implement stronger safeguards for users with suicidal thoughts.&lt;/p&gt;
&lt;p&gt;Altman maintains that the new detection tools will allow the company to "safely relax the restrictions" while still protecting vulnerable users. OpenAI has not yet specified what technical measures it will use for age verification or how the system will distinguish between allowed adult content and requests that might indicate mental health concerns, although the company typically uses moderation AI models that read the ongoing chat within ChatGPT and can interrupt it if it sees content that goes against OpenAI's policy instructions.&lt;/p&gt;
&lt;p&gt;OpenAI is not the first company to venture into AI companionship with mature content. Elon Musk's xAI previously launched an adult voice mode in its Grok app and flirty AI companions that appear as 3D anime models in the Grok app.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Sam Altman claims new tools can detect mental distress while relaxing limits for adults.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2023/12/openai-math-apples-300x169.jpg" width="300" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2023/12/openai-math-apples-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson | Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Tuesday, OpenAI CEO Sam Altman announced that the company will allow verified adult users to have erotic conversations with ChatGPT starting in December. The change represents a shift in how OpenAI approaches content restrictions, which the company had loosened in February but then dramatically tightened&amp;nbsp;after an August lawsuit from parents of a teen who died by suicide after allegedly receiving encouragement from ChatGPT.&lt;/p&gt;
&lt;p&gt;"In December, as we roll out age-gating more fully and as part of our 'treat adult users like adults' principle, we will allow even more, like erotica for verified adults," Altman wrote in his post on X (formerly Twitter). The announcement follows OpenAI's recent hint that it would allow developers to create "mature" ChatGPT applications once the company implements appropriate age verification and controls.&lt;/p&gt;
&lt;p&gt;Altman explained that OpenAI had made ChatGPT "pretty restrictive to make sure we were being careful with mental health issues" but acknowledged this approach made the chatbot "less useful/enjoyable to many users who had no mental health problems." The CEO said the company now has new tools to better detect when users are experiencing mental distress, allowing OpenAI to relax restrictions in most cases.&lt;/p&gt;
&lt;p&gt;Striking the right balance between freedom for adults and safety for users has been a difficult balancing act for OpenAI, which has vacillated between permissive and restrictive chat content controls over the past year.&lt;/p&gt;
&lt;p&gt;In February, the company updated its Model Spec to allow erotica in "appropriate contexts." But a March update made GPT-4o so agreeable that users complained about its "relentlessly positive tone." By August, Ars reported on cases where ChatGPT's sycophantic behavior had validated users' false beliefs to the point of causing mental health crises, and news of the aforementioned suicide lawsuit hit not long after.&lt;/p&gt;
&lt;p&gt;Aside from adjusting the behavioral outputs for its previous GPT-40 AI language model, new model changes have also created some turmoil among users. Since the launch of GPT-5 in early August, some users have been complaining that the new model feels less engaging than its predecessor, prompting OpenAI to bring back the older model as an option. Altman said the upcoming release will allow users to choose whether they want ChatGPT to "respond in a very human-like way, or use a ton of emoji, or act like a friend."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The December rollout will implement age verification for adult content, which OpenAI has not yet detailed technically. This represents a more explicit approach than the February policy change, which allowed erotica in certain contexts but lacked age-gating infrastructure.&lt;/p&gt;
&lt;h2&gt;Mental health concerns remain&lt;/h2&gt;
&lt;p&gt;Over time, as OpenAI allowed ChatGPT to express more humanlike simulated personality through revised system instructions and fine-tuning as a response to user feedback, ChatGPT has become more like a companion to some people than a work assistant. But dealing with the unexpected impacts of a reported 700 million users relying emotionally on largely unregulated and untested technology has been difficult for OpenAI, and the company has been forced to rapidly develop new safety initiatives and oversight bodies.&lt;/p&gt;
&lt;p&gt;OpenAI recently formed a council on "wellbeing and AI" to help guide the company's response to sensitive scenarios involving users in distress. The council includes eight researchers and experts who study how technology and AI affect mental health. However, as we previously reported, the council does not include any suicide prevention experts, despite recent calls from that community for OpenAI to implement stronger safeguards for users with suicidal thoughts.&lt;/p&gt;
&lt;p&gt;Altman maintains that the new detection tools will allow the company to "safely relax the restrictions" while still protecting vulnerable users. OpenAI has not yet specified what technical measures it will use for age verification or how the system will distinguish between allowed adult content and requests that might indicate mental health concerns, although the company typically uses moderation AI models that read the ongoing chat within ChatGPT and can interrupt it if it sees content that goes against OpenAI's policy instructions.&lt;/p&gt;
&lt;p&gt;OpenAI is not the first company to venture into AI companionship with mature content. Elon Musk's xAI previously launched an adult voice mode in its Grok app and flirty AI companions that appear as 3D anime models in the Grok app.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/10/chatgpt-will-soon-allow-erotic-chats-for-verified-adults-only/</guid><pubDate>Wed, 15 Oct 2025 15:14:52 +0000</pubDate></item><item><title>Google releases Veo 3.1, adds it to Flow video editor (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/15/google-releases-veo-3-1-adds-it-to-flow-video-editor/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-15-at-8.03.20PM.jpg?resize=1200,578" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google launched its new video model Veo 3.1 with improved audio output, granular editing controls, and better output for image to video. It said that Veo 3.1 builds on May’s Veo 3 release and generates more realistic clips and adheres to prompts better.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The model allows users to add an object to the video and have it blend into the clip’s style, Google said. Soon, users will be able to remove an existing object from the video in Flow, too.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Veo 3 already has edit features such as adding reference images to drive a character, providing the first and last frame to generate a clip using AI, and the ability to extend an existing video based on the last few frames. With Veo 3.1, Google is adding audio to all these features to make the clips more lively. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is rolling out the model to its video editor Flow, the  Gemini App, along with Vertex and Gemini APIs. It said that since Flow’s launch in May, users have created more than 275 million videos on the app. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-15-at-8.03.20PM.jpg?resize=1200,578" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google launched its new video model Veo 3.1 with improved audio output, granular editing controls, and better output for image to video. It said that Veo 3.1 builds on May’s Veo 3 release and generates more realistic clips and adheres to prompts better.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The model allows users to add an object to the video and have it blend into the clip’s style, Google said. Soon, users will be able to remove an existing object from the video in Flow, too.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Veo 3 already has edit features such as adding reference images to drive a character, providing the first and last frame to generate a clip using AI, and the ability to extend an existing video based on the last few frames. With Veo 3.1, Google is adding audio to all these features to make the clips more lively. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is rolling out the model to its video editor Flow, the  Gemini App, along with Vertex and Gemini APIs. It said that since Flow’s launch in May, users have created more than 275 million videos on the app. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/15/google-releases-veo-3-1-adds-it-to-flow-video-editor/</guid><pubDate>Wed, 15 Oct 2025 16:00:00 +0000</pubDate></item><item><title>Google’s AI videos get a big upgrade with Veo 3.1 (AI – Ars Technica)</title><link>https://arstechnica.com/google/2025/10/googles-ai-videos-get-a-big-upgrade-with-veo-3-1/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Veo 3.1 is coming to the Gemini app and the Flow filmmaking tool.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Veo 3.1" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/Veo-3.1-640x360.png" width="640" /&gt;
                  &lt;img alt="Veo 3.1" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/Veo-3.1-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;It's getting harder to know what's real on the Internet, and Google is not helping one bit with the announcement of Veo 3.1. The company's new video model supposedly offers better audio and realism, along with greater prompt accuracy. The updated video AI will be available throughout the Google ecosystem, including the Flow filmmaking tool, where the new model will unlock additional features. And if you're worried about the cost of conjuring all these AI videos, Google is also adding a "Fast" variant of Veo.&lt;/p&gt;
&lt;p&gt;Veo made waves when it debuted earlier this year, demonstrating a staggering improvement in AI video quality just a few months after Veo 2's release. It turns out that having all that video on YouTube is very useful for training AI models, so Google is already moving on to Veo 3.1 with a raft of new features.&lt;/p&gt;
&lt;p&gt;Google says Veo 3.1 offers stronger prompt adherence, which results in better video outputs and fewer wasted compute cycles. Audio, which was a hallmark feature of the Veo 3 release, has reportedly improved, too. Veo 3's text-to-video was limited to 720p landscape output, but there's an ever-increasing volume of vertical video on the Internet. So Veo 3.1 can produce both landscape and portrait 16:9 video.&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1080" id="video-2122592-1" preload="metadata" width="1920"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/Veo-3.1-opt.mp4?_=1" type="video/mp4" /&gt;&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
      &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Google previously said it would bring Veo video tools to YouTube Shorts, which use a vertical video format like TikTok. The release of Veo 3.1 probably opens the door to fulfilling that promise. You can bet Veo videos will show up more frequently on TikTok as well now that it fits the format. This release also keeps Google in its race with OpenAI, which recently released a Sora iPhone app with an impressive new version of its video-generating AI.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;A focus on filmmakers&lt;/h2&gt;
&lt;p&gt;The Veo 3.1 model will be available across Google's AI ecosystem. You'll be able to create content with Veo 3.1 and Veo 3.1 Fast via the Gemini app, and developers will have access in Vertex AI and through the Gemini API. Using the Fast variant will help keep costs down when paying per token. Presumably, users of the Gemini app will get more Fast video generations—we've asked Google about limits and will report if we hear back.&lt;/p&gt;
&lt;p&gt;Veo is the underlying model in Google's Flow filmmaking tool, and it's getting a few new capabilities thanks to the updated model. The Ingredients to Video, Frames to Video, and Extend features are now all compatible with generated audio. So you can upload multiple images as a reference or use images as a starting or end point while also adding custom audio to the clip. These same capabilities are offered in the API, and the Gemini app continues to accept reference images for Veo outputs. The app doesn't get all the Flow features, though.&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1080" id="video-2122592-2" preload="metadata" width="1920"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/Add-or-remove-object-opt.mp4?_=2" type="video/mp4" /&gt;&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
      &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;There are a couple of entirely new video features coming with Veo 3.1, too. Google says Veo 3.1 is better able to replicate the look of a video while making "precision" edits. So you'll be able to add an object to a clip while keeping the rest of it unchanged (more or less). Likewise, you can remove an element without changing the rest of the scene. Adding objects will be available in Flow and the API immediately. Removing objects won't be available in Flow just yet, but Google says that feature will be coming soon.&lt;/p&gt;
&lt;p&gt;The new video model begins rolling out today, so make sure you use a skeptical eye when scrolling through vertical videos.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Google Veo 3.1

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Veo 3.1 is coming to the Gemini app and the Flow filmmaking tool.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Veo 3.1" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/Veo-3.1-640x360.png" width="640" /&gt;
                  &lt;img alt="Veo 3.1" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/Veo-3.1-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;It's getting harder to know what's real on the Internet, and Google is not helping one bit with the announcement of Veo 3.1. The company's new video model supposedly offers better audio and realism, along with greater prompt accuracy. The updated video AI will be available throughout the Google ecosystem, including the Flow filmmaking tool, where the new model will unlock additional features. And if you're worried about the cost of conjuring all these AI videos, Google is also adding a "Fast" variant of Veo.&lt;/p&gt;
&lt;p&gt;Veo made waves when it debuted earlier this year, demonstrating a staggering improvement in AI video quality just a few months after Veo 2's release. It turns out that having all that video on YouTube is very useful for training AI models, so Google is already moving on to Veo 3.1 with a raft of new features.&lt;/p&gt;
&lt;p&gt;Google says Veo 3.1 offers stronger prompt adherence, which results in better video outputs and fewer wasted compute cycles. Audio, which was a hallmark feature of the Veo 3 release, has reportedly improved, too. Veo 3's text-to-video was limited to 720p landscape output, but there's an ever-increasing volume of vertical video on the Internet. So Veo 3.1 can produce both landscape and portrait 16:9 video.&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1080" id="video-2122592-1" preload="metadata" width="1920"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/Veo-3.1-opt.mp4?_=1" type="video/mp4" /&gt;&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
      &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Google previously said it would bring Veo video tools to YouTube Shorts, which use a vertical video format like TikTok. The release of Veo 3.1 probably opens the door to fulfilling that promise. You can bet Veo videos will show up more frequently on TikTok as well now that it fits the format. This release also keeps Google in its race with OpenAI, which recently released a Sora iPhone app with an impressive new version of its video-generating AI.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;A focus on filmmakers&lt;/h2&gt;
&lt;p&gt;The Veo 3.1 model will be available across Google's AI ecosystem. You'll be able to create content with Veo 3.1 and Veo 3.1 Fast via the Gemini app, and developers will have access in Vertex AI and through the Gemini API. Using the Fast variant will help keep costs down when paying per token. Presumably, users of the Gemini app will get more Fast video generations—we've asked Google about limits and will report if we hear back.&lt;/p&gt;
&lt;p&gt;Veo is the underlying model in Google's Flow filmmaking tool, and it's getting a few new capabilities thanks to the updated model. The Ingredients to Video, Frames to Video, and Extend features are now all compatible with generated audio. So you can upload multiple images as a reference or use images as a starting or end point while also adding custom audio to the clip. These same capabilities are offered in the API, and the Gemini app continues to accept reference images for Veo outputs. The app doesn't get all the Flow features, though.&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1080" id="video-2122592-2" preload="metadata" width="1920"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/Add-or-remove-object-opt.mp4?_=2" type="video/mp4" /&gt;&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
      &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;There are a couple of entirely new video features coming with Veo 3.1, too. Google says Veo 3.1 is better able to replicate the look of a video while making "precision" edits. So you'll be able to add an object to a clip while keeping the rest of it unchanged (more or less). Likewise, you can remove an element without changing the rest of the scene. Adding objects will be available in Flow and the API immediately. Removing objects won't be available in Flow just yet, but Google says that feature will be coming soon.&lt;/p&gt;
&lt;p&gt;The new video model begins rolling out today, so make sure you use a skeptical eye when scrolling through vertical videos.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Google Veo 3.1

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2025/10/googles-ai-videos-get-a-big-upgrade-with-veo-3-1/</guid><pubDate>Wed, 15 Oct 2025 16:00:51 +0000</pubDate></item><item><title>Introducing Veo 3.1 and advanced creative capabilities (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/introducing-veo-3-1-and-advanced-creative-capabilities/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Veo3.1_Social_v3.width-1300.png" /&gt;&lt;/div&gt;

            

            
            
&lt;!--article text--&gt;

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Five months ago, we introduced Flow, our AI filmmaking tool powered by Veo, and have been inspired by the creativity it has sparked with over 275 million videos generated in Flow



  &lt;sup&gt;1&lt;/sup&gt;

. We're always listening to your feedback, and we've heard that you want more artistic control within Flow, with increased support for audio across all features.&lt;/p&gt;&lt;p&gt;Today, we’re introducing new and enhanced creative capabilities to edit your clips, giving you more granular control over your final scene. For the first time, we’re also bringing audio to existing capabilities like “Ingredients to Video,” “Frames to Video” and “Extend.”&lt;/p&gt;&lt;p&gt;We’re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;Refine your narrative with audio and more control&lt;/h2&gt;&lt;p&gt;With Veo 3.1, we’re bringing audio to existing capabilities to help you craft the perfect scene. These features are experimental and actively improving, and we’re excited to see what you create as we iterate based on your feedback.&lt;/p&gt;&lt;p&gt;Now, with rich, generated audio, you can:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Craft the look of your scene.&lt;/b&gt; With "Ingredients to Video," you can use multiple reference images to control the characters, objects and style. Flow uses your ingredients to create a final scene that looks just as you envisioned.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Control the shot from start to finish.&lt;/b&gt; Provide a starting and ending image with “Frames to Video,” and Flow will generate a seamless video that bridges the two, perfect for artful and epic transitions.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Create longer, seamless shots.&lt;/b&gt; With "Extend," you can create longer videos, even lasting for a minute or more, that connect to and continue the action from your original clip. Each video is generated based on the final second of your previous clip, making it most useful for creating a longer establishing shot.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;Edit your ingredients and videos with more precision&lt;/h2&gt;&lt;p&gt;Great ideas can strike at any point in the creative process. For moments when the first take isn't the final one, we're introducing new editing capabilities directly within Flow to help you reimagine and perfect your scenes.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Add new elements to any scene.&lt;/b&gt; With “Insert,” introduce anything you can imagine, from realistic details to fantastical creatures. Flow now handles complex details like shadows and scene lighting, making the addition look natural.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Remove unwanted objects or characters seamlessly.&lt;/b&gt; Soon, you’ll be able to take anything out of a scene, and Flow will reconstruct the background and surroundings, making it look as though the object was never there.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;Start creating in Flow today&lt;/h2&gt;&lt;p&gt;With more precise editing capabilities, audio across all existing features and higher-quality outputs powered by Veo 3.1, we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.&lt;/p&gt;&lt;p&gt;The Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API



  &lt;sup&gt;2&lt;/sup&gt;

and Vertex AI



  &lt;sup&gt;3&lt;/sup&gt;

.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  


            
            

            
              


&lt;div class="
    uni-blog-article-tags
    article-tags
    
  "&gt;
  &lt;div class="uni-blog-article-tags__wrapper"&gt;
    &lt;span class="uni-blog-article-tags__label uni-eyebrow"&gt;POSTED IN:&lt;/span&gt;
  &lt;/div&gt;
  &lt;nav class="uni-blog-article-tags__container uni-click-tracker"&gt;
    &lt;ul class="uni-blog-article-tags__tags-list"&gt;
    
      &lt;li&gt;
        
        
        


  


AI


  


      &lt;/li&gt;
    

    
      &lt;li&gt;
        
        
        


  


Google DeepMind


  


      &lt;/li&gt;
    
      &lt;li&gt;
        
        
        


  


Google Labs


  


      &lt;/li&gt;
    
    &lt;/ul&gt;
  &lt;/nav&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Veo3.1_Social_v3.width-1300.png" /&gt;&lt;/div&gt;

            

            
            
&lt;!--article text--&gt;

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Five months ago, we introduced Flow, our AI filmmaking tool powered by Veo, and have been inspired by the creativity it has sparked with over 275 million videos generated in Flow



  &lt;sup&gt;1&lt;/sup&gt;

. We're always listening to your feedback, and we've heard that you want more artistic control within Flow, with increased support for audio across all features.&lt;/p&gt;&lt;p&gt;Today, we’re introducing new and enhanced creative capabilities to edit your clips, giving you more granular control over your final scene. For the first time, we’re also bringing audio to existing capabilities like “Ingredients to Video,” “Frames to Video” and “Extend.”&lt;/p&gt;&lt;p&gt;We’re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;Refine your narrative with audio and more control&lt;/h2&gt;&lt;p&gt;With Veo 3.1, we’re bringing audio to existing capabilities to help you craft the perfect scene. These features are experimental and actively improving, and we’re excited to see what you create as we iterate based on your feedback.&lt;/p&gt;&lt;p&gt;Now, with rich, generated audio, you can:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Craft the look of your scene.&lt;/b&gt; With "Ingredients to Video," you can use multiple reference images to control the characters, objects and style. Flow uses your ingredients to create a final scene that looks just as you envisioned.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Control the shot from start to finish.&lt;/b&gt; Provide a starting and ending image with “Frames to Video,” and Flow will generate a seamless video that bridges the two, perfect for artful and epic transitions.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Create longer, seamless shots.&lt;/b&gt; With "Extend," you can create longer videos, even lasting for a minute or more, that connect to and continue the action from your original clip. Each video is generated based on the final second of your previous clip, making it most useful for creating a longer establishing shot.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;Edit your ingredients and videos with more precision&lt;/h2&gt;&lt;p&gt;Great ideas can strike at any point in the creative process. For moments when the first take isn't the final one, we're introducing new editing capabilities directly within Flow to help you reimagine and perfect your scenes.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Add new elements to any scene.&lt;/b&gt; With “Insert,” introduce anything you can imagine, from realistic details to fantastical creatures. Flow now handles complex details like shadows and scene lighting, making the addition look natural.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Remove unwanted objects or characters seamlessly.&lt;/b&gt; Soon, you’ll be able to take anything out of a scene, and Flow will reconstruct the background and surroundings, making it look as though the object was never there.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;Start creating in Flow today&lt;/h2&gt;&lt;p&gt;With more precise editing capabilities, audio across all existing features and higher-quality outputs powered by Veo 3.1, we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.&lt;/p&gt;&lt;p&gt;The Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API



  &lt;sup&gt;2&lt;/sup&gt;

and Vertex AI



  &lt;sup&gt;3&lt;/sup&gt;

.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  


            
            

            
              


&lt;div class="
    uni-blog-article-tags
    article-tags
    
  "&gt;
  &lt;div class="uni-blog-article-tags__wrapper"&gt;
    &lt;span class="uni-blog-article-tags__label uni-eyebrow"&gt;POSTED IN:&lt;/span&gt;
  &lt;/div&gt;
  &lt;nav class="uni-blog-article-tags__container uni-click-tracker"&gt;
    &lt;ul class="uni-blog-article-tags__tags-list"&gt;
    
      &lt;li&gt;
        
        
        


  


AI


  


      &lt;/li&gt;
    

    
      &lt;li&gt;
        
        
        


  


Google DeepMind


  


      &lt;/li&gt;
    
      &lt;li&gt;
        
        
        


  


Google Labs


  


      &lt;/li&gt;
    
    &lt;/ul&gt;
  &lt;/nav&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/introducing-veo-3-1-and-advanced-creative-capabilities/</guid><pubDate>Wed, 15 Oct 2025 16:01:08 +0000</pubDate></item><item><title>Nscale inks massive AI infrastructure deal with Microsoft (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/15/nscale-inks-massive-ai-infrastructure-deal-with-microsoft/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2201260651.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI hyperscaler startup Nscale has signed a sizable deal with Microsoft to bring Nvidia AI hardware to multiple data centers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI cloud provider announced on Wednesday that it signed a deal with Microsoft to bring approximately 200,000 Nvidia GB300 GPUs to three data centers in Europe and one in the U.S.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;These GPUs will be delivered through Nscale-owned operations and through a joint venture with investment company Aker, one of Nscale’s investors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;About half of these GPUs, 104,000, will head to a data center in Texas leased by Ionic Digital, over the next 12 to 18 months. Nscale plans to increase its footprint at this location to 1.2 gigawatts, the company stated.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nscale will also deploy 12,600 GPUs to the Start Campus data center in Sines, Portugal, starting in the first quarter of 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The deal builds on previous plans with both Microsoft and Aker regarding data centers in Norway and the United Kingdom. Nscale will send 23,000 GPUS to its Loughton, England campus starting in 2027, and send the remaining 52,000 GPUs to Microsoft’s AI campus in Narvik, Norway.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This agreement confirms Nscale’s place as a partner of choice for the world’s most important technology leaders,” Josh Payne, founder and CEO of Nscale, said in a company press release. “Few companies are equipped to deliver GPU deployments at this scale, but we have the experience and have built the global pipeline to do so.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a bold claim by Payne, considering that Nscale was founded in 2024. Since its launch, the company has raised more than $1.7 billion from strategic partners including Aker, Nokia and Nvidia. Nscale has also raised from investors like Sandton Capital Partners, G Squared and Point72, among others. Payne told the Financial Times that the company is looking at an IPO as early as the end of next year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The pace with which we have expanded our capacity demonstrates both our readiness and our commitment to efficiency, sustainability and providing our customers with the most advanced technology available. It’s a clear signal that Nscale is setting a new standard for how the next wave of AI infrastructure will be delivered,” Payne said in the release.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GPU deals have picked up in recent weeks. OpenAI announced it was purchasing six gigawatts’ worth of chips from AMD last week. OpenAI also recently inked a deal with Nvidia in which Nvidia will invest up to $100 billion in the company in exchange for 10 gigawatts’ worth of chips in September.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2201260651.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI hyperscaler startup Nscale has signed a sizable deal with Microsoft to bring Nvidia AI hardware to multiple data centers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI cloud provider announced on Wednesday that it signed a deal with Microsoft to bring approximately 200,000 Nvidia GB300 GPUs to three data centers in Europe and one in the U.S.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;These GPUs will be delivered through Nscale-owned operations and through a joint venture with investment company Aker, one of Nscale’s investors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;About half of these GPUs, 104,000, will head to a data center in Texas leased by Ionic Digital, over the next 12 to 18 months. Nscale plans to increase its footprint at this location to 1.2 gigawatts, the company stated.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nscale will also deploy 12,600 GPUs to the Start Campus data center in Sines, Portugal, starting in the first quarter of 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The deal builds on previous plans with both Microsoft and Aker regarding data centers in Norway and the United Kingdom. Nscale will send 23,000 GPUS to its Loughton, England campus starting in 2027, and send the remaining 52,000 GPUs to Microsoft’s AI campus in Narvik, Norway.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This agreement confirms Nscale’s place as a partner of choice for the world’s most important technology leaders,” Josh Payne, founder and CEO of Nscale, said in a company press release. “Few companies are equipped to deliver GPU deployments at this scale, but we have the experience and have built the global pipeline to do so.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a bold claim by Payne, considering that Nscale was founded in 2024. Since its launch, the company has raised more than $1.7 billion from strategic partners including Aker, Nokia and Nvidia. Nscale has also raised from investors like Sandton Capital Partners, G Squared and Point72, among others. Payne told the Financial Times that the company is looking at an IPO as early as the end of next year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The pace with which we have expanded our capacity demonstrates both our readiness and our commitment to efficiency, sustainability and providing our customers with the most advanced technology available. It’s a clear signal that Nscale is setting a new standard for how the next wave of AI infrastructure will be delivered,” Payne said in the release.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GPU deals have picked up in recent weeks. OpenAI announced it was purchasing six gigawatts’ worth of chips from AMD last week. OpenAI also recently inked a deal with Nvidia in which Nvidia will invest up to $100 billion in the company in exchange for 10 gigawatts’ worth of chips in September.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/15/nscale-inks-massive-ai-infrastructure-deal-with-microsoft/</guid><pubDate>Wed, 15 Oct 2025 16:01:11 +0000</pubDate></item><item><title>Meta partners up with Arm to scale AI efforts (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/15/arm-partners-with-meta-to-scale-ai-efforts/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/GettyImages-2173579488.jpg?resize=1200,799" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Semiconductor design company Arm is partnering up with Meta to enhance the social media giant’s AI systems amid an unprecedented infrastructure buildout. Under the partnership, Meta’s ranking and recommendation systems will move to Arm’s Neoverse platform, which was recently optimized for AI systems in the cloud, among other implementations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AI is transforming how people connect and create,” Santosh Janardhan, Meta’s head of infrastructure, said in a statement. “Partnering with Arm enables us to efficiently scale that innovation to the more than 3 billion people who use Meta’s apps and technologies.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Best known for its mobile CPU architecture, Arm’s GPU offerings have often been overshadowed by competitors like Nvidia. But Arm is now emphasizing its advantage in low-power deployments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AI’s next era will be defined by delivering efficiency at scale,” Rene Haas, Arm’s CEO, said in a statement. “Partnering with Meta, we’re uniting Arm’s performance-per-watt leadership with Meta’s AI innovation.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The multi-year partnership comes as Meta invests in vastly expanding its data center network to keep pace with anticipated demand for AI services. One project, code-named “Prometheus,” is expected to come online with multiple gigawatts of power in 2027. Construction is currently underway in New Albany, Ohio, and a 200-megawatt natural gas project is being constructed to directly serve the project’s power needs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta is also building a data center campus, code-named “Hyperion,” across 2,250 acres in northwest Louisiana that’s meant to deliver 5 gigawatts of computational power when complete. Construction is expected to continue through 2030, although some portions may come online before then.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Crucially, Arm and Meta are&amp;nbsp;not exchanging ownership stakes or significant physical infrastructure, setting this partnership apart from a number of recent AI infrastructure deals. Nvidia has been particularly aggressive with its investments — it recently committed to a $100 billion phased investment into OpenAI, as well as billion-dollar investments into Elon Musk’s xAI, Mira Murati’s Thinking Machines Lab, and French AI lab Mistral.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;A rival to both Nvidia and Arm, AMD recently committed to supplying OpenAI with 6 gigawatts’ worth of compute capacity. As part of the deal, OpenAI will receive AMD stock options worth as much as 10% of the company.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/GettyImages-2173579488.jpg?resize=1200,799" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Semiconductor design company Arm is partnering up with Meta to enhance the social media giant’s AI systems amid an unprecedented infrastructure buildout. Under the partnership, Meta’s ranking and recommendation systems will move to Arm’s Neoverse platform, which was recently optimized for AI systems in the cloud, among other implementations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AI is transforming how people connect and create,” Santosh Janardhan, Meta’s head of infrastructure, said in a statement. “Partnering with Arm enables us to efficiently scale that innovation to the more than 3 billion people who use Meta’s apps and technologies.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Best known for its mobile CPU architecture, Arm’s GPU offerings have often been overshadowed by competitors like Nvidia. But Arm is now emphasizing its advantage in low-power deployments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AI’s next era will be defined by delivering efficiency at scale,” Rene Haas, Arm’s CEO, said in a statement. “Partnering with Meta, we’re uniting Arm’s performance-per-watt leadership with Meta’s AI innovation.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The multi-year partnership comes as Meta invests in vastly expanding its data center network to keep pace with anticipated demand for AI services. One project, code-named “Prometheus,” is expected to come online with multiple gigawatts of power in 2027. Construction is currently underway in New Albany, Ohio, and a 200-megawatt natural gas project is being constructed to directly serve the project’s power needs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta is also building a data center campus, code-named “Hyperion,” across 2,250 acres in northwest Louisiana that’s meant to deliver 5 gigawatts of computational power when complete. Construction is expected to continue through 2030, although some portions may come online before then.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Crucially, Arm and Meta are&amp;nbsp;not exchanging ownership stakes or significant physical infrastructure, setting this partnership apart from a number of recent AI infrastructure deals. Nvidia has been particularly aggressive with its investments — it recently committed to a $100 billion phased investment into OpenAI, as well as billion-dollar investments into Elon Musk’s xAI, Mira Murati’s Thinking Machines Lab, and French AI lab Mistral.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;A rival to both Nvidia and Arm, AMD recently committed to supplying OpenAI with 6 gigawatts’ worth of compute capacity. As part of the deal, OpenAI will receive AMD stock options worth as much as 10% of the company.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/15/arm-partners-with-meta-to-scale-ai-efforts/</guid><pubDate>Wed, 15 Oct 2025 16:05:00 +0000</pubDate></item><item><title>Anthropic launches new version of scaled-down ‘Haiku’ model (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/15/anthropic-launches-new-version-of-scaled-down-haiku-model/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Screen-Shot-2025-10-15-at-10.38.58-AM.jpg?w=800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Wednesday, Anthropic released Claude Haiku 4.5, the newest version of its smallest model, billed as offering similar performance to Sonnet 4 “at one-third the cost and more than twice the speed,” per a company blog post. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic cites a range of new benchmark results to back up those performance claims. In the company’s testing, Haiku scored 73% on SWE-Bench verified and 41% on the command-line-focused Terminal-Bench — below Sonnet 4.5, but on par with Sonnet 4, GPT-5, and Gemini 2.5 in each case. Tests show similar results on benchmarks for tool use, computer use, and visual reasoning.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The new version of Haiku will be immediately available under all free Anthropic plans and the company believes it will be particularly appealing for free versions of AI products, where it can provide significant capabilities while minimizing server loads. The lightweight nature of the model also means it’s easier to deploy multiple Haiku agents in parallel or in combination with a more sophisticated model.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a statement to press, Anthropic CPO Mike Krieger said the Haiku would make new styles of deployment possible in production for the first time. “It’s opening up entirely new categories of what’s possible with AI in production environments – with Sonnet handling complex planning while Haiku-powered sub-agents execute at speed,” Krieger said. “We’re giving people a complete agent toolbox where each model has the right combination of intelligence, speed, and cost for different parts of the job.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The most immediate applications are likely to come in software development tools, where Claude Code is already commonly used and latency is often a critical factor. In statements provided by Anthropic, Zencoder CEO Andrew Filev described the new version of Haiku as “unlocking an entirely new set of use cases.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Haiku 4.5 comes after a string of high-profile launches for Anthropic: just two weeks after the launch of Sonnet 4.5 and two months after the launch of Opus 4.1, both of which were hailed as state-of-the-art on release. The previous version of Haiku was released in October 2024.&lt;/p&gt;


&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Screen-Shot-2025-10-15-at-10.38.58-AM.jpg?w=800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Wednesday, Anthropic released Claude Haiku 4.5, the newest version of its smallest model, billed as offering similar performance to Sonnet 4 “at one-third the cost and more than twice the speed,” per a company blog post. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic cites a range of new benchmark results to back up those performance claims. In the company’s testing, Haiku scored 73% on SWE-Bench verified and 41% on the command-line-focused Terminal-Bench — below Sonnet 4.5, but on par with Sonnet 4, GPT-5, and Gemini 2.5 in each case. Tests show similar results on benchmarks for tool use, computer use, and visual reasoning.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The new version of Haiku will be immediately available under all free Anthropic plans and the company believes it will be particularly appealing for free versions of AI products, where it can provide significant capabilities while minimizing server loads. The lightweight nature of the model also means it’s easier to deploy multiple Haiku agents in parallel or in combination with a more sophisticated model.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a statement to press, Anthropic CPO Mike Krieger said the Haiku would make new styles of deployment possible in production for the first time. “It’s opening up entirely new categories of what’s possible with AI in production environments – with Sonnet handling complex planning while Haiku-powered sub-agents execute at speed,” Krieger said. “We’re giving people a complete agent toolbox where each model has the right combination of intelligence, speed, and cost for different parts of the job.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The most immediate applications are likely to come in software development tools, where Claude Code is already commonly used and latency is often a critical factor. In statements provided by Anthropic, Zencoder CEO Andrew Filev described the new version of Haiku as “unlocking an entirely new set of use cases.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Haiku 4.5 comes after a string of high-profile launches for Anthropic: just two weeks after the launch of Sonnet 4.5 and two months after the launch of Opus 4.1, both of which were hailed as state-of-the-art on release. The previous version of Haiku was released in October 2024.&lt;/p&gt;


&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/15/anthropic-launches-new-version-of-scaled-down-haiku-model/</guid><pubDate>Wed, 15 Oct 2025 17:00:00 +0000</pubDate></item><item><title>Liberate bags $50M at $300M valuation to bring AI deeper into insurance back offices (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/15/liberate-bags-50m-at-300m-valuation-to-bring-ai-deeper-into-insurance-back-offices/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Liberate, an AI startup automating insurance operations, has raised $50 million in a round led by Battery Ventures as it looks to scale its agentic deployments across carriers and agencies globally.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The all-equity round values the three-year-old startup at $300 million post-money, with participation from new investor Canapi Ventures and returning backers Redpoint Ventures, Eclipse, and Commerce Ventures.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The insurance industry has been navigating a difficult stretch, with rising operational costs, legacy system constraints, and increasing customer expectations. Specifically in the non-life segment, global premium growth is projected to slow through 2026, driven by heightened competition, weaker rate momentum, and new cost pressures, including tariffs, per a recent report by Deloitte. While some carriers experimented with AI, many early efforts stalled due to fragmented data and inflexible workflows. That is now changing, as insurers shift toward full-scale AI adoption — embedding it into the core of their operations rather than layering it on top. Liberate is stepping in to meet this shift head-on.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Founded in 2022, the San Francisco-based startup builds AI systems for property and casualty insurers, focusing on sales, service, and claims. At the front end, its voice AI assistant, Nicole, handles inbound and outbound calls to help sell policies or respond to service requests. Behind the scenes, a network of reasoning-based AI agents connects to insurers’ existing systems, gathering context and generating responses that Nicole delivers — all without human intervention.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Liberate’s AI agents are built to complete end-to-end tasks — not just respond to queries or escalate tickets. These include quoting policies, processing claims, and updating endorsements, among other routine functions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The agents can also operate over SMS and email, allowing insurers to interact with customers across different channels while automating more of their day-to-day workflows.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Insurance companies want to grow, but they’re not able to do so,” Liberate co-founder and CEO Amrish Singh (pictured above, center) said in an interview. “It’s the status quo where the opportunity is.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Singh co-founded Liberate after nearly four years at Metromile, the car insurance firm owned by Lemonade, where he worked across both back-office operations and technology. He teamed up with Ryan Eldridge, Liberate’s VP of engineering and also a former Metromile executive, and Jason St. Pierre, the company’s CPO, who previously held roles at Twitter, Google, and Verily, Alphabet’s life sciences arm.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Liberate’s AI systems have helped increase sales by an average of 15% and cut costs by 23%, Singh told TechCrunch, adding that the startup now has over 60 customers and focuses on the top 100 carriers and agencies, which together represent 70% to 80% of the U.S. property and casualty insurance market.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-3057757" height="1200" src="https://techcrunch.com/wp-content/uploads/2025/10/liberate-agent-orchestration-screenshot.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Liberate’s agent orchestration&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Liberate&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The technology uses reinforcement learning tailored for long, regulated insurance conversations. Each interaction is auditable and includes human-in-the-loop safeguards to meet compliance requirements, the startup said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Over the past year, Liberate has scaled from 10,000 monthly automations to 1.3 million automated resolutions, Singh stated. These include direct customer interactions via its voice AI, as well as back-office tasks handled by AI agents integrated into carriers’ core systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since AI systems can still make mistakes and are not foolproof &lt;em&gt;yet&lt;/em&gt;, Liberate uses an internal tool called Supervisor to monitor all interactions between its agents and customers. The software flags issues or anomalies and escalates to a human when the AI’s response may be off-track, Singh said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The advantage of servicing only one industry, and within that servicing only three specific use cases, is that you can put a lot more guardrails in place,” the executive noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Without disclosing the names of its clients, Liberate said that using its agents, hurricane claim response time dropped from 30 hours to 30 seconds.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI agents enable 24/7 sales operations, allowing customers to buy insurance even at midnight or early in the morning — times when human agents typically are not available, Singh said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Before this round, Liberate raised a $15 million Series A last year. Its voice AI-powered omnichannel experience and ability to fully automate tasks by integrating into existing systems were key factors that drew investors to back the company at a larger scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Mapping the process, modeling it, and making sure that all the systems connections are in place, well tested, and appropriately designed so that you can complete the task, not just communicate, is what Liberate is doing,” Marcus Ryu, a general partner at Battery Ventures, told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ryu, who previously worked with property and casualty insurers at Guidewire Software, focuses on enterprise software, fintech, and insurtech investments at Battery Ventures. He is joining Liberate’s board.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Series B funding will be used to expand Liberate’s reasoning capabilities and support broader deployment across insurers. The startup has raised $72 million to date and currently employs around 50 people.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Liberate, an AI startup automating insurance operations, has raised $50 million in a round led by Battery Ventures as it looks to scale its agentic deployments across carriers and agencies globally.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The all-equity round values the three-year-old startup at $300 million post-money, with participation from new investor Canapi Ventures and returning backers Redpoint Ventures, Eclipse, and Commerce Ventures.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The insurance industry has been navigating a difficult stretch, with rising operational costs, legacy system constraints, and increasing customer expectations. Specifically in the non-life segment, global premium growth is projected to slow through 2026, driven by heightened competition, weaker rate momentum, and new cost pressures, including tariffs, per a recent report by Deloitte. While some carriers experimented with AI, many early efforts stalled due to fragmented data and inflexible workflows. That is now changing, as insurers shift toward full-scale AI adoption — embedding it into the core of their operations rather than layering it on top. Liberate is stepping in to meet this shift head-on.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Founded in 2022, the San Francisco-based startup builds AI systems for property and casualty insurers, focusing on sales, service, and claims. At the front end, its voice AI assistant, Nicole, handles inbound and outbound calls to help sell policies or respond to service requests. Behind the scenes, a network of reasoning-based AI agents connects to insurers’ existing systems, gathering context and generating responses that Nicole delivers — all without human intervention.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Liberate’s AI agents are built to complete end-to-end tasks — not just respond to queries or escalate tickets. These include quoting policies, processing claims, and updating endorsements, among other routine functions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The agents can also operate over SMS and email, allowing insurers to interact with customers across different channels while automating more of their day-to-day workflows.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Insurance companies want to grow, but they’re not able to do so,” Liberate co-founder and CEO Amrish Singh (pictured above, center) said in an interview. “It’s the status quo where the opportunity is.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Singh co-founded Liberate after nearly four years at Metromile, the car insurance firm owned by Lemonade, where he worked across both back-office operations and technology. He teamed up with Ryan Eldridge, Liberate’s VP of engineering and also a former Metromile executive, and Jason St. Pierre, the company’s CPO, who previously held roles at Twitter, Google, and Verily, Alphabet’s life sciences arm.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Liberate’s AI systems have helped increase sales by an average of 15% and cut costs by 23%, Singh told TechCrunch, adding that the startup now has over 60 customers and focuses on the top 100 carriers and agencies, which together represent 70% to 80% of the U.S. property and casualty insurance market.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-3057757" height="1200" src="https://techcrunch.com/wp-content/uploads/2025/10/liberate-agent-orchestration-screenshot.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Liberate’s agent orchestration&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Liberate&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The technology uses reinforcement learning tailored for long, regulated insurance conversations. Each interaction is auditable and includes human-in-the-loop safeguards to meet compliance requirements, the startup said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Over the past year, Liberate has scaled from 10,000 monthly automations to 1.3 million automated resolutions, Singh stated. These include direct customer interactions via its voice AI, as well as back-office tasks handled by AI agents integrated into carriers’ core systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since AI systems can still make mistakes and are not foolproof &lt;em&gt;yet&lt;/em&gt;, Liberate uses an internal tool called Supervisor to monitor all interactions between its agents and customers. The software flags issues or anomalies and escalates to a human when the AI’s response may be off-track, Singh said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The advantage of servicing only one industry, and within that servicing only three specific use cases, is that you can put a lot more guardrails in place,” the executive noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Without disclosing the names of its clients, Liberate said that using its agents, hurricane claim response time dropped from 30 hours to 30 seconds.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI agents enable 24/7 sales operations, allowing customers to buy insurance even at midnight or early in the morning — times when human agents typically are not available, Singh said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Before this round, Liberate raised a $15 million Series A last year. Its voice AI-powered omnichannel experience and ability to fully automate tasks by integrating into existing systems were key factors that drew investors to back the company at a larger scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Mapping the process, modeling it, and making sure that all the systems connections are in place, well tested, and appropriately designed so that you can complete the task, not just communicate, is what Liberate is doing,” Marcus Ryu, a general partner at Battery Ventures, told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ryu, who previously worked with property and casualty insurers at Guidewire Software, focuses on enterprise software, fintech, and insurtech investments at Battery Ventures. He is joining Liberate’s board.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Series B funding will be used to expand Liberate’s reasoning capabilities and support broader deployment across insurers. The startup has raised $72 million to date and currently employs around 50 people.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/15/liberate-bags-50m-at-300m-valuation-to-bring-ai-deeper-into-insurance-back-offices/</guid><pubDate>Wed, 15 Oct 2025 17:00:00 +0000</pubDate></item><item><title>Eightfold co-founders raise $35M for Viven, an AI digital twin startup for querying unavailable co-workers (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/15/eightfold-co-founders-raise-35m-for-viven-an-ai-digital-twin-startup-for-querying-unavailable-coworkers/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Viven.jpg.jpeg?resize=1200,801" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;While employees spend much of their day communicating and coordinating amongst themselves on projects, this effort is often undermined by the availability of specific individuals. When a colleague with vital information is away — whether on vacation or in a different time zone — the rest of the team must delay progress until that person responds.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ashutosh Garg and Varun Kacholia, the co-founders of Eightfold — an AI recruiting startup last valued at $2.1 billion — believe that advances in LLMs and data privacy technologies can help solve some aspects of this costly problem. Earlier this year, they launched Viven, a digital twin startup with a mission to grant employees access to crucial information from teammates even when those colleagues are unavailable.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;On Wednesday, Viven emerged out of stealth mode with $35 million in seed funding from Khosla Ventures, Foundation Capital, FPV Ventures, and others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Viven develops a specialized LLM for each employee, effectively creating a digital twin by accessing their internal electronic documents such as email, Slack, and Google Docs. Other employees in the organization can then query that person’s digital twin to get immediate answers related to common projects and shared knowledge.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“When each and every person has a digital twin, you can just talk to their twin as if you’re talking to that person and get the response,” Ashutosh Garg told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One major hurdle is that people just can’t share everything with anyone who asks. Employees often handle sensitive  information or have personal files they want to keep private from the rest of the team.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Ashutosh Garg, Viven’s technology solves that complex problem through a concept known as pairwise context and privacy. This enables the startup’s LLMs to precisely determine what information can be shared and with whom across the organization.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Viven’s LLMs are smart enough to recognize personal context and know what information needs to stay private — like questions related to an employee’s personal life. But perhaps the most important safeguard is that everyone can see the query history of their digital twin, which acts as a deterrent against people asking inappropriate questions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s a very hard problem to solve, and until recently, it was unsolvable,” Ashu Garg, a general partner at Foundation Capital told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Viven is already in use by several enterprise clients, including Genpact and Eightfold. (Co-founders Ashutosh Garg and Varun Kacholia continue to lead Eightfold, splitting their time between that company and running Viven.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As for competition, Ashutosh Garg claims that no other company is tackling digital twins for the enterprise yet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He wasn’t sure that there were no competitors when he first started thinking about the idea. So he called Vinod Khosla to ask about it. The legendary investor assured Ashutosh Garg that nobody is doing this and agreed to invest.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ashu Garg of Foundation Capital was equally excited about Viven.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“When Ashutosh came to me and described the product, the big aha for me was: there’s this horizontal problem across all jobs of coordination and communication, which no one is automating,” Ashu Garg told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But just because there are no direct competitors now doesn’t mean that other companies won’t build digital twins for companies in the future. Ashu Garg said that Anthropic, Google’s Gemini, Microsoft Copilot, and OpenAI’s enterprise search products have a personalization component. But, if they do enter this market, Viven hopes its “pairwise” context technology will be its moat.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Viven.jpg.jpeg?resize=1200,801" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;While employees spend much of their day communicating and coordinating amongst themselves on projects, this effort is often undermined by the availability of specific individuals. When a colleague with vital information is away — whether on vacation or in a different time zone — the rest of the team must delay progress until that person responds.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ashutosh Garg and Varun Kacholia, the co-founders of Eightfold — an AI recruiting startup last valued at $2.1 billion — believe that advances in LLMs and data privacy technologies can help solve some aspects of this costly problem. Earlier this year, they launched Viven, a digital twin startup with a mission to grant employees access to crucial information from teammates even when those colleagues are unavailable.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;On Wednesday, Viven emerged out of stealth mode with $35 million in seed funding from Khosla Ventures, Foundation Capital, FPV Ventures, and others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Viven develops a specialized LLM for each employee, effectively creating a digital twin by accessing their internal electronic documents such as email, Slack, and Google Docs. Other employees in the organization can then query that person’s digital twin to get immediate answers related to common projects and shared knowledge.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“When each and every person has a digital twin, you can just talk to their twin as if you’re talking to that person and get the response,” Ashutosh Garg told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One major hurdle is that people just can’t share everything with anyone who asks. Employees often handle sensitive  information or have personal files they want to keep private from the rest of the team.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Ashutosh Garg, Viven’s technology solves that complex problem through a concept known as pairwise context and privacy. This enables the startup’s LLMs to precisely determine what information can be shared and with whom across the organization.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Viven’s LLMs are smart enough to recognize personal context and know what information needs to stay private — like questions related to an employee’s personal life. But perhaps the most important safeguard is that everyone can see the query history of their digital twin, which acts as a deterrent against people asking inappropriate questions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s a very hard problem to solve, and until recently, it was unsolvable,” Ashu Garg, a general partner at Foundation Capital told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Viven is already in use by several enterprise clients, including Genpact and Eightfold. (Co-founders Ashutosh Garg and Varun Kacholia continue to lead Eightfold, splitting their time between that company and running Viven.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As for competition, Ashutosh Garg claims that no other company is tackling digital twins for the enterprise yet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He wasn’t sure that there were no competitors when he first started thinking about the idea. So he called Vinod Khosla to ask about it. The legendary investor assured Ashutosh Garg that nobody is doing this and agreed to invest.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ashu Garg of Foundation Capital was equally excited about Viven.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“When Ashutosh came to me and described the product, the big aha for me was: there’s this horizontal problem across all jobs of coordination and communication, which no one is automating,” Ashu Garg told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But just because there are no direct competitors now doesn’t mean that other companies won’t build digital twins for companies in the future. Ashu Garg said that Anthropic, Google’s Gemini, Microsoft Copilot, and OpenAI’s enterprise search products have a personalization component. But, if they do enter this market, Viven hopes its “pairwise” context technology will be its moat.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/15/eightfold-co-founders-raise-35m-for-viven-an-ai-digital-twin-startup-for-querying-unavailable-coworkers/</guid><pubDate>Wed, 15 Oct 2025 17:15:00 +0000</pubDate></item><item><title>Blending neuroscience, AI, and music to create mental health innovations (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/kimaya-lecamwasam-blending-neuroscience-ai-music-to-create-mental-health-innovations-1015</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202509/mit-Kimaya-Lecamwasam.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Computational neuroscientist and singer/songwriter Kimaya (Kimy) Lecamwasam, who also plays electric bass and guitar, says music has been a core part of her life for as long as she can remember. She grew up in a musical family and played in bands all through high school.&lt;/p&gt;&lt;p&gt;“For most of my life, writing and playing music was the clearest way I had to express myself,” says Lecamwasam. “I was a really shy and anxious kid, and I struggled with speaking up for myself. Over time, composing and performing music became central to both how I communicated and to how I managed my own mental health.”&lt;/p&gt;&lt;p&gt;Along with equipping her with valuable skills and experiences, she credits her passion for music as the catalyst for her interest in neuroscience.&lt;/p&gt;&lt;p&gt;“I got to see firsthand not only the ways that audiences reacted to music, but also how much value music had for musicians,” she says. “That close connection between making music and feeling well is what first pushed me to ask why music has such a powerful hold on us, and eventually led me to study the science behind it.”&lt;/p&gt;&lt;p&gt;Lecamwasam earned a bachelor’s degree in 2021 from Wellesley College, where she studied neuroscience — specifically in the Systems and Computational Neuroscience track — and also music. During her first semester, she took a class in songwriting that she says made her more aware of the connections between music and emotions. While studying at Wellesley, she participated in the MIT Undergraduate Research Opportunities Program for three years. Working in the Department of Brain and Cognitive Sciences lab of Emery Brown, the Edward Hood Taplin Professor of Medical Engineering and Computational Neuroscience, she focused primarily on classifying consciousness in anesthetized patients and training brain-computer interface-enabled prosthetics using reinforcement learning.&lt;/p&gt;&lt;p&gt;“I still had a really deep love for music, which I was pursuing in parallel to all of my neuroscience work, but I really wanted to try to find a way to combine both of those things in grad school,” says Lecamwasam. Brown recommended that she look into the graduate programs at the MIT Media Lab within the Program in Media Arts and Sciences (MAS), which turned out to be an ideal fit.&lt;/p&gt;&lt;p&gt;“One thing I really love about where I am is that I get to be both an artist and a scientist,” says Lecamwasam. “That was something that was important to me when I was picking a graduate program. I wanted to make sure that I was going to be able to do work that was really rigorous, validated, and important, but also get to do cool, creative explorations and actually put the research that I was doing into practice in different ways.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Exploring the physical, mental, and emotional impacts of music&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Informed by her years of neuroscience research as an undergraduate and her passion for music, Lecamwasam focused her graduate research on harnessing the emotional potency of music into scalable, non-pharmacological mental health tools. Her master’s thesis focused on “pharmamusicology,” looking at how music might positively affect the physiology and psychology of those with anxiety.&lt;/p&gt;&lt;p&gt;The overarching theme of Lecamwasam’s research is exploring the various impacts of&amp;nbsp;music and affective computing — physically, mentally, and emotionally.&amp;nbsp;Now in the third year of her doctoral program in the&amp;nbsp;Opera of the Future group, she is currently investigating the impact of large-scale live music and concert experiences on the mental health and well-being of both audience members and performers. She is also working to clinically validate music listening, composition, and performance as health interventions, in combination with psychotherapy and pharmaceutical interventions.&lt;/p&gt;&lt;p&gt;Her recent work, in collaboration with Professor Anna Huang’s Human-AI Resonance Lab, assesses the emotional resonance of AI-generated music compared to human-composed music;&amp;nbsp;the aim is to identify more ethical applications of emotion-sensitive music generation and recommendation that preserve human creativity and agency, and can also be used as health interventions. She has co-led a wellness and music workshop at the Wellbeing Summit in Bilbao, Spain, and has presented her work at the 2023 CHI conference on Human Factors in Computing Systems in Hamburg, Germany and the 2024 Audio Mostly conference in Milan, Italy.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lecamwasam&amp;nbsp;has collaborated with organizations near and far to implement real-world applications of her research. She worked with Carnegie Hall's Weill Music Institute on its Well-Being Concerts and is currently partnering on a study assessing the impact of lullaby writing on perinatal health with the North Shore Lullaby Project in Massachusetts, an offshoot of Carnegie Hall’s Lullaby Project. Her main international collaboration is with a company called Myndstream, working on projects comparing the emotional resonance of AI-generated music to human-composed music and thinking of clinical and real-world applications. She is also working on a project with the companies PixMob and Empatica (an MIT Media Lab spinoff), centered on assessing the impact of interactive lighting and large-scale live music experiences on emotional resonance in stadium and arena settings.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Building community&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;“Kimy combines a deep love for — and sophisticated knowledge of — music with scientific curiosity and rigor in ways that represent the Media Lab/MAS spirit at its best,” says Professor Tod Machover, Lecamwasam’s research advisor, Media Lab faculty director, and director of the Opera of the Future group. “She has long believed that music is one of the most powerful and effective ways to create personalized interventions to help stabilize emotional distress and promote empathy and connection. It is this same desire to establish sane, safe, and sustaining environments for work and play that has led Kimy to become one of the most effective and devoted community-builders at the lab.”&lt;/p&gt;&lt;p&gt;Lecamwasam has participated in the SOS (Students Offering Support) program in MAS for a few years, which assists students from a variety of life experiences and backgrounds during the process of applying to the Program in Media Arts and Sciences. She will soon be the first MAS peer mentor as part of a new initiative through which she will establish and coordinate programs including a “buddy system,” pairing incoming master’s students with PhD students as a way to help them transition into graduate student life at MIT. She is also part of the Media Lab’s Studcom, a student-run organization that promotes, facilitates, and creates experiences meant to bring the community together.&lt;/p&gt;&lt;p&gt;“I think everything that I have gotten to do has been so supported by the friends I’ve made in my lab and department, as well as across departments,” says Lecamwasam. “I think everyone is just really excited about the work that they do and so supportive of one another. It makes it so that even when things are challenging or difficult, I’m motivated to do this work and be a part of this community.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202509/mit-Kimaya-Lecamwasam.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Computational neuroscientist and singer/songwriter Kimaya (Kimy) Lecamwasam, who also plays electric bass and guitar, says music has been a core part of her life for as long as she can remember. She grew up in a musical family and played in bands all through high school.&lt;/p&gt;&lt;p&gt;“For most of my life, writing and playing music was the clearest way I had to express myself,” says Lecamwasam. “I was a really shy and anxious kid, and I struggled with speaking up for myself. Over time, composing and performing music became central to both how I communicated and to how I managed my own mental health.”&lt;/p&gt;&lt;p&gt;Along with equipping her with valuable skills and experiences, she credits her passion for music as the catalyst for her interest in neuroscience.&lt;/p&gt;&lt;p&gt;“I got to see firsthand not only the ways that audiences reacted to music, but also how much value music had for musicians,” she says. “That close connection between making music and feeling well is what first pushed me to ask why music has such a powerful hold on us, and eventually led me to study the science behind it.”&lt;/p&gt;&lt;p&gt;Lecamwasam earned a bachelor’s degree in 2021 from Wellesley College, where she studied neuroscience — specifically in the Systems and Computational Neuroscience track — and also music. During her first semester, she took a class in songwriting that she says made her more aware of the connections between music and emotions. While studying at Wellesley, she participated in the MIT Undergraduate Research Opportunities Program for three years. Working in the Department of Brain and Cognitive Sciences lab of Emery Brown, the Edward Hood Taplin Professor of Medical Engineering and Computational Neuroscience, she focused primarily on classifying consciousness in anesthetized patients and training brain-computer interface-enabled prosthetics using reinforcement learning.&lt;/p&gt;&lt;p&gt;“I still had a really deep love for music, which I was pursuing in parallel to all of my neuroscience work, but I really wanted to try to find a way to combine both of those things in grad school,” says Lecamwasam. Brown recommended that she look into the graduate programs at the MIT Media Lab within the Program in Media Arts and Sciences (MAS), which turned out to be an ideal fit.&lt;/p&gt;&lt;p&gt;“One thing I really love about where I am is that I get to be both an artist and a scientist,” says Lecamwasam. “That was something that was important to me when I was picking a graduate program. I wanted to make sure that I was going to be able to do work that was really rigorous, validated, and important, but also get to do cool, creative explorations and actually put the research that I was doing into practice in different ways.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Exploring the physical, mental, and emotional impacts of music&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Informed by her years of neuroscience research as an undergraduate and her passion for music, Lecamwasam focused her graduate research on harnessing the emotional potency of music into scalable, non-pharmacological mental health tools. Her master’s thesis focused on “pharmamusicology,” looking at how music might positively affect the physiology and psychology of those with anxiety.&lt;/p&gt;&lt;p&gt;The overarching theme of Lecamwasam’s research is exploring the various impacts of&amp;nbsp;music and affective computing — physically, mentally, and emotionally.&amp;nbsp;Now in the third year of her doctoral program in the&amp;nbsp;Opera of the Future group, she is currently investigating the impact of large-scale live music and concert experiences on the mental health and well-being of both audience members and performers. She is also working to clinically validate music listening, composition, and performance as health interventions, in combination with psychotherapy and pharmaceutical interventions.&lt;/p&gt;&lt;p&gt;Her recent work, in collaboration with Professor Anna Huang’s Human-AI Resonance Lab, assesses the emotional resonance of AI-generated music compared to human-composed music;&amp;nbsp;the aim is to identify more ethical applications of emotion-sensitive music generation and recommendation that preserve human creativity and agency, and can also be used as health interventions. She has co-led a wellness and music workshop at the Wellbeing Summit in Bilbao, Spain, and has presented her work at the 2023 CHI conference on Human Factors in Computing Systems in Hamburg, Germany and the 2024 Audio Mostly conference in Milan, Italy.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lecamwasam&amp;nbsp;has collaborated with organizations near and far to implement real-world applications of her research. She worked with Carnegie Hall's Weill Music Institute on its Well-Being Concerts and is currently partnering on a study assessing the impact of lullaby writing on perinatal health with the North Shore Lullaby Project in Massachusetts, an offshoot of Carnegie Hall’s Lullaby Project. Her main international collaboration is with a company called Myndstream, working on projects comparing the emotional resonance of AI-generated music to human-composed music and thinking of clinical and real-world applications. She is also working on a project with the companies PixMob and Empatica (an MIT Media Lab spinoff), centered on assessing the impact of interactive lighting and large-scale live music experiences on emotional resonance in stadium and arena settings.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Building community&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;“Kimy combines a deep love for — and sophisticated knowledge of — music with scientific curiosity and rigor in ways that represent the Media Lab/MAS spirit at its best,” says Professor Tod Machover, Lecamwasam’s research advisor, Media Lab faculty director, and director of the Opera of the Future group. “She has long believed that music is one of the most powerful and effective ways to create personalized interventions to help stabilize emotional distress and promote empathy and connection. It is this same desire to establish sane, safe, and sustaining environments for work and play that has led Kimy to become one of the most effective and devoted community-builders at the lab.”&lt;/p&gt;&lt;p&gt;Lecamwasam has participated in the SOS (Students Offering Support) program in MAS for a few years, which assists students from a variety of life experiences and backgrounds during the process of applying to the Program in Media Arts and Sciences. She will soon be the first MAS peer mentor as part of a new initiative through which she will establish and coordinate programs including a “buddy system,” pairing incoming master’s students with PhD students as a way to help them transition into graduate student life at MIT. She is also part of the Media Lab’s Studcom, a student-run organization that promotes, facilitates, and creates experiences meant to bring the community together.&lt;/p&gt;&lt;p&gt;“I think everything that I have gotten to do has been so supported by the friends I’ve made in my lab and department, as well as across departments,” says Lecamwasam. “I think everyone is just really excited about the work that they do and so supportive of one another. It makes it so that even when things are challenging or difficult, I’m motivated to do this work and be a part of this community.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/kimaya-lecamwasam-blending-neuroscience-ai-music-to-create-mental-health-innovations-1015</guid><pubDate>Wed, 15 Oct 2025 17:20:00 +0000</pubDate></item><item><title>Remembering Professor Emerita Jeanne Shapiro  Bamberger, a pioneer in music education (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/remembering-professor-emerita-jeanne-shapiro-bamberger-1015</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202509/mit-jeanne-shapiro-bamberger-obit.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;MIT Music and Theater Arts fondly remembers the legacy of Professor Emerita Jeanne Shapiro&amp;nbsp;Bamberger, who passed away peacefully at home in Berkeley, California, of natural causes on Dec. 12, 2024 at the age of 100.&amp;nbsp;&lt;/p&gt;&lt;p&gt;For three decades at the Institute, Bamberger found ways to use computers to engage students and help them learn music. A trained pianist who became fascinated with the idea of using technology to gain insights into music education, Bamberger ultimately helped to change how music was taught at MIT and elsewhere.&lt;/p&gt;&lt;p&gt;Bamberger was born on Feb. 11, 1924 in Minneapolis, Minnesota. Her mother, Gertrude Shapiro (nee Kulberg), from a Romanian Jewish family, studied child psychology and was active in the League of Women Voters. Her father, Morse Shapiro, of Lithuanian and Polish Jewish heritage, was a groundbreaking pediatric cardiologist.&lt;/p&gt;&lt;p&gt;In 1969, Bamberger began her 32-year career at MIT, initially in the former MIT Education Department. While at MIT, Bamberger became the first woman to earn tenure in the Music and Theater Arts Section. She was know for pioneering the use of computer languages to teach children to learn music. She also used her computer innovations to study how children — and by extension, all humans — learn music, and this vector in particular became her life's work.&lt;/p&gt;&lt;p&gt;Ahead of her time, Bamberger worked in the MIT Artificial Intelligence Lab in the 1980s and developed computer languages (MusicLogo and Impromptu) while at the MIT Division for Study and Research in Education from 1975 to 1995. She became associate professor in music and theater arts in 1981, earned tenure soon thereafter, and chaired the department in 1989-90. During this period, she continued to perform as a concert pianist, taking part in concerts with the MIT Symphony Orchestra, and actively playing chamber music both at MIT and in the community. She also taught at the Harvard University Department of Education.&lt;/p&gt;&lt;p&gt;Institute Professor Marcus Thompson recollects, “During her time with us as a senior professor she was clearly a jewel in the crown. For someone who had studied piano with an historic legend in Artur Schnabel, who had studied with and known at least one of the French Six, Darius Milhaud, and worked with French composer and conductor Pierre Boulez, she was among that group of our professors who continually advocated for a new music building, considered the possibility of a graduate program in music at a time when we were being pushed to grow, at a time when she was our only senior woman when the need to do better was finally seen.” Both the dedicated music building and the graduate music program are now a reality.&lt;/p&gt;&lt;p&gt;Bamberger loved her work and was beloved and admired by her students and colleagues. Kenan Sahin Distinguished Professor Evan Ziporyn shares that she “was very much a shaping presence for our section — MIT Music and Theater Arts wouldn't be what we are today without her contributions. She’s also just a very cool person — I mean, how many 90-year-old academics end up working with Herbie Hancock and taking their research to the White House?”&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ziporyn adds that “among 7 million other singular accomplishments,” Bamberger published numerous articles and books&amp;nbsp;including&amp;nbsp;“The Art of Listening”&lt;em&gt;&amp;nbsp;&lt;/em&gt;with Howard Brofsky,&amp;nbsp;“The Mind Behind the Musical Ear,”&amp;nbsp;“Developing&amp;nbsp;Musical Intuitions,” and&amp;nbsp;“Discovering the Musical&amp;nbsp;Mind.”&lt;/p&gt;&lt;p&gt;While at MIT, Bamberger took many students under her wing and assisted many more with their academic careers. Elaine Chew SM ’98, PhD ’00, an operations researcher, pianist, current professor of engineering at King’s College London, and mentee of Bamberger, says, “I would not be doing what I am today if not for Jeanne. A child prodigy turned music philosopher, Jeanne was a pioneer in music and AI long before it was fashionable. She was deeply interested in people and passionate about how we learn. I will not forget the day when I came to her with complaints about things not working. Rather than telling me what to do, Jeanne said, ‘What are you going to do about it?’ prompting me to reflect on and develop my own sense of agency.” (Chew speaks more on Bamberger’s inspirational role in a 2016 interview.)&lt;/p&gt;&lt;p&gt;All told, Bamberger had a creative, fertile mind and loved to ask probing questions, a quality she passed to her progeny and community — it was her excitement and her passion.&lt;/p&gt;&lt;p&gt;While a professor at MIT, Bamberger was a force to be reckoned with. In addition to her long and productive academic career — in which she published four books and nearly 20 book chapters — she was politically active and supported the anti-Vietnam war and the civil rights movements. She continued teaching and publishing&amp;nbsp;her work well into her 90s and had a strong community of companions and colleagues to the end.&amp;nbsp;&lt;/p&gt;&lt;p&gt;In 2002, Bamberger became professor emerita at MIT and moved to Berkeley, California, continuing to teach in the Music Department at the University of California at Berkeley.&lt;/p&gt;&lt;p&gt;At 100, she was predeceased by her former husband, Frank K. Bamberger. She is survived by her two sons, Joshua and Paul (Chip); four grandchildren — Jerehme, Kaela, Eli, and Noah; and many caring relatives and friends.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202509/mit-jeanne-shapiro-bamberger-obit.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;MIT Music and Theater Arts fondly remembers the legacy of Professor Emerita Jeanne Shapiro&amp;nbsp;Bamberger, who passed away peacefully at home in Berkeley, California, of natural causes on Dec. 12, 2024 at the age of 100.&amp;nbsp;&lt;/p&gt;&lt;p&gt;For three decades at the Institute, Bamberger found ways to use computers to engage students and help them learn music. A trained pianist who became fascinated with the idea of using technology to gain insights into music education, Bamberger ultimately helped to change how music was taught at MIT and elsewhere.&lt;/p&gt;&lt;p&gt;Bamberger was born on Feb. 11, 1924 in Minneapolis, Minnesota. Her mother, Gertrude Shapiro (nee Kulberg), from a Romanian Jewish family, studied child psychology and was active in the League of Women Voters. Her father, Morse Shapiro, of Lithuanian and Polish Jewish heritage, was a groundbreaking pediatric cardiologist.&lt;/p&gt;&lt;p&gt;In 1969, Bamberger began her 32-year career at MIT, initially in the former MIT Education Department. While at MIT, Bamberger became the first woman to earn tenure in the Music and Theater Arts Section. She was know for pioneering the use of computer languages to teach children to learn music. She also used her computer innovations to study how children — and by extension, all humans — learn music, and this vector in particular became her life's work.&lt;/p&gt;&lt;p&gt;Ahead of her time, Bamberger worked in the MIT Artificial Intelligence Lab in the 1980s and developed computer languages (MusicLogo and Impromptu) while at the MIT Division for Study and Research in Education from 1975 to 1995. She became associate professor in music and theater arts in 1981, earned tenure soon thereafter, and chaired the department in 1989-90. During this period, she continued to perform as a concert pianist, taking part in concerts with the MIT Symphony Orchestra, and actively playing chamber music both at MIT and in the community. She also taught at the Harvard University Department of Education.&lt;/p&gt;&lt;p&gt;Institute Professor Marcus Thompson recollects, “During her time with us as a senior professor she was clearly a jewel in the crown. For someone who had studied piano with an historic legend in Artur Schnabel, who had studied with and known at least one of the French Six, Darius Milhaud, and worked with French composer and conductor Pierre Boulez, she was among that group of our professors who continually advocated for a new music building, considered the possibility of a graduate program in music at a time when we were being pushed to grow, at a time when she was our only senior woman when the need to do better was finally seen.” Both the dedicated music building and the graduate music program are now a reality.&lt;/p&gt;&lt;p&gt;Bamberger loved her work and was beloved and admired by her students and colleagues. Kenan Sahin Distinguished Professor Evan Ziporyn shares that she “was very much a shaping presence for our section — MIT Music and Theater Arts wouldn't be what we are today without her contributions. She’s also just a very cool person — I mean, how many 90-year-old academics end up working with Herbie Hancock and taking their research to the White House?”&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ziporyn adds that “among 7 million other singular accomplishments,” Bamberger published numerous articles and books&amp;nbsp;including&amp;nbsp;“The Art of Listening”&lt;em&gt;&amp;nbsp;&lt;/em&gt;with Howard Brofsky,&amp;nbsp;“The Mind Behind the Musical Ear,”&amp;nbsp;“Developing&amp;nbsp;Musical Intuitions,” and&amp;nbsp;“Discovering the Musical&amp;nbsp;Mind.”&lt;/p&gt;&lt;p&gt;While at MIT, Bamberger took many students under her wing and assisted many more with their academic careers. Elaine Chew SM ’98, PhD ’00, an operations researcher, pianist, current professor of engineering at King’s College London, and mentee of Bamberger, says, “I would not be doing what I am today if not for Jeanne. A child prodigy turned music philosopher, Jeanne was a pioneer in music and AI long before it was fashionable. She was deeply interested in people and passionate about how we learn. I will not forget the day when I came to her with complaints about things not working. Rather than telling me what to do, Jeanne said, ‘What are you going to do about it?’ prompting me to reflect on and develop my own sense of agency.” (Chew speaks more on Bamberger’s inspirational role in a 2016 interview.)&lt;/p&gt;&lt;p&gt;All told, Bamberger had a creative, fertile mind and loved to ask probing questions, a quality she passed to her progeny and community — it was her excitement and her passion.&lt;/p&gt;&lt;p&gt;While a professor at MIT, Bamberger was a force to be reckoned with. In addition to her long and productive academic career — in which she published four books and nearly 20 book chapters — she was politically active and supported the anti-Vietnam war and the civil rights movements. She continued teaching and publishing&amp;nbsp;her work well into her 90s and had a strong community of companions and colleagues to the end.&amp;nbsp;&lt;/p&gt;&lt;p&gt;In 2002, Bamberger became professor emerita at MIT and moved to Berkeley, California, continuing to teach in the Music Department at the University of California at Berkeley.&lt;/p&gt;&lt;p&gt;At 100, she was predeceased by her former husband, Frank K. Bamberger. She is survived by her two sons, Joshua and Paul (Chip); four grandchildren — Jerehme, Kaela, Eli, and Noah; and many caring relatives and friends.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/remembering-professor-emerita-jeanne-shapiro-bamberger-1015</guid><pubDate>Wed, 15 Oct 2025 17:25:00 +0000</pubDate></item><item><title>[NEW] Google releases new AI video model Veo 3.1 in Flow and API: what it means for enterprises (AI | VentureBeat)</title><link>https://venturebeat.com/ai/google-releases-new-ai-video-model-veo-3-1-in-flow-and-api-what-it-means-for</link><description>[unable to retrieve full-text content]&lt;p&gt;As expected after days of leaks and rumors online, Google has &lt;a href="https://blog.google/technology/ai/veo-updates-flow/"&gt;unveiled Veo 3.1&lt;/a&gt;, its latest AI video generation model, bringing a suite of creative and technical upgrades aimed at improving narrative control, audio integration, and realism in AI-generated video. &lt;/p&gt;&lt;p&gt;While the updates expand possibilities for hobbyists and content creators using Google’s online AI creation app, &lt;a href="https://labs.google/fx/tools/flow"&gt;Flow&lt;/a&gt;, the release also signals a growing opportunity for enterprises, developers, and creative teams seeking scalable, customizable video tools.&lt;/p&gt;&lt;p&gt;The quality is higher, the physics better, the pricing the same as before, and the control and editing features more robust and varied.&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;My &lt;a href="https://x.com/carlfranzen/status/1978522697014411322"&gt;initial tests&lt;/a&gt; showed it to be a powerful and performant model that immediately delights with each generation. However, the look is more cinematic, polished and a little more &amp;quot;artificial&amp;quot; than by default than rivals such as &lt;a href="https://venturebeat.com/ai/openai-debuts-sora-2-ai-video-generator-app-with-sound-and-self-insertion"&gt;OpenAI&amp;#x27;s new Sora 2&lt;/a&gt;, released late last month, which may or may not be what a particular user is going after (Sora excels at handheld and &amp;quot;candid&amp;quot; style videos). &lt;/p&gt;&lt;h3&gt;&lt;b&gt;Expanded Control Over Narrative and Audio&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Veo 3.1 builds on its predecessor, Veo 3 (&lt;a href="https://www.cnbc.com/2025/05/20/google-ai-video-generator-audio-veo-3.html"&gt;released back in May 2025&lt;/a&gt;) with enhanced support for dialogue, ambient sound, and other audio effects. &lt;/p&gt;&lt;p&gt;Native audio generation is now available across several key features in Flow, including “Frames to Video,” “Ingredients to Video,” and “Extend,&amp;quot; which give users the ability to, respectively: turn still images into video; use items, characters and objects from multiple images in a single video; and generate longer clips than the initial 8 seconds, to more than 30 seconds or even 1+ plus when continuing from a prior clip&amp;#x27;s final frame. &lt;/p&gt;&lt;p&gt;Before, you had to add audio manually after using these features. &lt;/p&gt;&lt;p&gt;This addition gives users greater command over tone, emotion, and storytelling — capabilities that have previously required post-production work.&lt;/p&gt;&lt;p&gt;In enterprise contexts, this level of control may reduce the need for separate audio pipelines, offering an integrated way to create training content, marketing videos, or digital experiences with synchronized sound and visuals.&lt;/p&gt;&lt;p&gt;Google noted in &lt;a href="https://blog.google/technology/ai/veo-updates-flow/"&gt;a blog post&lt;/a&gt; that the updates reflect user feedback calling for deeper artistic control and improved audio support. Gallegos emphasizes the importance of making edits and refinements possible directly in Flow, without reworking scenes from scratch.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Richer Inputs and Editing Capabilities&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;With Veo 3.1, Google introduces support for multiple input types and more granular control over generated outputs. The model accepts text prompts, images, and video clips as input, and also supports:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Reference images (up to three)&lt;/b&gt; to guide appearance and style in the final output&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;First and last frame interpolation&lt;/b&gt; to generate seamless scenes between fixed endpoints&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Scene extension&lt;/b&gt; that continues a video’s action or motion beyond its current duration&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;These tools aim to give enterprise users a way to fine-tune the look and feel of their content—useful for brand consistency or adherence to creative briefs.&lt;/p&gt;&lt;p&gt;Additional capabilities like “Insert” (add objects to scenes) and “Remove” (delete elements or characters) are also being introduced, though not all are immediately available through the Gemini API.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Deployment Across Platforms&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Veo 3.1 is accessible through several of Google’s existing AI services:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://labs.google/fx/tools/flow"&gt;&lt;b&gt;Flow&lt;/b&gt;&lt;/a&gt;, Google’s own interface for AI-assisted filmmaking&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/video?example=dialogue"&gt;&lt;b&gt;Gemini API&lt;/b&gt;&lt;/a&gt;, targeted at developers building video capabilities into applications&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/veo-video-generation"&gt;&lt;b&gt;Vertex AI&lt;/b&gt;&lt;/a&gt;, where enterprise integration will soon support Veo’s “Scene Extension” and other key features&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Availability through these platforms allows enterprise customers to choose the right environment—GUI-based or programmatic—based on their teams and workflows.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Pricing and Access&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The Veo 3.1 model is currently in &lt;b&gt;preview&lt;/b&gt; and available only on the &lt;b&gt;paid tier&lt;/b&gt; of the Gemini API. The cost structure is the same as Veo 3, the preceding generation of AI video models from Google.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Standard model&lt;/b&gt;: $0.40 per second of video&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Fast model&lt;/b&gt;: $0.15 per second&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;There is no free tier, and users are charged only if a video is successfully generated. This model is consistent with previous Veo versions and provides predictable pricing for budget-conscious enterprise teams.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Technical Specs and Output Control&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Veo 3.1 outputs video at &lt;b&gt;720p or 1080p resolution&lt;/b&gt;, with a &lt;b&gt;24 fps frame rate&lt;/b&gt;. &lt;/p&gt;&lt;p&gt;Duration options include &lt;b&gt;4, 6, or 8 seconds &lt;/b&gt;from a text prompt or uploaded images, with the ability to extend videos up to &lt;b&gt;148 seconds (more than 2 and half minutes!)&lt;/b&gt; when using the “Extend” feature.&lt;/p&gt;&lt;p&gt;New functionality also includes tighter control over subjects and environments. For example, enterprises can upload a product image or visual reference, and Veo 3.1 will generate scenes that preserve its appearance and stylistic cues across the video. This could streamline creative production pipelines for retail, advertising, and virtual content production teams.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Initial Reactions&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The broader creator and developer community has responded to Veo 3.1’s launch with a mix of optimism and tempered critique—particularly when comparing it to rival models like OpenAI’s Sora 2.&lt;/p&gt;&lt;p&gt;&lt;a href="https://x.com/mattshumer_/status/1978503288992461205"&gt;Matt Shumer,&lt;/a&gt; an AI founder of Otherside AI/Hyperwrite, and early adopter, described his initial reaction as “disappointment,” noting that Veo 3.1 is “noticeably worse than Sora 2” and also “quite a bit more expensive.”&lt;/p&gt;&lt;p&gt;However, he acknowledged that Google’s tooling—such as support for references and scene extension—is a bright spot in the release.&lt;/p&gt;&lt;p&gt;&lt;a href="https://x.com/MrDavids1/status/1978460666395505004"&gt;&lt;b&gt;Travis Davids&lt;/b&gt;&lt;/a&gt;, a 3D digital artist and AI content creator, echoed some of that sentiment. While he noted improvements in audio quality, particularly in sound effects and dialogue, he raised concerns about limitations that remain in the system. &lt;/p&gt;&lt;p&gt;These include the lack of custom voice support, an inability to select generated voices directly, and the continued cap at 8-second generations—despite some public claims about longer outputs.&lt;/p&gt;&lt;p&gt;Davids also pointed out that character consistency across changing camera angles still requires careful prompting, whereas other models like Sora 2 handle this more automatically. He questioned the absence of 1080p resolution for users on paid tiers like Flow Pro and expressed skepticism over feature parity.&lt;/p&gt;&lt;p&gt;On the more positive end, &lt;a href="https://x.com/kimmonismus"&gt;@kimmonismus,&lt;/a&gt; an AI newsletter writer, stated that “Veo 3.1 is amazing,” though still concluded that OpenAI’s latest model remains preferable overall.&lt;/p&gt;&lt;p&gt;Collectively, these early impressions suggest that while Veo 3.1 delivers meaningful tooling enhancements and new creative control features, expectations have shifted as competitors raise the bar on both quality and usability.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Adoption and Scale&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Since launching Flow five months ago, Google says over &lt;b&gt;275 million videos&lt;/b&gt; have been generated across various Veo models. &lt;/p&gt;&lt;p&gt;The pace of adoption suggests significant interest not only from individuals but also from developers and businesses experimenting with automated content creation.&lt;/p&gt;&lt;p&gt;Thomas Iljic, Director of Product Management at Google Labs, highlights that Veo 3.1’s release brings capabilities closer to how human filmmakers plan and shoot. These include scene composition, continuity across shots, and coordinated audio—all areas that enterprises increasingly look to automate or streamline.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Safety and Responsible AI Use&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Videos generated with Veo 3.1 are watermarked using Google’s &lt;b&gt;SynthID&lt;/b&gt; technology, which embeds an imperceptible identifier to signal that the content is AI-generated. &lt;/p&gt;&lt;p&gt;Google applies safety filters and moderation across its APIs to help minimize privacy and copyright risks. Generated content is stored temporarily and deleted after two days unless downloaded.&lt;/p&gt;&lt;p&gt;For developers and enterprises, these features provide reassurance around provenance and compliance—critical in regulated or brand-sensitive industries.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Where Veo 3.1 Stands Among a Crowded AI Video Model Space&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Veo 3.1 is not just an iteration on prior models—it represents a deeper integration of multimodal inputs, storytelling control, and enterprise-level tooling. While creative professionals may see immediate benefits in editing workflows and fidelity, businesses exploring automation in training, advertising, or virtual experiences may find even greater value in the model’s composability and API support.&lt;/p&gt;&lt;p&gt;The early user feedback highlights that while Veo 3.1 offers valuable tooling, expectations around realism, voice control, and generation length are evolving rapidly. As Google expands access through Vertex AI and continues refining Veo, its competitive positioning in enterprise video generation will hinge on how quickly these user pain points are addressed.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;As expected after days of leaks and rumors online, Google has &lt;a href="https://blog.google/technology/ai/veo-updates-flow/"&gt;unveiled Veo 3.1&lt;/a&gt;, its latest AI video generation model, bringing a suite of creative and technical upgrades aimed at improving narrative control, audio integration, and realism in AI-generated video. &lt;/p&gt;&lt;p&gt;While the updates expand possibilities for hobbyists and content creators using Google’s online AI creation app, &lt;a href="https://labs.google/fx/tools/flow"&gt;Flow&lt;/a&gt;, the release also signals a growing opportunity for enterprises, developers, and creative teams seeking scalable, customizable video tools.&lt;/p&gt;&lt;p&gt;The quality is higher, the physics better, the pricing the same as before, and the control and editing features more robust and varied.&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;My &lt;a href="https://x.com/carlfranzen/status/1978522697014411322"&gt;initial tests&lt;/a&gt; showed it to be a powerful and performant model that immediately delights with each generation. However, the look is more cinematic, polished and a little more &amp;quot;artificial&amp;quot; than by default than rivals such as &lt;a href="https://venturebeat.com/ai/openai-debuts-sora-2-ai-video-generator-app-with-sound-and-self-insertion"&gt;OpenAI&amp;#x27;s new Sora 2&lt;/a&gt;, released late last month, which may or may not be what a particular user is going after (Sora excels at handheld and &amp;quot;candid&amp;quot; style videos). &lt;/p&gt;&lt;h3&gt;&lt;b&gt;Expanded Control Over Narrative and Audio&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Veo 3.1 builds on its predecessor, Veo 3 (&lt;a href="https://www.cnbc.com/2025/05/20/google-ai-video-generator-audio-veo-3.html"&gt;released back in May 2025&lt;/a&gt;) with enhanced support for dialogue, ambient sound, and other audio effects. &lt;/p&gt;&lt;p&gt;Native audio generation is now available across several key features in Flow, including “Frames to Video,” “Ingredients to Video,” and “Extend,&amp;quot; which give users the ability to, respectively: turn still images into video; use items, characters and objects from multiple images in a single video; and generate longer clips than the initial 8 seconds, to more than 30 seconds or even 1+ plus when continuing from a prior clip&amp;#x27;s final frame. &lt;/p&gt;&lt;p&gt;Before, you had to add audio manually after using these features. &lt;/p&gt;&lt;p&gt;This addition gives users greater command over tone, emotion, and storytelling — capabilities that have previously required post-production work.&lt;/p&gt;&lt;p&gt;In enterprise contexts, this level of control may reduce the need for separate audio pipelines, offering an integrated way to create training content, marketing videos, or digital experiences with synchronized sound and visuals.&lt;/p&gt;&lt;p&gt;Google noted in &lt;a href="https://blog.google/technology/ai/veo-updates-flow/"&gt;a blog post&lt;/a&gt; that the updates reflect user feedback calling for deeper artistic control and improved audio support. Gallegos emphasizes the importance of making edits and refinements possible directly in Flow, without reworking scenes from scratch.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Richer Inputs and Editing Capabilities&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;With Veo 3.1, Google introduces support for multiple input types and more granular control over generated outputs. The model accepts text prompts, images, and video clips as input, and also supports:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Reference images (up to three)&lt;/b&gt; to guide appearance and style in the final output&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;First and last frame interpolation&lt;/b&gt; to generate seamless scenes between fixed endpoints&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Scene extension&lt;/b&gt; that continues a video’s action or motion beyond its current duration&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;These tools aim to give enterprise users a way to fine-tune the look and feel of their content—useful for brand consistency or adherence to creative briefs.&lt;/p&gt;&lt;p&gt;Additional capabilities like “Insert” (add objects to scenes) and “Remove” (delete elements or characters) are also being introduced, though not all are immediately available through the Gemini API.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Deployment Across Platforms&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Veo 3.1 is accessible through several of Google’s existing AI services:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://labs.google/fx/tools/flow"&gt;&lt;b&gt;Flow&lt;/b&gt;&lt;/a&gt;, Google’s own interface for AI-assisted filmmaking&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/video?example=dialogue"&gt;&lt;b&gt;Gemini API&lt;/b&gt;&lt;/a&gt;, targeted at developers building video capabilities into applications&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/veo-video-generation"&gt;&lt;b&gt;Vertex AI&lt;/b&gt;&lt;/a&gt;, where enterprise integration will soon support Veo’s “Scene Extension” and other key features&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Availability through these platforms allows enterprise customers to choose the right environment—GUI-based or programmatic—based on their teams and workflows.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Pricing and Access&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The Veo 3.1 model is currently in &lt;b&gt;preview&lt;/b&gt; and available only on the &lt;b&gt;paid tier&lt;/b&gt; of the Gemini API. The cost structure is the same as Veo 3, the preceding generation of AI video models from Google.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Standard model&lt;/b&gt;: $0.40 per second of video&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Fast model&lt;/b&gt;: $0.15 per second&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;There is no free tier, and users are charged only if a video is successfully generated. This model is consistent with previous Veo versions and provides predictable pricing for budget-conscious enterprise teams.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Technical Specs and Output Control&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Veo 3.1 outputs video at &lt;b&gt;720p or 1080p resolution&lt;/b&gt;, with a &lt;b&gt;24 fps frame rate&lt;/b&gt;. &lt;/p&gt;&lt;p&gt;Duration options include &lt;b&gt;4, 6, or 8 seconds &lt;/b&gt;from a text prompt or uploaded images, with the ability to extend videos up to &lt;b&gt;148 seconds (more than 2 and half minutes!)&lt;/b&gt; when using the “Extend” feature.&lt;/p&gt;&lt;p&gt;New functionality also includes tighter control over subjects and environments. For example, enterprises can upload a product image or visual reference, and Veo 3.1 will generate scenes that preserve its appearance and stylistic cues across the video. This could streamline creative production pipelines for retail, advertising, and virtual content production teams.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Initial Reactions&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The broader creator and developer community has responded to Veo 3.1’s launch with a mix of optimism and tempered critique—particularly when comparing it to rival models like OpenAI’s Sora 2.&lt;/p&gt;&lt;p&gt;&lt;a href="https://x.com/mattshumer_/status/1978503288992461205"&gt;Matt Shumer,&lt;/a&gt; an AI founder of Otherside AI/Hyperwrite, and early adopter, described his initial reaction as “disappointment,” noting that Veo 3.1 is “noticeably worse than Sora 2” and also “quite a bit more expensive.”&lt;/p&gt;&lt;p&gt;However, he acknowledged that Google’s tooling—such as support for references and scene extension—is a bright spot in the release.&lt;/p&gt;&lt;p&gt;&lt;a href="https://x.com/MrDavids1/status/1978460666395505004"&gt;&lt;b&gt;Travis Davids&lt;/b&gt;&lt;/a&gt;, a 3D digital artist and AI content creator, echoed some of that sentiment. While he noted improvements in audio quality, particularly in sound effects and dialogue, he raised concerns about limitations that remain in the system. &lt;/p&gt;&lt;p&gt;These include the lack of custom voice support, an inability to select generated voices directly, and the continued cap at 8-second generations—despite some public claims about longer outputs.&lt;/p&gt;&lt;p&gt;Davids also pointed out that character consistency across changing camera angles still requires careful prompting, whereas other models like Sora 2 handle this more automatically. He questioned the absence of 1080p resolution for users on paid tiers like Flow Pro and expressed skepticism over feature parity.&lt;/p&gt;&lt;p&gt;On the more positive end, &lt;a href="https://x.com/kimmonismus"&gt;@kimmonismus,&lt;/a&gt; an AI newsletter writer, stated that “Veo 3.1 is amazing,” though still concluded that OpenAI’s latest model remains preferable overall.&lt;/p&gt;&lt;p&gt;Collectively, these early impressions suggest that while Veo 3.1 delivers meaningful tooling enhancements and new creative control features, expectations have shifted as competitors raise the bar on both quality and usability.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Adoption and Scale&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Since launching Flow five months ago, Google says over &lt;b&gt;275 million videos&lt;/b&gt; have been generated across various Veo models. &lt;/p&gt;&lt;p&gt;The pace of adoption suggests significant interest not only from individuals but also from developers and businesses experimenting with automated content creation.&lt;/p&gt;&lt;p&gt;Thomas Iljic, Director of Product Management at Google Labs, highlights that Veo 3.1’s release brings capabilities closer to how human filmmakers plan and shoot. These include scene composition, continuity across shots, and coordinated audio—all areas that enterprises increasingly look to automate or streamline.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Safety and Responsible AI Use&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Videos generated with Veo 3.1 are watermarked using Google’s &lt;b&gt;SynthID&lt;/b&gt; technology, which embeds an imperceptible identifier to signal that the content is AI-generated. &lt;/p&gt;&lt;p&gt;Google applies safety filters and moderation across its APIs to help minimize privacy and copyright risks. Generated content is stored temporarily and deleted after two days unless downloaded.&lt;/p&gt;&lt;p&gt;For developers and enterprises, these features provide reassurance around provenance and compliance—critical in regulated or brand-sensitive industries.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Where Veo 3.1 Stands Among a Crowded AI Video Model Space&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Veo 3.1 is not just an iteration on prior models—it represents a deeper integration of multimodal inputs, storytelling control, and enterprise-level tooling. While creative professionals may see immediate benefits in editing workflows and fidelity, businesses exploring automation in training, advertising, or virtual experiences may find even greater value in the model’s composability and API support.&lt;/p&gt;&lt;p&gt;The early user feedback highlights that while Veo 3.1 offers valuable tooling, expectations around realism, voice control, and generation length are evolving rapidly. As Google expands access through Vertex AI and continues refining Veo, its competitive positioning in enterprise video generation will hinge on how quickly these user pain points are addressed.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/google-releases-new-ai-video-model-veo-3-1-in-flow-and-api-what-it-means-for</guid><pubDate>Wed, 15 Oct 2025 18:50:00 +0000</pubDate></item><item><title>[NEW] Anthropic’s Claude Haiku 4.5 matches May’s frontier model at fraction of cost (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/10/anthropics-claude-haiku-4-5-matches-mays-frontier-model-at-fraction-of-cost/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Tiny, fast model hits coding scores similar to GPT-5 and Sonnet 4.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Anthropic's Haiku 4.5 logo." class="absolute inset-0 w-full h-full object-cover hidden" height="359" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/haiku45_hero-640x359.jpg" width="640" /&gt;
                  &lt;img alt="Anthropic's Haiku 4.5 logo." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="624" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/haiku45_hero.jpg" width="1113" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Wednesday, Anthropic released Claude&amp;nbsp;Haiku 4.5, a small AI language model that reportedly delivers performance similar to what its frontier model Claude Sonnet 4 achieved five months ago but at one-third the cost and more than twice the speed. The new model is available now to all Claude app, web, and API users.&lt;/p&gt;
&lt;p&gt;If the benchmarks for Haiku 4.5 reported by Anthropic hold up to independent testing, the fact that the company can match some capabilities of its cutting-edge coding model from only five months ago (and GPT-5 in coding) while providing a dramatic speed increase and cost cut is notable.&lt;/p&gt;
&lt;p&gt;As a recap, Anthropic ships the Claude family in three model sizes: Haiku (small), Sonnet (medium), and Opus (large). The larger models are based on larger neural networks and typically include deeper contextual knowledge but are slower and more expensive to run. Due to a technique called distillation, companies like Anthropic have been able to craft smaller AI models that match the capability of larger, older models at functional tasks like coding, although it typically comes at the cost of omitting stored knowledge.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2122654 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Claude 4.5 Haiku benchmark results from Anthropic." class="center large" height="867" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/haiku45_benchmarks-1024x867.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Claude 4.5 Haiku benchmark results from Anthropic.

          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;That means if you wanted to converse with an AI model that might craft a deeper and more meaningful analysis of, say, foreign policy or world history, you might be better served talking to Sonnet or Opus (being aware that they can also be wrong and make things up). But if you just need quick coding assistance that's more about translation of concepts than general knowledge, Haiku might be the better pick due to its speed and lower cost.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;And speaking of cost, Haiku 4.5 is included for subscribers of the Claude web and app plans. Through the API (for developers), the small model is priced at $1 per million input tokens and $5 per million output tokens. That compares to Sonnet 4.5 at $3 per million input and $15 per million output tokens, and Opus 4.1 at $15 per million input and $75 per million output tokens.&lt;/p&gt;
&lt;p&gt;The model serves as a cheaper drop-in replacement for two older models, Haiku 3.5 and Sonnet 4. "Users who rely on AI for real-time, low-latency tasks like chat assistants, customer service agents, or pair programming will appreciate Haiku 4.5’s combination of high intelligence and remarkable speed," Anthropic writes.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2122655 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Claude 4.5 Haiku answers the classic Ars Technica AI question, &amp;quot;Would the color be called 'magenta' if the town of Magenta didn't exist?&amp;quot;" class="center large" height="907" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/haiku45_magenta-1024x907.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Claude 4.5 Haiku answers the classic Ars Technica AI question, "Would the color be called 'magenta' if the town of Magenta didn't exist?"

          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;On SWE-bench Verified, a test that measures performance on coding tasks, Haiku 4.5 scored 73.3 percent compared to Sonnet 4's similar performance level (72.7 percent). The model also reportedly surpasses Sonnet 4 at certain tasks like using computers, according to Anthropic's benchmarks. Claude Sonnet 4.5, released in late September, remains Anthropic's frontier model and what the company calls "the best coding model available."&lt;/p&gt;
&lt;p&gt;Haiku 4.5 also surprisingly edges up close to what OpenAI's GPT-5 can achieve in this particular set of benchmarks (as seen in the chart above), although since the results are self-reported and potentially cherry-picked to match a model's strengths, one should always take them with a grain of salt.&lt;/p&gt;
&lt;p&gt;Still, making a small, capable coding model may have unexpected advantages for agentic coding setups like Claude Code. Anthropic designed Haiku 4.5 to work alongside Sonnet 4.5 in multi-model workflows. In such a configuration, Anthropic says, Sonnet 4.5 could break down complex problems into multi-step plans, then coordinate multiple Haiku 4.5 instances to complete subtasks in parallel, like spinning off workers to get things done faster.&lt;/p&gt;
&lt;p&gt;For more details on the new model, Anthropic released a system card and documentation for developers.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Tiny, fast model hits coding scores similar to GPT-5 and Sonnet 4.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Anthropic's Haiku 4.5 logo." class="absolute inset-0 w-full h-full object-cover hidden" height="359" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/haiku45_hero-640x359.jpg" width="640" /&gt;
                  &lt;img alt="Anthropic's Haiku 4.5 logo." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="624" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/haiku45_hero.jpg" width="1113" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Wednesday, Anthropic released Claude&amp;nbsp;Haiku 4.5, a small AI language model that reportedly delivers performance similar to what its frontier model Claude Sonnet 4 achieved five months ago but at one-third the cost and more than twice the speed. The new model is available now to all Claude app, web, and API users.&lt;/p&gt;
&lt;p&gt;If the benchmarks for Haiku 4.5 reported by Anthropic hold up to independent testing, the fact that the company can match some capabilities of its cutting-edge coding model from only five months ago (and GPT-5 in coding) while providing a dramatic speed increase and cost cut is notable.&lt;/p&gt;
&lt;p&gt;As a recap, Anthropic ships the Claude family in three model sizes: Haiku (small), Sonnet (medium), and Opus (large). The larger models are based on larger neural networks and typically include deeper contextual knowledge but are slower and more expensive to run. Due to a technique called distillation, companies like Anthropic have been able to craft smaller AI models that match the capability of larger, older models at functional tasks like coding, although it typically comes at the cost of omitting stored knowledge.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2122654 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Claude 4.5 Haiku benchmark results from Anthropic." class="center large" height="867" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/haiku45_benchmarks-1024x867.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Claude 4.5 Haiku benchmark results from Anthropic.

          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;That means if you wanted to converse with an AI model that might craft a deeper and more meaningful analysis of, say, foreign policy or world history, you might be better served talking to Sonnet or Opus (being aware that they can also be wrong and make things up). But if you just need quick coding assistance that's more about translation of concepts than general knowledge, Haiku might be the better pick due to its speed and lower cost.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;And speaking of cost, Haiku 4.5 is included for subscribers of the Claude web and app plans. Through the API (for developers), the small model is priced at $1 per million input tokens and $5 per million output tokens. That compares to Sonnet 4.5 at $3 per million input and $15 per million output tokens, and Opus 4.1 at $15 per million input and $75 per million output tokens.&lt;/p&gt;
&lt;p&gt;The model serves as a cheaper drop-in replacement for two older models, Haiku 3.5 and Sonnet 4. "Users who rely on AI for real-time, low-latency tasks like chat assistants, customer service agents, or pair programming will appreciate Haiku 4.5’s combination of high intelligence and remarkable speed," Anthropic writes.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2122655 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Claude 4.5 Haiku answers the classic Ars Technica AI question, &amp;quot;Would the color be called 'magenta' if the town of Magenta didn't exist?&amp;quot;" class="center large" height="907" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/haiku45_magenta-1024x907.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Claude 4.5 Haiku answers the classic Ars Technica AI question, "Would the color be called 'magenta' if the town of Magenta didn't exist?"

          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;On SWE-bench Verified, a test that measures performance on coding tasks, Haiku 4.5 scored 73.3 percent compared to Sonnet 4's similar performance level (72.7 percent). The model also reportedly surpasses Sonnet 4 at certain tasks like using computers, according to Anthropic's benchmarks. Claude Sonnet 4.5, released in late September, remains Anthropic's frontier model and what the company calls "the best coding model available."&lt;/p&gt;
&lt;p&gt;Haiku 4.5 also surprisingly edges up close to what OpenAI's GPT-5 can achieve in this particular set of benchmarks (as seen in the chart above), although since the results are self-reported and potentially cherry-picked to match a model's strengths, one should always take them with a grain of salt.&lt;/p&gt;
&lt;p&gt;Still, making a small, capable coding model may have unexpected advantages for agentic coding setups like Claude Code. Anthropic designed Haiku 4.5 to work alongside Sonnet 4.5 in multi-model workflows. In such a configuration, Anthropic says, Sonnet 4.5 could break down complex problems into multi-step plans, then coordinate multiple Haiku 4.5 instances to complete subtasks in parallel, like spinning off workers to get things done faster.&lt;/p&gt;
&lt;p&gt;For more details on the new model, Anthropic released a system card and documentation for developers.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/10/anthropics-claude-haiku-4-5-matches-mays-frontier-model-at-fraction-of-cost/</guid><pubDate>Wed, 15 Oct 2025 18:53:00 +0000</pubDate></item><item><title>[NEW] Anthropic is giving away its powerful Claude Haiku 4.5 AI for free to take on OpenAI (AI | VentureBeat)</title><link>https://venturebeat.com/ai/anthropic-is-giving-away-its-powerful-claude-haiku-4-5-ai-for-free-to-take</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://anthropic.com/"&gt;&lt;u&gt;Anthropic&lt;/u&gt;&lt;/a&gt; released &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Claude Haiku 4.5&lt;/u&gt;&lt;/a&gt; on Wednesday, a smaller and significantly cheaper artificial intelligence model that matches the coding capabilities of systems that were considered cutting-edge just months ago, marking the latest salvo in an intensifying competition to dominate enterprise AI.&lt;/p&gt;&lt;p&gt;The model costs $1 per million input tokens and $5 per million output tokens — roughly one-third the price of Anthropic&amp;#x27;s mid-sized &lt;a href="https://www.anthropic.com/news/claude-4"&gt;&lt;u&gt;Sonnet 4 model&lt;/u&gt;&lt;/a&gt; released in May, while operating more than twice as fast. In certain tasks, particularly operating computers autonomously, Haiku 4.5 actually surpasses its more expensive predecessor.&lt;/p&gt;&lt;p&gt;&amp;quot;Haiku 4.5 is a clear leap in performance and is now largely as smart as Sonnet 4 while being significantly faster and one-third of the cost,&amp;quot; an Anthropic spokesperson told VentureBeat, underscoring how rapidly AI capabilities are becoming commoditized as the technology matures.&lt;/p&gt;&lt;p&gt;The launch comes just two weeks after Anthropic released &lt;a href="https://www.anthropic.com/news/claude-sonnet-4-5"&gt;&lt;u&gt;Claude Sonnet 4.5&lt;/u&gt;&lt;/a&gt;, which the company bills as the world&amp;#x27;s best coding model, and two months after introducing Opus 4.1. The breakneck pace of releases reflects mounting pressure from OpenAI, whose $500 billion valuation dwarfs &lt;a href="https://www.anthropic.com/news/anthropic-raises-series-f-at-usd183b-post-money-valuation"&gt;&lt;u&gt;Anthropic&amp;#x27;s $183 billion&lt;/u&gt;&lt;/a&gt;, and which has inked a series of multibillion-dollar infrastructure deals while expanding its product lineup.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How free access to advanced AI could reshape the enterprise market&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;In an unusual move that could reshape competitive dynamics in the AI market, Anthropic is making Haiku 4.5 available for all free users of its &lt;a href="http://claude.ai"&gt;&lt;u&gt;Claude.ai&lt;/u&gt;&lt;/a&gt; platform. The decision effectively democratizes access to what the company characterizes as &amp;quot;near-frontier-level intelligence&amp;quot; — capabilities that would have been available only in expensive, premium models months ago.&lt;/p&gt;&lt;p&gt;&amp;quot;The launch of Claude Haiku 4.5 means that near-frontier-level intelligence is available for free to all users through Claude.ai,&amp;quot; the Anthropic spokesperson told VentureBeat. &amp;quot;It also offers significant advantages to our enterprise customers: Sonnet 4.5 can handle frontier planning while Haiku 4.5 powers sub-agents, enabling multi-agent systems that tackle complex refactors, migrations, and large features builds with speed and quality.&amp;quot;&lt;/p&gt;&lt;p&gt;This multi-agent architecture signals a significant shift in how AI systems are deployed. Rather than relying on a single, monolithic model, enterprises can now orchestrate teams of specialized AI agents: a more sophisticated &lt;a href="https://www.anthropic.com/news/claude-sonnet-4-5"&gt;&lt;u&gt;Sonnet 4.5 model&lt;/u&gt;&lt;/a&gt; breaking down complex problems and delegating subtasks to multiple &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; agents working in parallel. For software development teams, this could mean Sonnet 4.5 plans a major code refactoring while Haiku 4.5 agents simultaneously execute changes across dozens of files.&lt;/p&gt;&lt;p&gt;The approach mirrors how human organizations distribute work, and could prove particularly valuable for enterprises seeking to balance performance with cost efficiency — a critical consideration as AI deployment scales.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Inside Anthropic&amp;#x27;s path to $7 billion in annual revenue&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The model launch coincides with revelations that Anthropic&amp;#x27;s business is experiencing explosive growth. The company&amp;#x27;s annual revenue run rate is &lt;a href="https://www.reuters.com/business/retail-consumer/anthropic-aims-nearly-triple-annualized-revenue-2026-sources-say-2025-10-15/"&gt;&lt;u&gt;approaching $7 billion this month&lt;/u&gt;&lt;/a&gt;, Anthropic told Reuters, up from more than $5 billion reported in August. Internal projections obtained by Reuters suggest the company is targeting between $20 billion and $26 billion in annualized revenue for 2026, representing growth of more than 200% to nearly 300%.&lt;/p&gt;&lt;p&gt;The company now serves more than 300,000 business customers, with enterprise products accounting for approximately 80% of revenue. Among Anthropic&amp;#x27;s most successful offerings is &lt;a href="https://www.claude.com/product/claude-code"&gt;&lt;u&gt;Claude Code&lt;/u&gt;&lt;/a&gt;, a code-generation tool that has reached nearly $1 billion in annualized revenue since launching earlier this year.&lt;/p&gt;&lt;p&gt;Those numbers come as artificial intelligence enters what many in the industry characterize as a critical inflection point. After two years of what Anthropic Chief Product Officer Mike Krieger recently described as &amp;quot;&lt;a href="https://www.businessinsider.com/anthropic-cpo-companies-success-metrics-avoid-ai-fomo-2025-10"&gt;&lt;u&gt;AI FOMO&lt;/u&gt;&lt;/a&gt;&amp;quot; — where companies adopted AI tools without clear success metrics — enterprises are now demanding measurable returns on investment.&lt;/p&gt;&lt;p&gt;&amp;quot;The best products can be grounded in some kind of success metric or evaluation,&amp;quot; Krieger said on the &lt;a href="https://podcasts.apple.com/us/podcast/inside-claude-the-ai-coworker-era-mike-krieger-anthropic/id1759013677?i=1000731964089"&gt;&lt;u&gt;&amp;quot;Superhuman AI&amp;quot; podcast&lt;/u&gt;&lt;/a&gt;. &amp;quot;I&amp;#x27;ve seen that a lot in talking to companies that are deploying AI.&amp;quot;&lt;/p&gt;&lt;p&gt;For enterprises evaluating AI tools, the calculus increasingly centers on concrete productivity gains. Google CEO Sundar Pichai claimed in June that AI had generated a 10% boost in engineering velocity at his company — though measuring such improvements across different roles and use cases remains challenging, as Krieger acknowledged.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why AI safety testing matters more than ever for enterprise adoption&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Anthropic&amp;#x27;s launch comes amid heightened scrutiny of the company&amp;#x27;s approach to AI safety and regulation. On Tuesday, David Sacks, the White House&amp;#x27;s AI &amp;quot;czar&amp;quot; and a venture capitalist, accused Anthropic of &amp;quot;&lt;a href="https://x.com/DavidSacks/status/1978145266269077891"&gt;&lt;u&gt;running a sophisticated regulatory capture strategy based on fear-mongering&amp;quot; that is &amp;quot;damaging the startup ecosystem&lt;/u&gt;&lt;/a&gt;.&amp;quot;&lt;/p&gt;&lt;p&gt;The attack targeted remarks by Jack Clark, Anthropic&amp;#x27;s British co-founder and head of policy, who had described being &amp;quot;&lt;a href="https://importai.substack.com/p/import-ai-431-technological-optimism"&gt;&lt;u&gt;deeply afraid&lt;/u&gt;&lt;/a&gt;&amp;quot; of AI&amp;#x27;s trajectory. Clark told Bloomberg he found Sacks&amp;#x27; criticism &amp;quot;&lt;a href="https://www.bloomberg.com/opinion/articles/2025-10-15/anthropic-s-ai-principles-make-it-a-white-house-target"&gt;&lt;u&gt;perplexing&lt;/u&gt;&lt;/a&gt;.&amp;quot;&lt;/p&gt;&lt;p&gt;Anthropic addressed such concerns head-on in its release materials, emphasizing that &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; underwent extensive safety testing. The company classified the model as &lt;a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy"&gt;&lt;u&gt;ASL-2&lt;/u&gt;&lt;/a&gt; — its AI Safety Level 2 standard — compared to the more restrictive &lt;a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy"&gt;&lt;u&gt;ASL-3&lt;/u&gt;&lt;/a&gt; designation for the more powerful Sonnet 4.5 and Opus 4.1 models.&lt;/p&gt;&lt;p&gt;&amp;quot;Our teams have red-teamed and tested our agentic capabilities to the limits in order to assess whether it can be used to engage in harmful activity like generating misinformation or promoting fraudulent behavior like scams,&amp;quot; the spokesperson told VentureBeat. &amp;quot;In our automated alignment assessment, it showed a statistically significantly lower overall rate of misaligned behaviors than both Claude Sonnet 4.5 and Claude Opus 4.1 — making it, by this metric, our safest model yet.&amp;quot;&lt;/p&gt;&lt;p&gt;The company said its safety testing showed &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; poses only limited risks regarding the production of chemical, biological, radiological and nuclear weapons. Anthropic has also implemented classifiers designed to detect and filter prompt injection attacks, a common method for attempting to manipulate AI systems into producing harmful content.&lt;/p&gt;&lt;p&gt;The emphasis on safety reflects Anthropic&amp;#x27;s founding mission. The company was established in 2021 by former OpenAI executives, including siblings Dario and Daniela Amodei, who left amid concerns about OpenAI&amp;#x27;s direction following its partnership with Microsoft. Anthropic has positioned itself as taking a more cautious, research-oriented approach to AI development.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Benchmark results show Haiku 4.5 competing with larger, more expensive models&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;According to Anthropic&amp;#x27;s benchmarks, &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; performs competitively with or exceeds several larger models across multiple evaluation criteria. On &lt;a href="https://www.swebench.com/"&gt;&lt;u&gt;SWE-bench Verified&lt;/u&gt;&lt;/a&gt;, a widely used test measuring AI systems&amp;#x27; ability to solve real-world software engineering problems, Haiku 4.5 scored 73.3% — slightly ahead of Sonnet 4&amp;#x27;s 72.7% and close to GPT-5 Codex&amp;#x27;s 74.5%.&lt;/p&gt;&lt;p&gt;The model demonstrated particular strength in computer use tasks, achieving 50.7% on the &lt;a href="https://os-world.github.io/"&gt;&lt;u&gt;OSWorld benchmark&lt;/u&gt;&lt;/a&gt; compared to Sonnet 4&amp;#x27;s 42.2%. This capability allows the AI to interact directly with computer interfaces — clicking buttons, filling forms, navigating applications — which could prove transformative for automating routine digital tasks.&lt;/p&gt;&lt;p&gt;In coding-specific benchmarks like &lt;a href="https://www.tbench.ai/"&gt;&lt;u&gt;Terminal-Bench&lt;/u&gt;&lt;/a&gt;, which tests AI agents&amp;#x27; ability to complete complex software tasks using command-line tools, Haiku 4.5 scored 41.0%, trailing only Sonnet 4.5&amp;#x27;s 50.0% among Claude models.&lt;/p&gt;&lt;p&gt;The model maintains a 200,000-token context window for standard users, with developers accessing the &lt;a href="https://www.claude.com/platform/api"&gt;&lt;u&gt;Claude Developer Platform&lt;/u&gt;&lt;/a&gt; able to use a 1-million-token context window. That expanded capacity means the model can process extremely large codebases or documents in a single request — roughly equivalent to a 1,500-page book.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What three major AI model releases in two months says about the competition&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;When asked about the rapid succession of model releases, the Anthropic spokesperson emphasized the company&amp;#x27;s focus on execution rather than competitive positioning.&lt;/p&gt;&lt;p&gt;&amp;quot;We&amp;#x27;re focused on shipping the best possible products for our customers — and our shipping velocity speaks for itself,&amp;quot; the spokesperson said. &amp;quot;What was state-of-the-art just five months ago is now faster, cheaper, and more accessible.&amp;quot;&lt;/p&gt;&lt;p&gt;That velocity stands in contrast to the company&amp;#x27;s earlier, more measured release schedule. Anthropic appeared to have paused development of its Haiku line after releasing version 3.5 at the end of last year, leading some observers to speculate the company had deprioritized smaller models.&lt;/p&gt;&lt;p&gt;That rapid price-performance improvement validates a core promise of artificial intelligence: that capabilities will become dramatically cheaper over time as the technology matures and companies optimize their models. For enterprises, it suggests that today&amp;#x27;s budget constraints around AI deployment may ease considerably in coming years.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;From customer service to code: Real-world applications for faster, cheaper AI&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The practical applications of &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; span a wide range of enterprise functions, from customer service to financial analysis to software development. The model&amp;#x27;s combination of speed and intelligence makes it particularly suited for real-time, low-latency tasks like chatbot conversations and customer support interactions, where delays of even a few seconds can degrade user experience.&lt;/p&gt;&lt;p&gt;In financial services, the multi-agent architecture enabled by pairing &lt;a href="https://www.anthropic.com/news/claude-sonnet-4-5"&gt;&lt;u&gt;Sonnet 4.5&lt;/u&gt;&lt;/a&gt; with &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; could transform how firms monitor markets and manage risk. Anthropic envisions Haiku 4.5 monitoring thousands of data streams simultaneously — tracking regulatory changes, market signals and portfolio risks — while Sonnet 4.5 handles complex predictive modeling and strategic analysis.&lt;/p&gt;&lt;p&gt;For research organizations, the division of labor could compress timelines dramatically. &lt;a href="https://www.anthropic.com/news/claude-sonnet-4-5"&gt;&lt;u&gt;Sonnet 4.5&lt;/u&gt;&lt;/a&gt; might orchestrate a comprehensive analysis while multiple &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; agents parallelize literature reviews, data gathering and document synthesis across dozens of sources, potentially &amp;quot;compressing weeks of research into hours,&amp;quot; according to Anthropic&amp;#x27;s use case descriptions.&lt;/p&gt;&lt;p&gt;Several companies have already integrated Haiku 4.5 and reported positive results. Guy Gur-Ari, co-founder of coding startup Augment, said the model &amp;quot;hit a sweet spot we didn&amp;#x27;t think was possible: near-frontier coding quality with blazing speed and cost efficiency.&amp;quot; In Augment&amp;#x27;s internal testing, Haiku 4.5 achieved 90% of Sonnet 4.5&amp;#x27;s performance while matching much larger models.&lt;/p&gt;&lt;p&gt;Jeff Wang, CEO of Windsurf, another coding-focused startup, said Haiku 4.5 &amp;quot;is blurring the lines&amp;quot; on traditional trade-offs between speed, cost and quality. &amp;quot;It&amp;#x27;s a fast frontier model that keeps costs efficient and signals where this class of models is headed.&amp;quot;&lt;/p&gt;&lt;p&gt;Jon Noronha, co-founder of presentation software company Gamma, reported that Haiku 4.5 &amp;quot;outperformed our current models on instruction-following for slide text generation, achieving 65% accuracy versus 44% from our premium tier model — that&amp;#x27;s a game-changer for our unit economics.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The price of progress: What plummeting AI costs mean for enterprise strategy&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;For enterprises evaluating AI strategies, &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; presents both opportunity and challenge. The opportunity lies in accessing sophisticated AI capabilities at dramatically lower costs, potentially making viable entire categories of applications that were previously too expensive to deploy at scale.&lt;/p&gt;&lt;p&gt;The challenge is keeping pace with a technology landscape that is evolving faster than most organizations can absorb. As Krieger noted in his recent podcast appearance, companies are moving beyond &amp;quot;&lt;a href="https://www.businessinsider.com/anthropic-cpo-companies-success-metrics-avoid-ai-fomo-2025-10"&gt;&lt;u&gt;AI FOMO&lt;/u&gt;&lt;/a&gt;&amp;quot; to demand concrete metrics and demonstrated value. But establishing those metrics and evaluation frameworks takes time — time that may be in short supply as competitors race ahead.&lt;/p&gt;&lt;p&gt;The shift from single-model deployments to multi-agent architectures also requires new ways of thinking about AI systems. Rather than viewing AI as a monolithic assistant, enterprises must learn to orchestrate multiple specialized agents, each optimized for particular tasks — more akin to managing a team than operating a tool.&lt;/p&gt;&lt;p&gt;The fundamental economics of AI are shifting with remarkable speed. Five months ago, Sonnet 4&amp;#x27;s capabilities commanded premium pricing and represented the cutting edge. Today, Haiku 4.5 delivers similar performance at a third of the cost. If that trajectory continues — and both Anthropic&amp;#x27;s release schedule and competitive pressure from OpenAI and Google suggest it will — the AI capabilities that seem remarkable today may be routine and inexpensive within a year.&lt;/p&gt;&lt;p&gt;For Anthropic, the challenge will be translating technical achievements into sustainable business growth while maintaining the safety-focused approach that differentiates it from competitors. The company&amp;#x27;s projected revenue growth to as much as $26 billion by 2026 suggests strong market traction, but achieving those targets will require continued innovation and successful execution across an increasingly complex product portfolio.&lt;/p&gt;&lt;p&gt;Whether enterprises will choose Claude over increasingly capable alternatives from OpenAI, Google and a growing field of competitors remains an open question. But Anthropic is making a clear bet: that the future of AI belongs not to whoever builds the single most powerful model, but to whoever can deliver the right intelligence, at the right speed, at the right price — and make it accessible to everyone.&lt;/p&gt;&lt;p&gt;In an industry where the promise of artificial intelligence has long outpaced reality, Anthropic is betting that delivering on that promise, faster and cheaper than anyone expected, will be enough to win. And with pricing dropping by two-thirds in just five months while performance holds steady, that promise is starting to look like reality.&lt;/p&gt;&lt;p&gt;

&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://anthropic.com/"&gt;&lt;u&gt;Anthropic&lt;/u&gt;&lt;/a&gt; released &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Claude Haiku 4.5&lt;/u&gt;&lt;/a&gt; on Wednesday, a smaller and significantly cheaper artificial intelligence model that matches the coding capabilities of systems that were considered cutting-edge just months ago, marking the latest salvo in an intensifying competition to dominate enterprise AI.&lt;/p&gt;&lt;p&gt;The model costs $1 per million input tokens and $5 per million output tokens — roughly one-third the price of Anthropic&amp;#x27;s mid-sized &lt;a href="https://www.anthropic.com/news/claude-4"&gt;&lt;u&gt;Sonnet 4 model&lt;/u&gt;&lt;/a&gt; released in May, while operating more than twice as fast. In certain tasks, particularly operating computers autonomously, Haiku 4.5 actually surpasses its more expensive predecessor.&lt;/p&gt;&lt;p&gt;&amp;quot;Haiku 4.5 is a clear leap in performance and is now largely as smart as Sonnet 4 while being significantly faster and one-third of the cost,&amp;quot; an Anthropic spokesperson told VentureBeat, underscoring how rapidly AI capabilities are becoming commoditized as the technology matures.&lt;/p&gt;&lt;p&gt;The launch comes just two weeks after Anthropic released &lt;a href="https://www.anthropic.com/news/claude-sonnet-4-5"&gt;&lt;u&gt;Claude Sonnet 4.5&lt;/u&gt;&lt;/a&gt;, which the company bills as the world&amp;#x27;s best coding model, and two months after introducing Opus 4.1. The breakneck pace of releases reflects mounting pressure from OpenAI, whose $500 billion valuation dwarfs &lt;a href="https://www.anthropic.com/news/anthropic-raises-series-f-at-usd183b-post-money-valuation"&gt;&lt;u&gt;Anthropic&amp;#x27;s $183 billion&lt;/u&gt;&lt;/a&gt;, and which has inked a series of multibillion-dollar infrastructure deals while expanding its product lineup.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How free access to advanced AI could reshape the enterprise market&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;In an unusual move that could reshape competitive dynamics in the AI market, Anthropic is making Haiku 4.5 available for all free users of its &lt;a href="http://claude.ai"&gt;&lt;u&gt;Claude.ai&lt;/u&gt;&lt;/a&gt; platform. The decision effectively democratizes access to what the company characterizes as &amp;quot;near-frontier-level intelligence&amp;quot; — capabilities that would have been available only in expensive, premium models months ago.&lt;/p&gt;&lt;p&gt;&amp;quot;The launch of Claude Haiku 4.5 means that near-frontier-level intelligence is available for free to all users through Claude.ai,&amp;quot; the Anthropic spokesperson told VentureBeat. &amp;quot;It also offers significant advantages to our enterprise customers: Sonnet 4.5 can handle frontier planning while Haiku 4.5 powers sub-agents, enabling multi-agent systems that tackle complex refactors, migrations, and large features builds with speed and quality.&amp;quot;&lt;/p&gt;&lt;p&gt;This multi-agent architecture signals a significant shift in how AI systems are deployed. Rather than relying on a single, monolithic model, enterprises can now orchestrate teams of specialized AI agents: a more sophisticated &lt;a href="https://www.anthropic.com/news/claude-sonnet-4-5"&gt;&lt;u&gt;Sonnet 4.5 model&lt;/u&gt;&lt;/a&gt; breaking down complex problems and delegating subtasks to multiple &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; agents working in parallel. For software development teams, this could mean Sonnet 4.5 plans a major code refactoring while Haiku 4.5 agents simultaneously execute changes across dozens of files.&lt;/p&gt;&lt;p&gt;The approach mirrors how human organizations distribute work, and could prove particularly valuable for enterprises seeking to balance performance with cost efficiency — a critical consideration as AI deployment scales.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Inside Anthropic&amp;#x27;s path to $7 billion in annual revenue&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The model launch coincides with revelations that Anthropic&amp;#x27;s business is experiencing explosive growth. The company&amp;#x27;s annual revenue run rate is &lt;a href="https://www.reuters.com/business/retail-consumer/anthropic-aims-nearly-triple-annualized-revenue-2026-sources-say-2025-10-15/"&gt;&lt;u&gt;approaching $7 billion this month&lt;/u&gt;&lt;/a&gt;, Anthropic told Reuters, up from more than $5 billion reported in August. Internal projections obtained by Reuters suggest the company is targeting between $20 billion and $26 billion in annualized revenue for 2026, representing growth of more than 200% to nearly 300%.&lt;/p&gt;&lt;p&gt;The company now serves more than 300,000 business customers, with enterprise products accounting for approximately 80% of revenue. Among Anthropic&amp;#x27;s most successful offerings is &lt;a href="https://www.claude.com/product/claude-code"&gt;&lt;u&gt;Claude Code&lt;/u&gt;&lt;/a&gt;, a code-generation tool that has reached nearly $1 billion in annualized revenue since launching earlier this year.&lt;/p&gt;&lt;p&gt;Those numbers come as artificial intelligence enters what many in the industry characterize as a critical inflection point. After two years of what Anthropic Chief Product Officer Mike Krieger recently described as &amp;quot;&lt;a href="https://www.businessinsider.com/anthropic-cpo-companies-success-metrics-avoid-ai-fomo-2025-10"&gt;&lt;u&gt;AI FOMO&lt;/u&gt;&lt;/a&gt;&amp;quot; — where companies adopted AI tools without clear success metrics — enterprises are now demanding measurable returns on investment.&lt;/p&gt;&lt;p&gt;&amp;quot;The best products can be grounded in some kind of success metric or evaluation,&amp;quot; Krieger said on the &lt;a href="https://podcasts.apple.com/us/podcast/inside-claude-the-ai-coworker-era-mike-krieger-anthropic/id1759013677?i=1000731964089"&gt;&lt;u&gt;&amp;quot;Superhuman AI&amp;quot; podcast&lt;/u&gt;&lt;/a&gt;. &amp;quot;I&amp;#x27;ve seen that a lot in talking to companies that are deploying AI.&amp;quot;&lt;/p&gt;&lt;p&gt;For enterprises evaluating AI tools, the calculus increasingly centers on concrete productivity gains. Google CEO Sundar Pichai claimed in June that AI had generated a 10% boost in engineering velocity at his company — though measuring such improvements across different roles and use cases remains challenging, as Krieger acknowledged.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why AI safety testing matters more than ever for enterprise adoption&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Anthropic&amp;#x27;s launch comes amid heightened scrutiny of the company&amp;#x27;s approach to AI safety and regulation. On Tuesday, David Sacks, the White House&amp;#x27;s AI &amp;quot;czar&amp;quot; and a venture capitalist, accused Anthropic of &amp;quot;&lt;a href="https://x.com/DavidSacks/status/1978145266269077891"&gt;&lt;u&gt;running a sophisticated regulatory capture strategy based on fear-mongering&amp;quot; that is &amp;quot;damaging the startup ecosystem&lt;/u&gt;&lt;/a&gt;.&amp;quot;&lt;/p&gt;&lt;p&gt;The attack targeted remarks by Jack Clark, Anthropic&amp;#x27;s British co-founder and head of policy, who had described being &amp;quot;&lt;a href="https://importai.substack.com/p/import-ai-431-technological-optimism"&gt;&lt;u&gt;deeply afraid&lt;/u&gt;&lt;/a&gt;&amp;quot; of AI&amp;#x27;s trajectory. Clark told Bloomberg he found Sacks&amp;#x27; criticism &amp;quot;&lt;a href="https://www.bloomberg.com/opinion/articles/2025-10-15/anthropic-s-ai-principles-make-it-a-white-house-target"&gt;&lt;u&gt;perplexing&lt;/u&gt;&lt;/a&gt;.&amp;quot;&lt;/p&gt;&lt;p&gt;Anthropic addressed such concerns head-on in its release materials, emphasizing that &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; underwent extensive safety testing. The company classified the model as &lt;a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy"&gt;&lt;u&gt;ASL-2&lt;/u&gt;&lt;/a&gt; — its AI Safety Level 2 standard — compared to the more restrictive &lt;a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy"&gt;&lt;u&gt;ASL-3&lt;/u&gt;&lt;/a&gt; designation for the more powerful Sonnet 4.5 and Opus 4.1 models.&lt;/p&gt;&lt;p&gt;&amp;quot;Our teams have red-teamed and tested our agentic capabilities to the limits in order to assess whether it can be used to engage in harmful activity like generating misinformation or promoting fraudulent behavior like scams,&amp;quot; the spokesperson told VentureBeat. &amp;quot;In our automated alignment assessment, it showed a statistically significantly lower overall rate of misaligned behaviors than both Claude Sonnet 4.5 and Claude Opus 4.1 — making it, by this metric, our safest model yet.&amp;quot;&lt;/p&gt;&lt;p&gt;The company said its safety testing showed &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; poses only limited risks regarding the production of chemical, biological, radiological and nuclear weapons. Anthropic has also implemented classifiers designed to detect and filter prompt injection attacks, a common method for attempting to manipulate AI systems into producing harmful content.&lt;/p&gt;&lt;p&gt;The emphasis on safety reflects Anthropic&amp;#x27;s founding mission. The company was established in 2021 by former OpenAI executives, including siblings Dario and Daniela Amodei, who left amid concerns about OpenAI&amp;#x27;s direction following its partnership with Microsoft. Anthropic has positioned itself as taking a more cautious, research-oriented approach to AI development.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Benchmark results show Haiku 4.5 competing with larger, more expensive models&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;According to Anthropic&amp;#x27;s benchmarks, &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; performs competitively with or exceeds several larger models across multiple evaluation criteria. On &lt;a href="https://www.swebench.com/"&gt;&lt;u&gt;SWE-bench Verified&lt;/u&gt;&lt;/a&gt;, a widely used test measuring AI systems&amp;#x27; ability to solve real-world software engineering problems, Haiku 4.5 scored 73.3% — slightly ahead of Sonnet 4&amp;#x27;s 72.7% and close to GPT-5 Codex&amp;#x27;s 74.5%.&lt;/p&gt;&lt;p&gt;The model demonstrated particular strength in computer use tasks, achieving 50.7% on the &lt;a href="https://os-world.github.io/"&gt;&lt;u&gt;OSWorld benchmark&lt;/u&gt;&lt;/a&gt; compared to Sonnet 4&amp;#x27;s 42.2%. This capability allows the AI to interact directly with computer interfaces — clicking buttons, filling forms, navigating applications — which could prove transformative for automating routine digital tasks.&lt;/p&gt;&lt;p&gt;In coding-specific benchmarks like &lt;a href="https://www.tbench.ai/"&gt;&lt;u&gt;Terminal-Bench&lt;/u&gt;&lt;/a&gt;, which tests AI agents&amp;#x27; ability to complete complex software tasks using command-line tools, Haiku 4.5 scored 41.0%, trailing only Sonnet 4.5&amp;#x27;s 50.0% among Claude models.&lt;/p&gt;&lt;p&gt;The model maintains a 200,000-token context window for standard users, with developers accessing the &lt;a href="https://www.claude.com/platform/api"&gt;&lt;u&gt;Claude Developer Platform&lt;/u&gt;&lt;/a&gt; able to use a 1-million-token context window. That expanded capacity means the model can process extremely large codebases or documents in a single request — roughly equivalent to a 1,500-page book.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What three major AI model releases in two months says about the competition&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;When asked about the rapid succession of model releases, the Anthropic spokesperson emphasized the company&amp;#x27;s focus on execution rather than competitive positioning.&lt;/p&gt;&lt;p&gt;&amp;quot;We&amp;#x27;re focused on shipping the best possible products for our customers — and our shipping velocity speaks for itself,&amp;quot; the spokesperson said. &amp;quot;What was state-of-the-art just five months ago is now faster, cheaper, and more accessible.&amp;quot;&lt;/p&gt;&lt;p&gt;That velocity stands in contrast to the company&amp;#x27;s earlier, more measured release schedule. Anthropic appeared to have paused development of its Haiku line after releasing version 3.5 at the end of last year, leading some observers to speculate the company had deprioritized smaller models.&lt;/p&gt;&lt;p&gt;That rapid price-performance improvement validates a core promise of artificial intelligence: that capabilities will become dramatically cheaper over time as the technology matures and companies optimize their models. For enterprises, it suggests that today&amp;#x27;s budget constraints around AI deployment may ease considerably in coming years.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;From customer service to code: Real-world applications for faster, cheaper AI&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The practical applications of &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; span a wide range of enterprise functions, from customer service to financial analysis to software development. The model&amp;#x27;s combination of speed and intelligence makes it particularly suited for real-time, low-latency tasks like chatbot conversations and customer support interactions, where delays of even a few seconds can degrade user experience.&lt;/p&gt;&lt;p&gt;In financial services, the multi-agent architecture enabled by pairing &lt;a href="https://www.anthropic.com/news/claude-sonnet-4-5"&gt;&lt;u&gt;Sonnet 4.5&lt;/u&gt;&lt;/a&gt; with &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; could transform how firms monitor markets and manage risk. Anthropic envisions Haiku 4.5 monitoring thousands of data streams simultaneously — tracking regulatory changes, market signals and portfolio risks — while Sonnet 4.5 handles complex predictive modeling and strategic analysis.&lt;/p&gt;&lt;p&gt;For research organizations, the division of labor could compress timelines dramatically. &lt;a href="https://www.anthropic.com/news/claude-sonnet-4-5"&gt;&lt;u&gt;Sonnet 4.5&lt;/u&gt;&lt;/a&gt; might orchestrate a comprehensive analysis while multiple &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; agents parallelize literature reviews, data gathering and document synthesis across dozens of sources, potentially &amp;quot;compressing weeks of research into hours,&amp;quot; according to Anthropic&amp;#x27;s use case descriptions.&lt;/p&gt;&lt;p&gt;Several companies have already integrated Haiku 4.5 and reported positive results. Guy Gur-Ari, co-founder of coding startup Augment, said the model &amp;quot;hit a sweet spot we didn&amp;#x27;t think was possible: near-frontier coding quality with blazing speed and cost efficiency.&amp;quot; In Augment&amp;#x27;s internal testing, Haiku 4.5 achieved 90% of Sonnet 4.5&amp;#x27;s performance while matching much larger models.&lt;/p&gt;&lt;p&gt;Jeff Wang, CEO of Windsurf, another coding-focused startup, said Haiku 4.5 &amp;quot;is blurring the lines&amp;quot; on traditional trade-offs between speed, cost and quality. &amp;quot;It&amp;#x27;s a fast frontier model that keeps costs efficient and signals where this class of models is headed.&amp;quot;&lt;/p&gt;&lt;p&gt;Jon Noronha, co-founder of presentation software company Gamma, reported that Haiku 4.5 &amp;quot;outperformed our current models on instruction-following for slide text generation, achieving 65% accuracy versus 44% from our premium tier model — that&amp;#x27;s a game-changer for our unit economics.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The price of progress: What plummeting AI costs mean for enterprise strategy&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;For enterprises evaluating AI strategies, &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; presents both opportunity and challenge. The opportunity lies in accessing sophisticated AI capabilities at dramatically lower costs, potentially making viable entire categories of applications that were previously too expensive to deploy at scale.&lt;/p&gt;&lt;p&gt;The challenge is keeping pace with a technology landscape that is evolving faster than most organizations can absorb. As Krieger noted in his recent podcast appearance, companies are moving beyond &amp;quot;&lt;a href="https://www.businessinsider.com/anthropic-cpo-companies-success-metrics-avoid-ai-fomo-2025-10"&gt;&lt;u&gt;AI FOMO&lt;/u&gt;&lt;/a&gt;&amp;quot; to demand concrete metrics and demonstrated value. But establishing those metrics and evaluation frameworks takes time — time that may be in short supply as competitors race ahead.&lt;/p&gt;&lt;p&gt;The shift from single-model deployments to multi-agent architectures also requires new ways of thinking about AI systems. Rather than viewing AI as a monolithic assistant, enterprises must learn to orchestrate multiple specialized agents, each optimized for particular tasks — more akin to managing a team than operating a tool.&lt;/p&gt;&lt;p&gt;The fundamental economics of AI are shifting with remarkable speed. Five months ago, Sonnet 4&amp;#x27;s capabilities commanded premium pricing and represented the cutting edge. Today, Haiku 4.5 delivers similar performance at a third of the cost. If that trajectory continues — and both Anthropic&amp;#x27;s release schedule and competitive pressure from OpenAI and Google suggest it will — the AI capabilities that seem remarkable today may be routine and inexpensive within a year.&lt;/p&gt;&lt;p&gt;For Anthropic, the challenge will be translating technical achievements into sustainable business growth while maintaining the safety-focused approach that differentiates it from competitors. The company&amp;#x27;s projected revenue growth to as much as $26 billion by 2026 suggests strong market traction, but achieving those targets will require continued innovation and successful execution across an increasingly complex product portfolio.&lt;/p&gt;&lt;p&gt;Whether enterprises will choose Claude over increasingly capable alternatives from OpenAI, Google and a growing field of competitors remains an open question. But Anthropic is making a clear bet: that the future of AI belongs not to whoever builds the single most powerful model, but to whoever can deliver the right intelligence, at the right speed, at the right price — and make it accessible to everyone.&lt;/p&gt;&lt;p&gt;In an industry where the promise of artificial intelligence has long outpaced reality, Anthropic is betting that delivering on that promise, faster and cheaper than anyone expected, will be enough to win. And with pricing dropping by two-thirds in just five months while performance holds steady, that promise is starting to look like reality.&lt;/p&gt;&lt;p&gt;

&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/anthropic-is-giving-away-its-powerful-claude-haiku-4-5-ai-for-free-to-take</guid><pubDate>Wed, 15 Oct 2025 19:00:00 +0000</pubDate></item><item><title>[NEW] Army general says he’s using AI to improve “decision-making” (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/10/army-general-says-hes-using-ai-to-improve-decision-making/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        "AI is one thing that, as a commander, it’s been very, very interesting for me."
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2219849110-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2219849110-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Team of army experts in data center analyzing missiles flight paths with deep learning tools.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Last month, OpenAI published a usage study showing that nearly 15 percent of work-related conversations on ChatGPT had to deal with "making decisions and solving problems." Now comes word that at least one high-level member of the US military is using LLMs for the same purpose.&lt;/p&gt;
&lt;p&gt;At the Association of the US Army Conference in Washington, DC, this week, Maj. Gen. William "Hank" Taylor reportedly said that "Chat and I are really close lately," using a distressingly familiar diminutive nickname to refer to an unspecified AI chatbot. "AI is one thing that, as a commander, it’s been very, very interesting for me."&lt;/p&gt;
&lt;p&gt;Military-focused news site DefenseScoop reports that Taylor told a roundtable group of reporters that he and the Eighth Army he commands out of South Korea are "regularly using" AI to modernize their predictive analysis for logistical planning and operational purposes. That is helpful for paperwork tasks like "just being able to write our weekly reports and things," Taylor said, but it also aids in informing their overall direction.&lt;/p&gt;
&lt;p&gt;“One of the things that recently I’ve been personally working on with my soldiers is decision-making—individual decision-making," Taylor said. "And how [we make decisions] in our own individual life, when we make decisions, it’s important. So, that’s something I’ve been asking and trying to build models to help all of us. Especially, [on] how do I make decisions, personal decisions, right — that affect not only me, but my organization and overall readiness?"&lt;/p&gt;
&lt;p&gt;That's still a far cry from the &lt;em&gt;Terminator&lt;/em&gt; vision of autonomous AI weapon systems that take lethal decisions out of human hands. Still, using LLMs for military decision-making&amp;nbsp;might give pause to anyone familiar with the models' well-known propensity to confabulate fake citations and sycophantically flatter users.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In May, the Army rolled out the Army Enterprise LLM Workspace—built on the commercial Ask Sage platform—to streamline simple text-based tasks such as press releases and personnel descriptions. For other so-called "back office" military work, though, early tests have shown that generative AI might not always be the most efficient use of the military budget.&lt;/p&gt;
&lt;p&gt;"There are many times that we find folks using this technology to answer something that we could just do in a spreadsheet with one math problem, and we’re paying a lot more money to do it,” Army CIO Leonel Garciga told DefenseScoop in August. "Is the juice worth the squeeze? Or is there another way to get at the same problem that may be less cool from a tech perspective, but more viable from an execution perspective?"&lt;/p&gt;
&lt;p&gt;In 2023, the US State Department listed the best practices for military use of AI, focused on ethical and responsible deployment of AI tools within a human chain of command. The report stressed that humans should remain in control of "decisions concerning nuclear weapons employment" and should maintain the capability to "disengage or deactivate deployed systems that demonstrate unintended behavior."&lt;/p&gt;
&lt;p&gt;Since then, the military has shown interest in using AI technology in the field for everything from automated targeting systems on drones to "improving situational awareness" via an OpenAI partnership with military contractor Anduril. In January 2024, OpenAI removed a prohibition on "military and warfare uses" from ChatGPT's usage policies, while still barring customers from "devlop[ing] or us[ing] weapons" via the LLM.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        "AI is one thing that, as a commander, it’s been very, very interesting for me."
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2219849110-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2219849110-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Team of army experts in data center analyzing missiles flight paths with deep learning tools.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Last month, OpenAI published a usage study showing that nearly 15 percent of work-related conversations on ChatGPT had to deal with "making decisions and solving problems." Now comes word that at least one high-level member of the US military is using LLMs for the same purpose.&lt;/p&gt;
&lt;p&gt;At the Association of the US Army Conference in Washington, DC, this week, Maj. Gen. William "Hank" Taylor reportedly said that "Chat and I are really close lately," using a distressingly familiar diminutive nickname to refer to an unspecified AI chatbot. "AI is one thing that, as a commander, it’s been very, very interesting for me."&lt;/p&gt;
&lt;p&gt;Military-focused news site DefenseScoop reports that Taylor told a roundtable group of reporters that he and the Eighth Army he commands out of South Korea are "regularly using" AI to modernize their predictive analysis for logistical planning and operational purposes. That is helpful for paperwork tasks like "just being able to write our weekly reports and things," Taylor said, but it also aids in informing their overall direction.&lt;/p&gt;
&lt;p&gt;“One of the things that recently I’ve been personally working on with my soldiers is decision-making—individual decision-making," Taylor said. "And how [we make decisions] in our own individual life, when we make decisions, it’s important. So, that’s something I’ve been asking and trying to build models to help all of us. Especially, [on] how do I make decisions, personal decisions, right — that affect not only me, but my organization and overall readiness?"&lt;/p&gt;
&lt;p&gt;That's still a far cry from the &lt;em&gt;Terminator&lt;/em&gt; vision of autonomous AI weapon systems that take lethal decisions out of human hands. Still, using LLMs for military decision-making&amp;nbsp;might give pause to anyone familiar with the models' well-known propensity to confabulate fake citations and sycophantically flatter users.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In May, the Army rolled out the Army Enterprise LLM Workspace—built on the commercial Ask Sage platform—to streamline simple text-based tasks such as press releases and personnel descriptions. For other so-called "back office" military work, though, early tests have shown that generative AI might not always be the most efficient use of the military budget.&lt;/p&gt;
&lt;p&gt;"There are many times that we find folks using this technology to answer something that we could just do in a spreadsheet with one math problem, and we’re paying a lot more money to do it,” Army CIO Leonel Garciga told DefenseScoop in August. "Is the juice worth the squeeze? Or is there another way to get at the same problem that may be less cool from a tech perspective, but more viable from an execution perspective?"&lt;/p&gt;
&lt;p&gt;In 2023, the US State Department listed the best practices for military use of AI, focused on ethical and responsible deployment of AI tools within a human chain of command. The report stressed that humans should remain in control of "decisions concerning nuclear weapons employment" and should maintain the capability to "disengage or deactivate deployed systems that demonstrate unintended behavior."&lt;/p&gt;
&lt;p&gt;Since then, the military has shown interest in using AI technology in the field for everything from automated targeting systems on drones to "improving situational awareness" via an OpenAI partnership with military contractor Anduril. In January 2024, OpenAI removed a prohibition on "military and warfare uses" from ChatGPT's usage policies, while still barring customers from "devlop[ing] or us[ing] weapons" via the LLM.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/10/army-general-says-hes-using-ai-to-improve-decision-making/</guid><pubDate>Wed, 15 Oct 2025 21:31:00 +0000</pubDate></item></channel></rss>