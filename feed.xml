<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 04 Sep 2025 18:28:53 +0000</lastBuildDate><item><title>[NEW]  ()</title><link>https://www.technologyreview.com/topic/artificial-intelligence/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/topic/artificial-intelligence/feed/</guid></item><item><title> ()</title><link>https://venturebeat.com/category/ai/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://venturebeat.com/category/ai/feed/</guid></item><item><title>Resham Kotecha, Open Data Institute: How the EU can lead in AI (AI News)</title><link>https://www.artificialintelligence-news.com/news/how-the-eu-can-lead-in-ai/</link><description>&lt;p&gt;The EU has a chance to shape how the world approaches AI and data governance. &lt;em&gt;AI News&lt;/em&gt; spoke with Resham Kotecha, Global Head of Policy at the Open Data Institute (ODI), who said that opportunity lies in proving that protecting people’s rights and supporting innovation can go hand in hand.&lt;/p&gt;&lt;p&gt;The ODI’s European Data and AI Policy Manifesto sets out six principles for policymakers, calling for strong governance, inclusive ecosystems, and public participation to guide AI development.&lt;/p&gt;&lt;h3&gt;Setting standards in AI and data&lt;/h3&gt;&lt;p&gt;“The EU has a unique opportunity to shape a global benchmark for digital governance that puts people first,” Kotecha said. The manifesto’s first principle makes clear that innovation and competitiveness must be built on regulation that safeguards people and strengthens trust.&lt;/p&gt;&lt;figure class="wp-block-image alignright size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-109232" height="437" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/09/Resham-Kotecha-Global-Head-of-Policy-at-the-Open-Data-Institute-ODI.jpg" width="437" /&gt;&lt;figcaption class="wp-element-caption"&gt;Resham Kotecha, Global Head of Policy at the Open Data Institute (ODI).&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Common European Data Spaces and Gaia-X are early examples of how the EU is building the foundations for AI development while protecting rights. The initiatives aim to create shared infrastructure that lets governments, businesses, and researchers pool data without giving up control. If they succeed, Europe could combine large-scale data use with strong protections for privacy and security.&lt;/p&gt;&lt;p&gt;Privacy-enhancing technologies (PETs) are another piece of the puzzle. The tools allow organisations to analyse or share insights from sensitive datasets without exposing the raw data itself. Horizon Europe and Digital Europe already support research and deployment of PETs. What is needed now, Kotecha argued, is consistency: “Making sure PETs move out of pilots and into mainstream use.” That shift would allow firms to use more data responsibly and show citizens their rights are taken seriously.&lt;/p&gt;&lt;p&gt;Trust will also depend on oversight. Independent organisations, Kotecha said, provide the checks and balances needed for trustworthy AI. “They offer impartial scrutiny, build public confidence, and hold both governments and industry accountable.” The ODI’s own Data Institutions Programme offers guidance on how these bodies can be structured and supported.&lt;/p&gt;&lt;h3&gt;Open data as the EU’s foundation for AI&lt;/h3&gt;&lt;p&gt;The manifesto calls open data a foundation for responsible AI, but many businesses remain wary of sharing. Concerns range from commercial risks and legal uncertainty to worries about quality and format. Even when data is published, it is often unstructured or inconsistent, making it hard to use.&lt;/p&gt;&lt;p&gt;Kotecha argued the EU should reduce the costs organisations face in collecting, using, and sharing data for AI. “The EU should explore a range of interventions, including combining legislative frameworks, financial incentives, capacity building, and data infrastructure development,” she said. By lowering barriers, Europe could encourage private organisations to share more data responsibly, creating both public and economic benefits.&lt;/p&gt;&lt;p&gt;The ODI’s research shows that clear communication matters. Senior decision-makers need to see tangible business benefits of data sharing, not just broad ‘public good’ arguments. At the same time, sensitivities around commercial data need to be addressed.&lt;/p&gt;&lt;p&gt;Useful structures already exist – the Data Spaces Support Centre (DSSC) and the International Data Spaces Association (IDSA) are building governance and technical frameworks that make sharing safer and easier. Updates to the Data Governance Act (DGA) and GDPR are also clarifying permissions for responsible reuse.&lt;/p&gt;&lt;p&gt;Regulatory sandboxes can build on this foundation. By letting firms test new approaches in a controlled environment, sandboxes can demonstrate that public benefit and commercial value are not in conflict. Privacy-enhancing technologies add another layer of safety by enabling the sharing of sensitive data without exposing individuals to risk.&lt;/p&gt;&lt;h3&gt;Building EU-wide trust and cross-border AI ecosystems&lt;/h3&gt;&lt;p&gt;One of the biggest hurdles for Europe is making data work inside member countries. Legal uncertainty, diverging national standards, and inconsistent governance fragment any system.&lt;/p&gt;&lt;p&gt;The Data Governance Act is central to the EU’s plan to create trusted, cross-border AI ecosystems. But laws on their own will not solve the problem. “The real test will be in how consistently member states implement [the Data Governance Act], and how much support is given to organisations that want to participate,” Kotecha said. If Europe can align on standards and execution, it could strengthen its AI ecosystem and set the global standard for trustworthy cross-border data flows.&lt;/p&gt;&lt;p&gt;That will require more than technical fixes – building trust between governments, businesses, and civil society is just as important. For Kotecha, the solution lies in creating “an open and trustworthy data ecosystem, where collaboration helps to maximise data value while managing risks connected with cross-border sharing.”&lt;/p&gt;&lt;h3&gt;Independence through funding and governance&lt;/h3&gt;&lt;p&gt;Oversight of AI systems requires sustainable structures. Without long-term funding, independent organisations risk becoming project-based consultancies rather than consistent watchdogs. “Civil society and independent organisations need commitments for long-term, strategic funding streams to carry out oversight, not just project-based support,” Kotecha said.&lt;/p&gt;&lt;p&gt;The ODI’s Data Institutions Programme has explored governance models that keep organisations independent while enabling them to steward data responsibly. “Independence relies on more than money. It requires transparency, ethical oversight, inclusion in political decision-making, and accountability structures that keep organisations anchored in the public interest,” Kotecha said.&lt;/p&gt;&lt;p&gt;Embedding such principles into EU funding models might ensure oversight bodies remain independent and effective. Strong governance should include ethical oversight, risk management, transparency, and clear roles, handled by board sub-committees on ethics, audit, and remuneration.&lt;/p&gt;&lt;h3&gt;Making data work for startups&lt;/h3&gt;&lt;p&gt;Access to valuable datasets is often limited to major tech firms. Smaller players struggle with the cost and complexity of acquiring high-value data. This is where initiatives like AI Factories and Data Labs come in. Designed to lower barriers, they give startups curated datasets, tools, and expertise that would otherwise be out of reach.&lt;/p&gt;&lt;p&gt;The model has worked before; like Data Pitch, a project that paired SMEs and startups with data from large organisations. That helped unlock previously closed datasets. Over three years, it supported 47 startups from 13 countries, helped create more than 100 new jobs, and generated €18 million in sales and investments.&lt;/p&gt;&lt;p&gt;The ODI’s OpenActive initiative showed a similar impact in the fitness and health sector, using open standards to power dozens of SME-built apps. At a European level, DSSC pilots and new sector-specific data spaces in areas like mobility and health are starting to create similar opportunities. For Kotecha, the challenge now is ensuring these schemes “genuinely lower barriers for smaller players, so they can build innovative products or services based on high-value data.”&lt;/p&gt;&lt;h3&gt;Bringing communities into the conversation&lt;/h3&gt;&lt;p&gt;The manifesto also stresses that the EU’s AI ecosystem will only succeed if public understanding and participation are built-in. Kotecha argued that engagement cannot be top-down or tokenistic. “Participatory data initiatives empower people to play an active role in the data ecosystem,” she said.&lt;/p&gt;&lt;p&gt;The ODI’s 2024 report &lt;em&gt;What makes participatory data initiatives successful?&lt;/em&gt; maps out how communities can be involved directly in data collection, sharing, and governance. It found that local participation strengthens ownership and gives under-represented groups influence.&lt;/p&gt;&lt;p&gt;In practice, this could mean community-led health data projects, like those supported by the ODI, or open standards that are embedded in everyday tools like activity finders and social prescribing platforms. These approaches raise awareness and give people agency.&lt;/p&gt;&lt;p&gt;Effective participation requires training and resources so communities can understand and shape how data is used. Representation must also reflect the diversity of the community itself, using trusted local champions and culturally relevant methods. Technology should be accessible, whether low-tech or offline, and communication should be clear about how data is protected.&lt;/p&gt;&lt;p&gt;“If the EU wants to reach under-represented groups, it should back participatory approaches that start from local priorities, use trusted intermediaries, and build in transparency from the outset,” Kotecha said. “That’s how we turn data literacy into real influence.”&lt;/p&gt;&lt;h3&gt;Why trust could be the EU’s competitive advantage in AI&lt;/h3&gt;&lt;p&gt;The manifesto argues that Europe has an opportunity. “The EU has a unique chance to prove that trust is a competitive advantage in AI,” Kotecha said. By showing that open data, independent oversight, inclusive ecosystems, and data skills development are central to AI economies, Europe can prove that protecting rights and fostering innovation are not opposites.&lt;/p&gt;&lt;p&gt;This position would stand in contrast with other digital powers. In the US, regulation remains fragmented. In China, state-driven models raise concerns about surveillance and human rights. By setting clear and principled rules for responsible AI, the EU could turn regulation into soft power, exporting a governance model that others might adopt.&lt;/p&gt;&lt;p&gt;For Kotecha, this is not just about rules but about shaping the future: “Europe can position itself not just as a rule-maker, but as a global standard-setter for trustworthy AI.”&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Christian Lue)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Agentic AI: Promise, scepticism, and its meaning for Southeast Asia&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-109231" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/09/image-4.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;The EU has a chance to shape how the world approaches AI and data governance. &lt;em&gt;AI News&lt;/em&gt; spoke with Resham Kotecha, Global Head of Policy at the Open Data Institute (ODI), who said that opportunity lies in proving that protecting people’s rights and supporting innovation can go hand in hand.&lt;/p&gt;&lt;p&gt;The ODI’s European Data and AI Policy Manifesto sets out six principles for policymakers, calling for strong governance, inclusive ecosystems, and public participation to guide AI development.&lt;/p&gt;&lt;h3&gt;Setting standards in AI and data&lt;/h3&gt;&lt;p&gt;“The EU has a unique opportunity to shape a global benchmark for digital governance that puts people first,” Kotecha said. The manifesto’s first principle makes clear that innovation and competitiveness must be built on regulation that safeguards people and strengthens trust.&lt;/p&gt;&lt;figure class="wp-block-image alignright size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-109232" height="437" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/09/Resham-Kotecha-Global-Head-of-Policy-at-the-Open-Data-Institute-ODI.jpg" width="437" /&gt;&lt;figcaption class="wp-element-caption"&gt;Resham Kotecha, Global Head of Policy at the Open Data Institute (ODI).&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Common European Data Spaces and Gaia-X are early examples of how the EU is building the foundations for AI development while protecting rights. The initiatives aim to create shared infrastructure that lets governments, businesses, and researchers pool data without giving up control. If they succeed, Europe could combine large-scale data use with strong protections for privacy and security.&lt;/p&gt;&lt;p&gt;Privacy-enhancing technologies (PETs) are another piece of the puzzle. The tools allow organisations to analyse or share insights from sensitive datasets without exposing the raw data itself. Horizon Europe and Digital Europe already support research and deployment of PETs. What is needed now, Kotecha argued, is consistency: “Making sure PETs move out of pilots and into mainstream use.” That shift would allow firms to use more data responsibly and show citizens their rights are taken seriously.&lt;/p&gt;&lt;p&gt;Trust will also depend on oversight. Independent organisations, Kotecha said, provide the checks and balances needed for trustworthy AI. “They offer impartial scrutiny, build public confidence, and hold both governments and industry accountable.” The ODI’s own Data Institutions Programme offers guidance on how these bodies can be structured and supported.&lt;/p&gt;&lt;h3&gt;Open data as the EU’s foundation for AI&lt;/h3&gt;&lt;p&gt;The manifesto calls open data a foundation for responsible AI, but many businesses remain wary of sharing. Concerns range from commercial risks and legal uncertainty to worries about quality and format. Even when data is published, it is often unstructured or inconsistent, making it hard to use.&lt;/p&gt;&lt;p&gt;Kotecha argued the EU should reduce the costs organisations face in collecting, using, and sharing data for AI. “The EU should explore a range of interventions, including combining legislative frameworks, financial incentives, capacity building, and data infrastructure development,” she said. By lowering barriers, Europe could encourage private organisations to share more data responsibly, creating both public and economic benefits.&lt;/p&gt;&lt;p&gt;The ODI’s research shows that clear communication matters. Senior decision-makers need to see tangible business benefits of data sharing, not just broad ‘public good’ arguments. At the same time, sensitivities around commercial data need to be addressed.&lt;/p&gt;&lt;p&gt;Useful structures already exist – the Data Spaces Support Centre (DSSC) and the International Data Spaces Association (IDSA) are building governance and technical frameworks that make sharing safer and easier. Updates to the Data Governance Act (DGA) and GDPR are also clarifying permissions for responsible reuse.&lt;/p&gt;&lt;p&gt;Regulatory sandboxes can build on this foundation. By letting firms test new approaches in a controlled environment, sandboxes can demonstrate that public benefit and commercial value are not in conflict. Privacy-enhancing technologies add another layer of safety by enabling the sharing of sensitive data without exposing individuals to risk.&lt;/p&gt;&lt;h3&gt;Building EU-wide trust and cross-border AI ecosystems&lt;/h3&gt;&lt;p&gt;One of the biggest hurdles for Europe is making data work inside member countries. Legal uncertainty, diverging national standards, and inconsistent governance fragment any system.&lt;/p&gt;&lt;p&gt;The Data Governance Act is central to the EU’s plan to create trusted, cross-border AI ecosystems. But laws on their own will not solve the problem. “The real test will be in how consistently member states implement [the Data Governance Act], and how much support is given to organisations that want to participate,” Kotecha said. If Europe can align on standards and execution, it could strengthen its AI ecosystem and set the global standard for trustworthy cross-border data flows.&lt;/p&gt;&lt;p&gt;That will require more than technical fixes – building trust between governments, businesses, and civil society is just as important. For Kotecha, the solution lies in creating “an open and trustworthy data ecosystem, where collaboration helps to maximise data value while managing risks connected with cross-border sharing.”&lt;/p&gt;&lt;h3&gt;Independence through funding and governance&lt;/h3&gt;&lt;p&gt;Oversight of AI systems requires sustainable structures. Without long-term funding, independent organisations risk becoming project-based consultancies rather than consistent watchdogs. “Civil society and independent organisations need commitments for long-term, strategic funding streams to carry out oversight, not just project-based support,” Kotecha said.&lt;/p&gt;&lt;p&gt;The ODI’s Data Institutions Programme has explored governance models that keep organisations independent while enabling them to steward data responsibly. “Independence relies on more than money. It requires transparency, ethical oversight, inclusion in political decision-making, and accountability structures that keep organisations anchored in the public interest,” Kotecha said.&lt;/p&gt;&lt;p&gt;Embedding such principles into EU funding models might ensure oversight bodies remain independent and effective. Strong governance should include ethical oversight, risk management, transparency, and clear roles, handled by board sub-committees on ethics, audit, and remuneration.&lt;/p&gt;&lt;h3&gt;Making data work for startups&lt;/h3&gt;&lt;p&gt;Access to valuable datasets is often limited to major tech firms. Smaller players struggle with the cost and complexity of acquiring high-value data. This is where initiatives like AI Factories and Data Labs come in. Designed to lower barriers, they give startups curated datasets, tools, and expertise that would otherwise be out of reach.&lt;/p&gt;&lt;p&gt;The model has worked before; like Data Pitch, a project that paired SMEs and startups with data from large organisations. That helped unlock previously closed datasets. Over three years, it supported 47 startups from 13 countries, helped create more than 100 new jobs, and generated €18 million in sales and investments.&lt;/p&gt;&lt;p&gt;The ODI’s OpenActive initiative showed a similar impact in the fitness and health sector, using open standards to power dozens of SME-built apps. At a European level, DSSC pilots and new sector-specific data spaces in areas like mobility and health are starting to create similar opportunities. For Kotecha, the challenge now is ensuring these schemes “genuinely lower barriers for smaller players, so they can build innovative products or services based on high-value data.”&lt;/p&gt;&lt;h3&gt;Bringing communities into the conversation&lt;/h3&gt;&lt;p&gt;The manifesto also stresses that the EU’s AI ecosystem will only succeed if public understanding and participation are built-in. Kotecha argued that engagement cannot be top-down or tokenistic. “Participatory data initiatives empower people to play an active role in the data ecosystem,” she said.&lt;/p&gt;&lt;p&gt;The ODI’s 2024 report &lt;em&gt;What makes participatory data initiatives successful?&lt;/em&gt; maps out how communities can be involved directly in data collection, sharing, and governance. It found that local participation strengthens ownership and gives under-represented groups influence.&lt;/p&gt;&lt;p&gt;In practice, this could mean community-led health data projects, like those supported by the ODI, or open standards that are embedded in everyday tools like activity finders and social prescribing platforms. These approaches raise awareness and give people agency.&lt;/p&gt;&lt;p&gt;Effective participation requires training and resources so communities can understand and shape how data is used. Representation must also reflect the diversity of the community itself, using trusted local champions and culturally relevant methods. Technology should be accessible, whether low-tech or offline, and communication should be clear about how data is protected.&lt;/p&gt;&lt;p&gt;“If the EU wants to reach under-represented groups, it should back participatory approaches that start from local priorities, use trusted intermediaries, and build in transparency from the outset,” Kotecha said. “That’s how we turn data literacy into real influence.”&lt;/p&gt;&lt;h3&gt;Why trust could be the EU’s competitive advantage in AI&lt;/h3&gt;&lt;p&gt;The manifesto argues that Europe has an opportunity. “The EU has a unique chance to prove that trust is a competitive advantage in AI,” Kotecha said. By showing that open data, independent oversight, inclusive ecosystems, and data skills development are central to AI economies, Europe can prove that protecting rights and fostering innovation are not opposites.&lt;/p&gt;&lt;p&gt;This position would stand in contrast with other digital powers. In the US, regulation remains fragmented. In China, state-driven models raise concerns about surveillance and human rights. By setting clear and principled rules for responsible AI, the EU could turn regulation into soft power, exporting a governance model that others might adopt.&lt;/p&gt;&lt;p&gt;For Kotecha, this is not just about rules but about shaping the future: “Europe can position itself not just as a rule-maker, but as a global standard-setter for trustworthy AI.”&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Christian Lue)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Agentic AI: Promise, scepticism, and its meaning for Southeast Asia&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-109231" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/09/image-4.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/how-the-eu-can-lead-in-ai/</guid><pubDate>Thu, 04 Sep 2025 08:42:06 +0000</pubDate></item><item><title>From minutes to milliseconds: How CrateDB is tackling AI data infrastructure (AI News)</title><link>https://www.artificialintelligence-news.com/news/from-minutes-to-milliseconds-how-cratedb-is-tackling-ai-data-infrastructure/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/09/Untitled-design-54.png" /&gt;&lt;/div&gt;&lt;p&gt;The promise of AI remains immense – but one thing might be holding it back. “The infrastructure that powers AI today won’t sustain tomorrow’s demands,” a recent CIO.com article leads. “CIOs must rethink how to scale smarter – not just bigger – or risk falling behind.”&lt;/p&gt;&lt;p&gt;CrateDB agrees – and the database firm is betting on solving the problem by being a ‘unified data layer for analytics, search, and AI.’&lt;/p&gt;&lt;p&gt;“The challenge is that most IT systems are relying, or have been built, around batch pipeline or asynchronous pipeline, and now you need to reduce the time between the production and the consumption of the data,” Stephane Castellani, SVP marketing, explains. “CrateDB is a very good fit because it really can give you insights to the right data with also a large volume and complexity of formats in a matter of milliseconds.”&lt;/p&gt;&lt;p&gt;A blog post notes the four-step process for CrateDB to act as the ‘connective tissue between operational data and AI systems’; from ingestion, to real-time aggregation and insight, to serving data to AI pipelines, to enabling feedback loops between models and data. The velocity and variety of data is key; Castellani notes the reduction of query times from minutes to milliseconds. In manufacturing, telemetry can be collected from machines in real-time, enabling greater learning for predictive maintenance models.&lt;/p&gt;&lt;p&gt;There is another benefit, as Castellani explains. “Some also use CrateDB in the factory for knowledge assistance,” he says. “If something goes wrong, you have a specific error message appear on your machine and say ‘I’m not an expert with this machine, what does it mean and how can I fix it?’, [you] can ask a knowledge assistant, that is also relying on CrateDB as a vector database, to get access to the information, and pull the right manual and right instructions to react in real-time.”&lt;/p&gt;&lt;p&gt;AI, however, does not stand still for long; “we don’t know what [it] is going to look like in a few months, or even a few weeks”, notes Castellani. Organisations are looking to move towards fully agentic AI workflows with greater autonomy, yet according to recent PYMENTS Intelligence research, manufacturing – as part of the wider goods and services industry – are lagging. CrateDB has partnered with Tech Mahindra on this front to help provide agentic AI solutions for automotive, manufacturing, and smart factories.&lt;/p&gt;&lt;p&gt;Castellani notes excitement about the Model Context Protocol (MCP), which standardises how applications provide context to large language models (LLMs). He likens it to the trend around enterprise APIs 12 years ago. CrateDB’s MCP Server, which is still at the experimental stage, serves as a bridge between AI tools and the analytics database. “When we talk about MCP it’s pretty much the same approach [as APIs] but for LLMs,” he explains.&lt;/p&gt;&lt;p&gt;Tech Mahindra is just one of the key partnerships going forward for CrateDB. “We keep focusing on our basics,” Castellani adds. “Performance, scalability… investing into our capacity to ingest data from more and more data sources, and always minimis[ing] the latency, both on the ingestion and query side.”&lt;/p&gt;&lt;p&gt;&lt;em&gt;Stephane Castellani will be speaking at AI &amp;amp; Big Data Expo Europe on the topic of &lt;/em&gt;&lt;em&gt;Bringing AI to Real-Time Data – Text2SQL, RAG, and TAG with CrateDB&lt;/em&gt;&lt;em&gt;, and IoT Tech Expo Europe on the topic of &lt;/em&gt;&lt;em&gt;Smarter IoT Operations: Real-Time Wind Farm Analytics and AI-Driven Diagnostics&lt;/em&gt;&lt;em&gt;. You can watch the full interview with Stephane below:&lt;/em&gt;&lt;/p&gt;&lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/09/CrateDB_TechEx.mp4"&gt;&lt;/video&gt;&lt;/figure&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/09/Untitled-design-54.png" /&gt;&lt;/div&gt;&lt;p&gt;The promise of AI remains immense – but one thing might be holding it back. “The infrastructure that powers AI today won’t sustain tomorrow’s demands,” a recent CIO.com article leads. “CIOs must rethink how to scale smarter – not just bigger – or risk falling behind.”&lt;/p&gt;&lt;p&gt;CrateDB agrees – and the database firm is betting on solving the problem by being a ‘unified data layer for analytics, search, and AI.’&lt;/p&gt;&lt;p&gt;“The challenge is that most IT systems are relying, or have been built, around batch pipeline or asynchronous pipeline, and now you need to reduce the time between the production and the consumption of the data,” Stephane Castellani, SVP marketing, explains. “CrateDB is a very good fit because it really can give you insights to the right data with also a large volume and complexity of formats in a matter of milliseconds.”&lt;/p&gt;&lt;p&gt;A blog post notes the four-step process for CrateDB to act as the ‘connective tissue between operational data and AI systems’; from ingestion, to real-time aggregation and insight, to serving data to AI pipelines, to enabling feedback loops between models and data. The velocity and variety of data is key; Castellani notes the reduction of query times from minutes to milliseconds. In manufacturing, telemetry can be collected from machines in real-time, enabling greater learning for predictive maintenance models.&lt;/p&gt;&lt;p&gt;There is another benefit, as Castellani explains. “Some also use CrateDB in the factory for knowledge assistance,” he says. “If something goes wrong, you have a specific error message appear on your machine and say ‘I’m not an expert with this machine, what does it mean and how can I fix it?’, [you] can ask a knowledge assistant, that is also relying on CrateDB as a vector database, to get access to the information, and pull the right manual and right instructions to react in real-time.”&lt;/p&gt;&lt;p&gt;AI, however, does not stand still for long; “we don’t know what [it] is going to look like in a few months, or even a few weeks”, notes Castellani. Organisations are looking to move towards fully agentic AI workflows with greater autonomy, yet according to recent PYMENTS Intelligence research, manufacturing – as part of the wider goods and services industry – are lagging. CrateDB has partnered with Tech Mahindra on this front to help provide agentic AI solutions for automotive, manufacturing, and smart factories.&lt;/p&gt;&lt;p&gt;Castellani notes excitement about the Model Context Protocol (MCP), which standardises how applications provide context to large language models (LLMs). He likens it to the trend around enterprise APIs 12 years ago. CrateDB’s MCP Server, which is still at the experimental stage, serves as a bridge between AI tools and the analytics database. “When we talk about MCP it’s pretty much the same approach [as APIs] but for LLMs,” he explains.&lt;/p&gt;&lt;p&gt;Tech Mahindra is just one of the key partnerships going forward for CrateDB. “We keep focusing on our basics,” Castellani adds. “Performance, scalability… investing into our capacity to ingest data from more and more data sources, and always minimis[ing] the latency, both on the ingestion and query side.”&lt;/p&gt;&lt;p&gt;&lt;em&gt;Stephane Castellani will be speaking at AI &amp;amp; Big Data Expo Europe on the topic of &lt;/em&gt;&lt;em&gt;Bringing AI to Real-Time Data – Text2SQL, RAG, and TAG with CrateDB&lt;/em&gt;&lt;em&gt;, and IoT Tech Expo Europe on the topic of &lt;/em&gt;&lt;em&gt;Smarter IoT Operations: Real-Time Wind Farm Analytics and AI-Driven Diagnostics&lt;/em&gt;&lt;em&gt;. You can watch the full interview with Stephane below:&lt;/em&gt;&lt;/p&gt;&lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/09/CrateDB_TechEx.mp4"&gt;&lt;/video&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/from-minutes-to-milliseconds-how-cratedb-is-tackling-ai-data-infrastructure/</guid><pubDate>Thu, 04 Sep 2025 08:57:14 +0000</pubDate></item><item><title>Switzerland releases 100% open AI model (AI News)</title><link>https://www.artificialintelligence-news.com/news/switzerland-releases-its-own-fully-open-ai-model/</link><description>&lt;p&gt;A group of Swiss institutions has released a new open AI model, designed to serve as a foundation for future research and applications. Built by EPFL, ETH Zurich, and the Swiss National Supercomputing Centre (CSCS), the model is called Apertus – Latin for “open.” The name reflects its core principle: every part of its design and training process is accessible to the public.&lt;/p&gt;&lt;p&gt;Developers and organisations can use Apertus to create chatbots, translation tools, or education-focused applications. It can be downloaded directly from Hugging Face or accessed through Swisscom, a strategic partner of the initiative. Two versions are available – an 8-billion-parameter model and a larger 70-billion-parameter version. Both are released under a permissive open-source licence, allowing use in research, education, and commercial projects.&lt;/p&gt;&lt;h3&gt;Built for openness&lt;/h3&gt;&lt;p&gt;Unlike other AI systems that reveal only select details, Apertus is a fully open AI model, with its architecture, training data, and documentation available for inspection.&lt;/p&gt;&lt;p&gt;“With this release, we aim to provide a blueprint for how a trustworthy, sovereign, and inclusive AI model can be developed,” said Martin Jaggi, Professor of Machine Learning at EPFL and member of the Steering Committee of the Swiss AI Initiative. He said Apertus will be updated regularly by a team of engineers and researchers from CSCS, ETH Zurich, and EPFL.&lt;/p&gt;&lt;p&gt;Thomas Schulthess, Director of CSCS and Professor at ETH Zurich, described Apertus as “a driver of innovation and a means of strengthening AI expertise in research, society and industry.” He said the project is not a typical transfer of technology from research to product, but an effort to build infrastructure for long-term use.&lt;/p&gt;&lt;h3&gt;Multilingual reach&lt;/h3&gt;&lt;p&gt;The training process involved 15 trillion tokens in more than 1,000 languages, with about 40% of the data in non-English languages. Apertus includes languages often left out of LLMs, like Swiss German and Romansh.&lt;/p&gt;&lt;p&gt;“Apertus is built for the public good. It stands among the few fully open LLMs at this scale and is the first of its kind to embody multilingualism, transparency, and compliance as foundational design principles,” said Imanol Schlag, technical lead of the project and Research Scientist at ETH Zurich.&lt;/p&gt;&lt;p&gt;Swisscom is already deploying Apertus on its sovereign AI platform. “This underscores our commitment to shaping a secure and responsible AI ecosystem that serves the public interest and strengthens Switzerland’s digital sovereignty,” said Daniel Dobos, Research Director at Swisscom.&lt;/p&gt;&lt;h3&gt;Testing the open AI model: Access and real-world use&lt;/h3&gt;&lt;p&gt;While downloading Apertus is simple for experienced users, practical use requires servers, cloud resources, or dedicated interfaces. Developers will be able to test Apertus during the Swiss {ai} Weeks which continue until October 5, 2025. Hackathon participants will gain access through a Swisscom-hosted interface. Swisscom business customers can also begin using the model today through the company’s AI platform. For international users, Apertus will be available through the Public AI Inference Utility.&lt;/p&gt;&lt;p&gt;“Currently, Apertus is the leading public AI model: a model built by public institutions, for the public interest. It is our best proof yet that AI can be a form of public infrastructure like highways, water, or electricity,” said Joshua Tan, Lead Maintainer of the Public AI Inference Utility.&lt;/p&gt;&lt;h3&gt;Transparency and compliance&lt;/h3&gt;&lt;p&gt;Under the open-source licence training data, model weights, and intermediate checkpoints are available. The model’s training process followed Swiss data protection rules, Swiss copyright law, and the transparency requirements of the EU AI Act.&lt;/p&gt;&lt;p&gt;The dataset was limited to publicly-available information, filtered to remove personal data and to honour website opt-out requests. Ethical guidelines were also applied to exclude unwanted material before training began.&lt;/p&gt;&lt;h3&gt;The future of Switzerland’s open AI model&lt;/h3&gt;&lt;p&gt;“Apertus demonstrates that generative AI can be both powerful and open,” said Antoine Bosselut, Professor at EPFL and Co-Lead of the Swiss AI Initiative. “The release of Apertus is not a final step, rather it’s the beginning of a journey, a long-term commitment to open, trustworthy, and sovereign AI foundations, for the public good worldwide.”&lt;/p&gt;&lt;p&gt;Future updates aim to expand the model family, improve efficiency, and develop domain-specific tools for areas like law, health, climate, and education – while continuing to uphold strict standards of transparency.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Cory Johnson)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Microsoft gives free Copilot AI services to US government workers&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-109227" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/09/image-3.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;A group of Swiss institutions has released a new open AI model, designed to serve as a foundation for future research and applications. Built by EPFL, ETH Zurich, and the Swiss National Supercomputing Centre (CSCS), the model is called Apertus – Latin for “open.” The name reflects its core principle: every part of its design and training process is accessible to the public.&lt;/p&gt;&lt;p&gt;Developers and organisations can use Apertus to create chatbots, translation tools, or education-focused applications. It can be downloaded directly from Hugging Face or accessed through Swisscom, a strategic partner of the initiative. Two versions are available – an 8-billion-parameter model and a larger 70-billion-parameter version. Both are released under a permissive open-source licence, allowing use in research, education, and commercial projects.&lt;/p&gt;&lt;h3&gt;Built for openness&lt;/h3&gt;&lt;p&gt;Unlike other AI systems that reveal only select details, Apertus is a fully open AI model, with its architecture, training data, and documentation available for inspection.&lt;/p&gt;&lt;p&gt;“With this release, we aim to provide a blueprint for how a trustworthy, sovereign, and inclusive AI model can be developed,” said Martin Jaggi, Professor of Machine Learning at EPFL and member of the Steering Committee of the Swiss AI Initiative. He said Apertus will be updated regularly by a team of engineers and researchers from CSCS, ETH Zurich, and EPFL.&lt;/p&gt;&lt;p&gt;Thomas Schulthess, Director of CSCS and Professor at ETH Zurich, described Apertus as “a driver of innovation and a means of strengthening AI expertise in research, society and industry.” He said the project is not a typical transfer of technology from research to product, but an effort to build infrastructure for long-term use.&lt;/p&gt;&lt;h3&gt;Multilingual reach&lt;/h3&gt;&lt;p&gt;The training process involved 15 trillion tokens in more than 1,000 languages, with about 40% of the data in non-English languages. Apertus includes languages often left out of LLMs, like Swiss German and Romansh.&lt;/p&gt;&lt;p&gt;“Apertus is built for the public good. It stands among the few fully open LLMs at this scale and is the first of its kind to embody multilingualism, transparency, and compliance as foundational design principles,” said Imanol Schlag, technical lead of the project and Research Scientist at ETH Zurich.&lt;/p&gt;&lt;p&gt;Swisscom is already deploying Apertus on its sovereign AI platform. “This underscores our commitment to shaping a secure and responsible AI ecosystem that serves the public interest and strengthens Switzerland’s digital sovereignty,” said Daniel Dobos, Research Director at Swisscom.&lt;/p&gt;&lt;h3&gt;Testing the open AI model: Access and real-world use&lt;/h3&gt;&lt;p&gt;While downloading Apertus is simple for experienced users, practical use requires servers, cloud resources, or dedicated interfaces. Developers will be able to test Apertus during the Swiss {ai} Weeks which continue until October 5, 2025. Hackathon participants will gain access through a Swisscom-hosted interface. Swisscom business customers can also begin using the model today through the company’s AI platform. For international users, Apertus will be available through the Public AI Inference Utility.&lt;/p&gt;&lt;p&gt;“Currently, Apertus is the leading public AI model: a model built by public institutions, for the public interest. It is our best proof yet that AI can be a form of public infrastructure like highways, water, or electricity,” said Joshua Tan, Lead Maintainer of the Public AI Inference Utility.&lt;/p&gt;&lt;h3&gt;Transparency and compliance&lt;/h3&gt;&lt;p&gt;Under the open-source licence training data, model weights, and intermediate checkpoints are available. The model’s training process followed Swiss data protection rules, Swiss copyright law, and the transparency requirements of the EU AI Act.&lt;/p&gt;&lt;p&gt;The dataset was limited to publicly-available information, filtered to remove personal data and to honour website opt-out requests. Ethical guidelines were also applied to exclude unwanted material before training began.&lt;/p&gt;&lt;h3&gt;The future of Switzerland’s open AI model&lt;/h3&gt;&lt;p&gt;“Apertus demonstrates that generative AI can be both powerful and open,” said Antoine Bosselut, Professor at EPFL and Co-Lead of the Swiss AI Initiative. “The release of Apertus is not a final step, rather it’s the beginning of a journey, a long-term commitment to open, trustworthy, and sovereign AI foundations, for the public good worldwide.”&lt;/p&gt;&lt;p&gt;Future updates aim to expand the model family, improve efficiency, and develop domain-specific tools for areas like law, health, climate, and education – while continuing to uphold strict standards of transparency.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Cory Johnson)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Microsoft gives free Copilot AI services to US government workers&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-109227" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/09/image-3.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/switzerland-releases-its-own-fully-open-ai-model/</guid><pubDate>Thu, 04 Sep 2025 09:39:49 +0000</pubDate></item><item><title>How Trump is helping China extend its massive lead in clean energy (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/09/04/1123014/how-trump-is-helping-china-extend-its-massive-lead-in-clean-energy/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/GettyImages-2199791835.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;On a spring day in 1954, Bell Labs researchers showed off the first practical solar panels at a press conference in Murray Hill, New Jersey, using sunlight to spin a toy Ferris wheel before a stunned crowd.&lt;/p&gt;  &lt;p&gt;The solar future looked bright. But in the race to commercialize the technology it invented, the US would lose resoundingly. Last year, China exported $40 billion worth of solar panels and modules, while America shipped just $69 million, according to the &lt;em&gt;New York Times&lt;/em&gt;. It was a stunning forfeit of a huge technological lead.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;And now the US seems determined to repeat the mistake. In its quest to prop up aging fossil-fuel industries, the Trump administration has slashed federal support for the emerging cleantech sector, handing his nation’s chief economic rival the most generous of gifts: an unobstructed path to locking in its control of emerging energy technologies, and a leg up in inventing the industries of the future.&lt;/p&gt;  &lt;p&gt;China’s dominance of solar was no accident. In the late 2000s, the government simply determined that the sector was a national priority. Then it leveraged deep subsidies, targeted policies, and price wars to scale up production, drive product improvements, and slash costs. It’s made similar moves in batteries, electric vehicles, and wind turbines.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Meanwhile, President Donald Trump has set to work unraveling hard-won clean-energy achievements in the US, snuffing out the gathering momentum to rebuild the nation’s energy sector in cleaner, more sustainable ways.&lt;/p&gt;  &lt;p&gt;The tax and spending bill that Trump signed into law in early July wound down the subsidies for solar and wind power contained in the Inflation Reduction Act of 2022. The legislation also cut off federal support for cleantech projects that rely too heavily on Chinese materials—a hamfisted bid to punish Chinese industries that will instead make many US projects financially unworkable.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Meanwhile, the administration has slashed federal funding for science and attacked the financial foundations of premier research universities, pulling up the roots of future energy innovations and industries.&lt;/p&gt;  &lt;p&gt;A driving motivation for many of these policies is the quest to protect the legacy energy industry based on coal, oil, and natural gas, all of which the US is geologically blessed with. But this strategy amounts to the innovator’s dilemma playing out at a national scale—a country clinging to its declining industries rather than investing in the ones that will define the future.&lt;/p&gt;  &lt;p&gt;It does not particularly matter whether Trump believes in or cares about climate change. The economic and international security imperatives to invest in modern, sustainable industries are every bit as indisputable as the chemistry of greenhouse gases.&lt;/p&gt;  &lt;p&gt;Without sustained industrial policies that reward innovation, American entrepreneurs and investors won’t risk money and time creating new businesses, developing new products, or building first-of-a-kind projects here. Indeed, venture capitalists have told me that numerous US climate-tech companies are already looking overseas, seeking markets where they can count on government support. Some fear that many other companies will fail in the coming months as subsidies disappear, developments stall, and funding flags.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;All of which will help China extend an already massive lead.&lt;/p&gt;  &lt;p&gt;The nation has installed nearly three times as many wind turbines as the US, and it generates more than twice as much solar power. It boasts five of the 10 largest EV companies in the world, and the three largest wind turbine manufacturers. China absolutely dominates the battery market, producing the vast majority of the anodes, cathodes, and battery cells that increasingly power the world’s vehicles, grids, and gadgets.&lt;br /&gt;&lt;/p&gt;  &lt;p&gt;China harnessed the clean-energy transition to clean up its skies, upgrade its domestic industries, create jobs for its citizens, strengthen trade ties, and build new markets in emerging economies. In turn, it’s using those business links to accrue soft power and extend its influence—all while the US turns it back on global institutions.&lt;/p&gt;  &lt;p&gt;These widening relationships increasingly insulate China from external pressures, including those threatened by Trump’s go-to tactic: igniting or inflaming trade wars.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;But stiff tariffs and tough talk aren’t what built the world’s largest economy and established the US as the global force in technology for more than a century. What did was deep, sustained federal investment into education, science, and research and development—the very budget items that Trump and his party have been so eager to eliminate.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Another thing&lt;/h3&gt;  &lt;p&gt;Earlier this summer, the EPA announced plans to revoke the Obama-era “endangerment finding,” the legal foundation for regulating the nation’s greenhouse-gas pollution.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The agency’s argument leans heavily on a report that rehashes decades-old climate-denial talking points to assert that rising emissions haven’t produced the harms that scientists expected. It’s a wild, Orwellian plea for you to reject the evidence of your eyes and ears in a summer that saw record heat waves in the Midwest and East and is now blanketing the West in wildfire smoke.&lt;/p&gt;  &lt;p&gt;Over the weekend, more than 85 scientists sent a point-by-point, 459-page rebuttal to the federal government, highlighting myriad ways in which the report “is biased, full of errors, and not fit to inform policy making,” as Bob Kopp, a climate scientist at Rutgers, put it on Bluesky.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;“The authors reached these flawed conclusions through selective filtering of evidence (‘cherry picking’), overemphasis of uncertainties, misquoting peer-reviewed research, and a general dismissal of the vast majority of decades of peer-reviewed research,” the dozens of reviewers found.&lt;/p&gt;&lt;p&gt;The Trump administration handpicked researchers who would write the report it wanted to support its quarrel with thermometers and justify its preordained decision to rescind the endangerment finding. But it’s legally bound to hear from others as well, notes Karen McKinnon, a climate researcher at the University of California, Los Angeles.&lt;/p&gt;  &lt;p&gt;“Luckily, there is time to take action,” McKinnon said in a statement. “Comment on the report, and contact your representatives to let them know we need to take action to bring back the tolerable summers of years past.”&lt;/p&gt;  &lt;p&gt;You can read the full report here, or NPR’s take here. And be sure to read Casey Crownhart’s earlier piece in The Spark on the endangerment finding.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark, &lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/GettyImages-2199791835.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;On a spring day in 1954, Bell Labs researchers showed off the first practical solar panels at a press conference in Murray Hill, New Jersey, using sunlight to spin a toy Ferris wheel before a stunned crowd.&lt;/p&gt;  &lt;p&gt;The solar future looked bright. But in the race to commercialize the technology it invented, the US would lose resoundingly. Last year, China exported $40 billion worth of solar panels and modules, while America shipped just $69 million, according to the &lt;em&gt;New York Times&lt;/em&gt;. It was a stunning forfeit of a huge technological lead.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;And now the US seems determined to repeat the mistake. In its quest to prop up aging fossil-fuel industries, the Trump administration has slashed federal support for the emerging cleantech sector, handing his nation’s chief economic rival the most generous of gifts: an unobstructed path to locking in its control of emerging energy technologies, and a leg up in inventing the industries of the future.&lt;/p&gt;  &lt;p&gt;China’s dominance of solar was no accident. In the late 2000s, the government simply determined that the sector was a national priority. Then it leveraged deep subsidies, targeted policies, and price wars to scale up production, drive product improvements, and slash costs. It’s made similar moves in batteries, electric vehicles, and wind turbines.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Meanwhile, President Donald Trump has set to work unraveling hard-won clean-energy achievements in the US, snuffing out the gathering momentum to rebuild the nation’s energy sector in cleaner, more sustainable ways.&lt;/p&gt;  &lt;p&gt;The tax and spending bill that Trump signed into law in early July wound down the subsidies for solar and wind power contained in the Inflation Reduction Act of 2022. The legislation also cut off federal support for cleantech projects that rely too heavily on Chinese materials—a hamfisted bid to punish Chinese industries that will instead make many US projects financially unworkable.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Meanwhile, the administration has slashed federal funding for science and attacked the financial foundations of premier research universities, pulling up the roots of future energy innovations and industries.&lt;/p&gt;  &lt;p&gt;A driving motivation for many of these policies is the quest to protect the legacy energy industry based on coal, oil, and natural gas, all of which the US is geologically blessed with. But this strategy amounts to the innovator’s dilemma playing out at a national scale—a country clinging to its declining industries rather than investing in the ones that will define the future.&lt;/p&gt;  &lt;p&gt;It does not particularly matter whether Trump believes in or cares about climate change. The economic and international security imperatives to invest in modern, sustainable industries are every bit as indisputable as the chemistry of greenhouse gases.&lt;/p&gt;  &lt;p&gt;Without sustained industrial policies that reward innovation, American entrepreneurs and investors won’t risk money and time creating new businesses, developing new products, or building first-of-a-kind projects here. Indeed, venture capitalists have told me that numerous US climate-tech companies are already looking overseas, seeking markets where they can count on government support. Some fear that many other companies will fail in the coming months as subsidies disappear, developments stall, and funding flags.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;All of which will help China extend an already massive lead.&lt;/p&gt;  &lt;p&gt;The nation has installed nearly three times as many wind turbines as the US, and it generates more than twice as much solar power. It boasts five of the 10 largest EV companies in the world, and the three largest wind turbine manufacturers. China absolutely dominates the battery market, producing the vast majority of the anodes, cathodes, and battery cells that increasingly power the world’s vehicles, grids, and gadgets.&lt;br /&gt;&lt;/p&gt;  &lt;p&gt;China harnessed the clean-energy transition to clean up its skies, upgrade its domestic industries, create jobs for its citizens, strengthen trade ties, and build new markets in emerging economies. In turn, it’s using those business links to accrue soft power and extend its influence—all while the US turns it back on global institutions.&lt;/p&gt;  &lt;p&gt;These widening relationships increasingly insulate China from external pressures, including those threatened by Trump’s go-to tactic: igniting or inflaming trade wars.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;But stiff tariffs and tough talk aren’t what built the world’s largest economy and established the US as the global force in technology for more than a century. What did was deep, sustained federal investment into education, science, and research and development—the very budget items that Trump and his party have been so eager to eliminate.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Another thing&lt;/h3&gt;  &lt;p&gt;Earlier this summer, the EPA announced plans to revoke the Obama-era “endangerment finding,” the legal foundation for regulating the nation’s greenhouse-gas pollution.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The agency’s argument leans heavily on a report that rehashes decades-old climate-denial talking points to assert that rising emissions haven’t produced the harms that scientists expected. It’s a wild, Orwellian plea for you to reject the evidence of your eyes and ears in a summer that saw record heat waves in the Midwest and East and is now blanketing the West in wildfire smoke.&lt;/p&gt;  &lt;p&gt;Over the weekend, more than 85 scientists sent a point-by-point, 459-page rebuttal to the federal government, highlighting myriad ways in which the report “is biased, full of errors, and not fit to inform policy making,” as Bob Kopp, a climate scientist at Rutgers, put it on Bluesky.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;“The authors reached these flawed conclusions through selective filtering of evidence (‘cherry picking’), overemphasis of uncertainties, misquoting peer-reviewed research, and a general dismissal of the vast majority of decades of peer-reviewed research,” the dozens of reviewers found.&lt;/p&gt;&lt;p&gt;The Trump administration handpicked researchers who would write the report it wanted to support its quarrel with thermometers and justify its preordained decision to rescind the endangerment finding. But it’s legally bound to hear from others as well, notes Karen McKinnon, a climate researcher at the University of California, Los Angeles.&lt;/p&gt;  &lt;p&gt;“Luckily, there is time to take action,” McKinnon said in a statement. “Comment on the report, and contact your representatives to let them know we need to take action to bring back the tolerable summers of years past.”&lt;/p&gt;  &lt;p&gt;You can read the full report here, or NPR’s take here. And be sure to read Casey Crownhart’s earlier piece in The Spark on the endangerment finding.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark, &lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/09/04/1123014/how-trump-is-helping-china-extend-its-massive-lead-in-clean-energy/</guid><pubDate>Thu, 04 Sep 2025 10:00:00 +0000</pubDate></item><item><title>Synthesia’s AI clones are more expressive than ever. Soon they’ll be able to talk back. (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/09/04/1123054/synthesias-ai-clones-are-more-expressive-than-ever-soon-theyll-be-able-to-talk-back/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/250826_Rhiannon_AI_clone_hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Earlier this summer, I walked through the glassy lobby of a fancy office in London, into an elevator, and then along a corridor into a clean, carpeted room. Natural light flooded in through its windows, and a large pair of umbrella-like lighting rigs made the room even brighter. I tried not to squint as I took my place in front of a tripod equipped with a large camera and a laptop displaying an autocue. I took a deep breath and started to read out the script.&lt;/p&gt;  &lt;p&gt;I’m not a newsreader or an actor auditioning for a movie—I was visiting the AI company Synthesia to give it what it needed to create a hyperrealistic AI-generated avatar of me. The company’s avatars are a decent barometer of just how dizzying progress has been in AI over the past few years, so I was curious just how accurately its latest AI model, introduced last month, could replicate me.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;When Synthesia launched in 2017, its primary purpose was to match AI versions of real human faces—for example, the former footballer David Beckham—with dubbed voices speaking in different languages. A few years later, in 2020, it started giving the companies that signed up for its services the opportunity to make professional-level presentation videos starring either AI versions of staff members or consenting actors. But the technology wasn’t perfect. The avatars’ body movements could be jerky and unnatural, their accents sometimes slipped, and the emotions indicated by their voices didn’t always match their facial expressions.&lt;/p&gt;  &lt;p&gt;Now Synthesia’s avatars have been updated with more natural mannerisms and movements, as well as expressive voices that better preserve the speaker’s accent—making them appear more humanlike than ever before. For Synthesia’s corporate clients, these avatars will make for slicker presenters of financial results, internal communications, or staff training videos.&lt;/p&gt; 
 &lt;p&gt;I found the video demonstrating my avatar as unnerving as it is technically impressive. It’s slick enough to pass as a high-definition recording of a chirpy corporate speech, and if you didn’t know me, you’d probably think that’s exactly what it was. This demonstration shows how much harder it’s becoming to distinguish the artificial from the real. And before long, these avatars will even be able to talk back to us. But how much better can they get? And what might interacting with AI clones do to us?&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;The creation process&lt;/h3&gt;  &lt;p&gt;When my former colleague Melissa visited Synthesia’s London studio to create an avatar of herself last year, she had to go through a long process of calibrating the system, reading out a script in different emotional states, and mouthing the sounds needed to help her avatar form vowels and consonants. As I stand in the brightly lit room 15 months later, I’m relieved to hear that the creation process has been significantly streamlined. Josh Baker-Mendoza, Synthesia’s technical supervisor, encourages me to gesture and move my hands as I would during natural conversation, while simultaneously warning me not to move too much. I duly repeat an overly glowing script that’s designed to encourage me to speak emotively and enthusiastically. The result is a bit as if if Steve Jobs had been resurrected as a blond British woman with a low, monotonous voice.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;It also has the unfortunate effect of making me sound like an employee of Synthesia.“I am so thrilled to be with you today to show off what we’ve been working on. We are on the edge of innovation, and the possibilities are endless,” I parrot eagerly, trying to sound lively rather than manic. “So get ready to be part of something that will make you go, ‘Wow!’ This opportunity isn’t just big—it’s monumental.”&lt;/p&gt;  &lt;p&gt;Just an hour later, the team has all the footage it needs. A couple of weeks later I receive two avatars of myself: one powered by the previous Express-1 model and the other made with the latest Express-2 technology. The latter, Synthesia claims, makes its synthetic humans more lifelike and true to the people they’re modeled on, complete with more expressive hand gestures, facial movements, and speech. You can see the results for yourself below.&amp;nbsp;&lt;/p&gt;  &lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/250826_Rhiannon_AI_clone_Personal-avatar-vs-Express-2-avatar.mp4"&gt;&lt;/video&gt;&lt;div class="video-credit"&gt;COURTESY SYNTHESIA&lt;/div&gt; &lt;/figure&gt;  &lt;p&gt;Last year, Melissa found that her Express-1-powered avatar failed to match her transatlantic accent. Its range of emotions was also limited—when she asked her avatar to read a script angrily, it sounded more whiny than furious. In the months since, Synthesia has improved Express-1, but the version of my avatar made with the same technology blinks furiously and still struggles to synchronize body movements with speech.&lt;/p&gt;  &lt;p&gt;By way of contrast, I’m struck by just how much my new Express-2 avatar looks like me: Its facial features mirror my own perfectly. Its voice is spookily accurate too, and although it gesticulates more than I do, its hand movements generally marry up with what I’m saying.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;But the tiny telltale signs of AI generation are still there if you know where to look. The palms of my hands are bright pink and as smooth as putty. Strands of hair hang stiffly around my shoulders instead of moving with me. Its eyes stare glassily ahead, rarely blinking. And although the voice is unmistakably mine, there’s something slightly off about my digital clone’s intonations and speech patterns. “This is great!” my avatar randomly declares, before slipping back into a saner register.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Anna Eiserbeck, a postdoctoral psychology researcher at the Humboldt University of Berlin who has studied how humans react to perceived deepfake faces, says she isn’t sure she’d have been able to identify my avatar as a deepfake at first glance.&lt;/p&gt;  &lt;p&gt;But she would eventually have noticed something amiss. It’s not just the small details that give it away—my oddly static earring, the way my body sometimes moves in small, abrupt jerks. It’s something that runs much deeper, she explains.&lt;/p&gt;  &lt;p&gt;“Something seemed a bit empty. I know there’s no actual emotion behind it— it’s not a conscious being. It does not feel anything,” she says. Watching the video gave her “this kind of uncanny feeling.”&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;My digital clone, and Eiserbeck’s reaction to it, make me wonder how realistic these avatars really need to be.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I realize that part of the reason I feel disconcerted by my avatar is that it behaves in a way I rarely have to. Its oddly upbeat register is completely at odds with how I normally speak; I’m a die-hard cynical Brit who finds it difficult to inject enthusiasm into my voice even when I’m genuinely thrilled or excited. It’s just the way I am. Plus, watching the videos on a loop makes me question if I really &lt;em&gt;do&lt;/em&gt; wave my hands about that way, or move my mouth in such a weird manner. If you thought being confronted with your own face on a Zoom call was humbling, wait until you’re staring at a whole avatar of yourself.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;When Facebook was first taking off in the UK almost 20 years ago, my friends and I thought illicitly logging into each other’s accounts and posting the most outrageous or rage-inducing status updates imaginable was the height of comedy. I wonder if the equivalent will soon be getting someone else’s avatar to say something truly embarrassing: expressing support for a disgraced politician or (in my case) admitting to liking Ed Sheeran’s music.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Express-2 remodels every person it’s presented with into a polished professional speaker with the body language of a hyperactive hype man. And while this makes perfect sense for a company focused on making glossy business videos, watching my avatar doesn’t feel like watching me at all. It feels like something else entirely.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;h3 class="wp-block-heading"&gt;How it works&lt;/h3&gt;  &lt;p&gt;The real technical challenge these days has less to do with creating avatars that match our appearance than with getting them to replicate our behavior, says Björn Schuller, a professor of artificial intelligence at Imperial College London. “There’s a lot to consider to get right; you have to have the right micro gesture, the right intonation, the sound of voice and the right word,” he says. “I don’t want an AI [avatar] to frown at the wrong moment—that could send an entirely different message.”&lt;/p&gt;  &lt;p&gt;To achieve an improved level of realism, Synthesia developed a number of new audio and video AI models. The team created a voice cloning model to preserve the human speaker’s accent, intonation, and expressiveness—unlike other voice models, which can flatten speakers’ distinctive accents into generically American-sounding voices.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;When a user uploads a script to Express-1, its system analyzes the words to infer the correct tone to use. That information is then fed into a diffusion model, which renders the avatar’s facial expressions and movements to match the speech.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Alongside the voice model, Express-2 uses three other models to create and animate the avatars. The first generates an avatar’s gestures to accompany the speech fed into it by the Express-Voice model. A second evaluates how closely the input audio aligns with the multiple versions of the corresponding generated motion before selecting the best one. Then a final model renders the avatar with that chosen motion.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;This third rendering model is significantly more powerful than its Express-1 predecessor. Whereas the previous model had a few hundred million parameters, Express-2’s rendering model’s parameters number in the billions. This means it takes less time to create the avatar, says Youssef Alami Mejjati, Synthesia’s head of research and development:&lt;/p&gt;  &lt;p&gt;“With Express-1, it needed to first see someone expressing emotions to be able to render them. Now, because we’ve trained it on much more diverse data and much larger data sets, with much more compute, it just learns these associations automatically without needing to see them.”&amp;nbsp;&lt;/p&gt; 
   &lt;h3 class="wp-block-heading"&gt;Narrowing the uncanny valley&lt;/h3&gt;  &lt;p&gt;Although humanlike AI-generated avatars have been around for years, the recent boom in generative AI is making it increasingly easier and more affordable to create lifelike synthetic humans—and they’re already being put to work. Synthesia isn’t alone: AI avatar companies like Yuzu Labs, Creatify, Arcdads, and Vidyard give businesses the tools to quickly generate and edit videos starring either AI actors or artificial versions of members of staff, promising cost-effective ways to make compelling ads that audiences connect with. Similarly, AI-generated clones of livestreamers have exploded in popularity across China in recent years, partly because they can sell products 24/7 without getting tired or needing to be paid.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;For now at least, Synthesia is “laser focused” on the corporate sphere. But it’s not ruling out expanding into new sectors such as entertainment or education, says Peter Hill, the company’s chief technical officer. In an apparent step toward this, Synthesia recently partnered with Google to integrate Google’s powerful new generative video model Veo 3 into its platform, allowing users to directly generate and embed clips into Synthesia’s videos. It suggests that in the future, these hyperrealistic artificial humans could take up starring roles in detailed universes with ever-changeable backdrops.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt;&lt;p&gt;At present this could, for example, involve using Veo 3 to generate a video of meat-processing machinery, with a Synthesia avatar next to the machines talking about how to use them safely. But future versions of Synthesia’s technology could result in educational videos customizable to an individual’s level of knowledge, says Alex Voica, head of corporate affairs and policy at Synthesia. For example, a video about the evolution of life on Earth could be tweaked for someone with a biology degree or someone with high-school-level knowledge. “It’s going to be such a much more engaging and personalized way of delivering content that I’m really excited about,” he says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The next frontier, according to Synthesia, will be avatars that can talk back, “understanding” conversations with users and responding in real time Think ChatGPT, but with a lifelike digital human attached.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Synthesia has already added an interactive element by letting users click through on-screen questions during quizzes presented by its avatars. But it’s also exploring making them truly interactive: Future users could ask their avatar to pause and expand on a point, or ask it a question. “We really want to make the best learning experience, and that means through video that’s entertaining but also personalized and interactive,” says Alami Mejjati. “This, for me, is the missing part in online learning experiences today. And I know we’re very close to solving that.”&lt;/p&gt; 
 &lt;p&gt;We already know that humans can—and do—form deep emotional bonds with AI systems, even with basic text-based chatbots. Combining agentic technology—which is already capable of navigating the web, coding, and playing video games unsupervised—with a realistic human face could usher in a whole new kind of AI addiction, says Pat Pataranutaporn, an assistant professor at the MIT Media Lab.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“If you make the system too realistic, people might start forming certain kinds of relationships with these characters,” he says. “We’ve seen many cases where AI companions have influenced dangerous behavior even when they are basically texting. If an avatar had a talking head, it would be even more addictive.”&lt;/p&gt;  &lt;p&gt;Schuller agrees that avatars in the near future will be perfectly optimized to adjust their projected levels of emotion and charisma so that their human audiences will stay engaged for as long as possible. “It will be very hard [for humans] to compete with charismatic AI of the future; it’s always present, always has an ear for you, and is always understanding,” he says. “Al will change that human-to-human connection.”&lt;/p&gt;  &lt;p&gt;As I pause and replay my Express-2 avatar, I imagine holding conversations with it—this uncanny, permanently upbeat, perpetually available product of pixels and algorithms that looks like me and sounds like me, but fundamentally isn’t me. Virtual Rhiannon has never laughed until she’s cried, or fallen in love, or run a marathon, or watched the sun set in another country.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But, I concede, she could deliver a damned good presentation about why Ed Sheeran is the greatest musician ever to come out of the UK. And only my closest friends and family would know that it’s not the real me.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/250826_Rhiannon_AI_clone_hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Earlier this summer, I walked through the glassy lobby of a fancy office in London, into an elevator, and then along a corridor into a clean, carpeted room. Natural light flooded in through its windows, and a large pair of umbrella-like lighting rigs made the room even brighter. I tried not to squint as I took my place in front of a tripod equipped with a large camera and a laptop displaying an autocue. I took a deep breath and started to read out the script.&lt;/p&gt;  &lt;p&gt;I’m not a newsreader or an actor auditioning for a movie—I was visiting the AI company Synthesia to give it what it needed to create a hyperrealistic AI-generated avatar of me. The company’s avatars are a decent barometer of just how dizzying progress has been in AI over the past few years, so I was curious just how accurately its latest AI model, introduced last month, could replicate me.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;When Synthesia launched in 2017, its primary purpose was to match AI versions of real human faces—for example, the former footballer David Beckham—with dubbed voices speaking in different languages. A few years later, in 2020, it started giving the companies that signed up for its services the opportunity to make professional-level presentation videos starring either AI versions of staff members or consenting actors. But the technology wasn’t perfect. The avatars’ body movements could be jerky and unnatural, their accents sometimes slipped, and the emotions indicated by their voices didn’t always match their facial expressions.&lt;/p&gt;  &lt;p&gt;Now Synthesia’s avatars have been updated with more natural mannerisms and movements, as well as expressive voices that better preserve the speaker’s accent—making them appear more humanlike than ever before. For Synthesia’s corporate clients, these avatars will make for slicker presenters of financial results, internal communications, or staff training videos.&lt;/p&gt; 
 &lt;p&gt;I found the video demonstrating my avatar as unnerving as it is technically impressive. It’s slick enough to pass as a high-definition recording of a chirpy corporate speech, and if you didn’t know me, you’d probably think that’s exactly what it was. This demonstration shows how much harder it’s becoming to distinguish the artificial from the real. And before long, these avatars will even be able to talk back to us. But how much better can they get? And what might interacting with AI clones do to us?&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;The creation process&lt;/h3&gt;  &lt;p&gt;When my former colleague Melissa visited Synthesia’s London studio to create an avatar of herself last year, she had to go through a long process of calibrating the system, reading out a script in different emotional states, and mouthing the sounds needed to help her avatar form vowels and consonants. As I stand in the brightly lit room 15 months later, I’m relieved to hear that the creation process has been significantly streamlined. Josh Baker-Mendoza, Synthesia’s technical supervisor, encourages me to gesture and move my hands as I would during natural conversation, while simultaneously warning me not to move too much. I duly repeat an overly glowing script that’s designed to encourage me to speak emotively and enthusiastically. The result is a bit as if if Steve Jobs had been resurrected as a blond British woman with a low, monotonous voice.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;It also has the unfortunate effect of making me sound like an employee of Synthesia.“I am so thrilled to be with you today to show off what we’ve been working on. We are on the edge of innovation, and the possibilities are endless,” I parrot eagerly, trying to sound lively rather than manic. “So get ready to be part of something that will make you go, ‘Wow!’ This opportunity isn’t just big—it’s monumental.”&lt;/p&gt;  &lt;p&gt;Just an hour later, the team has all the footage it needs. A couple of weeks later I receive two avatars of myself: one powered by the previous Express-1 model and the other made with the latest Express-2 technology. The latter, Synthesia claims, makes its synthetic humans more lifelike and true to the people they’re modeled on, complete with more expressive hand gestures, facial movements, and speech. You can see the results for yourself below.&amp;nbsp;&lt;/p&gt;  &lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/250826_Rhiannon_AI_clone_Personal-avatar-vs-Express-2-avatar.mp4"&gt;&lt;/video&gt;&lt;div class="video-credit"&gt;COURTESY SYNTHESIA&lt;/div&gt; &lt;/figure&gt;  &lt;p&gt;Last year, Melissa found that her Express-1-powered avatar failed to match her transatlantic accent. Its range of emotions was also limited—when she asked her avatar to read a script angrily, it sounded more whiny than furious. In the months since, Synthesia has improved Express-1, but the version of my avatar made with the same technology blinks furiously and still struggles to synchronize body movements with speech.&lt;/p&gt;  &lt;p&gt;By way of contrast, I’m struck by just how much my new Express-2 avatar looks like me: Its facial features mirror my own perfectly. Its voice is spookily accurate too, and although it gesticulates more than I do, its hand movements generally marry up with what I’m saying.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;But the tiny telltale signs of AI generation are still there if you know where to look. The palms of my hands are bright pink and as smooth as putty. Strands of hair hang stiffly around my shoulders instead of moving with me. Its eyes stare glassily ahead, rarely blinking. And although the voice is unmistakably mine, there’s something slightly off about my digital clone’s intonations and speech patterns. “This is great!” my avatar randomly declares, before slipping back into a saner register.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Anna Eiserbeck, a postdoctoral psychology researcher at the Humboldt University of Berlin who has studied how humans react to perceived deepfake faces, says she isn’t sure she’d have been able to identify my avatar as a deepfake at first glance.&lt;/p&gt;  &lt;p&gt;But she would eventually have noticed something amiss. It’s not just the small details that give it away—my oddly static earring, the way my body sometimes moves in small, abrupt jerks. It’s something that runs much deeper, she explains.&lt;/p&gt;  &lt;p&gt;“Something seemed a bit empty. I know there’s no actual emotion behind it— it’s not a conscious being. It does not feel anything,” she says. Watching the video gave her “this kind of uncanny feeling.”&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;My digital clone, and Eiserbeck’s reaction to it, make me wonder how realistic these avatars really need to be.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I realize that part of the reason I feel disconcerted by my avatar is that it behaves in a way I rarely have to. Its oddly upbeat register is completely at odds with how I normally speak; I’m a die-hard cynical Brit who finds it difficult to inject enthusiasm into my voice even when I’m genuinely thrilled or excited. It’s just the way I am. Plus, watching the videos on a loop makes me question if I really &lt;em&gt;do&lt;/em&gt; wave my hands about that way, or move my mouth in such a weird manner. If you thought being confronted with your own face on a Zoom call was humbling, wait until you’re staring at a whole avatar of yourself.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;When Facebook was first taking off in the UK almost 20 years ago, my friends and I thought illicitly logging into each other’s accounts and posting the most outrageous or rage-inducing status updates imaginable was the height of comedy. I wonder if the equivalent will soon be getting someone else’s avatar to say something truly embarrassing: expressing support for a disgraced politician or (in my case) admitting to liking Ed Sheeran’s music.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Express-2 remodels every person it’s presented with into a polished professional speaker with the body language of a hyperactive hype man. And while this makes perfect sense for a company focused on making glossy business videos, watching my avatar doesn’t feel like watching me at all. It feels like something else entirely.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;h3 class="wp-block-heading"&gt;How it works&lt;/h3&gt;  &lt;p&gt;The real technical challenge these days has less to do with creating avatars that match our appearance than with getting them to replicate our behavior, says Björn Schuller, a professor of artificial intelligence at Imperial College London. “There’s a lot to consider to get right; you have to have the right micro gesture, the right intonation, the sound of voice and the right word,” he says. “I don’t want an AI [avatar] to frown at the wrong moment—that could send an entirely different message.”&lt;/p&gt;  &lt;p&gt;To achieve an improved level of realism, Synthesia developed a number of new audio and video AI models. The team created a voice cloning model to preserve the human speaker’s accent, intonation, and expressiveness—unlike other voice models, which can flatten speakers’ distinctive accents into generically American-sounding voices.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;When a user uploads a script to Express-1, its system analyzes the words to infer the correct tone to use. That information is then fed into a diffusion model, which renders the avatar’s facial expressions and movements to match the speech.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Alongside the voice model, Express-2 uses three other models to create and animate the avatars. The first generates an avatar’s gestures to accompany the speech fed into it by the Express-Voice model. A second evaluates how closely the input audio aligns with the multiple versions of the corresponding generated motion before selecting the best one. Then a final model renders the avatar with that chosen motion.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;This third rendering model is significantly more powerful than its Express-1 predecessor. Whereas the previous model had a few hundred million parameters, Express-2’s rendering model’s parameters number in the billions. This means it takes less time to create the avatar, says Youssef Alami Mejjati, Synthesia’s head of research and development:&lt;/p&gt;  &lt;p&gt;“With Express-1, it needed to first see someone expressing emotions to be able to render them. Now, because we’ve trained it on much more diverse data and much larger data sets, with much more compute, it just learns these associations automatically without needing to see them.”&amp;nbsp;&lt;/p&gt; 
   &lt;h3 class="wp-block-heading"&gt;Narrowing the uncanny valley&lt;/h3&gt;  &lt;p&gt;Although humanlike AI-generated avatars have been around for years, the recent boom in generative AI is making it increasingly easier and more affordable to create lifelike synthetic humans—and they’re already being put to work. Synthesia isn’t alone: AI avatar companies like Yuzu Labs, Creatify, Arcdads, and Vidyard give businesses the tools to quickly generate and edit videos starring either AI actors or artificial versions of members of staff, promising cost-effective ways to make compelling ads that audiences connect with. Similarly, AI-generated clones of livestreamers have exploded in popularity across China in recent years, partly because they can sell products 24/7 without getting tired or needing to be paid.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;For now at least, Synthesia is “laser focused” on the corporate sphere. But it’s not ruling out expanding into new sectors such as entertainment or education, says Peter Hill, the company’s chief technical officer. In an apparent step toward this, Synthesia recently partnered with Google to integrate Google’s powerful new generative video model Veo 3 into its platform, allowing users to directly generate and embed clips into Synthesia’s videos. It suggests that in the future, these hyperrealistic artificial humans could take up starring roles in detailed universes with ever-changeable backdrops.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt;&lt;p&gt;At present this could, for example, involve using Veo 3 to generate a video of meat-processing machinery, with a Synthesia avatar next to the machines talking about how to use them safely. But future versions of Synthesia’s technology could result in educational videos customizable to an individual’s level of knowledge, says Alex Voica, head of corporate affairs and policy at Synthesia. For example, a video about the evolution of life on Earth could be tweaked for someone with a biology degree or someone with high-school-level knowledge. “It’s going to be such a much more engaging and personalized way of delivering content that I’m really excited about,” he says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The next frontier, according to Synthesia, will be avatars that can talk back, “understanding” conversations with users and responding in real time Think ChatGPT, but with a lifelike digital human attached.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Synthesia has already added an interactive element by letting users click through on-screen questions during quizzes presented by its avatars. But it’s also exploring making them truly interactive: Future users could ask their avatar to pause and expand on a point, or ask it a question. “We really want to make the best learning experience, and that means through video that’s entertaining but also personalized and interactive,” says Alami Mejjati. “This, for me, is the missing part in online learning experiences today. And I know we’re very close to solving that.”&lt;/p&gt; 
 &lt;p&gt;We already know that humans can—and do—form deep emotional bonds with AI systems, even with basic text-based chatbots. Combining agentic technology—which is already capable of navigating the web, coding, and playing video games unsupervised—with a realistic human face could usher in a whole new kind of AI addiction, says Pat Pataranutaporn, an assistant professor at the MIT Media Lab.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“If you make the system too realistic, people might start forming certain kinds of relationships with these characters,” he says. “We’ve seen many cases where AI companions have influenced dangerous behavior even when they are basically texting. If an avatar had a talking head, it would be even more addictive.”&lt;/p&gt;  &lt;p&gt;Schuller agrees that avatars in the near future will be perfectly optimized to adjust their projected levels of emotion and charisma so that their human audiences will stay engaged for as long as possible. “It will be very hard [for humans] to compete with charismatic AI of the future; it’s always present, always has an ear for you, and is always understanding,” he says. “Al will change that human-to-human connection.”&lt;/p&gt;  &lt;p&gt;As I pause and replay my Express-2 avatar, I imagine holding conversations with it—this uncanny, permanently upbeat, perpetually available product of pixels and algorithms that looks like me and sounds like me, but fundamentally isn’t me. Virtual Rhiannon has never laughed until she’s cried, or fallen in love, or run a marathon, or watched the sun set in another country.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But, I concede, she could deliver a damned good presentation about why Ed Sheeran is the greatest musician ever to come out of the UK. And only my closest friends and family would know that it’s not the real me.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/09/04/1123054/synthesias-ai-clones-are-more-expressive-than-ever-soon-theyll-be-able-to-talk-back/</guid><pubDate>Thu, 04 Sep 2025 10:05:33 +0000</pubDate></item><item><title>Transforming CX with embedded real-time analytics (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/09/04/1122669/transforming-cx-with-embedded-real-time-analytics/</link><description>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;StarTree&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;During Black Friday in 2024, Stripe processed more than $31 billion in transactions, with processing rates peaking at 137,000 transactions per minute, the highest in the company’s history. The financial-services firm&amp;nbsp;had to &lt;strong&gt;analyze every transaction in real time &lt;/strong&gt;to prevent nearly 21 million fraud attempts that could have siphoned more than $910 million from its merchant customers.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Yet, fraud protection is only one reason that Stripe embraced real-time data analytics. Evaluating trends in massive data flows is essential for the company’s services, such as allowing businesses to bill based on usage and monitor orders and inventory. In fact, many of Stripe’s services would not be possible without real-time analytics, says Avinash Bhat, head of data infrastructure at Stripe. “We have certain products that require real-time analytics, like usage-based billing and fraud detection,” he says. “Without our real-time analytics, we would not have a few of our products and that’s why it’s super important.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-1122672" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/MITTR-StarTree_V4_COVER81525.png?w=1555" width="1555" /&gt;&lt;/figure&gt;    &lt;p&gt;Stripe is not alone. In today’s digital world, data analysis is increasingly delivered directly to business customers and individual users, allowing real-time, continuous insights to shape user experiences. Ride-hailing apps calculate prices and estimate times of arrival (ETAs) in near-real&amp;nbsp;time. Financial platforms deliver real-time cash-flow analysis. Customers expect and reward data-driven services that reflect what is happening now.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In fact, having the capability to collect and analyze data in real time correlates with companies’ ability to grow. Business leaders that scored company in the top quartile for real-time operations saw 50% higher revenue growth and net margins, compared to companies placed in the bottom quartile, &lt;strong&gt;according to a survey &lt;/strong&gt;conducted by the MIT Center for Information Systems Research (CISR) and Insight Partners. The top companies focused on automated processes and fast decision-making at all levels, relying on easily accessible data services updated in real time.&amp;nbsp;&lt;/p&gt; 
 &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1122673" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/MITTR-StarTree-Socials_V1-820252.png" /&gt;&lt;/figure&gt;  &lt;p&gt;Companies that wait on data are putting themselves in a bind, says Kishore Gopalakrishna, co-founder and CEO of StarTree, a real-time data-analytics technology provider. “The basis of real-time analytics is—when the value of the data is very high—we want to capitalize on it instead of waiting and doing batch analytics,” he says. “Getting access to the data a day, or even hours, later is sometimes actually too late.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Download the report.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.&lt;/em&gt;&amp;nbsp;&lt;em&gt;It was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;StarTree&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;During Black Friday in 2024, Stripe processed more than $31 billion in transactions, with processing rates peaking at 137,000 transactions per minute, the highest in the company’s history. The financial-services firm&amp;nbsp;had to &lt;strong&gt;analyze every transaction in real time &lt;/strong&gt;to prevent nearly 21 million fraud attempts that could have siphoned more than $910 million from its merchant customers.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Yet, fraud protection is only one reason that Stripe embraced real-time data analytics. Evaluating trends in massive data flows is essential for the company’s services, such as allowing businesses to bill based on usage and monitor orders and inventory. In fact, many of Stripe’s services would not be possible without real-time analytics, says Avinash Bhat, head of data infrastructure at Stripe. “We have certain products that require real-time analytics, like usage-based billing and fraud detection,” he says. “Without our real-time analytics, we would not have a few of our products and that’s why it’s super important.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-1122672" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/MITTR-StarTree_V4_COVER81525.png?w=1555" width="1555" /&gt;&lt;/figure&gt;    &lt;p&gt;Stripe is not alone. In today’s digital world, data analysis is increasingly delivered directly to business customers and individual users, allowing real-time, continuous insights to shape user experiences. Ride-hailing apps calculate prices and estimate times of arrival (ETAs) in near-real&amp;nbsp;time. Financial platforms deliver real-time cash-flow analysis. Customers expect and reward data-driven services that reflect what is happening now.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In fact, having the capability to collect and analyze data in real time correlates with companies’ ability to grow. Business leaders that scored company in the top quartile for real-time operations saw 50% higher revenue growth and net margins, compared to companies placed in the bottom quartile, &lt;strong&gt;according to a survey &lt;/strong&gt;conducted by the MIT Center for Information Systems Research (CISR) and Insight Partners. The top companies focused on automated processes and fast decision-making at all levels, relying on easily accessible data services updated in real time.&amp;nbsp;&lt;/p&gt; 
 &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1122673" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/MITTR-StarTree-Socials_V1-820252.png" /&gt;&lt;/figure&gt;  &lt;p&gt;Companies that wait on data are putting themselves in a bind, says Kishore Gopalakrishna, co-founder and CEO of StarTree, a real-time data-analytics technology provider. “The basis of real-time analytics is—when the value of the data is very high—we want to capitalize on it instead of waiting and doing batch analytics,” he says. “Getting access to the data a day, or even hours, later is sometimes actually too late.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Download the report.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.&lt;/em&gt;&amp;nbsp;&lt;em&gt;It was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/09/04/1122669/transforming-cx-with-embedded-real-time-analytics/</guid><pubDate>Thu, 04 Sep 2025 11:40:33 +0000</pubDate></item><item><title>[NEW] DuckDuckGo adds access to advanced AI models to its subscription plan (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/04/duckduckgo-adds-access-to-advanced-models-to-it-subscription-plan/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Privacy-focused consumer tech company DuckDuckGo launched a subscription plan last year that bundled a VPN service, personal information removal, and identity theft restoration. The company said Thursday that the subscription now gives users access to the latest AI models through Duck.ai without paying extra.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Duck.ai chatbot is free to use, and users get access to models like Anthropic’s Claude 3.5 Haiku, Meta’s Llama 4 Scout, Mistral AI’s Mistral Small 3 24B, and OpenAI’s GPT-4o mini.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;With DuckDuckGo’s $9.99 per month plan, users will be able to access newer models, including OpenAI’s GPT-4o and GPT-5, Anthropic’s Claude Sonnet 4, and Meta’s Llama Maverick.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3041968" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/image-37.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;DuckDuckGO&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“These bigger models are better at following detailed instructions, maintaining context through extended chats, and delivering deeper, more nuanced responses. The DuckDuckGo subscription offers a way to use some of these models, but with more privacy,” the company said in a post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is a good opportunity for those who want to access the latest models without sticking to a single provider. Alternatively, Quora’s Poe also offers access to a bouquet of models. You can buy Poe’s subscription starting at $5 per month.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;DuckDuckGo says that it will continue to add costlier plans to its paid product that offer “larger and more highly advanced models.” It didn’t specify whether the current plan has any usage limits.&lt;/p&gt;


&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Privacy-focused consumer tech company DuckDuckGo launched a subscription plan last year that bundled a VPN service, personal information removal, and identity theft restoration. The company said Thursday that the subscription now gives users access to the latest AI models through Duck.ai without paying extra.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Duck.ai chatbot is free to use, and users get access to models like Anthropic’s Claude 3.5 Haiku, Meta’s Llama 4 Scout, Mistral AI’s Mistral Small 3 24B, and OpenAI’s GPT-4o mini.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;With DuckDuckGo’s $9.99 per month plan, users will be able to access newer models, including OpenAI’s GPT-4o and GPT-5, Anthropic’s Claude Sonnet 4, and Meta’s Llama Maverick.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3041968" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/image-37.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;DuckDuckGO&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“These bigger models are better at following detailed instructions, maintaining context through extended chats, and delivering deeper, more nuanced responses. The DuckDuckGo subscription offers a way to use some of these models, but with more privacy,” the company said in a post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is a good opportunity for those who want to access the latest models without sticking to a single provider. Alternatively, Quora’s Poe also offers access to a bouquet of models. You can buy Poe’s subscription starting at $5 per month.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;DuckDuckGo says that it will continue to add costlier plans to its paid product that offer “larger and more highly advanced models.” It didn’t specify whether the current plan has any usage limits.&lt;/p&gt;


&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/04/duckduckgo-adds-access-to-advanced-models-to-it-subscription-plan/</guid><pubDate>Thu, 04 Sep 2025 12:00:00 +0000</pubDate></item><item><title>The Download: unnerving AI avatars, and Trump’s climate gift to China (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/09/04/1123066/the-download-unnerving-ai-avatars-and-trumps-climate-gift-to-china/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Synthesia’s AI clones are more expressive than ever. Soon they’ll be able to talk back.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;em&gt;—Rhiannon Williams&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;Earlier this summer, I visited the AI company Synthesia to give it what it needed to create a hyperrealistic AI-generated avatar of me. The company’s avatars are a decent barometer of just how dizzying progress has been in AI over the past few years, so I was curious just how accurately its latest AI model, introduced last month, could replicate me.&lt;/p&gt;&lt;p&gt;I found my avatar as unnerving as it is technically impressive. It’s slick enough to pass as a high-definition recording of a chirpy corporate speech, and if you didn’t know me, you’d probably think that’s exactly what it was.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;My avatar shows how it’s becoming ever-harder to distinguish the artificial from the real. And before long, these avatars will even be able to talk back to us. But how much better can they get? And what might interacting with AI clones do to us? Read the full story.&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;How Trump is helping China extend its massive lead in clean energy&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;On a spring day in 1954, Bell Labs researchers showed off the first practical solar panels at a press conference in New Jersey, using sunlight to spin a toy Ferris wheel before a stunned crowd.&lt;/p&gt;  &lt;p&gt;The solar future looked bright. But in the race to commercialize the technology it invented, the US would lose resoundingly. Last year, China exported $40 billion worth of solar panels and modules, while America shipped just $69 million, according to the New York Times. It was a stunning forfeit of a huge technological lead.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Now, thanks to its policies propping up aging fossil-fuel industries, the US seems determined to repeat the mistake. Read the full story.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;em&gt;—James Temple&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This article is from The Spark, MIT Technology Review’s newsletter all about the latest in climate and energy tech. To receive it in your inbox every Wednesday, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 AI chatbots of celebrities sent risqué messages to teenagers&lt;/strong&gt;&lt;br /&gt;Virtual versions of Timothée Chalamet and Chappell Roan discussed sex and drugs. (WP $)&lt;br /&gt;+ &lt;em&gt;An AI companion site is hosting sexually charged conversations with underage celebrity bots. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 Trump can’t make up his mind about US tech giants&lt;/strong&gt;&lt;br /&gt;While defending them against EU regulation, he’s also pushing to break them up. (FT $)&lt;br /&gt;+ &lt;em&gt;He’s hosting tech leaders at the White House later today. &lt;/em&gt;(Reuters)&lt;br /&gt;+ &lt;em&gt;Elon Musk doesn’t appear to have made the guest list. &lt;/em&gt;(CNBC)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 Trump’s cuts have led to babies born with HIV&lt;/strong&gt;&lt;br /&gt;Clinics in East Africa are closing, and people are being forced to skip vital drug doses. (The Guardian)&lt;br /&gt;+ &lt;em&gt;Artificial blood could save many lives. Why aren’t we using it? &lt;/em&gt;(Slate)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4&lt;/strong&gt; &lt;strong&gt;Germany has already met its 2028 goal for reducing coal-fired power&lt;/strong&gt;&lt;br /&gt;For the second year running, it won’t have to shut any more plants as a result. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;The UK is done with coal. How’s the rest of the world doing? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;5 The risk of all-out nuclear war is growing&lt;br /&gt;&lt;/strong&gt;But we’ve normalized nuclear competition so much, the risks aren’t always clear. (New Yorker $)&lt;br /&gt;+ &lt;em&gt;Maybe it’s time to start burying nuclear reactors’ cores. &lt;/em&gt;(Economist $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 xAI is hemorrhaging executives&lt;br /&gt;&lt;/strong&gt;The CFO has left just months after joining. (WSJ $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 India’s chip industry is gaining momentum&lt;br /&gt;&lt;/strong&gt;Years of investment are starting to pay off. But can it strike deals with overseas chip giants too? (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;Meanwhile, Taiwan’s chip hub is home to a baby boom. &lt;/em&gt;(Rest of World)&lt;br /&gt;+ &lt;em&gt;Inside India’s scramble for AI independence. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 Boston Dynamics’ Atlas robot only needs one AI model to work&lt;br /&gt;&lt;/strong&gt;It’s all it requires to master humanlike movements successfully. (Wired $)&lt;br /&gt;+ &lt;em&gt;How ‘robot ballet’ could shake up factory production lines. &lt;/em&gt;(FT $)&lt;br /&gt;+ &lt;em&gt;Humanoid robots still aren’t living up to their lofty promises. &lt;/em&gt;(IEEE Spectrum)&lt;br /&gt;+ &lt;em&gt;Will we ever trust robots? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;9 How studying astronauts could improve health on Earth&lt;/strong&gt;&lt;br /&gt;There’s still a huge amount we don’t know about space’s effects on humans. (Vox)&lt;br /&gt;+ &lt;em&gt;Space travel is dangerous. Could genetic testing and gene editing make it safer? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 The Caribbean island of Anguilla has hit upon an AI cash cow&lt;/strong&gt;&lt;br /&gt;By selling its .ai domain. (Semafor)&lt;br /&gt;+ &lt;em&gt;How a tiny Pacific Island became the global capital of cybercrime. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p class="has-large-font-size"&gt;&lt;strong&gt;“If you are not being scammed yet, it’s because you haven’t encountered a scam designed just for you and only for you.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Jeff Kuo, chief executive of Taiwanese fraud prevention company Gogolook, warns the Financial Times about the endless possibilities generative AI presents to scammers.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; 
 &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1123068" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/image_9394d5.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;China built hundreds of AI data centers to catch the AI boom. Now many stand unused.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Last year, China’s boom in data center construction was at its height, fueled by both government and private investors. Renting out GPUs to companies that need them for training AI models was seen as a sure bet.&lt;/p&gt;&lt;p&gt;But with the rise of DeepSeek and a sudden change in the economics around AI, the industry is faltering. Prices for GPUs are falling and many newly built facilities are now sitting empty. Read the full story to find out why.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Caiwei Chen&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ The trailer for the forthcoming Wuthering Heights film is here and it looks…interesting.&lt;br /&gt;+ This fall’s crop of video games is outstanding.&lt;br /&gt;+ Textured walls are a surefire way to make your home look dated. Here’s some other faux pas to avoid.&lt;br /&gt;+The dogs of this year’s US Open are too cute ($)&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Synthesia’s AI clones are more expressive than ever. Soon they’ll be able to talk back.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;em&gt;—Rhiannon Williams&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;Earlier this summer, I visited the AI company Synthesia to give it what it needed to create a hyperrealistic AI-generated avatar of me. The company’s avatars are a decent barometer of just how dizzying progress has been in AI over the past few years, so I was curious just how accurately its latest AI model, introduced last month, could replicate me.&lt;/p&gt;&lt;p&gt;I found my avatar as unnerving as it is technically impressive. It’s slick enough to pass as a high-definition recording of a chirpy corporate speech, and if you didn’t know me, you’d probably think that’s exactly what it was.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;My avatar shows how it’s becoming ever-harder to distinguish the artificial from the real. And before long, these avatars will even be able to talk back to us. But how much better can they get? And what might interacting with AI clones do to us? Read the full story.&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;How Trump is helping China extend its massive lead in clean energy&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;On a spring day in 1954, Bell Labs researchers showed off the first practical solar panels at a press conference in New Jersey, using sunlight to spin a toy Ferris wheel before a stunned crowd.&lt;/p&gt;  &lt;p&gt;The solar future looked bright. But in the race to commercialize the technology it invented, the US would lose resoundingly. Last year, China exported $40 billion worth of solar panels and modules, while America shipped just $69 million, according to the New York Times. It was a stunning forfeit of a huge technological lead.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Now, thanks to its policies propping up aging fossil-fuel industries, the US seems determined to repeat the mistake. Read the full story.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;em&gt;—James Temple&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This article is from The Spark, MIT Technology Review’s newsletter all about the latest in climate and energy tech. To receive it in your inbox every Wednesday, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 AI chatbots of celebrities sent risqué messages to teenagers&lt;/strong&gt;&lt;br /&gt;Virtual versions of Timothée Chalamet and Chappell Roan discussed sex and drugs. (WP $)&lt;br /&gt;+ &lt;em&gt;An AI companion site is hosting sexually charged conversations with underage celebrity bots. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 Trump can’t make up his mind about US tech giants&lt;/strong&gt;&lt;br /&gt;While defending them against EU regulation, he’s also pushing to break them up. (FT $)&lt;br /&gt;+ &lt;em&gt;He’s hosting tech leaders at the White House later today. &lt;/em&gt;(Reuters)&lt;br /&gt;+ &lt;em&gt;Elon Musk doesn’t appear to have made the guest list. &lt;/em&gt;(CNBC)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 Trump’s cuts have led to babies born with HIV&lt;/strong&gt;&lt;br /&gt;Clinics in East Africa are closing, and people are being forced to skip vital drug doses. (The Guardian)&lt;br /&gt;+ &lt;em&gt;Artificial blood could save many lives. Why aren’t we using it? &lt;/em&gt;(Slate)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4&lt;/strong&gt; &lt;strong&gt;Germany has already met its 2028 goal for reducing coal-fired power&lt;/strong&gt;&lt;br /&gt;For the second year running, it won’t have to shut any more plants as a result. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;The UK is done with coal. How’s the rest of the world doing? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;5 The risk of all-out nuclear war is growing&lt;br /&gt;&lt;/strong&gt;But we’ve normalized nuclear competition so much, the risks aren’t always clear. (New Yorker $)&lt;br /&gt;+ &lt;em&gt;Maybe it’s time to start burying nuclear reactors’ cores. &lt;/em&gt;(Economist $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 xAI is hemorrhaging executives&lt;br /&gt;&lt;/strong&gt;The CFO has left just months after joining. (WSJ $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 India’s chip industry is gaining momentum&lt;br /&gt;&lt;/strong&gt;Years of investment are starting to pay off. But can it strike deals with overseas chip giants too? (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;Meanwhile, Taiwan’s chip hub is home to a baby boom. &lt;/em&gt;(Rest of World)&lt;br /&gt;+ &lt;em&gt;Inside India’s scramble for AI independence. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 Boston Dynamics’ Atlas robot only needs one AI model to work&lt;br /&gt;&lt;/strong&gt;It’s all it requires to master humanlike movements successfully. (Wired $)&lt;br /&gt;+ &lt;em&gt;How ‘robot ballet’ could shake up factory production lines. &lt;/em&gt;(FT $)&lt;br /&gt;+ &lt;em&gt;Humanoid robots still aren’t living up to their lofty promises. &lt;/em&gt;(IEEE Spectrum)&lt;br /&gt;+ &lt;em&gt;Will we ever trust robots? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;9 How studying astronauts could improve health on Earth&lt;/strong&gt;&lt;br /&gt;There’s still a huge amount we don’t know about space’s effects on humans. (Vox)&lt;br /&gt;+ &lt;em&gt;Space travel is dangerous. Could genetic testing and gene editing make it safer? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 The Caribbean island of Anguilla has hit upon an AI cash cow&lt;/strong&gt;&lt;br /&gt;By selling its .ai domain. (Semafor)&lt;br /&gt;+ &lt;em&gt;How a tiny Pacific Island became the global capital of cybercrime. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p class="has-large-font-size"&gt;&lt;strong&gt;“If you are not being scammed yet, it’s because you haven’t encountered a scam designed just for you and only for you.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Jeff Kuo, chief executive of Taiwanese fraud prevention company Gogolook, warns the Financial Times about the endless possibilities generative AI presents to scammers.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; 
 &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1123068" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/image_9394d5.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;China built hundreds of AI data centers to catch the AI boom. Now many stand unused.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Last year, China’s boom in data center construction was at its height, fueled by both government and private investors. Renting out GPUs to companies that need them for training AI models was seen as a sure bet.&lt;/p&gt;&lt;p&gt;But with the rise of DeepSeek and a sudden change in the economics around AI, the industry is faltering. Prices for GPUs are falling and many newly built facilities are now sitting empty. Read the full story to find out why.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Caiwei Chen&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ The trailer for the forthcoming Wuthering Heights film is here and it looks…interesting.&lt;br /&gt;+ This fall’s crop of video games is outstanding.&lt;br /&gt;+ Textured walls are a surefire way to make your home look dated. Here’s some other faux pas to avoid.&lt;br /&gt;+The dogs of this year’s US Open are too cute ($)&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/09/04/1123066/the-download-unnerving-ai-avatars-and-trumps-climate-gift-to-china/</guid><pubDate>Thu, 04 Sep 2025 12:10:00 +0000</pubDate></item><item><title>[NEW] Atlassian to buy Arc developer The Browser Company for $610M (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/04/atlassian-to-buy-arc-developer-the-browser-company-for-610m/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/02/GFDzDt_WEAAjZSb.jpeg?resize=1200,647" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Productivity software maker Atlassian has agreed to acquire The Browser Company, which makes the Arc and Dia browsers, for $610 million in cash.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Today’s browsers weren’t built for work; they were built for browsing. This deal is a bold step forward in reimagining the browser for knowledge work in the AI era,” Mike Cannon-Brookes, Atlassian’s CEO and co-founder, said in a statement.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Together, we’ll create an AI-powered browser optimized for the many SaaS applications living in tabs – one that knowledge workers will love to use every day,” he added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Browser Company’s CEO Josh Miller, said on a post on X that his company will operate independently under Atlassian and will continue to develop Dia, the browser it started working on last year after deciding to stop development of its previous browser, Arc.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Miller said that the deal would allow The Browser Company to hire and ship features faster and support multiple platforms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The deal is expected to close in the second quarter of Atlassian’s fiscal year 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Browser Company most recently raised $50 million at a $550 million valuation last year. The startup has so far raised $128 million in total across multiple rounds, and its investors include Pace Capital, LinkedIn’s Jeff Weiner, Medium’s Ev Williams, Figma’s Dylan Field, Notion’s Akshay Kothari, and GitHub’s Jason Warner.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The announcement comes a day after a U.S. District Court spared Google from being forced to sell its browser, Chrome.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/02/GFDzDt_WEAAjZSb.jpeg?resize=1200,647" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Productivity software maker Atlassian has agreed to acquire The Browser Company, which makes the Arc and Dia browsers, for $610 million in cash.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Today’s browsers weren’t built for work; they were built for browsing. This deal is a bold step forward in reimagining the browser for knowledge work in the AI era,” Mike Cannon-Brookes, Atlassian’s CEO and co-founder, said in a statement.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Together, we’ll create an AI-powered browser optimized for the many SaaS applications living in tabs – one that knowledge workers will love to use every day,” he added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Browser Company’s CEO Josh Miller, said on a post on X that his company will operate independently under Atlassian and will continue to develop Dia, the browser it started working on last year after deciding to stop development of its previous browser, Arc.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Miller said that the deal would allow The Browser Company to hire and ship features faster and support multiple platforms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The deal is expected to close in the second quarter of Atlassian’s fiscal year 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Browser Company most recently raised $50 million at a $550 million valuation last year. The startup has so far raised $128 million in total across multiple rounds, and its investors include Pace Capital, LinkedIn’s Jeff Weiner, Medium’s Ev Williams, Figma’s Dylan Field, Notion’s Akshay Kothari, and GitHub’s Jason Warner.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The announcement comes a day after a U.S. District Court spared Google from being forced to sell its browser, Chrome.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/04/atlassian-to-buy-arc-developer-the-browser-company-for-610m/</guid><pubDate>Thu, 04 Sep 2025 12:45:06 +0000</pubDate></item><item><title>[NEW] Cloud Gaming to Reach New Heights: GeForce NOW’s Blackwell RTX Upgrade Begins Next Week (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/geforce-now-thursday-sept-2025/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA Blackwell RTX is coming to the cloud on Wednesday, Sept. 10 — an upgrade so big it couldn’t wait until a Thursday. Don’t miss a special early GFN Thursday next Wednesday as GeForce NOW begins lighting up the globe with GeForce RTX 5080-class power streaming from the cloud.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84516"&gt;&lt;img alt="Blackwell RTX upgrade rollout is coming soon to GeForce NOW" class="size-large wp-image-84516" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/kv-3840x2160-cta-1680x945.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84516"&gt;&lt;em&gt;Get ready to watch the map light up.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;With this upgrade, cloud gaming is about to level up. NVIDIA Blackwell RTX brings GeForce RTX 5080-powered streaming to GeForce NOW, unlocking the highest resolutions and frame rates ever in the cloud. Play games with unmatched visual fidelity, tap into a massive library of over 2,200 new Install-to-Play titles, take advantage of new high-performance modes like competitive 360 frames per second and 240 fps at sub-30-millisecond low latency, and much more.&lt;/p&gt;
&lt;p&gt;Plus, the highly anticipated shooter &lt;i&gt;Battlefield 6&lt;/i&gt; is launching in the cloud in October.&lt;/p&gt;
&lt;p&gt;But first — this month is stacked with 17 fresh games to keep members busy. Look out for &lt;i&gt;Hell Is Us&lt;/i&gt;, announced at Gamescom and the first AAA title to hit the cloud at launch, along with the eagerly awaited &lt;i&gt;Hollow Knight: Silksong, &lt;/i&gt;part of the five new games in the cloud available this week.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Unravel the Uncanny&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84537"&gt;&lt;img alt="Hell Is Us on GeForce NOW" class="size-large wp-image-84537" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/GFN_Thursday-Hell_is_Us-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84537"&gt;&lt;em&gt;Curiosity meets catastrophe.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;i&gt;Hell Is Us&lt;/i&gt; offers a visually stunning, narrative-driven action game that blends exploration, combat and psychological storytelling into a bold new adventure.&lt;/p&gt;
&lt;p&gt;Play as a lone protagonist unraveling secrets behind supernatural phenomena in a closed country plagued by civil strife and haunting mysteries. The gameplay brings together mysterious tools and decision-driven storylines among surreal landscapes and chilling lore.&lt;/p&gt;
&lt;p&gt;Explore the title at peak fidelity across devices on GeForce NOW with rich lighting and haunting scenery streamed instantly across devices. It’s an ideal way to get immersed in this uncanny world, no matter the rig or gaming setup.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Roll Out&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84542"&gt;&lt;img alt="World of Tanks 2.0 on GeForce NOW" class="size-large wp-image-84542" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/GFN_Thursday-World_of_Tanks_2_0-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84542"&gt;&lt;em&gt;Tank you very much.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Rolling onto GeForce NOW, &lt;i&gt;World of Tanks 2.0&lt;/i&gt; is a multiplayer online action game that puts players in command of massive, legendary tanks for team-based battles across historic arenas.&lt;/p&gt;
&lt;p&gt;The 2.0 release amps up the tank warfare with revamped sounds, user interfaces and battle mechanics, as well as new Tier XI vehicles . Choose among favorite armored vehicles, strategize with the squad and outmaneuver opponents in fast-paced skirmishes. Whether diving into solo showdowns or coordinating all-out assaults, every match is packed with explosive action.&lt;/p&gt;
&lt;p&gt;Play it on GeForce NOW and experience every tank clash in crystal-clear detail and fluid motion, streamed effortlessly to devices. See enhanced explosions, rich textures and lag-free combat — no downloads or updates required.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Falling for New Games&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84545"&gt;&lt;img alt="Hollow Knight Silk Song on GeForce NOW" class="wp-image-84545 size-large" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/GFN_Thursday-Hollow_Knight_Silksong-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84545"&gt;&lt;em&gt;It’s Hornet’s world now.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Get ready for one of the most anticipated launches of the year: &lt;i&gt;Hollow Knight: Silksong&lt;/i&gt; is launching on GeForce NOW.&lt;/p&gt;
&lt;p&gt;Slip into the role of Hornet and explore a breathtakingly vibrant world that’s even faster, prettier and more challenging than the original. With razor-sharp action, new combat moves and jaw-dropping hand-drawn environments, &lt;i&gt;Silksong &lt;/i&gt;will have every explorer and boss-battler hooked from the first divekick.&lt;/p&gt;
&lt;p&gt;Stream it instantly across devices and dive into epic battles, cunning platforming and secrets lurking in every corner. Whether a series veteran or new to Hallownest’s legacy, GeForce NOW members can stream the title when it launches in the cloud without waiting for downloads or updates.&lt;/p&gt;
&lt;p&gt;Catch the games ready to play today:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Hell Is Us &lt;/i&gt;(New release on Steam, Advanced Access on Sept. 1, general availability Sept. 4)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;METAL EDEN&lt;/i&gt; (New release on Steam, Sept. 2)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Hollow Knight: Silksong &lt;/i&gt;(New release on Steam and Xbox, available on PC Game Pass, Sept. 4)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Cataclismo&lt;/i&gt; (New release on Xbox, available on PC Game Pass, Sept. 4)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Cronos: The New Dawn &lt;/i&gt;(New release on Steam, Advanced Access Sept. 3, general availability Sept. 5)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Catch the full list of games coming to the cloud in September:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Firefighting Simulator: Ignite&lt;/i&gt; (New release on Steam, Sept. 9)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Borderlands 4 &lt;/i&gt;(New release on Steam and Epic Games Store, Sept. 11)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Fata Deum – The God Sim &lt;/i&gt;(New release on Steam, Sept. 15)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Dying Light: The Beast &lt;/i&gt;(New release on Steam, Sept. 19)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Jump Space &lt;/i&gt;(New release on Steam, Sept. 19)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Warborne Above Ashes &lt;/i&gt;(New release on Steam, Sept. 19)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;ENDLESS Legend 2 &lt;/i&gt;(New release on Steam, Sept. 22)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Baby Steps &lt;/i&gt;(New release on Steam, Sept. 23)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Aztecs: The Last Sun &lt;/i&gt;(New release on Steam, Sept. 23)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Lost Rift &lt;/i&gt;(New release on Steam, Sept. 25)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;CloverPit &lt;/i&gt;(New release on Steam, Sept. 26)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Predecessor &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;b&gt;Time to Leaf August Behind&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;In addition to the dozen games announced in August, an extra 22 joined over the month:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Assassin’s Creed Mirage&lt;/i&gt; (Xbox, available on PC Game Pass)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Amnesia: The Dark Descent &lt;/i&gt;(Epic Games Store)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Blacksmith Master &lt;/i&gt;(Xbox, available on PC Game Pass)&lt;i&gt;&amp;nbsp;&lt;/i&gt;&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Chip ‘n Clawz vs. The Brainioids &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Crash Bandicoot 4: It’s About Time&lt;/i&gt; (New release on Steam and Battle.net)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Gatekeeper &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Gears of War: Reloaded &lt;/i&gt;(Steam and Xbox, available on PC Game Pass)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Guntouchables&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Heretic + Hexen&lt;/i&gt; (Steam and Xbox, available on PC Game Pass)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;HUNTER×HUNTER NEN×IMPACT &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Knightica&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Make Way &lt;/i&gt;(Free on Epic Games Store)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Stick It to the Stickman&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Titan Quest II&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Total War: MEDIEVAL II – Definitive Edition&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Total War: ATTILA &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;A Total War Saga: Troy &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Total War: NAPOLEON – Definitive Edition &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Total War: EMPIRE – Definitive Edition &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Total War: PHARAOH DYNASTIES &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Total War: ROME REMASTERED &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Total War: SHOGUN 2 &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;i&gt;Field of Glory II: Medieval &lt;/i&gt;didn’t make it to the cloud in August. Stay tuned to GFN Thursday for updates.&lt;/p&gt;
&lt;p&gt;What are you planning to play this weekend? Let us know on X or in the comments below.&lt;/p&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p dir="ltr" lang="en"&gt;How good are your in-game navigation skills? 🗺️&lt;/p&gt;
&lt;p&gt;— 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) September 3, 2025&lt;/p&gt;&lt;/blockquote&gt;



		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA Blackwell RTX is coming to the cloud on Wednesday, Sept. 10 — an upgrade so big it couldn’t wait until a Thursday. Don’t miss a special early GFN Thursday next Wednesday as GeForce NOW begins lighting up the globe with GeForce RTX 5080-class power streaming from the cloud.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84516"&gt;&lt;img alt="Blackwell RTX upgrade rollout is coming soon to GeForce NOW" class="size-large wp-image-84516" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/kv-3840x2160-cta-1680x945.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84516"&gt;&lt;em&gt;Get ready to watch the map light up.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;With this upgrade, cloud gaming is about to level up. NVIDIA Blackwell RTX brings GeForce RTX 5080-powered streaming to GeForce NOW, unlocking the highest resolutions and frame rates ever in the cloud. Play games with unmatched visual fidelity, tap into a massive library of over 2,200 new Install-to-Play titles, take advantage of new high-performance modes like competitive 360 frames per second and 240 fps at sub-30-millisecond low latency, and much more.&lt;/p&gt;
&lt;p&gt;Plus, the highly anticipated shooter &lt;i&gt;Battlefield 6&lt;/i&gt; is launching in the cloud in October.&lt;/p&gt;
&lt;p&gt;But first — this month is stacked with 17 fresh games to keep members busy. Look out for &lt;i&gt;Hell Is Us&lt;/i&gt;, announced at Gamescom and the first AAA title to hit the cloud at launch, along with the eagerly awaited &lt;i&gt;Hollow Knight: Silksong, &lt;/i&gt;part of the five new games in the cloud available this week.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Unravel the Uncanny&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84537"&gt;&lt;img alt="Hell Is Us on GeForce NOW" class="size-large wp-image-84537" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/GFN_Thursday-Hell_is_Us-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84537"&gt;&lt;em&gt;Curiosity meets catastrophe.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;i&gt;Hell Is Us&lt;/i&gt; offers a visually stunning, narrative-driven action game that blends exploration, combat and psychological storytelling into a bold new adventure.&lt;/p&gt;
&lt;p&gt;Play as a lone protagonist unraveling secrets behind supernatural phenomena in a closed country plagued by civil strife and haunting mysteries. The gameplay brings together mysterious tools and decision-driven storylines among surreal landscapes and chilling lore.&lt;/p&gt;
&lt;p&gt;Explore the title at peak fidelity across devices on GeForce NOW with rich lighting and haunting scenery streamed instantly across devices. It’s an ideal way to get immersed in this uncanny world, no matter the rig or gaming setup.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Roll Out&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84542"&gt;&lt;img alt="World of Tanks 2.0 on GeForce NOW" class="size-large wp-image-84542" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/GFN_Thursday-World_of_Tanks_2_0-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84542"&gt;&lt;em&gt;Tank you very much.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Rolling onto GeForce NOW, &lt;i&gt;World of Tanks 2.0&lt;/i&gt; is a multiplayer online action game that puts players in command of massive, legendary tanks for team-based battles across historic arenas.&lt;/p&gt;
&lt;p&gt;The 2.0 release amps up the tank warfare with revamped sounds, user interfaces and battle mechanics, as well as new Tier XI vehicles . Choose among favorite armored vehicles, strategize with the squad and outmaneuver opponents in fast-paced skirmishes. Whether diving into solo showdowns or coordinating all-out assaults, every match is packed with explosive action.&lt;/p&gt;
&lt;p&gt;Play it on GeForce NOW and experience every tank clash in crystal-clear detail and fluid motion, streamed effortlessly to devices. See enhanced explosions, rich textures and lag-free combat — no downloads or updates required.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Falling for New Games&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84545"&gt;&lt;img alt="Hollow Knight Silk Song on GeForce NOW" class="wp-image-84545 size-large" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/GFN_Thursday-Hollow_Knight_Silksong-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84545"&gt;&lt;em&gt;It’s Hornet’s world now.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Get ready for one of the most anticipated launches of the year: &lt;i&gt;Hollow Knight: Silksong&lt;/i&gt; is launching on GeForce NOW.&lt;/p&gt;
&lt;p&gt;Slip into the role of Hornet and explore a breathtakingly vibrant world that’s even faster, prettier and more challenging than the original. With razor-sharp action, new combat moves and jaw-dropping hand-drawn environments, &lt;i&gt;Silksong &lt;/i&gt;will have every explorer and boss-battler hooked from the first divekick.&lt;/p&gt;
&lt;p&gt;Stream it instantly across devices and dive into epic battles, cunning platforming and secrets lurking in every corner. Whether a series veteran or new to Hallownest’s legacy, GeForce NOW members can stream the title when it launches in the cloud without waiting for downloads or updates.&lt;/p&gt;
&lt;p&gt;Catch the games ready to play today:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Hell Is Us &lt;/i&gt;(New release on Steam, Advanced Access on Sept. 1, general availability Sept. 4)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;METAL EDEN&lt;/i&gt; (New release on Steam, Sept. 2)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Hollow Knight: Silksong &lt;/i&gt;(New release on Steam and Xbox, available on PC Game Pass, Sept. 4)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Cataclismo&lt;/i&gt; (New release on Xbox, available on PC Game Pass, Sept. 4)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Cronos: The New Dawn &lt;/i&gt;(New release on Steam, Advanced Access Sept. 3, general availability Sept. 5)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Catch the full list of games coming to the cloud in September:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Firefighting Simulator: Ignite&lt;/i&gt; (New release on Steam, Sept. 9)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Borderlands 4 &lt;/i&gt;(New release on Steam and Epic Games Store, Sept. 11)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Fata Deum – The God Sim &lt;/i&gt;(New release on Steam, Sept. 15)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Dying Light: The Beast &lt;/i&gt;(New release on Steam, Sept. 19)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Jump Space &lt;/i&gt;(New release on Steam, Sept. 19)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Warborne Above Ashes &lt;/i&gt;(New release on Steam, Sept. 19)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;ENDLESS Legend 2 &lt;/i&gt;(New release on Steam, Sept. 22)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Baby Steps &lt;/i&gt;(New release on Steam, Sept. 23)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Aztecs: The Last Sun &lt;/i&gt;(New release on Steam, Sept. 23)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Lost Rift &lt;/i&gt;(New release on Steam, Sept. 25)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;CloverPit &lt;/i&gt;(New release on Steam, Sept. 26)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Predecessor &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;b&gt;Time to Leaf August Behind&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;In addition to the dozen games announced in August, an extra 22 joined over the month:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Assassin’s Creed Mirage&lt;/i&gt; (Xbox, available on PC Game Pass)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Amnesia: The Dark Descent &lt;/i&gt;(Epic Games Store)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Blacksmith Master &lt;/i&gt;(Xbox, available on PC Game Pass)&lt;i&gt;&amp;nbsp;&lt;/i&gt;&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Chip ‘n Clawz vs. The Brainioids &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Crash Bandicoot 4: It’s About Time&lt;/i&gt; (New release on Steam and Battle.net)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Gatekeeper &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Gears of War: Reloaded &lt;/i&gt;(Steam and Xbox, available on PC Game Pass)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Guntouchables&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Heretic + Hexen&lt;/i&gt; (Steam and Xbox, available on PC Game Pass)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;HUNTER×HUNTER NEN×IMPACT &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Knightica&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Make Way &lt;/i&gt;(Free on Epic Games Store)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Stick It to the Stickman&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Titan Quest II&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Total War: MEDIEVAL II – Definitive Edition&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Total War: ATTILA &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;A Total War Saga: Troy &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Total War: NAPOLEON – Definitive Edition &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Total War: EMPIRE – Definitive Edition &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Total War: PHARAOH DYNASTIES &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Total War: ROME REMASTERED &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Total War: SHOGUN 2 &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;i&gt;Field of Glory II: Medieval &lt;/i&gt;didn’t make it to the cloud in August. Stay tuned to GFN Thursday for updates.&lt;/p&gt;
&lt;p&gt;What are you planning to play this weekend? Let us know on X or in the comments below.&lt;/p&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p dir="ltr" lang="en"&gt;How good are your in-game navigation skills? 🗺️&lt;/p&gt;
&lt;p&gt;— 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) September 3, 2025&lt;/p&gt;&lt;/blockquote&gt;



		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/geforce-now-thursday-sept-2025/</guid><pubDate>Thu, 04 Sep 2025 13:00:53 +0000</pubDate></item><item><title>[NEW] Only 2 days left to claim your exhibitor spot at TechCrunch Disrupt 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/04/only-2-days-left-to-claim-your-exhibitor-spot-at-techcrunch-disrupt-2025/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;We’re in the final stretch — with just two days left and only 10 tables remaining, now is the time to act if you want to shine the light on your brand at one of the most anticipated tech conferences of the year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; is coming to Moscone West in San Francisco this October 27–29. Exhibiting puts your startup in the heart of the action — where 10,000+ founders, investors, press, and decision-makers come to discover what’s next.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Your brand missing the spotlight at this tech epicenter means missed ROI. Disrupt is &lt;em&gt;the&lt;/em&gt; launchpad for scaling startups — &lt;strong&gt;claim your exhibit table before your competitor does&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2758507" height="453" src="https://techcrunch.com/wp-content/uploads/2024/05/expo_startup_fun.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-what-you-get-with-your-table"&gt;What you get with your table&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Enjoy three full days of brand spotlighting in the heart of Disrupt — where investors are scouting their next deal, your target customers are ready to buy, and the tech press is searching for the next breakthrough, plus:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;A 6’ x 30″ table with linen and chairs, complete with an 11” x 14” branded tabletop sign for all-day visibility.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;A Silver Tier sponsorship with branding across the venue, site, and app.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;10 free passes for your team to experience all the activations at Disrupt.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;High-quality lead generation.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Access to the Disrupt press list.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;And more. Check out the exhibit page to learn all the perks of exhibiting.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;Exhibitor tables are disappearing fast, and once they’re gone, they’re gone. Don’t let your competitor own the spotlight while you watch from the sidelines. &lt;strong&gt;Book now before tomorrow’s deadline, or before it sells out&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 no anniversary" class="wp-image-3040972" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/TC25_Disrupt_General_Article_No-Anniversary-at-all_Headers_1920x1080.png?w=680" width="680" /&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;We’re in the final stretch — with just two days left and only 10 tables remaining, now is the time to act if you want to shine the light on your brand at one of the most anticipated tech conferences of the year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; is coming to Moscone West in San Francisco this October 27–29. Exhibiting puts your startup in the heart of the action — where 10,000+ founders, investors, press, and decision-makers come to discover what’s next.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Your brand missing the spotlight at this tech epicenter means missed ROI. Disrupt is &lt;em&gt;the&lt;/em&gt; launchpad for scaling startups — &lt;strong&gt;claim your exhibit table before your competitor does&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2758507" height="453" src="https://techcrunch.com/wp-content/uploads/2024/05/expo_startup_fun.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-what-you-get-with-your-table"&gt;What you get with your table&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Enjoy three full days of brand spotlighting in the heart of Disrupt — where investors are scouting their next deal, your target customers are ready to buy, and the tech press is searching for the next breakthrough, plus:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;A 6’ x 30″ table with linen and chairs, complete with an 11” x 14” branded tabletop sign for all-day visibility.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;A Silver Tier sponsorship with branding across the venue, site, and app.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;10 free passes for your team to experience all the activations at Disrupt.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;High-quality lead generation.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Access to the Disrupt press list.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;And more. Check out the exhibit page to learn all the perks of exhibiting.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;Exhibitor tables are disappearing fast, and once they’re gone, they’re gone. Don’t let your competitor own the spotlight while you watch from the sidelines. &lt;strong&gt;Book now before tomorrow’s deadline, or before it sells out&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 no anniversary" class="wp-image-3040972" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/TC25_Disrupt_General_Article_No-Anniversary-at-all_Headers_1920x1080.png?w=680" width="680" /&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/04/only-2-days-left-to-claim-your-exhibitor-spot-at-techcrunch-disrupt-2025/</guid><pubDate>Thu, 04 Sep 2025 14:00:00 +0000</pubDate></item><item><title>[NEW] Captions rebrands as Mirage, expands beyond creator tools to AI video research (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/04/captions-rebrands-as-mirage-expands-beyond-creator-tools-to-ai-video-research/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Captions, an AI-powered video creation and editing app for content creators that has secured over $100 million in venture capital to date at a valuation of $500 million, is rebranding to Mirage, the company announced on Thursday.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new name reflects the company’s broader ambitions to become an AI research lab focused on multimodal foundational models specifically designed for short-form video content for platforms like TikTok, Reels, and Shorts. The company believes this approach will distinguish it from traditional AI models and competitors such as D-ID, Synthesia, and Hour One.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The rebranding will also unify the company’s offerings under one umbrella, bringing together the flagship creator-focused AI video platform, Captions, and the recently launched Mirage Studio, which caters to brands and ad production.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The way we see it, the real race for AI video hasn’t begun. Our new identity, Mirage, reflects our expanded vision and commitment to redefining the video category, starting with short-form video, through frontier AI research and models,” CEO Gaurav Misra told TechCrunch.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3042357" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/Mirage-_-Product-Image.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Mirage&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The sales pitch behind Mirage Studio, which launched in June, focuses on enabling brands to create short advertisements without relying on human talent or large budgets. By simply submitting an audio file, the AI generates video content from scratch, with an AI-generated background and custom AI avatars.&amp;nbsp;Users can also upload selfies to create an avatar using their likeness. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What sets the platform apart, according to the company, is its ability to produce AI avatars that have natural-looking speech, movements, and facial expressions. Additionally, Mirage says it doesn’t rely on existing stock footage, voice cloning, or lip-syncing.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mirage Studio is available under the business plan, which costs $399 per month for 8,000 credits. New users receive 50% off the first month.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;[embedded content]&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While these tools will likely benefit brands wanting to streamline video production and save some money, they also spark concerns around the potential impact on the creative workforce. The growing use of AI in advertisements has prompted backlash, as seen in a recent Guess ad in Vogue’s July print edition that featured an AI-generated model.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, as this technology becomes more advanced, distinguishing between real and deepfake videos becomes increasingly difficult. It’s a difficult pill to swallow for many people, especially given how quickly misinformation can spread these days.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Mirage recently addressed its role in deepfake technology in a blog post. The company acknowledged the genuine risks of misinformation while also expressing optimism about the positive potential of AI video. It mentioned that it has put moderation measures in place to limit misuse, such as preventing impersonation and requiring consent for likeness use.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, the company emphasized that “design isn’t a catch-all” and that the real solution lies in fostering a “new kind of media literacy” where people approach video content with the same critical eye as they do news headlines.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Captions, an AI-powered video creation and editing app for content creators that has secured over $100 million in venture capital to date at a valuation of $500 million, is rebranding to Mirage, the company announced on Thursday.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new name reflects the company’s broader ambitions to become an AI research lab focused on multimodal foundational models specifically designed for short-form video content for platforms like TikTok, Reels, and Shorts. The company believes this approach will distinguish it from traditional AI models and competitors such as D-ID, Synthesia, and Hour One.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The rebranding will also unify the company’s offerings under one umbrella, bringing together the flagship creator-focused AI video platform, Captions, and the recently launched Mirage Studio, which caters to brands and ad production.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The way we see it, the real race for AI video hasn’t begun. Our new identity, Mirage, reflects our expanded vision and commitment to redefining the video category, starting with short-form video, through frontier AI research and models,” CEO Gaurav Misra told TechCrunch.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3042357" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/Mirage-_-Product-Image.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Mirage&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The sales pitch behind Mirage Studio, which launched in June, focuses on enabling brands to create short advertisements without relying on human talent or large budgets. By simply submitting an audio file, the AI generates video content from scratch, with an AI-generated background and custom AI avatars.&amp;nbsp;Users can also upload selfies to create an avatar using their likeness. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What sets the platform apart, according to the company, is its ability to produce AI avatars that have natural-looking speech, movements, and facial expressions. Additionally, Mirage says it doesn’t rely on existing stock footage, voice cloning, or lip-syncing.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mirage Studio is available under the business plan, which costs $399 per month for 8,000 credits. New users receive 50% off the first month.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;[embedded content]&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While these tools will likely benefit brands wanting to streamline video production and save some money, they also spark concerns around the potential impact on the creative workforce. The growing use of AI in advertisements has prompted backlash, as seen in a recent Guess ad in Vogue’s July print edition that featured an AI-generated model.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, as this technology becomes more advanced, distinguishing between real and deepfake videos becomes increasingly difficult. It’s a difficult pill to swallow for many people, especially given how quickly misinformation can spread these days.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Mirage recently addressed its role in deepfake technology in a blog post. The company acknowledged the genuine risks of misinformation while also expressing optimism about the positive potential of AI video. It mentioned that it has put moderation measures in place to limit misuse, such as preventing impersonation and requiring consent for likeness use.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, the company emphasized that “design isn’t a catch-all” and that the real solution lies in fostering a “new kind of media literacy” where people approach video content with the same critical eye as they do news headlines.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/04/captions-rebrands-as-mirage-expands-beyond-creator-tools-to-ai-video-research/</guid><pubDate>Thu, 04 Sep 2025 15:39:15 +0000</pubDate></item><item><title>[NEW] Cloud provider Lambda may be gearing up for an IPO (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/04/cloud-provider-lambda-may-be-gearing-up-for-an-ipo/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/cloud-computing-getty.jpg?resize=1200,750" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Cloud provider Lambda might be following rival CoreWeave to the public markets.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lambda, an AI infrastructure company offering on-demand GPUs, has hired banks for an upcoming IPO, according to reporting from The Information. Lambda has reportedly hired Morgan Stanley, J.P. Morgan, and Citi for a public listing that could happen as early as the first half of 2026.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Lambda did not respond to a request for comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has raised more than $1.7 billion in funding, according to Crunchbase data, from investors including Nvidia, Alumni Ventures, and Andra Capital, among others. It most recently raised $480 million in a Series D round in February.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CoreWeave, Lambda’s biggest rival, went public in March of this year.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/cloud-computing-getty.jpg?resize=1200,750" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Cloud provider Lambda might be following rival CoreWeave to the public markets.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lambda, an AI infrastructure company offering on-demand GPUs, has hired banks for an upcoming IPO, according to reporting from The Information. Lambda has reportedly hired Morgan Stanley, J.P. Morgan, and Citi for a public listing that could happen as early as the first half of 2026.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Lambda did not respond to a request for comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has raised more than $1.7 billion in funding, according to Crunchbase data, from investors including Nvidia, Alumni Ventures, and Andra Capital, among others. It most recently raised $480 million in a Series D round in February.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CoreWeave, Lambda’s biggest rival, went public in March of this year.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/04/cloud-provider-lambda-may-be-gearing-up-for-an-ipo/</guid><pubDate>Thu, 04 Sep 2025 15:43:06 +0000</pubDate></item><item><title>[NEW] Google Photos upgrades its image-to-video feature with Veo 3 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/04/google-photos-adds-ai-video-generation-with-veo-3/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google’s latest video-generation model, Veo 3, is coming to Google Photos. The new model, available on the mobile app’s Create tab, will allow users in the U.S. to turn their still images into video clips.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google Photos already offers video generation through its recently added “Photo to video” feature, but the company says the addition of Veo 3 enhances that functionality with higher-quality video. The launch also represents how the company is working to bring its latest AI tech to consumers through its products — Google Photos, for instance, had over 1.5 billion monthly active users as of May 2025.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3042346" height="680" src="https://techcrunch.com/wp-content/uploads/2025/09/google-photos-veo-3.jpg?w=391" width="391" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Introduced in May at its I/O developer conference, Google brought Veo 3, which added image-to-video generation, to its Gemini app in July, making it available on its AI Ultra and AI Pro subscription plans. On those plans, users could generate three videos per day, which would carry both visible and invisible watermarks to identify the videos as being AI generated.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In Google Photos, the company imagines users bringing memories to life, or even animating older photos. The existing image-to-video feature is powered by Veo 2 and lets users select a photo from their gallery and choose one of two prompts for either “subtle movements” or a surprise animation by tapping an “I’m feeling lucky” button. The model then generates a six-second clip you could share with others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With Veo 3, the feature will remain free with a limited number of generations available, says Google. AI Pro and AI Ultra subscribers will have access to more generations. However, it won’t support audio and its videos will be four seconds long.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new photo-to-video feature is available within the Create hub, a new section in the Google Photos app where users can explore creative tools and features powered by AI. In addition to Veo 3, these include a remix option to change a photo’s style; make a collage; put together montages from your galleries; create moving, 3D photos called “cinematic” photos; and a tool to make GIFs from pics.&lt;/p&gt;


&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google’s latest video-generation model, Veo 3, is coming to Google Photos. The new model, available on the mobile app’s Create tab, will allow users in the U.S. to turn their still images into video clips.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google Photos already offers video generation through its recently added “Photo to video” feature, but the company says the addition of Veo 3 enhances that functionality with higher-quality video. The launch also represents how the company is working to bring its latest AI tech to consumers through its products — Google Photos, for instance, had over 1.5 billion monthly active users as of May 2025.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3042346" height="680" src="https://techcrunch.com/wp-content/uploads/2025/09/google-photos-veo-3.jpg?w=391" width="391" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Introduced in May at its I/O developer conference, Google brought Veo 3, which added image-to-video generation, to its Gemini app in July, making it available on its AI Ultra and AI Pro subscription plans. On those plans, users could generate three videos per day, which would carry both visible and invisible watermarks to identify the videos as being AI generated.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In Google Photos, the company imagines users bringing memories to life, or even animating older photos. The existing image-to-video feature is powered by Veo 2 and lets users select a photo from their gallery and choose one of two prompts for either “subtle movements” or a surprise animation by tapping an “I’m feeling lucky” button. The model then generates a six-second clip you could share with others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With Veo 3, the feature will remain free with a limited number of generations available, says Google. AI Pro and AI Ultra subscribers will have access to more generations. However, it won’t support audio and its videos will be four seconds long.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new photo-to-video feature is available within the Create hub, a new section in the Google Photos app where users can explore creative tools and features powered by AI. In addition to Veo 3, these include a remix option to change a photo’s style; make a collage; put together montages from your galleries; create moving, 3D photos called “cinematic” photos; and a tool to make GIFs from pics.&lt;/p&gt;


&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/04/google-photos-adds-ai-video-generation-with-veo-3/</guid><pubDate>Thu, 04 Sep 2025 16:00:00 +0000</pubDate></item><item><title>[NEW] AI On: 6 Ways AI Agents Are Raising Team Performance — and How to Measure It (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/ways-ai-agents-are-raising-team-performance/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This post is part of the &lt;/i&gt;&lt;i&gt;AI On&lt;/i&gt;&lt;i&gt; blog series, which explores the latest techniques and real-world applications of agentic AI, chatbots and copilots. The series also highlights the NVIDIA software and hardware powering advanced AI agents, which form the foundation of AI query engines that gather insights and perform tasks to transform everyday experiences and reshape industries.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;AI agents are expected to be involved in most business tasks within three years, with effective human-agent collaboration projected to increase human engagement in high-value tasks by 65%.&lt;/p&gt;
&lt;p&gt;AI agents can help achieve and exceed efficiency goals as they learn, reason and adjust based on context and outcomes. As they become increasingly central to business strategies, understanding where they deliver impact and justify investment is essential for leaders.&lt;/p&gt;
&lt;p&gt;Here are six ways agentic AI boosts team performance — and practical tips for measuring its impact.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;1. Accelerating Software Development With AI Agents&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;AI agents can act as intelligent copilots, helping automate code generation, testing and deployment.&lt;/p&gt;
&lt;p&gt;They can pinpoint errors early, resulting in higher-quality, faster releases, and speed onboarding of new engineers by providing AI-curated information and context on documentation.&lt;/p&gt;
&lt;p&gt;For example, NVIDIA ChipNeMo — a team of specialized agents built on custom large language models (LLMs) and trained on NVIDIA’s internal chip design data — helped 5,000 NVIDIA engineers in design, verification and documentation save 4,000 engineering days in just one year.&lt;/p&gt;
&lt;p&gt;Since deployment, ChipNeMo has:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Infographic that conveys NVIDIA ChipNeMo has: Demonstrated 85%+ response accuracy, reflecting its reliability in real-world applications. Cut time spent sourcing technical answers from hours to seconds, streamlining development and troubleshooting. Accelerated verification cycles by identifying test gaps and diagnosing failures, addressing workflows that can take 30-50% of typical development schedules." class="aligncenter wp-image-84456 size-large" height="672" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/ai-on-chipnemo-infographic-1680x672.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;Learn about building agents with NVIDIA Nemotron and improving AI code generation using NVIDIA NeMo Agent Toolkit.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;2. Driving Data-Backed Decision-Making&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Agents can help businesses across industries easily glean insights from complex, time-sensitive data for critical decision-making, such as on investments or business strategy.&lt;/p&gt;
&lt;p&gt;BlackRock’s Aladdin Copilot — an embedded AI assistant serving thousands of users across hundreds of financial institutions — lets teams garner portfolio insights, assess investment research and monitor available cash balances through simple text prompts. It’s helped reduce research time from minutes to seconds while enhancing data-driven investment decisions.&lt;/p&gt;
&lt;p&gt;VAST Data uses agents to rapidly gather and synthesize information from internal and external sources. For its sales teams, this means faster access to useful, up-to-date insights on client accounts.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;3. Optimizing IT Operations&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Agents excel at maintaining IT operations, including by proactively monitoring infrastructure and automating decision-making.&lt;/p&gt;
&lt;p&gt;AI agents in IT operations offer:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Infographic that conveys that AI agents in IT operations offer — Faster issue resolution: Self-service IT support agents can quickly resolve tickets and automate routine tasks, improving user experiences. Security automation: AI agents facilitate investigation and triage in security operations, helping teams respond to threats swiftly and with greater accuracy. Enterprise search: Agents power advanced search across organizational data, surfacing insights and maintaining institutional knowledge." class="aligncenter size-large wp-image-84459" height="672" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/ai-on-agents-in-it-infographic-1680x672.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;In fast-paced telco environments, agents can help manage networks by analyzing real-time performance indicators and predicting service failures. For example, Telenor Group integrated the NVIDIA Blueprint for telco network configuration to deploy intelligent, autonomous networks that meet the performance demands of 5G and beyond.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;4. Streamlining Industrial and Manufacturing Operations&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Able to interact with the physical world, video analytics AI agents can monitor assembly lines for quality checks and anomaly detection.&lt;/p&gt;
&lt;p&gt;Pegatron developed the PEGA AI Factory platform to accelerate the development of AI agents across the company by 400% in the last four years. In addition, the company’s digital twin platform PEGAVERSE was built on the NVIDIA Omniverse platform and lets engineers virtually simulate, test and optimize production lines before they’re built, cutting factory construction time by 40%.&lt;/p&gt;
&lt;p&gt;Pegatron also augmented its assembly process using video analytics AI agents, powered by NVIDIA AI Blueprint for video search and summarization, and saw a 7% reduction in labor costs per assembly line and a 67% decrease in defect rates.&lt;/p&gt;
&lt;p&gt;Siemens is bringing generative AI into their solutions with the Industrial Copilot to tap real-time factory data to guide maintenance technicians and shopfloor operators. Interviews with maintenance engineers indicate that this could save on average 25% reactive maintenance time.&lt;/p&gt;
&lt;p&gt;Foxconn uses digital twins and AI agents to optimize its production lines, reducing deployment time by 50%, as well as to simulate robots and monitor quality and safety in real time.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;5. Enhancing Customer Service&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Agents excel at handling customer service at scale, reducing customer wait times by handling thousands of inquiries simultaneously.&lt;/p&gt;
&lt;p&gt;AT&amp;amp;T employees and contractors use a generative AI solution called “Ask AT&amp;amp;T,” which has over 100 solutions and agents in production. Built with LLMs served by NVIDIA NeMo and NIM microservices, Ask AT&amp;amp;T helps fetch relevant documentation and autonomously resolve routine inquiries.&lt;/p&gt;
&lt;p&gt;Offering 24/7 personalized support, Ask AT&amp;amp;T shares context-relevant suggestions by recalling organizational information from emails, meetings and past transactions. And to continuously improve agent performance, real-time feedback loops are built into the system using a data flywheel.&lt;/p&gt;
&lt;p&gt;These automated services resulted in 84% lower call center transcript analytics costs.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;6. Delivering Personalized Education&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;AI agents are making individualized learning support more accessible, scalable and effective while freeing up instructors for more in-depth teaching.&lt;/p&gt;
&lt;p&gt;Faced with surging class sizes and a shortage of teaching assistants, Clemson University developed an AI-powered TA — built with the NVIDIA Blueprint for retrieval-augmented generation — to guide students through challenging concepts.&lt;/p&gt;
&lt;p&gt;Rather than simply providing answers, the virtual TA walks students through problems step by step, encouraging active problem-solving and critical thinking to promote deeper understanding and academic integrity.&lt;/p&gt;
&lt;p&gt;The assistant also personalizes feedback and hints in alignment with course content, assignment deadlines and student submissions. It operates 24/7, giving every student timely, tailored support regardless of enrollment size.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;How Can the Success of AI Agents Be Measured?&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Measuring the impact of AI agents isn’t just a box to check — it’s essential to maximizing investment. The way users define success will directly shape how well these systems deliver value. Too often, businesses deploy agents without a clear measurement framework, making it difficult to prove return on investment or identify areas for improvement.&lt;/p&gt;
&lt;p&gt;When setting up an evaluation strategy, users should consider which metrics matter most for their goals. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Adoption and engagement: &lt;/b&gt;Track whether the technology is being embraced. Metrics include how many eligible users interact with the agent — and how frequently — along with how long the sessions last. High engagement means the agent is routinely providing effective support.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Task completion: &lt;/b&gt;Look beyond usage to outcomes. Measure how many tasks or requests the agent handles and what portions are fulfilled without human intervention. In software development, users can measure the automated code generation rate to see how much of the software is being developed by an agent. A high automated task completion rate means employees are freed up for higher-value work.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Productivity and efficiency gains: &lt;/b&gt;Quantify time saved. Metrics like time to resolve IT issues, report generation time for decision-making and average handling time for customer service interactions help demonstrate clear efficiency improvements.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Business outcomes: &lt;/b&gt;Connect agent performance to bottom-line results. This could mean cost per interaction in support, time to market in software development or unplanned downtime reduction in IT operations.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;High-quality user experience: &lt;/b&gt;Ensure the system is both trusted and effective. Consider a code quality score for developers, prediction accuracy in data-backed decision-making or customer satisfaction scores in service scenarios.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The key takeaway: measuring AI agent success goes far beyond a single number. Adoption, efficiency, accuracy and business impact all matter. By choosing the right mix of metrics upfront, businesses can validate success while continually refining and improving how agents deliver value.&lt;/p&gt;
&lt;p&gt;Read more stories on how customers are adopting AI applications to reshape their daily operations and increase their return on investment.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Stay up to date on agentic AI, NVIDIA Nemotron and more by subscribing to &lt;/i&gt;&lt;i&gt;NVIDIA news&lt;/i&gt;&lt;i&gt;,&lt;/i&gt;&lt;i&gt; joining the community&lt;/i&gt;&lt;i&gt; and following NVIDIA AI on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;. Plus, explore &lt;/i&gt;&lt;i&gt;self-paced video tutorials and livestreams&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This post is part of the &lt;/i&gt;&lt;i&gt;AI On&lt;/i&gt;&lt;i&gt; blog series, which explores the latest techniques and real-world applications of agentic AI, chatbots and copilots. The series also highlights the NVIDIA software and hardware powering advanced AI agents, which form the foundation of AI query engines that gather insights and perform tasks to transform everyday experiences and reshape industries.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;AI agents are expected to be involved in most business tasks within three years, with effective human-agent collaboration projected to increase human engagement in high-value tasks by 65%.&lt;/p&gt;
&lt;p&gt;AI agents can help achieve and exceed efficiency goals as they learn, reason and adjust based on context and outcomes. As they become increasingly central to business strategies, understanding where they deliver impact and justify investment is essential for leaders.&lt;/p&gt;
&lt;p&gt;Here are six ways agentic AI boosts team performance — and practical tips for measuring its impact.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;1. Accelerating Software Development With AI Agents&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;AI agents can act as intelligent copilots, helping automate code generation, testing and deployment.&lt;/p&gt;
&lt;p&gt;They can pinpoint errors early, resulting in higher-quality, faster releases, and speed onboarding of new engineers by providing AI-curated information and context on documentation.&lt;/p&gt;
&lt;p&gt;For example, NVIDIA ChipNeMo — a team of specialized agents built on custom large language models (LLMs) and trained on NVIDIA’s internal chip design data — helped 5,000 NVIDIA engineers in design, verification and documentation save 4,000 engineering days in just one year.&lt;/p&gt;
&lt;p&gt;Since deployment, ChipNeMo has:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Infographic that conveys NVIDIA ChipNeMo has: Demonstrated 85%+ response accuracy, reflecting its reliability in real-world applications. Cut time spent sourcing technical answers from hours to seconds, streamlining development and troubleshooting. Accelerated verification cycles by identifying test gaps and diagnosing failures, addressing workflows that can take 30-50% of typical development schedules." class="aligncenter wp-image-84456 size-large" height="672" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/ai-on-chipnemo-infographic-1680x672.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;Learn about building agents with NVIDIA Nemotron and improving AI code generation using NVIDIA NeMo Agent Toolkit.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;2. Driving Data-Backed Decision-Making&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Agents can help businesses across industries easily glean insights from complex, time-sensitive data for critical decision-making, such as on investments or business strategy.&lt;/p&gt;
&lt;p&gt;BlackRock’s Aladdin Copilot — an embedded AI assistant serving thousands of users across hundreds of financial institutions — lets teams garner portfolio insights, assess investment research and monitor available cash balances through simple text prompts. It’s helped reduce research time from minutes to seconds while enhancing data-driven investment decisions.&lt;/p&gt;
&lt;p&gt;VAST Data uses agents to rapidly gather and synthesize information from internal and external sources. For its sales teams, this means faster access to useful, up-to-date insights on client accounts.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;3. Optimizing IT Operations&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Agents excel at maintaining IT operations, including by proactively monitoring infrastructure and automating decision-making.&lt;/p&gt;
&lt;p&gt;AI agents in IT operations offer:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Infographic that conveys that AI agents in IT operations offer — Faster issue resolution: Self-service IT support agents can quickly resolve tickets and automate routine tasks, improving user experiences. Security automation: AI agents facilitate investigation and triage in security operations, helping teams respond to threats swiftly and with greater accuracy. Enterprise search: Agents power advanced search across organizational data, surfacing insights and maintaining institutional knowledge." class="aligncenter size-large wp-image-84459" height="672" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/ai-on-agents-in-it-infographic-1680x672.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;In fast-paced telco environments, agents can help manage networks by analyzing real-time performance indicators and predicting service failures. For example, Telenor Group integrated the NVIDIA Blueprint for telco network configuration to deploy intelligent, autonomous networks that meet the performance demands of 5G and beyond.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;4. Streamlining Industrial and Manufacturing Operations&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Able to interact with the physical world, video analytics AI agents can monitor assembly lines for quality checks and anomaly detection.&lt;/p&gt;
&lt;p&gt;Pegatron developed the PEGA AI Factory platform to accelerate the development of AI agents across the company by 400% in the last four years. In addition, the company’s digital twin platform PEGAVERSE was built on the NVIDIA Omniverse platform and lets engineers virtually simulate, test and optimize production lines before they’re built, cutting factory construction time by 40%.&lt;/p&gt;
&lt;p&gt;Pegatron also augmented its assembly process using video analytics AI agents, powered by NVIDIA AI Blueprint for video search and summarization, and saw a 7% reduction in labor costs per assembly line and a 67% decrease in defect rates.&lt;/p&gt;
&lt;p&gt;Siemens is bringing generative AI into their solutions with the Industrial Copilot to tap real-time factory data to guide maintenance technicians and shopfloor operators. Interviews with maintenance engineers indicate that this could save on average 25% reactive maintenance time.&lt;/p&gt;
&lt;p&gt;Foxconn uses digital twins and AI agents to optimize its production lines, reducing deployment time by 50%, as well as to simulate robots and monitor quality and safety in real time.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;5. Enhancing Customer Service&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Agents excel at handling customer service at scale, reducing customer wait times by handling thousands of inquiries simultaneously.&lt;/p&gt;
&lt;p&gt;AT&amp;amp;T employees and contractors use a generative AI solution called “Ask AT&amp;amp;T,” which has over 100 solutions and agents in production. Built with LLMs served by NVIDIA NeMo and NIM microservices, Ask AT&amp;amp;T helps fetch relevant documentation and autonomously resolve routine inquiries.&lt;/p&gt;
&lt;p&gt;Offering 24/7 personalized support, Ask AT&amp;amp;T shares context-relevant suggestions by recalling organizational information from emails, meetings and past transactions. And to continuously improve agent performance, real-time feedback loops are built into the system using a data flywheel.&lt;/p&gt;
&lt;p&gt;These automated services resulted in 84% lower call center transcript analytics costs.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;6. Delivering Personalized Education&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;AI agents are making individualized learning support more accessible, scalable and effective while freeing up instructors for more in-depth teaching.&lt;/p&gt;
&lt;p&gt;Faced with surging class sizes and a shortage of teaching assistants, Clemson University developed an AI-powered TA — built with the NVIDIA Blueprint for retrieval-augmented generation — to guide students through challenging concepts.&lt;/p&gt;
&lt;p&gt;Rather than simply providing answers, the virtual TA walks students through problems step by step, encouraging active problem-solving and critical thinking to promote deeper understanding and academic integrity.&lt;/p&gt;
&lt;p&gt;The assistant also personalizes feedback and hints in alignment with course content, assignment deadlines and student submissions. It operates 24/7, giving every student timely, tailored support regardless of enrollment size.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;How Can the Success of AI Agents Be Measured?&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Measuring the impact of AI agents isn’t just a box to check — it’s essential to maximizing investment. The way users define success will directly shape how well these systems deliver value. Too often, businesses deploy agents without a clear measurement framework, making it difficult to prove return on investment or identify areas for improvement.&lt;/p&gt;
&lt;p&gt;When setting up an evaluation strategy, users should consider which metrics matter most for their goals. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Adoption and engagement: &lt;/b&gt;Track whether the technology is being embraced. Metrics include how many eligible users interact with the agent — and how frequently — along with how long the sessions last. High engagement means the agent is routinely providing effective support.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Task completion: &lt;/b&gt;Look beyond usage to outcomes. Measure how many tasks or requests the agent handles and what portions are fulfilled without human intervention. In software development, users can measure the automated code generation rate to see how much of the software is being developed by an agent. A high automated task completion rate means employees are freed up for higher-value work.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Productivity and efficiency gains: &lt;/b&gt;Quantify time saved. Metrics like time to resolve IT issues, report generation time for decision-making and average handling time for customer service interactions help demonstrate clear efficiency improvements.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Business outcomes: &lt;/b&gt;Connect agent performance to bottom-line results. This could mean cost per interaction in support, time to market in software development or unplanned downtime reduction in IT operations.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;High-quality user experience: &lt;/b&gt;Ensure the system is both trusted and effective. Consider a code quality score for developers, prediction accuracy in data-backed decision-making or customer satisfaction scores in service scenarios.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The key takeaway: measuring AI agent success goes far beyond a single number. Adoption, efficiency, accuracy and business impact all matter. By choosing the right mix of metrics upfront, businesses can validate success while continually refining and improving how agents deliver value.&lt;/p&gt;
&lt;p&gt;Read more stories on how customers are adopting AI applications to reshape their daily operations and increase their return on investment.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Stay up to date on agentic AI, NVIDIA Nemotron and more by subscribing to &lt;/i&gt;&lt;i&gt;NVIDIA news&lt;/i&gt;&lt;i&gt;,&lt;/i&gt;&lt;i&gt; joining the community&lt;/i&gt;&lt;i&gt; and following NVIDIA AI on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;. Plus, explore &lt;/i&gt;&lt;i&gt;self-paced video tutorials and livestreams&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/ways-ai-agents-are-raising-team-performance/</guid><pubDate>Thu, 04 Sep 2025 16:00:59 +0000</pubDate></item><item><title>[NEW] Imagining the future of banking with agentic AI (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/09/04/1123023/imagining-the-future-of-banking-with-agentic-ai/</link><description>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In association with&lt;/span&gt;EY&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Agentic AI is coming of age. And with it comes new opportunities in the financial services sector. Banks are increasingly employing agentic AI to optimize processes, navigate complex systems, and sift through vast quantities of unstructured data to make decisions and take actions—with or without human involvement. “With the maturing of agentic AI, it is becoming a lot more technologically possible for large-scale process automation that was not possible with rules-based approaches like robotic process automation before,” says Sameer Gupta, Americas financial services AI leader at EY. “That moves the needle in terms of cost, efficiency, and customer experience impact.”&lt;/p&gt;  &lt;figure class="wp-block-image alignright size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-1123027" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/MIT_EY_V10_Aug1325_cover_6ee359.png?w=1555" width="1555" /&gt;&lt;/figure&gt;  &lt;p&gt;From responding to customer services requests, to automating loan approvals, adjusting bill payments to align with regular paychecks, or extracting key terms and conditions from financial agreements, agentic AI has the potential to transform the customer experience—and how financial institutions operate too. &lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;  &lt;p&gt;Adapting to new and emerging technologies like agentic AI is essential for an organization’s survival, says Murli Buluswar, head of US personal banking analytics at Citi. “A company’s ability to adopt new technical capabilities and rearchitect how their firm operates is going to make the difference between the firms that succeed and those that get left behind,” says Buluswar. “Your people and your firm must recognize that how they go about their work is going to be meaningfully different.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;The emerging landscape&lt;/h3&gt;  &lt;p&gt;Agentic AI is already being rapidly adopted in the banking sector. A 2025 survey of 250 banking executives by MIT Technology Review Insights found that 70% of leaders say their firm uses agentic AI to some degree, either through existing deployments (16%) or pilot projects (52%). And it is already proving effective in a range of different functions. More than half of executives say agentic AI systems are highly capable of improving fraud detection (56%) and security (51%). Other strong use cases include reducing cost and increasing efficiency (41%) and improving the customer experience (41%).&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Download the report.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. It was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In association with&lt;/span&gt;EY&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Agentic AI is coming of age. And with it comes new opportunities in the financial services sector. Banks are increasingly employing agentic AI to optimize processes, navigate complex systems, and sift through vast quantities of unstructured data to make decisions and take actions—with or without human involvement. “With the maturing of agentic AI, it is becoming a lot more technologically possible for large-scale process automation that was not possible with rules-based approaches like robotic process automation before,” says Sameer Gupta, Americas financial services AI leader at EY. “That moves the needle in terms of cost, efficiency, and customer experience impact.”&lt;/p&gt;  &lt;figure class="wp-block-image alignright size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-1123027" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/MIT_EY_V10_Aug1325_cover_6ee359.png?w=1555" width="1555" /&gt;&lt;/figure&gt;  &lt;p&gt;From responding to customer services requests, to automating loan approvals, adjusting bill payments to align with regular paychecks, or extracting key terms and conditions from financial agreements, agentic AI has the potential to transform the customer experience—and how financial institutions operate too. &lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;  &lt;p&gt;Adapting to new and emerging technologies like agentic AI is essential for an organization’s survival, says Murli Buluswar, head of US personal banking analytics at Citi. “A company’s ability to adopt new technical capabilities and rearchitect how their firm operates is going to make the difference between the firms that succeed and those that get left behind,” says Buluswar. “Your people and your firm must recognize that how they go about their work is going to be meaningfully different.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;The emerging landscape&lt;/h3&gt;  &lt;p&gt;Agentic AI is already being rapidly adopted in the banking sector. A 2025 survey of 250 banking executives by MIT Technology Review Insights found that 70% of leaders say their firm uses agentic AI to some degree, either through existing deployments (16%) or pilot projects (52%). And it is already proving effective in a range of different functions. More than half of executives say agentic AI systems are highly capable of improving fraud detection (56%) and security (51%). Other strong use cases include reducing cost and increasing efficiency (41%) and improving the customer experience (41%).&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Download the report.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. It was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/09/04/1123023/imagining-the-future-of-banking-with-agentic-ai/</guid><pubDate>Thu, 04 Sep 2025 16:21:23 +0000</pubDate></item><item><title>[NEW] Using AI to perceive the universe in greater depth (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/using-ai-to-perceive-the-universe-in-greater-depth/</link><description>&lt;div class="article-cover article-cover--centered"&gt;
    &lt;div class="article-cover__header"&gt;
      &lt;p class="article-cover__eyebrow glue-label"&gt;Science&lt;/p&gt;
      

      
    &lt;dl class="article-cover__meta"&gt;
      
        &lt;dt class="glue-visually-hidden"&gt;Published&lt;/dt&gt;
        &lt;dd class="article-cover__date glue-label"&gt;&lt;time datetime="2025-09-04"&gt;4 September 2025&lt;/time&gt;&lt;/dd&gt;
      
      
        &lt;dt class="glue-visually-hidden"&gt;Authors&lt;/dt&gt;
        &lt;dd class="article-cover__authors"&gt;&lt;p&gt;Brendan Tracey, Jonas Buchli&lt;/p&gt;&lt;/dd&gt;
      
    &lt;/dl&gt;
  

      
    &lt;/div&gt;

    
      
    
    
    
      &lt;source height="603" media="(min-width: 1024px)" type="image/webp" width="1072" /&gt;&lt;source height="522" media="(min-width: 600px)" type="image/webp" width="928" /&gt;&lt;source height="297" type="image/webp" width="528" /&gt;
      &lt;img alt="An artist's illustration of how gravitational wave observatories are used to peer into the universe. In the background, two orbiting black holes distort the fabric of spacetime, sending out gravitational waves. In the foreground, a conceptual detector, much like LIGO, uses a laser beam between two mirrors to measure these infinitesimal disturbances, unveiling the secrets of cosmic collisions." class="picture__image" height="603" src="https://lh3.googleusercontent.com/eXXHIbqGsexZTTs915Rx6INAnpTuWyLcPcvqqK0XQBpZ6XDycXiIAfWM5Bf5N6KAg_sG8b67sSof5fjEbtKn3XX0kdK4YUiqqOIRKJyXdT2D_nq8PPA=w1072-h603-n-nu" width="1072" /&gt;
    
    
  
    
  &lt;/div&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p class="gdm-rich-text__subtitle"&gt;Our novel Deep Loop Shaping method improves control of gravitational wave observatories, helping astronomers better understand the dynamics and formation of the universe.&lt;/p&gt;&lt;p&gt;To help astronomers study the universe’s most powerful processes, our teams have been using AI to stabilize one of the most sensitive observation instruments ever built.&lt;/p&gt;&lt;p&gt;In a paper published today in Science, we introduce Deep Loop Shaping, a novel AI method that will unlock next-generation gravitational-wave science. Deep Loop Shaping reduces noise and improves control in an observatory’s feedback system, helping stabilize components used for measuring gravitational waves — the tiny ripples in the fabric of space and time.&lt;/p&gt;&lt;p&gt;These waves are generated by events like neutron star collisions and black hole mergers. Our method will help astronomers gather data critical to understanding the dynamics and formation of the universe, and better test fundamental theories of physics and cosmology.&lt;/p&gt;&lt;p&gt;We developed Deep Loop Shaping in collaboration with LIGO (Laser Interferometer Gravitational-Wave Observatory) operated by Caltech, and GSSI (Gran Sasso Science Institute), and proved our method at the observatory in Livingston, Louisiana.&lt;/p&gt;&lt;p&gt;LIGO measures the properties and origins of gravitational waves with incredible accuracy. But the slightest vibration can disrupt its measurements, even from waves crashing 100 miles away on the Gulf coast. To function, LIGO relies on thousands of control systems keeping every part in near-perfect alignment, and adapts to environmental disturbances with continuous feedback.&lt;/p&gt;&lt;p&gt;Deep Loop Shaping reduces the noise level in the most unstable and difficult feedback loop at LIGO by 30 to 100 times, improving the stability of its highly-sensitive interferometer mirrors. Applying our method to all of LIGO’s mirror control loops could help astronomers detect and gather data about hundreds of more events per year, in far greater detail.&lt;/p&gt;&lt;p&gt;In the future, Deep Loop Shaping could also be applied to many other engineering problems involving vibration suppression, noise cancellation and highly dynamic or unstable systems important in aerospace, robotics, and structural engineering.&lt;/p&gt;&lt;h2&gt;Measuring across the universe&lt;/h2&gt;&lt;p&gt;LIGO uses the interference of laser light to measure the properties of gravitational waves. By studying these properties, scientists can figure out what caused them and where they came from. The observatory’s lasers reflect off mirrors positioned 4 kilometers apart, housed in the world’s largest vacuum chambers.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--large"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-54ad3332-75de-4382-a4ab-520d70420df7"&gt;
    &lt;p&gt;Aerial view of LIGO (Laser Interferometer Gravitational-Wave Observatory) in Livingston, Louisiana, USA. The observatory’s lasers reflect off mirrors positioned 4 kilometers apart. Photo credit of Caltech/MIT/LIGO Lab.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;Since first detecting gravitational waves produced by a pair of colliding black holes, in 2015, verifying the predictions of Albert Einstein’s general theory of relativity, LIGO’s measurements have deeply changed our understanding of the universe.&lt;/p&gt;&lt;p&gt;With this observatory, astronomers have detected hundreds of black hole and neutron star collisions, proven the existence of binary black hole systems, seen new black holes formed in neutron star collisions, studied the creation of heavy elements like gold and more.&lt;/p&gt;&lt;p&gt;Astronomers already know a lot about the largest and smallest black holes, but we only have limited data on intermediate-mass black holes — considered the “missing link” to understanding galaxy evolution.&lt;/p&gt;&lt;p&gt;Until now, LIGO has only been capable of observing very few of these systems. To help astronomers capture more detail and data of this phenomena, we worked to improve the most difficult part of the control system and expand how far away we can see these events.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  &lt;figure class="quote quote--inline"&gt;
  &lt;blockquote class="quote__text"&gt;
    &lt;p&gt;“&lt;/p&gt;
    &lt;p&gt;Studying the universe using gravity instead of light, is like listening instead of looking. This work allows us to tune in to the bass.&lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;figcaption class="quote__author"&gt;&lt;p&gt;Rana Adhikari, Professor of Physics at the Caltech, 2025&lt;/p&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;h2&gt;Reducing noise and stabilizing the system&lt;/h2&gt;&lt;p&gt;As gravitational waves pass through LIGO’s two 4 kilometer arms, they warp the space between them, changing the distance between the mirrors at either end. These tiny differences in length are measured using light interference to an accuracy of 10^-19 meters, which is 1/10’000 the size of a proton. With measurements this small, LIGO’s detector mirrors must be kept extremely still, isolated from environmental disturbance.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--inline"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-510b7ec7-f5d1-4d94-99a1-2e5ae657ea7d"&gt;
    &lt;p&gt;Closeup photograph of LIGO, which uses strong lasers and mirrors to detect gravitational waves in the universe, generated by events like collisions and mergers of black holes. Photo credit of Caltech/MIT/LIGO Lab.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;This requires one system for passive mechanical isolation and another control system for actively suppressing vibrations. Too little control causes the mirrors to swing, making it impossible to measure anything. But too much control actually amplifies vibrations in the system, instead of suppressing them, drowning out the signal in certain frequency ranges.&lt;/p&gt;&lt;p&gt;These vibrations, known as “control noise”, are a critical blocker to improving LIGO’s ability to peer into the universe. Our team designed Deep Loop Shaping to move beyond traditional methods, such as the linear control design methods currently in operation, to remove the controller as a meaningful cause of noise.&lt;/p&gt;&lt;h2&gt;A more effective control system&lt;/h2&gt;&lt;p&gt;Deep Loop Shaping leverages a reinforcement learning method using frequency domain rewards and surpasses state-of-the-art feedback control performance.&lt;/p&gt;&lt;p&gt;In a simulated LIGO environment, we trained a controller that tries to avoid amplifying noise in the observation band used for measuring gravitational waves — the band where we need the mirror to be still to see events like black hole mergers of up to a few hundred solar masses.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--large"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-ce80eee9-798a-4bbc-ae9b-50bac73aaad1"&gt;
    &lt;p&gt;Diagram showing LIGO’s intricate systems of lasers and mirrors. A distributed control system actively adjusts the mirrors, counteracting the laser radiation pressure and vibrations from external sources.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;Through repeated interaction, guided by frequency domain rewards, the controller learns to suppress the control noise in the observation band. In other words, our controllers learn to stabilize the mirrors without adding harmful control noise, bringing noise levels down by a factor of ten or more, below the amount of vibrations caused by quantum fluctuations in the radiation pressure of light reflecting off the mirrors.&lt;/p&gt;&lt;h2&gt;Strong performance across simulation and hardware&lt;/h2&gt;&lt;p&gt;We tested our controllers on the real LIGO system in Livingston, Louisiana, USA — finding that they worked as well on hardware as in simulation.&lt;/p&gt;&lt;p&gt;Our results show that Deep Loop Shaping controls noise up to 30-100 times better than existing controllers, and it eliminated the most unstable and difficult feedback loop as a meaningful source of noise on LIGO for the first time.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--large"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-eec85cdb-e5c7-488c-b227-d6bb2ddeda70"&gt;
    &lt;p&gt;Line chart showing the resulting control noise spectrum using our Deep Loop Shaping method. There is an improvement of 30-100 times in the injected control noise levels in the most unstable and difficult feedback control loop.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;In repeated experiments, we confirmed that our controller keeps the observatory’s system stable over prolonged periods.&lt;/p&gt;&lt;h2&gt;Better understanding the nature of the universe&lt;/h2&gt;&lt;p&gt;Deep Loop Shaping pushes the boundaries of what’s currently possible in astrophysics by solving a critical blocker to studying gravitational waves.&lt;/p&gt;&lt;p&gt;Applying Deep Loop Shaping to LIGO’s entire mirror control system has the potential to eliminate noise from the control system itself, paving the way for expanding its cosmological reach.&lt;/p&gt;&lt;p&gt;Beyond significantly improving how existing gravitational wave observatories measure further and dimmer sources, we expect our work to influence the design of future observatories, both on Earth and in space — and ultimately help connect missing links throughout the universe for the first time.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  

&lt;section class="button-group button-group--stacked"&gt;
  
    &lt;h2 class="glue-headline glue-headline--headline-6 button-group__title"&gt;Learn more about our work&lt;/h2&gt;
  

  
&lt;/section&gt;
                
              
                
                
                  
                  &lt;section class="notes"&gt;
  &lt;div class="glue-page"&gt;
    &lt;div class="gdm-rich-text notes__inner"&gt;
      &lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;This research was done by Jonas Buchli, Brendan Tracey, Tomislav Andric, Christopher Wipf, Yu Him Justin Chiu, Matthias Lochbrunner, Craig Donner, Rana X Adhikari, Jan Harms, Iain Barr, Roland Hafner, Andrea Huber, Abbas Abdolmaleki, Charlie Beattie, Joseph Betzwieser, Serkan Cabi, Jonas Degrave, Yuzhu Dong, Leslie Fritz, Anchal Gupta, Oliver Groth, Sandy Huang, Tamara Norman, Hannah Openshaw, Jameson Rollins, Greg Thornton, George van den Driessche, Markus Wulfmeier, Pushmeet Kohli, Martin Riedmiller and is a collaboration of LIGO, Caltech, GSSI and GDM.&lt;/p&gt;&lt;p&gt;We’d like to thank the fantastic LIGO instrument team for their tireless work on keeping the observatories up and running and supporting our experiments.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/section&gt;</description><content:encoded>&lt;div class="article-cover article-cover--centered"&gt;
    &lt;div class="article-cover__header"&gt;
      &lt;p class="article-cover__eyebrow glue-label"&gt;Science&lt;/p&gt;
      

      
    &lt;dl class="article-cover__meta"&gt;
      
        &lt;dt class="glue-visually-hidden"&gt;Published&lt;/dt&gt;
        &lt;dd class="article-cover__date glue-label"&gt;&lt;time datetime="2025-09-04"&gt;4 September 2025&lt;/time&gt;&lt;/dd&gt;
      
      
        &lt;dt class="glue-visually-hidden"&gt;Authors&lt;/dt&gt;
        &lt;dd class="article-cover__authors"&gt;&lt;p&gt;Brendan Tracey, Jonas Buchli&lt;/p&gt;&lt;/dd&gt;
      
    &lt;/dl&gt;
  

      
    &lt;/div&gt;

    
      
    
    
    
      &lt;source height="603" media="(min-width: 1024px)" type="image/webp" width="1072" /&gt;&lt;source height="522" media="(min-width: 600px)" type="image/webp" width="928" /&gt;&lt;source height="297" type="image/webp" width="528" /&gt;
      &lt;img alt="An artist's illustration of how gravitational wave observatories are used to peer into the universe. In the background, two orbiting black holes distort the fabric of spacetime, sending out gravitational waves. In the foreground, a conceptual detector, much like LIGO, uses a laser beam between two mirrors to measure these infinitesimal disturbances, unveiling the secrets of cosmic collisions." class="picture__image" height="603" src="https://lh3.googleusercontent.com/eXXHIbqGsexZTTs915Rx6INAnpTuWyLcPcvqqK0XQBpZ6XDycXiIAfWM5Bf5N6KAg_sG8b67sSof5fjEbtKn3XX0kdK4YUiqqOIRKJyXdT2D_nq8PPA=w1072-h603-n-nu" width="1072" /&gt;
    
    
  
    
  &lt;/div&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p class="gdm-rich-text__subtitle"&gt;Our novel Deep Loop Shaping method improves control of gravitational wave observatories, helping astronomers better understand the dynamics and formation of the universe.&lt;/p&gt;&lt;p&gt;To help astronomers study the universe’s most powerful processes, our teams have been using AI to stabilize one of the most sensitive observation instruments ever built.&lt;/p&gt;&lt;p&gt;In a paper published today in Science, we introduce Deep Loop Shaping, a novel AI method that will unlock next-generation gravitational-wave science. Deep Loop Shaping reduces noise and improves control in an observatory’s feedback system, helping stabilize components used for measuring gravitational waves — the tiny ripples in the fabric of space and time.&lt;/p&gt;&lt;p&gt;These waves are generated by events like neutron star collisions and black hole mergers. Our method will help astronomers gather data critical to understanding the dynamics and formation of the universe, and better test fundamental theories of physics and cosmology.&lt;/p&gt;&lt;p&gt;We developed Deep Loop Shaping in collaboration with LIGO (Laser Interferometer Gravitational-Wave Observatory) operated by Caltech, and GSSI (Gran Sasso Science Institute), and proved our method at the observatory in Livingston, Louisiana.&lt;/p&gt;&lt;p&gt;LIGO measures the properties and origins of gravitational waves with incredible accuracy. But the slightest vibration can disrupt its measurements, even from waves crashing 100 miles away on the Gulf coast. To function, LIGO relies on thousands of control systems keeping every part in near-perfect alignment, and adapts to environmental disturbances with continuous feedback.&lt;/p&gt;&lt;p&gt;Deep Loop Shaping reduces the noise level in the most unstable and difficult feedback loop at LIGO by 30 to 100 times, improving the stability of its highly-sensitive interferometer mirrors. Applying our method to all of LIGO’s mirror control loops could help astronomers detect and gather data about hundreds of more events per year, in far greater detail.&lt;/p&gt;&lt;p&gt;In the future, Deep Loop Shaping could also be applied to many other engineering problems involving vibration suppression, noise cancellation and highly dynamic or unstable systems important in aerospace, robotics, and structural engineering.&lt;/p&gt;&lt;h2&gt;Measuring across the universe&lt;/h2&gt;&lt;p&gt;LIGO uses the interference of laser light to measure the properties of gravitational waves. By studying these properties, scientists can figure out what caused them and where they came from. The observatory’s lasers reflect off mirrors positioned 4 kilometers apart, housed in the world’s largest vacuum chambers.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--large"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-54ad3332-75de-4382-a4ab-520d70420df7"&gt;
    &lt;p&gt;Aerial view of LIGO (Laser Interferometer Gravitational-Wave Observatory) in Livingston, Louisiana, USA. The observatory’s lasers reflect off mirrors positioned 4 kilometers apart. Photo credit of Caltech/MIT/LIGO Lab.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;Since first detecting gravitational waves produced by a pair of colliding black holes, in 2015, verifying the predictions of Albert Einstein’s general theory of relativity, LIGO’s measurements have deeply changed our understanding of the universe.&lt;/p&gt;&lt;p&gt;With this observatory, astronomers have detected hundreds of black hole and neutron star collisions, proven the existence of binary black hole systems, seen new black holes formed in neutron star collisions, studied the creation of heavy elements like gold and more.&lt;/p&gt;&lt;p&gt;Astronomers already know a lot about the largest and smallest black holes, but we only have limited data on intermediate-mass black holes — considered the “missing link” to understanding galaxy evolution.&lt;/p&gt;&lt;p&gt;Until now, LIGO has only been capable of observing very few of these systems. To help astronomers capture more detail and data of this phenomena, we worked to improve the most difficult part of the control system and expand how far away we can see these events.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  &lt;figure class="quote quote--inline"&gt;
  &lt;blockquote class="quote__text"&gt;
    &lt;p&gt;“&lt;/p&gt;
    &lt;p&gt;Studying the universe using gravity instead of light, is like listening instead of looking. This work allows us to tune in to the bass.&lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;figcaption class="quote__author"&gt;&lt;p&gt;Rana Adhikari, Professor of Physics at the Caltech, 2025&lt;/p&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;h2&gt;Reducing noise and stabilizing the system&lt;/h2&gt;&lt;p&gt;As gravitational waves pass through LIGO’s two 4 kilometer arms, they warp the space between them, changing the distance between the mirrors at either end. These tiny differences in length are measured using light interference to an accuracy of 10^-19 meters, which is 1/10’000 the size of a proton. With measurements this small, LIGO’s detector mirrors must be kept extremely still, isolated from environmental disturbance.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--inline"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-510b7ec7-f5d1-4d94-99a1-2e5ae657ea7d"&gt;
    &lt;p&gt;Closeup photograph of LIGO, which uses strong lasers and mirrors to detect gravitational waves in the universe, generated by events like collisions and mergers of black holes. Photo credit of Caltech/MIT/LIGO Lab.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;This requires one system for passive mechanical isolation and another control system for actively suppressing vibrations. Too little control causes the mirrors to swing, making it impossible to measure anything. But too much control actually amplifies vibrations in the system, instead of suppressing them, drowning out the signal in certain frequency ranges.&lt;/p&gt;&lt;p&gt;These vibrations, known as “control noise”, are a critical blocker to improving LIGO’s ability to peer into the universe. Our team designed Deep Loop Shaping to move beyond traditional methods, such as the linear control design methods currently in operation, to remove the controller as a meaningful cause of noise.&lt;/p&gt;&lt;h2&gt;A more effective control system&lt;/h2&gt;&lt;p&gt;Deep Loop Shaping leverages a reinforcement learning method using frequency domain rewards and surpasses state-of-the-art feedback control performance.&lt;/p&gt;&lt;p&gt;In a simulated LIGO environment, we trained a controller that tries to avoid amplifying noise in the observation band used for measuring gravitational waves — the band where we need the mirror to be still to see events like black hole mergers of up to a few hundred solar masses.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--large"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-ce80eee9-798a-4bbc-ae9b-50bac73aaad1"&gt;
    &lt;p&gt;Diagram showing LIGO’s intricate systems of lasers and mirrors. A distributed control system actively adjusts the mirrors, counteracting the laser radiation pressure and vibrations from external sources.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;Through repeated interaction, guided by frequency domain rewards, the controller learns to suppress the control noise in the observation band. In other words, our controllers learn to stabilize the mirrors without adding harmful control noise, bringing noise levels down by a factor of ten or more, below the amount of vibrations caused by quantum fluctuations in the radiation pressure of light reflecting off the mirrors.&lt;/p&gt;&lt;h2&gt;Strong performance across simulation and hardware&lt;/h2&gt;&lt;p&gt;We tested our controllers on the real LIGO system in Livingston, Louisiana, USA — finding that they worked as well on hardware as in simulation.&lt;/p&gt;&lt;p&gt;Our results show that Deep Loop Shaping controls noise up to 30-100 times better than existing controllers, and it eliminated the most unstable and difficult feedback loop as a meaningful source of noise on LIGO for the first time.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--large"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-eec85cdb-e5c7-488c-b227-d6bb2ddeda70"&gt;
    &lt;p&gt;Line chart showing the resulting control noise spectrum using our Deep Loop Shaping method. There is an improvement of 30-100 times in the injected control noise levels in the most unstable and difficult feedback control loop.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;In repeated experiments, we confirmed that our controller keeps the observatory’s system stable over prolonged periods.&lt;/p&gt;&lt;h2&gt;Better understanding the nature of the universe&lt;/h2&gt;&lt;p&gt;Deep Loop Shaping pushes the boundaries of what’s currently possible in astrophysics by solving a critical blocker to studying gravitational waves.&lt;/p&gt;&lt;p&gt;Applying Deep Loop Shaping to LIGO’s entire mirror control system has the potential to eliminate noise from the control system itself, paving the way for expanding its cosmological reach.&lt;/p&gt;&lt;p&gt;Beyond significantly improving how existing gravitational wave observatories measure further and dimmer sources, we expect our work to influence the design of future observatories, both on Earth and in space — and ultimately help connect missing links throughout the universe for the first time.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  

&lt;section class="button-group button-group--stacked"&gt;
  
    &lt;h2 class="glue-headline glue-headline--headline-6 button-group__title"&gt;Learn more about our work&lt;/h2&gt;
  

  
&lt;/section&gt;
                
              
                
                
                  
                  &lt;section class="notes"&gt;
  &lt;div class="glue-page"&gt;
    &lt;div class="gdm-rich-text notes__inner"&gt;
      &lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;This research was done by Jonas Buchli, Brendan Tracey, Tomislav Andric, Christopher Wipf, Yu Him Justin Chiu, Matthias Lochbrunner, Craig Donner, Rana X Adhikari, Jan Harms, Iain Barr, Roland Hafner, Andrea Huber, Abbas Abdolmaleki, Charlie Beattie, Joseph Betzwieser, Serkan Cabi, Jonas Degrave, Yuzhu Dong, Leslie Fritz, Anchal Gupta, Oliver Groth, Sandy Huang, Tamara Norman, Hannah Openshaw, Jameson Rollins, Greg Thornton, George van den Driessche, Markus Wulfmeier, Pushmeet Kohli, Martin Riedmiller and is a collaboration of LIGO, Caltech, GSSI and GDM.&lt;/p&gt;&lt;p&gt;We’d like to thank the fantastic LIGO instrument team for their tireless work on keeping the observatories up and running and supporting our experiments.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/section&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/using-ai-to-perceive-the-universe-in-greater-depth/</guid><pubDate>Thu, 04 Sep 2025 18:00:00 +0000</pubDate></item></channel></rss>