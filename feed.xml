<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 18 Dec 2025 01:48:55 +0000</lastBuildDate><item><title>Skana Robotics helps fleets of underwater robots communicate with each other (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/17/skana-robotics-helps-fleets-of-underwater-robots-communicate-with-each-other/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Skana.jpg?resize=1200,1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Underwater autonomous vessels and robots could play a substantial role in defense operations, but submersibles have historically had trouble communicating across large distances unless they rose to the surface. But coming up to transmit poses the very obvious risk of being exposed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Skana Robotics thinks it’s made a breakthrough with underwater communications using AI — but not the large language models the industry touts today.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Tel Aviv-based Skana has developed a new capability for its fleet management software system, SeaSphere, that allows groups of vessels to communicate with each other underwater across long distances using AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The system allows vessels to share data and react to what they hear from other robots. This, Skana says, gives individual units the ability to autonomously adapt to the information they receive and change their course or task while still working toward the same general mission as the fleet. The startup says its software can also be used to secure underwater infrastructure and supply chains.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Communication between vessels is one of the main challenges during the deployment of multi-domain, multi-vessel operations,” Idan Levy, the co-founder and CEO of Skana Robotics, told TechCrunch. “The problem that we tackle is how you can deploy hundreds of unmanned vessels in an operation, share data, communicate on the surface level and under the water.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Teddy Lazebnik, an AI scientist and professor at the University of Haifa in Israel, led the research to develop this new capability. Lazebnik told TechCrunch that to build this decision-making algorithm, they couldn’t turn to the latest AI technology, but had to use AI algorithms that are a bit older and more mathematically driven.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The new algorithms have two properties: they are more powerful, but as a result, are less predictable,” Lazebnik said. “Hypothetically, you’re paying in the performance or the ‘wow effect’ of the of this algorithm, but the older ones, you gain explainability, predictability, and actually generality.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Skana Robotics was founded in 2024 and exited stealth mode earlier this year. The company is currently focused on selling to governments and companies in Europe, as maritime threat levels increase due to the war between Russia and Ukraine.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Levy said the company is in talks for a sizable government contract that it hopes to close by the end of the year. In 2026, Skana hopes to release the commercial version of its product and start proving its tech out in the wild.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We want to show we can use this in scale,” Lazebnik said. “We argue that our software can handle complex maneuvers, etc. We want to show it. We claim we know how to manage an operation. We want admirals from EU and in EU countries to actually check this argument and see by themselves that we actually get results.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Skana.jpg?resize=1200,1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Underwater autonomous vessels and robots could play a substantial role in defense operations, but submersibles have historically had trouble communicating across large distances unless they rose to the surface. But coming up to transmit poses the very obvious risk of being exposed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Skana Robotics thinks it’s made a breakthrough with underwater communications using AI — but not the large language models the industry touts today.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Tel Aviv-based Skana has developed a new capability for its fleet management software system, SeaSphere, that allows groups of vessels to communicate with each other underwater across long distances using AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The system allows vessels to share data and react to what they hear from other robots. This, Skana says, gives individual units the ability to autonomously adapt to the information they receive and change their course or task while still working toward the same general mission as the fleet. The startup says its software can also be used to secure underwater infrastructure and supply chains.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Communication between vessels is one of the main challenges during the deployment of multi-domain, multi-vessel operations,” Idan Levy, the co-founder and CEO of Skana Robotics, told TechCrunch. “The problem that we tackle is how you can deploy hundreds of unmanned vessels in an operation, share data, communicate on the surface level and under the water.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Teddy Lazebnik, an AI scientist and professor at the University of Haifa in Israel, led the research to develop this new capability. Lazebnik told TechCrunch that to build this decision-making algorithm, they couldn’t turn to the latest AI technology, but had to use AI algorithms that are a bit older and more mathematically driven.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The new algorithms have two properties: they are more powerful, but as a result, are less predictable,” Lazebnik said. “Hypothetically, you’re paying in the performance or the ‘wow effect’ of the of this algorithm, but the older ones, you gain explainability, predictability, and actually generality.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Skana Robotics was founded in 2024 and exited stealth mode earlier this year. The company is currently focused on selling to governments and companies in Europe, as maritime threat levels increase due to the war between Russia and Ukraine.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Levy said the company is in talks for a sizable government contract that it hopes to close by the end of the year. In 2026, Skana hopes to release the commercial version of its product and start proving its tech out in the wild.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We want to show we can use this in scale,” Lazebnik said. “We argue that our software can handle complex maneuvers, etc. We want to show it. We claim we know how to manage an operation. We want admirals from EU and in EU countries to actually check this argument and see by themselves that we actually get results.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/17/skana-robotics-helps-fleets-of-underwater-robots-communicate-with-each-other/</guid><pubDate>Wed, 17 Dec 2025 14:05:00 +0000</pubDate></item><item><title>Google’s vibe-coding tool Opal comes to Gemini (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/17/googles-vibe-coding-tool-opal-comes-to-gemini/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/Google-Opal.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google’s vibe-coding tool, Opal, is making its way to Gemini. The company on Wednesday said it is integrating the tool, which lets you build AI-powered mini apps, inside the Gemini web app, allowing users to create their own custom apps, which Google calls Gems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Introduced in 2024, Gems are customized versions of Gemini designed for specific tasks or scenarios. For instance, some of Google’s pre-made Gems include a learning coach, a brainstorming assistant, a career guide, a coding partner, and an editor.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Opal, meanwhile, focuses on helping users create mini-apps or mix existing apps. To use the feature, users describe in natural language the app they want to make, and the tool will use the different Gemini models to create it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, Opal is directly available from Gemini on the web, where it’s found in the Gems manager. The tool has a visual editor that lays out the steps required to create an application. From the editor, users can rearrange steps and link them together, without writing code.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google notes that the visual editor also includes a new view in Gemini that will take the user’s written prompts and turn them into a list of steps. This makes it even easier to build apps and see how they work.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For more advanced customization options, users can move from Gemini to the Advanced Editor at opal.google.com. The mini apps can be reused after they’re created.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Known as “vibe-coding,” using AI to program and make apps has skyrocketed in popularity over the past couple of years. The market now has apps from startups like&amp;nbsp;Lovable&amp;nbsp;and&amp;nbsp;Cursor, as well as offerings from AI providers like Anthropic and OpenAI. There are also tools focused more directly on consumers, like those from AI-powered app-building startup Wabi.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Gemini’s web app is available at gemini.google.com.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/Google-Opal.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google’s vibe-coding tool, Opal, is making its way to Gemini. The company on Wednesday said it is integrating the tool, which lets you build AI-powered mini apps, inside the Gemini web app, allowing users to create their own custom apps, which Google calls Gems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Introduced in 2024, Gems are customized versions of Gemini designed for specific tasks or scenarios. For instance, some of Google’s pre-made Gems include a learning coach, a brainstorming assistant, a career guide, a coding partner, and an editor.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Opal, meanwhile, focuses on helping users create mini-apps or mix existing apps. To use the feature, users describe in natural language the app they want to make, and the tool will use the different Gemini models to create it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, Opal is directly available from Gemini on the web, where it’s found in the Gems manager. The tool has a visual editor that lays out the steps required to create an application. From the editor, users can rearrange steps and link them together, without writing code.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google notes that the visual editor also includes a new view in Gemini that will take the user’s written prompts and turn them into a list of steps. This makes it even easier to build apps and see how they work.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For more advanced customization options, users can move from Gemini to the Advanced Editor at opal.google.com. The mini apps can be reused after they’re created.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Known as “vibe-coding,” using AI to program and make apps has skyrocketed in popularity over the past couple of years. The market now has apps from startups like&amp;nbsp;Lovable&amp;nbsp;and&amp;nbsp;Cursor, as well as offerings from AI providers like Anthropic and OpenAI. There are also tools focused more directly on consumers, like those from AI-powered app-building startup Wabi.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Gemini’s web app is available at gemini.google.com.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/17/googles-vibe-coding-tool-opal-comes-to-gemini/</guid><pubDate>Wed, 17 Dec 2025 15:16:42 +0000</pubDate></item><item><title>Mozilla’s new CEO says AI is coming to Firefox, but will remain a choice (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/17/mozillas-new-ceo-says-ai-is-coming-to-firefox-but-will-remain-a-choice/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2020/08/GettyImages-956152050.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Mozilla has appointed Anthony Enzor-DeMeo as its CEO as the Firefox browser maker scrambles to adapt in a rapidly changing browser market.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The appointment comes at a time when web browsers are seeing a revitalization of sorts as AI changes how people use the internet. After more than a decade of dominating the market, incumbents like Firefox, Google Chrome, and Apple’s Safari are facing a fresh challenge from companies like Perplexity, Arc, OpenAI, and Opera, which are focused on baking AI models and agents into their browsers to bring AI to users at the first point of contact with the internet: the web browser.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;These changes don’t seem to be lost on Mozilla, which consists of several organizations, one of which is the Mozilla Corporation, which develops Firefox and other technologies, and another of which is its nonprofit and tax-exempt Mozilla Foundation, which oversees Mozilla’s corporate governance structure and sets the browser maker’s policies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has had a tough time lately: It’s gone through a restructuring, and last year laid off 30% of its employees and dropped its advocacy and global programs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the potential to make a comeback amid the modern browser wars doesn’t seem to be lost on the company. Mozilla will be investing in AI and will add AI features to Firefox, Enzor-DeMeo said in a blog post announcing his appointment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That said, Mozilla seems intent on not infuriating users who’ve chosen Firefox for its lack of AI features: Enzor-DeMeo said the company will make AI features optional within Firefox and its other products. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AI should always be a choice — something people can easily turn off. People should know why a feature works the way it does and what value they get from it,” he wrote.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The company will also be investing in diversifying its revenue beyond search (in exchange for having Google as its default search engine, Mozilla makes a significant portion of its revenue from the search giant), and Enzor-DeMeo said Mozilla plans to flesh Firefox out into “a broader ecosystem of trusted software.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Currently, the company also develops the Thunderbird email client, a VPN, and last year launched an AI-powered website creator aimed at small businesses.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Before this appointment, Enzor-DeMeo was general manager of Firefox, and is now taking over from interim CEO, Laura Chambers, who was in the role for the past couple of years. Enzor-DeMeo previously held product roles at Roofstock, Better, and Wayfair. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2020/08/GettyImages-956152050.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Mozilla has appointed Anthony Enzor-DeMeo as its CEO as the Firefox browser maker scrambles to adapt in a rapidly changing browser market.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The appointment comes at a time when web browsers are seeing a revitalization of sorts as AI changes how people use the internet. After more than a decade of dominating the market, incumbents like Firefox, Google Chrome, and Apple’s Safari are facing a fresh challenge from companies like Perplexity, Arc, OpenAI, and Opera, which are focused on baking AI models and agents into their browsers to bring AI to users at the first point of contact with the internet: the web browser.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;These changes don’t seem to be lost on Mozilla, which consists of several organizations, one of which is the Mozilla Corporation, which develops Firefox and other technologies, and another of which is its nonprofit and tax-exempt Mozilla Foundation, which oversees Mozilla’s corporate governance structure and sets the browser maker’s policies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has had a tough time lately: It’s gone through a restructuring, and last year laid off 30% of its employees and dropped its advocacy and global programs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the potential to make a comeback amid the modern browser wars doesn’t seem to be lost on the company. Mozilla will be investing in AI and will add AI features to Firefox, Enzor-DeMeo said in a blog post announcing his appointment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That said, Mozilla seems intent on not infuriating users who’ve chosen Firefox for its lack of AI features: Enzor-DeMeo said the company will make AI features optional within Firefox and its other products. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AI should always be a choice — something people can easily turn off. People should know why a feature works the way it does and what value they get from it,” he wrote.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The company will also be investing in diversifying its revenue beyond search (in exchange for having Google as its default search engine, Mozilla makes a significant portion of its revenue from the search giant), and Enzor-DeMeo said Mozilla plans to flesh Firefox out into “a broader ecosystem of trusted software.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Currently, the company also develops the Thunderbird email client, a VPN, and last year launched an AI-powered website creator aimed at small businesses.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Before this appointment, Enzor-DeMeo was general manager of Firefox, and is now taking over from interim CEO, Laura Chambers, who was in the role for the past couple of years. Enzor-DeMeo previously held product roles at Roofstock, Better, and Wayfair. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/17/mozillas-new-ceo-says-ai-is-coming-to-firefox-but-will-remain-a-choice/</guid><pubDate>Wed, 17 Dec 2025 15:17:52 +0000</pubDate></item><item><title>Browser extensions with 8 million users collect extended AI conversations (AI – Ars Technica)</title><link>https://arstechnica.com/security/2025/12/browser-extensions-with-8-million-users-collect-extended-ai-conversations/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The extensions, available for Chromium browsers, harvest full AI conversations over months.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="448" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/electronic-privacy-invasion-640x448.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/electronic-privacy-invasion-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Browser extensions with more than 8 million installs are harvesting complete and extended conversations from users’ AI conversations and selling them for marketing purposes, according to data collected from the Google and Microsoft pages hosting them.&lt;/p&gt;
&lt;p&gt;Security firm Koi discovered the eight extensions, which as of late Tuesday night remained available in both Google’s and Microsoft’s extension stores. Seven of them carry “Featured” badges, which are endorsements meant to signal that the companies have determined the extensions meet their quality standards. The free extensions provide functions such as VPN routing to safeguard online privacy and ad blocking for ad-free browsing. All provide assurances that user data remains anonymous and isn’t shared for purposes other than their described use.&lt;/p&gt;
&lt;h2&gt;A gold mine for marketers and data brokers&lt;/h2&gt;
&lt;p&gt;An examination of the extensions’ underlying code tells a much more complicated story. Each contains eight of what Koi calls “executor” scripts, with each being unique for ChatGPT, Claude, Gemini, and five other leading AI chat platforms. The scripts are injected into webpages anytime the user visits one of these platforms. From there, the scripts override browsers’ built-in functions for making network requests and receiving responses.&lt;/p&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="ars-gallery-1-up my-5"&gt;
  &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="426" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/executor-script.png" width="1064" /&gt;
  
  &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="1262" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/extension-ai-platforms.png" width="844" /&gt;
  
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/extension-flags.png" width="578" /&gt;
  
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
              &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;As a result, all interaction between the browser and the AI bots is routed not by the legitimate browser APIs—in this case fetch() and HttpRequest—but through the executor script. The extensions eventually compress the data and send it to endpoints belonging to the extension maker.&lt;/p&gt;
&lt;p&gt;“By overriding the [browser APIs], the extension inserts itself into that flow and captures a copy of everything before the page even displays it,” Koi CTO Idan Dardikman wrote in an email. “The consequence: The extension sees your complete conversation in raw form—your prompts, the AI’s responses, timestamps, everything—and sends a copy to their servers.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Besides ChatGPT, Claude, and Gemini, the extensions harvest all conversations from Copilot, Perplexity, DeepSeek, Grok, and Meta AI. Koi said the full description of the data captured includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Every prompt a user sends to the AI&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Every response received&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Conversation identifiers and timestamps&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Session metadata&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;The specific AI platform and model used&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The executor script runs independently from the VPN networking, ad blocking, or other core functionality. That means that even when a user toggles off VPN networking, AI protection, ad blocking, or other functions, the conversation collection continues. The only way to stop the harvesting is to disable the extension in the browser settings or to uninstall it.&lt;/p&gt;
&lt;p&gt;Koi said it first discovered the conversation harvesting in Urban VPN Proxy, a VPN routing extension that lists “AI protection” as one of its benefits. The data collection began in early July with the release of version 5.5.0.&lt;/p&gt;
&lt;p&gt;“Anyone who used ChatGPT, Claude, Gemini, or the other targeted platforms while Urban VPN was installed after July 9, 2025 should assume those conversations are now on Urban VPN’s servers and have been shared with third parties,” the company said. “Medical questions, financial details, proprietary code, personal dilemmas—all of it, sold for ‘marketing analytics purposes.'”&lt;/p&gt;
&lt;p&gt;Following that discovery, the security firm uncovered seven additional extensions with identical AI harvesting functionality. Four of the extensions are available in the Chrome Web Store. The other four are on the Edge add-ons page. Collectively, they have been installed more than 8 million times.&lt;/p&gt;
&lt;p&gt;They are:&lt;/p&gt;
&lt;p&gt;Chrome Store&lt;/p&gt;
&lt;ul&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Urban VPN Proxy: 6 million users&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;1ClickVPN Proxy: 600,000 users&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Urban Browser Guard: 40,000 users&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Urban Ad Blocker: 10,000 users&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Edge Add-ons:&lt;/p&gt;
&lt;ul&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Urban VPN Proxy: 1,32 million users&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;1ClickVPN Proxy: 36,459 users&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Urban Browser Guard – 12,624 users&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Urban Ad Blocker – 6,476 users&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Read the fine print&lt;/h2&gt;
&lt;p&gt;The extensions come with conflicting messages about how they handle bot conversations, which often contain deeply personal information about users’ physical and mental health, finances, personal relationships, and other sensitive information that could be a gold mine for marketers and data brokers. The Urban VPN Proxy in the Chrome Web Store, for instance, lists “AI protection” as a benefit. It goes on to say:&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;blockquote&gt;&lt;p&gt;Our VPN provides added security features to help shield your browsing experience from phishing attempts, malware, intrusive ads and AI protection which checks prompts for personal data (like an email or phone number), checks AI chat responses for suspicious or unsafe links and displays a warning before click or submit your prompt.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;On the privacy policy for the extension, Google says the developer has declared that user data isn’t sold to third parties outside of approved use cases and won’t be “used or transferred for purposes that are unrelated to the item’s core functionality.” The page goes on to list the personal data handled as location, web history, and website content.&lt;/p&gt;
&lt;p&gt;Koi said that a consent prompt that the extensions display during setup notifies the user that they process “ChatAI communication,” “pages you visit,” and “security signals.” The notification goes on to say that the data is processed to “provide these protections,” which presumably means the core functions such as VPN routing or ad blocking.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132319 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="1262" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/consent-prompt.png" width="642" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Koi

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The only explicit mention of AI conversations being harvested is in legalese buried in the privacy policy, such as this 6,000-word one for Urban VPN Proxy, posted on each extension website. There, it says that the extension will “collect the prompts and outputs queried by the End-User or generated by the AI chat provider, as applicable.” It goes on to say that the extension developer will “disclose the AI prompts for marketing analytics purposes.”&lt;/p&gt;
&lt;p&gt;All eight extensions and the privacy policies covering them are developed and written by Urban Cyber Security, a company that says its apps and extensions are used by 100 million people. The policies say the extensions share “Web Browsing Data” with “our affiliated company,” which is listed as both BiScience and B.I Science. The affiliated company “uses this raw data and creates insights which are commercially used and shared with Business Partners.” The policy goes on to refer users to the BiScience privacy policy. BiScience, whose privacy practices have been scrutinized before, says its services “transform enormous volumes of digital signals into clear, actionable market intelligence.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;It’s hard to fathom how both Google and Microsoft would allow such extensions onto their platforms at all, let alone go out of their way to endorse seven of them with a featured badge. Google didn’t return an email asking how it decides which extensions qualify for such a distinction, if they have plans to stop making them available to Chrome and Edge users, or why the privacy policies are so unclear to normal users. More than three hours after this post went live, a Microsoft representative said the company didn’t have anything to “share.”&lt;/p&gt;
&lt;p&gt;Messages sent to both individual extension developers and Urban Cyber Security went unanswered. BiScience provides no email. A call to the company’s New York office was answered by someone who said they were in Israel and to call back during normal business hours in that country.&lt;/p&gt;
&lt;p&gt;Koi’s discovery is the latest cautionary tale illustrating the growing perils of being online. It’s questionable in the first place whether people should trust their most intimate secrets and sensitive business information to AI chatbots, which come with no HIPAA assurances, attorney-client privilege, or expectations of privacy. Yet increasingly, that’s exactly what AI companies are encouraging, and users, it seems, are more than willing to comply.&lt;/p&gt;
&lt;p&gt;Compounding the risk is the rush to install free apps and extensions—particularly those from little-known developers and providing at best minimal benefits—on devices storing and transmitting these chats. Taken together, they’re a recipe for disaster, and that’s exactly what we have here.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The extensions, available for Chromium browsers, harvest full AI conversations over months.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="448" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/electronic-privacy-invasion-640x448.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/electronic-privacy-invasion-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Browser extensions with more than 8 million installs are harvesting complete and extended conversations from users’ AI conversations and selling them for marketing purposes, according to data collected from the Google and Microsoft pages hosting them.&lt;/p&gt;
&lt;p&gt;Security firm Koi discovered the eight extensions, which as of late Tuesday night remained available in both Google’s and Microsoft’s extension stores. Seven of them carry “Featured” badges, which are endorsements meant to signal that the companies have determined the extensions meet their quality standards. The free extensions provide functions such as VPN routing to safeguard online privacy and ad blocking for ad-free browsing. All provide assurances that user data remains anonymous and isn’t shared for purposes other than their described use.&lt;/p&gt;
&lt;h2&gt;A gold mine for marketers and data brokers&lt;/h2&gt;
&lt;p&gt;An examination of the extensions’ underlying code tells a much more complicated story. Each contains eight of what Koi calls “executor” scripts, with each being unique for ChatGPT, Claude, Gemini, and five other leading AI chat platforms. The scripts are injected into webpages anytime the user visits one of these platforms. From there, the scripts override browsers’ built-in functions for making network requests and receiving responses.&lt;/p&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="ars-gallery-1-up my-5"&gt;
  &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="426" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/executor-script.png" width="1064" /&gt;
  
  &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="1262" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/extension-ai-platforms.png" width="844" /&gt;
  
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/extension-flags.png" width="578" /&gt;
  
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
              &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;As a result, all interaction between the browser and the AI bots is routed not by the legitimate browser APIs—in this case fetch() and HttpRequest—but through the executor script. The extensions eventually compress the data and send it to endpoints belonging to the extension maker.&lt;/p&gt;
&lt;p&gt;“By overriding the [browser APIs], the extension inserts itself into that flow and captures a copy of everything before the page even displays it,” Koi CTO Idan Dardikman wrote in an email. “The consequence: The extension sees your complete conversation in raw form—your prompts, the AI’s responses, timestamps, everything—and sends a copy to their servers.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Besides ChatGPT, Claude, and Gemini, the extensions harvest all conversations from Copilot, Perplexity, DeepSeek, Grok, and Meta AI. Koi said the full description of the data captured includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Every prompt a user sends to the AI&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Every response received&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Conversation identifiers and timestamps&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Session metadata&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;The specific AI platform and model used&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The executor script runs independently from the VPN networking, ad blocking, or other core functionality. That means that even when a user toggles off VPN networking, AI protection, ad blocking, or other functions, the conversation collection continues. The only way to stop the harvesting is to disable the extension in the browser settings or to uninstall it.&lt;/p&gt;
&lt;p&gt;Koi said it first discovered the conversation harvesting in Urban VPN Proxy, a VPN routing extension that lists “AI protection” as one of its benefits. The data collection began in early July with the release of version 5.5.0.&lt;/p&gt;
&lt;p&gt;“Anyone who used ChatGPT, Claude, Gemini, or the other targeted platforms while Urban VPN was installed after July 9, 2025 should assume those conversations are now on Urban VPN’s servers and have been shared with third parties,” the company said. “Medical questions, financial details, proprietary code, personal dilemmas—all of it, sold for ‘marketing analytics purposes.'”&lt;/p&gt;
&lt;p&gt;Following that discovery, the security firm uncovered seven additional extensions with identical AI harvesting functionality. Four of the extensions are available in the Chrome Web Store. The other four are on the Edge add-ons page. Collectively, they have been installed more than 8 million times.&lt;/p&gt;
&lt;p&gt;They are:&lt;/p&gt;
&lt;p&gt;Chrome Store&lt;/p&gt;
&lt;ul&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Urban VPN Proxy: 6 million users&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;1ClickVPN Proxy: 600,000 users&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Urban Browser Guard: 40,000 users&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Urban Ad Blocker: 10,000 users&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Edge Add-ons:&lt;/p&gt;
&lt;ul&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Urban VPN Proxy: 1,32 million users&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;1ClickVPN Proxy: 36,459 users&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Urban Browser Guard – 12,624 users&lt;/span&gt;&lt;/li&gt;
&lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;Urban Ad Blocker – 6,476 users&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Read the fine print&lt;/h2&gt;
&lt;p&gt;The extensions come with conflicting messages about how they handle bot conversations, which often contain deeply personal information about users’ physical and mental health, finances, personal relationships, and other sensitive information that could be a gold mine for marketers and data brokers. The Urban VPN Proxy in the Chrome Web Store, for instance, lists “AI protection” as a benefit. It goes on to say:&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;blockquote&gt;&lt;p&gt;Our VPN provides added security features to help shield your browsing experience from phishing attempts, malware, intrusive ads and AI protection which checks prompts for personal data (like an email or phone number), checks AI chat responses for suspicious or unsafe links and displays a warning before click or submit your prompt.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;On the privacy policy for the extension, Google says the developer has declared that user data isn’t sold to third parties outside of approved use cases and won’t be “used or transferred for purposes that are unrelated to the item’s core functionality.” The page goes on to list the personal data handled as location, web history, and website content.&lt;/p&gt;
&lt;p&gt;Koi said that a consent prompt that the extensions display during setup notifies the user that they process “ChatAI communication,” “pages you visit,” and “security signals.” The notification goes on to say that the data is processed to “provide these protections,” which presumably means the core functions such as VPN routing or ad blocking.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132319 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="1262" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/consent-prompt.png" width="642" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Koi

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The only explicit mention of AI conversations being harvested is in legalese buried in the privacy policy, such as this 6,000-word one for Urban VPN Proxy, posted on each extension website. There, it says that the extension will “collect the prompts and outputs queried by the End-User or generated by the AI chat provider, as applicable.” It goes on to say that the extension developer will “disclose the AI prompts for marketing analytics purposes.”&lt;/p&gt;
&lt;p&gt;All eight extensions and the privacy policies covering them are developed and written by Urban Cyber Security, a company that says its apps and extensions are used by 100 million people. The policies say the extensions share “Web Browsing Data” with “our affiliated company,” which is listed as both BiScience and B.I Science. The affiliated company “uses this raw data and creates insights which are commercially used and shared with Business Partners.” The policy goes on to refer users to the BiScience privacy policy. BiScience, whose privacy practices have been scrutinized before, says its services “transform enormous volumes of digital signals into clear, actionable market intelligence.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;It’s hard to fathom how both Google and Microsoft would allow such extensions onto their platforms at all, let alone go out of their way to endorse seven of them with a featured badge. Google didn’t return an email asking how it decides which extensions qualify for such a distinction, if they have plans to stop making them available to Chrome and Edge users, or why the privacy policies are so unclear to normal users. More than three hours after this post went live, a Microsoft representative said the company didn’t have anything to “share.”&lt;/p&gt;
&lt;p&gt;Messages sent to both individual extension developers and Urban Cyber Security went unanswered. BiScience provides no email. A call to the company’s New York office was answered by someone who said they were in Israel and to call back during normal business hours in that country.&lt;/p&gt;
&lt;p&gt;Koi’s discovery is the latest cautionary tale illustrating the growing perils of being online. It’s questionable in the first place whether people should trust their most intimate secrets and sensitive business information to AI chatbots, which come with no HIPAA assurances, attorney-client privilege, or expectations of privacy. Yet increasingly, that’s exactly what AI companies are encouraging, and users, it seems, are more than willing to comply.&lt;/p&gt;
&lt;p&gt;Compounding the risk is the rush to install free apps and extensions—particularly those from little-known developers and providing at best minimal benefits—on devices storing and transmitting these chats. Taken together, they’re a recipe for disaster, and that’s exactly what we have here.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/security/2025/12/browser-extensions-with-8-million-users-collect-extended-ai-conversations/</guid><pubDate>Wed, 17 Dec 2025 15:25:25 +0000</pubDate></item><item><title>Google launches Gemini 3 Flash, makes it the default model in the Gemini app (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/17/google-launches-gemini-3-flash-makes-it-the-default-model-in-the-gemini-app/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google today released its fast and cheap Gemini 3 Flash model, based on the Gemini 3 released last month, looking to steal OpenAI’s thunder. The company is also making this the default model in the Gemini app and AI mode in search.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new Flash model arrives six months after Google announced the Gemini 2.5 Flash model, offering significant improvements. On the benchmark, the Gemini 3 Flash model outperforms its predecessor by a significant margin and matches the performance of other frontier models, like Gemini 3 Pro and GPT 5.2, in some measures.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For instance, it scored 33.7% without tool use on Humanity’s Last Exam benchmark, which is designed to test expertise across different domains. In comparison, Gemini 3 Pro scored 37.5%, Gemini 2.5 Flash scored 11%, and the newly released GPT-5.2 scored 34.5%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On the multimodality and reasoning benchmark MMMU-Pro, the new model outscored all competitors with an 81.2% score. &lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-consumer-rollout"&gt;Consumer rollout&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Google is making Gemini 3 Flash the default model in the Gemini app globally, replacing Gemini 2.5 Flash. Users can still choose the Pro model from the model picker for math and coding questions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company says the new model is good at identifying multimodal content and giving you an answer based on that. For instance, you can upload your pickleball short video and ask for tips; you can try drawing a sketch and have the model guess what you are drawing; or you can upload an audio recording to get analysis or generate a quiz.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company also said the model better understands the intent of users’ queries and can generate more visual answers with elements like images and tables.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;You can also use the new model to create app prototypes in the Gemini app using prompts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Gemini 3 Pro is now available to everyone in the U.S. for search and more people in the U.S. can access the Nano Banana Pro image model in search, as well.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-enterprise-and-developer-availability"&gt;Enterprise and developer availability&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Google noted that companies like JetBrains, Figma, Cursor, Harvey, and Latitude are already using the Gemini 3 Flash model, which is available through Vertex AI and Gemini Enterprise.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For developers, the company is making the model available in a preview model through the API and in Antigravity, Google’s new coding tool released last month. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said the Gemini 3 Pro scores 78% on the SWE-bench verified coding benchmark, only outperformed by GPT-5.2. It added that the model is ideal for video analysis, data extraction, and visual Q&amp;amp;A, and because of its speed, it is suited for quick and repeatable workflows.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3076854" height="680" src="https://techcrunch.com/wp-content/uploads/2025/12/gemini-3-flash-swe-bench.jpeg?w=544" width="544" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Model pricing is $0.50 per 1 million input tokens and $3.00 per 1 million output tokens. This is slightly more expensive than $0.30 per 1 million input tokens and $2.50 per 1 million output tokens of Gemini Flash 2.5. But Google claims that the new model outperforms the Gemini 2.5 Pro model while being three times faster. And, for thinking tasks, it uses 30% fewer tokens on average than 2.5 Pro. That means overall, you might save on the number of tokens for certain tasks.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3076855" height="680" src="https://techcrunch.com/wp-content/uploads/2025/12/gemini-3-flash-token-efficiency.jpeg?w=544" width="544" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“We really position flash as more of your workhorse model. So if you look at, for example, even the input and output prices at the top of this table, Flash is just a much cheaper offering from an input and output price perspective. And so it actually allows for, for many companies, bulk tasks,” Tulsee Doshi, senior director &amp;amp; head of Product for Gemini Models, told TechCrunch in a briefing&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since it released Gemini 3, Google has processed over 1 trillion tokens per day on its API, amid its fierce release and performance war with OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this month, Sam Altman reportedly sent an internal “Code Red” memo to the OpenAI team after ChatGPT’s traffic dipped as Google’s market share in consumers rose. Post that, OpenAI has released GPT-5.2 and a new image generation model. OpenAI also boasted about its growing enterprise use and said the ChatGPT messages volume has grown 8x since November 2024.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Google didn’t directly address the competition with OpenAI, it said that the release of new models is challenging all companies to be active.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Just about what’s happening across the industry is like all of these models are continuing to be awesome, challenge each other, push the frontier. And I think what’s also awesome is as companies are releasing these models,” Doshi said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We’re also introducing new benchmarks and new ways of evaluating these models. And so that’s also encouraging us.”&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google today released its fast and cheap Gemini 3 Flash model, based on the Gemini 3 released last month, looking to steal OpenAI’s thunder. The company is also making this the default model in the Gemini app and AI mode in search.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new Flash model arrives six months after Google announced the Gemini 2.5 Flash model, offering significant improvements. On the benchmark, the Gemini 3 Flash model outperforms its predecessor by a significant margin and matches the performance of other frontier models, like Gemini 3 Pro and GPT 5.2, in some measures.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For instance, it scored 33.7% without tool use on Humanity’s Last Exam benchmark, which is designed to test expertise across different domains. In comparison, Gemini 3 Pro scored 37.5%, Gemini 2.5 Flash scored 11%, and the newly released GPT-5.2 scored 34.5%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On the multimodality and reasoning benchmark MMMU-Pro, the new model outscored all competitors with an 81.2% score. &lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-consumer-rollout"&gt;Consumer rollout&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Google is making Gemini 3 Flash the default model in the Gemini app globally, replacing Gemini 2.5 Flash. Users can still choose the Pro model from the model picker for math and coding questions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company says the new model is good at identifying multimodal content and giving you an answer based on that. For instance, you can upload your pickleball short video and ask for tips; you can try drawing a sketch and have the model guess what you are drawing; or you can upload an audio recording to get analysis or generate a quiz.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company also said the model better understands the intent of users’ queries and can generate more visual answers with elements like images and tables.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;You can also use the new model to create app prototypes in the Gemini app using prompts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Gemini 3 Pro is now available to everyone in the U.S. for search and more people in the U.S. can access the Nano Banana Pro image model in search, as well.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-enterprise-and-developer-availability"&gt;Enterprise and developer availability&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Google noted that companies like JetBrains, Figma, Cursor, Harvey, and Latitude are already using the Gemini 3 Flash model, which is available through Vertex AI and Gemini Enterprise.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For developers, the company is making the model available in a preview model through the API and in Antigravity, Google’s new coding tool released last month. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said the Gemini 3 Pro scores 78% on the SWE-bench verified coding benchmark, only outperformed by GPT-5.2. It added that the model is ideal for video analysis, data extraction, and visual Q&amp;amp;A, and because of its speed, it is suited for quick and repeatable workflows.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3076854" height="680" src="https://techcrunch.com/wp-content/uploads/2025/12/gemini-3-flash-swe-bench.jpeg?w=544" width="544" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Model pricing is $0.50 per 1 million input tokens and $3.00 per 1 million output tokens. This is slightly more expensive than $0.30 per 1 million input tokens and $2.50 per 1 million output tokens of Gemini Flash 2.5. But Google claims that the new model outperforms the Gemini 2.5 Pro model while being three times faster. And, for thinking tasks, it uses 30% fewer tokens on average than 2.5 Pro. That means overall, you might save on the number of tokens for certain tasks.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3076855" height="680" src="https://techcrunch.com/wp-content/uploads/2025/12/gemini-3-flash-token-efficiency.jpeg?w=544" width="544" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“We really position flash as more of your workhorse model. So if you look at, for example, even the input and output prices at the top of this table, Flash is just a much cheaper offering from an input and output price perspective. And so it actually allows for, for many companies, bulk tasks,” Tulsee Doshi, senior director &amp;amp; head of Product for Gemini Models, told TechCrunch in a briefing&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since it released Gemini 3, Google has processed over 1 trillion tokens per day on its API, amid its fierce release and performance war with OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this month, Sam Altman reportedly sent an internal “Code Red” memo to the OpenAI team after ChatGPT’s traffic dipped as Google’s market share in consumers rose. Post that, OpenAI has released GPT-5.2 and a new image generation model. OpenAI also boasted about its growing enterprise use and said the ChatGPT messages volume has grown 8x since November 2024.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Google didn’t directly address the competition with OpenAI, it said that the release of new models is challenging all companies to be active.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Just about what’s happening across the industry is like all of these models are continuing to be awesome, challenge each other, push the frontier. And I think what’s also awesome is as companies are releasing these models,” Doshi said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We’re also introducing new benchmarks and new ways of evaluating these models. And so that’s also encouraging us.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/17/google-launches-gemini-3-flash-makes-it-the-default-model-in-the-gemini-app/</guid><pubDate>Wed, 17 Dec 2025 16:00:00 +0000</pubDate></item><item><title>UC San Diego Lab Advances Generative AI Research With NVIDIA DGX B200 System (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/ucsd-generative-ai-research-dgx-b200/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;The Hao AI Lab research team at the University of California San Diego &amp;nbsp;— at the forefront of pioneering AI model innovation — recently received an NVIDIA DGX B200 system to elevate their critical work in large language model inference.&lt;/p&gt;
&lt;p&gt;Many LLM inference platforms in production today, such as NVIDIA Dynamo, use research concepts that originated in the Hao AI Lab, including DistServe.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;How Is Hao AI Lab Using the DGX B200? &lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption alignnone" id="attachment_88356"&gt;&lt;img alt="Researchers standing around the DGX B200 system inside the San Diego Supercomputing Center. " class="size-large wp-image-88356" height="1120" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/UCSD--1680x1120.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88356"&gt;Members of the Hao AI Lab standing with the NVIDIA DGX B200 system.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;With the DGX B200 now fully accessible to the Hao AI Lab and broader UC San Diego community at the School of Computing, Information and Data Sciences’ San Diego Supercomputer Center, the research opportunities are boundless.&lt;/p&gt;
&lt;p&gt;“DGX B200 is one of the most powerful AI systems from NVIDIA to date, which means that its performance is among the best in the world,” said Hao Zhang, assistant professor in the Halıcıoğlu Data Science Institute and department of computer science and engineering at UC San Diego. “It enables us to prototype and experiment much faster than using previous-generation hardware.”&lt;/p&gt;
&lt;p&gt;Two Hao AI Lab projects the DGX B200 is accelerating are FastVideo and the Lmgame benchmark.&lt;/p&gt;
&lt;p&gt;FastVideo focuses on training a family of video generation models to produce a five-second video based on a given text prompt — in just five seconds.&lt;/p&gt;
&lt;p&gt;The research phase of FastVideo taps into NVIDIA H200 GPUs in addition to the DGX B200 system.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Lmgame-bench is a benchmarking suite that puts LLMs to the test using popular online games including &lt;i&gt;Tetris&lt;/i&gt; and &lt;i&gt;Super Mario Bros&lt;/i&gt;. Users can test one model at a time or put two models up against each other to measure their performance.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88360"&gt;&lt;img alt="Illustrated image of Lmgame-Bench workflow. " class="size-large wp-image-88360" height="579" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Cute-Robot-UCSD-1680x579.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88360"&gt;The illustrated workflow of Hao AI Lab’s Lmgame-Bench project.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Other ongoing projects at Hao AI Labs explore new ways to achieve low-latency LLM serving, pushing large language models toward real-time responsiveness.&lt;/p&gt;
&lt;p&gt;“Our current research uses the DGX B200 to explore the next frontier of low-latency LLM-serving on the awesome hardware specs the system gives us,” said Junda Chen, a doctoral candidate in computer science at UC San Diego.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;How DistServe Influenced Disaggregated Serving&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Disaggregated inference is a way to ensure large-scale LLM-serving engines can achieve the optimal aggregate system throughput while maintaining acceptably low latency for user requests.&lt;/p&gt;
&lt;p&gt;The benefit of disaggregated inference lies in optimizing what DistServe calls “goodput” instead of “throughput” in the LLM-serving engine.&lt;/p&gt;
&lt;p&gt;Here’s the difference:&lt;/p&gt;
&lt;p&gt;Throughput is measured by the number of tokens per second that the entire system can generate. Higher throughput means lower cost to generate each token to serve the user. For a long time, throughput was the only metric used by LLM-serving engines to measure their performance against one another.&lt;/p&gt;
&lt;p&gt;While throughput measures the aggregate performance of the system, it doesn’t directly correlate to the latency that a user perceives. If a user demands lower latency to generate the tokens, the system has to sacrifice throughput.&lt;/p&gt;
&lt;p&gt;This natural trade-off between throughput and latency is what led the DistServe team to propose a new metric, “goodput”: the measure of throughput while satisfying the user-specified latency objectives, usually called service-level objectives. In other words, goodput represents the overall health of a system while satisfying user experience.&lt;/p&gt;
&lt;p&gt;DistServe shows that goodput is a much better metric for LLM-serving systems, as it factors in both cost and service quality. Goodput leads to optimal efficiency and ideal output from a model.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;How Can Developers Achieve Optimal Goodput? &amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;When a user makes a request in an LLM system, the system takes the user input and generates the first token, known as prefill. Then, the system creates numerous output tokens, one after another, predicting each token’s future behavior based on past requests’ outcomes. This process is known as decode.&lt;/p&gt;

&lt;p&gt;Prefill and decode have historically run on the same GPU, but the researchers behind DistServe found that splitting them onto different GPUs maximizes goodput.&lt;/p&gt;
&lt;p&gt;“Previously, if you put these two jobs on a GPU, they would compete with each other for resources, which could make it slow from a user perspective,” Chen said. “Now, if I split the jobs onto two different sets of GPUs — one doing prefill, which is compute intensive, and the other doing decode, which is more memory intensive — we can fundamentally eliminate the interference between the two jobs, making both jobs run faster.&lt;/p&gt;
&lt;p&gt;This process is called prefill/decode disaggregation, or separating the prefill from decode to get greater goodput.&lt;/p&gt;
&lt;p&gt;Increasing goodput and using the disaggregated inference method enables the continuous scaling of workloads without compromising on low-latency or high-quality model responses.&lt;/p&gt;
&lt;p&gt;NVIDIA Dynamo — an open-source framework designed to accelerate and scale generative AI models at the highest efficiency levels with the lowest cost — enables scaling disaggregated inference.&lt;/p&gt;
&lt;p&gt;In addition to these projects, cross-departmental collaborations, such as in healthcare and biology, are underway at UC San Diego to further optimize an array of research projects using the NVIDIA DGX B200, as researchers continue exploring how AI platforms can accelerate innovation.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about the &lt;/i&gt;&lt;i&gt;NVIDIA DGX B200&lt;/i&gt;&lt;i&gt; system.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;The Hao AI Lab research team at the University of California San Diego &amp;nbsp;— at the forefront of pioneering AI model innovation — recently received an NVIDIA DGX B200 system to elevate their critical work in large language model inference.&lt;/p&gt;
&lt;p&gt;Many LLM inference platforms in production today, such as NVIDIA Dynamo, use research concepts that originated in the Hao AI Lab, including DistServe.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;How Is Hao AI Lab Using the DGX B200? &lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption alignnone" id="attachment_88356"&gt;&lt;img alt="Researchers standing around the DGX B200 system inside the San Diego Supercomputing Center. " class="size-large wp-image-88356" height="1120" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/UCSD--1680x1120.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88356"&gt;Members of the Hao AI Lab standing with the NVIDIA DGX B200 system.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;With the DGX B200 now fully accessible to the Hao AI Lab and broader UC San Diego community at the School of Computing, Information and Data Sciences’ San Diego Supercomputer Center, the research opportunities are boundless.&lt;/p&gt;
&lt;p&gt;“DGX B200 is one of the most powerful AI systems from NVIDIA to date, which means that its performance is among the best in the world,” said Hao Zhang, assistant professor in the Halıcıoğlu Data Science Institute and department of computer science and engineering at UC San Diego. “It enables us to prototype and experiment much faster than using previous-generation hardware.”&lt;/p&gt;
&lt;p&gt;Two Hao AI Lab projects the DGX B200 is accelerating are FastVideo and the Lmgame benchmark.&lt;/p&gt;
&lt;p&gt;FastVideo focuses on training a family of video generation models to produce a five-second video based on a given text prompt — in just five seconds.&lt;/p&gt;
&lt;p&gt;The research phase of FastVideo taps into NVIDIA H200 GPUs in addition to the DGX B200 system.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Lmgame-bench is a benchmarking suite that puts LLMs to the test using popular online games including &lt;i&gt;Tetris&lt;/i&gt; and &lt;i&gt;Super Mario Bros&lt;/i&gt;. Users can test one model at a time or put two models up against each other to measure their performance.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88360"&gt;&lt;img alt="Illustrated image of Lmgame-Bench workflow. " class="size-large wp-image-88360" height="579" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Cute-Robot-UCSD-1680x579.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88360"&gt;The illustrated workflow of Hao AI Lab’s Lmgame-Bench project.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Other ongoing projects at Hao AI Labs explore new ways to achieve low-latency LLM serving, pushing large language models toward real-time responsiveness.&lt;/p&gt;
&lt;p&gt;“Our current research uses the DGX B200 to explore the next frontier of low-latency LLM-serving on the awesome hardware specs the system gives us,” said Junda Chen, a doctoral candidate in computer science at UC San Diego.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;How DistServe Influenced Disaggregated Serving&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Disaggregated inference is a way to ensure large-scale LLM-serving engines can achieve the optimal aggregate system throughput while maintaining acceptably low latency for user requests.&lt;/p&gt;
&lt;p&gt;The benefit of disaggregated inference lies in optimizing what DistServe calls “goodput” instead of “throughput” in the LLM-serving engine.&lt;/p&gt;
&lt;p&gt;Here’s the difference:&lt;/p&gt;
&lt;p&gt;Throughput is measured by the number of tokens per second that the entire system can generate. Higher throughput means lower cost to generate each token to serve the user. For a long time, throughput was the only metric used by LLM-serving engines to measure their performance against one another.&lt;/p&gt;
&lt;p&gt;While throughput measures the aggregate performance of the system, it doesn’t directly correlate to the latency that a user perceives. If a user demands lower latency to generate the tokens, the system has to sacrifice throughput.&lt;/p&gt;
&lt;p&gt;This natural trade-off between throughput and latency is what led the DistServe team to propose a new metric, “goodput”: the measure of throughput while satisfying the user-specified latency objectives, usually called service-level objectives. In other words, goodput represents the overall health of a system while satisfying user experience.&lt;/p&gt;
&lt;p&gt;DistServe shows that goodput is a much better metric for LLM-serving systems, as it factors in both cost and service quality. Goodput leads to optimal efficiency and ideal output from a model.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;How Can Developers Achieve Optimal Goodput? &amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;When a user makes a request in an LLM system, the system takes the user input and generates the first token, known as prefill. Then, the system creates numerous output tokens, one after another, predicting each token’s future behavior based on past requests’ outcomes. This process is known as decode.&lt;/p&gt;

&lt;p&gt;Prefill and decode have historically run on the same GPU, but the researchers behind DistServe found that splitting them onto different GPUs maximizes goodput.&lt;/p&gt;
&lt;p&gt;“Previously, if you put these two jobs on a GPU, they would compete with each other for resources, which could make it slow from a user perspective,” Chen said. “Now, if I split the jobs onto two different sets of GPUs — one doing prefill, which is compute intensive, and the other doing decode, which is more memory intensive — we can fundamentally eliminate the interference between the two jobs, making both jobs run faster.&lt;/p&gt;
&lt;p&gt;This process is called prefill/decode disaggregation, or separating the prefill from decode to get greater goodput.&lt;/p&gt;
&lt;p&gt;Increasing goodput and using the disaggregated inference method enables the continuous scaling of workloads without compromising on low-latency or high-quality model responses.&lt;/p&gt;
&lt;p&gt;NVIDIA Dynamo — an open-source framework designed to accelerate and scale generative AI models at the highest efficiency levels with the lowest cost — enables scaling disaggregated inference.&lt;/p&gt;
&lt;p&gt;In addition to these projects, cross-departmental collaborations, such as in healthcare and biology, are underway at UC San Diego to further optimize an array of research projects using the NVIDIA DGX B200, as researchers continue exploring how AI platforms can accelerate innovation.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about the &lt;/i&gt;&lt;i&gt;NVIDIA DGX B200&lt;/i&gt;&lt;i&gt; system.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/ucsd-generative-ai-research-dgx-b200/</guid><pubDate>Wed, 17 Dec 2025 16:00:15 +0000</pubDate></item><item><title>Google releases Gemini 3 Flash, promising improved intelligence and efficiency (AI – Ars Technica)</title><link>https://arstechnica.com/google/2025/12/google-releases-gemini-3-flash-promising-improved-intelligence-and-efficiency/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google’s Gemini 3 family is now complete with release of Gemini 3 Flash.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="361" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/gemini-3_flash_model_blog_header_dark_bleed_2096x1182-640x361.png" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/gemini-3_flash_model_blog_header_dark_bleed_2096x1182-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Add a lightning bolt because it's fast. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google began its transition to Gemini 3 a few weeks ago with the launch of the Pro model, and the arrival of Gemini 3 Flash kicks it into high gear. The new, faster Gemini 3 model is coming to the Gemini app and search, and developers will be able to access it immediately via the Gemini API, Vertex AI, AI Studio, and Antigravity. Google’s bigger gen AI model is also picking up steam, with both Gemini 3 Pro and its image component (Nano Banana Pro) expanding in search.&lt;/p&gt;
&lt;p&gt;This may come as a shock, but Google says Gemini 3 Flash is faster and more capable than its previous base model. As usual, Google has a raft of benchmark numbers that show modest improvements for the new model. It bests the old 2.5 Flash in basic academic and reasoning tests like GPQA Diamond and MMMU Pro (where it even beats 3 Pro). It gets a larger boost in Humanity’s Last Exam (HLE), which tests advanced domain-specific knowledge. Gemini 3 Flash has tripled the old models’ score in HLE, landing at 33.7 percent without tool use. That’s just a few points behind the Gemini 3 Pro model.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132361 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="Gemini HLE test" class="fullwidth full" height="1350" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/gemini-3-flash-hle.png" width="1080" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Google is talking up Gemini 3 Flash’s coding skills, and the provided benchmarks seem to back that talk up. Over the past year, Google has mostly pushed its Pro models as the best for generating code, but 3 Flash has done a lot of catching up. In the popular SWE-Bench Verified test, Gemini 3 Flash has gained almost 20 points on the 2.5 branch.&lt;/p&gt;
&lt;p&gt;The new model is also a lot less likely to get general-knowledge questions wrong. In the Simple QA Verified test, Gemini 3 Flash scored 68.7 percent, which is only a little below Gemini 3 Pro. The last Flash model scored just 28.1 percent on that test. At least as far as the evaluation scores go, Gemini 3 Flash performs much closer to Google’s Pro model versus the older 2.5 family. At the same time, it’s considerably more efficient, according to Google.&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="930" id="video-2132353-1" preload="metadata" width="736"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Gemini-3-Flash-App-opt.mp4?_=1" type="video/mp4" /&gt;&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
      &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;One of Gemini 3 Pro’s defining advances was its ability to generate interactive simulations and multimodal content. Gemini 3 Flash reportedly retains that underlying capability. Gemini 3 Flash offers better performance than Gemini 2.5 Pro did, but it runs workloads three times faster. It’s also a lot cheaper than the Pro models if you’re paying per token. One million input tokens for 3 Flash will run devs $0.50, and a million output tokens will cost $3. However, that’s an increase compared to Gemini 2.5 Flash input and output at $0.30 and $2.50, respectively. The Pro model’s tokens are $2 (1M input) and $12 (1M output).&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Simplified model selection&lt;/h2&gt;
&lt;p&gt;Google’s rapid-fire release of new AI models and tools has occasionally made the Gemini app a bit confusing. Over recent weeks, the settings have been pared down and rearranged. With the release of Gemini 3 Flash, that will become the new default model in the Gemini app and web interface—that’s the Fast setting in the app, as well as the one labeled Thinking, which uses simulated reasoning for better outputs.&lt;/p&gt;
&lt;p&gt;Gemini 3 Pro will continue to be available under the Pro option. That’s still a bit misleading, though, as both versions of Gemini 3 can use the reasoning process that Google likes to call “thinking” to generate answers. Whichever one you choose in the app, you can then select tools like image generation, canvas, and Deep Research.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132356 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Gemini model picker new" class="fullwidth full" height="298" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Gemini-3-Flash-debut.png" width="786" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Fast and Thinking are both Gemini 3 Flash; the Pro option invokes Gemini 3 Pro, which always “thinks.”

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;In addition to its debut in the Gemini app, the new Flash model will be coming to search immediately. When Google says “search” in this context, it mostly means AI Mode. Gemini 3 Flash will be the default model in AI Mode going forward. That means free users will see a notable improvement when using the Gemini app.&lt;/p&gt;
&lt;p&gt;There are no specific changes to AI Overviews. Google says AI Overviews will continue to use the best model for the job. Due to its place at the top of organic search results, though, you’ll probably see it lean on less capable (but faster) models. Gemini 3 Flash could show up there—even Gemini 3 Pro could power some complex queries in AI Overviews for paying subscribers.&lt;/p&gt;
&lt;p&gt;Gemini 3 Pro is also expanding in AI Mode for all US-based users. Likewise, Gemini 3 Pro Image (Nano Banana Pro) will also arrive in AI mode for all. There will be limits on free access to these models, but Google hasn’t specified what those are. It does say that Pro and Ultra subscribers will enjoy much higher usage limits.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google’s Gemini 3 family is now complete with release of Gemini 3 Flash.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="361" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/gemini-3_flash_model_blog_header_dark_bleed_2096x1182-640x361.png" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/gemini-3_flash_model_blog_header_dark_bleed_2096x1182-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Add a lightning bolt because it's fast. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google began its transition to Gemini 3 a few weeks ago with the launch of the Pro model, and the arrival of Gemini 3 Flash kicks it into high gear. The new, faster Gemini 3 model is coming to the Gemini app and search, and developers will be able to access it immediately via the Gemini API, Vertex AI, AI Studio, and Antigravity. Google’s bigger gen AI model is also picking up steam, with both Gemini 3 Pro and its image component (Nano Banana Pro) expanding in search.&lt;/p&gt;
&lt;p&gt;This may come as a shock, but Google says Gemini 3 Flash is faster and more capable than its previous base model. As usual, Google has a raft of benchmark numbers that show modest improvements for the new model. It bests the old 2.5 Flash in basic academic and reasoning tests like GPQA Diamond and MMMU Pro (where it even beats 3 Pro). It gets a larger boost in Humanity’s Last Exam (HLE), which tests advanced domain-specific knowledge. Gemini 3 Flash has tripled the old models’ score in HLE, landing at 33.7 percent without tool use. That’s just a few points behind the Gemini 3 Pro model.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132361 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="Gemini HLE test" class="fullwidth full" height="1350" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/gemini-3-flash-hle.png" width="1080" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Google is talking up Gemini 3 Flash’s coding skills, and the provided benchmarks seem to back that talk up. Over the past year, Google has mostly pushed its Pro models as the best for generating code, but 3 Flash has done a lot of catching up. In the popular SWE-Bench Verified test, Gemini 3 Flash has gained almost 20 points on the 2.5 branch.&lt;/p&gt;
&lt;p&gt;The new model is also a lot less likely to get general-knowledge questions wrong. In the Simple QA Verified test, Gemini 3 Flash scored 68.7 percent, which is only a little below Gemini 3 Pro. The last Flash model scored just 28.1 percent on that test. At least as far as the evaluation scores go, Gemini 3 Flash performs much closer to Google’s Pro model versus the older 2.5 family. At the same time, it’s considerably more efficient, according to Google.&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="930" id="video-2132353-1" preload="metadata" width="736"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Gemini-3-Flash-App-opt.mp4?_=1" type="video/mp4" /&gt;&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
      &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;One of Gemini 3 Pro’s defining advances was its ability to generate interactive simulations and multimodal content. Gemini 3 Flash reportedly retains that underlying capability. Gemini 3 Flash offers better performance than Gemini 2.5 Pro did, but it runs workloads three times faster. It’s also a lot cheaper than the Pro models if you’re paying per token. One million input tokens for 3 Flash will run devs $0.50, and a million output tokens will cost $3. However, that’s an increase compared to Gemini 2.5 Flash input and output at $0.30 and $2.50, respectively. The Pro model’s tokens are $2 (1M input) and $12 (1M output).&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Simplified model selection&lt;/h2&gt;
&lt;p&gt;Google’s rapid-fire release of new AI models and tools has occasionally made the Gemini app a bit confusing. Over recent weeks, the settings have been pared down and rearranged. With the release of Gemini 3 Flash, that will become the new default model in the Gemini app and web interface—that’s the Fast setting in the app, as well as the one labeled Thinking, which uses simulated reasoning for better outputs.&lt;/p&gt;
&lt;p&gt;Gemini 3 Pro will continue to be available under the Pro option. That’s still a bit misleading, though, as both versions of Gemini 3 can use the reasoning process that Google likes to call “thinking” to generate answers. Whichever one you choose in the app, you can then select tools like image generation, canvas, and Deep Research.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132356 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Gemini model picker new" class="fullwidth full" height="298" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Gemini-3-Flash-debut.png" width="786" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Fast and Thinking are both Gemini 3 Flash; the Pro option invokes Gemini 3 Pro, which always “thinks.”

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;In addition to its debut in the Gemini app, the new Flash model will be coming to search immediately. When Google says “search” in this context, it mostly means AI Mode. Gemini 3 Flash will be the default model in AI Mode going forward. That means free users will see a notable improvement when using the Gemini app.&lt;/p&gt;
&lt;p&gt;There are no specific changes to AI Overviews. Google says AI Overviews will continue to use the best model for the job. Due to its place at the top of organic search results, though, you’ll probably see it lean on less capable (but faster) models. Gemini 3 Flash could show up there—even Gemini 3 Pro could power some complex queries in AI Overviews for paying subscribers.&lt;/p&gt;
&lt;p&gt;Gemini 3 Pro is also expanding in AI Mode for all US-based users. Likewise, Gemini 3 Pro Image (Nano Banana Pro) will also arrive in AI mode for all. There will be limits on free access to these models, but Google hasn’t specified what those are. It does say that Pro and Ultra subscribers will enjoy much higher usage limits.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2025/12/google-releases-gemini-3-flash-promising-improved-intelligence-and-efficiency/</guid><pubDate>Wed, 17 Dec 2025 16:00:25 +0000</pubDate></item><item><title>Into the Omniverse: OpenUSD and NVIDIA Halos Accelerate Safety for Robotaxis, Physical AI Systems (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/openusd-halos-safety-robotaxi-physical-ai/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This post is part of &lt;/i&gt;&lt;i&gt;Into the Omniverse&lt;/i&gt;&lt;i&gt;, a series focused on how developers, 3D practitioners and enterprises can transform their workflows using the latest advancements in &lt;/i&gt;&lt;i&gt;OpenUSD&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;NVIDIA Omniverse&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Physical AI is moving from research labs into the real world, powering intelligent robots and autonomous vehicles (AVs) — such as robotaxis — that must reliably sense, reason and act amid unpredictable conditions.&lt;/p&gt;
&lt;p&gt;To safely scale these systems, developers need workflows that connect real-world data, high-fidelity simulation and robust AI models atop the common foundation provided by the OpenUSD framework.&lt;/p&gt;
&lt;p&gt;The recently published OpenUSD Core Specification 1.0, OpenUSD — aka Universal Scene Description — now defines standard data types, file formats and composition behaviors, giving developers predictable, interoperable USD pipelines as they scale autonomous systems.&lt;/p&gt;
&lt;p&gt;Powered by OpenUSD, NVIDIA Omniverse libraries combine NVIDIA RTX rendering, physics simulation and efficient runtimes to create digital twins and simulation-ready (SimReady) assets that accurately reflect real-world environments for synthetic data generation and testing.&lt;/p&gt;
&lt;p&gt;NVIDIA Cosmos world foundation models can run on top of these simulations to amplify data variation, generating new weather, lighting and terrain conditions from the same scenes so teams can safely cover rare and challenging edge cases.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more by watching the OpenUSD livestream today at 11 a.m. PT or in replay, part of the NVIDIA Omniverse OpenUSD Insiders series:&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;In addition, advancements in synthetic data generation, multimodal datasets and SimReady workflows are now converging with the NVIDIA Halos framework for AV safety, creating a standards-based path to safer, faster, more cost-effective deployment of next-generation autonomous machines.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Building the Foundation for Safe Physical AI&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;&lt;b&gt;Open Standards and SimReady Assets&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;The OpenUSD Core Specification 1.0 establishes the standard data models and behaviors that underpin SimReady assets, enabling developers to build interoperable simulation pipelines for AI factories and robotics on OpenUSD.&lt;/p&gt;
&lt;p&gt;Built on this foundation, SimReady 3D assets can be reused across tools and teams and loaded directly into NVIDIA Isaac Sim, where USDPhysics colliders, rigid body dynamics and composition-arc–based variants let teams test robots in virtual facilities that closely mirror real operations.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Open-Source Learning&amp;nbsp;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;The Learn OpenUSD curriculum is now open source and available on GitHub, enabling contributors to localize and adapt templates, exercises and content for different audiences, languages and use cases. This gives educators a ready-made foundation to onboard new teams into OpenUSD-centric simulation workflows.​&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Generative Worlds as Safety Multiplier&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Gaussian splatting — a technique that uses editable 3D elements to render environments quickly and with high fidelity — and world models are accelerating simulation pipelines for safe robotics testing and validation.&lt;/p&gt;
&lt;p&gt;At SIGGRAPH Asia, the NVIDIA Research team introduced Play4D, a streaming pipeline that enables 4D Gaussian splatting to accurately render dynamic scenes and improve realism.&lt;/p&gt;
&lt;p&gt;Spatial intelligence company World Labs is using its Marble generative world model with NVIDIA Isaac Sim and Omniverse NuRec so researchers can turn text prompts and sample images into photorealistic, Gaussian-based physics-ready 3D environments in hours instead of weeks.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-full wp-image-88411" height="338" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/WorldLabs_IsaacSim_Clip.gif" width="600" /&gt;&lt;/p&gt;
&lt;p&gt;Those worlds can then be used for physical AI training, testing and sim-to-real transfer. This high-fidelity simulation workflow expands the range of scenarios robots can practice in while keeping experimentation safely in simulation.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Lightwheel Helps Teams Scale Robot Training With SimReady Assets&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Powered by OpenUSD, Lightwheel’s SimReady asset library includes a common scene description layer, making it easy to assemble high-fidelity digital twins for robots. The SimReady assets are embedded with precise geometry, materials and validated physical properties, which can be loaded directly into NVIDIA Isaac Sim and Isaac Lab for robot training. This allows robots to experience realistic contacts, dynamics and sensor feedback as they learn.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;End-to-End Autonomous Vehicle Safety&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;End-to-end autonomous vehicle safety advancements are accelerating with new research, open frameworks and inspection services that make validation more rigorous and scalable.&lt;/p&gt;
&lt;p&gt;NVIDIA researchers, with collaborators at Harvard University and Stanford University, recently introduced the Sim2Val framework to statistically combine real-world and simulated test results, reducing AV developers’ need for costly physical mileage while demonstrating how robotaxis and AVs can behave safely across rare and safety-critical scenarios.&lt;/p&gt;
&lt;p&gt;Learn more by watching NVIDIA’s “Safety in the Loop” livestream:&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;These innovations are complemented by a new, open-source NVIDIA Omniverse NuRec Fixer, a Cosmos-based model trained on AV data that removes artifacts in neural reconstructions to produce higher-quality SimReady assets.&lt;/p&gt;
&lt;p&gt;To align these advances with rigorous global standards, the NVIDIA Halos AI Systems Inspection Lab — accredited by ANAB — provides impartial inspection and certification of Halos elements across robotaxi fleets, AV stacks, sensors and manufacturer platforms through the Halos Certification Program.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AV Ecosystem Leaders Putting Physical AI Safety to Work&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Bosch, Nuro and Wayve are among the first participants in the NVIDIA Halos AI Systems Inspection Lab, which aims to accelerate the safe, large-scale deployment of robotaxi fleets. Onsemi, which makes sensor systems for AVs, industrial automation and medical applications, has recently become the first company to pass inspection for the NVIDIA Halos AI Systems Inspection Lab.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;The open-source CARLA simulator integrates NVIDIA NuRec and Cosmos Transfer to generate reconstructed drives and diverse scenario variations, while Voxel51’s FiftyOne engine, linked to Cosmos Dataset Search, NuRec and Cosmos Transfer, helps teams curate, annotate and evaluate multimodal datasets across the AV pipeline.​&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Mcity at the University of Michigan is enhancing the digital twin of its 32-acre AV test facility using Omniverse libraries and technologies. The team is integrating the NVIDIA Blueprint for AV simulation and Omniverse Sensor RTX application programming interfaces to create physics-based models of camera, lidar, radar and ultrasonic sensors.&lt;/p&gt;
&lt;p&gt;By aligning real sensor recordings with high-fidelity simulated data and sharing assets openly, Mcity enables safe, repeatable testing of rare and hazardous driving scenarios before vehicles operate on public roads.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Get Plugged Into the World of OpenUSD and Physical AI Safety&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Learn more about OpenUSD, NVIDIA Halos and physical AI safety by exploring these resources:&lt;/p&gt;

&lt;p&gt;&lt;i&gt;Stay up to date by subscribing to&lt;/i&gt; &lt;i&gt;NVIDIA news&lt;/i&gt;&lt;i&gt;, joining the &lt;/i&gt;&lt;i&gt;community&lt;/i&gt;&lt;i&gt; and following NVIDIA Omniverse on &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Medium&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This post is part of &lt;/i&gt;&lt;i&gt;Into the Omniverse&lt;/i&gt;&lt;i&gt;, a series focused on how developers, 3D practitioners and enterprises can transform their workflows using the latest advancements in &lt;/i&gt;&lt;i&gt;OpenUSD&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;NVIDIA Omniverse&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Physical AI is moving from research labs into the real world, powering intelligent robots and autonomous vehicles (AVs) — such as robotaxis — that must reliably sense, reason and act amid unpredictable conditions.&lt;/p&gt;
&lt;p&gt;To safely scale these systems, developers need workflows that connect real-world data, high-fidelity simulation and robust AI models atop the common foundation provided by the OpenUSD framework.&lt;/p&gt;
&lt;p&gt;The recently published OpenUSD Core Specification 1.0, OpenUSD — aka Universal Scene Description — now defines standard data types, file formats and composition behaviors, giving developers predictable, interoperable USD pipelines as they scale autonomous systems.&lt;/p&gt;
&lt;p&gt;Powered by OpenUSD, NVIDIA Omniverse libraries combine NVIDIA RTX rendering, physics simulation and efficient runtimes to create digital twins and simulation-ready (SimReady) assets that accurately reflect real-world environments for synthetic data generation and testing.&lt;/p&gt;
&lt;p&gt;NVIDIA Cosmos world foundation models can run on top of these simulations to amplify data variation, generating new weather, lighting and terrain conditions from the same scenes so teams can safely cover rare and challenging edge cases.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more by watching the OpenUSD livestream today at 11 a.m. PT or in replay, part of the NVIDIA Omniverse OpenUSD Insiders series:&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;In addition, advancements in synthetic data generation, multimodal datasets and SimReady workflows are now converging with the NVIDIA Halos framework for AV safety, creating a standards-based path to safer, faster, more cost-effective deployment of next-generation autonomous machines.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Building the Foundation for Safe Physical AI&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;&lt;b&gt;Open Standards and SimReady Assets&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;The OpenUSD Core Specification 1.0 establishes the standard data models and behaviors that underpin SimReady assets, enabling developers to build interoperable simulation pipelines for AI factories and robotics on OpenUSD.&lt;/p&gt;
&lt;p&gt;Built on this foundation, SimReady 3D assets can be reused across tools and teams and loaded directly into NVIDIA Isaac Sim, where USDPhysics colliders, rigid body dynamics and composition-arc–based variants let teams test robots in virtual facilities that closely mirror real operations.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Open-Source Learning&amp;nbsp;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;The Learn OpenUSD curriculum is now open source and available on GitHub, enabling contributors to localize and adapt templates, exercises and content for different audiences, languages and use cases. This gives educators a ready-made foundation to onboard new teams into OpenUSD-centric simulation workflows.​&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Generative Worlds as Safety Multiplier&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Gaussian splatting — a technique that uses editable 3D elements to render environments quickly and with high fidelity — and world models are accelerating simulation pipelines for safe robotics testing and validation.&lt;/p&gt;
&lt;p&gt;At SIGGRAPH Asia, the NVIDIA Research team introduced Play4D, a streaming pipeline that enables 4D Gaussian splatting to accurately render dynamic scenes and improve realism.&lt;/p&gt;
&lt;p&gt;Spatial intelligence company World Labs is using its Marble generative world model with NVIDIA Isaac Sim and Omniverse NuRec so researchers can turn text prompts and sample images into photorealistic, Gaussian-based physics-ready 3D environments in hours instead of weeks.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-full wp-image-88411" height="338" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/WorldLabs_IsaacSim_Clip.gif" width="600" /&gt;&lt;/p&gt;
&lt;p&gt;Those worlds can then be used for physical AI training, testing and sim-to-real transfer. This high-fidelity simulation workflow expands the range of scenarios robots can practice in while keeping experimentation safely in simulation.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Lightwheel Helps Teams Scale Robot Training With SimReady Assets&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Powered by OpenUSD, Lightwheel’s SimReady asset library includes a common scene description layer, making it easy to assemble high-fidelity digital twins for robots. The SimReady assets are embedded with precise geometry, materials and validated physical properties, which can be loaded directly into NVIDIA Isaac Sim and Isaac Lab for robot training. This allows robots to experience realistic contacts, dynamics and sensor feedback as they learn.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;End-to-End Autonomous Vehicle Safety&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;End-to-end autonomous vehicle safety advancements are accelerating with new research, open frameworks and inspection services that make validation more rigorous and scalable.&lt;/p&gt;
&lt;p&gt;NVIDIA researchers, with collaborators at Harvard University and Stanford University, recently introduced the Sim2Val framework to statistically combine real-world and simulated test results, reducing AV developers’ need for costly physical mileage while demonstrating how robotaxis and AVs can behave safely across rare and safety-critical scenarios.&lt;/p&gt;
&lt;p&gt;Learn more by watching NVIDIA’s “Safety in the Loop” livestream:&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;These innovations are complemented by a new, open-source NVIDIA Omniverse NuRec Fixer, a Cosmos-based model trained on AV data that removes artifacts in neural reconstructions to produce higher-quality SimReady assets.&lt;/p&gt;
&lt;p&gt;To align these advances with rigorous global standards, the NVIDIA Halos AI Systems Inspection Lab — accredited by ANAB — provides impartial inspection and certification of Halos elements across robotaxi fleets, AV stacks, sensors and manufacturer platforms through the Halos Certification Program.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AV Ecosystem Leaders Putting Physical AI Safety to Work&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Bosch, Nuro and Wayve are among the first participants in the NVIDIA Halos AI Systems Inspection Lab, which aims to accelerate the safe, large-scale deployment of robotaxi fleets. Onsemi, which makes sensor systems for AVs, industrial automation and medical applications, has recently become the first company to pass inspection for the NVIDIA Halos AI Systems Inspection Lab.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;The open-source CARLA simulator integrates NVIDIA NuRec and Cosmos Transfer to generate reconstructed drives and diverse scenario variations, while Voxel51’s FiftyOne engine, linked to Cosmos Dataset Search, NuRec and Cosmos Transfer, helps teams curate, annotate and evaluate multimodal datasets across the AV pipeline.​&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Mcity at the University of Michigan is enhancing the digital twin of its 32-acre AV test facility using Omniverse libraries and technologies. The team is integrating the NVIDIA Blueprint for AV simulation and Omniverse Sensor RTX application programming interfaces to create physics-based models of camera, lidar, radar and ultrasonic sensors.&lt;/p&gt;
&lt;p&gt;By aligning real sensor recordings with high-fidelity simulated data and sharing assets openly, Mcity enables safe, repeatable testing of rare and hazardous driving scenarios before vehicles operate on public roads.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Get Plugged Into the World of OpenUSD and Physical AI Safety&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Learn more about OpenUSD, NVIDIA Halos and physical AI safety by exploring these resources:&lt;/p&gt;

&lt;p&gt;&lt;i&gt;Stay up to date by subscribing to&lt;/i&gt; &lt;i&gt;NVIDIA news&lt;/i&gt;&lt;i&gt;, joining the &lt;/i&gt;&lt;i&gt;community&lt;/i&gt;&lt;i&gt; and following NVIDIA Omniverse on &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Medium&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/openusd-halos-safety-robotaxi-physical-ai/</guid><pubDate>Wed, 17 Dec 2025 17:00:49 +0000</pubDate></item><item><title>Amazon appoints longtime AWS exec Peter DeSantis to lead new AI org (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/17/amazon-appoints-longtime-aws-exec-peter-desantis-to-lead-new-ai-org/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/amazon-data-center.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon CEO Andy Jassy announced in a message to staff on Wednesday that longtime AWS executive Peter DeSantis will lead a new AI-focused organization within the company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This organization will be responsible for Amazon’s AI models like Nova, as well as silicon development and quantum computing, which help make AI tools faster and more efficient.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;DeSantis has spent 27 years at Amazon, including eight years as an SVP for AWS, the cloud provider that powers about one-third of the internet. At AWS’s recent re:Invent event, Amazon hammered home its commitment to AI for enterprise use, so it makes sense that the company is spinning out a new team from AWS leadership.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“With our Nova 2 models just launched at re:Invent, our custom silicon growing rapidly, and the advantages of optimizing across models, chips, and cloud software and infrastructure, we wanted to free Peter up to focus his energy, invention cycles, and leadership on these new areas,” Jassy wrote.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon’s increasing emphasis on AI comes at a time when the company is eager to strengthen its foothold in the AI race, perhaps more through investments than its own innovations. Last month, AWS announced a $50 billion investment in the U.S. government’s AI infrastructure. Amazon is also reportedly in talks to invest $10 billion in OpenAI, and has already invested $8 billion in OpenAI rival Anthropic.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/amazon-data-center.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon CEO Andy Jassy announced in a message to staff on Wednesday that longtime AWS executive Peter DeSantis will lead a new AI-focused organization within the company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This organization will be responsible for Amazon’s AI models like Nova, as well as silicon development and quantum computing, which help make AI tools faster and more efficient.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;DeSantis has spent 27 years at Amazon, including eight years as an SVP for AWS, the cloud provider that powers about one-third of the internet. At AWS’s recent re:Invent event, Amazon hammered home its commitment to AI for enterprise use, so it makes sense that the company is spinning out a new team from AWS leadership.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“With our Nova 2 models just launched at re:Invent, our custom silicon growing rapidly, and the advantages of optimizing across models, chips, and cloud software and infrastructure, we wanted to free Peter up to focus his energy, invention cycles, and leadership on these new areas,” Jassy wrote.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon’s increasing emphasis on AI comes at a time when the company is eager to strengthen its foothold in the AI race, perhaps more through investments than its own innovations. Last month, AWS announced a $50 billion investment in the U.S. government’s AI infrastructure. Amazon is also reportedly in talks to invest $10 billion in OpenAI, and has already invested $8 billion in OpenAI rival Anthropic.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/17/amazon-appoints-longtime-aws-exec-peter-desantis-to-lead-new-ai-org/</guid><pubDate>Wed, 17 Dec 2025 18:06:29 +0000</pubDate></item><item><title>[NEW] Bursting AI bubble may be EU’s “secret weapon” in clash with Trump, expert says (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/12/us-threatens-crackdown-on-eu-firms-as-clash-over-tech-regulations-intensifies/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Spotify and Accenture caught in crossfire as Trump attacks EU tech regulations.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="376" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2233373547-640x376.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2233373547-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Moor Studio | DigitalVision Vectors

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;The US threatened to restrict some of the largest service providers in the European Union as retaliation for EU tech regulations and investigations are increasingly drawing Donald Trump’s ire.&lt;/p&gt;
&lt;p&gt;On Tuesday, the Office of the US Trade Representative (USTR) issued a warning on X, naming Spotify, Accenture, Amadeus, Mistral, Publicis, and DHL among nine firms suddenly yanked into the middle of the US-EU tech fight.&lt;/p&gt;
&lt;p&gt;“The European Union and certain EU Member States have persisted in a continuing course of discriminatory and harassing lawsuits, taxes, fines, and directives against US service providers,” USTR’s post said.&lt;/p&gt;
&lt;p&gt;The clash comes after Elon Musk’s X became the first tech company fined for violating the EU’s Digital Services Act, which is widely considered among the world’s strictest tech regulations. Trump was not appeased by the European Commission (EC) noting that X was not ordered to pay the maximum possible fine. Instead, the $140 million fine sparked backlash within the Trump administration, including from Vice President JD Vance, who slammed the fine as “censorship” of X and its users.&lt;/p&gt;
&lt;p&gt;Asked for comment on the USTR’s post, an EC spokesperson told Ars that the EU intends to defend its tech regulations while implementing commitments from a Trump trade deal that the EU struck in August.&lt;/p&gt;
&lt;p&gt;“The EU is an open and rules-based market, where companies from all over the world do business successfully and profitably,” the EC’s spokesperson said. “As we have made clear many times, our rules apply equally and fairly to all companies operating in the EU,” ensuring “a safe, fair and level playing field in the EU, in line with the expectations of our citizens. We will continue to enforce our rules fairly, and without discrimination.”&lt;/p&gt;
&lt;h2&gt;Trump on shaky ground due to “AI bubble”&lt;/h2&gt;
&lt;p&gt;On X, the USTR account suggested that the EU was overlooking that US companies “provide substantial free services to EU citizens and reliable enterprise services to EU companies,” while supporting “millions of jobs and more than $100 billion in direct investment in Europe.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;To stop what Trump views as “overseas extortion” of American tech companies, the USTR said the US was prepared to go after EU service providers, which “have been able to operate freely in the United States for decades, benefitting from access to our market and consumers on a level playing field.”&lt;/p&gt;
&lt;p&gt;“If the EU and EU Member States insist on continuing to restrict, limit, and deter the competitiveness of US service providers through discriminatory means, the United States will have no choice but to begin using every tool at its disposal to counter these unreasonable measures,” USTR’s post said. “Should responsive measures be necessary, US law permits the assessment of fees or restrictions on foreign services, among other actions.”&lt;/p&gt;
&lt;p&gt;The pushback comes after the Trump administration released a November national security report that questioned how long the EU could remain a “reliable” ally as overregulation of its tech industry could hobble both its economy and military strength. Claiming that the EU was only “doubling down” on such regulations, the EU “will be unrecognizable in 20 years or less,” the report predicted.&lt;/p&gt;
&lt;p&gt;“We want Europe to remain European, to regain its civilizational self-confidence, and to abandon its failed focus on regulatory suffocation,” the report said.&lt;/p&gt;
&lt;p&gt;However, the report acknowledged that “Europe remains strategically and culturally vital to the United States.”&lt;/p&gt;
&lt;p&gt;“Transatlantic trade remains one of the pillars of the global economy and of American prosperity,” the report said. “European sectors from manufacturing to technology to energy remain among the world’s most robust. Europe is home to cutting-edge scientific research and world-leading cultural institutions. Not only can we not afford to write Europe off—doing so would be self-defeating for what this strategy aims to achieve.”&lt;/p&gt;
&lt;p&gt;At least one expert in the EU has suggested that the EU can use this acknowledgement as leverage, while perhaps even using the looming threat of the supposed American “AI bubble” bursting to pressure Trump into backing off EU tech laws.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In an op-ed for The Guardian, Johnny Ryan, the director of Enforce, a unit of the Irish Council for Civil Liberties, suggested that the EU could even throw Trump’s presidency into “crisis” by taking bold steps that Trump may not see coming.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;EU can take steps to burst “AI bubble”&lt;/h2&gt;
&lt;p&gt;According to Ryan, the national security report made clear that the EU must fight the US or else “perish.” However, the EU has two “strong cards” to play if it wants to win the fight, he suggested.&lt;/p&gt;
&lt;p&gt;Right now, market analysts are fretting about an “AI bubble,” with US investment in AI far outpacing potential gains until perhaps 2030. A Harvard University business professor focused on helping businesses implement cutting-edge technology like generative AI, Andy Wu, recently explained that AI’s big problem is that “everyone can imagine how useful the technology will be, but no one has figured out yet how to make money.”&lt;/p&gt;
&lt;p&gt;“If the market can keep the faith to persist, it buys the necessary time for the technology to mature, for the costs to come down, and for companies to figure out the business model,” Wu said. But US “companies can end up underwater if AI grows fast but less rapidly than they hope for,” he suggested.&lt;/p&gt;
&lt;p&gt;During this moment, Ryan wrote, it’s not just AI firms with skin in the game, but potentially all of Trump’s supporters. The US is currently on “shaky economic ground” with AI investment accounting “for virtually all (92 percent) GDP growth in the first half of this year.”&lt;/p&gt;
&lt;p&gt;“The US’s bet on AI is now so gigantic that every MAGA voter’s pension is bound to the bubble’s precarious survival,” Ryan said.&lt;/p&gt;
&lt;p&gt;Ursula von der Leyen, the president of the European Commission, could exploit this apparent weakness first by messing with one of the biggest players in America’s AI industry, Nvidia, then by ramping up enforcement of the tech laws Trump loathes.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;According to Ryan, “Dutch company ASML commands a global monopoly on the microchip-etching machines that use light to carve patterns on silicon,” and Nvidia needs those machines if it wants to remain the world’s most valuable company. Should the US GDP remain reliant on AI investment for growth, von der Leyen could use export curbs on that technology like a “lever,” Ryan said, controlling “whether and by how much the US economy expands or contracts.”&lt;/p&gt;
&lt;p&gt;Withholding those machines “would be difficult for Europe” and “extremely painful for the Dutch economy,” Ryan noted, but “it would be far more painful for Trump.”&lt;/p&gt;
&lt;p&gt;Another step the EU could take is even “easier,” Ryan suggested. It could go even harder on the enforcement of tech regulations based on evidence of mismanaged data surfaced in lawsuits against giants like Google and Meta. For example, it seems clear that Meta may have violated the EU’s General Data Protection Regulation (GDPR), after the Facebook owner was “unable to tell a US court that what its internal systems do with your data, or who can access it, or for what purpose.”&lt;/p&gt;
&lt;p&gt;“This data free-for-all lets big tech companies train their AI models on masses of everyone’s data, but it is illegal in Europe, where companies are required to carefully control and account for how they use personal data,” Ryan wrote. “All Brussels has to do is crack down on Ireland, which for years has been a wild west of lax data enforcement, and the repercussions will be felt far beyond.”&lt;/p&gt;
&lt;p&gt;Taking that step would also arguably make it harder for tech companies to secure AI investments, since firms would have to disclose that their “AI tools are barred from accessing Europe’s valuable markets,” Ryan said.&lt;/p&gt;
&lt;p&gt;Calling the reaction to the X fine “extreme,” Ryan pushed for von der Leyen to advance on both fronts, forecasting that “the AI bubble would be unlikely to survive this double shock” and likely neither could Trump’s approval ratings. There’s also a possibility that tech firms could pressure Trump to back down if coping with any increased enforcement threatens AI progress.&lt;/p&gt;
&lt;p&gt;Although Wu suggested that Big Tech firms like Google and Meta would likely be “insulated” from the AI bubble bursting, Google CEO Sundar Pichai doesn’t seem so sure. In November, Pichai told the BBC that if AI investments didn’t pay off quickly enough, he thinks “no company is going to be immune, including us.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Spotify and Accenture caught in crossfire as Trump attacks EU tech regulations.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="376" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2233373547-640x376.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2233373547-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Moor Studio | DigitalVision Vectors

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;The US threatened to restrict some of the largest service providers in the European Union as retaliation for EU tech regulations and investigations are increasingly drawing Donald Trump’s ire.&lt;/p&gt;
&lt;p&gt;On Tuesday, the Office of the US Trade Representative (USTR) issued a warning on X, naming Spotify, Accenture, Amadeus, Mistral, Publicis, and DHL among nine firms suddenly yanked into the middle of the US-EU tech fight.&lt;/p&gt;
&lt;p&gt;“The European Union and certain EU Member States have persisted in a continuing course of discriminatory and harassing lawsuits, taxes, fines, and directives against US service providers,” USTR’s post said.&lt;/p&gt;
&lt;p&gt;The clash comes after Elon Musk’s X became the first tech company fined for violating the EU’s Digital Services Act, which is widely considered among the world’s strictest tech regulations. Trump was not appeased by the European Commission (EC) noting that X was not ordered to pay the maximum possible fine. Instead, the $140 million fine sparked backlash within the Trump administration, including from Vice President JD Vance, who slammed the fine as “censorship” of X and its users.&lt;/p&gt;
&lt;p&gt;Asked for comment on the USTR’s post, an EC spokesperson told Ars that the EU intends to defend its tech regulations while implementing commitments from a Trump trade deal that the EU struck in August.&lt;/p&gt;
&lt;p&gt;“The EU is an open and rules-based market, where companies from all over the world do business successfully and profitably,” the EC’s spokesperson said. “As we have made clear many times, our rules apply equally and fairly to all companies operating in the EU,” ensuring “a safe, fair and level playing field in the EU, in line with the expectations of our citizens. We will continue to enforce our rules fairly, and without discrimination.”&lt;/p&gt;
&lt;h2&gt;Trump on shaky ground due to “AI bubble”&lt;/h2&gt;
&lt;p&gt;On X, the USTR account suggested that the EU was overlooking that US companies “provide substantial free services to EU citizens and reliable enterprise services to EU companies,” while supporting “millions of jobs and more than $100 billion in direct investment in Europe.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;To stop what Trump views as “overseas extortion” of American tech companies, the USTR said the US was prepared to go after EU service providers, which “have been able to operate freely in the United States for decades, benefitting from access to our market and consumers on a level playing field.”&lt;/p&gt;
&lt;p&gt;“If the EU and EU Member States insist on continuing to restrict, limit, and deter the competitiveness of US service providers through discriminatory means, the United States will have no choice but to begin using every tool at its disposal to counter these unreasonable measures,” USTR’s post said. “Should responsive measures be necessary, US law permits the assessment of fees or restrictions on foreign services, among other actions.”&lt;/p&gt;
&lt;p&gt;The pushback comes after the Trump administration released a November national security report that questioned how long the EU could remain a “reliable” ally as overregulation of its tech industry could hobble both its economy and military strength. Claiming that the EU was only “doubling down” on such regulations, the EU “will be unrecognizable in 20 years or less,” the report predicted.&lt;/p&gt;
&lt;p&gt;“We want Europe to remain European, to regain its civilizational self-confidence, and to abandon its failed focus on regulatory suffocation,” the report said.&lt;/p&gt;
&lt;p&gt;However, the report acknowledged that “Europe remains strategically and culturally vital to the United States.”&lt;/p&gt;
&lt;p&gt;“Transatlantic trade remains one of the pillars of the global economy and of American prosperity,” the report said. “European sectors from manufacturing to technology to energy remain among the world’s most robust. Europe is home to cutting-edge scientific research and world-leading cultural institutions. Not only can we not afford to write Europe off—doing so would be self-defeating for what this strategy aims to achieve.”&lt;/p&gt;
&lt;p&gt;At least one expert in the EU has suggested that the EU can use this acknowledgement as leverage, while perhaps even using the looming threat of the supposed American “AI bubble” bursting to pressure Trump into backing off EU tech laws.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In an op-ed for The Guardian, Johnny Ryan, the director of Enforce, a unit of the Irish Council for Civil Liberties, suggested that the EU could even throw Trump’s presidency into “crisis” by taking bold steps that Trump may not see coming.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;EU can take steps to burst “AI bubble”&lt;/h2&gt;
&lt;p&gt;According to Ryan, the national security report made clear that the EU must fight the US or else “perish.” However, the EU has two “strong cards” to play if it wants to win the fight, he suggested.&lt;/p&gt;
&lt;p&gt;Right now, market analysts are fretting about an “AI bubble,” with US investment in AI far outpacing potential gains until perhaps 2030. A Harvard University business professor focused on helping businesses implement cutting-edge technology like generative AI, Andy Wu, recently explained that AI’s big problem is that “everyone can imagine how useful the technology will be, but no one has figured out yet how to make money.”&lt;/p&gt;
&lt;p&gt;“If the market can keep the faith to persist, it buys the necessary time for the technology to mature, for the costs to come down, and for companies to figure out the business model,” Wu said. But US “companies can end up underwater if AI grows fast but less rapidly than they hope for,” he suggested.&lt;/p&gt;
&lt;p&gt;During this moment, Ryan wrote, it’s not just AI firms with skin in the game, but potentially all of Trump’s supporters. The US is currently on “shaky economic ground” with AI investment accounting “for virtually all (92 percent) GDP growth in the first half of this year.”&lt;/p&gt;
&lt;p&gt;“The US’s bet on AI is now so gigantic that every MAGA voter’s pension is bound to the bubble’s precarious survival,” Ryan said.&lt;/p&gt;
&lt;p&gt;Ursula von der Leyen, the president of the European Commission, could exploit this apparent weakness first by messing with one of the biggest players in America’s AI industry, Nvidia, then by ramping up enforcement of the tech laws Trump loathes.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;According to Ryan, “Dutch company ASML commands a global monopoly on the microchip-etching machines that use light to carve patterns on silicon,” and Nvidia needs those machines if it wants to remain the world’s most valuable company. Should the US GDP remain reliant on AI investment for growth, von der Leyen could use export curbs on that technology like a “lever,” Ryan said, controlling “whether and by how much the US economy expands or contracts.”&lt;/p&gt;
&lt;p&gt;Withholding those machines “would be difficult for Europe” and “extremely painful for the Dutch economy,” Ryan noted, but “it would be far more painful for Trump.”&lt;/p&gt;
&lt;p&gt;Another step the EU could take is even “easier,” Ryan suggested. It could go even harder on the enforcement of tech regulations based on evidence of mismanaged data surfaced in lawsuits against giants like Google and Meta. For example, it seems clear that Meta may have violated the EU’s General Data Protection Regulation (GDPR), after the Facebook owner was “unable to tell a US court that what its internal systems do with your data, or who can access it, or for what purpose.”&lt;/p&gt;
&lt;p&gt;“This data free-for-all lets big tech companies train their AI models on masses of everyone’s data, but it is illegal in Europe, where companies are required to carefully control and account for how they use personal data,” Ryan wrote. “All Brussels has to do is crack down on Ireland, which for years has been a wild west of lax data enforcement, and the repercussions will be felt far beyond.”&lt;/p&gt;
&lt;p&gt;Taking that step would also arguably make it harder for tech companies to secure AI investments, since firms would have to disclose that their “AI tools are barred from accessing Europe’s valuable markets,” Ryan said.&lt;/p&gt;
&lt;p&gt;Calling the reaction to the X fine “extreme,” Ryan pushed for von der Leyen to advance on both fronts, forecasting that “the AI bubble would be unlikely to survive this double shock” and likely neither could Trump’s approval ratings. There’s also a possibility that tech firms could pressure Trump to back down if coping with any increased enforcement threatens AI progress.&lt;/p&gt;
&lt;p&gt;Although Wu suggested that Big Tech firms like Google and Meta would likely be “insulated” from the AI bubble bursting, Google CEO Sundar Pichai doesn’t seem so sure. In November, Pichai told the BBC that if AI investments didn’t pay off quickly enough, he thinks “no company is going to be immune, including us.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/12/us-threatens-crackdown-on-eu-firms-as-clash-over-tech-regulations-intensifies/</guid><pubDate>Wed, 17 Dec 2025 18:41:25 +0000</pubDate></item><item><title>[NEW] A “scientific sandbox” lets researchers explore the evolution of vision systems (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/scientific-sandbox-lets-researchers-explore-evolution-vision-systems-1217</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/MIT_Eye-Evolution-01.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Why did humans evolve the eyes we have today?&lt;/p&gt;&lt;p&gt;While scientists can’t go back in time to study the environmental pressures that shaped the evolution of the diverse vision systems that exist in nature, a new computational framework developed by MIT researchers allows them to explore this evolution in artificial intelligence agents.&lt;/p&gt;&lt;p&gt;The framework they developed, in which embodied AI agents evolve eyes and learn to see over many generations, is like a “scientific sandbox” that allows researchers to recreate different evolutionary trees. The user does this by changing the structure of the world and the tasks AI agents complete, such as finding food or telling objects apart.&lt;/p&gt;&lt;p&gt;This allows them to study why one animal may have evolved simple, light-sensitive patches as eyes, while another has complex, camera-type eyes.&lt;/p&gt;&lt;p&gt;The researchers’ experiments with this framework showcase how tasks drove eye evolution in the agents. For instance, they found that navigation tasks often led to the evolution of compound eyes with many individual units, like the eyes of insects and crustaceans.&lt;/p&gt;&lt;p&gt;On the other hand, if agents focused on object discrimination, they were more likely to evolve camera-type eyes with irises and retinas.&lt;/p&gt;&lt;p&gt;This framework could enable scientists to probe “what-if” questions about vision systems that are difficult to study experimentally. It could also guide the design of novel sensors and cameras for robots, drones, and wearable devices that balance performance with real-world constraints like energy efficiency and manufacturability.&lt;/p&gt;&lt;p&gt;“While we can never go back and figure out every detail of how evolution took place, in this work we’ve created an environment where we can, in a sense, recreate evolution and probe the environment in all these different ways. This method of doing science opens to the door to a lot of possibilities,” says Kushagra Tiwary, a graduate student at the MIT Media Lab and co-lead author of a paper on this research.&lt;/p&gt;&lt;p&gt;He is joined on the paper by co-lead author and fellow graduate student Aaron Young; graduate student Tzofi Klinghoffer; former postdoc Akshat Dave, who is now an assistant professor at Stony Brook University; Tomaso Poggio, the Eugene McDermott Professor in the Department of Brain and Cognitive Sciences, an investigator in the McGovern Institute, and co-director of the Center for Brains, Minds, and Machines; co-senior authors Brian Cheung, a postdoc in the&amp;nbsp; Center for Brains, Minds, and Machines and an incoming assistant professor at the University of California San Francisco; and Ramesh Raskar, associate professor of media arts and sciences and leader of the Camera Culture Group at MIT; as well as others at Rice University and Lund University. The research appears today in &lt;em&gt;Science Advances&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Building a scientific sandbox&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The paper began as a conversation among the researchers about discovering new vision systems that could be useful in different fields, like robotics. To test their “what-if” questions, the researchers decided to use AI to explore the many evolutionary possibilities.&lt;/p&gt;&lt;p&gt;“What-if questions inspired me when I was growing up to study science. With AI, we have a unique opportunity to create these embodied agents that allow us to ask the kinds of questions that would usually be impossible to answer,” Tiwary says.&lt;/p&gt;&lt;p&gt;To build this evolutionary sandbox, the researchers took all the elements of a camera, like the sensors, lenses, apertures, and processors, and converted them into parameters that an embodied AI agent could learn.&lt;/p&gt;&lt;p&gt;They used those building blocks as the starting point for an algorithmic learning mechanism an agent would use as it evolved eyes over time.&lt;/p&gt;&lt;p&gt;“We couldn’t simulate the entire universe atom-by-atom. It was challenging to determine which ingredients we needed, which ingredients we didn’t need, and how to allocate resources over those different elements,” Cheung says.&lt;/p&gt;&lt;p&gt;In their framework, this evolutionary algorithm can choose which elements to evolve based on the constraints of the environment and the task of the agent.&lt;/p&gt;&lt;p&gt;Each environment has a single task, such as navigation, food identification, or prey tracking, designed to mimic real visual tasks animals must overcome to survive. The agents start with a single photoreceptor that looks out at the world and an associated neural network model that processes visual information.&lt;/p&gt;&lt;p&gt;Then, over each agent’s lifetime, it is trained using reinforcement learning, a trial-and-error technique where the agent is rewarded for accomplishing the goal of its task. The environment also incorporates constraints, like a certain number of pixels for an agent’s visual sensors.&lt;/p&gt;&lt;p&gt;“These constraints drive the design process, the same way we have physical constraints in our world, like the physics of light, that have driven the design of our own eyes,” Tiwary says.&lt;/p&gt;&lt;p&gt;Over many generations, agents evolve different elements of vision systems that maximize rewards.&lt;/p&gt;&lt;p&gt;Their framework uses a genetic encoding mechanism to computationally mimic evolution, where individual genes mutate to control an agent’s development.&lt;/p&gt;&lt;p&gt;For instance, morphological genes capture how the agent views the environment and control eye placement; optical genes determine how the eye interacts with light and dictate the number of photoreceptors; and neural genes control the learning capacity of the agents.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Testing hypotheses&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;When the researchers set up experiments in this framework, they found that tasks had a major influence on the vision systems the agents evolved.&lt;/p&gt;&lt;p&gt;For instance, agents that were focused on navigation tasks developed eyes designed to maximize spatial awareness through low-resolution sensing, while agents tasked with detecting objects developed eyes focused more on frontal acuity, rather than peripheral vision.&lt;/p&gt;&lt;p&gt;Another experiment indicated that a bigger brain isn’t always better when it comes to processing visual information. Only so much visual information can go into the system at a time, based on physical constraints like the number of photoreceptors in the eyes.&lt;/p&gt;&lt;p&gt;“At some point a bigger brain doesn’t help the agents at all, and in nature that would be a waste of resources,” Cheung says.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to use this simulator to explore the best vision systems for specific applications, which could help scientists develop task-specific sensors and cameras. They also want to integrate LLMs into their framework to make it easier for users to ask “what-if” questions and study additional possibilities.&lt;/p&gt;&lt;p&gt;“There’s a real benefit that comes from asking questions in a more imaginative way. I hope this inspires others to create larger frameworks, where instead of focusing on narrow questions that cover a specific area, they are looking to answer questions with a much wider scope,” Cheung says.&lt;/p&gt;&lt;p&gt;This work was supported, in part, by the Center for Brains, Minds, and Machines and&amp;nbsp;the Defense Advanced Research Projects Agency (DARPA) Mathematics for the Discovery of Algorithms and Architectures (DIAL) program.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/MIT_Eye-Evolution-01.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Why did humans evolve the eyes we have today?&lt;/p&gt;&lt;p&gt;While scientists can’t go back in time to study the environmental pressures that shaped the evolution of the diverse vision systems that exist in nature, a new computational framework developed by MIT researchers allows them to explore this evolution in artificial intelligence agents.&lt;/p&gt;&lt;p&gt;The framework they developed, in which embodied AI agents evolve eyes and learn to see over many generations, is like a “scientific sandbox” that allows researchers to recreate different evolutionary trees. The user does this by changing the structure of the world and the tasks AI agents complete, such as finding food or telling objects apart.&lt;/p&gt;&lt;p&gt;This allows them to study why one animal may have evolved simple, light-sensitive patches as eyes, while another has complex, camera-type eyes.&lt;/p&gt;&lt;p&gt;The researchers’ experiments with this framework showcase how tasks drove eye evolution in the agents. For instance, they found that navigation tasks often led to the evolution of compound eyes with many individual units, like the eyes of insects and crustaceans.&lt;/p&gt;&lt;p&gt;On the other hand, if agents focused on object discrimination, they were more likely to evolve camera-type eyes with irises and retinas.&lt;/p&gt;&lt;p&gt;This framework could enable scientists to probe “what-if” questions about vision systems that are difficult to study experimentally. It could also guide the design of novel sensors and cameras for robots, drones, and wearable devices that balance performance with real-world constraints like energy efficiency and manufacturability.&lt;/p&gt;&lt;p&gt;“While we can never go back and figure out every detail of how evolution took place, in this work we’ve created an environment where we can, in a sense, recreate evolution and probe the environment in all these different ways. This method of doing science opens to the door to a lot of possibilities,” says Kushagra Tiwary, a graduate student at the MIT Media Lab and co-lead author of a paper on this research.&lt;/p&gt;&lt;p&gt;He is joined on the paper by co-lead author and fellow graduate student Aaron Young; graduate student Tzofi Klinghoffer; former postdoc Akshat Dave, who is now an assistant professor at Stony Brook University; Tomaso Poggio, the Eugene McDermott Professor in the Department of Brain and Cognitive Sciences, an investigator in the McGovern Institute, and co-director of the Center for Brains, Minds, and Machines; co-senior authors Brian Cheung, a postdoc in the&amp;nbsp; Center for Brains, Minds, and Machines and an incoming assistant professor at the University of California San Francisco; and Ramesh Raskar, associate professor of media arts and sciences and leader of the Camera Culture Group at MIT; as well as others at Rice University and Lund University. The research appears today in &lt;em&gt;Science Advances&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Building a scientific sandbox&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The paper began as a conversation among the researchers about discovering new vision systems that could be useful in different fields, like robotics. To test their “what-if” questions, the researchers decided to use AI to explore the many evolutionary possibilities.&lt;/p&gt;&lt;p&gt;“What-if questions inspired me when I was growing up to study science. With AI, we have a unique opportunity to create these embodied agents that allow us to ask the kinds of questions that would usually be impossible to answer,” Tiwary says.&lt;/p&gt;&lt;p&gt;To build this evolutionary sandbox, the researchers took all the elements of a camera, like the sensors, lenses, apertures, and processors, and converted them into parameters that an embodied AI agent could learn.&lt;/p&gt;&lt;p&gt;They used those building blocks as the starting point for an algorithmic learning mechanism an agent would use as it evolved eyes over time.&lt;/p&gt;&lt;p&gt;“We couldn’t simulate the entire universe atom-by-atom. It was challenging to determine which ingredients we needed, which ingredients we didn’t need, and how to allocate resources over those different elements,” Cheung says.&lt;/p&gt;&lt;p&gt;In their framework, this evolutionary algorithm can choose which elements to evolve based on the constraints of the environment and the task of the agent.&lt;/p&gt;&lt;p&gt;Each environment has a single task, such as navigation, food identification, or prey tracking, designed to mimic real visual tasks animals must overcome to survive. The agents start with a single photoreceptor that looks out at the world and an associated neural network model that processes visual information.&lt;/p&gt;&lt;p&gt;Then, over each agent’s lifetime, it is trained using reinforcement learning, a trial-and-error technique where the agent is rewarded for accomplishing the goal of its task. The environment also incorporates constraints, like a certain number of pixels for an agent’s visual sensors.&lt;/p&gt;&lt;p&gt;“These constraints drive the design process, the same way we have physical constraints in our world, like the physics of light, that have driven the design of our own eyes,” Tiwary says.&lt;/p&gt;&lt;p&gt;Over many generations, agents evolve different elements of vision systems that maximize rewards.&lt;/p&gt;&lt;p&gt;Their framework uses a genetic encoding mechanism to computationally mimic evolution, where individual genes mutate to control an agent’s development.&lt;/p&gt;&lt;p&gt;For instance, morphological genes capture how the agent views the environment and control eye placement; optical genes determine how the eye interacts with light and dictate the number of photoreceptors; and neural genes control the learning capacity of the agents.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Testing hypotheses&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;When the researchers set up experiments in this framework, they found that tasks had a major influence on the vision systems the agents evolved.&lt;/p&gt;&lt;p&gt;For instance, agents that were focused on navigation tasks developed eyes designed to maximize spatial awareness through low-resolution sensing, while agents tasked with detecting objects developed eyes focused more on frontal acuity, rather than peripheral vision.&lt;/p&gt;&lt;p&gt;Another experiment indicated that a bigger brain isn’t always better when it comes to processing visual information. Only so much visual information can go into the system at a time, based on physical constraints like the number of photoreceptors in the eyes.&lt;/p&gt;&lt;p&gt;“At some point a bigger brain doesn’t help the agents at all, and in nature that would be a waste of resources,” Cheung says.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to use this simulator to explore the best vision systems for specific applications, which could help scientists develop task-specific sensors and cameras. They also want to integrate LLMs into their framework to make it easier for users to ask “what-if” questions and study additional possibilities.&lt;/p&gt;&lt;p&gt;“There’s a real benefit that comes from asking questions in a more imaginative way. I hope this inspires others to create larger frameworks, where instead of focusing on narrow questions that cover a specific area, they are looking to answer questions with a much wider scope,” Cheung says.&lt;/p&gt;&lt;p&gt;This work was supported, in part, by the Center for Brains, Minds, and Machines and&amp;nbsp;the Defense Advanced Research Projects Agency (DARPA) Mathematics for the Discovery of Algorithms and Architectures (DIAL) program.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/scientific-sandbox-lets-researchers-explore-evolution-vision-systems-1217</guid><pubDate>Wed, 17 Dec 2025 19:00:00 +0000</pubDate></item><item><title>[NEW] Gemini 3 Flash arrives with reduced costs and latency — a powerful combo for enterprises (AI | VentureBeat)</title><link>https://venturebeat.com/technology/gemini-3-flash-arrives-with-reduced-costs-and-latency-a-powerful-combo-for</link><description>[unable to retrieve full-text content]&lt;p&gt;Enterprises can now harness the power of a large language model that&amp;#x27;s near that of the state-of-the-art&lt;a href="https://venturebeat.com/ai/google-unveils-gemini-3-claiming-the-lead-in-math-science-multimodal-and"&gt; Google’s Gemini 3 Pro&lt;/a&gt;, but at a fraction of the cost and with increased speed, thanks to the &lt;a href="https://blog.google/products/gemini/gemini-3-flash/"&gt;newly released Gemini 3 Flash&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The model joins the flagship Gemini 3 Pro, Gemini 3 Deep Think, and Gemini Agent, all of which were announced and released last month.&lt;/p&gt;&lt;p&gt;Gemini 3 Flash, now available on Gemini Enterprise, Google Antigravity, Gemini CLI, AI Studio, and on preview in Vertex AI, processes information in near real-time and helps build quick, responsive agentic applications. &lt;/p&gt;&lt;p&gt;The company &lt;a href="https://cloud.google.com/blog/products/ai-machine-learning/gemini-3-flash-for-enterprises"&gt;said in a blog post&lt;/a&gt; that Gemini 3 Flash “builds on the model series that developers and enterprises already love, optimized for high-frequency workflows that demand speed, without sacrificing quality.&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;The model is also the default for AI Mode on Google Search and the Gemini application. &lt;/p&gt;&lt;p&gt;Tulsee Doshi, senior director, product management on the Gemini team, said in a &lt;a href="https://blog.google/products/gemini/gemini-3-flash/"&gt;separate blog post&lt;/a&gt; that the model “demonstrates that speed and scale don’t have to come at the cost of intelligence.”&lt;/p&gt;&lt;p&gt;“Gemini 3 Flash is made for iterative development, offering Gemini 3’s Pro-grade coding performance with low latency — it’s able to reason and solve tasks quickly in high-frequency workflows,” Doshi said. “It strikes an ideal balance for agentic coding, production-ready systems and responsive interactive applications.”&lt;/p&gt;&lt;p&gt;Early adoption by specialized firms proves the model&amp;#x27;s reliability in high-stakes fields. Harvey, an AI platform for law firms, reported a 7% jump in reasoning on their internal &amp;#x27;BigLaw Bench,&amp;#x27; while Resemble AI discovered that Gemini 3 Flash could process complex forensic data for deepfake detection 4x faster than Gemini 2.5 Pro. These aren&amp;#x27;t just speed gains; they are enabling &amp;#x27;near real-time&amp;#x27; workflows that were previously impossible.&lt;/p&gt;&lt;h2&gt;More efficient at a lower cost&lt;/h2&gt;&lt;p&gt;Enterprise AI builders have become more aware of &lt;a href="https://venturebeat.com/ai/ais-financial-blind-spot-why-long-term-success-depends-on-cost-transparency"&gt;the cost of running AI models&lt;/a&gt;, especially as they try to convince stakeholders to put more budget into agentic workflows that run on expensive models. Organizations have turned to &lt;a href="https://venturebeat.com/ai/model-minimalism-the-new-ai-strategy-saving-companies-millions"&gt;smaller or distilled models&lt;/a&gt;, &lt;a href="https://venturebeat.com/ai/that-cheap-open-source-ai-model-is-actually-burning-through-your-compute-budget"&gt;focusing on open models&lt;/a&gt; or other &lt;a href="https://venturebeat.com/ai/googles-new-framework-helps-ai-agents-spend-their-compute-and-tool-budget"&gt;research and prompting techniques&lt;/a&gt; to help manage bloated AI costs.&lt;/p&gt;&lt;p&gt;For enterprises, the biggest value proposition for Gemini 3 Flash is that it offers the same level of advanced multimodal capabilities, such as complex video analysis and data extraction, as its larger Gemini counterparts, but is far faster and cheaper. &lt;/p&gt;&lt;p&gt;While Google’s internal materials highlight a 3x speed increase over the 2.5 Pro series, data from independent &lt;a href="https://x.com/ArtificialAnlys/status/2001335953290670301"&gt;benchmarking firm Artificial Analysis&lt;/a&gt; adds a layer of crucial nuance. &lt;/p&gt;&lt;p&gt;In the latter organization&amp;#x27;s pre-release testing, Gemini 3 Flash Preview recorded a raw throughput of 218 output tokens per second. This makes it 22% slower than the previous &amp;#x27;non-reasoning&amp;#x27; Gemini 2.5 Flash, but it is still significantly faster than rivals including OpenAI&amp;#x27;s GPT-5.1 high (125 t/s) and DeepSeek V3.2 reasoning (30 t/s).&lt;/p&gt;&lt;p&gt;Most notably, Artificial Analysis crowned Gemini 3 Flash as the new leader in their AA-Omniscience knowledge benchmark, where it achieved the highest knowledge accuracy of any model tested to date. However, this intelligence comes with a &amp;#x27;reasoning tax&amp;#x27;: the model more than doubles its token usage compared to the 2.5 Flash series when tackling complex indexes. &lt;/p&gt;&lt;p&gt;This high token density is offset by Google&amp;#x27;s aggressive pricing: when accessing through the Gemini API, Gemini 3 Flash costs $0.50 per 1 million input tokens, compared to $1.25/1M input tokens for Gemini 2.5 Pro, and $3/1M output tokens, compared to $ 10/1 M output tokens for Gemini 2.5 Pro. This allows Gemini 3 Flash to claim the title of the most cost-efficient model for its intelligence tier, despite being one of the most &amp;#x27;talkative&amp;#x27; models in terms of raw token volume. Here&amp;#x27;s how it stacks up to rival LLM offerings:&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Model&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;Input (/1M)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;Output (/1M)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;Total Cost&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;Source&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen 3 Turbo&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.05&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4.1 Fast (reasoning)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4.1 Fast (non-reasoning)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;deepseek-chat (V3.2-Exp)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.28&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.42&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;DeepSeek&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;deepseek-reasoner (V3.2-Exp)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.28&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.42&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;DeepSeek&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen 3 Plus&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.60&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 5.0&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.85&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$4.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Blfmc9do4"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Gemini 3 Flash Preview&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$0.50&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$3.00&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$3.50&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;&lt;b&gt;Google&lt;/b&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Haiku 4.5&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$5.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$6.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.anthropic.com/claude/docs"&gt;Anthropic&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen-Max&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.60&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$6.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$8.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 3 Pro (≤200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$2.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$12.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$14.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GPT-5.2&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.75&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$14.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.75&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://openai.com/api/pricing/"&gt;OpenAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Sonnet 4.5&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$18.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.anthropic.com/claude/docs"&gt;Anthropic&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 3 Pro (&amp;gt;200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$4.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$18.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$22.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Opus 4.5&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$5.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$25.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$30.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.anthropic.com/claude/docs"&gt;Anthropic&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GPT-5.2 Pro&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$21.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$168.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$189.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://openai.com/api/pricing/"&gt;OpenAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;More ways to save&lt;/h2&gt;&lt;p&gt;But enterprise developers and users can cut costs further by eliminating the lag most larger models often have, which racks up token usage. Google said the model “is able to modulate how much it thinks,” so that it uses more thinking and therefore more tokens for more complex tasks than for quick prompts. The company noted Gemini 3 Flash uses 30% fewer tokens than Gemini 2.5 Pro. &lt;/p&gt;&lt;p&gt;To balance this new reasoning power with strict corporate latency requirements, Google has introduced a &amp;#x27;Thinking Level&amp;#x27; parameter. Developers can toggle between &amp;#x27;Low&amp;#x27;—to minimize cost and latency for simple chat tasks—and &amp;#x27;High&amp;#x27;—to maximize reasoning depth for complex data extraction. This granular control allows teams to build &amp;#x27;variable-speed&amp;#x27; applications that only consume expensive &amp;#x27;thinking tokens&amp;#x27; when a problem actually demands PhD-level lo&lt;/p&gt;&lt;p&gt;The economic story extends beyond simple token prices. With the standard inclusion of Context Caching, enterprises processing massive, static datasets—such as entire legal libraries or codebase repositories—can see a 90% reduction in costs for repeated queries. When combined with the Batch API’s 50% discount, the total cost of ownership for a Gemini-powered agent drops significantly below the threshold of competing frontier models&lt;/p&gt;&lt;p&gt;“Gemini 3 Flash delivers exceptional performance on coding and agentic tasks combined with a lower price point, allowing teams to deploy sophisticated reasoning costs across high-volume processes without hitting barriers,” Google said. &lt;/p&gt;&lt;p&gt;By offering a model that delivers strong multimodal performance at a more affordable price, Google is making the case that enterprises concerned with controlling their AI spend should choose its models, especially Gemini 3 Flash. &lt;/p&gt;&lt;h2&gt;Strong benchmark performance &lt;/h2&gt;&lt;p&gt;But how does Gemini 3 Flash stack up against other models in terms of its performance? &lt;/p&gt;&lt;p&gt;Doshi said the model achieved a score of 78% on the SWE-Bench Verified benchmark testing for coding agents, outperforming both the preceding Gemini 2.5 family and the newer Gemini 3 Pro itself!&lt;/p&gt;&lt;p&gt;For enterprises, this means high-volume software maintenance and bug-fixing tasks can now be offloaded to a model that is both faster and cheaper than previous flagship models, without a degradation in code quality.&lt;/p&gt;&lt;p&gt;The model also performed strongly on other benchmarks, scoring 81.2% on the MMMU Pro benchmark, comparable to Gemini 3 Pro. &lt;/p&gt;&lt;p&gt;While most Flash type models are explicitly optimized for short, quick tasks like generating code, Google claims Gemini 3 Flash’s performance “in reasoning, tool use and multimodal capabilities is ideal for developers looking to do more complex video analysis, data extraction and visual Q&amp;amp;A, which means it can enable more intelligent applications — like in-game assistants or A/B test experiments — that demand both quick answers and deep reasoning.”&lt;/p&gt;&lt;h2&gt;First impressions from early users&lt;/h2&gt;&lt;p&gt;So far, early users have been largely impressed with the model, particularly its benchmark performance. &lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;&lt;/div&gt;&lt;h3&gt;&lt;b&gt;What It Means for Enterprise AI Usage&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;With Gemini 3 Flash now serving as the default engine across Google Search and the Gemini app, we are witnessing the &amp;quot;Flash-ification&amp;quot; of frontier intelligence. By making Pro-level reasoning the new baseline, Google is setting a trap for slower incumbents. &lt;/p&gt;&lt;p&gt;The integration into platforms like Google Antigravity suggests that Google isn&amp;#x27;t just selling a model; it&amp;#x27;s selling the infrastructure for the autonomous enterprise. &lt;/p&gt;&lt;p&gt;As developers hit the ground running with 3x faster speeds and a 90% discount on context caching, the &amp;quot;Gemini-first&amp;quot; strategy becomes a compelling financial argument. In the high-velocity race for AI dominance, Gemini 3 Flash may be the model that finally turns &amp;quot;vibe coding&amp;quot; from an experimental hobby into a production-ready reality.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Enterprises can now harness the power of a large language model that&amp;#x27;s near that of the state-of-the-art&lt;a href="https://venturebeat.com/ai/google-unveils-gemini-3-claiming-the-lead-in-math-science-multimodal-and"&gt; Google’s Gemini 3 Pro&lt;/a&gt;, but at a fraction of the cost and with increased speed, thanks to the &lt;a href="https://blog.google/products/gemini/gemini-3-flash/"&gt;newly released Gemini 3 Flash&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The model joins the flagship Gemini 3 Pro, Gemini 3 Deep Think, and Gemini Agent, all of which were announced and released last month.&lt;/p&gt;&lt;p&gt;Gemini 3 Flash, now available on Gemini Enterprise, Google Antigravity, Gemini CLI, AI Studio, and on preview in Vertex AI, processes information in near real-time and helps build quick, responsive agentic applications. &lt;/p&gt;&lt;p&gt;The company &lt;a href="https://cloud.google.com/blog/products/ai-machine-learning/gemini-3-flash-for-enterprises"&gt;said in a blog post&lt;/a&gt; that Gemini 3 Flash “builds on the model series that developers and enterprises already love, optimized for high-frequency workflows that demand speed, without sacrificing quality.&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;The model is also the default for AI Mode on Google Search and the Gemini application. &lt;/p&gt;&lt;p&gt;Tulsee Doshi, senior director, product management on the Gemini team, said in a &lt;a href="https://blog.google/products/gemini/gemini-3-flash/"&gt;separate blog post&lt;/a&gt; that the model “demonstrates that speed and scale don’t have to come at the cost of intelligence.”&lt;/p&gt;&lt;p&gt;“Gemini 3 Flash is made for iterative development, offering Gemini 3’s Pro-grade coding performance with low latency — it’s able to reason and solve tasks quickly in high-frequency workflows,” Doshi said. “It strikes an ideal balance for agentic coding, production-ready systems and responsive interactive applications.”&lt;/p&gt;&lt;p&gt;Early adoption by specialized firms proves the model&amp;#x27;s reliability in high-stakes fields. Harvey, an AI platform for law firms, reported a 7% jump in reasoning on their internal &amp;#x27;BigLaw Bench,&amp;#x27; while Resemble AI discovered that Gemini 3 Flash could process complex forensic data for deepfake detection 4x faster than Gemini 2.5 Pro. These aren&amp;#x27;t just speed gains; they are enabling &amp;#x27;near real-time&amp;#x27; workflows that were previously impossible.&lt;/p&gt;&lt;h2&gt;More efficient at a lower cost&lt;/h2&gt;&lt;p&gt;Enterprise AI builders have become more aware of &lt;a href="https://venturebeat.com/ai/ais-financial-blind-spot-why-long-term-success-depends-on-cost-transparency"&gt;the cost of running AI models&lt;/a&gt;, especially as they try to convince stakeholders to put more budget into agentic workflows that run on expensive models. Organizations have turned to &lt;a href="https://venturebeat.com/ai/model-minimalism-the-new-ai-strategy-saving-companies-millions"&gt;smaller or distilled models&lt;/a&gt;, &lt;a href="https://venturebeat.com/ai/that-cheap-open-source-ai-model-is-actually-burning-through-your-compute-budget"&gt;focusing on open models&lt;/a&gt; or other &lt;a href="https://venturebeat.com/ai/googles-new-framework-helps-ai-agents-spend-their-compute-and-tool-budget"&gt;research and prompting techniques&lt;/a&gt; to help manage bloated AI costs.&lt;/p&gt;&lt;p&gt;For enterprises, the biggest value proposition for Gemini 3 Flash is that it offers the same level of advanced multimodal capabilities, such as complex video analysis and data extraction, as its larger Gemini counterparts, but is far faster and cheaper. &lt;/p&gt;&lt;p&gt;While Google’s internal materials highlight a 3x speed increase over the 2.5 Pro series, data from independent &lt;a href="https://x.com/ArtificialAnlys/status/2001335953290670301"&gt;benchmarking firm Artificial Analysis&lt;/a&gt; adds a layer of crucial nuance. &lt;/p&gt;&lt;p&gt;In the latter organization&amp;#x27;s pre-release testing, Gemini 3 Flash Preview recorded a raw throughput of 218 output tokens per second. This makes it 22% slower than the previous &amp;#x27;non-reasoning&amp;#x27; Gemini 2.5 Flash, but it is still significantly faster than rivals including OpenAI&amp;#x27;s GPT-5.1 high (125 t/s) and DeepSeek V3.2 reasoning (30 t/s).&lt;/p&gt;&lt;p&gt;Most notably, Artificial Analysis crowned Gemini 3 Flash as the new leader in their AA-Omniscience knowledge benchmark, where it achieved the highest knowledge accuracy of any model tested to date. However, this intelligence comes with a &amp;#x27;reasoning tax&amp;#x27;: the model more than doubles its token usage compared to the 2.5 Flash series when tackling complex indexes. &lt;/p&gt;&lt;p&gt;This high token density is offset by Google&amp;#x27;s aggressive pricing: when accessing through the Gemini API, Gemini 3 Flash costs $0.50 per 1 million input tokens, compared to $1.25/1M input tokens for Gemini 2.5 Pro, and $3/1M output tokens, compared to $ 10/1 M output tokens for Gemini 2.5 Pro. This allows Gemini 3 Flash to claim the title of the most cost-efficient model for its intelligence tier, despite being one of the most &amp;#x27;talkative&amp;#x27; models in terms of raw token volume. Here&amp;#x27;s how it stacks up to rival LLM offerings:&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Model&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;Input (/1M)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;Output (/1M)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;Total Cost&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;Source&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen 3 Turbo&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.05&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4.1 Fast (reasoning)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4.1 Fast (non-reasoning)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;deepseek-chat (V3.2-Exp)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.28&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.42&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;DeepSeek&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;deepseek-reasoner (V3.2-Exp)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.28&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.42&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;DeepSeek&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen 3 Plus&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.60&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 5.0&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.85&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$4.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Blfmc9do4"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Gemini 3 Flash Preview&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$0.50&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$3.00&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$3.50&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;&lt;b&gt;Google&lt;/b&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Haiku 4.5&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$5.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$6.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.anthropic.com/claude/docs"&gt;Anthropic&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen-Max&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.60&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$6.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$8.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 3 Pro (≤200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$2.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$12.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$14.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GPT-5.2&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.75&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$14.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.75&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://openai.com/api/pricing/"&gt;OpenAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Sonnet 4.5&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$18.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.anthropic.com/claude/docs"&gt;Anthropic&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 3 Pro (&amp;gt;200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$4.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$18.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$22.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Opus 4.5&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$5.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$25.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$30.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.anthropic.com/claude/docs"&gt;Anthropic&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GPT-5.2 Pro&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$21.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$168.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$189.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://openai.com/api/pricing/"&gt;OpenAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;More ways to save&lt;/h2&gt;&lt;p&gt;But enterprise developers and users can cut costs further by eliminating the lag most larger models often have, which racks up token usage. Google said the model “is able to modulate how much it thinks,” so that it uses more thinking and therefore more tokens for more complex tasks than for quick prompts. The company noted Gemini 3 Flash uses 30% fewer tokens than Gemini 2.5 Pro. &lt;/p&gt;&lt;p&gt;To balance this new reasoning power with strict corporate latency requirements, Google has introduced a &amp;#x27;Thinking Level&amp;#x27; parameter. Developers can toggle between &amp;#x27;Low&amp;#x27;—to minimize cost and latency for simple chat tasks—and &amp;#x27;High&amp;#x27;—to maximize reasoning depth for complex data extraction. This granular control allows teams to build &amp;#x27;variable-speed&amp;#x27; applications that only consume expensive &amp;#x27;thinking tokens&amp;#x27; when a problem actually demands PhD-level lo&lt;/p&gt;&lt;p&gt;The economic story extends beyond simple token prices. With the standard inclusion of Context Caching, enterprises processing massive, static datasets—such as entire legal libraries or codebase repositories—can see a 90% reduction in costs for repeated queries. When combined with the Batch API’s 50% discount, the total cost of ownership for a Gemini-powered agent drops significantly below the threshold of competing frontier models&lt;/p&gt;&lt;p&gt;“Gemini 3 Flash delivers exceptional performance on coding and agentic tasks combined with a lower price point, allowing teams to deploy sophisticated reasoning costs across high-volume processes without hitting barriers,” Google said. &lt;/p&gt;&lt;p&gt;By offering a model that delivers strong multimodal performance at a more affordable price, Google is making the case that enterprises concerned with controlling their AI spend should choose its models, especially Gemini 3 Flash. &lt;/p&gt;&lt;h2&gt;Strong benchmark performance &lt;/h2&gt;&lt;p&gt;But how does Gemini 3 Flash stack up against other models in terms of its performance? &lt;/p&gt;&lt;p&gt;Doshi said the model achieved a score of 78% on the SWE-Bench Verified benchmark testing for coding agents, outperforming both the preceding Gemini 2.5 family and the newer Gemini 3 Pro itself!&lt;/p&gt;&lt;p&gt;For enterprises, this means high-volume software maintenance and bug-fixing tasks can now be offloaded to a model that is both faster and cheaper than previous flagship models, without a degradation in code quality.&lt;/p&gt;&lt;p&gt;The model also performed strongly on other benchmarks, scoring 81.2% on the MMMU Pro benchmark, comparable to Gemini 3 Pro. &lt;/p&gt;&lt;p&gt;While most Flash type models are explicitly optimized for short, quick tasks like generating code, Google claims Gemini 3 Flash’s performance “in reasoning, tool use and multimodal capabilities is ideal for developers looking to do more complex video analysis, data extraction and visual Q&amp;amp;A, which means it can enable more intelligent applications — like in-game assistants or A/B test experiments — that demand both quick answers and deep reasoning.”&lt;/p&gt;&lt;h2&gt;First impressions from early users&lt;/h2&gt;&lt;p&gt;So far, early users have been largely impressed with the model, particularly its benchmark performance. &lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;&lt;/div&gt;&lt;h3&gt;&lt;b&gt;What It Means for Enterprise AI Usage&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;With Gemini 3 Flash now serving as the default engine across Google Search and the Gemini app, we are witnessing the &amp;quot;Flash-ification&amp;quot; of frontier intelligence. By making Pro-level reasoning the new baseline, Google is setting a trap for slower incumbents. &lt;/p&gt;&lt;p&gt;The integration into platforms like Google Antigravity suggests that Google isn&amp;#x27;t just selling a model; it&amp;#x27;s selling the infrastructure for the autonomous enterprise. &lt;/p&gt;&lt;p&gt;As developers hit the ground running with 3x faster speeds and a 90% discount on context caching, the &amp;quot;Gemini-first&amp;quot; strategy becomes a compelling financial argument. In the high-velocity race for AI dominance, Gemini 3 Flash may be the model that finally turns &amp;quot;vibe coding&amp;quot; from an experimental hobby into a production-ready reality.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/technology/gemini-3-flash-arrives-with-reduced-costs-and-latency-a-powerful-combo-for</guid><pubDate>Wed, 17 Dec 2025 19:24:00 +0000</pubDate></item><item><title>[NEW] OpenAI’s new ChatGPT image generator makes faking photos easy (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/12/openais-new-chatgpt-image-generator-makes-faking-photos-easy/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        New GPT Image 1.5 allows more detailed conversational image editing, for better or worse.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A GPT Image 1.5 generation created with the classic prompt, &amp;quot;a muscular barbarian with weapons beside a CRT television set, cinematic, 8K, studio lighting.&amp;quot;" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/barbarian_crt_1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="A GPT Image 1.5 generation created with the classic prompt, &amp;quot;a muscular barbarian with weapons beside a CRT television set, cinematic, 8K, studio lighting.&amp;quot;" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/barbarian_crt_1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A GPT Image 1.5 generation created with the classic prompt, "a muscular barbarian with weapons beside a CRT television set, cinematic, 8K, studio lighting."

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          OpenAI / ChatGPT

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;For most of photography’s roughly 200-year history, altering a photo convincingly required either a darkroom, some Photoshop expertise, or, at minimum, a steady hand with scissors and glue. On Tuesday, OpenAI released a tool that reduces the process to typing a sentence.&lt;/p&gt;
&lt;p&gt;It’s not the first company to do so. While OpenAI had a conversational image-editing model in the works since GPT-4o in 2024, Google beat OpenAI to market in March with a public prototype, then refined it to a popular model called Nano Banana image model (and Nano Banana Pro). The enthusiastic response to Google’s image-editing model in the AI community got OpenAI’s attention.&lt;/p&gt;
&lt;p&gt;OpenAI’s new GPT Image 1.5 is an AI image synthesis model that reportedly generates images up to four times faster than its predecessor and costs about 20 percent less through the API. The model rolled out to all ChatGPT users on Tuesday and represents another step toward making photorealistic image manipulation a casual process that requires no particular visual skills.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132464 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="The &amp;quot;Galactic Queen of the Universe&amp;quot; added to a photo of a room with a sofa using GPT Image 1.5 in ChatGPT." class="center large" height="791" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/queen_of_the_universe_on_a_sofa-1024x791.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The “Galactic Queen of the Universe” added to a photo of a room with a sofa using GPT Image 1.5 in ChatGPT.

          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;GPT Image 1.5 is notable because it’s a “native multimodal” image model, meaning image generation happens inside the same neural network that processes language prompts. (In contrast, DALL-E 3, an earlier OpenAI image generator previously built into ChatGPT, used a different technique called diffusion to generate images.)&lt;/p&gt;
&lt;p&gt;This newer type of model, which we covered in more detail in March, treats images and text as the same kind of thing: chunks of data called “tokens” to be predicted, patterns to be completed. If you upload a photo of your dad and type “put him in a tuxedo at a wedding,” the model processes your words and the image pixels in a unified space, then outputs new pixels the same way it would output the next word in a sentence.&lt;/p&gt;
&lt;p&gt;Using this technique, GPT Image 1.5 can more easily alter visual reality than earlier AI image models, changing someone’s pose or position, or rendering a scene from a slightly different angle, with varying degrees of success. It can also remove objects, change visual styles, adjust clothing, and refine specific areas while preserving facial likeness across successive edits. You can converse with the AI model about a photograph, refining and revising, the same way you might workshop a draft of an email in ChatGPT.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Fidji Simo, OpenAI’s CEO of applications, wrote in a blog post that ChatGPT’s chat interface was never designed for visual work. “Creating and editing images is a different kind of task and deserves a space built for visuals,” Simo wrote. To that end, OpenAI introduced a dedicated image creation space in ChatGPT’s sidebar with preset filters and trending prompts.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132473 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Harrelson Hall, a famous circular building on NCSU's campus (now demolished), always looked like it could take off. With GPT Image 1.5, it can." class="center large" height="812" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/harrelson_ufo-1024x812.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Harrelson Hall, a famous circular building on North Carolina State University’s campus (now demolished), always looked like it could take off. With GPT Image 1.5, it can.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards / ChatGPT

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The release’s timing seems like a direct response to Google’s technical gains in AI, including a massive growth in chatbot user base. In particular, Google’s Nano Banana image model (and Nano Banana Pro) became popular on social media after its August release, thanks to its ability to render text relatively clearly and preserve faces consistently across edits.&lt;/p&gt;
&lt;p&gt;OpenAI’s previous token-based image synthesis model could make some targeted edits based on conversational prompts, but it often changed facial details and other elements that users might have wanted to keep. GPT Image 1.5 appears designed to match the editing features that Google already shipped. But if you happen to prefer the older ChatGPT image generator, OpenAI says the previous version will remain available as a custom GPT (for now) for users who prefer it.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;The friction keeps dropping&lt;/h2&gt;
&lt;p&gt;GPT Image 1.5 is not perfect. In our brief testing, it didn’t always follow prompting directions very well. But when it does work, the results seem more convincing and detailed than OpenAI’s previous multimodal image model. For a more detailed comparison, a software consultant named Shaun Pedicini has put together an instructive site (“GenAI Image Editing Showdown”) that conducts A/B testing of various AI image models.&lt;/p&gt;
&lt;p&gt;And while we’ve written about this a lot over the past few years, it’s probably worth repeating that barriers to realistic photo editing and manipulation keep dropping. This kind of seamless, realistic, effortless AI image manipulation may prompt (pun intended) a cultural recalibration of what visual images mean to society. It can also feel a little scary, for someone who grew up in an earlier media era, to see yourself put into situations that didn’t really happen.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2132498 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="A photo of Benj Edwards holding a guitar, edited with GPT Image 1.5 to include a smiling man." class="center large" height="816" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/benj_man_2-1024x816.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A photo of Benj Edwards holding a guitar, edited with GPT Image 1.5 to include a smiling man.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards / OpenAI

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;For most of photography’s history, a convincing forgery required skill, time, and resources. Those barriers made fakery rare enough that we could treat many photographs as a reasonable proxy for truth, although they could be manipulated (and often were). That era has ended due to AI, but GPT Image 1.5 seems to remove yet more of the remaining friction.&lt;/p&gt;
&lt;p&gt;The capability to preserve facial likeness across edits has obvious utility for legitimate photo editing and equally obvious potential for misuse. Image generators have already been used to create non-consensual intimate imagery and impersonate real people.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132466 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="A close-up of the &amp;quot;Galactic Queen of the Universe&amp;quot; and a barbarian holding a CRT added to a photo of a room with a sofa using GPT Image 1.5 in ChatGPT." class="fullwidth full" height="533" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/chatgpt_barbarian_queen.jpg" width="800" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A close-up of the “Galactic Queen of the Universe” and a barbarian holding a CRT added to a photo of a room with a sofa using GPT Image 1.5 in ChatGPT.

          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;With those hazards in mind, OpenAI’s image generators have always included a filter that usually blocks sexual or violent outputs. But it’s still possible to create embarrassing images of people without their consent (even though it violates OpenAI’s terms of service) while avoiding those topics. The company says generated images include C2PA metadata identifying them as AI-created, though that data can be stripped by resaving the file.&lt;/p&gt;
&lt;p&gt;Speaking of fakes, text rendering has been a long-standing weakness in image generators that has slowly gotten better. By prompting some older image synthesis models to create a sign or poster with specific words, the results often come back garbled or misspelled.&lt;/p&gt;
&lt;p&gt;OpenAI says GPT Image 1.5 can handle denser and smaller text. The company’s blog post includes a demonstration where the model generated an image of a newspaper with a multi-paragraph article, complete with headlines, a byline, benchmark tables, and body text that remains legible at the paragraph level. Whether this holds up across varied prompts will require broader testing.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132441 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="An example of the new GPT Image 1.5 model rendering complex and dense text on a simulated newspaper." class="fullwidth full" height="1536" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/newspaper_gpt5.2.webp" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      An example of the new GPT Image 1.5 model rendering complex and dense text on a simulated newspaper.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          OpenAI

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;While the newspaper in the example looks fake now, it’s another step toward the potential erosion of the public’s perception of the pre-Internet historical record as image synthesis becomes more realistic.&lt;/p&gt;
&lt;p&gt;OpenAI acknowledged in its blog post that the new model still has problems, including limited support for certain drawing styles and mistakes when generating images that require scientific accuracy. But they think it will get better over time. “We believe we’re still at the beginning of what image generation can enable,” the company wrote. And if the past three years of progress in image synthesis are any indication, they may be correct.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        New GPT Image 1.5 allows more detailed conversational image editing, for better or worse.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A GPT Image 1.5 generation created with the classic prompt, &amp;quot;a muscular barbarian with weapons beside a CRT television set, cinematic, 8K, studio lighting.&amp;quot;" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/barbarian_crt_1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="A GPT Image 1.5 generation created with the classic prompt, &amp;quot;a muscular barbarian with weapons beside a CRT television set, cinematic, 8K, studio lighting.&amp;quot;" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/barbarian_crt_1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A GPT Image 1.5 generation created with the classic prompt, "a muscular barbarian with weapons beside a CRT television set, cinematic, 8K, studio lighting."

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          OpenAI / ChatGPT

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;For most of photography’s roughly 200-year history, altering a photo convincingly required either a darkroom, some Photoshop expertise, or, at minimum, a steady hand with scissors and glue. On Tuesday, OpenAI released a tool that reduces the process to typing a sentence.&lt;/p&gt;
&lt;p&gt;It’s not the first company to do so. While OpenAI had a conversational image-editing model in the works since GPT-4o in 2024, Google beat OpenAI to market in March with a public prototype, then refined it to a popular model called Nano Banana image model (and Nano Banana Pro). The enthusiastic response to Google’s image-editing model in the AI community got OpenAI’s attention.&lt;/p&gt;
&lt;p&gt;OpenAI’s new GPT Image 1.5 is an AI image synthesis model that reportedly generates images up to four times faster than its predecessor and costs about 20 percent less through the API. The model rolled out to all ChatGPT users on Tuesday and represents another step toward making photorealistic image manipulation a casual process that requires no particular visual skills.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132464 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="The &amp;quot;Galactic Queen of the Universe&amp;quot; added to a photo of a room with a sofa using GPT Image 1.5 in ChatGPT." class="center large" height="791" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/queen_of_the_universe_on_a_sofa-1024x791.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The “Galactic Queen of the Universe” added to a photo of a room with a sofa using GPT Image 1.5 in ChatGPT.

          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;GPT Image 1.5 is notable because it’s a “native multimodal” image model, meaning image generation happens inside the same neural network that processes language prompts. (In contrast, DALL-E 3, an earlier OpenAI image generator previously built into ChatGPT, used a different technique called diffusion to generate images.)&lt;/p&gt;
&lt;p&gt;This newer type of model, which we covered in more detail in March, treats images and text as the same kind of thing: chunks of data called “tokens” to be predicted, patterns to be completed. If you upload a photo of your dad and type “put him in a tuxedo at a wedding,” the model processes your words and the image pixels in a unified space, then outputs new pixels the same way it would output the next word in a sentence.&lt;/p&gt;
&lt;p&gt;Using this technique, GPT Image 1.5 can more easily alter visual reality than earlier AI image models, changing someone’s pose or position, or rendering a scene from a slightly different angle, with varying degrees of success. It can also remove objects, change visual styles, adjust clothing, and refine specific areas while preserving facial likeness across successive edits. You can converse with the AI model about a photograph, refining and revising, the same way you might workshop a draft of an email in ChatGPT.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Fidji Simo, OpenAI’s CEO of applications, wrote in a blog post that ChatGPT’s chat interface was never designed for visual work. “Creating and editing images is a different kind of task and deserves a space built for visuals,” Simo wrote. To that end, OpenAI introduced a dedicated image creation space in ChatGPT’s sidebar with preset filters and trending prompts.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132473 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Harrelson Hall, a famous circular building on NCSU's campus (now demolished), always looked like it could take off. With GPT Image 1.5, it can." class="center large" height="812" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/harrelson_ufo-1024x812.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Harrelson Hall, a famous circular building on North Carolina State University’s campus (now demolished), always looked like it could take off. With GPT Image 1.5, it can.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards / ChatGPT

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The release’s timing seems like a direct response to Google’s technical gains in AI, including a massive growth in chatbot user base. In particular, Google’s Nano Banana image model (and Nano Banana Pro) became popular on social media after its August release, thanks to its ability to render text relatively clearly and preserve faces consistently across edits.&lt;/p&gt;
&lt;p&gt;OpenAI’s previous token-based image synthesis model could make some targeted edits based on conversational prompts, but it often changed facial details and other elements that users might have wanted to keep. GPT Image 1.5 appears designed to match the editing features that Google already shipped. But if you happen to prefer the older ChatGPT image generator, OpenAI says the previous version will remain available as a custom GPT (for now) for users who prefer it.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;The friction keeps dropping&lt;/h2&gt;
&lt;p&gt;GPT Image 1.5 is not perfect. In our brief testing, it didn’t always follow prompting directions very well. But when it does work, the results seem more convincing and detailed than OpenAI’s previous multimodal image model. For a more detailed comparison, a software consultant named Shaun Pedicini has put together an instructive site (“GenAI Image Editing Showdown”) that conducts A/B testing of various AI image models.&lt;/p&gt;
&lt;p&gt;And while we’ve written about this a lot over the past few years, it’s probably worth repeating that barriers to realistic photo editing and manipulation keep dropping. This kind of seamless, realistic, effortless AI image manipulation may prompt (pun intended) a cultural recalibration of what visual images mean to society. It can also feel a little scary, for someone who grew up in an earlier media era, to see yourself put into situations that didn’t really happen.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2132498 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="A photo of Benj Edwards holding a guitar, edited with GPT Image 1.5 to include a smiling man." class="center large" height="816" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/benj_man_2-1024x816.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A photo of Benj Edwards holding a guitar, edited with GPT Image 1.5 to include a smiling man.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards / OpenAI

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;For most of photography’s history, a convincing forgery required skill, time, and resources. Those barriers made fakery rare enough that we could treat many photographs as a reasonable proxy for truth, although they could be manipulated (and often were). That era has ended due to AI, but GPT Image 1.5 seems to remove yet more of the remaining friction.&lt;/p&gt;
&lt;p&gt;The capability to preserve facial likeness across edits has obvious utility for legitimate photo editing and equally obvious potential for misuse. Image generators have already been used to create non-consensual intimate imagery and impersonate real people.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132466 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="A close-up of the &amp;quot;Galactic Queen of the Universe&amp;quot; and a barbarian holding a CRT added to a photo of a room with a sofa using GPT Image 1.5 in ChatGPT." class="fullwidth full" height="533" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/chatgpt_barbarian_queen.jpg" width="800" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A close-up of the “Galactic Queen of the Universe” and a barbarian holding a CRT added to a photo of a room with a sofa using GPT Image 1.5 in ChatGPT.

          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;With those hazards in mind, OpenAI’s image generators have always included a filter that usually blocks sexual or violent outputs. But it’s still possible to create embarrassing images of people without their consent (even though it violates OpenAI’s terms of service) while avoiding those topics. The company says generated images include C2PA metadata identifying them as AI-created, though that data can be stripped by resaving the file.&lt;/p&gt;
&lt;p&gt;Speaking of fakes, text rendering has been a long-standing weakness in image generators that has slowly gotten better. By prompting some older image synthesis models to create a sign or poster with specific words, the results often come back garbled or misspelled.&lt;/p&gt;
&lt;p&gt;OpenAI says GPT Image 1.5 can handle denser and smaller text. The company’s blog post includes a demonstration where the model generated an image of a newspaper with a multi-paragraph article, complete with headlines, a byline, benchmark tables, and body text that remains legible at the paragraph level. Whether this holds up across varied prompts will require broader testing.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132441 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="An example of the new GPT Image 1.5 model rendering complex and dense text on a simulated newspaper." class="fullwidth full" height="1536" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/newspaper_gpt5.2.webp" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      An example of the new GPT Image 1.5 model rendering complex and dense text on a simulated newspaper.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          OpenAI

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;While the newspaper in the example looks fake now, it’s another step toward the potential erosion of the public’s perception of the pre-Internet historical record as image synthesis becomes more realistic.&lt;/p&gt;
&lt;p&gt;OpenAI acknowledged in its blog post that the new model still has problems, including limited support for certain drawing styles and mistakes when generating images that require scientific accuracy. But they think it will get better over time. “We believe we’re still at the beginning of what image generation can enable,” the company wrote. And if the past three years of progress in image synthesis are any indication, they may be correct.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/12/openais-new-chatgpt-image-generator-makes-faking-photos-easy/</guid><pubDate>Wed, 17 Dec 2025 22:22:33 +0000</pubDate></item><item><title>[NEW] Adobe hit with proposed class-action, accused of misusing authors’ work in AI training (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/17/adobe-hit-with-proposed-class-action-accused-of-misusing-authors-work-in-ai-training/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/09/GettyImages-2162453288.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Like pretty much every other tech company in existence, Adobe has leaned heavily into AI over the past several years. The software firm has launched a number of different AI services since 2023, including Firefly — its AI-powered media-generation suite. Now, however, the company’s full-throated embrace of the technology may have led to trouble, as a new lawsuit claims it used pirated books to train one of its AI models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A proposed class-action lawsuit filed on behalf of Elizabeth Lyon, an author from Oregon, claims that Adobe used pirated versions of numerous books — including her own — to train the company’s SlimLM program.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Adobe describes SlimLM as a small language model series that can be “optimized for document assistance tasks on mobile devices.” It states that SlimLM was pre-trained on SlimPajama-627B, a “deduplicated, multi-corpora, open-source dataset” released by Cerebras in June of 2023. Lyon, who has written a number of guidebooks for non-fiction writing, says that some of her works were included in a pretraining dataset that Adobe had used.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lyon’s lawsuit, which was originally reported on by Reuters, says that her writing was included in a processed subset of a manipulated dataset that was the basis of Adobe’s program: “The SlimPajama dataset was created by copying and manipulating the RedPajama dataset (including copying Books3),” the lawsuit says. “Thus, because it is a derivative copy of the RedPajama dataset, SlimPajama contains the Books3 dataset, including the copyrighted works of Plaintiff and the Class members.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Books3” — a huge collection of 191,000 books that have been used to train GenAI systems — has been an ongoing source of legal trouble for the tech community. RedPajama has also been cited in a number of litigation cases. In September, a lawsuit against Apple claimed the company had used copyrighted material to train its Apple Intelligence model. The litigation mentioned the dataset and accused the tech company of copying protected works “without consent and without credit or compensation.” In October, a similar lawsuit against Salesforce also claimed the company had used RedPajama for training purposes.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unfortunately for the tech industry, such lawsuits have, by now, become somewhat commonplace. AI algorithms are trained on massive datasets and, in some cases, those datasets have allegedly included pirated materials. In September, Anthropic agreed to pay $1.5 billion to a number of authors who had sued it and accused it of using pirated versions of their work to train its chatbot, Claude. The case was considered a potential turning point in the ongoing legal battles over copyrighted material in AI training data, of which there are many.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/09/GettyImages-2162453288.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Like pretty much every other tech company in existence, Adobe has leaned heavily into AI over the past several years. The software firm has launched a number of different AI services since 2023, including Firefly — its AI-powered media-generation suite. Now, however, the company’s full-throated embrace of the technology may have led to trouble, as a new lawsuit claims it used pirated books to train one of its AI models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A proposed class-action lawsuit filed on behalf of Elizabeth Lyon, an author from Oregon, claims that Adobe used pirated versions of numerous books — including her own — to train the company’s SlimLM program.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Adobe describes SlimLM as a small language model series that can be “optimized for document assistance tasks on mobile devices.” It states that SlimLM was pre-trained on SlimPajama-627B, a “deduplicated, multi-corpora, open-source dataset” released by Cerebras in June of 2023. Lyon, who has written a number of guidebooks for non-fiction writing, says that some of her works were included in a pretraining dataset that Adobe had used.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lyon’s lawsuit, which was originally reported on by Reuters, says that her writing was included in a processed subset of a manipulated dataset that was the basis of Adobe’s program: “The SlimPajama dataset was created by copying and manipulating the RedPajama dataset (including copying Books3),” the lawsuit says. “Thus, because it is a derivative copy of the RedPajama dataset, SlimPajama contains the Books3 dataset, including the copyrighted works of Plaintiff and the Class members.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Books3” — a huge collection of 191,000 books that have been used to train GenAI systems — has been an ongoing source of legal trouble for the tech community. RedPajama has also been cited in a number of litigation cases. In September, a lawsuit against Apple claimed the company had used copyrighted material to train its Apple Intelligence model. The litigation mentioned the dataset and accused the tech company of copying protected works “without consent and without credit or compensation.” In October, a similar lawsuit against Salesforce also claimed the company had used RedPajama for training purposes.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unfortunately for the tech industry, such lawsuits have, by now, become somewhat commonplace. AI algorithms are trained on massive datasets and, in some cases, those datasets have allegedly included pirated materials. In September, Anthropic agreed to pay $1.5 billion to a number of authors who had sued it and accused it of using pirated versions of their work to train its chatbot, Claude. The case was considered a potential turning point in the ongoing legal battles over copyrighted material in AI training data, of which there are many.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/17/adobe-hit-with-proposed-class-action-accused-of-misusing-authors-work-in-ai-training/</guid><pubDate>Thu, 18 Dec 2025 00:44:55 +0000</pubDate></item><item><title>A new way to increase the capabilities of large language models (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/new-way-to-increase-large-language-model-capabilities-1217</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/mit-watson-cats-in-a-box.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Most languages use word position and sentence structure to extract meaning. For example, “The cat sat on the box,” is not the same as “The box was on the cat.” Over a long text, like a financial document or a novel, the syntax of these words likely evolves.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Similarly, a person might be tracking variables in a piece of code or following instructions that have conditional actions. These are examples of state changes and sequential reasoning that we expect state-of-the-art artificial intelligence systems to excel at; however, the existing, cutting-edge attention mechanism within transformers — the primarily architecture used in large language models (LLMs) for determining the importance of words — has theoretical and empirical limitations when it comes to such capabilities.&lt;/p&gt;&lt;p&gt;An attention mechanism allows an LLM to look back at earlier parts of a query or document and, based on its training, determine which details and words matter most; however, this mechanism alone does not understand word order. It “sees” all of the input words, a.k.a. tokens, at the same time and handles them in the order that they’re presented, so researchers have developed techniques to encode position information. This is key for domains that are highly structured, like language. But the predominant position-encoding method, called rotary position encoding (RoPE), only takes into account the relative distance between tokens in a sequence and is independent of the input data. This means that, for example, words that are four positions apart, like “cat” and “box” in the example above, will all receive the same fixed mathematical rotation specific to that relative distance.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Now research led by MIT and the MIT-IBM Watson AI Lab has produced an encoding technique known as “PaTH Attention” that makes positional information adaptive and context-aware rather than static, as with RoPE.&lt;/p&gt;&lt;p&gt;“Transformers enable accurate and scalable modeling of many domains, but they have these limitations vis-a-vis state tracking, a class of phenomena that is thought to underlie important capabilities that we want in our AI systems. So, the important question is: How can we maintain the scalability and efficiency of transformers, while enabling state tracking?” says the paper’s senior author Yoon Kim, an associate professor in the Department of Electrical Engineering and Computer Science (EECS), a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL), and a researcher with the MIT-IBM Watson AI Lab.&lt;/p&gt;&lt;p&gt;A new paper on this work was presented earlier this month at the Conference on Neural Information Processing Systems (NeurIPS). Kim’s co-authors include lead author Songlin Yang, an EECS graduate student and former MIT-IBM Watson AI Lab Summer Program intern; Kaiyue Wen of Stanford University; Liliang Ren of Microsoft; and Yikang Shen, Shawn Tan, Mayank Mishra, and Rameswar Panda of IBM Research and the MIT-IBM Watson AI Lab.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Path to understanding&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Instead of assigning every word a fixed rotation based on relative distance between tokens, as RoPE does, PaTH Attention is flexible, treating the in-between words as a path made up of small, data-dependent transformations. Each transformation, based on a mathematical operation called a Householder reflection, acts like a tiny mirror that adjusts depending on the content of each token it passes. Each step in a sequence can influence how the model interprets information later on. The cumulative effect lets the system model how the meaning changes along the path between words, not just how far apart they are. This approach allows transformers to keep track of how entities and relationships change over time, giving it a sense of “positional memory.” Think of this as walking a path while experiencing your environment and how it affects you. Further, the team also developed a hardware-efficient algorithm to more efficiently compute attention scores between every pair of tokens so that the cumulative mathematical transformation from PaTH Attention is compressed and broken down into smaller computations so that it’s compatible with fast processing on GPUs.&lt;/p&gt;&lt;p&gt;The MIT-IBM researchers then explored PaTH Attention’s performance on synthetic and real-world tasks, including reasoning, long-context benchmarks, and full LLM training to see whether it improved a model’s ability to track information over time. The team tested its ability to follow the most recent “write” command despite many distracting steps and multi-step recall tests, tasks that are difficult for standard positional encoding methods like RoPE. The researchers also trained mid-size LLMs and compared them against other methods. PaTH Attention improved perplexity and outcompeted other methods on reasoning benchmarks it wasn’t trained on. They also evaluated retrieval, reasoning, and stability with inputs of tens of thousands of tokens. PaTH Attention consistently proved capable of content-awareness.&lt;/p&gt;&lt;p&gt;“We found that both on diagnostic tasks that are designed to test the limitations of transformers and on real-world language modeling tasks, our new approach was able to outperform existing attention mechanisms, while maintaining their efficiency,” says Kim. Further, “I’d be excited to see whether these types of data-dependent position encodings, like PATH, improve the performance of transformers on structured domains like biology, in [analyzing] proteins or DNA.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Thinking bigger and more efficiently&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The researchers then investigated how the PaTH Attention mechanism would perform if it more similarly mimicked human cognition, where we ignore old or less-relevant information when making decisions. To do this, they combined PaTH Attention with another position encoding scheme known as the Forgetting Transformer (FoX), which allows models to selectively “forget.” The resulting PaTH-FoX system adds a way to down-weight information in a data-dependent way, achieving strong results across reasoning, long-context understanding, and language modeling benchmarks. In this way, PaTH Attention extends the expressive power of transformer architectures.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kim says research like this is part of a broader effort to develop the “next big thing” in AI. He explains that a major driver of both the deep learning and generative AI revolutions has been the creation of “general-purpose building blocks that can be applied to wide domains,” such as “convolution layers, RNN [recurrent neural network] layers,” and, most recently, transformers. Looking ahead, Kim notes that considerations like accuracy, expressivity, flexibility, and hardware scalability have been and will be essential. As he puts it, “the core enterprise of modern architecture research is trying to come up with these new primitives that maintain or improve the expressivity, while also being scalable.”&lt;/p&gt;&lt;p&gt;This work was supported, in part, by the MIT-IBM Watson AI Lab and the AI2050 program at Schmidt Sciences.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/mit-watson-cats-in-a-box.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Most languages use word position and sentence structure to extract meaning. For example, “The cat sat on the box,” is not the same as “The box was on the cat.” Over a long text, like a financial document or a novel, the syntax of these words likely evolves.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Similarly, a person might be tracking variables in a piece of code or following instructions that have conditional actions. These are examples of state changes and sequential reasoning that we expect state-of-the-art artificial intelligence systems to excel at; however, the existing, cutting-edge attention mechanism within transformers — the primarily architecture used in large language models (LLMs) for determining the importance of words — has theoretical and empirical limitations when it comes to such capabilities.&lt;/p&gt;&lt;p&gt;An attention mechanism allows an LLM to look back at earlier parts of a query or document and, based on its training, determine which details and words matter most; however, this mechanism alone does not understand word order. It “sees” all of the input words, a.k.a. tokens, at the same time and handles them in the order that they’re presented, so researchers have developed techniques to encode position information. This is key for domains that are highly structured, like language. But the predominant position-encoding method, called rotary position encoding (RoPE), only takes into account the relative distance between tokens in a sequence and is independent of the input data. This means that, for example, words that are four positions apart, like “cat” and “box” in the example above, will all receive the same fixed mathematical rotation specific to that relative distance.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Now research led by MIT and the MIT-IBM Watson AI Lab has produced an encoding technique known as “PaTH Attention” that makes positional information adaptive and context-aware rather than static, as with RoPE.&lt;/p&gt;&lt;p&gt;“Transformers enable accurate and scalable modeling of many domains, but they have these limitations vis-a-vis state tracking, a class of phenomena that is thought to underlie important capabilities that we want in our AI systems. So, the important question is: How can we maintain the scalability and efficiency of transformers, while enabling state tracking?” says the paper’s senior author Yoon Kim, an associate professor in the Department of Electrical Engineering and Computer Science (EECS), a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL), and a researcher with the MIT-IBM Watson AI Lab.&lt;/p&gt;&lt;p&gt;A new paper on this work was presented earlier this month at the Conference on Neural Information Processing Systems (NeurIPS). Kim’s co-authors include lead author Songlin Yang, an EECS graduate student and former MIT-IBM Watson AI Lab Summer Program intern; Kaiyue Wen of Stanford University; Liliang Ren of Microsoft; and Yikang Shen, Shawn Tan, Mayank Mishra, and Rameswar Panda of IBM Research and the MIT-IBM Watson AI Lab.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Path to understanding&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Instead of assigning every word a fixed rotation based on relative distance between tokens, as RoPE does, PaTH Attention is flexible, treating the in-between words as a path made up of small, data-dependent transformations. Each transformation, based on a mathematical operation called a Householder reflection, acts like a tiny mirror that adjusts depending on the content of each token it passes. Each step in a sequence can influence how the model interprets information later on. The cumulative effect lets the system model how the meaning changes along the path between words, not just how far apart they are. This approach allows transformers to keep track of how entities and relationships change over time, giving it a sense of “positional memory.” Think of this as walking a path while experiencing your environment and how it affects you. Further, the team also developed a hardware-efficient algorithm to more efficiently compute attention scores between every pair of tokens so that the cumulative mathematical transformation from PaTH Attention is compressed and broken down into smaller computations so that it’s compatible with fast processing on GPUs.&lt;/p&gt;&lt;p&gt;The MIT-IBM researchers then explored PaTH Attention’s performance on synthetic and real-world tasks, including reasoning, long-context benchmarks, and full LLM training to see whether it improved a model’s ability to track information over time. The team tested its ability to follow the most recent “write” command despite many distracting steps and multi-step recall tests, tasks that are difficult for standard positional encoding methods like RoPE. The researchers also trained mid-size LLMs and compared them against other methods. PaTH Attention improved perplexity and outcompeted other methods on reasoning benchmarks it wasn’t trained on. They also evaluated retrieval, reasoning, and stability with inputs of tens of thousands of tokens. PaTH Attention consistently proved capable of content-awareness.&lt;/p&gt;&lt;p&gt;“We found that both on diagnostic tasks that are designed to test the limitations of transformers and on real-world language modeling tasks, our new approach was able to outperform existing attention mechanisms, while maintaining their efficiency,” says Kim. Further, “I’d be excited to see whether these types of data-dependent position encodings, like PATH, improve the performance of transformers on structured domains like biology, in [analyzing] proteins or DNA.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Thinking bigger and more efficiently&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The researchers then investigated how the PaTH Attention mechanism would perform if it more similarly mimicked human cognition, where we ignore old or less-relevant information when making decisions. To do this, they combined PaTH Attention with another position encoding scheme known as the Forgetting Transformer (FoX), which allows models to selectively “forget.” The resulting PaTH-FoX system adds a way to down-weight information in a data-dependent way, achieving strong results across reasoning, long-context understanding, and language modeling benchmarks. In this way, PaTH Attention extends the expressive power of transformer architectures.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kim says research like this is part of a broader effort to develop the “next big thing” in AI. He explains that a major driver of both the deep learning and generative AI revolutions has been the creation of “general-purpose building blocks that can be applied to wide domains,” such as “convolution layers, RNN [recurrent neural network] layers,” and, most recently, transformers. Looking ahead, Kim notes that considerations like accuracy, expressivity, flexibility, and hardware scalability have been and will be essential. As he puts it, “the core enterprise of modern architecture research is trying to come up with these new primitives that maintain or improve the expressivity, while also being scalable.”&lt;/p&gt;&lt;p&gt;This work was supported, in part, by the MIT-IBM Watson AI Lab and the AI2050 program at Schmidt Sciences.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/new-way-to-increase-large-language-model-capabilities-1217</guid><pubDate>Thu, 18 Dec 2025 04:10:00 +0000</pubDate></item></channel></rss>