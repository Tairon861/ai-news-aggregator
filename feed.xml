<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 04 Dec 2025 12:50:50 +0000</lastBuildDate><item><title>A smarter way for large language models to think about hard problems (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/smarter-way-large-language-models-think-about-hard-problems-1204</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/MIT-RewardModel-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;To make large language models (LLMs) more accurate when answering harder questions, researchers can let the model spend more time thinking about potential solutions.&lt;/p&gt;&lt;p&gt;But common&amp;nbsp;approaches that give LLMs this capability set a fixed computational budget for every problem, regardless of how complex it is. This means the LLM might waste computational resources on simpler questions or be unable to tackle intricate problems that require more reasoning.&lt;/p&gt;&lt;p&gt;To address this, MIT researchers developed a smarter way to allocate computational effort as the LLM solves a problem. Their method enables the model to dynamically adjust its computational budget based on the difficulty of the question and the likelihood that each partial solution will lead to the correct answer.&lt;/p&gt;&lt;p&gt;The researchers found that their new approach enabled LLMs to use as little as one-half the computation as existing methods, while&amp;nbsp;achieving&amp;nbsp;comparable accuracy on a range of questions with varying difficulties. In addition, their method allows smaller, less resource-intensive LLMs to perform as well as or even better than larger models on complex problems.&lt;/p&gt;&lt;p&gt;By improving the reliability and efficiency of LLMs, especially when they tackle complex reasoning tasks, this technique could reduce the energy consumption of generative AI systems and enable the use of LLMs in more high-stakes and time-sensitive applications.&lt;/p&gt;&lt;p&gt;“The computational cost of inference has quickly become a major bottleneck for frontier model providers, and they are actively trying to find ways to improve computational efficiency per&amp;nbsp;user queries. For instance, the recent GPT-5.1 release highlights the efficacy of the ‘adaptive reasoning’ approach our paper proposes. By endowing the models with the ability to know what they don’t know, we can enable them to spend more compute on the hardest problems and most promising solution paths, and use far fewer tokens on easy ones. That makes reasoning both more reliable and far more efficient,” says Navid Azizan, the Alfred H. and Jean M. Hayes Career Development Assistant Professor in the Department of Mechanical Engineering and the Institute for Data, Systems, and Society (IDSS), a principal investigator of the Laboratory for Information and Decision Systems (LIDS), and the&amp;nbsp;senior author of a paper on this technique.&lt;/p&gt;&lt;p&gt;Azizan is joined on the paper by lead author Young-Jin Park, a LIDS/MechE graduate student; Kristjan Greenewald, a research scientist in the MIT-IBM Watson AI Lab; Kaveh Alim, an IDSS graduate student; and Hao Wang, a research scientist at the MIT-IBM Watson AI Lab and the Red Hat AI Innovation Team. The research is being presented this week at the Conference on Neural Information Processing Systems.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Computation for contemplation&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;A recent approach called inference-time scaling lets a large language model take more time to reason about difficult problems.&lt;/p&gt;&lt;p&gt;Using inference-time scaling, the LLM might generate multiple solution attempts at once or explore different reasoning paths, then choose the best ones to pursue from those candidates.&lt;/p&gt;&lt;p&gt;A separate model, known as a process reward model (PRM), scores each potential solution or reasoning path. The LLM uses these scores to identify the most promising ones.&amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/p&gt;&lt;p&gt;Typical inference-time scaling approaches assign a fixed amount of computation for the LLM to break the problem down and reason about the steps.&lt;/p&gt;&lt;p&gt;Instead, the researchers’ method, known as instance-adaptive scaling, dynamically adjusts the number of potential solutions or reasoning steps based on how likely they are to succeed, as the model wrestles with the problem.&lt;/p&gt;&lt;p&gt;“This is how humans solve problems. We come up with some partial solutions and then decide, should I go further with any of these, or stop and revise, or even go back to my previous step and continue solving the problem from there?” Wang explains.&lt;/p&gt;&lt;p&gt;To do this, the framework uses the PRM to estimate the difficulty of the question, helping the LLM assess how much computational budget to utilize for generating and reasoning about potential solutions.&lt;/p&gt;&lt;p&gt;At every step in the model’s reasoning process, the PRM looks at the question and partial answers and evaluates how promising each one is for getting to the right solution. If the LLM is more confident, it can reduce the number of potential solutions or reasoning trajectories to pursue, saving computational resources.&lt;/p&gt;&lt;p&gt;But the researchers found that existing PRMs often overestimate the model’s probability of success.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Overcoming overconfidence&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;“If we were to just trust current PRMs, which often overestimate the chance of success, our system would reduce the computational budget too aggressively. So we first had to find a way to better calibrate PRMs to make inference-time scaling more efficient and reliable,” Park says.&lt;/p&gt;&lt;p&gt;The researchers introduced a calibration method that enables PRMs to generate a range of probability scores rather than a single value. In this way, the PRM creates more reliable uncertainty estimates that better reflect the true probability of success.&lt;/p&gt;&lt;p&gt;With a well-calibrated PRM, their instance-adaptive scaling framework can use the probability scores to effectively reduce computation while maintaining the accuracy of the model’s outputs.&lt;/p&gt;&lt;p&gt;When they compared their method to standard inference-time scaling approaches on a series of mathematical reasoning tasks, it utilized less computation to solve each problem while achieving similar&amp;nbsp;accuracy.&lt;/p&gt;&lt;p&gt;“The beauty of our approach is that this adaptation happens on the fly, as the problem is being solved, rather than happening all at once at the beginning of the process,” says Greenewald.&lt;/p&gt;&lt;p&gt;In the future, the researchers are interested in applying this technique to other applications, such as code generation and AI agents. They are also planning to explore additional uses for their PRM calibration method, like for&amp;nbsp;reinforcement learning and fine-tuning.&lt;/p&gt;&lt;p&gt;“Human employees learn on the job — some CEOs even started as interns — but today’s agents remain largely static pieces of probabilistic software. Work like this paper is an important step toward changing that: helping agents understand what they don’t know and building mechanisms for continual self-improvement. These capabilities are essential if we want agents that can operate safely, adapt to new situations, and deliver consistent results at scale,” says Akash Srivastava, director and chief architect of Core AI at IBM Software, who was not involved with this work.&lt;/p&gt;&lt;p&gt;This work was funded, in part, by the MIT-IBM Watson AI Lab, the MIT-Amazon Science Hub, the MIT-Google Program for Computing Innovation, and MathWorks.&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/MIT-RewardModel-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;To make large language models (LLMs) more accurate when answering harder questions, researchers can let the model spend more time thinking about potential solutions.&lt;/p&gt;&lt;p&gt;But common&amp;nbsp;approaches that give LLMs this capability set a fixed computational budget for every problem, regardless of how complex it is. This means the LLM might waste computational resources on simpler questions or be unable to tackle intricate problems that require more reasoning.&lt;/p&gt;&lt;p&gt;To address this, MIT researchers developed a smarter way to allocate computational effort as the LLM solves a problem. Their method enables the model to dynamically adjust its computational budget based on the difficulty of the question and the likelihood that each partial solution will lead to the correct answer.&lt;/p&gt;&lt;p&gt;The researchers found that their new approach enabled LLMs to use as little as one-half the computation as existing methods, while&amp;nbsp;achieving&amp;nbsp;comparable accuracy on a range of questions with varying difficulties. In addition, their method allows smaller, less resource-intensive LLMs to perform as well as or even better than larger models on complex problems.&lt;/p&gt;&lt;p&gt;By improving the reliability and efficiency of LLMs, especially when they tackle complex reasoning tasks, this technique could reduce the energy consumption of generative AI systems and enable the use of LLMs in more high-stakes and time-sensitive applications.&lt;/p&gt;&lt;p&gt;“The computational cost of inference has quickly become a major bottleneck for frontier model providers, and they are actively trying to find ways to improve computational efficiency per&amp;nbsp;user queries. For instance, the recent GPT-5.1 release highlights the efficacy of the ‘adaptive reasoning’ approach our paper proposes. By endowing the models with the ability to know what they don’t know, we can enable them to spend more compute on the hardest problems and most promising solution paths, and use far fewer tokens on easy ones. That makes reasoning both more reliable and far more efficient,” says Navid Azizan, the Alfred H. and Jean M. Hayes Career Development Assistant Professor in the Department of Mechanical Engineering and the Institute for Data, Systems, and Society (IDSS), a principal investigator of the Laboratory for Information and Decision Systems (LIDS), and the&amp;nbsp;senior author of a paper on this technique.&lt;/p&gt;&lt;p&gt;Azizan is joined on the paper by lead author Young-Jin Park, a LIDS/MechE graduate student; Kristjan Greenewald, a research scientist in the MIT-IBM Watson AI Lab; Kaveh Alim, an IDSS graduate student; and Hao Wang, a research scientist at the MIT-IBM Watson AI Lab and the Red Hat AI Innovation Team. The research is being presented this week at the Conference on Neural Information Processing Systems.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Computation for contemplation&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;A recent approach called inference-time scaling lets a large language model take more time to reason about difficult problems.&lt;/p&gt;&lt;p&gt;Using inference-time scaling, the LLM might generate multiple solution attempts at once or explore different reasoning paths, then choose the best ones to pursue from those candidates.&lt;/p&gt;&lt;p&gt;A separate model, known as a process reward model (PRM), scores each potential solution or reasoning path. The LLM uses these scores to identify the most promising ones.&amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/p&gt;&lt;p&gt;Typical inference-time scaling approaches assign a fixed amount of computation for the LLM to break the problem down and reason about the steps.&lt;/p&gt;&lt;p&gt;Instead, the researchers’ method, known as instance-adaptive scaling, dynamically adjusts the number of potential solutions or reasoning steps based on how likely they are to succeed, as the model wrestles with the problem.&lt;/p&gt;&lt;p&gt;“This is how humans solve problems. We come up with some partial solutions and then decide, should I go further with any of these, or stop and revise, or even go back to my previous step and continue solving the problem from there?” Wang explains.&lt;/p&gt;&lt;p&gt;To do this, the framework uses the PRM to estimate the difficulty of the question, helping the LLM assess how much computational budget to utilize for generating and reasoning about potential solutions.&lt;/p&gt;&lt;p&gt;At every step in the model’s reasoning process, the PRM looks at the question and partial answers and evaluates how promising each one is for getting to the right solution. If the LLM is more confident, it can reduce the number of potential solutions or reasoning trajectories to pursue, saving computational resources.&lt;/p&gt;&lt;p&gt;But the researchers found that existing PRMs often overestimate the model’s probability of success.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Overcoming overconfidence&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;“If we were to just trust current PRMs, which often overestimate the chance of success, our system would reduce the computational budget too aggressively. So we first had to find a way to better calibrate PRMs to make inference-time scaling more efficient and reliable,” Park says.&lt;/p&gt;&lt;p&gt;The researchers introduced a calibration method that enables PRMs to generate a range of probability scores rather than a single value. In this way, the PRM creates more reliable uncertainty estimates that better reflect the true probability of success.&lt;/p&gt;&lt;p&gt;With a well-calibrated PRM, their instance-adaptive scaling framework can use the probability scores to effectively reduce computation while maintaining the accuracy of the model’s outputs.&lt;/p&gt;&lt;p&gt;When they compared their method to standard inference-time scaling approaches on a series of mathematical reasoning tasks, it utilized less computation to solve each problem while achieving similar&amp;nbsp;accuracy.&lt;/p&gt;&lt;p&gt;“The beauty of our approach is that this adaptation happens on the fly, as the problem is being solved, rather than happening all at once at the beginning of the process,” says Greenewald.&lt;/p&gt;&lt;p&gt;In the future, the researchers are interested in applying this technique to other applications, such as code generation and AI agents. They are also planning to explore additional uses for their PRM calibration method, like for&amp;nbsp;reinforcement learning and fine-tuning.&lt;/p&gt;&lt;p&gt;“Human employees learn on the job — some CEOs even started as interns — but today’s agents remain largely static pieces of probabilistic software. Work like this paper is an important step toward changing that: helping agents understand what they don’t know and building mechanisms for continual self-improvement. These capabilities are essential if we want agents that can operate safely, adapt to new situations, and deliver consistent results at scale,” says Akash Srivastava, director and chief architect of Core AI at IBM Software, who was not involved with this work.&lt;/p&gt;&lt;p&gt;This work was funded, in part, by the MIT-IBM Watson AI Lab, the MIT-Amazon Science Hub, the MIT-Google Program for Computing Innovation, and MathWorks.&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/smarter-way-large-language-models-think-about-hard-problems-1204</guid><pubDate>Thu, 04 Dec 2025 05:00:00 +0000</pubDate></item><item><title>[NEW] AI memory hunger forces Micron’s consumer exodus: A turning point in semiconductor economics (AI News)</title><link>https://www.artificialintelligence-news.com/news/ai-memory-hunger-micron-consumer-exit/</link><description>&lt;p&gt;In the basement of a Boise, Idaho, dental office in 1978, four engineers founded what would become one of America’s semiconductor giants. Ward Parkinson, Joe Parkinson, Dennis Wilson, and Doug Pitman started Micron Technology as a modest design consultancy, backed by local investors including potato magnate J.R. Simplot.&lt;/p&gt;&lt;p&gt;By 1983, they had achieved a technological breakthrough – producing chips roughly half the size of Japan’s leading products. Nearly five decades later, that same company has made a decision that crystallises artificial intelligence’s profound impact on hardware economics: AI memory hunger is forcing manufacturers to abandon entire market segments.&lt;/p&gt;&lt;p&gt;On December 3, 2025, Micron announced it would completely exit the consumer memory market, discontinuing its 29-year-old Crucial brand by February 2026. “The AI-driven growth in the data centre has led to a surge in demand for memory and storage,” said Sumit Sadana, Micron’s executive vice president and chief business officer.&lt;/p&gt;&lt;p&gt;“Micron has made the difficult decision to exit the Crucial consumer business to improve supply and support for our larger, strategic customers in faster-growing segments.”&lt;/p&gt;&lt;p&gt;Translation: data centres running AI workloads will pay substantially more for memory than individual consumers ever could, and Micron’s fabrication capacity cannot serve both markets simultaneously.&lt;/p&gt;&lt;p&gt;The announcement represents both a business decision and a watershed moment revealing how AI memory hunger demands are restructuring global semiconductor supply chains, forcing manufacturers to make stark choices about which customers ‘deserve’ access to finite production capacity.&lt;/p&gt;&lt;h3&gt;The economics driving AI memory hunger&lt;/h3&gt;&lt;p&gt;Micron’s withdrawal reflects economic realities. As the world’s third-largest DRAM producer with an approximately 20% of global market share, the company sits between South Korean giants Samsung Electronics (43%) and SK Hynix (35%). Together, these three control roughly 95% of worldwide DRAM production – an oligopoly now facing unprecedented demand from AI infrastructure builders.&lt;/p&gt;&lt;p&gt;The margin differentials tell the story. Consumer RAM modules compete in volatile retail markets with razor-thin profitability. Enterprise contracts for high-bandwidth memory (HBM) used in AI accelerators and DDR5 modules for data centre servers deliver substantially higher average selling prices, multi-year commitments, and predictable demand.&lt;/p&gt;&lt;p&gt;For memory manufacturers, each fabrication wafer committed to consumer products represents foregone revenue from higher-value enterprise contracts – an opportunity cost that has become economically indefensible as AI demand accelerates.&lt;/p&gt;&lt;p&gt;The numbers illustrate the magnitude of the shift. Micron reported record fiscal 2025 revenue of US$37.38 billion, representing nearly 50% year-over-year growth driven primarily by data centre and AI applications, which accounted for 56% of total revenue. SK Hynix has reportedly sold out its entire 2026 production capacity for DRAM, HBM, and NAND products.&lt;/p&gt;&lt;p&gt;Consumer memory prices have surged accordingly. DRAM spot prices increased 172% year-over-year as of Q3 2025, with retail prices for 32GB DDR5 modules jumping 163-619% in global markets since September 2025. Component suppliers report paying US$13 for 16GB DDR5 chips that cost US$7 just six weeks earlier – increases sufficient to eliminate entire gross margins for third-party brands.&lt;/p&gt;&lt;h3&gt;Consumer market restructuring amid AI memory hunger&lt;/h3&gt;&lt;p&gt;Micron’s exit alters the consumer memory landscape. Third-party brands, including Corsair, G.Skill, Kingston, and ADATA source their DRAM chips from the major manufacturers. With Micron withdrawing entirely, these vendors must compete more aggressively for allocation from Samsung and SK Hynix – both simultaneously prioritising high-bandwidth memory production for AI accelerators.&lt;/p&gt;&lt;p&gt;The concentration creates vulnerabilities. Samsung and SK Hynix now comprise the only major suppliers serving both consumer and enterprise markets. Both face identical capacity allocation pressures. If AI infrastructure investment maintains current trajectories, additional manufacturers may reduce or restructure consumer operations.&lt;/p&gt;&lt;p&gt;Supply chain constraints are already materialising beyond DRAM. NAND flash wafer contract prices increased by over 60% in November 2025. Graphics memory markets face pressures as manufacturers shift to GDDR7 for next-generation GPUs, creating GDDR6 shortages that inflated prices by approximately 30%. Hard drive manufacturers increased prices 5-10%, citing limited supply.&lt;/p&gt;&lt;p&gt;For consumers and small businesses, the implications extend beyond pricing. Product availability may become increasingly constrained during peak demand periods. The reduction in direct supplier participation may compress product differentiation and limit competitive pricing dynamics that previously benefited buyers.&lt;/p&gt;&lt;h3&gt;The broader industry realignment&lt;/h3&gt;&lt;p&gt;Micron’s consumer exodus signals a structural transformation rather than a temporary reallocation. The AI infrastructure boom differs fundamentally from previous technology transitions. Personal computing, internet expansion, and mobile devices created sustained memory demand over decades with gradual capacity adjustments.&lt;/p&gt;&lt;p&gt;AI infrastructure deployment compresses that timeline dramatically – hyperscale operators are committing hundreds of billions in data centre construction over just a few years. Data centre semiconductor markets illustrate the scale. The total addressable market reached US$209 billion in 2024, and is projected to grow to nearly US$500 billion by 2030, driven primarily by AI and high-performance computing.&lt;/p&gt;&lt;p&gt;GPU revenue alone is forecast to expand from US$100 billion in 2024 to US$215 billion by 2030, with each GPU requiring substantial high-bandwidth memory allocation.&lt;/p&gt;&lt;p&gt;Memory architecture evolution compounds the challenge. AI training workloads increasingly require HBM3E modules, which offer superior bandwidth and power efficiency, while inference workloads demand DDR5 with tight latency specifications.&lt;/p&gt;&lt;p&gt;Automotive applications adopting zonal architectures require multi-gigabyte DRAM configurations. Each application commands premium pricing and long-term contracts – economic incentives systematically pulling manufacturing capacity away from consumer markets.&lt;/p&gt;&lt;p&gt;The manufacturing response reflects these priorities. Samsung is advancing 1c DRAM production and planning mass production of HBM4 in 2025 while phasing out DDR4 entirely. Micron began mass production of DRAM using Extreme Ultraviolet (EUV) lithography in 2025.&lt;/p&gt;&lt;p&gt;SK Hynix focuses development resources on HBM and advanced LPDDR solutions. All three manufacturers are directing research and capital investment toward applications offering superior returns.&lt;/p&gt;&lt;h3&gt;What this means for enterprise buyers&lt;/h3&gt;&lt;p&gt;Enterprise procurement teams face their own challenges as memory markets restructure. Memory represents 10-25% of bill-of-materials costs for typical servers and commercial PCs. Price increases of 20-30% in memory components translate to 5-10% increases in total system costs, compounding into millions in additional expenditure for organisations procuring at scale.&lt;/p&gt;&lt;p&gt;Strategic responses include forward purchasing agreements, establishing stronger direct relationships with manufacturers, and diversifying vendor partnerships. The timing uncertainty presents particular challenges. New fabrication capacity is under construction, supported by government incentives, but requires years to reach production readiness.&lt;/p&gt;&lt;h3&gt;Critical questions ahead&lt;/h3&gt;&lt;p&gt;Micron’s consumer market exit raises fundamental questions. Will Samsung and SK Hynix maintain consumer product lines, or will similar capacity pressures force comparable reductions? If consumer memory becomes primarily a third-party brand market sourcing chips from manufacturers prioritising enterprise customers, what happens to product innovation and competitive pricing?&lt;/p&gt;&lt;p&gt;The concentration among just two major manufacturers serving consumer markets creates potential vulnerabilities. Supply chain disruptions affecting either Samsung or SK Hynix would have an outsized impact on global consumer product availability.&lt;/p&gt;&lt;p&gt;Broader implications extend to technology accessibility. If memory pricing remains elevated or availability constrained for consumer products, the costs of personal computing and small business infrastructure increase accordingly, potentially widening digital divides.&lt;/p&gt;&lt;p&gt;Micron’s decision crystallises artificial intelligence’s role as a transformative force reshaping not just software, but the fundamental economics of hardware manufacturing. The Crucial brand’s retirement after 29 years marks the end of a time when memory manufacturers could serve both consumer and enterprise segments simultaneously and profitably.&lt;/p&gt;&lt;p&gt;For the broader technology ecosystem, hunger for AI memory has become the semiconductor industry’s dominant growth driver, commanding resources at levels that fundamentally alter which markets manufacturers choose to serve.&lt;/p&gt;&lt;p&gt;(Photo: &lt;em&gt;Micron Technology&lt;/em&gt;)&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110949" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-18.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;In the basement of a Boise, Idaho, dental office in 1978, four engineers founded what would become one of America’s semiconductor giants. Ward Parkinson, Joe Parkinson, Dennis Wilson, and Doug Pitman started Micron Technology as a modest design consultancy, backed by local investors including potato magnate J.R. Simplot.&lt;/p&gt;&lt;p&gt;By 1983, they had achieved a technological breakthrough – producing chips roughly half the size of Japan’s leading products. Nearly five decades later, that same company has made a decision that crystallises artificial intelligence’s profound impact on hardware economics: AI memory hunger is forcing manufacturers to abandon entire market segments.&lt;/p&gt;&lt;p&gt;On December 3, 2025, Micron announced it would completely exit the consumer memory market, discontinuing its 29-year-old Crucial brand by February 2026. “The AI-driven growth in the data centre has led to a surge in demand for memory and storage,” said Sumit Sadana, Micron’s executive vice president and chief business officer.&lt;/p&gt;&lt;p&gt;“Micron has made the difficult decision to exit the Crucial consumer business to improve supply and support for our larger, strategic customers in faster-growing segments.”&lt;/p&gt;&lt;p&gt;Translation: data centres running AI workloads will pay substantially more for memory than individual consumers ever could, and Micron’s fabrication capacity cannot serve both markets simultaneously.&lt;/p&gt;&lt;p&gt;The announcement represents both a business decision and a watershed moment revealing how AI memory hunger demands are restructuring global semiconductor supply chains, forcing manufacturers to make stark choices about which customers ‘deserve’ access to finite production capacity.&lt;/p&gt;&lt;h3&gt;The economics driving AI memory hunger&lt;/h3&gt;&lt;p&gt;Micron’s withdrawal reflects economic realities. As the world’s third-largest DRAM producer with an approximately 20% of global market share, the company sits between South Korean giants Samsung Electronics (43%) and SK Hynix (35%). Together, these three control roughly 95% of worldwide DRAM production – an oligopoly now facing unprecedented demand from AI infrastructure builders.&lt;/p&gt;&lt;p&gt;The margin differentials tell the story. Consumer RAM modules compete in volatile retail markets with razor-thin profitability. Enterprise contracts for high-bandwidth memory (HBM) used in AI accelerators and DDR5 modules for data centre servers deliver substantially higher average selling prices, multi-year commitments, and predictable demand.&lt;/p&gt;&lt;p&gt;For memory manufacturers, each fabrication wafer committed to consumer products represents foregone revenue from higher-value enterprise contracts – an opportunity cost that has become economically indefensible as AI demand accelerates.&lt;/p&gt;&lt;p&gt;The numbers illustrate the magnitude of the shift. Micron reported record fiscal 2025 revenue of US$37.38 billion, representing nearly 50% year-over-year growth driven primarily by data centre and AI applications, which accounted for 56% of total revenue. SK Hynix has reportedly sold out its entire 2026 production capacity for DRAM, HBM, and NAND products.&lt;/p&gt;&lt;p&gt;Consumer memory prices have surged accordingly. DRAM spot prices increased 172% year-over-year as of Q3 2025, with retail prices for 32GB DDR5 modules jumping 163-619% in global markets since September 2025. Component suppliers report paying US$13 for 16GB DDR5 chips that cost US$7 just six weeks earlier – increases sufficient to eliminate entire gross margins for third-party brands.&lt;/p&gt;&lt;h3&gt;Consumer market restructuring amid AI memory hunger&lt;/h3&gt;&lt;p&gt;Micron’s exit alters the consumer memory landscape. Third-party brands, including Corsair, G.Skill, Kingston, and ADATA source their DRAM chips from the major manufacturers. With Micron withdrawing entirely, these vendors must compete more aggressively for allocation from Samsung and SK Hynix – both simultaneously prioritising high-bandwidth memory production for AI accelerators.&lt;/p&gt;&lt;p&gt;The concentration creates vulnerabilities. Samsung and SK Hynix now comprise the only major suppliers serving both consumer and enterprise markets. Both face identical capacity allocation pressures. If AI infrastructure investment maintains current trajectories, additional manufacturers may reduce or restructure consumer operations.&lt;/p&gt;&lt;p&gt;Supply chain constraints are already materialising beyond DRAM. NAND flash wafer contract prices increased by over 60% in November 2025. Graphics memory markets face pressures as manufacturers shift to GDDR7 for next-generation GPUs, creating GDDR6 shortages that inflated prices by approximately 30%. Hard drive manufacturers increased prices 5-10%, citing limited supply.&lt;/p&gt;&lt;p&gt;For consumers and small businesses, the implications extend beyond pricing. Product availability may become increasingly constrained during peak demand periods. The reduction in direct supplier participation may compress product differentiation and limit competitive pricing dynamics that previously benefited buyers.&lt;/p&gt;&lt;h3&gt;The broader industry realignment&lt;/h3&gt;&lt;p&gt;Micron’s consumer exodus signals a structural transformation rather than a temporary reallocation. The AI infrastructure boom differs fundamentally from previous technology transitions. Personal computing, internet expansion, and mobile devices created sustained memory demand over decades with gradual capacity adjustments.&lt;/p&gt;&lt;p&gt;AI infrastructure deployment compresses that timeline dramatically – hyperscale operators are committing hundreds of billions in data centre construction over just a few years. Data centre semiconductor markets illustrate the scale. The total addressable market reached US$209 billion in 2024, and is projected to grow to nearly US$500 billion by 2030, driven primarily by AI and high-performance computing.&lt;/p&gt;&lt;p&gt;GPU revenue alone is forecast to expand from US$100 billion in 2024 to US$215 billion by 2030, with each GPU requiring substantial high-bandwidth memory allocation.&lt;/p&gt;&lt;p&gt;Memory architecture evolution compounds the challenge. AI training workloads increasingly require HBM3E modules, which offer superior bandwidth and power efficiency, while inference workloads demand DDR5 with tight latency specifications.&lt;/p&gt;&lt;p&gt;Automotive applications adopting zonal architectures require multi-gigabyte DRAM configurations. Each application commands premium pricing and long-term contracts – economic incentives systematically pulling manufacturing capacity away from consumer markets.&lt;/p&gt;&lt;p&gt;The manufacturing response reflects these priorities. Samsung is advancing 1c DRAM production and planning mass production of HBM4 in 2025 while phasing out DDR4 entirely. Micron began mass production of DRAM using Extreme Ultraviolet (EUV) lithography in 2025.&lt;/p&gt;&lt;p&gt;SK Hynix focuses development resources on HBM and advanced LPDDR solutions. All three manufacturers are directing research and capital investment toward applications offering superior returns.&lt;/p&gt;&lt;h3&gt;What this means for enterprise buyers&lt;/h3&gt;&lt;p&gt;Enterprise procurement teams face their own challenges as memory markets restructure. Memory represents 10-25% of bill-of-materials costs for typical servers and commercial PCs. Price increases of 20-30% in memory components translate to 5-10% increases in total system costs, compounding into millions in additional expenditure for organisations procuring at scale.&lt;/p&gt;&lt;p&gt;Strategic responses include forward purchasing agreements, establishing stronger direct relationships with manufacturers, and diversifying vendor partnerships. The timing uncertainty presents particular challenges. New fabrication capacity is under construction, supported by government incentives, but requires years to reach production readiness.&lt;/p&gt;&lt;h3&gt;Critical questions ahead&lt;/h3&gt;&lt;p&gt;Micron’s consumer market exit raises fundamental questions. Will Samsung and SK Hynix maintain consumer product lines, or will similar capacity pressures force comparable reductions? If consumer memory becomes primarily a third-party brand market sourcing chips from manufacturers prioritising enterprise customers, what happens to product innovation and competitive pricing?&lt;/p&gt;&lt;p&gt;The concentration among just two major manufacturers serving consumer markets creates potential vulnerabilities. Supply chain disruptions affecting either Samsung or SK Hynix would have an outsized impact on global consumer product availability.&lt;/p&gt;&lt;p&gt;Broader implications extend to technology accessibility. If memory pricing remains elevated or availability constrained for consumer products, the costs of personal computing and small business infrastructure increase accordingly, potentially widening digital divides.&lt;/p&gt;&lt;p&gt;Micron’s decision crystallises artificial intelligence’s role as a transformative force reshaping not just software, but the fundamental economics of hardware manufacturing. The Crucial brand’s retirement after 29 years marks the end of a time when memory manufacturers could serve both consumer and enterprise segments simultaneously and profitably.&lt;/p&gt;&lt;p&gt;For the broader technology ecosystem, hunger for AI memory has become the semiconductor industry’s dominant growth driver, commanding resources at levels that fundamentally alter which markets manufacturers choose to serve.&lt;/p&gt;&lt;p&gt;(Photo: &lt;em&gt;Micron Technology&lt;/em&gt;)&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110949" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-18.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/ai-memory-hunger-micron-consumer-exit/</guid><pubDate>Thu, 04 Dec 2025 06:05:00 +0000</pubDate></item><item><title>[NEW] Why the grid relies on nuclear reactors in the winter (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/12/04/1128754/nuclear-power-reliability/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;As many of us are ramping up with shopping, baking, and planning for the holiday season, nuclear power plants are also getting ready for one of their busiest seasons of the year.&lt;/p&gt;  &lt;p&gt;Here in the US, nuclear reactors follow predictable seasonal trends. Summer and winter tend to see the highest electricity demand, so plant operators schedule maintenance and refueling for other parts of the year.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;This scheduled regularity might seem mundane, but it’s quite the feat that operational reactors are as reliable and predictable as they are. It leaves some big shoes to fill for next-generation technology hoping to join the fleet in the next few years.&lt;/p&gt;  &lt;p&gt;Generally, nuclear reactors operate at constant levels, as close to full capacity as possible. In 2024, for commercial reactors worldwide, the average capacity factor—the ratio of actual energy output to the theoretical maxiumum—was 83%. North America rang in at an average of about 90%.&lt;/p&gt; 
 &lt;p&gt;(I’ll note here that it’s not always fair to just look at this number to compare different kinds of power plants—natural-gas plants can have lower capacity factors, but it’s mostly because they’re more likely to be intentionally turned on and off to help meet uneven demand.)&lt;/p&gt;  &lt;p&gt;Those high capacity factors also undersell the fleet’s true reliability—a lot of the downtime is scheduled. Reactors need to refuel every 18 to 24 months, and operators tend to schedule those outages for the spring and fall, when electricity demand isn’t as high as when we’re all running our air conditioners or heaters at full tilt.&lt;/p&gt; 
 &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1128760" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/Screenshot-2025-12-03-at-5.00.44-PM.png" /&gt;&lt;/figure&gt;  &lt;p&gt;Take a look at this chart of nuclear outages from the US Energy Information Administration. There are some days, especially at the height of summer, when outages are low, and nearly all commercial reactors in the US are operating at nearly full capacity. On July 28 of this year, the fleet was operating at 99.6%. Compare that with&amp;nbsp; the 77.6% of capacity on October 18, as reactors were taken offline for refueling and maintenance. Now we’re heading into another busy season, when reactors are coming back online and shutdowns are entering another low point.&lt;/p&gt;  &lt;p&gt;That’s not to say all outages are planned. At the Sequoyah nuclear power plant in Tennessee, a generator failure in July 2024 took one of two reactors offline, an outage that lasted nearly a year. (The utility also did some maintenance during that time to extend the life of the plant.) Then, just days after that reactor started back up, the entire plant had to shut down because of low water levels.&lt;/p&gt;  &lt;p&gt;And who can forget the incident earlier this year when jellyfish wreaked havoc on not one but two nuclear power plants in France? In the second instance, the squishy creatures got into the filters of equipment that sucks water out of the English Channel for cooling at the Paluel nuclear plant. They forced the plant to cut output by nearly half, though it was restored within days.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Barring jellyfish disasters and occasional maintenance, the global nuclear fleet operates quite reliably. That wasn’t always the case, though. In the 1970s, reactors operated at an average capacity factor of just 60%. They were shut down nearly as often as they were running.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;The fleet of reactors today has benefited from decades of experience. Now we’re seeing a growing pool of companies aiming to bring new technologies to the nuclear industry.&lt;/p&gt;  &lt;p&gt;Next-generation reactors that use new materials for fuel or cooling will be able to borrow some lessons from the existing fleet, but they’ll also face novel challenges.&lt;/p&gt;  &lt;p&gt;That could mean early demonstration reactors aren’t as reliable as the current commercial fleet at first. “First-of-a-kind nuclear, just like with any other first-of-a-kind technologies, is very challenging,” says Koroush Shirvan, a professor of nuclear science and engineering at MIT.&lt;/p&gt;  &lt;p&gt;That means it will probably take time for molten-salt reactors or small modular reactors, or any of the other designs out there to overcome technical hurdles and settle into their own rhythm. It’s taken decades to get to a place where we take it for granted that the nuclear fleet can follow a neat seasonal curve based on electricity demand.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;There will always be hurricanes and electrical failures and jellyfish invasions that cause some unexpected problems and force nuclear plants (or any power plants, for that matter) to shut down. But overall, the fleet today operates at an extremely high level of consistency. One of the major challenges ahead for next-generation technologies will be proving that they can do the same.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark, &lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;As many of us are ramping up with shopping, baking, and planning for the holiday season, nuclear power plants are also getting ready for one of their busiest seasons of the year.&lt;/p&gt;  &lt;p&gt;Here in the US, nuclear reactors follow predictable seasonal trends. Summer and winter tend to see the highest electricity demand, so plant operators schedule maintenance and refueling for other parts of the year.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;This scheduled regularity might seem mundane, but it’s quite the feat that operational reactors are as reliable and predictable as they are. It leaves some big shoes to fill for next-generation technology hoping to join the fleet in the next few years.&lt;/p&gt;  &lt;p&gt;Generally, nuclear reactors operate at constant levels, as close to full capacity as possible. In 2024, for commercial reactors worldwide, the average capacity factor—the ratio of actual energy output to the theoretical maxiumum—was 83%. North America rang in at an average of about 90%.&lt;/p&gt; 
 &lt;p&gt;(I’ll note here that it’s not always fair to just look at this number to compare different kinds of power plants—natural-gas plants can have lower capacity factors, but it’s mostly because they’re more likely to be intentionally turned on and off to help meet uneven demand.)&lt;/p&gt;  &lt;p&gt;Those high capacity factors also undersell the fleet’s true reliability—a lot of the downtime is scheduled. Reactors need to refuel every 18 to 24 months, and operators tend to schedule those outages for the spring and fall, when electricity demand isn’t as high as when we’re all running our air conditioners or heaters at full tilt.&lt;/p&gt; 
 &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1128760" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/Screenshot-2025-12-03-at-5.00.44-PM.png" /&gt;&lt;/figure&gt;  &lt;p&gt;Take a look at this chart of nuclear outages from the US Energy Information Administration. There are some days, especially at the height of summer, when outages are low, and nearly all commercial reactors in the US are operating at nearly full capacity. On July 28 of this year, the fleet was operating at 99.6%. Compare that with&amp;nbsp; the 77.6% of capacity on October 18, as reactors were taken offline for refueling and maintenance. Now we’re heading into another busy season, when reactors are coming back online and shutdowns are entering another low point.&lt;/p&gt;  &lt;p&gt;That’s not to say all outages are planned. At the Sequoyah nuclear power plant in Tennessee, a generator failure in July 2024 took one of two reactors offline, an outage that lasted nearly a year. (The utility also did some maintenance during that time to extend the life of the plant.) Then, just days after that reactor started back up, the entire plant had to shut down because of low water levels.&lt;/p&gt;  &lt;p&gt;And who can forget the incident earlier this year when jellyfish wreaked havoc on not one but two nuclear power plants in France? In the second instance, the squishy creatures got into the filters of equipment that sucks water out of the English Channel for cooling at the Paluel nuclear plant. They forced the plant to cut output by nearly half, though it was restored within days.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Barring jellyfish disasters and occasional maintenance, the global nuclear fleet operates quite reliably. That wasn’t always the case, though. In the 1970s, reactors operated at an average capacity factor of just 60%. They were shut down nearly as often as they were running.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;The fleet of reactors today has benefited from decades of experience. Now we’re seeing a growing pool of companies aiming to bring new technologies to the nuclear industry.&lt;/p&gt;  &lt;p&gt;Next-generation reactors that use new materials for fuel or cooling will be able to borrow some lessons from the existing fleet, but they’ll also face novel challenges.&lt;/p&gt;  &lt;p&gt;That could mean early demonstration reactors aren’t as reliable as the current commercial fleet at first. “First-of-a-kind nuclear, just like with any other first-of-a-kind technologies, is very challenging,” says Koroush Shirvan, a professor of nuclear science and engineering at MIT.&lt;/p&gt;  &lt;p&gt;That means it will probably take time for molten-salt reactors or small modular reactors, or any of the other designs out there to overcome technical hurdles and settle into their own rhythm. It’s taken decades to get to a place where we take it for granted that the nuclear fleet can follow a neat seasonal curve based on electricity demand.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;There will always be hurricanes and electrical failures and jellyfish invasions that cause some unexpected problems and force nuclear plants (or any power plants, for that matter) to shut down. But overall, the fleet today operates at an extremely high level of consistency. One of the major challenges ahead for next-generation technologies will be proving that they can do the same.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark, &lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/12/04/1128754/nuclear-power-reliability/</guid><pubDate>Thu, 04 Dec 2025 11:00:00 +0000</pubDate></item><item><title>[NEW] Nexus isn’t going all-in on AI, keeping half of its new $700M fund for India startups (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/04/nexus-isnt-going-all-in-on-ai-keeping-its-new-700m-fund-balanced-with-india-bets/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/nexus-venture-partners.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;While many venture firms seem to only have eyes for AI these days, Nexus Venture Partners is deliberately splitting its focus for its new $700 million fund. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The firm will back AI startups and seek out India-focused startups in consumer, fintech, and digital infrastructure.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AI has soaked up most of the venture capital raised globally and the 20-year-old VC firm also sees AI as a defining technological shift. But it argues crowding into a single, overheated category carries its own risks. India’s digital economy provides a counterbalance: an expanding market where AI adoption is rising and opportunities remain more diverse.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Nexus, that balance is rooted in its origins. The Delaware-headquartered firm, with offices in Menlo Park, Mumbai and Bengaluru, has operated as a single fund and an integrated U.S.–India team since its founding in 2006. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It backs early-stage software and India-focused startups from the same pool of capital. Over time, its cross-border software bets have encompassed a range from infrastructure and developer tools to AI agent startups. U.S. portfolio includes companies such as Postman, Apollo, MinIO, Giga, and Firecrawl, which have become widely adopted in developer tooling and AI infrastructure.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile its India portfolio has broadened across consumer, fintech, logistics, and digital infrastructure. Some of its bets there include Zepto, Delhivery, Rapido, Turtlemint, and Infra.Market&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AI is a huge inflection point, and we are anchoring on that,” Jishnu Bhattacharjee, a managing partner at Nexus Venture Partners in the U.S., told TechCrunch in an interview. “But we are also seeing that many of these AI innovations are actually getting used to serve the masses better.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Nexus manages $3.2 billion in capital across its funds and has invested in more than 130 companies over the years. The firm has recorded more than 30 exits to date, including several IPOs, underscoring the depth of its early-stage, long-horizon approach.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Abhishek Sharma, a managing partner at Nexus Venture Partners in the U.S., told TechCrunch the firm’s sweet spot remains inception to seed and Series A, often beginning with checks as small as a few hundred thousand dollars or around $1 million.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nexus, which operates with an eight-member investment team, began with a $100 million fund and has kept its fund size at $700 million since launching Fund VII in 2023. It typically raises every 2.5 to 3 years. Bhattacharjee said the reason for keeping the eighth fund the same size was the firm believes $700 million is the right amount for its early-stage strategy.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We don’t want to raise money for the sake of raising,” he noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Even though India’s AI journey is not as advanced as the U.S.’s in many areas, Nexus believes India could leapfrog in several parts of the AI ecosystem. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bhattacharjee underlined the country’s large talent pool, rising digital infrastructure, and demand for localized models that support India’s many languages and service needs. These dynamics, he said, are pushing Indian startups to build AI applications and agents faster, often atop open-source tools and emerging domestic AI infrastructure companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The partners pointed to companies backed by Nexus, such as Zepto and Neysa, to illustrate how AI is taking shape in India. They said Zepto, the quick-commerce platform, uses AI extensively across its operations — from customer support to routing and fulfillment — demonstrating how consumer businesses are becoming deeply AI-native. Besides, infrastructure players like Neysa are emerging to address India-specific needs, including sovereign AI workloads, localized data handling and support for the country’s many languages.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nexus did not share fund metrics. The partners said its funds have been realizing significant enough returns over the years to largely fill this fund from returning limited partners. The firm’s LP base spans the U.S., Europe, the Middle East, Southeast Asia and Japan.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/nexus-venture-partners.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;While many venture firms seem to only have eyes for AI these days, Nexus Venture Partners is deliberately splitting its focus for its new $700 million fund. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The firm will back AI startups and seek out India-focused startups in consumer, fintech, and digital infrastructure.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AI has soaked up most of the venture capital raised globally and the 20-year-old VC firm also sees AI as a defining technological shift. But it argues crowding into a single, overheated category carries its own risks. India’s digital economy provides a counterbalance: an expanding market where AI adoption is rising and opportunities remain more diverse.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Nexus, that balance is rooted in its origins. The Delaware-headquartered firm, with offices in Menlo Park, Mumbai and Bengaluru, has operated as a single fund and an integrated U.S.–India team since its founding in 2006. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It backs early-stage software and India-focused startups from the same pool of capital. Over time, its cross-border software bets have encompassed a range from infrastructure and developer tools to AI agent startups. U.S. portfolio includes companies such as Postman, Apollo, MinIO, Giga, and Firecrawl, which have become widely adopted in developer tooling and AI infrastructure.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile its India portfolio has broadened across consumer, fintech, logistics, and digital infrastructure. Some of its bets there include Zepto, Delhivery, Rapido, Turtlemint, and Infra.Market&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AI is a huge inflection point, and we are anchoring on that,” Jishnu Bhattacharjee, a managing partner at Nexus Venture Partners in the U.S., told TechCrunch in an interview. “But we are also seeing that many of these AI innovations are actually getting used to serve the masses better.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Nexus manages $3.2 billion in capital across its funds and has invested in more than 130 companies over the years. The firm has recorded more than 30 exits to date, including several IPOs, underscoring the depth of its early-stage, long-horizon approach.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Abhishek Sharma, a managing partner at Nexus Venture Partners in the U.S., told TechCrunch the firm’s sweet spot remains inception to seed and Series A, often beginning with checks as small as a few hundred thousand dollars or around $1 million.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nexus, which operates with an eight-member investment team, began with a $100 million fund and has kept its fund size at $700 million since launching Fund VII in 2023. It typically raises every 2.5 to 3 years. Bhattacharjee said the reason for keeping the eighth fund the same size was the firm believes $700 million is the right amount for its early-stage strategy.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We don’t want to raise money for the sake of raising,” he noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Even though India’s AI journey is not as advanced as the U.S.’s in many areas, Nexus believes India could leapfrog in several parts of the AI ecosystem. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bhattacharjee underlined the country’s large talent pool, rising digital infrastructure, and demand for localized models that support India’s many languages and service needs. These dynamics, he said, are pushing Indian startups to build AI applications and agents faster, often atop open-source tools and emerging domestic AI infrastructure companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The partners pointed to companies backed by Nexus, such as Zepto and Neysa, to illustrate how AI is taking shape in India. They said Zepto, the quick-commerce platform, uses AI extensively across its operations — from customer support to routing and fulfillment — demonstrating how consumer businesses are becoming deeply AI-native. Besides, infrastructure players like Neysa are emerging to address India-specific needs, including sovereign AI workloads, localized data handling and support for the country’s many languages.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nexus did not share fund metrics. The partners said its funds have been realizing significant enough returns over the years to largely fill this fund from returning limited partners. The firm’s LP base spans the U.S., Europe, the Middle East, Southeast Asia and Japan.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/04/nexus-isnt-going-all-in-on-ai-keeping-its-new-700m-fund-balanced-with-india-bets/</guid><pubDate>Thu, 04 Dec 2025 11:00:00 +0000</pubDate></item><item><title>[NEW] The NPU in your phone keeps improving—why isn’t that making AI better? (AI – Ars Technica)</title><link>https://arstechnica.com/gadgets/2025/12/the-npu-in-your-phone-keeps-improving-why-isnt-that-making-ai-better/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 md:my-10 md:py-8 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      Shrinking AI for your phone is no simple matter.
    &lt;/p&gt;

          
    
    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="npu phone" class="intro-image" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/npu_phone.jpg" width="2560" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The NPU in your phone might not be doing very much. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson | Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Almost every technological innovation of the past several years has been laser-focused on one thing: generative AI. Many of these supposedly revolutionary systems run on big, expensive servers in a data center somewhere, but at the same time, chipmakers are crowing about the power of the neural processing units (NPU) they have brought to consumer devices. Every few months, it’s the same thing: This new NPU is 30 or 40 percent faster than the last one. That’s supposed to let you do something important, but no one really gets around to explaining what that is.&lt;/p&gt;
&lt;p&gt;Experts envision a future of secure, personal AI tools with on-device intelligence, but does that match the reality of the AI boom? AI on the “edge” sounds great, but almost every AI tool of consequence is running in the cloud. So what’s that chip in your phone even doing?&lt;/p&gt;
&lt;h2&gt;What is an NPU?&lt;/h2&gt;
&lt;p&gt;Companies launching a new product often get bogged down in superlatives and vague marketing speak, so they do a poor job of explaining technical details. It’s not clear to most people buying a phone why they need the hardware to run AI workloads, and the supposed benefits are largely theoretical.&lt;/p&gt;
&lt;p&gt;Many of today’s flagship consumer processors are systems-on-a-chip (SoC) because they incorporate multiple computing elements—like CPU cores, GPUs, and imaging controllers—on a single piece of silicon. This is true of mobile parts like Qualcomm’s Snapdragon or Google’s Tensor, as well as PC components like the Intel Core Ultra.&lt;/p&gt;
&lt;p&gt;The NPU is a newer addition to chips, but it didn’t just appear one day—there’s a lineage that brought us here. NPUs are good at what they do because they emphasize parallel computing, something that’s also important in other SoC components.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Qualcomm devotes significant time during its new product unveilings to talk about its Hexagon NPUs. Keen observers may recall that this branding has been reused from the company’s line of digital signal processors (DSPs), and there’s a good reason for that.&lt;/p&gt;
&lt;p&gt;“Our journey into AI processing started probably 15 or 20 years ago, wherein our first anchor point was looking at signal processing,” said Vinesh Sukumar, Qualcomm’s head of AI products. DSPs have a similar architecture compared to NPUs, but they’re much simpler, with a focus on processing audio (e.g., speech recognition) and modem signals.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2130184 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Qualcomm chip design NPU" class="fullwidth full" height="2250" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Qualcomm-AI-Engine.jpg" width="4000" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The NPU is one of multiple components in modern SoCs.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Qualcomm

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;As the collection of technologies we refer to as “artificial intelligence” developed, engineers began using DSPs for more types of parallel processing, like long short-term memory (LSTM). Sukumar explained that as the industry became enamored with convolutional neural networks (CNNs), the technology underlying applications like computer vision, DSPs became focused on matrix functions, which are essential to generative AI processing as well.&lt;/p&gt;
&lt;p&gt;While there is an architectural lineage here, it’s not quite right to say NPUs are just fancy DSPs. “If you talk about DSPs in the general term of the word, yes, [an NPU] is a digital signal processor,” said MediaTek Assistant Vice President Mark Odani. “But it’s all come a long way and it’s a lot more optimized for parallelism, how the transformers work, and holding huge numbers of parameters for processing.”&lt;/p&gt;
&lt;p&gt;Despite being so prominent in new chips, NPUs are not strictly necessary for running AI workloads on the “edge,” a term that differentiates local AI processing from cloud-based systems. CPUs are slower than NPUs but can handle some light workloads without using as much power. Meanwhile, GPUs can often chew through more data than an NPU, but they use more power to do it. And there are times you may want to do that, according to Qualcomm’s Sukumar. For example, running AI workloads while a game is running could favor the GPU.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;“Here, your measurement of success is that you cannot drop your frame rate while maintaining the spatial resolution, the dynamic range of the pixel, and also being able to provide AI recommendations for the player within that space,” says Sukumar. “In this kind of use case, it actually makes sense to run that in the graphics engine, because then you don’t have to keep shifting between the graphics and a domain-specific AI engine like an NPU.”&lt;/p&gt;
&lt;h2&gt;Livin’ on the edge is hard&lt;/h2&gt;
&lt;p&gt;Unfortunately, the NPUs in many devices sit idle (and not just during gaming). The mix of local versus cloud AI tools favors the latter because that’s the natural habitat of LLMs. AI models are trained and fine-tuned on powerful servers, and that’s where they run best.&lt;/p&gt;
&lt;p&gt;A server-based AI, like the full-fat versions of Gemini and ChatGPT, is not resource-constrained like a model running on your phone’s NPU. Consider the latest version of Google’s on-device Gemini Nano model, which has a context window of 32k tokens. That is a more than 2x improvement over the last version. However, the cloud-based Gemini models have context windows of up to 1 million tokens, meaning they can process much larger volumes of data.&lt;/p&gt;
&lt;p&gt;Both cloud-based and edge AI hardware will continue getting better, but the balance may not shift in the NPU’s favor. “The cloud will always have more compute resources versus a mobile device,” said Google’s Shenaz Zack, senior product manager on the Pixel team.&lt;/p&gt;
&lt;p&gt;“If you want the most accurate models or the most brute force models, that all has to be done in the cloud,” Odani said. “But what we’re finding is that, in a lot of the use cases where there’s just summarizing some text or you’re talking to your voice assistant, a lot of those things can fit within three billion parameters.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Squeezing AI models onto a phone or laptop involves some compromise—for example, by reducing the parameters included in the model. Odani explained that cloud-based models run hundreds of billions of parameters, the weighting that determines how a model processes input tokens to generate outputs. You can’t run anything like that on a consumer device right now, so developers have to vastly scale back the size of models for the edge. Odani says MediaTek’s latest ninth-generation NPU can handle about 3 billion parameters—a difference of several orders of magnitude.&lt;/p&gt;
&lt;p&gt;The amount of memory available in a phone or laptop is also a limiting factor, so mobile-optimized AI models are usually quantized. That means the model’s estimation of the next token runs with less precision. Let’s say you want to run one of the larger open models, like Llama or Gemma 7b, on your device. The de facto standard is FP16, known as half-precision. At that level, a model with 7 billion parameters will lock up 13 or 14 gigabytes of memory. Stepping down to FP4 (quarter-precision) brings the size of the model in memory to a few gigs.&lt;/p&gt;
&lt;p&gt;“When you compress to, let’s say, between three and four gigabytes, it’s a sweet spot for integration into memory constrained form factors like a smartphone,” Sukumar said. “And there’s been a lot of investment in the ecosystem and at Qualcomm to look at various ways of compressing the models without losing quality.”&lt;/p&gt;
&lt;p&gt;It’s difficult to create a generalized AI with these limitations for mobile devices, but computers—and especially smartphones—are a wellspring of data that can be pumped into models to generate supposedly helpful outputs. That’s why most edge AI is geared toward specific, narrow use cases, like analyzing screenshots or suggesting calendar appointments. Google says its latest Pixel phones run more than 100 AI models, both generative and traditional.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Even AI skeptics can recognize that the landscape is changing quickly. In the time it takes to shrink and optimize AI models for a phone or laptop, new cloud models may appear that make that work obsolete. This is also why third-party developers have been slow to utilize NPU processing in apps. They either have to plug into an existing on-device model, which involves restrictions and rapidly moving development targets, or deploy their own custom models. Neither is a great option currently.&lt;/p&gt;
&lt;h2&gt;A matter of trust&lt;/h2&gt;
&lt;p&gt;If the cloud is faster and easier, why go to the trouble of optimizing for the edge and burning more power with an NPU? Leaning on the cloud means accepting a level of dependence and trust in the people operating AI data centers that may not always be appropriate.&lt;/p&gt;
&lt;p&gt;“We always start off with user privacy as an element,” said Qualcomm’s Sukumar. He explained that the best inference is not general in nature—it’s personalized based on the user’s interests and what’s happening in their lives. Fine-tuning models to deliver that experience calls for personal data, and it’s safer to store and process that data locally.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Even when companies say the right things about privacy in their cloud services, they’re far from guarantees. The helpful, friendly vibe of general chatbots also encourages people to divulge a lot of personal information, and if that assistant is running in the cloud, your data is there as well. OpenAI’s copyright fight with The New York Times could lead to millions of private chats being handed over to the publisher. The explosive growth and uncertain regulatory framework of gen AI make it hard to know what’s going to happen to your data.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“People are using a lot of these generative AI assistants like a therapist,” Odani said. “And you don’t know one day if all this stuff is going to come out on the Internet.”&lt;/p&gt;
&lt;p&gt;Not everyone is so concerned. Zack claims Google has built “the world’s most secure cloud infrastructure,” allowing it to process data where it delivers the best results. Zack uses Video Boost and Pixel Studio as examples of this approach, noting that Google’s cloud is the only way to make these experiences fast and high-quality. The company recently announced its new Private AI Compute system, which it claims is just as safe as local AI.&lt;/p&gt;
&lt;p&gt;Even if that’s true, the edge has other advantages—edge AI is just more reliable than a cloud service. “On-device is fast,” Odani said. “Sometimes I’m talking to ChatGPT and my Wi-Fi goes out or whatever, and it skips a beat.”&lt;/p&gt;
&lt;p&gt;The services hosting cloud-based AI models aren’t just a single website—the Internet of today is massively interdependent, with content delivery networks, DNS providers, hosting, and other services that could degrade or shut down your favorite AI in the event of a glitch. When Cloudflare suffered a self-inflicted outage recently, ChatGPT users were annoyed to find their trusty chatbot was unavailable. Local AI features don’t have that drawback.&lt;/p&gt;
&lt;h2&gt;Cloud dominance&lt;/h2&gt;
&lt;p&gt;Everyone seems to agree that a hybrid approach is necessary to deliver truly useful AI features (assuming those exist), sending data to more powerful cloud services when necessary—Google, Apple, and every other phone maker does this. But the pursuit of a seamless experience can also obscure what’s happening with your data. More often than not, the AI features on your phone aren’t running in a secure, local way, even when the device has the hardware to do that.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Take, for example, the new OnePlus 15. This phone has Qualcomm’s brand-new Snapdragon 8 Elite Gen 5, which has an NPU that is 37 percent faster than the last one, for whatever that’s worth. Even with all that on-device AI might, OnePlus is heavily reliant on the cloud to analyze your personal data. Features like AI Writer and the AI Recorder connect to the company’s servers for processing, a system OnePlus assures us is totally safe and private.&lt;/p&gt;
&lt;p&gt;Similarly, Motorola released a new line of foldable Razr phones over the summer that are loaded with AI features from multiple providers. These phones can summarize your notifications using AI, but you might be surprised how much of it happens in the cloud unless you read the terms and conditions. If you buy the Razr Ultra, that summarization happens on your phone. However, the cheaper models with less RAM and NPU power use cloud services to process your notifications. Again, Motorola says this system is secure, but a more secure option would have been to re-optimize the model for its cheaper phones.&lt;/p&gt;
&lt;p&gt;Even when an OEM focuses on using the NPU hardware, the results can be lacking. Look at Google’s Daily Hub and Samsung’s Now Brief. These features are supposed to chew through all the data on your phone and generate useful recommendations and actions, but they rarely do anything aside from showing calendar events. In fact, Google has temporarily removed Daily Hub from Pixels because the feature did so little, and Google is a pioneer in local AI with Gemini Nano. Google has actually moved some parts of its mobile AI experience from local to cloud-based processing in recent months.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Those “brute force” models appear to be winning, and it doesn’t hurt that companies also get more data when you interact with their private computing cloud services.&lt;/p&gt;
&lt;h2&gt;Maybe take what you can get?&lt;/h2&gt;
&lt;p&gt;There’s plenty of interest in local AI, but so far, that hasn’t translated to an AI revolution in your pocket. Most of the AI advances we’ve seen so far depend on the ever-increasing scale of cloud systems and the generalized models that run there. Industry experts say that extensive work is happening behind the scenes to shrink AI models to work on phones and laptops, but it will take time for that to make an impact.&lt;/p&gt;
&lt;p&gt;In the meantime, local AI processing is out there in a limited way. Google still makes use of the Tensor NPU to handle sensitive data for features like Magic Cue, and Samsung really makes the most of Qualcomm’s AI-focused chipsets. While Now Brief is of questionable utility, Samsung is cognizant of how reliance on the cloud may impact users, offering a toggle in the system settings that restricts AI processing to run only on the device. This limits the number of available AI features, and others don’t work as well, but you’ll know none of your personal data is being shared. No one else offers this option on a smartphone.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2130180 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Galaxy AI toggle" class="fullwidth full" height="2160" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Galaxy-AI-toggle-1.jpg" width="3840" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Samsung offers an easy toggle to disable cloud AI and run all workloads on-device.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Samsung spokesperson Elise Sembach said the company’s AI efforts are grounded in enhancing experiences while maintaining user control. “The on-device processing toggle in One UI reflects this approach. It gives users the option to process AI tasks locally for faster performance, added privacy, and reliability even without a network connection,” Sembach said.&lt;/p&gt;
&lt;p&gt;Interest in edge AI might be a good thing even if you don’t use it. Planning for this AI-rich future can encourage device makers to invest in better hardware—like more memory to run all those theoretical AI models.&lt;/p&gt;
&lt;p&gt;“We definitely recommend our partners increase their RAM capacity,” said Sukumar. Indeed, Google, Samsung, and others have boosted memory capacity in large part to support on-device AI. Even if the cloud is winning, we’ll take the extra RAM.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 md:my-10 md:py-8 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      Shrinking AI for your phone is no simple matter.
    &lt;/p&gt;

          
    
    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="npu phone" class="intro-image" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/npu_phone.jpg" width="2560" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The NPU in your phone might not be doing very much. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson | Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Almost every technological innovation of the past several years has been laser-focused on one thing: generative AI. Many of these supposedly revolutionary systems run on big, expensive servers in a data center somewhere, but at the same time, chipmakers are crowing about the power of the neural processing units (NPU) they have brought to consumer devices. Every few months, it’s the same thing: This new NPU is 30 or 40 percent faster than the last one. That’s supposed to let you do something important, but no one really gets around to explaining what that is.&lt;/p&gt;
&lt;p&gt;Experts envision a future of secure, personal AI tools with on-device intelligence, but does that match the reality of the AI boom? AI on the “edge” sounds great, but almost every AI tool of consequence is running in the cloud. So what’s that chip in your phone even doing?&lt;/p&gt;
&lt;h2&gt;What is an NPU?&lt;/h2&gt;
&lt;p&gt;Companies launching a new product often get bogged down in superlatives and vague marketing speak, so they do a poor job of explaining technical details. It’s not clear to most people buying a phone why they need the hardware to run AI workloads, and the supposed benefits are largely theoretical.&lt;/p&gt;
&lt;p&gt;Many of today’s flagship consumer processors are systems-on-a-chip (SoC) because they incorporate multiple computing elements—like CPU cores, GPUs, and imaging controllers—on a single piece of silicon. This is true of mobile parts like Qualcomm’s Snapdragon or Google’s Tensor, as well as PC components like the Intel Core Ultra.&lt;/p&gt;
&lt;p&gt;The NPU is a newer addition to chips, but it didn’t just appear one day—there’s a lineage that brought us here. NPUs are good at what they do because they emphasize parallel computing, something that’s also important in other SoC components.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Qualcomm devotes significant time during its new product unveilings to talk about its Hexagon NPUs. Keen observers may recall that this branding has been reused from the company’s line of digital signal processors (DSPs), and there’s a good reason for that.&lt;/p&gt;
&lt;p&gt;“Our journey into AI processing started probably 15 or 20 years ago, wherein our first anchor point was looking at signal processing,” said Vinesh Sukumar, Qualcomm’s head of AI products. DSPs have a similar architecture compared to NPUs, but they’re much simpler, with a focus on processing audio (e.g., speech recognition) and modem signals.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2130184 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Qualcomm chip design NPU" class="fullwidth full" height="2250" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Qualcomm-AI-Engine.jpg" width="4000" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The NPU is one of multiple components in modern SoCs.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Qualcomm

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;As the collection of technologies we refer to as “artificial intelligence” developed, engineers began using DSPs for more types of parallel processing, like long short-term memory (LSTM). Sukumar explained that as the industry became enamored with convolutional neural networks (CNNs), the technology underlying applications like computer vision, DSPs became focused on matrix functions, which are essential to generative AI processing as well.&lt;/p&gt;
&lt;p&gt;While there is an architectural lineage here, it’s not quite right to say NPUs are just fancy DSPs. “If you talk about DSPs in the general term of the word, yes, [an NPU] is a digital signal processor,” said MediaTek Assistant Vice President Mark Odani. “But it’s all come a long way and it’s a lot more optimized for parallelism, how the transformers work, and holding huge numbers of parameters for processing.”&lt;/p&gt;
&lt;p&gt;Despite being so prominent in new chips, NPUs are not strictly necessary for running AI workloads on the “edge,” a term that differentiates local AI processing from cloud-based systems. CPUs are slower than NPUs but can handle some light workloads without using as much power. Meanwhile, GPUs can often chew through more data than an NPU, but they use more power to do it. And there are times you may want to do that, according to Qualcomm’s Sukumar. For example, running AI workloads while a game is running could favor the GPU.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;“Here, your measurement of success is that you cannot drop your frame rate while maintaining the spatial resolution, the dynamic range of the pixel, and also being able to provide AI recommendations for the player within that space,” says Sukumar. “In this kind of use case, it actually makes sense to run that in the graphics engine, because then you don’t have to keep shifting between the graphics and a domain-specific AI engine like an NPU.”&lt;/p&gt;
&lt;h2&gt;Livin’ on the edge is hard&lt;/h2&gt;
&lt;p&gt;Unfortunately, the NPUs in many devices sit idle (and not just during gaming). The mix of local versus cloud AI tools favors the latter because that’s the natural habitat of LLMs. AI models are trained and fine-tuned on powerful servers, and that’s where they run best.&lt;/p&gt;
&lt;p&gt;A server-based AI, like the full-fat versions of Gemini and ChatGPT, is not resource-constrained like a model running on your phone’s NPU. Consider the latest version of Google’s on-device Gemini Nano model, which has a context window of 32k tokens. That is a more than 2x improvement over the last version. However, the cloud-based Gemini models have context windows of up to 1 million tokens, meaning they can process much larger volumes of data.&lt;/p&gt;
&lt;p&gt;Both cloud-based and edge AI hardware will continue getting better, but the balance may not shift in the NPU’s favor. “The cloud will always have more compute resources versus a mobile device,” said Google’s Shenaz Zack, senior product manager on the Pixel team.&lt;/p&gt;
&lt;p&gt;“If you want the most accurate models or the most brute force models, that all has to be done in the cloud,” Odani said. “But what we’re finding is that, in a lot of the use cases where there’s just summarizing some text or you’re talking to your voice assistant, a lot of those things can fit within three billion parameters.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Squeezing AI models onto a phone or laptop involves some compromise—for example, by reducing the parameters included in the model. Odani explained that cloud-based models run hundreds of billions of parameters, the weighting that determines how a model processes input tokens to generate outputs. You can’t run anything like that on a consumer device right now, so developers have to vastly scale back the size of models for the edge. Odani says MediaTek’s latest ninth-generation NPU can handle about 3 billion parameters—a difference of several orders of magnitude.&lt;/p&gt;
&lt;p&gt;The amount of memory available in a phone or laptop is also a limiting factor, so mobile-optimized AI models are usually quantized. That means the model’s estimation of the next token runs with less precision. Let’s say you want to run one of the larger open models, like Llama or Gemma 7b, on your device. The de facto standard is FP16, known as half-precision. At that level, a model with 7 billion parameters will lock up 13 or 14 gigabytes of memory. Stepping down to FP4 (quarter-precision) brings the size of the model in memory to a few gigs.&lt;/p&gt;
&lt;p&gt;“When you compress to, let’s say, between three and four gigabytes, it’s a sweet spot for integration into memory constrained form factors like a smartphone,” Sukumar said. “And there’s been a lot of investment in the ecosystem and at Qualcomm to look at various ways of compressing the models without losing quality.”&lt;/p&gt;
&lt;p&gt;It’s difficult to create a generalized AI with these limitations for mobile devices, but computers—and especially smartphones—are a wellspring of data that can be pumped into models to generate supposedly helpful outputs. That’s why most edge AI is geared toward specific, narrow use cases, like analyzing screenshots or suggesting calendar appointments. Google says its latest Pixel phones run more than 100 AI models, both generative and traditional.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Even AI skeptics can recognize that the landscape is changing quickly. In the time it takes to shrink and optimize AI models for a phone or laptop, new cloud models may appear that make that work obsolete. This is also why third-party developers have been slow to utilize NPU processing in apps. They either have to plug into an existing on-device model, which involves restrictions and rapidly moving development targets, or deploy their own custom models. Neither is a great option currently.&lt;/p&gt;
&lt;h2&gt;A matter of trust&lt;/h2&gt;
&lt;p&gt;If the cloud is faster and easier, why go to the trouble of optimizing for the edge and burning more power with an NPU? Leaning on the cloud means accepting a level of dependence and trust in the people operating AI data centers that may not always be appropriate.&lt;/p&gt;
&lt;p&gt;“We always start off with user privacy as an element,” said Qualcomm’s Sukumar. He explained that the best inference is not general in nature—it’s personalized based on the user’s interests and what’s happening in their lives. Fine-tuning models to deliver that experience calls for personal data, and it’s safer to store and process that data locally.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Even when companies say the right things about privacy in their cloud services, they’re far from guarantees. The helpful, friendly vibe of general chatbots also encourages people to divulge a lot of personal information, and if that assistant is running in the cloud, your data is there as well. OpenAI’s copyright fight with The New York Times could lead to millions of private chats being handed over to the publisher. The explosive growth and uncertain regulatory framework of gen AI make it hard to know what’s going to happen to your data.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“People are using a lot of these generative AI assistants like a therapist,” Odani said. “And you don’t know one day if all this stuff is going to come out on the Internet.”&lt;/p&gt;
&lt;p&gt;Not everyone is so concerned. Zack claims Google has built “the world’s most secure cloud infrastructure,” allowing it to process data where it delivers the best results. Zack uses Video Boost and Pixel Studio as examples of this approach, noting that Google’s cloud is the only way to make these experiences fast and high-quality. The company recently announced its new Private AI Compute system, which it claims is just as safe as local AI.&lt;/p&gt;
&lt;p&gt;Even if that’s true, the edge has other advantages—edge AI is just more reliable than a cloud service. “On-device is fast,” Odani said. “Sometimes I’m talking to ChatGPT and my Wi-Fi goes out or whatever, and it skips a beat.”&lt;/p&gt;
&lt;p&gt;The services hosting cloud-based AI models aren’t just a single website—the Internet of today is massively interdependent, with content delivery networks, DNS providers, hosting, and other services that could degrade or shut down your favorite AI in the event of a glitch. When Cloudflare suffered a self-inflicted outage recently, ChatGPT users were annoyed to find their trusty chatbot was unavailable. Local AI features don’t have that drawback.&lt;/p&gt;
&lt;h2&gt;Cloud dominance&lt;/h2&gt;
&lt;p&gt;Everyone seems to agree that a hybrid approach is necessary to deliver truly useful AI features (assuming those exist), sending data to more powerful cloud services when necessary—Google, Apple, and every other phone maker does this. But the pursuit of a seamless experience can also obscure what’s happening with your data. More often than not, the AI features on your phone aren’t running in a secure, local way, even when the device has the hardware to do that.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Take, for example, the new OnePlus 15. This phone has Qualcomm’s brand-new Snapdragon 8 Elite Gen 5, which has an NPU that is 37 percent faster than the last one, for whatever that’s worth. Even with all that on-device AI might, OnePlus is heavily reliant on the cloud to analyze your personal data. Features like AI Writer and the AI Recorder connect to the company’s servers for processing, a system OnePlus assures us is totally safe and private.&lt;/p&gt;
&lt;p&gt;Similarly, Motorola released a new line of foldable Razr phones over the summer that are loaded with AI features from multiple providers. These phones can summarize your notifications using AI, but you might be surprised how much of it happens in the cloud unless you read the terms and conditions. If you buy the Razr Ultra, that summarization happens on your phone. However, the cheaper models with less RAM and NPU power use cloud services to process your notifications. Again, Motorola says this system is secure, but a more secure option would have been to re-optimize the model for its cheaper phones.&lt;/p&gt;
&lt;p&gt;Even when an OEM focuses on using the NPU hardware, the results can be lacking. Look at Google’s Daily Hub and Samsung’s Now Brief. These features are supposed to chew through all the data on your phone and generate useful recommendations and actions, but they rarely do anything aside from showing calendar events. In fact, Google has temporarily removed Daily Hub from Pixels because the feature did so little, and Google is a pioneer in local AI with Gemini Nano. Google has actually moved some parts of its mobile AI experience from local to cloud-based processing in recent months.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Those “brute force” models appear to be winning, and it doesn’t hurt that companies also get more data when you interact with their private computing cloud services.&lt;/p&gt;
&lt;h2&gt;Maybe take what you can get?&lt;/h2&gt;
&lt;p&gt;There’s plenty of interest in local AI, but so far, that hasn’t translated to an AI revolution in your pocket. Most of the AI advances we’ve seen so far depend on the ever-increasing scale of cloud systems and the generalized models that run there. Industry experts say that extensive work is happening behind the scenes to shrink AI models to work on phones and laptops, but it will take time for that to make an impact.&lt;/p&gt;
&lt;p&gt;In the meantime, local AI processing is out there in a limited way. Google still makes use of the Tensor NPU to handle sensitive data for features like Magic Cue, and Samsung really makes the most of Qualcomm’s AI-focused chipsets. While Now Brief is of questionable utility, Samsung is cognizant of how reliance on the cloud may impact users, offering a toggle in the system settings that restricts AI processing to run only on the device. This limits the number of available AI features, and others don’t work as well, but you’ll know none of your personal data is being shared. No one else offers this option on a smartphone.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2130180 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Galaxy AI toggle" class="fullwidth full" height="2160" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Galaxy-AI-toggle-1.jpg" width="3840" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Samsung offers an easy toggle to disable cloud AI and run all workloads on-device.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Samsung spokesperson Elise Sembach said the company’s AI efforts are grounded in enhancing experiences while maintaining user control. “The on-device processing toggle in One UI reflects this approach. It gives users the option to process AI tasks locally for faster performance, added privacy, and reliability even without a network connection,” Sembach said.&lt;/p&gt;
&lt;p&gt;Interest in edge AI might be a good thing even if you don’t use it. Planning for this AI-rich future can encourage device makers to invest in better hardware—like more memory to run all those theoretical AI models.&lt;/p&gt;
&lt;p&gt;“We definitely recommend our partners increase their RAM capacity,” said Sukumar. Indeed, Google, Samsung, and others have boosted memory capacity in large part to support on-device AI. Even if the cloud is winning, we’ll take the extra RAM.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/gadgets/2025/12/the-npu-in-your-phone-keeps-improving-why-isnt-that-making-ai-better/</guid><pubDate>Thu, 04 Dec 2025 12:00:42 +0000</pubDate></item></channel></rss>