<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 12 Dec 2025 01:52:07 +0000</lastBuildDate><item><title>Marble enters the race to bring AI to tax work, armed with $9 million and a free research tool (AI | VentureBeat)</title><link>https://venturebeat.com/ai/marble-enters-the-race-to-bring-ai-to-tax-work-armed-with-usd9-million-and-a</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="http://marble.ai/"&gt;Marble&lt;/a&gt;, a startup building artificial intelligence agents for tax professionals, has raised $9 million in seed funding as the accounting industry grapples with a deepening labor shortage and mounting regulatory complexity.&lt;/p&gt;&lt;p&gt;The round, led by &lt;a href="https://www.susaventures.com/"&gt;Susa Ventures&lt;/a&gt; with participation from &lt;a href="https://mxv.vc/"&gt;MXV Capital&lt;/a&gt; and Konrad Capital, positions Marble to compete in a market where AI adoption has lagged significantly behind other knowledge industries like law and software development.&lt;/p&gt;&lt;p&gt;&amp;quot;When we looked at the economy and asked ourselves where AI is going to transform the way businesses operate, we focused on knowledge industries ‚Äî specifically businesses with hourly fee-based service models,&amp;quot; said Bhavin Shah, Marble&amp;#x27;s chief executive officer, in an exclusive interview with VentureBeat. &amp;quot;Accounting generates $250 billion in fee-based billing in the US every year. There&amp;#x27;s a tremendous opportunity to increase efficiency and improve margins for accounting firms.&amp;quot;&lt;/p&gt;&lt;p&gt;The company has launched a &lt;a href="https://marble.ai/"&gt;free AI-powered tax research tool&lt;/a&gt; on its website that converts complex government tax data into accessible, citation-backed answers for practitioners. Marble plans to expand into AI agents that can analyze compliance scenarios and eventually automate portions of tax preparation workflows.&lt;/p&gt;&lt;p&gt;Marble&amp;#x27;s backers share Shah&amp;#x27;s conviction about the market. &amp;quot;Marble is rethinking the accounting system from the ground up. Accounting is one of the biggest ‚Äî and most overlooked ‚Äî markets in professional services,&amp;quot; Chad Byers, general partner at Susa Ventures, told VentureBeat. &amp;quot;We&amp;#x27;ve known Bhavin from his time as an executive in the Susa portfolio, and have seen firsthand how sharp and execution-driven he is. He and Geordie bring the perfect mix of operational depth and product instinct to a space long overdue for change ‚Äî and they see the same massive opportunity we do.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The accounting industry lost 340,000 workers in four years ‚Äî and replacements aren&amp;#x27;t coming&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Marble enters a market shaped by structural forces that have fundamentally altered the economics of professional accounting.&lt;/p&gt;&lt;p&gt;The accounting profession has &lt;a href="https://www.bls.gov/ooh/office-and-administrative-support/bookkeeping-accounting-and-auditing-clerks.htm"&gt;shed roughly 340,000 workers since 2019&lt;/a&gt;, a 17% decline that has left firms scrambling to meet client demands. First-time candidates for the Certified Public Accountant exam dropped 33% between 2016 and 2021, according to &lt;a href="https://www.cpajournal.com/2025/08/15/the-accounting-profession-is-in-crisis-3/"&gt;AICPA data&lt;/a&gt;, and 2022 saw the lowest number of exam takers in 17 years.&lt;/p&gt;&lt;p&gt;The exodus comes as baby boomers exit en masse. The American Institute of CPAs estimates that approximately &lt;a href="https://www.forbes.com/councils/forbesfinancecouncil/2025/05/12/disruption-in-accounting-the-cpa-shortage-meets-the-rise-of-ai/"&gt;75% of all licensed CPAs&lt;/a&gt; reached retirement age by 2019, creating a demographic cliff that the profession has struggled to address.&lt;/p&gt;&lt;p&gt;‚ÄúFewer CPAs are getting certified year over year,&amp;quot; Shah said. &amp;quot;The industry is compressing at the same time that there&amp;#x27;s more work to be done and the tax code is getting more complicated.&amp;quot;&lt;/p&gt;&lt;p&gt;The &lt;a href="https://www.accountingpipeline.org/"&gt;National Pipeline Advisory Group&lt;/a&gt;, a multi-stakeholder body formed by the AICPA in July 2023, released a report identifying the &lt;a href="https://sc.cpa/2025/01/10/national-pipeline-advisory-group-npag-releases-official-report/"&gt;150-hour education requirement&lt;/a&gt; for CPA licensure as a significant barrier to entry. A separate &lt;a href="https://www.thecaq.org/increasing-diversity-in-the-accounting-profession-pipeline-2022"&gt;survey&lt;/a&gt; by the Center for Audit Quality found that 57% of business majors who chose not to pursue accounting cited the additional credit hours as a deterrent.&lt;/p&gt;&lt;p&gt;Recent legislative changes reflect the urgency. Ohio now offers alternatives to the 150-hour requirement, signaling that states are willing to experiment with pathways that could reverse enrollment declines.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why AI transformed law and software development but left accounting behind&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Despite the profession&amp;#x27;s challenges, AI adoption in accounting has moved more slowly than in adjacent knowledge industries. &lt;a href="https://www.harvey.ai/"&gt;Harvey&lt;/a&gt; and &lt;a href="https://legora.com/"&gt;Legora&lt;/a&gt; have raised hundreds of millions to bring AI to legal work. &lt;a href="https://cursor.com/agents"&gt;Cursor&lt;/a&gt; and other coding assistants have transformed software development. Accounting, by contrast, remains largely dependent on legacy research platforms and manual processes.&lt;/p&gt;&lt;p&gt;Geordie Konrad, Marble&amp;#x27;s executive chairman and a co-founder of restaurant software company TouchBistro, attributes the gap to how people conceptualize AI&amp;#x27;s capabilities.&lt;/p&gt;&lt;p&gt;‚ÄúIt was obvious to many people that LLMs could do meaningful work by manipulating code for software developers and manipulating words for lawyers. In the accounting industry, LLMs are going to be used as reasoning agents,&amp;quot; Konrad said. &amp;quot; That requires a bit more of a two-step analysis to see why it&amp;#x27;s a big opportunity.&amp;quot;&lt;/p&gt;&lt;p&gt;The technical challenge is substantial. Tax regulations form one of the most complex, interconnected information systems that humans have created ‚Äî tens of thousands of interlocking rules, guidance documents, and jurisdiction-specific requirements that frequently overlap or conflict.&lt;/p&gt;&lt;p&gt;&amp;quot;If you want to put AI through its paces and ask how far it&amp;#x27;s come in replicating cognitive functions, this is an unbelievable playground to work in,&amp;quot; Konrad said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;A dramatic shift: AI adoption among tax and finance teams doubles in one year&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Recent data suggests the accounting profession&amp;#x27;s stance toward AI is shifting rapidly.&lt;/p&gt;&lt;p&gt;A 2025 survey from &lt;a href="https://www.avalara.com/us/en/learn/ai-reinventing-finance-and-tax-state-of-finance-report.html"&gt;Hanover Research and Avalara&lt;/a&gt; found that 84% of finance and tax teams now use AI heavily in their operations, up from 47% in 2024. The 2025 Generative AI in Professional Services Report from &lt;a href="https://www.cpapracticeadvisor.com/2025/04/15/79-of-tax-and-accounting-firms-expect-significant-genai-integration-by-2027/159172/"&gt;Thomson Reuters Institute&lt;/a&gt; found that 21% of tax firms already use generative AI technology, with 53% either planning to adopt it or actively considering it.&lt;/p&gt;&lt;p&gt;Large accounting firms have invested heavily in AI infrastructure. &lt;a href="https://www.deloitte.com/us/en/what-we-do/capabilities/applied-artificial-intelligence/articles/agentic-ai-enterprise-2028.html?id=us:2ps:3gl:aisgm26:awa:CONS:nonem:K0218784:111725:kwd-2025710122928:188372336109:784136672833::Brand_AI-SGO_BU_K0218784_Google:Brand_AI-SGO-Advisory:deloitte-generative-ai:&amp;amp;gclsrc=aw.ds&amp;amp;gad_source=1&amp;amp;gad_campaignid=23269751515&amp;amp;gbraid=0AAAAADenGPA6ZvC8cLRrT031O1up6RiOj&amp;amp;gclid=Cj0KCQiAi9rJBhCYARIsALyPDtudMaZrpHm8026n16yuyeM0sYKCTm24yHqR4Dyg_s6yZBgPnxfjDOwaAvN5EALw_wcB"&gt;Deloitte&lt;/a&gt; has developed generative AI capabilities within its audit platform. &lt;a href="https://www.bdo.com/insights/press-releases/bdo-usa-unveils-comprehensive-artificial-intelligence-strategy-fusing-practical-innovation"&gt;BDO&lt;/a&gt; announced a $1B investment in AI over the next five years. &lt;a href="https://www.ey.com/en_gl/newsroom/2025/11/ey-unveils-suite-of-powerful-ai-capabilities-to-accelerate-tax-transformation-and-deliver-strategic-value"&gt;EY&lt;/a&gt; launched an AI platform combining technology with strategy, transactions, and tax services. &lt;a href="https://www.pwc.com/us/en/tech-effect/ai-analytics/ai-predictions.html"&gt;PwC&lt;/a&gt; estimates a complete AI-driven audit solution will launch by 2026.&lt;/p&gt;&lt;p&gt;But adoption at smaller firms remains uneven. According to &lt;a href="http://thomsonreuters.com/en-us/posts/technology/genai-professional-services-report-2025/"&gt;Thomson Reuters research&lt;/a&gt;, 52% of tax firm respondents who use generative AI rely on open-source tools like ChatGPT rather than industry-specific solutions‚Äîa pattern that could shift as purpose-built alternatives emerge.&lt;/p&gt;&lt;p&gt;Marble&amp;#x27;s founders believe the hesitance stems not from technophobia but from a lack of compelling options.&lt;/p&gt;&lt;p&gt;‚ÄúFirms want to embrace AI,&amp;quot; Shah said. ‚ÄúThey just haven&amp;#x27;t seen great software and tooling made for them. That&amp;#x27;s part of the opportunity ‚Äî to work with them and build something they&amp;#x27;re excited to use on a day-to-day basis.‚Äù&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Can artificial intelligence rescue accounting&amp;#x27;s billable-hour business model?&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;AI&amp;#x27;s arrival in accounting raises questions about the profession&amp;#x27;s billing structure.&lt;/p&gt;&lt;p&gt;Accounting firms have traditionally generated profits by billing clients for staff time, often at multiples of employee compensation costs. Junior associates performing compliance work represent a significant revenue stream. If AI can automate that work, does it undercut the business model firms depend on?&lt;/p&gt;&lt;p&gt;Marble&amp;#x27;s founders argue the opposite. The chronic staffing shortage has already constrained firms&amp;#x27; ability to capture available revenue. Advisory and consulting work ‚Äî higher-margin services that clients actively want ‚Äî goes undone because practitioners are buried in compliance tasks.&lt;/p&gt;&lt;p&gt;&amp;quot;Everyone in the industry agrees that an enormous amount of advisory work simply isn&amp;#x27;t getting done,&amp;quot; Konrad said. &amp;quot;Customers want it. Firms want to do it because it&amp;#x27;s high-margin, great work. But nobody gets to it.&amp;quot;&lt;/p&gt;&lt;p&gt;The &lt;a href="https://www.aicpa-cima.com/news/article/cpa-firms-report-steady-growth-in-revenue-and-profit-aicpa-research-finds"&gt;2025 AICPA National Management of an Accounting Practice Survey&lt;/a&gt; supports this view. Firms reported a median 6.7% increase in net client fees over the prior year, with growth in audit, assurance, tax services, and client accounting advisory. Net remaining per partner climbed 11.9% from fiscal year 2022 to fiscal year 2024, reaching $252,663.&lt;/p&gt;&lt;p&gt;The survey also found growing interest in AI adoption, though most firms have yet to allocate formal budgets or develop structured training programs. Continued adoption, the survey suggested, could help expand services and fuel continued growth.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Accountants won&amp;#x27;t adopt AI tools they can&amp;#x27;t trust with sensitive client data&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;For AI to succeed in accounting, it must clear a high bar for data security. Accounting firms handle some of the most sensitive financial information in the economy. Practitioners cannot adopt tools that create compliance or confidentiality risks.&lt;/p&gt;&lt;p&gt;According to &lt;a href="https://www.avalara.com/us/en/learn/ai-reinventing-finance-and-tax-state-of-finance-report.html"&gt;Avalara&amp;#x27;s survey&lt;/a&gt;, 63% of respondents cited data security and privacy concerns as the top barriers to automating tax and finance functions. The concern persists throughout the adoption lifecycle, from initial selection through implementation and ongoing use.&lt;/p&gt;&lt;p&gt;&lt;a href="https://marble.ai/"&gt;Marble&lt;/a&gt; has made security a foundational priority. The company obtained software compliance certification before releasing any product and maintains that data privacy is embedded in its operational culture from day one.&lt;/p&gt;&lt;p&gt;&amp;quot;Security is at the core of what we are building,&amp;quot; Shah said. &amp;quot;Every employee knows that security is critical. It&amp;#x27;s a part of our onboarding and something that we consider in everything we do.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;From number crunchers to strategic advisors: How AI could reshape accounting careers&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Marble&amp;#x27;s founders reject the narrative that AI will only take away from accounting jobs. They propose instead that AI will result in accounting jobs becoming more strategic and less characterized by repetitive execution.¬†&lt;/p&gt;&lt;p&gt;They draw an analogy to architecture, where computer-aided design replaced laborious manual drafting. Architects did not disappear ‚Äî they gained tools that let them spend more time on creative design and less on mechanical reproduction.&lt;/p&gt;&lt;p&gt;&amp;quot;If you take some of the hours-intensive, less creative work out of what being a junior or intermediate accountant is, and you replace it with a role where you&amp;#x27;re a professional who is being creative, synthesizing ideas, and able to delegate a lot of tasks to AI assistant platform solutions, you end up with an industry that&amp;#x27;s just a lot more fun to operate in,&amp;quot; Konrad said.&lt;/p&gt;&lt;p&gt;The shift could also improve client outcomes. When accountants spend less time on compliance, they can invest more in the strategic advisory work that clients value.&lt;/p&gt;&lt;p&gt;&amp;quot;Not only does the work become more enjoyable because of what you can focus on, but that&amp;#x27;s also what your clients are going to value more from you,&amp;quot; Shah said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The competitive landscape: Marble faces well-funded rivals and legacy giants&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;a href="http://marble.ai/"&gt;Marble&lt;/a&gt; enters a market with formidable incumbents and well-funded competitors. &lt;a href="https://venturebeat.com/ai/how-ai-tax-startup-blue-j-torched-its-entire-business-model-for-chatgpt-and"&gt;BlueJ&lt;/a&gt;, a global tax research platform, has raised over $100 million. &lt;a href="https://www.thomsonreuters.com/en"&gt;Thomson Reuters&lt;/a&gt;, &lt;a href="https://www.cch.com/"&gt;CCH&lt;/a&gt;, and &lt;a href="https://www.intuit.com/"&gt;Intuit&lt;/a&gt; have deep customer relationships built over decades.&lt;/p&gt;&lt;p&gt;But the founders see opportunity in the transition moment.&lt;/p&gt;&lt;p&gt;&amp;quot;AI has changed what‚Äôs possible in the industry,&amp;quot; Shah said. &amp;quot;We are going to work with and integrate with some technology players in the industry and also compete with other players with new products powered by AI. In some cases we are going to forget about the existing technology solution for doing things and go back to the task itself. We have totally new technological capabilities ‚Äî how would you design something from a blank canvas that works with humans to accomplish that task?&amp;quot;&amp;quot;&lt;/p&gt;&lt;p&gt;The decision to offer a free research tool reflects Marble&amp;#x27;s go-to-market philosophy. By giving practitioners access without a paywall, the company aims to build trust and demonstrate capability.&lt;/p&gt;&lt;p&gt;&amp;quot;It allows us to expose a really compelling product that is purpose-built to those that are worried about how to use AI or question how to adopt it.¬† Now they don‚Äôt have to think about purchasing something that is cost-prohibitive when they don&amp;#x27;t know how to integrate it into their workflow,&amp;quot; Shah said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The $250 billion question: Can a startup transform how America does its taxes?&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Marble&amp;#x27;s roadmap extends beyond research. The company plans to develop AI agents capable of analyzing complex tax scenarios, identifying compliance issues, and eventually automating significant portions of compliance workflows ‚Äî all while keeping practitioners in control.&lt;/p&gt;&lt;p&gt;The founders frame success not in terms of disruption but rebalancing. Today&amp;#x27;s tax work skews heavily toward compliance, leaving the strategic advisory services that clients crave ‚Äî and that generate higher margins‚Äîperpetually undone. Marble&amp;#x27;s bet is that AI can flip that equation.&lt;/p&gt;&lt;p&gt;&amp;quot;Everyone wants it to look more like compliance is done simpler, and you spend time talking about strategy and planning,&amp;quot; Konrad said. &amp;quot;How do we change that blend of compliance versus strategy and planning to strategy and planning first‚Äîwith compliance as something that has been made dramatically simpler?&amp;quot;&lt;/p&gt;&lt;p&gt;Whether Marble can execute on that vision remains to be seen. The company faces entrenched competitors, a profession that has historically resisted technological change, and the inherent unpredictability of building AI systems for high-stakes financial work.&lt;/p&gt;&lt;p&gt;But the founders are betting that the industry&amp;#x27;s demographic shift will accelerate adoption in ways that previous technology waves could not. With fewer accountants entering the profession each year and client demands only growing, firms may have an increased appetite to embrace tools that let their remaining staff do more.&lt;/p&gt;&lt;p&gt;&amp;quot;AI is going to change every industry ‚Äî in some cases in ways that will help business models and in some cases in ways that will challenge them. We believe AI is ultimately going to make accounting firms‚Äô businesses better and more profitable and at the same time end clients will get better services at better prices,&amp;quot; Shah said.&lt;/p&gt;&lt;p&gt;The accounting profession, it seems, is about to find out which side of that equation it lands on.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="http://marble.ai/"&gt;Marble&lt;/a&gt;, a startup building artificial intelligence agents for tax professionals, has raised $9 million in seed funding as the accounting industry grapples with a deepening labor shortage and mounting regulatory complexity.&lt;/p&gt;&lt;p&gt;The round, led by &lt;a href="https://www.susaventures.com/"&gt;Susa Ventures&lt;/a&gt; with participation from &lt;a href="https://mxv.vc/"&gt;MXV Capital&lt;/a&gt; and Konrad Capital, positions Marble to compete in a market where AI adoption has lagged significantly behind other knowledge industries like law and software development.&lt;/p&gt;&lt;p&gt;&amp;quot;When we looked at the economy and asked ourselves where AI is going to transform the way businesses operate, we focused on knowledge industries ‚Äî specifically businesses with hourly fee-based service models,&amp;quot; said Bhavin Shah, Marble&amp;#x27;s chief executive officer, in an exclusive interview with VentureBeat. &amp;quot;Accounting generates $250 billion in fee-based billing in the US every year. There&amp;#x27;s a tremendous opportunity to increase efficiency and improve margins for accounting firms.&amp;quot;&lt;/p&gt;&lt;p&gt;The company has launched a &lt;a href="https://marble.ai/"&gt;free AI-powered tax research tool&lt;/a&gt; on its website that converts complex government tax data into accessible, citation-backed answers for practitioners. Marble plans to expand into AI agents that can analyze compliance scenarios and eventually automate portions of tax preparation workflows.&lt;/p&gt;&lt;p&gt;Marble&amp;#x27;s backers share Shah&amp;#x27;s conviction about the market. &amp;quot;Marble is rethinking the accounting system from the ground up. Accounting is one of the biggest ‚Äî and most overlooked ‚Äî markets in professional services,&amp;quot; Chad Byers, general partner at Susa Ventures, told VentureBeat. &amp;quot;We&amp;#x27;ve known Bhavin from his time as an executive in the Susa portfolio, and have seen firsthand how sharp and execution-driven he is. He and Geordie bring the perfect mix of operational depth and product instinct to a space long overdue for change ‚Äî and they see the same massive opportunity we do.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The accounting industry lost 340,000 workers in four years ‚Äî and replacements aren&amp;#x27;t coming&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Marble enters a market shaped by structural forces that have fundamentally altered the economics of professional accounting.&lt;/p&gt;&lt;p&gt;The accounting profession has &lt;a href="https://www.bls.gov/ooh/office-and-administrative-support/bookkeeping-accounting-and-auditing-clerks.htm"&gt;shed roughly 340,000 workers since 2019&lt;/a&gt;, a 17% decline that has left firms scrambling to meet client demands. First-time candidates for the Certified Public Accountant exam dropped 33% between 2016 and 2021, according to &lt;a href="https://www.cpajournal.com/2025/08/15/the-accounting-profession-is-in-crisis-3/"&gt;AICPA data&lt;/a&gt;, and 2022 saw the lowest number of exam takers in 17 years.&lt;/p&gt;&lt;p&gt;The exodus comes as baby boomers exit en masse. The American Institute of CPAs estimates that approximately &lt;a href="https://www.forbes.com/councils/forbesfinancecouncil/2025/05/12/disruption-in-accounting-the-cpa-shortage-meets-the-rise-of-ai/"&gt;75% of all licensed CPAs&lt;/a&gt; reached retirement age by 2019, creating a demographic cliff that the profession has struggled to address.&lt;/p&gt;&lt;p&gt;‚ÄúFewer CPAs are getting certified year over year,&amp;quot; Shah said. &amp;quot;The industry is compressing at the same time that there&amp;#x27;s more work to be done and the tax code is getting more complicated.&amp;quot;&lt;/p&gt;&lt;p&gt;The &lt;a href="https://www.accountingpipeline.org/"&gt;National Pipeline Advisory Group&lt;/a&gt;, a multi-stakeholder body formed by the AICPA in July 2023, released a report identifying the &lt;a href="https://sc.cpa/2025/01/10/national-pipeline-advisory-group-npag-releases-official-report/"&gt;150-hour education requirement&lt;/a&gt; for CPA licensure as a significant barrier to entry. A separate &lt;a href="https://www.thecaq.org/increasing-diversity-in-the-accounting-profession-pipeline-2022"&gt;survey&lt;/a&gt; by the Center for Audit Quality found that 57% of business majors who chose not to pursue accounting cited the additional credit hours as a deterrent.&lt;/p&gt;&lt;p&gt;Recent legislative changes reflect the urgency. Ohio now offers alternatives to the 150-hour requirement, signaling that states are willing to experiment with pathways that could reverse enrollment declines.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why AI transformed law and software development but left accounting behind&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Despite the profession&amp;#x27;s challenges, AI adoption in accounting has moved more slowly than in adjacent knowledge industries. &lt;a href="https://www.harvey.ai/"&gt;Harvey&lt;/a&gt; and &lt;a href="https://legora.com/"&gt;Legora&lt;/a&gt; have raised hundreds of millions to bring AI to legal work. &lt;a href="https://cursor.com/agents"&gt;Cursor&lt;/a&gt; and other coding assistants have transformed software development. Accounting, by contrast, remains largely dependent on legacy research platforms and manual processes.&lt;/p&gt;&lt;p&gt;Geordie Konrad, Marble&amp;#x27;s executive chairman and a co-founder of restaurant software company TouchBistro, attributes the gap to how people conceptualize AI&amp;#x27;s capabilities.&lt;/p&gt;&lt;p&gt;‚ÄúIt was obvious to many people that LLMs could do meaningful work by manipulating code for software developers and manipulating words for lawyers. In the accounting industry, LLMs are going to be used as reasoning agents,&amp;quot; Konrad said. &amp;quot; That requires a bit more of a two-step analysis to see why it&amp;#x27;s a big opportunity.&amp;quot;&lt;/p&gt;&lt;p&gt;The technical challenge is substantial. Tax regulations form one of the most complex, interconnected information systems that humans have created ‚Äî tens of thousands of interlocking rules, guidance documents, and jurisdiction-specific requirements that frequently overlap or conflict.&lt;/p&gt;&lt;p&gt;&amp;quot;If you want to put AI through its paces and ask how far it&amp;#x27;s come in replicating cognitive functions, this is an unbelievable playground to work in,&amp;quot; Konrad said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;A dramatic shift: AI adoption among tax and finance teams doubles in one year&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Recent data suggests the accounting profession&amp;#x27;s stance toward AI is shifting rapidly.&lt;/p&gt;&lt;p&gt;A 2025 survey from &lt;a href="https://www.avalara.com/us/en/learn/ai-reinventing-finance-and-tax-state-of-finance-report.html"&gt;Hanover Research and Avalara&lt;/a&gt; found that 84% of finance and tax teams now use AI heavily in their operations, up from 47% in 2024. The 2025 Generative AI in Professional Services Report from &lt;a href="https://www.cpapracticeadvisor.com/2025/04/15/79-of-tax-and-accounting-firms-expect-significant-genai-integration-by-2027/159172/"&gt;Thomson Reuters Institute&lt;/a&gt; found that 21% of tax firms already use generative AI technology, with 53% either planning to adopt it or actively considering it.&lt;/p&gt;&lt;p&gt;Large accounting firms have invested heavily in AI infrastructure. &lt;a href="https://www.deloitte.com/us/en/what-we-do/capabilities/applied-artificial-intelligence/articles/agentic-ai-enterprise-2028.html?id=us:2ps:3gl:aisgm26:awa:CONS:nonem:K0218784:111725:kwd-2025710122928:188372336109:784136672833::Brand_AI-SGO_BU_K0218784_Google:Brand_AI-SGO-Advisory:deloitte-generative-ai:&amp;amp;gclsrc=aw.ds&amp;amp;gad_source=1&amp;amp;gad_campaignid=23269751515&amp;amp;gbraid=0AAAAADenGPA6ZvC8cLRrT031O1up6RiOj&amp;amp;gclid=Cj0KCQiAi9rJBhCYARIsALyPDtudMaZrpHm8026n16yuyeM0sYKCTm24yHqR4Dyg_s6yZBgPnxfjDOwaAvN5EALw_wcB"&gt;Deloitte&lt;/a&gt; has developed generative AI capabilities within its audit platform. &lt;a href="https://www.bdo.com/insights/press-releases/bdo-usa-unveils-comprehensive-artificial-intelligence-strategy-fusing-practical-innovation"&gt;BDO&lt;/a&gt; announced a $1B investment in AI over the next five years. &lt;a href="https://www.ey.com/en_gl/newsroom/2025/11/ey-unveils-suite-of-powerful-ai-capabilities-to-accelerate-tax-transformation-and-deliver-strategic-value"&gt;EY&lt;/a&gt; launched an AI platform combining technology with strategy, transactions, and tax services. &lt;a href="https://www.pwc.com/us/en/tech-effect/ai-analytics/ai-predictions.html"&gt;PwC&lt;/a&gt; estimates a complete AI-driven audit solution will launch by 2026.&lt;/p&gt;&lt;p&gt;But adoption at smaller firms remains uneven. According to &lt;a href="http://thomsonreuters.com/en-us/posts/technology/genai-professional-services-report-2025/"&gt;Thomson Reuters research&lt;/a&gt;, 52% of tax firm respondents who use generative AI rely on open-source tools like ChatGPT rather than industry-specific solutions‚Äîa pattern that could shift as purpose-built alternatives emerge.&lt;/p&gt;&lt;p&gt;Marble&amp;#x27;s founders believe the hesitance stems not from technophobia but from a lack of compelling options.&lt;/p&gt;&lt;p&gt;‚ÄúFirms want to embrace AI,&amp;quot; Shah said. ‚ÄúThey just haven&amp;#x27;t seen great software and tooling made for them. That&amp;#x27;s part of the opportunity ‚Äî to work with them and build something they&amp;#x27;re excited to use on a day-to-day basis.‚Äù&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Can artificial intelligence rescue accounting&amp;#x27;s billable-hour business model?&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;AI&amp;#x27;s arrival in accounting raises questions about the profession&amp;#x27;s billing structure.&lt;/p&gt;&lt;p&gt;Accounting firms have traditionally generated profits by billing clients for staff time, often at multiples of employee compensation costs. Junior associates performing compliance work represent a significant revenue stream. If AI can automate that work, does it undercut the business model firms depend on?&lt;/p&gt;&lt;p&gt;Marble&amp;#x27;s founders argue the opposite. The chronic staffing shortage has already constrained firms&amp;#x27; ability to capture available revenue. Advisory and consulting work ‚Äî higher-margin services that clients actively want ‚Äî goes undone because practitioners are buried in compliance tasks.&lt;/p&gt;&lt;p&gt;&amp;quot;Everyone in the industry agrees that an enormous amount of advisory work simply isn&amp;#x27;t getting done,&amp;quot; Konrad said. &amp;quot;Customers want it. Firms want to do it because it&amp;#x27;s high-margin, great work. But nobody gets to it.&amp;quot;&lt;/p&gt;&lt;p&gt;The &lt;a href="https://www.aicpa-cima.com/news/article/cpa-firms-report-steady-growth-in-revenue-and-profit-aicpa-research-finds"&gt;2025 AICPA National Management of an Accounting Practice Survey&lt;/a&gt; supports this view. Firms reported a median 6.7% increase in net client fees over the prior year, with growth in audit, assurance, tax services, and client accounting advisory. Net remaining per partner climbed 11.9% from fiscal year 2022 to fiscal year 2024, reaching $252,663.&lt;/p&gt;&lt;p&gt;The survey also found growing interest in AI adoption, though most firms have yet to allocate formal budgets or develop structured training programs. Continued adoption, the survey suggested, could help expand services and fuel continued growth.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Accountants won&amp;#x27;t adopt AI tools they can&amp;#x27;t trust with sensitive client data&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;For AI to succeed in accounting, it must clear a high bar for data security. Accounting firms handle some of the most sensitive financial information in the economy. Practitioners cannot adopt tools that create compliance or confidentiality risks.&lt;/p&gt;&lt;p&gt;According to &lt;a href="https://www.avalara.com/us/en/learn/ai-reinventing-finance-and-tax-state-of-finance-report.html"&gt;Avalara&amp;#x27;s survey&lt;/a&gt;, 63% of respondents cited data security and privacy concerns as the top barriers to automating tax and finance functions. The concern persists throughout the adoption lifecycle, from initial selection through implementation and ongoing use.&lt;/p&gt;&lt;p&gt;&lt;a href="https://marble.ai/"&gt;Marble&lt;/a&gt; has made security a foundational priority. The company obtained software compliance certification before releasing any product and maintains that data privacy is embedded in its operational culture from day one.&lt;/p&gt;&lt;p&gt;&amp;quot;Security is at the core of what we are building,&amp;quot; Shah said. &amp;quot;Every employee knows that security is critical. It&amp;#x27;s a part of our onboarding and something that we consider in everything we do.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;From number crunchers to strategic advisors: How AI could reshape accounting careers&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Marble&amp;#x27;s founders reject the narrative that AI will only take away from accounting jobs. They propose instead that AI will result in accounting jobs becoming more strategic and less characterized by repetitive execution.¬†&lt;/p&gt;&lt;p&gt;They draw an analogy to architecture, where computer-aided design replaced laborious manual drafting. Architects did not disappear ‚Äî they gained tools that let them spend more time on creative design and less on mechanical reproduction.&lt;/p&gt;&lt;p&gt;&amp;quot;If you take some of the hours-intensive, less creative work out of what being a junior or intermediate accountant is, and you replace it with a role where you&amp;#x27;re a professional who is being creative, synthesizing ideas, and able to delegate a lot of tasks to AI assistant platform solutions, you end up with an industry that&amp;#x27;s just a lot more fun to operate in,&amp;quot; Konrad said.&lt;/p&gt;&lt;p&gt;The shift could also improve client outcomes. When accountants spend less time on compliance, they can invest more in the strategic advisory work that clients value.&lt;/p&gt;&lt;p&gt;&amp;quot;Not only does the work become more enjoyable because of what you can focus on, but that&amp;#x27;s also what your clients are going to value more from you,&amp;quot; Shah said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The competitive landscape: Marble faces well-funded rivals and legacy giants&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;a href="http://marble.ai/"&gt;Marble&lt;/a&gt; enters a market with formidable incumbents and well-funded competitors. &lt;a href="https://venturebeat.com/ai/how-ai-tax-startup-blue-j-torched-its-entire-business-model-for-chatgpt-and"&gt;BlueJ&lt;/a&gt;, a global tax research platform, has raised over $100 million. &lt;a href="https://www.thomsonreuters.com/en"&gt;Thomson Reuters&lt;/a&gt;, &lt;a href="https://www.cch.com/"&gt;CCH&lt;/a&gt;, and &lt;a href="https://www.intuit.com/"&gt;Intuit&lt;/a&gt; have deep customer relationships built over decades.&lt;/p&gt;&lt;p&gt;But the founders see opportunity in the transition moment.&lt;/p&gt;&lt;p&gt;&amp;quot;AI has changed what‚Äôs possible in the industry,&amp;quot; Shah said. &amp;quot;We are going to work with and integrate with some technology players in the industry and also compete with other players with new products powered by AI. In some cases we are going to forget about the existing technology solution for doing things and go back to the task itself. We have totally new technological capabilities ‚Äî how would you design something from a blank canvas that works with humans to accomplish that task?&amp;quot;&amp;quot;&lt;/p&gt;&lt;p&gt;The decision to offer a free research tool reflects Marble&amp;#x27;s go-to-market philosophy. By giving practitioners access without a paywall, the company aims to build trust and demonstrate capability.&lt;/p&gt;&lt;p&gt;&amp;quot;It allows us to expose a really compelling product that is purpose-built to those that are worried about how to use AI or question how to adopt it.¬† Now they don‚Äôt have to think about purchasing something that is cost-prohibitive when they don&amp;#x27;t know how to integrate it into their workflow,&amp;quot; Shah said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The $250 billion question: Can a startup transform how America does its taxes?&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Marble&amp;#x27;s roadmap extends beyond research. The company plans to develop AI agents capable of analyzing complex tax scenarios, identifying compliance issues, and eventually automating significant portions of compliance workflows ‚Äî all while keeping practitioners in control.&lt;/p&gt;&lt;p&gt;The founders frame success not in terms of disruption but rebalancing. Today&amp;#x27;s tax work skews heavily toward compliance, leaving the strategic advisory services that clients crave ‚Äî and that generate higher margins‚Äîperpetually undone. Marble&amp;#x27;s bet is that AI can flip that equation.&lt;/p&gt;&lt;p&gt;&amp;quot;Everyone wants it to look more like compliance is done simpler, and you spend time talking about strategy and planning,&amp;quot; Konrad said. &amp;quot;How do we change that blend of compliance versus strategy and planning to strategy and planning first‚Äîwith compliance as something that has been made dramatically simpler?&amp;quot;&lt;/p&gt;&lt;p&gt;Whether Marble can execute on that vision remains to be seen. The company faces entrenched competitors, a profession that has historically resisted technological change, and the inherent unpredictability of building AI systems for high-stakes financial work.&lt;/p&gt;&lt;p&gt;But the founders are betting that the industry&amp;#x27;s demographic shift will accelerate adoption in ways that previous technology waves could not. With fewer accountants entering the profession each year and client demands only growing, firms may have an increased appetite to embrace tools that let their remaining staff do more.&lt;/p&gt;&lt;p&gt;&amp;quot;AI is going to change every industry ‚Äî in some cases in ways that will help business models and in some cases in ways that will challenge them. We believe AI is ultimately going to make accounting firms‚Äô businesses better and more profitable and at the same time end clients will get better services at better prices,&amp;quot; Shah said.&lt;/p&gt;&lt;p&gt;The accounting profession, it seems, is about to find out which side of that equation it lands on.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/marble-enters-the-race-to-bring-ai-to-tax-work-armed-with-usd9-million-and-a</guid><pubDate>Thu, 11 Dec 2025 14:00:00 +0000</pubDate></item><item><title>Ride Into Adventure With Capcom‚Äôs ‚ÄòMonster Hunter Stories‚Äô Series in the Cloud (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/geforce-now-thursday-monster-hunter-stories/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Hunters, saddle up ‚Äî adventure awaits in the cloud.&lt;/p&gt;
&lt;p&gt;Journey into the world of &lt;i&gt;Monster Hunter Stories&lt;/i&gt; as Capcom‚Äôs acclaimed role-playing classics join GeForce NOW.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Monster Hunter Stories &lt;/i&gt;and &lt;i&gt;Monster Hunter Stories 2: Wings of Ruin&lt;/i&gt; are soaring into the cloud this week, bringing colorful worlds, charming companions and turn-based monster battles across devices.&lt;/p&gt;
&lt;p&gt;They lead seven new games joining the cloud this week, on top of an &lt;i&gt;ARC Raiders&lt;/i&gt; ‚ÄúElectrician Backpack: Emerald Wave Variant‚Äù reward for Ultimate members who want to drop into battle in style.&lt;/p&gt;
&lt;p&gt;It‚Äôs also been a big year for games, and this year‚Äôs major gaming awards nominees show just how strong gaming is right now ‚Äî with many of those fan-favorite titles playable on GeForce NOW, no downloads required.&lt;/p&gt;
&lt;p&gt;Look for the ‚ÄúThe Game Awards‚Äù row in the GeForce NOW app to dive in instantly.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Saddle Up&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Capcom‚Äôs &lt;i&gt;Monster Hunter Stories&lt;/i&gt; and &lt;i&gt;Monster Hunter Stories 2: Wings of Ruin&lt;/i&gt; arrive in the cloud this week. Members can explore vibrant worlds, bond with quirky monsters and experience turn-based role-playing game (RPG) adventures across devices.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88239"&gt;&lt;img alt="Monster Hunter Stories on GeForce NOW" class="size-large wp-image-88239" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/GFN_Thursday-Monster_Hunter_Stories-1680x945.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88239"&gt;&lt;em&gt;Saddle up for egg-citement.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;In &lt;i&gt;Monster Hunter Stories,&lt;/i&gt; an RPG that expands the &lt;i&gt;Monster Hunter &lt;/i&gt;world, players are no longer hunting monsters but raising them. In this story featuring heroes known as Monster Riders, players live alongside monsters and form lifelong bonds with them.&lt;/p&gt;
&lt;p&gt;The first installment of the &lt;i&gt;Monster Hunter Stories&lt;/i&gt; series returns, fully voiced in Japanese and English, with additional features such as a new museum mode where players can listen to music and view concept art ‚Äî offering an even deeper dive into the world of &lt;i&gt;Monster Hunter Stories.&lt;/i&gt;&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88245"&gt;&lt;img alt="monster hunster stories 2 on gfn" class="size-large wp-image-88245" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/09_NL_Rush_ScreenShot_00-1-1680x945.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88245"&gt;&lt;em&gt;When fate calls, answer on a dragon‚Äôs back.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;A new adventure awaits in &lt;i&gt;Monster Hunter Stories 2: Wings of Ruin: &lt;/i&gt;the second installment of the turn-based RPG series. Become a Monster Rider and form bonds with friendly monsters known as Monsties to fight alongside them in the game‚Äôs epic story.&lt;/p&gt;
&lt;p&gt;These adventures can be enjoyed on almost any device, powered by high-performance GeForce RTX technology. Seamlessly switch between phones, laptops and desktops, and experience every lush landscape and thrilling battle with cloud-streamed visuals and smooth gameplay ‚Äî no downloads, installs or upgrades required.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;And the Cloud Goes to ‚Ä¶ GeForce NOW&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;It‚Äôs a big month for games, and this year‚Äôs major gaming awards make it an especially great time to be a gamer. Many of the buzziest nominees and fan-favorite titles are playable on GeForce NOW, where it‚Äôs easy to jump into the action, catch up on the hits and see what the hype‚Äôs all about ‚Äî all instantly, no downloads required.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88233"&gt;&lt;img alt="Clair Obscure on GeForce NOW" class="size-large wp-image-88233" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/GFN_Thursday-Clair_Obscur_Expedition_33-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88233"&gt;&lt;em&gt;What a game.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;A stack of nominated titles are available in the cloud through GeForce NOW, including a majority of Game of the Year (GOTY) contenders like &lt;i&gt;Clair Obscur: Expedition 33&lt;/i&gt;, &lt;i&gt;Hollow Knight: Silksong&lt;/i&gt; and &lt;i&gt;Kingdom Come: Deliverance II&lt;/i&gt;.&lt;/p&gt;
&lt;p&gt;RPG fans can marathon some of the year‚Äôs best titles in the genre through the cloud. &lt;i&gt;Avowed, Clair Obscur: Expedition 33, Kingdom Come: Deliverance II, Monster Hunter Wilds &lt;/i&gt;and &lt;i&gt;The Outer Worlds 2&lt;/i&gt; are all streamable on GeForce NOW.‚Äã&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88242"&gt;&lt;img alt="Silksong on GeForce NOW" class="size-large wp-image-88242" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/GFN_Thursday-Silksong-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88242"&gt;&lt;em&gt;A masterpiece.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Members can also dive into other nominated favorites such as &lt;i&gt;Battlefield 6&lt;/i&gt; and &lt;i&gt;DOOM: The Dark Ages&lt;/i&gt; for Best Action, &lt;i&gt;Indiana Jones and the Ancient Circle&lt;/i&gt; and &lt;i&gt;Split Fiction&lt;/i&gt; for Best Adventure, &lt;i&gt;The Alters&lt;/i&gt; and &lt;i&gt;Sid Meier‚Äôs Civilization VII&lt;/i&gt; for Best Sim/Strategy,&lt;i&gt; ARC Raiders&lt;/i&gt; and &lt;i&gt;PEAK &lt;/i&gt;for Best Multiplayer, plus esports staples like &lt;i&gt;Counter-Strike 2 and DOTA 2&lt;/i&gt;.&lt;/p&gt;
&lt;p&gt;Whether chasing GOTY, exploring indies like &lt;i&gt;Blue Prince&lt;/i&gt; and &lt;i&gt;Hollow Knight: Silksong&lt;/i&gt;, or sticking to long-running hits like &lt;i&gt;Fortnite &lt;/i&gt;and&lt;i&gt; No Man‚Äôs Sky&lt;/i&gt;, gamers can always find a top title ready to stream.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Loot, Shoot and Look Good Doing It&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88236"&gt;&lt;img alt="ARC Raiders reward on GeForce NOW" class="size-large wp-image-88236" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/GFN_Thursday-ARC_Raiders_GFN_Members_Reward-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88236"&gt;&lt;em&gt;Pack up and stand out.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Ultimate members can claim the &lt;i&gt;ARC Raiders&lt;/i&gt; ‚ÄúElectrician Backpack: Emerald Wave Variant‚Äù ‚Äî an in-game cosmetic item that adds a distinct look. The Emerald Wave design offers a clean, modern touch for Raiders ready to stand out during extraction missions.‚Äã‚Äã&lt;/p&gt;
&lt;p&gt;Jump into battle with style, powered by GeForce RTX 5080-class servers on GeForce NOW, delivering up to 2.8x higher frame rates and a new Cinematic-Quality Streaming mode that makes every firefight shine.&lt;/p&gt;
&lt;p&gt;The reward is available to Ultimate members through Sunday, Jan. 4, 2026, or while supplies last. Keep an eye on email for instructions to redeem this stylish advantage for the journey ahead. Once claimed, navigate to the Electrician Backpack and select the Emerald Wave color variant to equip it and head off in style.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Fresh Crop of Games&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88227"&gt;&lt;img alt="Everdream village on GeForce NOW" class="size-large wp-image-88227" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/GFN_Thursday-Everdream_Village-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88227"&gt;&lt;em&gt;Where the cows know your name.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;i&gt;Everdream Village&lt;/i&gt;, a cozy farming adventure from publisher Untold Tales, lets players turn a sleepy island settlement into a thriving, story-filled village. Tend crops, befriend quirky villagers and wrangle a menagerie of charming animals while terraforming the land and sail off to discover new magical islands ‚Äî all while shaping a laid-back little paradise.&lt;/p&gt;
&lt;p&gt;In addition, members can look for the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Skate Story &lt;/i&gt;(New release on Steam, Dec. 8)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Dome Keeper &lt;/i&gt;(New release on Xbox, available on Game Pass, Dec. 9)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Death Howl &lt;/i&gt;(New release on Steam and Xbox, available on Game Pass, Dec. 9)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;RuneQuest: Warlords&lt;/i&gt; (New release on Steam, Dec. 9)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Everdream Village &lt;/i&gt;(New release on Steam, Dec. 12)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Monster Hunter Stories &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Monster Hunter Stories 2 &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;GeForce RTX 5080-ready games:&lt;/p&gt;

&lt;p&gt;What are you planning to play this weekend? Let us know on X or in the comments below.&lt;/p&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p dir="ltr" lang="en"&gt;What's on the top of your gaming wishlist this holiday? ‚úçÔ∏è‚ùÑÔ∏è&lt;/p&gt;
&lt;p&gt;‚Äî üå©Ô∏è NVIDIA GeForce NOW (@NVIDIAGFN) December 10, 2025&lt;/p&gt;&lt;/blockquote&gt;


		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Hunters, saddle up ‚Äî adventure awaits in the cloud.&lt;/p&gt;
&lt;p&gt;Journey into the world of &lt;i&gt;Monster Hunter Stories&lt;/i&gt; as Capcom‚Äôs acclaimed role-playing classics join GeForce NOW.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Monster Hunter Stories &lt;/i&gt;and &lt;i&gt;Monster Hunter Stories 2: Wings of Ruin&lt;/i&gt; are soaring into the cloud this week, bringing colorful worlds, charming companions and turn-based monster battles across devices.&lt;/p&gt;
&lt;p&gt;They lead seven new games joining the cloud this week, on top of an &lt;i&gt;ARC Raiders&lt;/i&gt; ‚ÄúElectrician Backpack: Emerald Wave Variant‚Äù reward for Ultimate members who want to drop into battle in style.&lt;/p&gt;
&lt;p&gt;It‚Äôs also been a big year for games, and this year‚Äôs major gaming awards nominees show just how strong gaming is right now ‚Äî with many of those fan-favorite titles playable on GeForce NOW, no downloads required.&lt;/p&gt;
&lt;p&gt;Look for the ‚ÄúThe Game Awards‚Äù row in the GeForce NOW app to dive in instantly.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Saddle Up&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Capcom‚Äôs &lt;i&gt;Monster Hunter Stories&lt;/i&gt; and &lt;i&gt;Monster Hunter Stories 2: Wings of Ruin&lt;/i&gt; arrive in the cloud this week. Members can explore vibrant worlds, bond with quirky monsters and experience turn-based role-playing game (RPG) adventures across devices.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88239"&gt;&lt;img alt="Monster Hunter Stories on GeForce NOW" class="size-large wp-image-88239" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/GFN_Thursday-Monster_Hunter_Stories-1680x945.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88239"&gt;&lt;em&gt;Saddle up for egg-citement.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;In &lt;i&gt;Monster Hunter Stories,&lt;/i&gt; an RPG that expands the &lt;i&gt;Monster Hunter &lt;/i&gt;world, players are no longer hunting monsters but raising them. In this story featuring heroes known as Monster Riders, players live alongside monsters and form lifelong bonds with them.&lt;/p&gt;
&lt;p&gt;The first installment of the &lt;i&gt;Monster Hunter Stories&lt;/i&gt; series returns, fully voiced in Japanese and English, with additional features such as a new museum mode where players can listen to music and view concept art ‚Äî offering an even deeper dive into the world of &lt;i&gt;Monster Hunter Stories.&lt;/i&gt;&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88245"&gt;&lt;img alt="monster hunster stories 2 on gfn" class="size-large wp-image-88245" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/09_NL_Rush_ScreenShot_00-1-1680x945.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88245"&gt;&lt;em&gt;When fate calls, answer on a dragon‚Äôs back.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;A new adventure awaits in &lt;i&gt;Monster Hunter Stories 2: Wings of Ruin: &lt;/i&gt;the second installment of the turn-based RPG series. Become a Monster Rider and form bonds with friendly monsters known as Monsties to fight alongside them in the game‚Äôs epic story.&lt;/p&gt;
&lt;p&gt;These adventures can be enjoyed on almost any device, powered by high-performance GeForce RTX technology. Seamlessly switch between phones, laptops and desktops, and experience every lush landscape and thrilling battle with cloud-streamed visuals and smooth gameplay ‚Äî no downloads, installs or upgrades required.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;And the Cloud Goes to ‚Ä¶ GeForce NOW&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;It‚Äôs a big month for games, and this year‚Äôs major gaming awards make it an especially great time to be a gamer. Many of the buzziest nominees and fan-favorite titles are playable on GeForce NOW, where it‚Äôs easy to jump into the action, catch up on the hits and see what the hype‚Äôs all about ‚Äî all instantly, no downloads required.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88233"&gt;&lt;img alt="Clair Obscure on GeForce NOW" class="size-large wp-image-88233" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/GFN_Thursday-Clair_Obscur_Expedition_33-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88233"&gt;&lt;em&gt;What a game.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;A stack of nominated titles are available in the cloud through GeForce NOW, including a majority of Game of the Year (GOTY) contenders like &lt;i&gt;Clair Obscur: Expedition 33&lt;/i&gt;, &lt;i&gt;Hollow Knight: Silksong&lt;/i&gt; and &lt;i&gt;Kingdom Come: Deliverance II&lt;/i&gt;.&lt;/p&gt;
&lt;p&gt;RPG fans can marathon some of the year‚Äôs best titles in the genre through the cloud. &lt;i&gt;Avowed, Clair Obscur: Expedition 33, Kingdom Come: Deliverance II, Monster Hunter Wilds &lt;/i&gt;and &lt;i&gt;The Outer Worlds 2&lt;/i&gt; are all streamable on GeForce NOW.‚Äã&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88242"&gt;&lt;img alt="Silksong on GeForce NOW" class="size-large wp-image-88242" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/GFN_Thursday-Silksong-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88242"&gt;&lt;em&gt;A masterpiece.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Members can also dive into other nominated favorites such as &lt;i&gt;Battlefield 6&lt;/i&gt; and &lt;i&gt;DOOM: The Dark Ages&lt;/i&gt; for Best Action, &lt;i&gt;Indiana Jones and the Ancient Circle&lt;/i&gt; and &lt;i&gt;Split Fiction&lt;/i&gt; for Best Adventure, &lt;i&gt;The Alters&lt;/i&gt; and &lt;i&gt;Sid Meier‚Äôs Civilization VII&lt;/i&gt; for Best Sim/Strategy,&lt;i&gt; ARC Raiders&lt;/i&gt; and &lt;i&gt;PEAK &lt;/i&gt;for Best Multiplayer, plus esports staples like &lt;i&gt;Counter-Strike 2 and DOTA 2&lt;/i&gt;.&lt;/p&gt;
&lt;p&gt;Whether chasing GOTY, exploring indies like &lt;i&gt;Blue Prince&lt;/i&gt; and &lt;i&gt;Hollow Knight: Silksong&lt;/i&gt;, or sticking to long-running hits like &lt;i&gt;Fortnite &lt;/i&gt;and&lt;i&gt; No Man‚Äôs Sky&lt;/i&gt;, gamers can always find a top title ready to stream.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Loot, Shoot and Look Good Doing It&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88236"&gt;&lt;img alt="ARC Raiders reward on GeForce NOW" class="size-large wp-image-88236" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/GFN_Thursday-ARC_Raiders_GFN_Members_Reward-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88236"&gt;&lt;em&gt;Pack up and stand out.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Ultimate members can claim the &lt;i&gt;ARC Raiders&lt;/i&gt; ‚ÄúElectrician Backpack: Emerald Wave Variant‚Äù ‚Äî an in-game cosmetic item that adds a distinct look. The Emerald Wave design offers a clean, modern touch for Raiders ready to stand out during extraction missions.‚Äã‚Äã&lt;/p&gt;
&lt;p&gt;Jump into battle with style, powered by GeForce RTX 5080-class servers on GeForce NOW, delivering up to 2.8x higher frame rates and a new Cinematic-Quality Streaming mode that makes every firefight shine.&lt;/p&gt;
&lt;p&gt;The reward is available to Ultimate members through Sunday, Jan. 4, 2026, or while supplies last. Keep an eye on email for instructions to redeem this stylish advantage for the journey ahead. Once claimed, navigate to the Electrician Backpack and select the Emerald Wave color variant to equip it and head off in style.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Fresh Crop of Games&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88227"&gt;&lt;img alt="Everdream village on GeForce NOW" class="size-large wp-image-88227" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/GFN_Thursday-Everdream_Village-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88227"&gt;&lt;em&gt;Where the cows know your name.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;i&gt;Everdream Village&lt;/i&gt;, a cozy farming adventure from publisher Untold Tales, lets players turn a sleepy island settlement into a thriving, story-filled village. Tend crops, befriend quirky villagers and wrangle a menagerie of charming animals while terraforming the land and sail off to discover new magical islands ‚Äî all while shaping a laid-back little paradise.&lt;/p&gt;
&lt;p&gt;In addition, members can look for the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Skate Story &lt;/i&gt;(New release on Steam, Dec. 8)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Dome Keeper &lt;/i&gt;(New release on Xbox, available on Game Pass, Dec. 9)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Death Howl &lt;/i&gt;(New release on Steam and Xbox, available on Game Pass, Dec. 9)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;RuneQuest: Warlords&lt;/i&gt; (New release on Steam, Dec. 9)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Everdream Village &lt;/i&gt;(New release on Steam, Dec. 12)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Monster Hunter Stories &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Monster Hunter Stories 2 &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;GeForce RTX 5080-ready games:&lt;/p&gt;

&lt;p&gt;What are you planning to play this weekend? Let us know on X or in the comments below.&lt;/p&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p dir="ltr" lang="en"&gt;What's on the top of your gaming wishlist this holiday? ‚úçÔ∏è‚ùÑÔ∏è&lt;/p&gt;
&lt;p&gt;‚Äî üå©Ô∏è NVIDIA GeForce NOW (@NVIDIAGFN) December 10, 2025&lt;/p&gt;&lt;/blockquote&gt;


		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/geforce-now-thursday-monster-hunter-stories/</guid><pubDate>Thu, 11 Dec 2025 14:00:09 +0000</pubDate></item><item><title>Microsoft ‚ÄòPromptions‚Äô fix AI prompts failing to deliver (AI News)</title><link>https://www.artificialintelligence-news.com/news/microsoft-promptions-fix-ai-prompts-failing-to-deliver/</link><description>&lt;p&gt;Microsoft believes it has a fix for AI prompts being given, the response missing the mark, and the cycle repeating.&lt;/p&gt;&lt;p&gt;This inefficiency is a drain on resources. The ‚Äútrial-and-error loop can feel unpredictable and discouraging,‚Äù turning what should be a productivity booster into a time sink. Knowledge workers often spend more time managing the interaction itself than understanding the material they hoped to learn.&lt;/p&gt;&lt;p&gt;Microsoft has released Promptions (prompt + options), a UI framework designed to address this friction by replacing vague natural language requests with precise, dynamic interface controls. The open-source tool offers a method to standardise how workforces interact with large language models (LLMs), moving away from unstructured chat toward guided and reliable workflows.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-comprehension-bottleneck"&gt;The comprehension bottleneck&lt;/h3&gt;&lt;p&gt;Public attention often centres on AI producing text or images, but a massive component of enterprise usage involves understanding‚Äîasking AI to explain, clarify, or teach. This distinction is vital for internal tooling.&lt;/p&gt;&lt;p&gt;Consider a spreadsheet formula: one user may want a simple syntax breakdown, another a debugging guide, and another an explanation suitable for teaching colleagues. The same formula can require entirely different explanations depending on the user‚Äôs role, expertise, and goals.&lt;/p&gt;&lt;p&gt;Current chat interfaces rarely capture this intent effectively. Users often find that the way they phrase a question doesn‚Äôt match the level of detail the AI needs. ‚ÄúClarifying what they really want can require long, carefully worded prompts that are tiring to produce,‚Äù Microsoft explains.&lt;/p&gt;&lt;p&gt;Promptions operates as a middleware layer to fix this familiar issue with AI prompts. Instead of forcing users to type lengthy specifications, the system analyses the intent and conversation history to generate clickable options ‚Äì such as explanation length, tone, or specific focus areas ‚Äì in real-time.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-efficiency-vs-complexity"&gt;Efficiency vs complexity&lt;/h3&gt;&lt;p&gt;Microsoft researchers tested this approach by comparing static controls against the new dynamic system. The findings offer a realistic view of how such tools function in a live environment.&lt;/p&gt;&lt;p&gt;Participants consistently reported that dynamic controls made it easier to express the specifics of their tasks without repeatedly rephrasing their prompts. This reduced the effort of prompt engineering and allowed users to focus more on understanding content than managing the mechanics of phrasing. By surfacing options like ‚ÄúLearning Objective‚Äù and ‚ÄúResponse Format,‚Äù the system prompted participants to think more deliberately about their goals.&lt;/p&gt;&lt;p&gt;Yet, adoption brings trade-offs. Participants valued adaptability but also found the system more difficult to interpret. Some struggled to anticipate how a selected option would influence the response, noting that the controls seemed opaque because the effect became evident only after the output appeared.&lt;/p&gt;&lt;p&gt;This highlights a balance to strike. Dynamic interfaces can streamline complex tasks but may introduce a learning curve where the connection between a checkbox and the final output requires user adaptation.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-promptions-the-solution-to-fix-ai-prompts"&gt;Promptions: The solution to fix AI prompts?&lt;/h3&gt;&lt;p&gt;Promptions is designed to be lightweight, functioning as a middleware layer sitting between the user and the underlying language model.&lt;/p&gt;&lt;p&gt;The architecture consists of two primary components:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Option Module:&lt;/strong&gt; Reviews the user‚Äôs prompt and conversation history to generate relevant UI elements.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Chat Module:&lt;/strong&gt; Incorporates these selections to produce the AI‚Äôs response.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Of particular note for security teams, ‚Äúthere‚Äôs no need to store data between sessions, which keeps implementation simple.‚Äù This stateless design mitigates data governance concerns typically associated with complex AI overlays.&lt;/p&gt;&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt; [embedded content]&lt;/p&gt;&lt;/figure&gt;&lt;p&gt;Moving from ‚Äúprompt engineering‚Äù to ‚Äúprompt selection‚Äù offers a pathway to more consistent AI outputs across an organisation. By implementing UI frameworks that guide user intent, technology leaders can reduce the variability of AI responses and improve workforce efficiency.&lt;/p&gt;&lt;p&gt;Success depends on calibration. Usability challenges remain regarding how dynamic options affect AI output and managing the complexity of multiple controls. Leaders should view this not as a complete solution to fix the results of AI prompts, but as a design pattern to test within their internal developer platforms and support tools.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Perplexity: AI agents are taking over complex enterprise tasks&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-111183" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/image-3.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Microsoft believes it has a fix for AI prompts being given, the response missing the mark, and the cycle repeating.&lt;/p&gt;&lt;p&gt;This inefficiency is a drain on resources. The ‚Äútrial-and-error loop can feel unpredictable and discouraging,‚Äù turning what should be a productivity booster into a time sink. Knowledge workers often spend more time managing the interaction itself than understanding the material they hoped to learn.&lt;/p&gt;&lt;p&gt;Microsoft has released Promptions (prompt + options), a UI framework designed to address this friction by replacing vague natural language requests with precise, dynamic interface controls. The open-source tool offers a method to standardise how workforces interact with large language models (LLMs), moving away from unstructured chat toward guided and reliable workflows.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-comprehension-bottleneck"&gt;The comprehension bottleneck&lt;/h3&gt;&lt;p&gt;Public attention often centres on AI producing text or images, but a massive component of enterprise usage involves understanding‚Äîasking AI to explain, clarify, or teach. This distinction is vital for internal tooling.&lt;/p&gt;&lt;p&gt;Consider a spreadsheet formula: one user may want a simple syntax breakdown, another a debugging guide, and another an explanation suitable for teaching colleagues. The same formula can require entirely different explanations depending on the user‚Äôs role, expertise, and goals.&lt;/p&gt;&lt;p&gt;Current chat interfaces rarely capture this intent effectively. Users often find that the way they phrase a question doesn‚Äôt match the level of detail the AI needs. ‚ÄúClarifying what they really want can require long, carefully worded prompts that are tiring to produce,‚Äù Microsoft explains.&lt;/p&gt;&lt;p&gt;Promptions operates as a middleware layer to fix this familiar issue with AI prompts. Instead of forcing users to type lengthy specifications, the system analyses the intent and conversation history to generate clickable options ‚Äì such as explanation length, tone, or specific focus areas ‚Äì in real-time.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-efficiency-vs-complexity"&gt;Efficiency vs complexity&lt;/h3&gt;&lt;p&gt;Microsoft researchers tested this approach by comparing static controls against the new dynamic system. The findings offer a realistic view of how such tools function in a live environment.&lt;/p&gt;&lt;p&gt;Participants consistently reported that dynamic controls made it easier to express the specifics of their tasks without repeatedly rephrasing their prompts. This reduced the effort of prompt engineering and allowed users to focus more on understanding content than managing the mechanics of phrasing. By surfacing options like ‚ÄúLearning Objective‚Äù and ‚ÄúResponse Format,‚Äù the system prompted participants to think more deliberately about their goals.&lt;/p&gt;&lt;p&gt;Yet, adoption brings trade-offs. Participants valued adaptability but also found the system more difficult to interpret. Some struggled to anticipate how a selected option would influence the response, noting that the controls seemed opaque because the effect became evident only after the output appeared.&lt;/p&gt;&lt;p&gt;This highlights a balance to strike. Dynamic interfaces can streamline complex tasks but may introduce a learning curve where the connection between a checkbox and the final output requires user adaptation.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-promptions-the-solution-to-fix-ai-prompts"&gt;Promptions: The solution to fix AI prompts?&lt;/h3&gt;&lt;p&gt;Promptions is designed to be lightweight, functioning as a middleware layer sitting between the user and the underlying language model.&lt;/p&gt;&lt;p&gt;The architecture consists of two primary components:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Option Module:&lt;/strong&gt; Reviews the user‚Äôs prompt and conversation history to generate relevant UI elements.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Chat Module:&lt;/strong&gt; Incorporates these selections to produce the AI‚Äôs response.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Of particular note for security teams, ‚Äúthere‚Äôs no need to store data between sessions, which keeps implementation simple.‚Äù This stateless design mitigates data governance concerns typically associated with complex AI overlays.&lt;/p&gt;&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt; [embedded content]&lt;/p&gt;&lt;/figure&gt;&lt;p&gt;Moving from ‚Äúprompt engineering‚Äù to ‚Äúprompt selection‚Äù offers a pathway to more consistent AI outputs across an organisation. By implementing UI frameworks that guide user intent, technology leaders can reduce the variability of AI responses and improve workforce efficiency.&lt;/p&gt;&lt;p&gt;Success depends on calibration. Usability challenges remain regarding how dynamic options affect AI output and managing the complexity of multiple controls. Leaders should view this not as a complete solution to fix the results of AI prompts, but as a design pattern to test within their internal developer platforms and support tools.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Perplexity: AI agents are taking over complex enterprise tasks&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-111183" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/image-3.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/microsoft-promptions-fix-ai-prompts-failing-to-deliver/</guid><pubDate>Thu, 11 Dec 2025 14:11:36 +0000</pubDate></item><item><title>TIME names ‚ÄòArchitects of AI‚Äô its Person of the Year (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/11/time-names-architects-of-ai-its-person-of-the-year/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/TIME-person-of-the-year.png?resize=1200,769" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Each December, TIME Magazine names a person of the year ‚Äî someone who has most influenced the news and world, for good or ill. Last year, TIME chose President Donald Trump for the second time. The year before that, it was Taylor Swift, who many claimed saved the economy from a recession with her Eras Tour. In 1938, the magazine chose Adolf Hitler.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This year, TIME has chosen to bestow its award on not just one person, but a group of people: the so-called ‚ÄúArchitects of AI,‚Äù comprising the CEOs shaping the global AI race from the U.S.&amp;nbsp;With AI on everyone‚Äôs minds, embodying hope for a small minority and economic anxiety for a majority, per recent Edelman data, this tracks.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;‚ÄúFor decades, humankind steeled itself for the rise of thinking machines,‚Äù the article reads. ‚ÄúLeaders striving to develop the technology, including Sam Altman and Elon Musk, warned that the pursuit of its powers could create unforeseen catastrophe [‚Ä¶] This year, the debate about how to wield AI responsibly gave way to a sprint to deploy it as fast as possible.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Based on one of TIME‚Äôs two cover photos, some of those people appear to be Nvidia‚Äôs Jensen Huang, Tesla‚Äôs Elon Musk, OpenAI‚Äôs Sam Altman, Meta‚Äôs Mark Zuckerberg, AMD‚Äôs Lisa Su, Anthropic‚Äôs Dario Amodei, Google DeepMind‚Äôs Demis Hassabis, and World Labs‚Äô Fei-Fei Li ‚Äî all individuals who raced ‚Äúboth beside and against each other.‚Äù&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TIME writes that these individuals, through their multibillion-dollar bets on ‚Äúone of the biggest physical infrastructure projects of all time,‚Äù have reshaped government policy, turned up the heat on geopolitical competition, and pushed AI adoption forward.&amp;nbsp;&lt;/p&gt;

&lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt;
&lt;p class="wp-block-paragraph"&gt;This is the story of how AI changed our world in 2025, in new and exciting and sometimes frightening ways. It is the story of how Huang and other tech titans grabbed the wheel of history, developing technology and making decisions that are reshaping the information landscape, the climate, and our livelihoods‚Ä¶ AI emerged as arguably the most consequential tool in great-power competition since the advent of nuclear weapons.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class="wp-block-paragraph"&gt;TIME only announced the news on Thursday morning, but images of the cover photo were leaked on prediction market Polymarket on Wednesday evening.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/TIME-person-of-the-year.png?resize=1200,769" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Each December, TIME Magazine names a person of the year ‚Äî someone who has most influenced the news and world, for good or ill. Last year, TIME chose President Donald Trump for the second time. The year before that, it was Taylor Swift, who many claimed saved the economy from a recession with her Eras Tour. In 1938, the magazine chose Adolf Hitler.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This year, TIME has chosen to bestow its award on not just one person, but a group of people: the so-called ‚ÄúArchitects of AI,‚Äù comprising the CEOs shaping the global AI race from the U.S.&amp;nbsp;With AI on everyone‚Äôs minds, embodying hope for a small minority and economic anxiety for a majority, per recent Edelman data, this tracks.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;‚ÄúFor decades, humankind steeled itself for the rise of thinking machines,‚Äù the article reads. ‚ÄúLeaders striving to develop the technology, including Sam Altman and Elon Musk, warned that the pursuit of its powers could create unforeseen catastrophe [‚Ä¶] This year, the debate about how to wield AI responsibly gave way to a sprint to deploy it as fast as possible.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Based on one of TIME‚Äôs two cover photos, some of those people appear to be Nvidia‚Äôs Jensen Huang, Tesla‚Äôs Elon Musk, OpenAI‚Äôs Sam Altman, Meta‚Äôs Mark Zuckerberg, AMD‚Äôs Lisa Su, Anthropic‚Äôs Dario Amodei, Google DeepMind‚Äôs Demis Hassabis, and World Labs‚Äô Fei-Fei Li ‚Äî all individuals who raced ‚Äúboth beside and against each other.‚Äù&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TIME writes that these individuals, through their multibillion-dollar bets on ‚Äúone of the biggest physical infrastructure projects of all time,‚Äù have reshaped government policy, turned up the heat on geopolitical competition, and pushed AI adoption forward.&amp;nbsp;&lt;/p&gt;

&lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt;
&lt;p class="wp-block-paragraph"&gt;This is the story of how AI changed our world in 2025, in new and exciting and sometimes frightening ways. It is the story of how Huang and other tech titans grabbed the wheel of history, developing technology and making decisions that are reshaping the information landscape, the climate, and our livelihoods‚Ä¶ AI emerged as arguably the most consequential tool in great-power competition since the advent of nuclear weapons.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class="wp-block-paragraph"&gt;TIME only announced the news on Thursday morning, but images of the cover photo were leaked on prediction market Polymarket on Wednesday evening.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/11/time-names-architects-of-ai-its-person-of-the-year/</guid><pubDate>Thu, 11 Dec 2025 14:38:06 +0000</pubDate></item><item><title>Oracle shares slide on $15B increase in data center spending (AI ‚Äì Ars Technica)</title><link>https://arstechnica.com/information-technology/2025/12/oracle-shares-slide-on-15b-increase-in-data-center-spending/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Company raises its capital expenditure forecast as it doubles down on AI infrastructure bet.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1867844462-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1867844462-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Mesut Dogan

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Oracle stock dropped after it reported disappointing revenues on Wednesday alongside a $15 billion increase in its planned spending on data centers this year to serve artificial intelligence groups.&lt;/p&gt;
&lt;p&gt;Shares in Larry Ellison‚Äôs database company fell 11 percent in pre-market trading on Thursday after it reported revenues of $16.1 billion in the last quarter, up 14 percent from the previous year, but below analysts‚Äô estimates.&lt;/p&gt;
&lt;p&gt;Oracle raised its forecast for capital expenditure this financial year by more than 40 percent to $50 billion. The outlay, largely directed to building data centers, climbed to $12 billion in the quarter, above expectations of $8.4 billion.&lt;/p&gt;
&lt;p&gt;Its long-term debt increased to $99.9 billion, up 25 percent from a year ago.&lt;/p&gt;
&lt;p&gt;Oracle has launched an aggressive bid to catch up to much larger cloud players such as Google, Amazon, and Microsoft in the race to supply the vast amount of computing power that AI groups including OpenAI and Anthropic need to train and run their models.&lt;/p&gt;
&lt;p&gt;Clay Magouyrk, Oracle‚Äôs co-chief executive, said its cloud contracts would ‚Äúquickly add revenue and margin to our infrastructure business‚Äù as he defended the vast investments.&lt;/p&gt;
&lt;p&gt;Yet the company said it expected full-year revenues to remain unchanged from its previous forecast of $67 billion. It expected to generate $4 billion more in revenue the following fiscal year.&lt;/p&gt;
&lt;p&gt;Total bookings for future revenue, known as remaining performance obligations, rose 15 percent to $523 billion in the three months to the end of November, supported by deals with Meta and Nvidia.&lt;/p&gt;
&lt;p&gt;Investors initially welcomed the push into AI from Oracle. Shares surged after its last earnings in September when it disclosed it had added more than $300 billion in bookings, largely driven by data center contracts with OpenAI.&lt;/p&gt;
&lt;p&gt;But the stock has given up its gains since then as investors worry about the large amounts Oracle will have to borrow and spend on infrastructure for the ChatGPT maker‚Äîand concerns over the start-up‚Äôs ability to pay for these contracts in the years ahead. OpenAI has struck deals to spend $1.4 trillion over the next eight years on computing power.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Oracle‚Äôs Big Tech rivals such as Amazon, Microsoft, and Google have helped reassure investors about their large capital investments by posting strong earnings from their vast cloud units.&lt;/p&gt;
&lt;p&gt;But in the last quarter, Oracle‚Äôs cloud infrastructure business, which includes its data centers, posted worse than expected revenues of $4.1 billion. Ellison‚Äôs company is also relying more heavily on debt to fuel its expansion.&lt;/p&gt;
&lt;p&gt;Net income rose to $6.1 billion in the quarter, boosted by a $2.7 billion pre-tax gain from the sale of semiconductor company Ampere to SoftBank.&lt;/p&gt;
&lt;p&gt;The company added an additional 400 MW of data center capacity in the quarter, Magouyrk told investors. Construction was on track at its large data center cluster in Abilene, Texas, which is being built for OpenAI, he added.&lt;/p&gt;
&lt;p&gt;Magouyrk, who took over from Safra Catz in September, said there was ample demand from other clients for Oracle‚Äôs data centers if OpenAI did not take up the full amount it had contracted for.&lt;/p&gt;
&lt;p&gt;‚ÄúWe have a customer base with a lot of demand such that whenever we find ourselves [with] capacity that‚Äôs not being used, it very quickly gets allocated,‚Äù he said.&lt;/p&gt;
&lt;p&gt;Co-founded by Ellison as a business software provider, Oracle was slow to pivot to cloud computing. The billionaire remains chair and its largest shareholder.&lt;/p&gt;
&lt;p&gt;Investors and analysts have raised concerns in recent months about the upfront spending required by Oracle to honor its AI infrastructure contracts. Moody‚Äôs in September flagged the company‚Äôs reliance on a small number of large customers such as OpenAI.&lt;/p&gt;
&lt;p&gt;Morgan Stanley forecasts that Oracle‚Äôs net debt will soar to about $290 billion by 2028. The company sold $18 billion of bonds in September and is in talks to raise $38 billion in debt financing through a number of US banks.&lt;/p&gt;
&lt;p&gt;Brent Thill, an analyst at Jefferies, said Oracle‚Äôs software business‚Äîwhich generated $5.9 billion in the quarter‚Äîprovided some buffer amid accelerated spending. ‚ÄúBut the timing mismatch between upfront capex and delayed monetization creates near-term pressure.‚Äù&lt;/p&gt;
&lt;p&gt;Doug Kehring, principal financial officer, said the company was renting capacity from data center specialists to reduce its direct borrowing.&lt;/p&gt;
&lt;p&gt;The debt to build the Abilene site was raised by start-up Crusoe and investment group Blue Owl Capital, and Oracle has signed a 15-year lease for the site.&lt;/p&gt;
&lt;p&gt;‚ÄúOracle does not pay for these leases until the completed data centers‚Ä¶‚Äâare delivered to us,‚Äù Kehring said, adding that the company was ‚Äúcommitted to maintaining our investment-grade debt ratings.‚Äù&lt;/p&gt;
&lt;p&gt;&lt;em&gt;¬© 2025 The Financial Times Ltd. All rights reserved. Not to be redistributed, copied, or modified in any way.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Company raises its capital expenditure forecast as it doubles down on AI infrastructure bet.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1867844462-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1867844462-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Mesut Dogan

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Oracle stock dropped after it reported disappointing revenues on Wednesday alongside a $15 billion increase in its planned spending on data centers this year to serve artificial intelligence groups.&lt;/p&gt;
&lt;p&gt;Shares in Larry Ellison‚Äôs database company fell 11 percent in pre-market trading on Thursday after it reported revenues of $16.1 billion in the last quarter, up 14 percent from the previous year, but below analysts‚Äô estimates.&lt;/p&gt;
&lt;p&gt;Oracle raised its forecast for capital expenditure this financial year by more than 40 percent to $50 billion. The outlay, largely directed to building data centers, climbed to $12 billion in the quarter, above expectations of $8.4 billion.&lt;/p&gt;
&lt;p&gt;Its long-term debt increased to $99.9 billion, up 25 percent from a year ago.&lt;/p&gt;
&lt;p&gt;Oracle has launched an aggressive bid to catch up to much larger cloud players such as Google, Amazon, and Microsoft in the race to supply the vast amount of computing power that AI groups including OpenAI and Anthropic need to train and run their models.&lt;/p&gt;
&lt;p&gt;Clay Magouyrk, Oracle‚Äôs co-chief executive, said its cloud contracts would ‚Äúquickly add revenue and margin to our infrastructure business‚Äù as he defended the vast investments.&lt;/p&gt;
&lt;p&gt;Yet the company said it expected full-year revenues to remain unchanged from its previous forecast of $67 billion. It expected to generate $4 billion more in revenue the following fiscal year.&lt;/p&gt;
&lt;p&gt;Total bookings for future revenue, known as remaining performance obligations, rose 15 percent to $523 billion in the three months to the end of November, supported by deals with Meta and Nvidia.&lt;/p&gt;
&lt;p&gt;Investors initially welcomed the push into AI from Oracle. Shares surged after its last earnings in September when it disclosed it had added more than $300 billion in bookings, largely driven by data center contracts with OpenAI.&lt;/p&gt;
&lt;p&gt;But the stock has given up its gains since then as investors worry about the large amounts Oracle will have to borrow and spend on infrastructure for the ChatGPT maker‚Äîand concerns over the start-up‚Äôs ability to pay for these contracts in the years ahead. OpenAI has struck deals to spend $1.4 trillion over the next eight years on computing power.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Oracle‚Äôs Big Tech rivals such as Amazon, Microsoft, and Google have helped reassure investors about their large capital investments by posting strong earnings from their vast cloud units.&lt;/p&gt;
&lt;p&gt;But in the last quarter, Oracle‚Äôs cloud infrastructure business, which includes its data centers, posted worse than expected revenues of $4.1 billion. Ellison‚Äôs company is also relying more heavily on debt to fuel its expansion.&lt;/p&gt;
&lt;p&gt;Net income rose to $6.1 billion in the quarter, boosted by a $2.7 billion pre-tax gain from the sale of semiconductor company Ampere to SoftBank.&lt;/p&gt;
&lt;p&gt;The company added an additional 400 MW of data center capacity in the quarter, Magouyrk told investors. Construction was on track at its large data center cluster in Abilene, Texas, which is being built for OpenAI, he added.&lt;/p&gt;
&lt;p&gt;Magouyrk, who took over from Safra Catz in September, said there was ample demand from other clients for Oracle‚Äôs data centers if OpenAI did not take up the full amount it had contracted for.&lt;/p&gt;
&lt;p&gt;‚ÄúWe have a customer base with a lot of demand such that whenever we find ourselves [with] capacity that‚Äôs not being used, it very quickly gets allocated,‚Äù he said.&lt;/p&gt;
&lt;p&gt;Co-founded by Ellison as a business software provider, Oracle was slow to pivot to cloud computing. The billionaire remains chair and its largest shareholder.&lt;/p&gt;
&lt;p&gt;Investors and analysts have raised concerns in recent months about the upfront spending required by Oracle to honor its AI infrastructure contracts. Moody‚Äôs in September flagged the company‚Äôs reliance on a small number of large customers such as OpenAI.&lt;/p&gt;
&lt;p&gt;Morgan Stanley forecasts that Oracle‚Äôs net debt will soar to about $290 billion by 2028. The company sold $18 billion of bonds in September and is in talks to raise $38 billion in debt financing through a number of US banks.&lt;/p&gt;
&lt;p&gt;Brent Thill, an analyst at Jefferies, said Oracle‚Äôs software business‚Äîwhich generated $5.9 billion in the quarter‚Äîprovided some buffer amid accelerated spending. ‚ÄúBut the timing mismatch between upfront capex and delayed monetization creates near-term pressure.‚Äù&lt;/p&gt;
&lt;p&gt;Doug Kehring, principal financial officer, said the company was renting capacity from data center specialists to reduce its direct borrowing.&lt;/p&gt;
&lt;p&gt;The debt to build the Abilene site was raised by start-up Crusoe and investment group Blue Owl Capital, and Oracle has signed a 15-year lease for the site.&lt;/p&gt;
&lt;p&gt;‚ÄúOracle does not pay for these leases until the completed data centers‚Ä¶‚Äâare delivered to us,‚Äù Kehring said, adding that the company was ‚Äúcommitted to maintaining our investment-grade debt ratings.‚Äù&lt;/p&gt;
&lt;p&gt;&lt;em&gt;¬© 2025 The Financial Times Ltd. All rights reserved. Not to be redistributed, copied, or modified in any way.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/information-technology/2025/12/oracle-shares-slide-on-15b-increase-in-data-center-spending/</guid><pubDate>Thu, 11 Dec 2025 14:39:21 +0000</pubDate></item><item><title>Disney signs deal with OpenAI to allow Sora to generate AI videos featuring its characters (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/11/disney-signs-deal-with-openai-to-allow-sora-to-generate-ai-videos-featuring-its-characters/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/GettyImages-1387623215.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Walt Disney Company announced on Thursday that it has signed a three-year partnership with OpenAI that will bring its iconic characters to the company‚Äôs Sora AI video generator. Disney is also making a $1 billion equity investment in OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Launched in September, Sora allows users to create short videos using simple prompts. With this new agreement, users will be able to draw on more than 200 animated, masked, and creature characters from Disney, Marvel, Pixar, and Star Wars, including costumes, props, vehicles, and more.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;These characters include iconic faces like Mickey Mouse, Ariel, Belle, Cinderella, Baymax, and Simba, as well as characters from Encanto, Frozen, Inside Out, Moana, Monsters, Inc., Toy Story, Up, and Zootopia. Users will also be able to draw on animated or illustrated versions of Marvel and Lucasfilm characters like Black Panther, Captain America, Deadpool, Groot, Iron Man, Darth Vader, Han Solo, Stormtroopers, and more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users will also be able to draw on these characters while using ChatGPT Images, the feature in ChatGPT that allows users to create visuals using text prompts. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The agreement does not include any talent likenesses or voices, Disney says. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúThe rapid advancement of artificial intelligence marks an important moment for our industry, and through this collaboration with OpenAI we will thoughtfully and responsibly extend the reach of our storytelling through generative AI, while respecting and protecting creators and their works,‚Äù said Disney CEO Bob Iger in a statement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Disney says that alongside the agreement, it will ‚Äúbecome a major customer of OpenAI,‚Äù as it will use&amp;nbsp;its APIs to build new products, tools, and experiences, including for Disney+.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúDisney is the global gold standard for storytelling, and we‚Äôre excited to partner to allow Sora and ChatGPT Images to expand the way people create and experience great content,‚Äù said Sam Altman, co-founder and CEO of OpenAI, in a statement. ‚ÄúThis agreement shows how AI companies and creative leaders can work together responsibly to promote innovation that benefits society, respect the importance of creativity, and help works reach vast new audiences.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It‚Äôs worth noting that Disney has sued the generative AI platform Midjourney for ignoring requests to stop violating its intellectual property rights. Disney also sent a cease-and-desist letter to Character.AI, urging the chatbot company to remove Disney characters from among the millions of AI companions on its platform.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Disney‚Äôs agreement with OpenAI indicates the company isn‚Äôt fully closing the door on AI platforms.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/GettyImages-1387623215.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Walt Disney Company announced on Thursday that it has signed a three-year partnership with OpenAI that will bring its iconic characters to the company‚Äôs Sora AI video generator. Disney is also making a $1 billion equity investment in OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Launched in September, Sora allows users to create short videos using simple prompts. With this new agreement, users will be able to draw on more than 200 animated, masked, and creature characters from Disney, Marvel, Pixar, and Star Wars, including costumes, props, vehicles, and more.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;These characters include iconic faces like Mickey Mouse, Ariel, Belle, Cinderella, Baymax, and Simba, as well as characters from Encanto, Frozen, Inside Out, Moana, Monsters, Inc., Toy Story, Up, and Zootopia. Users will also be able to draw on animated or illustrated versions of Marvel and Lucasfilm characters like Black Panther, Captain America, Deadpool, Groot, Iron Man, Darth Vader, Han Solo, Stormtroopers, and more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users will also be able to draw on these characters while using ChatGPT Images, the feature in ChatGPT that allows users to create visuals using text prompts. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The agreement does not include any talent likenesses or voices, Disney says. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúThe rapid advancement of artificial intelligence marks an important moment for our industry, and through this collaboration with OpenAI we will thoughtfully and responsibly extend the reach of our storytelling through generative AI, while respecting and protecting creators and their works,‚Äù said Disney CEO Bob Iger in a statement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Disney says that alongside the agreement, it will ‚Äúbecome a major customer of OpenAI,‚Äù as it will use&amp;nbsp;its APIs to build new products, tools, and experiences, including for Disney+.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúDisney is the global gold standard for storytelling, and we‚Äôre excited to partner to allow Sora and ChatGPT Images to expand the way people create and experience great content,‚Äù said Sam Altman, co-founder and CEO of OpenAI, in a statement. ‚ÄúThis agreement shows how AI companies and creative leaders can work together responsibly to promote innovation that benefits society, respect the importance of creativity, and help works reach vast new audiences.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It‚Äôs worth noting that Disney has sued the generative AI platform Midjourney for ignoring requests to stop violating its intellectual property rights. Disney also sent a cease-and-desist letter to Character.AI, urging the chatbot company to remove Disney characters from among the millions of AI companions on its platform.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Disney‚Äôs agreement with OpenAI indicates the company isn‚Äôt fully closing the door on AI platforms.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/11/disney-signs-deal-with-openai-to-allow-sora-to-generate-ai-videos-featuring-its-characters/</guid><pubDate>Thu, 11 Dec 2025 15:21:05 +0000</pubDate></item><item><title>New in llama.cpp: Model Management (Hugging Face - Blog)</title><link>https://huggingface.co/blog/ggml-org/model-management-in-llamacpp</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://cdn-uploads.huggingface.co/production/uploads/5f17f0a0925b9863e28ad517/wGWbyTyYFKKGDMaAnTz7M.png" /&gt;&lt;/div&gt;&lt;!-- HTML_TAG_START --&gt;
llama.cpp server now ships with &lt;strong&gt;router mode&lt;/strong&gt;, which lets you dynamically load, unload, and switch between multiple models without restarting.
&lt;blockquote&gt;
&lt;p&gt;Reminder: llama.cpp server is a lightweight, OpenAI-compatible HTTP server for running LLMs locally.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This feature was a popular request to bring Ollama-style model management to llama.cpp. It uses a multi-process architecture where each model runs in its own process, so if one model crashes, others remain unaffected.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Quick Start
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Start the server in router mode by &lt;strong&gt;not specifying a model&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;llama-server
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This auto-discovers models from your llama.cpp cache (&lt;code&gt;LLAMA_CACHE&lt;/code&gt; or &lt;code&gt;~/.cache/llama.cpp&lt;/code&gt;). If you've previously downloaded models via &lt;code&gt;llama-server -hf user/model&lt;/code&gt;, they'll be available automatically.&lt;/p&gt;
&lt;p&gt;You can also point to a local directory of GGUF files:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;llama-server --models-dir ./my-models
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Features
	&lt;/span&gt;
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Auto-discovery&lt;/strong&gt;: Scans your llama.cpp cache (default) or a custom &lt;code&gt;--models-dir&lt;/code&gt; folder for GGUF files&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;On-demand loading&lt;/strong&gt;: Models load automatically when first requested&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LRU eviction&lt;/strong&gt;: When you hit &lt;code&gt;--models-max&lt;/code&gt; (default: 4), the least-recently-used model unloads&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Request routing&lt;/strong&gt;: The &lt;code&gt;model&lt;/code&gt; field in your request determines which model handles it&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Examples
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Chat with a specific model
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;curl http://localhost:8080/v1/chat/completions \
  -H &lt;span class="hljs-string"&gt;"Content-Type: application/json"&lt;/span&gt; \
  -d &lt;span class="hljs-string"&gt;'{&lt;/span&gt;
&lt;span class="hljs-string"&gt;    "model": "ggml-org/gemma-3-4b-it-GGUF:Q4_K_M",&lt;/span&gt;
&lt;span class="hljs-string"&gt;    "messages": [{"role": "user", "content": "Hello!"}]&lt;/span&gt;
&lt;span class="hljs-string"&gt;  }'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the first request, the server automatically loads the model into memory (loading time depends on model size). Subsequent requests to the same model are instant since it's already loaded.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		List available models
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;curl http://localhost:8080/models
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Returns all discovered models with their status (&lt;code&gt;loaded&lt;/code&gt;, &lt;code&gt;loading&lt;/code&gt;, or &lt;code&gt;unloaded&lt;/code&gt;).&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Manually load a model
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -X POST http://localhost:8080/models/load \
  -H &lt;span class="hljs-string"&gt;"Content-Type: application/json"&lt;/span&gt; \
  -d &lt;span class="hljs-string"&gt;'{"model": "my-model.gguf"}'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Unload a model to free VRAM
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -X POST http://localhost:8080/models/unload \
  -H &lt;span class="hljs-string"&gt;"Content-Type: application/json"&lt;/span&gt; \
  -d &lt;span class="hljs-string"&gt;'{"model": "my-model.gguf"}'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Key Options
	&lt;/span&gt;
&lt;/h2&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Flag&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--models-dir PATH&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Directory containing your GGUF files&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--models-max N&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Max models loaded simultaneously (default: 4)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--no-models-autoload&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Disable auto-loading; require explicit &lt;code&gt;/models/load&lt;/code&gt; calls&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;All model instances inherit settings from the router:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;llama-server --models-dir ./models -c 8192 -ngl 99
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All loaded models will use 8192 context and full GPU offload. You can also define per-model settings using presets:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;llama-server --models-preset config.ini
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-ini"&gt;&lt;span class="hljs-section"&gt;[my-model]&lt;/span&gt;
&lt;span class="hljs-attr"&gt;model&lt;/span&gt; = /path/to/model.gguf
&lt;span class="hljs-attr"&gt;ctx-size&lt;/span&gt; = &lt;span class="hljs-number"&gt;65536&lt;/span&gt;
&lt;span class="hljs-attr"&gt;temp&lt;/span&gt; = &lt;span class="hljs-number"&gt;0.7&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Also available in the Web UI
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The built-in web UI also supports model switching. Just select a model from the dropdown and it loads automatically.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Join the Conversation
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We hope this feature makes it easier to A/B test different model versions, run multi-tenant deployments, or simply switch models during development without restarting the server.&lt;/p&gt;
&lt;p&gt;Have questions or feedback? Drop a comment below or open an issue on GitHub.&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://cdn-uploads.huggingface.co/production/uploads/5f17f0a0925b9863e28ad517/wGWbyTyYFKKGDMaAnTz7M.png" /&gt;&lt;/div&gt;&lt;!-- HTML_TAG_START --&gt;
llama.cpp server now ships with &lt;strong&gt;router mode&lt;/strong&gt;, which lets you dynamically load, unload, and switch between multiple models without restarting.
&lt;blockquote&gt;
&lt;p&gt;Reminder: llama.cpp server is a lightweight, OpenAI-compatible HTTP server for running LLMs locally.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This feature was a popular request to bring Ollama-style model management to llama.cpp. It uses a multi-process architecture where each model runs in its own process, so if one model crashes, others remain unaffected.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Quick Start
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Start the server in router mode by &lt;strong&gt;not specifying a model&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;llama-server
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This auto-discovers models from your llama.cpp cache (&lt;code&gt;LLAMA_CACHE&lt;/code&gt; or &lt;code&gt;~/.cache/llama.cpp&lt;/code&gt;). If you've previously downloaded models via &lt;code&gt;llama-server -hf user/model&lt;/code&gt;, they'll be available automatically.&lt;/p&gt;
&lt;p&gt;You can also point to a local directory of GGUF files:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;llama-server --models-dir ./my-models
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Features
	&lt;/span&gt;
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Auto-discovery&lt;/strong&gt;: Scans your llama.cpp cache (default) or a custom &lt;code&gt;--models-dir&lt;/code&gt; folder for GGUF files&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;On-demand loading&lt;/strong&gt;: Models load automatically when first requested&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LRU eviction&lt;/strong&gt;: When you hit &lt;code&gt;--models-max&lt;/code&gt; (default: 4), the least-recently-used model unloads&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Request routing&lt;/strong&gt;: The &lt;code&gt;model&lt;/code&gt; field in your request determines which model handles it&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Examples
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Chat with a specific model
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;curl http://localhost:8080/v1/chat/completions \
  -H &lt;span class="hljs-string"&gt;"Content-Type: application/json"&lt;/span&gt; \
  -d &lt;span class="hljs-string"&gt;'{&lt;/span&gt;
&lt;span class="hljs-string"&gt;    "model": "ggml-org/gemma-3-4b-it-GGUF:Q4_K_M",&lt;/span&gt;
&lt;span class="hljs-string"&gt;    "messages": [{"role": "user", "content": "Hello!"}]&lt;/span&gt;
&lt;span class="hljs-string"&gt;  }'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the first request, the server automatically loads the model into memory (loading time depends on model size). Subsequent requests to the same model are instant since it's already loaded.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		List available models
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;curl http://localhost:8080/models
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Returns all discovered models with their status (&lt;code&gt;loaded&lt;/code&gt;, &lt;code&gt;loading&lt;/code&gt;, or &lt;code&gt;unloaded&lt;/code&gt;).&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Manually load a model
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -X POST http://localhost:8080/models/load \
  -H &lt;span class="hljs-string"&gt;"Content-Type: application/json"&lt;/span&gt; \
  -d &lt;span class="hljs-string"&gt;'{"model": "my-model.gguf"}'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Unload a model to free VRAM
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -X POST http://localhost:8080/models/unload \
  -H &lt;span class="hljs-string"&gt;"Content-Type: application/json"&lt;/span&gt; \
  -d &lt;span class="hljs-string"&gt;'{"model": "my-model.gguf"}'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Key Options
	&lt;/span&gt;
&lt;/h2&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Flag&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--models-dir PATH&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Directory containing your GGUF files&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--models-max N&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Max models loaded simultaneously (default: 4)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--no-models-autoload&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Disable auto-loading; require explicit &lt;code&gt;/models/load&lt;/code&gt; calls&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;All model instances inherit settings from the router:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;llama-server --models-dir ./models -c 8192 -ngl 99
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All loaded models will use 8192 context and full GPU offload. You can also define per-model settings using presets:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;llama-server --models-preset config.ini
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-ini"&gt;&lt;span class="hljs-section"&gt;[my-model]&lt;/span&gt;
&lt;span class="hljs-attr"&gt;model&lt;/span&gt; = /path/to/model.gguf
&lt;span class="hljs-attr"&gt;ctx-size&lt;/span&gt; = &lt;span class="hljs-number"&gt;65536&lt;/span&gt;
&lt;span class="hljs-attr"&gt;temp&lt;/span&gt; = &lt;span class="hljs-number"&gt;0.7&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Also available in the Web UI
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The built-in web UI also supports model switching. Just select a model from the dropdown and it loads automatically.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Join the Conversation
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We hope this feature makes it easier to A/B test different model versions, run multi-tenant deployments, or simply switch models during development without restarting the server.&lt;/p&gt;
&lt;p&gt;Have questions or feedback? Drop a comment below or open an issue on GitHub.&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/ggml-org/model-management-in-llamacpp</guid><pubDate>Thu, 11 Dec 2025 15:47:44 +0000</pubDate></item><item><title>Disney invests $1 billion in OpenAI, licenses 200 characters for AI video app Sora (AI ‚Äì Ars Technica)</title><link>https://arstechnica.com/ai/2025/12/disney-invests-1-billion-in-openai-licenses-200-characters-for-ai-video-app-sora/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Three-year deal lets users create AI videos of Mickey Mouse, Darth Vader, and more.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A visitor examines a sculpture at &amp;quot;Mickey: The True Original &amp;amp; Ever Curious&amp;quot; exhibition on July 26, 2022 in Beijing, China." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/disney_1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="A visitor examines a sculpture at &amp;quot;Mickey: The True Original &amp;amp; Ever Curious&amp;quot; exhibition on July 26, 2022 in Beijing, China." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/disney_1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A visitor examines a sculpture at "Mickey: The True Original &amp;amp; Ever Curious" exhibition on July 26, 2022, in Beijing, China.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          China News Service via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Thursday, The Walt Disney Company announced a $1 billion investment in OpenAI and a three-year licensing agreement that will allow users of OpenAI‚Äôs Sora video generator to create short clips featuring more than 200 Disney, Marvel, Pixar, and Star Wars characters. It‚Äôs the first major content licensing partnership between a Hollywood studio related to the most recent version of OpenAI‚Äôs AI video platform, which drew criticism from some parts of the entertainment industry when it launched in late September.&lt;/p&gt;
&lt;p&gt;‚ÄúTechnological innovation has continually shaped the evolution of entertainment, bringing with it new ways to create and share great stories with the world,‚Äù said Disney CEO Robert A. Iger in the announcement. ‚ÄúThe rapid advancement of artificial intelligence marks an important moment for our industry, and through this collaboration with OpenAI we will thoughtfully and responsibly extend the reach of our storytelling through generative AI, while respecting and protecting creators and their works.‚Äù&lt;/p&gt;
&lt;p&gt;The deal creates interesting bedfellows between a company that basically defined modern US copyright policy through congressional lobbying back in the 1990s and one that has argued in a submission to the UK House of Lords that useful AI models cannot be created without copyrighted material.&lt;/p&gt;
&lt;p&gt;Tech companies that build AI models traditionally gather those materials without rightsholder permission due to the sheer number of examples needed to train a reasonably useful generative AI model. However, since breaking out with the mainstream success of ChatGPT and becoming flush with investment cash (and facing some gnarly lawsuits), OpenAI in particular has taken steps to license content from IP owners after the fact.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2120254 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="An AI-generated version of OpenAI CEO Sam Altman, seen in a still capture from a video generated by Sora 2." class="center large" height="623" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/fake_altman-1024x623.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      An AI-generated version of OpenAI CEO Sam Altman seen in a still capture from a video generated by Sora 2.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          OpenAI

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Under the new agreement with Disney, Sora users will be able to generate short videos using characters such as Mickey Mouse, Darth Vader, Iron Man, Simba, and characters from franchises including &lt;em&gt;Frozen&lt;/em&gt;, &lt;em&gt;Inside Out&lt;/em&gt;, &lt;em&gt;Toy Story&lt;/em&gt;, and &lt;em&gt;The Mandalorian&lt;/em&gt;, along with costumes, props, vehicles, and environments.&lt;/p&gt;
&lt;p&gt;The ChatGPT image generator will also gain official access to the same intellectual property, although that information was trained into these AI models long ago. What‚Äôs changing is that OpenAI will allow Disney-related content generated by its AI models to officially pass through its content moderation filters and reach the user, sanctioned by Disney.&lt;/p&gt;
&lt;p&gt;On Disney‚Äôs end of the deal, the company plans to deploy ChatGPT for its employees and use OpenAI‚Äôs technology to build new features for Disney+. A curated selection of fan-made Sora videos will stream on the Disney+ platform starting in early 2026.&lt;/p&gt;
&lt;p&gt;The agreement does not include any talent likenesses or voices. Disney and OpenAI said they have committed to ‚Äúmaintaining robust controls to prevent the generation of illegal or harmful content‚Äù and to ‚Äúrespect the rights of individuals to appropriately control the use of their voice and likeness.‚Äù&lt;/p&gt;
&lt;p&gt;OpenAI CEO Sam Altman called the deal a model for collaboration between AI companies and studios. ‚ÄúThis agreement shows how AI companies and creative leaders can work together responsibly to promote innovation that benefits society, respect the importance of creativity, and help works reach vast new audiences,‚Äù Altman said.&lt;/p&gt;
&lt;h2&gt;From adversary to partner&lt;/h2&gt;
&lt;p&gt;Money opens all kinds of doors, and the new partnership represents a dramatic reversal in Disney‚Äôs approach to OpenAI from just a few months ago. At that time, Disney and other major studios refused to participate in Sora 2 following its launch on September 30.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;OpenAI‚Äôs initial policy allowed copyrighted characters to appear in user-generated videos unless rights holders explicitly opted out. The LA Times reported that OpenAI had contacted talent agencies and studios before the launch, telling them that IP holders ‚Äúwould have to explicitly ask OpenAI not to include their copyright material in videos the tool creates.‚Äù&lt;/p&gt;
&lt;p&gt;Hollywood‚Äôs response to Sora 2 was swift and generally negative. According to CNBC, the Creative Artists Agency called it a ‚Äúsignificant risk‚Äù to its clients, while United Talent Agency labeled it ‚Äúexploitation, not innovation.‚Äù The WME talent agency sent a memo to agents notifying OpenAI that all of the agency‚Äôs clients were opted out of Sora. The Motion Picture Association also demanded ‚Äúimmediate and decisive action‚Äù from OpenAI.&lt;/p&gt;
&lt;p&gt;OpenAI CEO Sam Altman reversed course within days of the reaction, promising to give rights holders ‚Äúmore granular control‚Äù and floating a potential revenue-sharing model. The company also partnered with actor Bryan Cranston and SAG-AFTRA in October to implement new safety guardrails around likeness rights.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2131491 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="OpenAI and Disney announcement graphic." class="center large" height="576" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/OAI_Disney_Hero_16x9-1024x576.webp" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          OpenAI

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;While Disney and OpenAI are apparently friends now, the company has simultaneously taken an aggressive stance against some AI companies it has not partnered with.&lt;/p&gt;
&lt;p&gt;On Wednesday, Disney sent a cease-and-desist letter to Google, accusing the company of ‚Äúinfringing Disney‚Äôs copyrights on a massive scale‚Äù through its AI services, including YouTube. Disney has also sent similar letters to Meta and Character.AI and filed lawsuits against image-synthesis service Midjourney alongside NBCUniversal and Warner Bros. Discovery.&lt;/p&gt;
&lt;p&gt;A few major questions about the deal remain unanswered, including the actual licensing fees, whether Disney content will be used to train future OpenAI models, and whether this deal is even finalized. The announcement also notes it remains ‚Äúsubject to negotiation of definitive agreements,‚Äù so expect potential updates or clarifications ahead.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Three-year deal lets users create AI videos of Mickey Mouse, Darth Vader, and more.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A visitor examines a sculpture at &amp;quot;Mickey: The True Original &amp;amp; Ever Curious&amp;quot; exhibition on July 26, 2022 in Beijing, China." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/disney_1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="A visitor examines a sculpture at &amp;quot;Mickey: The True Original &amp;amp; Ever Curious&amp;quot; exhibition on July 26, 2022 in Beijing, China." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/disney_1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A visitor examines a sculpture at "Mickey: The True Original &amp;amp; Ever Curious" exhibition on July 26, 2022, in Beijing, China.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          China News Service via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Thursday, The Walt Disney Company announced a $1 billion investment in OpenAI and a three-year licensing agreement that will allow users of OpenAI‚Äôs Sora video generator to create short clips featuring more than 200 Disney, Marvel, Pixar, and Star Wars characters. It‚Äôs the first major content licensing partnership between a Hollywood studio related to the most recent version of OpenAI‚Äôs AI video platform, which drew criticism from some parts of the entertainment industry when it launched in late September.&lt;/p&gt;
&lt;p&gt;‚ÄúTechnological innovation has continually shaped the evolution of entertainment, bringing with it new ways to create and share great stories with the world,‚Äù said Disney CEO Robert A. Iger in the announcement. ‚ÄúThe rapid advancement of artificial intelligence marks an important moment for our industry, and through this collaboration with OpenAI we will thoughtfully and responsibly extend the reach of our storytelling through generative AI, while respecting and protecting creators and their works.‚Äù&lt;/p&gt;
&lt;p&gt;The deal creates interesting bedfellows between a company that basically defined modern US copyright policy through congressional lobbying back in the 1990s and one that has argued in a submission to the UK House of Lords that useful AI models cannot be created without copyrighted material.&lt;/p&gt;
&lt;p&gt;Tech companies that build AI models traditionally gather those materials without rightsholder permission due to the sheer number of examples needed to train a reasonably useful generative AI model. However, since breaking out with the mainstream success of ChatGPT and becoming flush with investment cash (and facing some gnarly lawsuits), OpenAI in particular has taken steps to license content from IP owners after the fact.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2120254 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="An AI-generated version of OpenAI CEO Sam Altman, seen in a still capture from a video generated by Sora 2." class="center large" height="623" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/fake_altman-1024x623.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      An AI-generated version of OpenAI CEO Sam Altman seen in a still capture from a video generated by Sora 2.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          OpenAI

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Under the new agreement with Disney, Sora users will be able to generate short videos using characters such as Mickey Mouse, Darth Vader, Iron Man, Simba, and characters from franchises including &lt;em&gt;Frozen&lt;/em&gt;, &lt;em&gt;Inside Out&lt;/em&gt;, &lt;em&gt;Toy Story&lt;/em&gt;, and &lt;em&gt;The Mandalorian&lt;/em&gt;, along with costumes, props, vehicles, and environments.&lt;/p&gt;
&lt;p&gt;The ChatGPT image generator will also gain official access to the same intellectual property, although that information was trained into these AI models long ago. What‚Äôs changing is that OpenAI will allow Disney-related content generated by its AI models to officially pass through its content moderation filters and reach the user, sanctioned by Disney.&lt;/p&gt;
&lt;p&gt;On Disney‚Äôs end of the deal, the company plans to deploy ChatGPT for its employees and use OpenAI‚Äôs technology to build new features for Disney+. A curated selection of fan-made Sora videos will stream on the Disney+ platform starting in early 2026.&lt;/p&gt;
&lt;p&gt;The agreement does not include any talent likenesses or voices. Disney and OpenAI said they have committed to ‚Äúmaintaining robust controls to prevent the generation of illegal or harmful content‚Äù and to ‚Äúrespect the rights of individuals to appropriately control the use of their voice and likeness.‚Äù&lt;/p&gt;
&lt;p&gt;OpenAI CEO Sam Altman called the deal a model for collaboration between AI companies and studios. ‚ÄúThis agreement shows how AI companies and creative leaders can work together responsibly to promote innovation that benefits society, respect the importance of creativity, and help works reach vast new audiences,‚Äù Altman said.&lt;/p&gt;
&lt;h2&gt;From adversary to partner&lt;/h2&gt;
&lt;p&gt;Money opens all kinds of doors, and the new partnership represents a dramatic reversal in Disney‚Äôs approach to OpenAI from just a few months ago. At that time, Disney and other major studios refused to participate in Sora 2 following its launch on September 30.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;OpenAI‚Äôs initial policy allowed copyrighted characters to appear in user-generated videos unless rights holders explicitly opted out. The LA Times reported that OpenAI had contacted talent agencies and studios before the launch, telling them that IP holders ‚Äúwould have to explicitly ask OpenAI not to include their copyright material in videos the tool creates.‚Äù&lt;/p&gt;
&lt;p&gt;Hollywood‚Äôs response to Sora 2 was swift and generally negative. According to CNBC, the Creative Artists Agency called it a ‚Äúsignificant risk‚Äù to its clients, while United Talent Agency labeled it ‚Äúexploitation, not innovation.‚Äù The WME talent agency sent a memo to agents notifying OpenAI that all of the agency‚Äôs clients were opted out of Sora. The Motion Picture Association also demanded ‚Äúimmediate and decisive action‚Äù from OpenAI.&lt;/p&gt;
&lt;p&gt;OpenAI CEO Sam Altman reversed course within days of the reaction, promising to give rights holders ‚Äúmore granular control‚Äù and floating a potential revenue-sharing model. The company also partnered with actor Bryan Cranston and SAG-AFTRA in October to implement new safety guardrails around likeness rights.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2131491 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="OpenAI and Disney announcement graphic." class="center large" height="576" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/OAI_Disney_Hero_16x9-1024x576.webp" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          OpenAI

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;While Disney and OpenAI are apparently friends now, the company has simultaneously taken an aggressive stance against some AI companies it has not partnered with.&lt;/p&gt;
&lt;p&gt;On Wednesday, Disney sent a cease-and-desist letter to Google, accusing the company of ‚Äúinfringing Disney‚Äôs copyrights on a massive scale‚Äù through its AI services, including YouTube. Disney has also sent similar letters to Meta and Character.AI and filed lawsuits against image-synthesis service Midjourney alongside NBCUniversal and Warner Bros. Discovery.&lt;/p&gt;
&lt;p&gt;A few major questions about the deal remain unanswered, including the actual licensing fees, whether Disney content will be used to train future OpenAI models, and whether this deal is even finalized. The announcement also notes it remains ‚Äúsubject to negotiation of definitive agreements,‚Äù so expect potential updates or clarifications ahead.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/12/disney-invests-1-billion-in-openai-licenses-200-characters-for-ai-video-app-sora/</guid><pubDate>Thu, 11 Dec 2025 16:43:30 +0000</pubDate></item><item><title>Agent Lightning: Adding reinforcement learning to AI agents without code rewrites (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/agent-lightning-adding-reinforcement-learning-to-ai-agents-without-code-rewrites/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Three white line icons on a blue-to-purple gradient background: the first icon shows a simple flowchart with connected squares and a diamond, the second icon shows a network of interconnected circles, and the third icon shows three user profile symbols linked together." class="wp-image-1158206" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/AgentLightning-BlogHeroFeature-1400x788_NEW.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;AI agents are reshaping software development, from writing code to carrying out complex instructions. Yet LLM-based agents are prone to errors and often perform poorly on complicated, multi-step tasks. Reinforcement learning (RL) is an approach where AI systems learn to make optimal decisions by receiving rewards or penalties for their actions, improving through trial and error. RL can help agents improve, but it typically requires developers to extensively rewrite their code. This discourages adoption, even though the data these agents generate could significantly boost performance through RL training.&lt;/p&gt;



&lt;p&gt;To address this, a research team from Microsoft Research Asia ‚Äì Shanghai has introduced Agent Lightning. This open-source&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; framework makes AI agents trainable through RL by separating how agents execute tasks from model training, allowing developers to add RL capabilities with virtually no code modification.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="capturing-agent-behavior-for-training"&gt;Capturing agent behavior for training&lt;/h2&gt;



&lt;p&gt;Agent Lightning converts an agent‚Äôs experience into a format that RL can use by treating the agent‚Äôs execution as a sequence of states and actions, where each state captures the agent‚Äôs status and each LLM call is an action that moves the agent to a new state.&lt;/p&gt;



&lt;p&gt;This approach works for any workflow, no matter how complex. Whether it involves multiple collaborating agents or dynamic tool use, Agent Lightning breaks it down into a sequence of transitions. Each transition captures the LLM‚Äôs input, output, and reward (Figure 1). This standardized format means the data can be used for training without any&amp;nbsp;additional&amp;nbsp;steps.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1: Diagram illustrating Agent Lightning‚Äôs unified data interface for a retrieval-augmented generation (RAG) agent. On the left, four states (state‚ÇÄ to state‚ÇÉ) show the agent‚Äôs execution flow, where semantic variables‚ÄîUserInput, Query, Passages, and Answer‚Äîare updated after each component call (LLM or Search). Green blocks represent populated variables; gray blocks indicate empty ones. On the right, the unified data interface converts these transitions into a trajectory format containing prompt, generation, and immediate reward for RL training. " class="wp-image-1158102" height="891" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-1-scaled.jpg" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. An illustration of Agent Lightning‚Äôs standardized format using a retrieval-augmented generation (RAG) agent. Left: The full agent workflow, where the agent‚Äôs state updates after each component step. The green blocks show assigned variables, and the gray blocks indicate variables without content. Right: The collected transitions are based on the standardized format for the RL training process, with each transition corresponding to one LLM step that contains its prompt, result, and immediate reward.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="hierarchical-reinforcement-learning"&gt;Hierarchical reinforcement learning&lt;/h2&gt;



&lt;p&gt;Traditional RL training for agents that make multiple LLM requests involves stitching together all content into one long sequence and then identifying which parts should be learned and which ignored during training. This approach is difficult to implement and can create excessively long sequences that degrade model performance.&lt;/p&gt;



&lt;p&gt;Instead, Agent Lightning‚Äôs LightningRL algorithm takes a hierarchical approach. After a task completes, a credit assignment module determines how much each LLM request contributed to the outcome and assigns it a corresponding reward. These independent steps, now paired with their own reward scores, can be used with any existing single-step RL algorithm, such as Proximal Policy Optimization (PPO) or Group Relative Policy Optimization (GRPO) (Figure 2).&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 2: Comparison of three reinforcement learning approaches for LLM tasks. (a) Single-step GRPO: The model completes the task in one call, and multiple outputs for the same task are compared with associated rewards. (b) Previous multi-step GRPO: The task spans multiple LLM calls, forming trajectories; non-LLM tokens (gray boxes) are ignored during training, and entire multi-step runs are compared. (c) LightningRL: Breaks multi-step runs into individual LLM calls, each including input, context, output, and reward assigned by a credit assignment module. Calls from the same task are grouped for reinforcement. " class="wp-image-1158259" height="835" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-2-new-scaled.jpg" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. (a) Single-step GRPO: The LLM completes the task in one call. Multiple responses for the same task are compared to determine how strongly each should be reinforced. (b) Previous multi-step GRPO: The task involves multiple LLM calls. Multiple multi-step runs of the same task are compared, with non-LLM generated tokens (grey boxes) ignored during training. (c) LightningRL: The multi-step run is divided into individual LLM calls. Calls from the same task are compared to determine how strongly each should be reinforced. Each call includes its input, context, output, and reward, assigned by the credit assignment module.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;This design offers several benefits. It remains fully compatible with widely used single-step RL algorithms, allowing existing training methods to be applied without modification. Organizing data as a sequence of independent transitions lets developers flexibly construct the LLM input as needed, supporting complex behaviors like agents that use multiple tools or&amp;nbsp;work&amp;nbsp;with other agents. Additionally, by keeping sequences short, the approach scales cleanly and keeps training efficient.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="agent-lightning-as-middleware"&gt;Agent Lightning as middleware&lt;/h2&gt;



&lt;p&gt;Agent Lightning serves as middleware between RL algorithms and agent environments, providing modular components that enable scalable RL through standardized protocols and well-defined interfaces.&lt;/p&gt;



&lt;p&gt;An &lt;strong&gt;agent runner&lt;/strong&gt; manages the agents as they complete tasks. It distributes work and collects and stores the results and progress data. It operates separately from the LLMs, enabling them to run on different resources and scale to support multiple agents running concurrently.&lt;/p&gt;



&lt;p&gt;An &lt;strong&gt;algorithm&lt;/strong&gt; trains the models and hosts the LLMs used for inference and training. It orchestrates the overall RL cycle, managing which tasks are assigned, how agents complete them, and how models are updated based on what the agents learn. It typically runs on GPU resources and communicates with the agent runner through shared protocols.&lt;/p&gt;



&lt;p&gt;The LightningStore&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; serves as the central repository for all data exchanges within the system. It provides standardized interfaces and a shared format, ensuring that the different components can work together and enabling the algorithm and agent runner to communicate effectively.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 3: Diagram showing the architecture of Agent Lightning (AGL). On the left, the AGL Algorithm block includes an inference engine (e.g., vLLM), an algorithm iteration loop, and an adapter for trainable data and weights update. In the center, the AGL Core contains LightningStore, which manages tasks, resources, spans, and LLM calls. On the right, the AGL Agent Runner &amp;amp; Tracer includes a user-defined agent using OpenAI chat completion and agl.emit(). Arrows indicate flows of prompts, responses, tasks, resources, spans, and datasets between components, with roles for algorithm researchers and agent developers highlighted. " class="wp-image-1158104" height="942" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-3-scaled.png" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 3. The Agent Lightning framework&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;All RL cycles follow two steps: (1) Agent Lightning collects agent execution data (called ‚Äúspans‚Äù) and store them in the data store; (2) it then retrieves the required data and sends it to the algorithm for training. Through this design, the algorithm can delegate tasks asynchronously to the agent runner, which completes them and reports the results back (Figure 4).&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Figure 4: Diagram of the training loop in Agent Lightning. The central element is ‚ÄòTrainer,‚Äô with arrows forming a cycle between three components: Agent on the left, Algorithm on the right, and Trainer in the middle. The top arrow labeled ‚ÄòTasks‚Äô flows from Algorithm to Agent, while the bottom arrow labeled ‚ÄòSpans‚Äô flows from Agent to Algorithm. ‚ÄòPrompt Templates‚Äô is noted above the cycle, indicating its role in task generation. " class="wp-image-1158290" height="840" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-4-new.png" width="2347" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 4. Agent Lightning‚Äôs RL cycle&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;One key advantage of this approach is its algorithmic flexibility. The system makes it easy for developers to customize how agents learn, whether they‚Äôre defining different rewards, capturing intermediate data, or experimenting with different training approaches.&lt;/p&gt;



&lt;p&gt;Another advantage is resource efficiency. Agentic RL systems are complex, integrating agentic systems, LLM inference engines, and training frameworks. By separating these components, Agent Lightning makes this complexity manageable and allows each part to be optimized independently&lt;/p&gt;



&lt;p&gt;A decoupled design allows each component to use the hardware that suits it best. The agent runner can use CPUs while model training uses GPUs. Each component can also scale independently, improving efficiency and making the system easier to maintain. In practice, developers can keep their existing agent frameworks and switch model calls to the Agent Lightning API without changing their agent code (Figure 5). &lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 5: Side-by-side code comparison showing agent implementation before and after integrating Agent Lightning. The left panel (dark background) displays the original agent code written by the developer, including logic for LLM calls, tool usage, and reward assignment. The right panel (light background) shows the modified version using Agent Lightning, where most of the agent logic remains unchanged but includes additional imports and calls to Agent Lightning components such as agl.PromptTemplate, agl.emit(), and agl.Trainer for training and credit assignment. A stylized lightning icon is centered between the two panels. " class="wp-image-1158107" height="1024" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-5-new.png" width="1238" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 5. On the left, the developer implements the agent code. On the bottom right is the code required for Agent Lightning. The main body of the agent code is unchanged.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="evaluation-across-three-real-world-scenarios"&gt;Evaluation across three real-world scenarios&lt;/h2&gt;



&lt;p&gt;Agent Lightning was tested on three distinct tasks, achieving consistent performance improvements across all scenarios (Figure 6):&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Text-to-SQL (LangChain):&lt;/strong&gt; In a system with three agents handling SQL generation, checking, and rewriting, Agent Lightning simultaneously optimized two of them, significantly improving the accuracy of generating executable SQL from natural language queries.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Retrieval-augmented generation (OpenAI Agents SDK implementation):&lt;/strong&gt; On the multi-hop question-answering dataset MuSiQue, which requires querying a large Wikipedia database, Agent Lightning helped the agent generate more effective search queries and reason better from retrieved content.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Mathematical QA and tool use (AutoGen implementation):&lt;/strong&gt; For complex math problems, Agent Lightning trained LLMs to more accurately determine when and how to call the tool and integrate the results into its reasoning, increasing accuracy.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 6: Figure with six line charts showing reward curves across three evaluation scenarios (Spider, MuSiQue, Calculator) for train and test splits. Top row: Train Rewards on Spider, MuSiQue, and Calculator‚Äîeach plot shows a blue line with noisy upward trend over steps, indicating increasing rewards; Spider and Calculator rise faster with more variance, MuSiQue climbs more gradually. Bottom row: Test Rewards on Spider, MuSiQue, and Calculator‚Äîeach plot shows a blue line that increases and then stabilizes at higher rewards; Calculator reaches near-plateau earliest, Spider shows steady gains with minor fluctuations, MuSiQue improves more slowly. All plots use ‚ÄòSteps‚Äô on the x‚Äëaxis and ‚ÄòRewards‚Äô on the y‚Äëaxis, with a legend labeled ‚Äòours‚Äô and light gridlines. " class="wp-image-1158109" height="1169" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-6-scaled.jpg" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 6. Reward curves across the three evaluation scenarios&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="enabling-continuous-agent-improvement"&gt;Enabling continuous agent improvement&lt;/h2&gt;



&lt;p&gt;By simplifying RL integration, Agent Lightning can make it easier for developers to build, iterate, and deploy high-performance agents. We plan to expand Agent Lightning‚Äôs capabilities to include automatic prompt optimization and additional RL algorithms.&lt;/p&gt;



&lt;p&gt;The framework is designed to serve as an open platform where any AI agent can improve through real-world practice. By bridging existing agentic&amp;nbsp;systems with reinforcement learning, Agent Lightning aims to help create AI systems that learn from experience and improve over time.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Three white line icons on a blue-to-purple gradient background: the first icon shows a simple flowchart with connected squares and a diamond, the second icon shows a network of interconnected circles, and the third icon shows three user profile symbols linked together." class="wp-image-1158206" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/AgentLightning-BlogHeroFeature-1400x788_NEW.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;AI agents are reshaping software development, from writing code to carrying out complex instructions. Yet LLM-based agents are prone to errors and often perform poorly on complicated, multi-step tasks. Reinforcement learning (RL) is an approach where AI systems learn to make optimal decisions by receiving rewards or penalties for their actions, improving through trial and error. RL can help agents improve, but it typically requires developers to extensively rewrite their code. This discourages adoption, even though the data these agents generate could significantly boost performance through RL training.&lt;/p&gt;



&lt;p&gt;To address this, a research team from Microsoft Research Asia ‚Äì Shanghai has introduced Agent Lightning. This open-source&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; framework makes AI agents trainable through RL by separating how agents execute tasks from model training, allowing developers to add RL capabilities with virtually no code modification.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="capturing-agent-behavior-for-training"&gt;Capturing agent behavior for training&lt;/h2&gt;



&lt;p&gt;Agent Lightning converts an agent‚Äôs experience into a format that RL can use by treating the agent‚Äôs execution as a sequence of states and actions, where each state captures the agent‚Äôs status and each LLM call is an action that moves the agent to a new state.&lt;/p&gt;



&lt;p&gt;This approach works for any workflow, no matter how complex. Whether it involves multiple collaborating agents or dynamic tool use, Agent Lightning breaks it down into a sequence of transitions. Each transition captures the LLM‚Äôs input, output, and reward (Figure 1). This standardized format means the data can be used for training without any&amp;nbsp;additional&amp;nbsp;steps.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1: Diagram illustrating Agent Lightning‚Äôs unified data interface for a retrieval-augmented generation (RAG) agent. On the left, four states (state‚ÇÄ to state‚ÇÉ) show the agent‚Äôs execution flow, where semantic variables‚ÄîUserInput, Query, Passages, and Answer‚Äîare updated after each component call (LLM or Search). Green blocks represent populated variables; gray blocks indicate empty ones. On the right, the unified data interface converts these transitions into a trajectory format containing prompt, generation, and immediate reward for RL training. " class="wp-image-1158102" height="891" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-1-scaled.jpg" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. An illustration of Agent Lightning‚Äôs standardized format using a retrieval-augmented generation (RAG) agent. Left: The full agent workflow, where the agent‚Äôs state updates after each component step. The green blocks show assigned variables, and the gray blocks indicate variables without content. Right: The collected transitions are based on the standardized format for the RL training process, with each transition corresponding to one LLM step that contains its prompt, result, and immediate reward.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="hierarchical-reinforcement-learning"&gt;Hierarchical reinforcement learning&lt;/h2&gt;



&lt;p&gt;Traditional RL training for agents that make multiple LLM requests involves stitching together all content into one long sequence and then identifying which parts should be learned and which ignored during training. This approach is difficult to implement and can create excessively long sequences that degrade model performance.&lt;/p&gt;



&lt;p&gt;Instead, Agent Lightning‚Äôs LightningRL algorithm takes a hierarchical approach. After a task completes, a credit assignment module determines how much each LLM request contributed to the outcome and assigns it a corresponding reward. These independent steps, now paired with their own reward scores, can be used with any existing single-step RL algorithm, such as Proximal Policy Optimization (PPO) or Group Relative Policy Optimization (GRPO) (Figure 2).&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 2: Comparison of three reinforcement learning approaches for LLM tasks. (a) Single-step GRPO: The model completes the task in one call, and multiple outputs for the same task are compared with associated rewards. (b) Previous multi-step GRPO: The task spans multiple LLM calls, forming trajectories; non-LLM tokens (gray boxes) are ignored during training, and entire multi-step runs are compared. (c) LightningRL: Breaks multi-step runs into individual LLM calls, each including input, context, output, and reward assigned by a credit assignment module. Calls from the same task are grouped for reinforcement. " class="wp-image-1158259" height="835" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-2-new-scaled.jpg" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. (a) Single-step GRPO: The LLM completes the task in one call. Multiple responses for the same task are compared to determine how strongly each should be reinforced. (b) Previous multi-step GRPO: The task involves multiple LLM calls. Multiple multi-step runs of the same task are compared, with non-LLM generated tokens (grey boxes) ignored during training. (c) LightningRL: The multi-step run is divided into individual LLM calls. Calls from the same task are compared to determine how strongly each should be reinforced. Each call includes its input, context, output, and reward, assigned by the credit assignment module.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;This design offers several benefits. It remains fully compatible with widely used single-step RL algorithms, allowing existing training methods to be applied without modification. Organizing data as a sequence of independent transitions lets developers flexibly construct the LLM input as needed, supporting complex behaviors like agents that use multiple tools or&amp;nbsp;work&amp;nbsp;with other agents. Additionally, by keeping sequences short, the approach scales cleanly and keeps training efficient.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="agent-lightning-as-middleware"&gt;Agent Lightning as middleware&lt;/h2&gt;



&lt;p&gt;Agent Lightning serves as middleware between RL algorithms and agent environments, providing modular components that enable scalable RL through standardized protocols and well-defined interfaces.&lt;/p&gt;



&lt;p&gt;An &lt;strong&gt;agent runner&lt;/strong&gt; manages the agents as they complete tasks. It distributes work and collects and stores the results and progress data. It operates separately from the LLMs, enabling them to run on different resources and scale to support multiple agents running concurrently.&lt;/p&gt;



&lt;p&gt;An &lt;strong&gt;algorithm&lt;/strong&gt; trains the models and hosts the LLMs used for inference and training. It orchestrates the overall RL cycle, managing which tasks are assigned, how agents complete them, and how models are updated based on what the agents learn. It typically runs on GPU resources and communicates with the agent runner through shared protocols.&lt;/p&gt;



&lt;p&gt;The LightningStore&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; serves as the central repository for all data exchanges within the system. It provides standardized interfaces and a shared format, ensuring that the different components can work together and enabling the algorithm and agent runner to communicate effectively.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 3: Diagram showing the architecture of Agent Lightning (AGL). On the left, the AGL Algorithm block includes an inference engine (e.g., vLLM), an algorithm iteration loop, and an adapter for trainable data and weights update. In the center, the AGL Core contains LightningStore, which manages tasks, resources, spans, and LLM calls. On the right, the AGL Agent Runner &amp;amp; Tracer includes a user-defined agent using OpenAI chat completion and agl.emit(). Arrows indicate flows of prompts, responses, tasks, resources, spans, and datasets between components, with roles for algorithm researchers and agent developers highlighted. " class="wp-image-1158104" height="942" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-3-scaled.png" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 3. The Agent Lightning framework&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;All RL cycles follow two steps: (1) Agent Lightning collects agent execution data (called ‚Äúspans‚Äù) and store them in the data store; (2) it then retrieves the required data and sends it to the algorithm for training. Through this design, the algorithm can delegate tasks asynchronously to the agent runner, which completes them and reports the results back (Figure 4).&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Figure 4: Diagram of the training loop in Agent Lightning. The central element is ‚ÄòTrainer,‚Äô with arrows forming a cycle between three components: Agent on the left, Algorithm on the right, and Trainer in the middle. The top arrow labeled ‚ÄòTasks‚Äô flows from Algorithm to Agent, while the bottom arrow labeled ‚ÄòSpans‚Äô flows from Agent to Algorithm. ‚ÄòPrompt Templates‚Äô is noted above the cycle, indicating its role in task generation. " class="wp-image-1158290" height="840" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-4-new.png" width="2347" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 4. Agent Lightning‚Äôs RL cycle&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;One key advantage of this approach is its algorithmic flexibility. The system makes it easy for developers to customize how agents learn, whether they‚Äôre defining different rewards, capturing intermediate data, or experimenting with different training approaches.&lt;/p&gt;



&lt;p&gt;Another advantage is resource efficiency. Agentic RL systems are complex, integrating agentic systems, LLM inference engines, and training frameworks. By separating these components, Agent Lightning makes this complexity manageable and allows each part to be optimized independently&lt;/p&gt;



&lt;p&gt;A decoupled design allows each component to use the hardware that suits it best. The agent runner can use CPUs while model training uses GPUs. Each component can also scale independently, improving efficiency and making the system easier to maintain. In practice, developers can keep their existing agent frameworks and switch model calls to the Agent Lightning API without changing their agent code (Figure 5). &lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 5: Side-by-side code comparison showing agent implementation before and after integrating Agent Lightning. The left panel (dark background) displays the original agent code written by the developer, including logic for LLM calls, tool usage, and reward assignment. The right panel (light background) shows the modified version using Agent Lightning, where most of the agent logic remains unchanged but includes additional imports and calls to Agent Lightning components such as agl.PromptTemplate, agl.emit(), and agl.Trainer for training and credit assignment. A stylized lightning icon is centered between the two panels. " class="wp-image-1158107" height="1024" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-5-new.png" width="1238" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 5. On the left, the developer implements the agent code. On the bottom right is the code required for Agent Lightning. The main body of the agent code is unchanged.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="evaluation-across-three-real-world-scenarios"&gt;Evaluation across three real-world scenarios&lt;/h2&gt;



&lt;p&gt;Agent Lightning was tested on three distinct tasks, achieving consistent performance improvements across all scenarios (Figure 6):&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Text-to-SQL (LangChain):&lt;/strong&gt; In a system with three agents handling SQL generation, checking, and rewriting, Agent Lightning simultaneously optimized two of them, significantly improving the accuracy of generating executable SQL from natural language queries.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Retrieval-augmented generation (OpenAI Agents SDK implementation):&lt;/strong&gt; On the multi-hop question-answering dataset MuSiQue, which requires querying a large Wikipedia database, Agent Lightning helped the agent generate more effective search queries and reason better from retrieved content.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Mathematical QA and tool use (AutoGen implementation):&lt;/strong&gt; For complex math problems, Agent Lightning trained LLMs to more accurately determine when and how to call the tool and integrate the results into its reasoning, increasing accuracy.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 6: Figure with six line charts showing reward curves across three evaluation scenarios (Spider, MuSiQue, Calculator) for train and test splits. Top row: Train Rewards on Spider, MuSiQue, and Calculator‚Äîeach plot shows a blue line with noisy upward trend over steps, indicating increasing rewards; Spider and Calculator rise faster with more variance, MuSiQue climbs more gradually. Bottom row: Test Rewards on Spider, MuSiQue, and Calculator‚Äîeach plot shows a blue line that increases and then stabilizes at higher rewards; Calculator reaches near-plateau earliest, Spider shows steady gains with minor fluctuations, MuSiQue improves more slowly. All plots use ‚ÄòSteps‚Äô on the x‚Äëaxis and ‚ÄòRewards‚Äô on the y‚Äëaxis, with a legend labeled ‚Äòours‚Äô and light gridlines. " class="wp-image-1158109" height="1169" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-6-scaled.jpg" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 6. Reward curves across the three evaluation scenarios&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="enabling-continuous-agent-improvement"&gt;Enabling continuous agent improvement&lt;/h2&gt;



&lt;p&gt;By simplifying RL integration, Agent Lightning can make it easier for developers to build, iterate, and deploy high-performance agents. We plan to expand Agent Lightning‚Äôs capabilities to include automatic prompt optimization and additional RL algorithms.&lt;/p&gt;



&lt;p&gt;The framework is designed to serve as an open platform where any AI agent can improve through real-world practice. By bridging existing agentic&amp;nbsp;systems with reinforcement learning, Agent Lightning aims to help create AI systems that learn from experience and improve over time.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/agent-lightning-adding-reinforcement-learning-to-ai-agents-without-code-rewrites/</guid><pubDate>Thu, 11 Dec 2025 17:00:00 +0000</pubDate></item><item><title>Runway releases its first world model, adds native audio to latest video model (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/11/runway-releases-its-first-world-model-adds-native-audio-to-latest-video-model/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The race to release world models is on as AI image and video generation company Runway joins an increasing number of startups and Big Tech companies by launching its first one. Dubbed GWM-1, the model works through frame-by-frame prediction, creating a simulation with an understanding of physics and how the world actually behaves over time, the company said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A world model is an AI system that learns an internal simulation of how the world works so it can reason, plan, and act without needing to be trained on every scenario possible in real life. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Runway, which earlier this month launched its Gen 4.5 video model that surpassed both Google and OpenAI on the Video Arena leaderboard, said its GWM-1 world model is more ‚Äúgeneral‚Äù than Google‚Äôs Genie-3 and other competitors. The firm is pitching it as a model that can create simulations to train agents in different domains like robotics and life sciences.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúTo build a world model, we first needed to build a really great video model. We believe that the right path to building a world model is teaching models to predict pixels directly is the best way to achieve general-purpose simulation. At sufficient scale and with the right data, you can build a model that has sufficient understanding of how the world works,‚Äù the company‚Äôs CTO, Anastasis Germanidis, said during the livestream.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Runway released specific slants or versions to the new world model called GWM-Worlds, GWM-Robotics, and GWM-Avatars.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3075175" height="380" src="https://techcrunch.com/wp-content/uploads/2025/12/Worlds.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Runway&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;GWM-Worlds is an app for the model that lets you create an interactive project. Users can set a scene through a prompt or an image reference, and as you explore the space, the model generates the world with an understanding of geometry, physics, and lighting. The company mentioned that the simulation runs at 24 fps and 720p resolution. Runway said that while Worlds could be useful for gaming, it‚Äôs also well-positioned to teach agents how to navigate and behave in the physical world.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With GWM-Robotics, the company aims to use synthetic data enriched with new parameters like changing  weather conditions or obstacles. Runway says this method could also reveal when and how robots might violate policies and instructions in different scenarios.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Runway is also building realistic avatars under GWM-Avatars to simulate human behavior. Companies like D-ID, Synthesia, Soul Machines, and even Google have worked on creating human avatars that look real and work in areas like communication and training.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company noted that technically Worlds, Robotics, and Avatars are separate models, but eventually it plans to merge all these into one model.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Besides releasing a new world model, the company is also updating its foundational Gen 4.5 model released earlier in the month. The new update brings native audio and long-form, multi-shot generation capabilities to the model. The company said that with this model, users can generate one-minute videos with character consistency, native dialogue, background audio, and complex shots from various angles. The company said that you can also edit existing audio and add dialogues. Plus, you can edit multi-shot videos of any length.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Gen 4.5 update nudges Runway closer to competitor Kling‚Äôs all-in-one video suite, which also launched earlier this month, particularly around native audio and multi-shot storytelling. It also signals that video generation models are moving from prototype to production-ready tools. Runway‚Äôs updated Gen 4.5 model is available to all paid plan users.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3075176" height="383" src="https://techcrunch.com/wp-content/uploads/2025/12/G4p5.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Runway&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company said that it will make GWM-Robotics available through an SDK. It added that it is in active conversation with several robotics firms and enterprises for the use of GWM-Robotics and GWM-Avatars.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The race to release world models is on as AI image and video generation company Runway joins an increasing number of startups and Big Tech companies by launching its first one. Dubbed GWM-1, the model works through frame-by-frame prediction, creating a simulation with an understanding of physics and how the world actually behaves over time, the company said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A world model is an AI system that learns an internal simulation of how the world works so it can reason, plan, and act without needing to be trained on every scenario possible in real life. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Runway, which earlier this month launched its Gen 4.5 video model that surpassed both Google and OpenAI on the Video Arena leaderboard, said its GWM-1 world model is more ‚Äúgeneral‚Äù than Google‚Äôs Genie-3 and other competitors. The firm is pitching it as a model that can create simulations to train agents in different domains like robotics and life sciences.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúTo build a world model, we first needed to build a really great video model. We believe that the right path to building a world model is teaching models to predict pixels directly is the best way to achieve general-purpose simulation. At sufficient scale and with the right data, you can build a model that has sufficient understanding of how the world works,‚Äù the company‚Äôs CTO, Anastasis Germanidis, said during the livestream.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Runway released specific slants or versions to the new world model called GWM-Worlds, GWM-Robotics, and GWM-Avatars.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3075175" height="380" src="https://techcrunch.com/wp-content/uploads/2025/12/Worlds.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Runway&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;GWM-Worlds is an app for the model that lets you create an interactive project. Users can set a scene through a prompt or an image reference, and as you explore the space, the model generates the world with an understanding of geometry, physics, and lighting. The company mentioned that the simulation runs at 24 fps and 720p resolution. Runway said that while Worlds could be useful for gaming, it‚Äôs also well-positioned to teach agents how to navigate and behave in the physical world.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With GWM-Robotics, the company aims to use synthetic data enriched with new parameters like changing  weather conditions or obstacles. Runway says this method could also reveal when and how robots might violate policies and instructions in different scenarios.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Runway is also building realistic avatars under GWM-Avatars to simulate human behavior. Companies like D-ID, Synthesia, Soul Machines, and even Google have worked on creating human avatars that look real and work in areas like communication and training.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company noted that technically Worlds, Robotics, and Avatars are separate models, but eventually it plans to merge all these into one model.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Besides releasing a new world model, the company is also updating its foundational Gen 4.5 model released earlier in the month. The new update brings native audio and long-form, multi-shot generation capabilities to the model. The company said that with this model, users can generate one-minute videos with character consistency, native dialogue, background audio, and complex shots from various angles. The company said that you can also edit existing audio and add dialogues. Plus, you can edit multi-shot videos of any length.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Gen 4.5 update nudges Runway closer to competitor Kling‚Äôs all-in-one video suite, which also launched earlier this month, particularly around native audio and multi-shot storytelling. It also signals that video generation models are moving from prototype to production-ready tools. Runway‚Äôs updated Gen 4.5 model is available to all paid plan users.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3075176" height="383" src="https://techcrunch.com/wp-content/uploads/2025/12/G4p5.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Runway&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company said that it will make GWM-Robotics available through an SDK. It added that it is in active conversation with several robotics firms and enterprises for the use of GWM-Robotics and GWM-Avatars.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/11/runway-releases-its-first-world-model-adds-native-audio-to-latest-video-model/</guid><pubDate>Thu, 11 Dec 2025 17:00:00 +0000</pubDate></item><item><title>Google debuts ‚ÄòDisco,‚Äô a Gemini-powered tool for making web apps from browser tabs (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/11/google-debuts-disco-a-gemini-powered-tool-for-making-web-apps-from-browser-tabs/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google on Thursday introduced a new AI experiment for the web browser: the Gemini-powered product Disco, which helps to turn your open tabs into custom applications. With Disco, you can create what Google is calling ‚ÄúGenTabs,‚Äù a tool that proactively suggests interactive web apps that can help you complete tasks related to what you‚Äôre browsing and allows you to build your own apps via written prompts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, if you‚Äôre studying a particular subject, GenTabs might suggest building a web app to visualize the information, which could help you better understand the core principles. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3075230" height="377" src="https://techcrunch.com/wp-content/uploads/2025/12/1_Solar-System.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Or, in a less academic scenario, you could use GenTabs to help you create a meal plan from a series of online recipes or help you plan a trip when you‚Äôre researching travel.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;These are things that you can already do today with some AI-powered chatbots, but GenTabs builds these custom experiences on the fly using Gemini 3, using the information in your browser and in your Gemini chat history. After the app is built, you can also continue to refine it using natural language commands.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The resulting generative elements in the GenTabs experience will link back to the original sources, Google notes. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3075228" height="377" src="https://techcrunch.com/wp-content/uploads/2025/12/3_Meal-Plan.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Like others in the AI market, Google has been experimenting with bringing AI deeper into the web-browsing experience. Instead of building its own stand-alone AI browser, like Perplexity‚Äôs Comet or ChatGPT Atlas, Google integrated its AI assistant Gemini into the Chrome browser, where it can optionally be used to ask questions about the web page you‚Äôre on.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With GenTabs, the focus is not only on what you‚Äôre currently viewing, but also on your overall browsing, spanning multiple tabs ‚Äî whether that‚Äôs research, learning, or something else.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;However, the feature is only initially going to be available to a small number of testers through Google Labs, who will offer feedback about the experience. The company says that interesting ideas that are developed through Disco may one day find their way into other, larger Google products. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It also suggests that GenTabs will be one of many Disco features to come over time, noting that GenTabs is the ‚Äúfirst feature‚Äù being tested. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To access Disco, users will need to join a waitlist to download the app, starting on macOS.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google on Thursday introduced a new AI experiment for the web browser: the Gemini-powered product Disco, which helps to turn your open tabs into custom applications. With Disco, you can create what Google is calling ‚ÄúGenTabs,‚Äù a tool that proactively suggests interactive web apps that can help you complete tasks related to what you‚Äôre browsing and allows you to build your own apps via written prompts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, if you‚Äôre studying a particular subject, GenTabs might suggest building a web app to visualize the information, which could help you better understand the core principles. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3075230" height="377" src="https://techcrunch.com/wp-content/uploads/2025/12/1_Solar-System.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Or, in a less academic scenario, you could use GenTabs to help you create a meal plan from a series of online recipes or help you plan a trip when you‚Äôre researching travel.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;These are things that you can already do today with some AI-powered chatbots, but GenTabs builds these custom experiences on the fly using Gemini 3, using the information in your browser and in your Gemini chat history. After the app is built, you can also continue to refine it using natural language commands.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The resulting generative elements in the GenTabs experience will link back to the original sources, Google notes. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3075228" height="377" src="https://techcrunch.com/wp-content/uploads/2025/12/3_Meal-Plan.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Like others in the AI market, Google has been experimenting with bringing AI deeper into the web-browsing experience. Instead of building its own stand-alone AI browser, like Perplexity‚Äôs Comet or ChatGPT Atlas, Google integrated its AI assistant Gemini into the Chrome browser, where it can optionally be used to ask questions about the web page you‚Äôre on.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With GenTabs, the focus is not only on what you‚Äôre currently viewing, but also on your overall browsing, spanning multiple tabs ‚Äî whether that‚Äôs research, learning, or something else.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;However, the feature is only initially going to be available to a small number of testers through Google Labs, who will offer feedback about the experience. The company says that interesting ideas that are developed through Disco may one day find their way into other, larger Google products. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It also suggests that GenTabs will be one of many Disco features to come over time, noting that GenTabs is the ‚Äúfirst feature‚Äù being tested. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To access Disco, users will need to join a waitlist to download the app, starting on macOS.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/11/google-debuts-disco-a-gemini-powered-tool-for-making-web-apps-from-browser-tabs/</guid><pubDate>Thu, 11 Dec 2025 18:00:00 +0000</pubDate></item><item><title>OpenAI fires back at Google with GPT-5.2 after ‚Äòcode red‚Äô memo (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/11/openai-fires-back-at-google-with-gpt-5-2-after-code-red-memo/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/GettyImages-2213399157.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI launched its latest frontier model, GPT-5.2, on Thursday amid increasing competition from Google, pitching it as its most advanced model yet and one designed for developers and everyday professional use.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI‚Äôs GPT-5.2 is coming to ChatGPT paid users and developers via the API in three flavors: Instant, a speed-optimized model for routine queries like information-seeking, writing, and translation; Thinking, which excels at complex structured work like coding, analyzing long documents, math, and planning; and Pro, the top-end model aimed at delivering maximum accuracy and reliability for difficult problems.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;‚ÄúWe designed 5.2 to unlock even more economic value for people,‚Äù Fidji Simo, OpenAI‚Äôs chief product officer, said Thursday during a briefing with journalists. ‚ÄúIt‚Äôs better at creating spreadsheets, building presentations, writing code, perceiving images, understanding long context, using tools and then linking complex, multi-step projects.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GPT-5.2 lands in the middle of an arms race with Google‚Äôs Gemini 3, which is topping LMArena‚Äôs leaderboard across most benchmarks (apart from coding ‚Äî which Anthropic‚Äôs Claude Opus-4.5 still has on lock).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Early this month, The Information reported that CEO Sam Altman released an internal ‚Äúcode red‚Äù memo to staff amid ChatGPT traffic decline and concerns that it is losing consumer market share to Google. The code red called for a shift in priorities, including stalling on commitments like introducing ads and instead focusing on creating a better ChatGPT experience.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GPT-5.2 is OpenAI‚Äôs push to reclaim leadership, even as some employees reportedly asked for the model release to be pushed back so the company could have more time to improve it. And despite indications that OpenAI would focus its attention on consumer use cases by adding more personalization and customization to ChatGPT, the launch of GPT-5.2 looks to beef up its enterprise opportunities.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is specifically targeting developers and the tooling ecosystem, aiming to become the default foundation for building AI-powered applications. Earlier this week, OpenAI released new data showing enterprise usage of its AI tools has surged dramatically over the past year.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;This comes as Gemini 3 has become tightly integrated into Google‚Äôs product and cloud ecosystem for multimodal and agentic workflows. Google this week launched managed MCP servers that make its Google and Cloud services like Maps and BigQuery easier for agents to plug into. (MCPs are the connectors between AI systems and data and tools.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI says GPT-5.2 sets new benchmark scores in coding, math, science, vision, long-context reasoning, and tool use, which the company claims could lead to ‚Äúmore reliable agentic workflows, production-grade code, and complex systems that operate across large contexts and real-world data.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Those capabilities put it in direct competition with Gemini 3‚Äôs Deep Think mode, which has been touted as a major reasoning advancement targeting math, logic, and science. On OpenAI‚Äôs own benchmark chart, GPT-5.2 Thinking edges out Gemini 3 and Anthropic‚Äôs Claude Opus 4.5 in nearly every listed reasoning test, from real-world software engineering tasks (SWE-Bench Pro) and doctoral-level science knowledge (GPQA Diamond) to abstract reasoning and pattern discovery (ARC-AGI suites).&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Research lead Aidan Clark said that stronger math scores aren‚Äôt just about solving equations. Mathematical reasoning, he explained, is a proxy for whether a model can follow multi-step logic, keep numbers consistent over time, and avoid subtle errors that could compound over time.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúThese are all properties that really matter across a wide range of different workloads,‚Äù Clark said. ‚ÄúThings like financial modeling, forecasting, doing an analysis of data.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During the briefing, OpenAI product lead Max Schwarzer said GPT-5.2 ‚Äúmakes substantial improvements to code generation and debugging‚Äù and can walk through complex math and logic step by step. Coding startups like Windsurf and CharlieCode, he added, report ‚Äústate-of-the-art agent coding performance‚Äù and measurable gains on complex multi-step workflows.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond coding, Schwarzer said that GPT-5.2 Thinking responses contain 38% fewer errors than its predecessor, making the model more dependable for day-to-day decision-making, research, and writing.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GPT-5.2 appears to be less a reinvention and more of a consolidation of OpenAI‚Äôs last two upgrades. GPT-5, which dropped in August, was a reset that laid the groundwork for a unified system with a router to toggle the model between a fast default model and a deeper ‚ÄúThinking‚Äù mode. November‚Äôs GPT-5.1 focused on making that system warmer, more conversational, and better suited to agentic and coding tasks. The latest model, GPT-5.2, seems to turn up the dial on all of those advancements, making it a more reliable foundation for production use.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For OpenAI, the stakes have never been higher. The company has made commitments to the tune of $1.4 trillion for AI infrastructure buildouts over the next few years to support its growth ‚Äî commitments it made when it still had the first-mover advantage among AI companies. But now that Google, which lagged behind at the start, is pushing ahead, that bet might be what‚Äôs driving Altman‚Äôs ‚Äúcode red.‚Äù&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI‚Äôs renewed focus on reasoning models is also a risky flex. The systems behind its Thinking and Deep Research modes are more expensive to run than standard chatbots because they chew through more compute. By doubling down on that kind of model with GPT-5.2, OpenAI may be setting up a vicious cycle: spend more on compute to win the leaderboard, then spend even more to keep those high-cost models running at scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI is already reportedly spending more on compute than it had previously let on. As TechCrunch reported recently, most of OpenAI‚Äôs inference spend ‚Äî the money it spends on compute to run a trained AI model ‚Äî is being paid in cash rather than through cloud credits, suggesting the company‚Äôs compute costs have grown beyond what partnerships and credits can subsidize.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;During the call, Simo suggested that as OpenAI scales, it is able to offer more products and services to generate more revenue to pay for additional compute. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúBut I think it‚Äôs important to place that in the grand arc of efficiency,‚Äù Simo said. ‚ÄúYou are getting, today, a lot more intelligence for the same amount of compute and the same amount of dollars as you were a year ago.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For all its focus on reasoning, one thing that‚Äôs absent from today‚Äôs launch is a new image generator. Altman reportedly said in his code red memo that image generation would be a key priority moving forward, particularly after Google‚Äôs Nano Banana (the nickname for Google‚Äôs Gemini 2.5 Flash Image model) had a viral moment following its August release.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last month, Google launched Nano Banana Pro (aka Gemini 3 Pro Image), an upgraded version with even better text rendering, world knowledge, and an eerie, real-life, unedited vibe to its photos. It also integrates better across Google‚Äôs products, as demonstrated over the past week as it pops up in tools and workflows like Google Labs Mixboard for automated presentation generation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI reportedly plans to release another new model in January with better images, improved speed, and better personality, though the company didn‚Äôt confirm these plans Thursday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI also said Thursday it‚Äôs rolling out new safety measures around mental health use and age verification for teens, but didn‚Äôt spend much of the launch pitching those changes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article has been updated with more information about OpenAI‚Äôs compute efficiency status. &lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents? We‚Äôre reporting on the inner workings of the AI industry ‚Äî from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at&amp;nbsp;rebecca.bellan@techcrunch.com&lt;/em&gt;&amp;nbsp;&lt;em&gt;or Russell Brandom at&amp;nbsp;russell.brandom@techcrunch.com. For secure communication, you can contact them via Signal at&amp;nbsp;@rebeccabellan.491&lt;/em&gt;&amp;nbsp;&lt;em&gt;and russellbrandom.49.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/GettyImages-2213399157.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI launched its latest frontier model, GPT-5.2, on Thursday amid increasing competition from Google, pitching it as its most advanced model yet and one designed for developers and everyday professional use.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI‚Äôs GPT-5.2 is coming to ChatGPT paid users and developers via the API in three flavors: Instant, a speed-optimized model for routine queries like information-seeking, writing, and translation; Thinking, which excels at complex structured work like coding, analyzing long documents, math, and planning; and Pro, the top-end model aimed at delivering maximum accuracy and reliability for difficult problems.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;‚ÄúWe designed 5.2 to unlock even more economic value for people,‚Äù Fidji Simo, OpenAI‚Äôs chief product officer, said Thursday during a briefing with journalists. ‚ÄúIt‚Äôs better at creating spreadsheets, building presentations, writing code, perceiving images, understanding long context, using tools and then linking complex, multi-step projects.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GPT-5.2 lands in the middle of an arms race with Google‚Äôs Gemini 3, which is topping LMArena‚Äôs leaderboard across most benchmarks (apart from coding ‚Äî which Anthropic‚Äôs Claude Opus-4.5 still has on lock).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Early this month, The Information reported that CEO Sam Altman released an internal ‚Äúcode red‚Äù memo to staff amid ChatGPT traffic decline and concerns that it is losing consumer market share to Google. The code red called for a shift in priorities, including stalling on commitments like introducing ads and instead focusing on creating a better ChatGPT experience.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GPT-5.2 is OpenAI‚Äôs push to reclaim leadership, even as some employees reportedly asked for the model release to be pushed back so the company could have more time to improve it. And despite indications that OpenAI would focus its attention on consumer use cases by adding more personalization and customization to ChatGPT, the launch of GPT-5.2 looks to beef up its enterprise opportunities.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is specifically targeting developers and the tooling ecosystem, aiming to become the default foundation for building AI-powered applications. Earlier this week, OpenAI released new data showing enterprise usage of its AI tools has surged dramatically over the past year.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;This comes as Gemini 3 has become tightly integrated into Google‚Äôs product and cloud ecosystem for multimodal and agentic workflows. Google this week launched managed MCP servers that make its Google and Cloud services like Maps and BigQuery easier for agents to plug into. (MCPs are the connectors between AI systems and data and tools.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI says GPT-5.2 sets new benchmark scores in coding, math, science, vision, long-context reasoning, and tool use, which the company claims could lead to ‚Äúmore reliable agentic workflows, production-grade code, and complex systems that operate across large contexts and real-world data.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Those capabilities put it in direct competition with Gemini 3‚Äôs Deep Think mode, which has been touted as a major reasoning advancement targeting math, logic, and science. On OpenAI‚Äôs own benchmark chart, GPT-5.2 Thinking edges out Gemini 3 and Anthropic‚Äôs Claude Opus 4.5 in nearly every listed reasoning test, from real-world software engineering tasks (SWE-Bench Pro) and doctoral-level science knowledge (GPQA Diamond) to abstract reasoning and pattern discovery (ARC-AGI suites).&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Research lead Aidan Clark said that stronger math scores aren‚Äôt just about solving equations. Mathematical reasoning, he explained, is a proxy for whether a model can follow multi-step logic, keep numbers consistent over time, and avoid subtle errors that could compound over time.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúThese are all properties that really matter across a wide range of different workloads,‚Äù Clark said. ‚ÄúThings like financial modeling, forecasting, doing an analysis of data.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During the briefing, OpenAI product lead Max Schwarzer said GPT-5.2 ‚Äúmakes substantial improvements to code generation and debugging‚Äù and can walk through complex math and logic step by step. Coding startups like Windsurf and CharlieCode, he added, report ‚Äústate-of-the-art agent coding performance‚Äù and measurable gains on complex multi-step workflows.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond coding, Schwarzer said that GPT-5.2 Thinking responses contain 38% fewer errors than its predecessor, making the model more dependable for day-to-day decision-making, research, and writing.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GPT-5.2 appears to be less a reinvention and more of a consolidation of OpenAI‚Äôs last two upgrades. GPT-5, which dropped in August, was a reset that laid the groundwork for a unified system with a router to toggle the model between a fast default model and a deeper ‚ÄúThinking‚Äù mode. November‚Äôs GPT-5.1 focused on making that system warmer, more conversational, and better suited to agentic and coding tasks. The latest model, GPT-5.2, seems to turn up the dial on all of those advancements, making it a more reliable foundation for production use.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For OpenAI, the stakes have never been higher. The company has made commitments to the tune of $1.4 trillion for AI infrastructure buildouts over the next few years to support its growth ‚Äî commitments it made when it still had the first-mover advantage among AI companies. But now that Google, which lagged behind at the start, is pushing ahead, that bet might be what‚Äôs driving Altman‚Äôs ‚Äúcode red.‚Äù&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI‚Äôs renewed focus on reasoning models is also a risky flex. The systems behind its Thinking and Deep Research modes are more expensive to run than standard chatbots because they chew through more compute. By doubling down on that kind of model with GPT-5.2, OpenAI may be setting up a vicious cycle: spend more on compute to win the leaderboard, then spend even more to keep those high-cost models running at scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI is already reportedly spending more on compute than it had previously let on. As TechCrunch reported recently, most of OpenAI‚Äôs inference spend ‚Äî the money it spends on compute to run a trained AI model ‚Äî is being paid in cash rather than through cloud credits, suggesting the company‚Äôs compute costs have grown beyond what partnerships and credits can subsidize.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;During the call, Simo suggested that as OpenAI scales, it is able to offer more products and services to generate more revenue to pay for additional compute. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúBut I think it‚Äôs important to place that in the grand arc of efficiency,‚Äù Simo said. ‚ÄúYou are getting, today, a lot more intelligence for the same amount of compute and the same amount of dollars as you were a year ago.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For all its focus on reasoning, one thing that‚Äôs absent from today‚Äôs launch is a new image generator. Altman reportedly said in his code red memo that image generation would be a key priority moving forward, particularly after Google‚Äôs Nano Banana (the nickname for Google‚Äôs Gemini 2.5 Flash Image model) had a viral moment following its August release.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last month, Google launched Nano Banana Pro (aka Gemini 3 Pro Image), an upgraded version with even better text rendering, world knowledge, and an eerie, real-life, unedited vibe to its photos. It also integrates better across Google‚Äôs products, as demonstrated over the past week as it pops up in tools and workflows like Google Labs Mixboard for automated presentation generation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI reportedly plans to release another new model in January with better images, improved speed, and better personality, though the company didn‚Äôt confirm these plans Thursday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI also said Thursday it‚Äôs rolling out new safety measures around mental health use and age verification for teens, but didn‚Äôt spend much of the launch pitching those changes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article has been updated with more information about OpenAI‚Äôs compute efficiency status. &lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents? We‚Äôre reporting on the inner workings of the AI industry ‚Äî from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at&amp;nbsp;rebecca.bellan@techcrunch.com&lt;/em&gt;&amp;nbsp;&lt;em&gt;or Russell Brandom at&amp;nbsp;russell.brandom@techcrunch.com. For secure communication, you can contact them via Signal at&amp;nbsp;@rebeccabellan.491&lt;/em&gt;&amp;nbsp;&lt;em&gt;and russellbrandom.49.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/11/openai-fires-back-at-google-with-gpt-5-2-after-code-red-memo/</guid><pubDate>Thu, 11 Dec 2025 18:02:44 +0000</pubDate></item><item><title>Google‚Äôs AI try-on feature for clothes now works with just a selfie (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/11/googles-ai-try-on-feature-for-clothes-now-works-with-just-a-selfie/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google is updating its AI try-on feature to let you virtually try on clothes using just a selfie, the company announced on Thursday. In the past, users had to upload a full-body picture of themselves to virtually try on a piece of clothing. Now they can use a selfie and&amp;nbsp;Nano Banana, Google‚Äôs Gemini 2.5 Flash Image model, to generate a full-body digital version of themselves for virtual try-ons.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users can select their usual clothing size, and the feature will then generate several images. From there, users can choose one to make it their default try-on photo.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;If desired, users still have the option to use a full-body photo or select from a range of models with diverse body types.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new capability is launching in the United States today.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3075274" height="680" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-11-at-12.55.46-PM.png?w=446" width="446" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google first launched the try-on feature in July, allowing users to try on apparel items from its Shopping Graph across Search, Google Shopping, and Google Images. To use the feature, users need to tap on a product listing or apparel product result and select the ‚Äútry it on‚Äù icon.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The move comes as Google has been investing in the virtual AI try-on space, as the company has a separate app dedicated specifically to that purpose. The app, called Doppl, is designed to help visualize how different outfits might look on you using AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this week, the tech giant updated it with a shoppable discovery feed that displays recommendations so users can discover and virtually try on new items. Nearly everything in the feed is shoppable, with direct links to merchants.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The discovery feed features AI-generated videos of real products and suggests outfits based on your personalized style. While some may not be fond of an AI-generated feed, Google likely views it as a way to showcase products in a format that people are already familiar with, thanks to platforms like TikTok and Instagram.&lt;br /&gt;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google is updating its AI try-on feature to let you virtually try on clothes using just a selfie, the company announced on Thursday. In the past, users had to upload a full-body picture of themselves to virtually try on a piece of clothing. Now they can use a selfie and&amp;nbsp;Nano Banana, Google‚Äôs Gemini 2.5 Flash Image model, to generate a full-body digital version of themselves for virtual try-ons.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users can select their usual clothing size, and the feature will then generate several images. From there, users can choose one to make it their default try-on photo.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;If desired, users still have the option to use a full-body photo or select from a range of models with diverse body types.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new capability is launching in the United States today.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3075274" height="680" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-11-at-12.55.46-PM.png?w=446" width="446" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google first launched the try-on feature in July, allowing users to try on apparel items from its Shopping Graph across Search, Google Shopping, and Google Images. To use the feature, users need to tap on a product listing or apparel product result and select the ‚Äútry it on‚Äù icon.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The move comes as Google has been investing in the virtual AI try-on space, as the company has a separate app dedicated specifically to that purpose. The app, called Doppl, is designed to help visualize how different outfits might look on you using AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this week, the tech giant updated it with a shoppable discovery feed that displays recommendations so users can discover and virtually try on new items. Nearly everything in the feed is shoppable, with direct links to merchants.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The discovery feed features AI-generated videos of real products and suggests outfits based on your personalized style. While some may not be fond of an AI-generated feed, Google likely views it as a way to showcase products in a format that people are already familiar with, thanks to platforms like TikTok and Instagram.&lt;br /&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/11/googles-ai-try-on-feature-for-clothes-now-works-with-just-a-selfie/</guid><pubDate>Thu, 11 Dec 2025 18:09:27 +0000</pubDate></item><item><title>OpenAI's GPT-5.2 is here: what enterprises need to know (AI | VentureBeat)</title><link>https://venturebeat.com/ai/openais-gpt-5-2-is-here-what-enterprises-need-to-know</link><description>[unable to retrieve full-text content]&lt;p&gt;The rumors were true: OpenAI on Thursday announced the release of its new frontier large language model (LLM) family, &lt;a href="https://openai.com/index/introducing-gpt-5-2/"&gt;&lt;b&gt;GPT-5.2&lt;/b&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;It comes at a pivotal moment for the AI pioneer, which has faced intensifying pressure since rival &lt;a href="https://venturebeat.com/ai/google-unveils-gemini-3-claiming-the-lead-in-math-science-multimodal-and"&gt;Google‚Äôs Gemini 3 LLM seized the top spot&lt;/a&gt; on major third-party performance leaderboards and many key benchmarks last month, though OpenAI leaders stressed in a press briefing that the timing of this release had been discussed and worked on well in advance of the release of Gemini 3.&lt;/p&gt;&lt;p&gt;OpenAI describes GPT-5.2 as its &amp;quot;most capable model series yet for professional knowledge work,&amp;quot; aiming to reclaim the performance crown with significant gains in reasoning, coding, and agentic workflows.&lt;/p&gt;&lt;p&gt;&amp;quot;It‚Äôs our most advanced frontier model and the strongest yet in the market for professional use,&amp;quot; Fidji Simo, OpenAI‚Äôs CEO of Applications, said during a press briefing today. &amp;quot;We designed 5.2 to unlock even more economic value for people. It&amp;#x27;s better at creating spreadsheets, building presentations, writing code, perceiving images, understanding long context, using tools, and handling complex, multi-step projects.&amp;quot;&lt;/p&gt;&lt;p&gt;GPT-5.2 features a massive 400,000-token context window ‚Äî allowing it to ingest hundreds of documents or large code repositories at once ‚Äî and a 128,000 max output token limit, enabling it to generate extensive reports or full applications in a single go.&lt;/p&gt;&lt;p&gt;The model also features a knowledge cutoff of August 31, 2025, ensuring it is up-to-date with relatively recent world events and technical documentation. It explicitly includes &amp;quot;Reasoning token support,&amp;quot; confirming the underlying architecture uses the chain-of-thought processing popularized by the &amp;quot;o1&amp;quot; series.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The &amp;#x27;Code Red&amp;#x27; Reality Check&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The release arrives following&lt;i&gt; &lt;/i&gt;&lt;a href="https://www.theinformation.com/articles/openai-ceo-declares-code-red-combat-threats-chatgpt-delays-ads-effort"&gt;&lt;i&gt;The Information&lt;/i&gt;&amp;#x27;s report&lt;/a&gt; of an emergency &amp;quot;Code Red&amp;quot; directive to OpenAI staff from CEO Sam Altman to improve ChaTGPT ‚Äî a move reportedly designed to mobilize resources following the &amp;quot;quality gap&amp;quot; exposed by Gemini 3.&lt;a href="https://www.theverge.com/report/838857/openai-gpt-5-2-release-date-code-red-google-response"&gt;&lt;i&gt; The Verge&lt;/i&gt;&lt;/a&gt; similarly reported on the timing of GPT-5.2&amp;#x27;s release ahead of the official announcement. &lt;/p&gt;&lt;p&gt;During the briefing, OpenAI executives acknowledged the directive but pushed back on the narrative that the model was rushed solely to answer Google.&lt;/p&gt;&lt;p&gt;&amp;quot;It is important to note this has been in the works for many, many months,&amp;quot; Simo told reporters. She clarified that while the &amp;quot;Code Red&amp;quot; helped focus the company, it wasn&amp;#x27;t the sole driver of the timeline. &lt;/p&gt;&lt;p&gt;&amp;quot;We announced this Code Red to really signal to the company that we want to marshal resources in one particular area... but that&amp;#x27;s not the reason it&amp;#x27;s coming out this week in particular.&amp;quot;&lt;/p&gt;&lt;p&gt;Max Schwarzer, lead of OpenAI&amp;#x27;s post-training team, echoed this sentiment to dispel the idea of a panic launch. &amp;quot;We&amp;#x27;ve been planning for this release since a very long time ago... this specific week we talked about many months ago.&amp;quot;&lt;/p&gt;&lt;p&gt;A spokesperson from OpenAI further clarified that the &amp;quot;Code Red&amp;quot; call applied to ChatGPT as a product, not solely underlying model development or the release of new models.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Under the Hood: Instant, Thinking, and Pro&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;OpenAI is segmenting the GPT-5.2 release into three distinct tiers within ChatGPT, a strategy likely designed to balance the massive compute costs of &amp;quot;reasoning&amp;quot; models with user demand for speed:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GPT-5.2 Instant:&lt;/b&gt; Optimized for speed and daily tasks like writing, translation, and information seeking.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GPT-5.2 Thinking:&lt;/b&gt; Designed for &amp;quot;complex, structured work&amp;quot; and long-running agents, this model leverages deeper reasoning chains to handle coding, math, and multi-step projects.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GPT-5.2 Pro:&lt;/b&gt; The new heavyweight champion. OpenAI describes this as its &amp;quot;smartest and most trustworthy option,&amp;quot; delivering the highest accuracy for difficult questions where quality outweighs latency.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;For developers, the models are available immediately in the application programming interface (API) as &lt;code&gt;gpt-5.2&lt;/code&gt;, &lt;code&gt;gpt-5.2-chat-latest&lt;/code&gt; (Instant), and &lt;code&gt;gpt-5.2-pro&lt;/code&gt;.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The Numbers: Beating the Benchmarks&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The GPT-5.2 release includes leading metrics across most domains ‚Äî specifically those that target the &amp;quot;professional knowledge work&amp;quot; gap where competitors have recently gained ground.&lt;/p&gt;&lt;p&gt;OpenAI highlighted a new benchmark called GDPval, which measures performance on &amp;quot;well-specified knowledge work tasks&amp;quot; across 44 occupations. &lt;/p&gt;&lt;p&gt;&amp;quot;GPT-5.2 Thinking is now state-of-the-art on that benchmark... and beats or ties top industry professionals on 70.9% of well-specified professional tasks like spreadsheets, presentations, and document creation, according to expert human judges,&amp;quot; Simo said.&lt;/p&gt;&lt;p&gt;In the critical arena of coding, OpenAI is claiming a decisive lead. Schwarzer noted that on SWE-bench Pro, a rigorous evaluation of real-world software engineering, GPT-5.2 Thinking sets a new state-of-the-art score of 55.6%. &lt;/p&gt;&lt;p&gt;He emphasized that this benchmark is &amp;quot;more contamination resistant, challenging, diverse, and industrially relevant than previous benchmarks like SWE-bench Verified.&amp;quot;Other key benchmark results include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GPQA Diamond (Science):&lt;/b&gt; GPT-5.2 Pro scored 93.2%, edging out GPT-5.2 Thinking (92.4%) and surpassing GPT-5.1 Thinking (88.1%).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;FrontierMath:&lt;/b&gt; On Tier 1-3 problems, GPT-5.2 Thinking solved 40.3%, a significant jump from the 31.0% achieved by its predecessor.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;ARC-AGI-1:&lt;/b&gt; GPT-5.2 Pro is reportedly the first model to cross the 90% threshold on this general reasoning benchmark, scoring &lt;b&gt;90.5%&lt;/b&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;&lt;b&gt;The Price of Intelligence&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Performance comes at a premium. While ChatGPT subscription pricing remains unchanged for now, the API costs for the new flagship models are steep compared to previous generations, reflecting the high compute demands of &amp;quot;thinking&amp;quot; mode. They&amp;#x27;re also on the upper-end of API costs for the industry.  &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GPT-5.2 Thinking:&lt;/b&gt; Priced at &lt;b&gt;$1.75&lt;/b&gt; per 1 million input tokens and &lt;b&gt;$14&lt;/b&gt; per 1 million output tokens.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GPT-5.2 Pro:&lt;/b&gt; The costs jump significantly to &lt;b&gt;$21&lt;/b&gt; per 1 million input tokens and &lt;b&gt;$168&lt;/b&gt; per 1 million output tokens.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;GPT-5.2 Thinking is priced 40% higher in the API than the standard GPT-5.1 ($1.25/$10), signaling that OpenAI views the new reasoning capabilities as a tangible value-add rather than a mere efficiency update.&lt;/p&gt;&lt;p&gt;The high-end GPT-5.2 Pro follows the same pattern, costing 40% more than the previous GPT-5 Pro ($15/$120). While expensive, it still undercuts OpenAI‚Äôs most specialized reasoning model, o1-pro, which remains the most costly offering on the menu at a staggering $150 per million input tokens and $600 per million output tokens.&lt;/p&gt;&lt;p&gt;OpenAI argues that despite the higher per-token cost, the model‚Äôs &amp;quot;greater token efficiency&amp;quot; and ability to solve tasks in fewer turns make it economically viable for high-value enterprise workflows.&lt;/p&gt;&lt;p&gt;Here&amp;#x27;s how it compares to the current API costs for other competing models across the LLM field:&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Input (/1M)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Output (/1M)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Total Cost&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Source&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen 3 Turbo&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.05&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4.1 Fast (reasoning)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4.1 Fast (non-reasoning)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;deepseek-chat (V3.2-Exp)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.28&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.42&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;DeepSeek&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;deepseek-reasoner (V3.2-Exp)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.28&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.42&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;DeepSeek&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen 3 Plus&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.60&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 5.0&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.85&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$4.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Blfmc9do4"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Haiku 4.5&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$5.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$6.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.anthropic.com/claude/docs"&gt;Anthropic&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen-Max&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.60&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$6.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$8.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 3 Pro (‚â§200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$2.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$12.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$14.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GPT-5.2&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$1.75&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$14.00&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$15.75&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://openai.com/api/pricing/"&gt;&lt;b&gt;OpenAI&lt;/b&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 3 Pro (&amp;gt;200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$4.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$18.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$22.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Sonnet 4.5&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$18.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.anthropic.com/claude/docs"&gt;Anthropic&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Opus 4.5&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$5.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$25.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$30.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.anthropic.com/claude/docs"&gt;Anthropic&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GPT-5.2 Pro&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$21.00&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$168.00&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$189.00&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://openai.com/api/pricing/"&gt;&lt;b&gt;OpenAI&lt;/b&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3&gt;&lt;b&gt;Image Generation: Nothing New Yet...But &amp;#x27;More to Come&amp;#x27;&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;During the briefing, VentureBeat asked the OpenAI participants if the new release included any boost to image generation capabilities, noting the excitement around similar features in recent competitor launches like Google&amp;#x27;s Gemini 3 Image aka Nano Banana Pro. &lt;/p&gt;&lt;p&gt;Unfortunately for those seeking to recreate the kind of text-and-information heavy graphics and image editing capabilities, OpenAI executives clarified that GPT-5.2 comes with no current image improvements over the prior GPT-5.1 and OpenAI&amp;#x27;s integrated DALL-E 3 and gpt-4o native image generation models.&lt;/p&gt;&lt;p&gt;&amp;quot;On image Gen, nothing to announce today, but more to come,&amp;quot; Simo said. She acknowledged the popularity of the feature, adding, &amp;quot;We know this is a very important use case that people love, that we introduced [to] the market, and so definitely more to come there.&amp;quot; &lt;/p&gt;&lt;p&gt;Aidan Clark, OpenAI&amp;#x27;s lead of training, also declined to comment on visual generation specifics, stating simply, &amp;quot;I can&amp;#x27;t really speak to image Gen myself.&amp;quot; &lt;/p&gt;&lt;h3&gt;&lt;b&gt;The &amp;#x27;Mega-Agent&amp;#x27; Era&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Beyond raw scores, OpenAI is positioning GPT-5.2 as the engine for a new generation of &amp;quot;long-running agents&amp;quot; capable of executing multi-step workflows without human hand-holding.&amp;quot;&lt;/p&gt;&lt;p&gt;Box found that 5.2 can extract information from long, complex documents about 40% faster, and also saw a 40% boost in reasoning accuracy for Life Sciences and healthcare,&amp;quot; Simo said. &lt;/p&gt;&lt;p&gt;She also noted that Notion reported the model &amp;quot;outperforms 5.1 across every dimension... and it excels at the kind of really ambiguous, longer rising tasks that define real knowledge work.&amp;quot;Schwarzer added that coding startups like Augment Code found the model &amp;quot;delivered substantially stronger deep code capabilities than any prior model,&amp;quot; which is why it was selected to power their new code review agent.Visual capabilities have also seen an upgrade. &lt;/p&gt;&lt;p&gt;OpenAI&amp;#x27;s release blog post shows an example where &amp;quot;a traveler reports a delayed flight, a missed connection, an overnight stay in New York, and a medical seating requirement.&amp;quot;&lt;/p&gt;&lt;p&gt;The outcome? &amp;quot;GPT‚Äë5.2 manages the entire chain of tasks‚Äîrebooking, special-assistance seating, and compensation‚Äîdelivering a more complete outcome than GPT‚Äë5.1.&amp;quot;&lt;/p&gt;&lt;p&gt;A new evaluation called ScreenSpot-Pro, which tests a model&amp;#x27;s ability to understand GUI screenshots, shows GPT-5.2 Thinking achieving 86.3% accuracy, compared to just 64.2% for GPT-5.1.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Science and Reliability&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;OpenAI leaders also stressed the model&amp;#x27;s utility for scientific research, attempting to move the conversation beyond simple chatbots to research assistants. &lt;/p&gt;&lt;p&gt;Aidan Clark, lead of the training team, shared an example of a senior immunology researcher testing the model.&lt;/p&gt;&lt;p&gt;&amp;quot;They tested it by asking it to generate the most important unanswered questions about the immune system,&amp;quot; Clark said. &amp;quot;That immunology researcher reported that GPT-5.2 produced sharper questions and stronger explanations for why those questions... matter compared to any previous pro model.&lt;/p&gt;&lt;p&gt;&amp;quot;Reliability was another key focus. Schwarzer claimed the new model &amp;quot;hallucinates substantially less than GPT-5.1,&amp;quot; noting that on a set of de-identified queries, &amp;quot;responses contained errors 38% less often.&amp;quot;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The &amp;#x27;Vibe&amp;#x27; Shift&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Interestingly, OpenAI acknowledged that not every user might immediately prefer the new models. &lt;/p&gt;&lt;p&gt;When asked why legacy models like GPT-5.1 would remain available, Schwarzer admitted that &amp;quot;models change a little bit every time.&lt;/p&gt;&lt;p&gt;&amp;quot;Some users may find that they prefer the vibes of the previous model, even though we think the latest one is across the board generally much better,&amp;quot; Schwarzer said. He also noted that for some enterprise customers who have &amp;quot;really fine-tuned a prompt for a specific model,&amp;quot; there might be &amp;quot;small regressions,&amp;quot; necessitating access to the older versions.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Safety, &amp;#x27;Adult Mode,&amp;#x27; and Future Roadmap&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Addressing safety concerns, Simo confirmed that the company is preparing to roll out an &amp;quot;Adult Mode&amp;quot; in the first quarter of next year, following the implementation of a new age prediction system.&lt;/p&gt;&lt;p&gt;&amp;quot;We&amp;#x27;re in the process of improving that,&amp;quot; Simo said regarding the age prediction technology. &lt;/p&gt;&lt;p&gt;&amp;quot;We want to do that ahead of launching adult mode.&amp;quot;Looking further ahead, industry reports suggest OpenAI is working on a more fundamental architectural shift under the codename &amp;quot;Project Garlic,&amp;quot; targeting a flagship release in early 2026. &lt;/p&gt;&lt;p&gt;While executives did not comment on specific future roadmaps during the briefing, Simo remained optimistic about the economics of their current trajectory.&lt;/p&gt;&lt;p&gt;&amp;quot;If you look at historical trends, compute has increased about 3x every year for the last three years,&amp;quot; she explained. &amp;quot;Revenue has also increased at the same pace... creating this virtuous cycle.&amp;quot;&lt;/p&gt;&lt;p&gt;Clark added that efficiency is improving rapidly: &amp;quot;The model we&amp;#x27;re releasing today achieves an even better score [on ARC-AGI] with almost 400 times less cost and less compute associated with it&amp;quot; compared to models from a year ago.&lt;/p&gt;&lt;p&gt;GPT-5.2 Instant, Thinking, and Pro begin rolling out in ChatGPT today to paid users (Plus, Pro, Team, and Enterprise). The company notes the rollout will be gradual to maintain stability.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;The rumors were true: OpenAI on Thursday announced the release of its new frontier large language model (LLM) family, &lt;a href="https://openai.com/index/introducing-gpt-5-2/"&gt;&lt;b&gt;GPT-5.2&lt;/b&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;It comes at a pivotal moment for the AI pioneer, which has faced intensifying pressure since rival &lt;a href="https://venturebeat.com/ai/google-unveils-gemini-3-claiming-the-lead-in-math-science-multimodal-and"&gt;Google‚Äôs Gemini 3 LLM seized the top spot&lt;/a&gt; on major third-party performance leaderboards and many key benchmarks last month, though OpenAI leaders stressed in a press briefing that the timing of this release had been discussed and worked on well in advance of the release of Gemini 3.&lt;/p&gt;&lt;p&gt;OpenAI describes GPT-5.2 as its &amp;quot;most capable model series yet for professional knowledge work,&amp;quot; aiming to reclaim the performance crown with significant gains in reasoning, coding, and agentic workflows.&lt;/p&gt;&lt;p&gt;&amp;quot;It‚Äôs our most advanced frontier model and the strongest yet in the market for professional use,&amp;quot; Fidji Simo, OpenAI‚Äôs CEO of Applications, said during a press briefing today. &amp;quot;We designed 5.2 to unlock even more economic value for people. It&amp;#x27;s better at creating spreadsheets, building presentations, writing code, perceiving images, understanding long context, using tools, and handling complex, multi-step projects.&amp;quot;&lt;/p&gt;&lt;p&gt;GPT-5.2 features a massive 400,000-token context window ‚Äî allowing it to ingest hundreds of documents or large code repositories at once ‚Äî and a 128,000 max output token limit, enabling it to generate extensive reports or full applications in a single go.&lt;/p&gt;&lt;p&gt;The model also features a knowledge cutoff of August 31, 2025, ensuring it is up-to-date with relatively recent world events and technical documentation. It explicitly includes &amp;quot;Reasoning token support,&amp;quot; confirming the underlying architecture uses the chain-of-thought processing popularized by the &amp;quot;o1&amp;quot; series.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The &amp;#x27;Code Red&amp;#x27; Reality Check&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The release arrives following&lt;i&gt; &lt;/i&gt;&lt;a href="https://www.theinformation.com/articles/openai-ceo-declares-code-red-combat-threats-chatgpt-delays-ads-effort"&gt;&lt;i&gt;The Information&lt;/i&gt;&amp;#x27;s report&lt;/a&gt; of an emergency &amp;quot;Code Red&amp;quot; directive to OpenAI staff from CEO Sam Altman to improve ChaTGPT ‚Äî a move reportedly designed to mobilize resources following the &amp;quot;quality gap&amp;quot; exposed by Gemini 3.&lt;a href="https://www.theverge.com/report/838857/openai-gpt-5-2-release-date-code-red-google-response"&gt;&lt;i&gt; The Verge&lt;/i&gt;&lt;/a&gt; similarly reported on the timing of GPT-5.2&amp;#x27;s release ahead of the official announcement. &lt;/p&gt;&lt;p&gt;During the briefing, OpenAI executives acknowledged the directive but pushed back on the narrative that the model was rushed solely to answer Google.&lt;/p&gt;&lt;p&gt;&amp;quot;It is important to note this has been in the works for many, many months,&amp;quot; Simo told reporters. She clarified that while the &amp;quot;Code Red&amp;quot; helped focus the company, it wasn&amp;#x27;t the sole driver of the timeline. &lt;/p&gt;&lt;p&gt;&amp;quot;We announced this Code Red to really signal to the company that we want to marshal resources in one particular area... but that&amp;#x27;s not the reason it&amp;#x27;s coming out this week in particular.&amp;quot;&lt;/p&gt;&lt;p&gt;Max Schwarzer, lead of OpenAI&amp;#x27;s post-training team, echoed this sentiment to dispel the idea of a panic launch. &amp;quot;We&amp;#x27;ve been planning for this release since a very long time ago... this specific week we talked about many months ago.&amp;quot;&lt;/p&gt;&lt;p&gt;A spokesperson from OpenAI further clarified that the &amp;quot;Code Red&amp;quot; call applied to ChatGPT as a product, not solely underlying model development or the release of new models.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Under the Hood: Instant, Thinking, and Pro&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;OpenAI is segmenting the GPT-5.2 release into three distinct tiers within ChatGPT, a strategy likely designed to balance the massive compute costs of &amp;quot;reasoning&amp;quot; models with user demand for speed:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GPT-5.2 Instant:&lt;/b&gt; Optimized for speed and daily tasks like writing, translation, and information seeking.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GPT-5.2 Thinking:&lt;/b&gt; Designed for &amp;quot;complex, structured work&amp;quot; and long-running agents, this model leverages deeper reasoning chains to handle coding, math, and multi-step projects.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GPT-5.2 Pro:&lt;/b&gt; The new heavyweight champion. OpenAI describes this as its &amp;quot;smartest and most trustworthy option,&amp;quot; delivering the highest accuracy for difficult questions where quality outweighs latency.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;For developers, the models are available immediately in the application programming interface (API) as &lt;code&gt;gpt-5.2&lt;/code&gt;, &lt;code&gt;gpt-5.2-chat-latest&lt;/code&gt; (Instant), and &lt;code&gt;gpt-5.2-pro&lt;/code&gt;.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The Numbers: Beating the Benchmarks&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The GPT-5.2 release includes leading metrics across most domains ‚Äî specifically those that target the &amp;quot;professional knowledge work&amp;quot; gap where competitors have recently gained ground.&lt;/p&gt;&lt;p&gt;OpenAI highlighted a new benchmark called GDPval, which measures performance on &amp;quot;well-specified knowledge work tasks&amp;quot; across 44 occupations. &lt;/p&gt;&lt;p&gt;&amp;quot;GPT-5.2 Thinking is now state-of-the-art on that benchmark... and beats or ties top industry professionals on 70.9% of well-specified professional tasks like spreadsheets, presentations, and document creation, according to expert human judges,&amp;quot; Simo said.&lt;/p&gt;&lt;p&gt;In the critical arena of coding, OpenAI is claiming a decisive lead. Schwarzer noted that on SWE-bench Pro, a rigorous evaluation of real-world software engineering, GPT-5.2 Thinking sets a new state-of-the-art score of 55.6%. &lt;/p&gt;&lt;p&gt;He emphasized that this benchmark is &amp;quot;more contamination resistant, challenging, diverse, and industrially relevant than previous benchmarks like SWE-bench Verified.&amp;quot;Other key benchmark results include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GPQA Diamond (Science):&lt;/b&gt; GPT-5.2 Pro scored 93.2%, edging out GPT-5.2 Thinking (92.4%) and surpassing GPT-5.1 Thinking (88.1%).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;FrontierMath:&lt;/b&gt; On Tier 1-3 problems, GPT-5.2 Thinking solved 40.3%, a significant jump from the 31.0% achieved by its predecessor.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;ARC-AGI-1:&lt;/b&gt; GPT-5.2 Pro is reportedly the first model to cross the 90% threshold on this general reasoning benchmark, scoring &lt;b&gt;90.5%&lt;/b&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;&lt;b&gt;The Price of Intelligence&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Performance comes at a premium. While ChatGPT subscription pricing remains unchanged for now, the API costs for the new flagship models are steep compared to previous generations, reflecting the high compute demands of &amp;quot;thinking&amp;quot; mode. They&amp;#x27;re also on the upper-end of API costs for the industry.  &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GPT-5.2 Thinking:&lt;/b&gt; Priced at &lt;b&gt;$1.75&lt;/b&gt; per 1 million input tokens and &lt;b&gt;$14&lt;/b&gt; per 1 million output tokens.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GPT-5.2 Pro:&lt;/b&gt; The costs jump significantly to &lt;b&gt;$21&lt;/b&gt; per 1 million input tokens and &lt;b&gt;$168&lt;/b&gt; per 1 million output tokens.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;GPT-5.2 Thinking is priced 40% higher in the API than the standard GPT-5.1 ($1.25/$10), signaling that OpenAI views the new reasoning capabilities as a tangible value-add rather than a mere efficiency update.&lt;/p&gt;&lt;p&gt;The high-end GPT-5.2 Pro follows the same pattern, costing 40% more than the previous GPT-5 Pro ($15/$120). While expensive, it still undercuts OpenAI‚Äôs most specialized reasoning model, o1-pro, which remains the most costly offering on the menu at a staggering $150 per million input tokens and $600 per million output tokens.&lt;/p&gt;&lt;p&gt;OpenAI argues that despite the higher per-token cost, the model‚Äôs &amp;quot;greater token efficiency&amp;quot; and ability to solve tasks in fewer turns make it economically viable for high-value enterprise workflows.&lt;/p&gt;&lt;p&gt;Here&amp;#x27;s how it compares to the current API costs for other competing models across the LLM field:&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Input (/1M)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Output (/1M)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Total Cost&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Source&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen 3 Turbo&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.05&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4.1 Fast (reasoning)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4.1 Fast (non-reasoning)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;deepseek-chat (V3.2-Exp)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.28&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.42&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;DeepSeek&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;deepseek-reasoner (V3.2-Exp)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.28&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.42&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;DeepSeek&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen 3 Plus&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.60&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 5.0&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.85&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$4.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Blfmc9do4"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Haiku 4.5&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$5.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$6.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.anthropic.com/claude/docs"&gt;Anthropic&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen-Max&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.60&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$6.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$8.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 3 Pro (‚â§200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$2.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$12.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$14.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GPT-5.2&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$1.75&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$14.00&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$15.75&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://openai.com/api/pricing/"&gt;&lt;b&gt;OpenAI&lt;/b&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 3 Pro (&amp;gt;200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$4.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$18.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$22.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Sonnet 4.5&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$18.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.anthropic.com/claude/docs"&gt;Anthropic&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Opus 4.5&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$5.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$25.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$30.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.anthropic.com/claude/docs"&gt;Anthropic&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GPT-5.2 Pro&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$21.00&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$168.00&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$189.00&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://openai.com/api/pricing/"&gt;&lt;b&gt;OpenAI&lt;/b&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3&gt;&lt;b&gt;Image Generation: Nothing New Yet...But &amp;#x27;More to Come&amp;#x27;&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;During the briefing, VentureBeat asked the OpenAI participants if the new release included any boost to image generation capabilities, noting the excitement around similar features in recent competitor launches like Google&amp;#x27;s Gemini 3 Image aka Nano Banana Pro. &lt;/p&gt;&lt;p&gt;Unfortunately for those seeking to recreate the kind of text-and-information heavy graphics and image editing capabilities, OpenAI executives clarified that GPT-5.2 comes with no current image improvements over the prior GPT-5.1 and OpenAI&amp;#x27;s integrated DALL-E 3 and gpt-4o native image generation models.&lt;/p&gt;&lt;p&gt;&amp;quot;On image Gen, nothing to announce today, but more to come,&amp;quot; Simo said. She acknowledged the popularity of the feature, adding, &amp;quot;We know this is a very important use case that people love, that we introduced [to] the market, and so definitely more to come there.&amp;quot; &lt;/p&gt;&lt;p&gt;Aidan Clark, OpenAI&amp;#x27;s lead of training, also declined to comment on visual generation specifics, stating simply, &amp;quot;I can&amp;#x27;t really speak to image Gen myself.&amp;quot; &lt;/p&gt;&lt;h3&gt;&lt;b&gt;The &amp;#x27;Mega-Agent&amp;#x27; Era&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Beyond raw scores, OpenAI is positioning GPT-5.2 as the engine for a new generation of &amp;quot;long-running agents&amp;quot; capable of executing multi-step workflows without human hand-holding.&amp;quot;&lt;/p&gt;&lt;p&gt;Box found that 5.2 can extract information from long, complex documents about 40% faster, and also saw a 40% boost in reasoning accuracy for Life Sciences and healthcare,&amp;quot; Simo said. &lt;/p&gt;&lt;p&gt;She also noted that Notion reported the model &amp;quot;outperforms 5.1 across every dimension... and it excels at the kind of really ambiguous, longer rising tasks that define real knowledge work.&amp;quot;Schwarzer added that coding startups like Augment Code found the model &amp;quot;delivered substantially stronger deep code capabilities than any prior model,&amp;quot; which is why it was selected to power their new code review agent.Visual capabilities have also seen an upgrade. &lt;/p&gt;&lt;p&gt;OpenAI&amp;#x27;s release blog post shows an example where &amp;quot;a traveler reports a delayed flight, a missed connection, an overnight stay in New York, and a medical seating requirement.&amp;quot;&lt;/p&gt;&lt;p&gt;The outcome? &amp;quot;GPT‚Äë5.2 manages the entire chain of tasks‚Äîrebooking, special-assistance seating, and compensation‚Äîdelivering a more complete outcome than GPT‚Äë5.1.&amp;quot;&lt;/p&gt;&lt;p&gt;A new evaluation called ScreenSpot-Pro, which tests a model&amp;#x27;s ability to understand GUI screenshots, shows GPT-5.2 Thinking achieving 86.3% accuracy, compared to just 64.2% for GPT-5.1.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Science and Reliability&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;OpenAI leaders also stressed the model&amp;#x27;s utility for scientific research, attempting to move the conversation beyond simple chatbots to research assistants. &lt;/p&gt;&lt;p&gt;Aidan Clark, lead of the training team, shared an example of a senior immunology researcher testing the model.&lt;/p&gt;&lt;p&gt;&amp;quot;They tested it by asking it to generate the most important unanswered questions about the immune system,&amp;quot; Clark said. &amp;quot;That immunology researcher reported that GPT-5.2 produced sharper questions and stronger explanations for why those questions... matter compared to any previous pro model.&lt;/p&gt;&lt;p&gt;&amp;quot;Reliability was another key focus. Schwarzer claimed the new model &amp;quot;hallucinates substantially less than GPT-5.1,&amp;quot; noting that on a set of de-identified queries, &amp;quot;responses contained errors 38% less often.&amp;quot;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The &amp;#x27;Vibe&amp;#x27; Shift&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Interestingly, OpenAI acknowledged that not every user might immediately prefer the new models. &lt;/p&gt;&lt;p&gt;When asked why legacy models like GPT-5.1 would remain available, Schwarzer admitted that &amp;quot;models change a little bit every time.&lt;/p&gt;&lt;p&gt;&amp;quot;Some users may find that they prefer the vibes of the previous model, even though we think the latest one is across the board generally much better,&amp;quot; Schwarzer said. He also noted that for some enterprise customers who have &amp;quot;really fine-tuned a prompt for a specific model,&amp;quot; there might be &amp;quot;small regressions,&amp;quot; necessitating access to the older versions.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Safety, &amp;#x27;Adult Mode,&amp;#x27; and Future Roadmap&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Addressing safety concerns, Simo confirmed that the company is preparing to roll out an &amp;quot;Adult Mode&amp;quot; in the first quarter of next year, following the implementation of a new age prediction system.&lt;/p&gt;&lt;p&gt;&amp;quot;We&amp;#x27;re in the process of improving that,&amp;quot; Simo said regarding the age prediction technology. &lt;/p&gt;&lt;p&gt;&amp;quot;We want to do that ahead of launching adult mode.&amp;quot;Looking further ahead, industry reports suggest OpenAI is working on a more fundamental architectural shift under the codename &amp;quot;Project Garlic,&amp;quot; targeting a flagship release in early 2026. &lt;/p&gt;&lt;p&gt;While executives did not comment on specific future roadmaps during the briefing, Simo remained optimistic about the economics of their current trajectory.&lt;/p&gt;&lt;p&gt;&amp;quot;If you look at historical trends, compute has increased about 3x every year for the last three years,&amp;quot; she explained. &amp;quot;Revenue has also increased at the same pace... creating this virtuous cycle.&amp;quot;&lt;/p&gt;&lt;p&gt;Clark added that efficiency is improving rapidly: &amp;quot;The model we&amp;#x27;re releasing today achieves an even better score [on ARC-AGI] with almost 400 times less cost and less compute associated with it&amp;quot; compared to models from a year ago.&lt;/p&gt;&lt;p&gt;GPT-5.2 Instant, Thinking, and Pro begin rolling out in ChatGPT today to paid users (Plus, Pro, Team, and Enterprise). The company notes the rollout will be gradual to maintain stability.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/openais-gpt-5-2-is-here-what-enterprises-need-to-know</guid><pubDate>Thu, 11 Dec 2025 18:16:00 +0000</pubDate></item><item><title>[NEW] Disney hits Google with cease-and-desist claiming ‚Äòmassive‚Äô copyright infringement (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/11/disney-hits-google-with-cease-and-desist-claiming-massive-copyright-infringement/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/10/GettyImages-1337403704.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Disney sent a cease-and-desist letter to Google on Wednesday, alleging that the tech giant has infringed on its copyrights, Variety reports.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Disney is accusing the tech giant of copyright infringement on a ‚Äúmassive scale,‚Äù claiming it has used AI models and services to commercially distribute unauthorized images and videos, according to the letter seen by Variety.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;‚ÄúGoogle operates as a virtual vending machine, capable of reproducing, rendering, and distributing copies of Disney‚Äôs valuable library of copyrighted characters and other works on a mass scale,‚Äù the letter reads. ‚ÄúAnd compounding Google‚Äôs blatant infringement, many of the infringing images generated by Google‚Äôs AI Services are branded with Google‚Äôs Gemini logo, falsely implying that Google‚Äôs exploitation of Disney‚Äôs intellectual property is authorized and endorsed by Disney.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The letter alleges that Google‚Äôs AI systems infringe characters from ‚ÄúFrozen,‚Äù ‚ÄúThe Lion King,‚Äù ‚ÄúMoana,‚Äù ‚ÄúThe Little Mermaid,‚Äù ‚ÄúDeadpool,‚Äù and more. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google didn‚Äôt confirm or deny Disney‚Äôs allegations but did say it will ‚Äúengage‚Äù with the company. ‚ÄúWe have a longstanding and mutually beneficial relationship with Disney, and will continue to engage with them. More generally, we use public data from the open web to build our AI and have built additional innovative copyright controls like Google-extended and Content ID for YouTube, which give sites and copyright holders control over their content,‚Äù a spokesperson said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Disney‚Äôs move comes the same day that it signed a $1 billion, three-year deal with OpenAI that will bring its iconic characters to the company‚Äôs Sora AI video generator.&lt;/p&gt;


&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/10/GettyImages-1337403704.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Disney sent a cease-and-desist letter to Google on Wednesday, alleging that the tech giant has infringed on its copyrights, Variety reports.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Disney is accusing the tech giant of copyright infringement on a ‚Äúmassive scale,‚Äù claiming it has used AI models and services to commercially distribute unauthorized images and videos, according to the letter seen by Variety.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;‚ÄúGoogle operates as a virtual vending machine, capable of reproducing, rendering, and distributing copies of Disney‚Äôs valuable library of copyrighted characters and other works on a mass scale,‚Äù the letter reads. ‚ÄúAnd compounding Google‚Äôs blatant infringement, many of the infringing images generated by Google‚Äôs AI Services are branded with Google‚Äôs Gemini logo, falsely implying that Google‚Äôs exploitation of Disney‚Äôs intellectual property is authorized and endorsed by Disney.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The letter alleges that Google‚Äôs AI systems infringe characters from ‚ÄúFrozen,‚Äù ‚ÄúThe Lion King,‚Äù ‚ÄúMoana,‚Äù ‚ÄúThe Little Mermaid,‚Äù ‚ÄúDeadpool,‚Äù and more. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google didn‚Äôt confirm or deny Disney‚Äôs allegations but did say it will ‚Äúengage‚Äù with the company. ‚ÄúWe have a longstanding and mutually beneficial relationship with Disney, and will continue to engage with them. More generally, we use public data from the open web to build our AI and have built additional innovative copyright controls like Google-extended and Content ID for YouTube, which give sites and copyright holders control over their content,‚Äù a spokesperson said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Disney‚Äôs move comes the same day that it signed a $1 billion, three-year deal with OpenAI that will bring its iconic characters to the company‚Äôs Sora AI video generator.&lt;/p&gt;


&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/11/disney-hits-google-with-cease-and-desist-claiming-massive-copyright-infringement/</guid><pubDate>Thu, 11 Dec 2025 18:53:09 +0000</pubDate></item><item><title>[NEW] As AI Grows More Complex, Model Builders Rely on NVIDIA (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/leading-models-nvidia/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/end-to-end-press-best-models-trained-1920x1080-4660123.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Unveiling what it describes as the most capable model series yet for professional knowledge work, OpenAI launched GPT-5.2 today. The model was trained and deployed on NVIDIA infrastructure, including NVIDIA Hopper and GB200 NVL72 systems.&lt;/p&gt;
&lt;p&gt;It‚Äôs the latest example of how leading AI builders train and deploy at scale on NVIDIA‚Äôs full-stack AI infrastructure.&lt;/p&gt;
&lt;h2&gt;Pretraining: The Bedrock of Intelligence&lt;/h2&gt;
&lt;p&gt;AI models are getting more capable thanks to three scaling laws: pretraining, post-training and test-time scaling.&lt;/p&gt;
&lt;p&gt;Reasoning models, which apply compute during inference to tackle complex queries, using multiple networks working together, are now everywhere.&lt;/p&gt;
&lt;p&gt;But pretraining and post-training remain the bedrock of intelligence. They‚Äôre core to making reasoning models smarter and more useful.&lt;/p&gt;
&lt;p&gt;And getting there takes scale. Training frontier models from scratch isn‚Äôt a small job.&lt;/p&gt;
&lt;p&gt;It takes tens of thousands, even hundreds of thousands, of GPUs working together effectively.&lt;/p&gt;
&lt;p&gt;That level of scale demands excellence across many dimensions. It requires world-class accelerators, advanced networking across scale-up, scale-out and increasingly scale-across architectures, plus a fully optimized software stack. In short, a purpose-built infrastructure platform built to deliver performance at scale.&lt;/p&gt;
&lt;p&gt;Compared with the NVIDIA Hopper architecture, NVIDIA GB200 NVL72 systems delivered 3x faster training performance on the largest model tested in the latest MLPerf Training industry benchmarks, and nearly 2x better performance per dollar.&lt;/p&gt;
&lt;p&gt;And NVIDIA GB300 NVL72 delivers a more than 4x speedup compared with NVIDIA Hopper.&lt;/p&gt;
&lt;p&gt;These performance gains help AI developers shorten development cycles and deploy new models more quickly.&lt;/p&gt;
&lt;h2&gt;Proof in the Models Across Every Modality&lt;/h2&gt;
&lt;p&gt;The majority of today‚Äôs leading large language models were trained on NVIDIA platforms.&lt;/p&gt;
&lt;p&gt;AI isn‚Äôt just about text.&lt;/p&gt;
&lt;p&gt;NVIDIA supports AI development across multiple modalities, including speech, image and video generation, as well as emerging areas like biology and robotics.&lt;/p&gt;
&lt;p&gt;For example, models like Evo 2 decode genetic sequences, OpenFold3 predicts 3D protein structures and Boltz-2 simulates drug interactions, helping researchers identify promising candidates faster.&lt;/p&gt;
&lt;p&gt;On the clinical side, NVIDIA Clara synthesis models generate realistic medical images to advance screening and diagnosis without exposing patient data.&lt;/p&gt;
&lt;p&gt;Companies like Runway and Inworld train on NVIDIA infrastructure.&lt;/p&gt;
&lt;p&gt;Runway last week announced Gen-4.5, a new frontier video generation model that‚Äôs the current top-rated video model in the world, according to the Artificial Analysis leaderboard.&lt;/p&gt;
&lt;p&gt;Now optimized for NVIDIA Blackwell, Gen-4.5 was developed entirely on NVIDIA GPUs across initial research and development, pre-training, post-training and inference.&lt;/p&gt;
&lt;p&gt;Runway also announced GWM-1, a state-of-the-art general world model trained on NVIDIA Blackwell that‚Äôs built to simulate reality in real time. It‚Äôs interactive, controllable and general-purpose, with applications in video games, education, science, entertainment and robotics.&lt;/p&gt;
&lt;p&gt;Benchmarks show why.&lt;/p&gt;
&lt;p&gt;MLPerf is the industry-standard benchmark for training performance. In the latest round, NVIDIA submitted results across all seven MLPerf Training 5.1 benchmarks, showing strong performance and versatility. It was the only platform to submit in every category.&lt;/p&gt;
&lt;p&gt;NVIDIA‚Äôs ability to support diverse AI workloads helps data centers use resources more efficiently.&lt;/p&gt;
&lt;p&gt;That‚Äôs why AI labs such as Black Forest Labs, Cohere, Mistral, OpenAI, Reflection and Thinking Machines Lab and are all training on the NVIDIA Blackwell platform.&lt;/p&gt;
&lt;h2&gt;NVIDIA Blackwell Across Clouds and Data Centers&lt;/h2&gt;
&lt;p&gt;NVIDIA Blackwell is widely available from leading cloud service providers, neo-clouds and server makers.&lt;/p&gt;
&lt;p&gt;And NVIDIA Blackwell Ultra, offering additional compute, memory and architecture improvements, is now rolling out from server makers and cloud service providers.&lt;/p&gt;
&lt;p&gt;Major cloud service providers and NVIDIA Cloud Partners, including Amazon Web Services, CoreWeave, Google Cloud, Lambda, Microsoft Azure, Nebius, Oracle Cloud Infrastructure and Together AI, to name a few, already offer instances powered by NVIDIA Blackwell, ensuring scalable performance as pretraining scaling continues.&lt;/p&gt;
&lt;p&gt;From frontier models to everyday AI, the future is being built on NVIDIA.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about the &lt;/i&gt;&lt;i&gt;NVIDIA Blackwell platform&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/end-to-end-press-best-models-trained-1920x1080-4660123.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Unveiling what it describes as the most capable model series yet for professional knowledge work, OpenAI launched GPT-5.2 today. The model was trained and deployed on NVIDIA infrastructure, including NVIDIA Hopper and GB200 NVL72 systems.&lt;/p&gt;
&lt;p&gt;It‚Äôs the latest example of how leading AI builders train and deploy at scale on NVIDIA‚Äôs full-stack AI infrastructure.&lt;/p&gt;
&lt;h2&gt;Pretraining: The Bedrock of Intelligence&lt;/h2&gt;
&lt;p&gt;AI models are getting more capable thanks to three scaling laws: pretraining, post-training and test-time scaling.&lt;/p&gt;
&lt;p&gt;Reasoning models, which apply compute during inference to tackle complex queries, using multiple networks working together, are now everywhere.&lt;/p&gt;
&lt;p&gt;But pretraining and post-training remain the bedrock of intelligence. They‚Äôre core to making reasoning models smarter and more useful.&lt;/p&gt;
&lt;p&gt;And getting there takes scale. Training frontier models from scratch isn‚Äôt a small job.&lt;/p&gt;
&lt;p&gt;It takes tens of thousands, even hundreds of thousands, of GPUs working together effectively.&lt;/p&gt;
&lt;p&gt;That level of scale demands excellence across many dimensions. It requires world-class accelerators, advanced networking across scale-up, scale-out and increasingly scale-across architectures, plus a fully optimized software stack. In short, a purpose-built infrastructure platform built to deliver performance at scale.&lt;/p&gt;
&lt;p&gt;Compared with the NVIDIA Hopper architecture, NVIDIA GB200 NVL72 systems delivered 3x faster training performance on the largest model tested in the latest MLPerf Training industry benchmarks, and nearly 2x better performance per dollar.&lt;/p&gt;
&lt;p&gt;And NVIDIA GB300 NVL72 delivers a more than 4x speedup compared with NVIDIA Hopper.&lt;/p&gt;
&lt;p&gt;These performance gains help AI developers shorten development cycles and deploy new models more quickly.&lt;/p&gt;
&lt;h2&gt;Proof in the Models Across Every Modality&lt;/h2&gt;
&lt;p&gt;The majority of today‚Äôs leading large language models were trained on NVIDIA platforms.&lt;/p&gt;
&lt;p&gt;AI isn‚Äôt just about text.&lt;/p&gt;
&lt;p&gt;NVIDIA supports AI development across multiple modalities, including speech, image and video generation, as well as emerging areas like biology and robotics.&lt;/p&gt;
&lt;p&gt;For example, models like Evo 2 decode genetic sequences, OpenFold3 predicts 3D protein structures and Boltz-2 simulates drug interactions, helping researchers identify promising candidates faster.&lt;/p&gt;
&lt;p&gt;On the clinical side, NVIDIA Clara synthesis models generate realistic medical images to advance screening and diagnosis without exposing patient data.&lt;/p&gt;
&lt;p&gt;Companies like Runway and Inworld train on NVIDIA infrastructure.&lt;/p&gt;
&lt;p&gt;Runway last week announced Gen-4.5, a new frontier video generation model that‚Äôs the current top-rated video model in the world, according to the Artificial Analysis leaderboard.&lt;/p&gt;
&lt;p&gt;Now optimized for NVIDIA Blackwell, Gen-4.5 was developed entirely on NVIDIA GPUs across initial research and development, pre-training, post-training and inference.&lt;/p&gt;
&lt;p&gt;Runway also announced GWM-1, a state-of-the-art general world model trained on NVIDIA Blackwell that‚Äôs built to simulate reality in real time. It‚Äôs interactive, controllable and general-purpose, with applications in video games, education, science, entertainment and robotics.&lt;/p&gt;
&lt;p&gt;Benchmarks show why.&lt;/p&gt;
&lt;p&gt;MLPerf is the industry-standard benchmark for training performance. In the latest round, NVIDIA submitted results across all seven MLPerf Training 5.1 benchmarks, showing strong performance and versatility. It was the only platform to submit in every category.&lt;/p&gt;
&lt;p&gt;NVIDIA‚Äôs ability to support diverse AI workloads helps data centers use resources more efficiently.&lt;/p&gt;
&lt;p&gt;That‚Äôs why AI labs such as Black Forest Labs, Cohere, Mistral, OpenAI, Reflection and Thinking Machines Lab and are all training on the NVIDIA Blackwell platform.&lt;/p&gt;
&lt;h2&gt;NVIDIA Blackwell Across Clouds and Data Centers&lt;/h2&gt;
&lt;p&gt;NVIDIA Blackwell is widely available from leading cloud service providers, neo-clouds and server makers.&lt;/p&gt;
&lt;p&gt;And NVIDIA Blackwell Ultra, offering additional compute, memory and architecture improvements, is now rolling out from server makers and cloud service providers.&lt;/p&gt;
&lt;p&gt;Major cloud service providers and NVIDIA Cloud Partners, including Amazon Web Services, CoreWeave, Google Cloud, Lambda, Microsoft Azure, Nebius, Oracle Cloud Infrastructure and Together AI, to name a few, already offer instances powered by NVIDIA Blackwell, ensuring scalable performance as pretraining scaling continues.&lt;/p&gt;
&lt;p&gt;From frontier models to everyday AI, the future is being built on NVIDIA.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about the &lt;/i&gt;&lt;i&gt;NVIDIA Blackwell platform&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/leading-models-nvidia/</guid><pubDate>Thu, 11 Dec 2025 19:19:57 +0000</pubDate></item><item><title>[NEW] Disney says Google AI infringes copyright ‚Äúon a massive scale‚Äù (AI ‚Äì Ars Technica)</title><link>https://arstechnica.com/google/2025/12/disney-says-google-ai-infringes-copyright-on-a-massive-scale/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Disney demands that Google immediately block its copyrighted content from appearing in AI outputs.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2018/01/I-Cant-Believe-Its-Not-Mickey-300x169.jpg" width="300" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="563" src="https://cdn.arstechnica.net/wp-content/uploads/2018/01/I-Cant-Believe-Its-Not-Mickey.jpg" width="1000" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Aurich Lawson

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The Wild West of copyrighted characters in AI may be coming to an end. There has been legal wrangling over the role of copyright in the AI era, but the mother of all legal teams may now be gearing up for a fight. Disney has sent a cease and desist to Google, alleging the company‚Äôs AI tools are infringing Disney‚Äôs copyrights ‚Äúon a massive scale.‚Äù&lt;/p&gt;
&lt;p&gt;According to the letter, Google is violating the entertainment conglomerate‚Äôs intellectual property in multiple ways. The legal notice says Google has copied a ‚Äúlarge corpus‚Äù of Disney‚Äôs works to train its gen AI models, which is believable, as Google‚Äôs image and video models will happily produce popular Disney characters‚Äîthey couldn‚Äôt do that without feeding the models lots of Disney data.&lt;/p&gt;
&lt;p&gt;The C&amp;amp;D also takes issue with Google for distributing ‚Äúcopies of its protected works‚Äù to consumers. So all those memes you‚Äôve been making with Disney characters? Yeah, Disney doesn‚Äôt like that, either. The letter calls out a huge number of Disney-owned properties that can be prompted into existence in Google AI, including The Lion King, Deadpool, and Star Wars.&lt;/p&gt;
&lt;p&gt;The company calls on Google to immediately stop using Disney content in its AI tools and create measures to ensure that future AI outputs don‚Äôt produce any characters that Disney owns. Disney is famously litigious and has an army of lawyers dedicated to defending its copyrights. The nature of copyright law in the US is a direct result of Disney‚Äôs legal maneuvering, which has extended its control of iconic characters by decades.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;While Disney wants its characters out of Google AI generally, the letter specifically cited the AI tools in YouTube. Google has started adding its Veo AI video model to YouTube, allowing creators to more easily create and publish videos. That seems to be a greater concern for Disney than image models like Nano Banana.&lt;/p&gt;
&lt;p&gt;Google has said little about Disney‚Äôs warning‚Äîa warning Google must have known was coming. A Google spokesperson has issued the following brief statement on the mater.&lt;/p&gt;
&lt;p&gt;‚ÄúWe have a longstanding and mutually beneficial relationship with Disney, and will continue to engage with them,‚Äù Google says. ‚ÄúMore generally, we use public data from the open web to build our AI and have built additional innovative copyright controls like Google-extended and Content ID for YouTube, which give sites and copyright holders control over their content.‚Äù&lt;/p&gt;
&lt;p&gt;Perhaps this is previewing Google‚Äôs argument in a theoretical lawsuit. That copyrighted Disney content was all over the open internet, so is it really Google‚Äôs fault it ended up baked into the AI?&lt;/p&gt;
&lt;h2&gt;Content silos for AI&lt;/h2&gt;
&lt;p&gt;The generative AI boom has treated copyright as a mere suggestion as companies race to gobble up training data and remix it as ‚Äúnew‚Äù content. A cavalcade of companies, including The New York Times and Getty Images, have sued over how their material has been used and replicated by AI. Disney itself threatened a lawsuit against Character.AI earlier this year, leading to the removal of Disney content from the service.&lt;/p&gt;
&lt;p&gt;Google isn‚Äôt Character.AI, though. It‚Äôs probably no coincidence that Disney is challenging Google at the same time it is entering into a content deal with OpenAI. Disney has invested $1 billion in the AI firm and agreed to a three-year licensing deal that officially brings Disney characters to OpenAI‚Äôs Sora video app. The specifics of that arrangement are still subject to negotiations.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The launch of the Sora app earlier this year was widely derided by the entertainment industry, but that‚Äôs nothing a little money can‚Äôt solve. OpenAI required copyright owners to opt out of having their content included in the service, but it later reversed course to an opt-in model. The Disney deal is OpenAI‚Äôs first major content tie-in for AI.&lt;/p&gt;
&lt;p&gt;Meanwhile, Google‚Äôs AI tools don‚Äôt pay any mind to copyright. If you want to create images and videos with The Avengers, Super Mario, or any other character, Google doesn‚Äôt stand in your way. Whether or not that remains the case depends on how Google responds to Disney‚Äôs lawyers. There‚Äôs no indication that Disney‚Äôs licensing deal with OpenAI is exclusive, so it‚Äôs possible Google and Disney will reach an agreement to allow AI recreations. Google could also choose to fight back against this particular interpretation of copyright.&lt;/p&gt;
&lt;p&gt;Most companies would channel Character.AI and avoid a fight with Disney‚Äôs lawyers, but Google‚Äôs scale gives it more options. In either case, we could soon see the AI content ecosystem become a patchwork of content silos not unlike streaming media. If you want to generate an image featuring Moana, well, you‚Äôll need to go to OpenAI. If a DC character is more your speed, there may be a different AI firm that has a deal to let you do that. It‚Äôs hard to know who to root for in a battle between giant AI firms and equally giant entertainment companies.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Updated 12/11 with statement from Google.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Disney demands that Google immediately block its copyrighted content from appearing in AI outputs.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2018/01/I-Cant-Believe-Its-Not-Mickey-300x169.jpg" width="300" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="563" src="https://cdn.arstechnica.net/wp-content/uploads/2018/01/I-Cant-Believe-Its-Not-Mickey.jpg" width="1000" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Aurich Lawson

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The Wild West of copyrighted characters in AI may be coming to an end. There has been legal wrangling over the role of copyright in the AI era, but the mother of all legal teams may now be gearing up for a fight. Disney has sent a cease and desist to Google, alleging the company‚Äôs AI tools are infringing Disney‚Äôs copyrights ‚Äúon a massive scale.‚Äù&lt;/p&gt;
&lt;p&gt;According to the letter, Google is violating the entertainment conglomerate‚Äôs intellectual property in multiple ways. The legal notice says Google has copied a ‚Äúlarge corpus‚Äù of Disney‚Äôs works to train its gen AI models, which is believable, as Google‚Äôs image and video models will happily produce popular Disney characters‚Äîthey couldn‚Äôt do that without feeding the models lots of Disney data.&lt;/p&gt;
&lt;p&gt;The C&amp;amp;D also takes issue with Google for distributing ‚Äúcopies of its protected works‚Äù to consumers. So all those memes you‚Äôve been making with Disney characters? Yeah, Disney doesn‚Äôt like that, either. The letter calls out a huge number of Disney-owned properties that can be prompted into existence in Google AI, including The Lion King, Deadpool, and Star Wars.&lt;/p&gt;
&lt;p&gt;The company calls on Google to immediately stop using Disney content in its AI tools and create measures to ensure that future AI outputs don‚Äôt produce any characters that Disney owns. Disney is famously litigious and has an army of lawyers dedicated to defending its copyrights. The nature of copyright law in the US is a direct result of Disney‚Äôs legal maneuvering, which has extended its control of iconic characters by decades.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;While Disney wants its characters out of Google AI generally, the letter specifically cited the AI tools in YouTube. Google has started adding its Veo AI video model to YouTube, allowing creators to more easily create and publish videos. That seems to be a greater concern for Disney than image models like Nano Banana.&lt;/p&gt;
&lt;p&gt;Google has said little about Disney‚Äôs warning‚Äîa warning Google must have known was coming. A Google spokesperson has issued the following brief statement on the mater.&lt;/p&gt;
&lt;p&gt;‚ÄúWe have a longstanding and mutually beneficial relationship with Disney, and will continue to engage with them,‚Äù Google says. ‚ÄúMore generally, we use public data from the open web to build our AI and have built additional innovative copyright controls like Google-extended and Content ID for YouTube, which give sites and copyright holders control over their content.‚Äù&lt;/p&gt;
&lt;p&gt;Perhaps this is previewing Google‚Äôs argument in a theoretical lawsuit. That copyrighted Disney content was all over the open internet, so is it really Google‚Äôs fault it ended up baked into the AI?&lt;/p&gt;
&lt;h2&gt;Content silos for AI&lt;/h2&gt;
&lt;p&gt;The generative AI boom has treated copyright as a mere suggestion as companies race to gobble up training data and remix it as ‚Äúnew‚Äù content. A cavalcade of companies, including The New York Times and Getty Images, have sued over how their material has been used and replicated by AI. Disney itself threatened a lawsuit against Character.AI earlier this year, leading to the removal of Disney content from the service.&lt;/p&gt;
&lt;p&gt;Google isn‚Äôt Character.AI, though. It‚Äôs probably no coincidence that Disney is challenging Google at the same time it is entering into a content deal with OpenAI. Disney has invested $1 billion in the AI firm and agreed to a three-year licensing deal that officially brings Disney characters to OpenAI‚Äôs Sora video app. The specifics of that arrangement are still subject to negotiations.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The launch of the Sora app earlier this year was widely derided by the entertainment industry, but that‚Äôs nothing a little money can‚Äôt solve. OpenAI required copyright owners to opt out of having their content included in the service, but it later reversed course to an opt-in model. The Disney deal is OpenAI‚Äôs first major content tie-in for AI.&lt;/p&gt;
&lt;p&gt;Meanwhile, Google‚Äôs AI tools don‚Äôt pay any mind to copyright. If you want to create images and videos with The Avengers, Super Mario, or any other character, Google doesn‚Äôt stand in your way. Whether or not that remains the case depends on how Google responds to Disney‚Äôs lawyers. There‚Äôs no indication that Disney‚Äôs licensing deal with OpenAI is exclusive, so it‚Äôs possible Google and Disney will reach an agreement to allow AI recreations. Google could also choose to fight back against this particular interpretation of copyright.&lt;/p&gt;
&lt;p&gt;Most companies would channel Character.AI and avoid a fight with Disney‚Äôs lawyers, but Google‚Äôs scale gives it more options. In either case, we could soon see the AI content ecosystem become a patchwork of content silos not unlike streaming media. If you want to generate an image featuring Moana, well, you‚Äôll need to go to OpenAI. If a DC character is more your speed, there may be a different AI firm that has a deal to let you do that. It‚Äôs hard to know who to root for in a battle between giant AI firms and equally giant entertainment companies.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Updated 12/11 with statement from Google.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2025/12/disney-says-google-ai-infringes-copyright-on-a-massive-scale/</guid><pubDate>Thu, 11 Dec 2025 19:29:29 +0000</pubDate></item><item><title>[NEW] OpenAI releases GPT-5.2 after ‚Äúcode red‚Äù Google threat alert (AI ‚Äì Ars Technica)</title><link>https://arstechnica.com/information-technology/2025/12/openai-releases-gpt-5-2-after-code-red-google-threat-alert/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Company claims new AI model tops Gemini and matches humans on 70% of work tasks.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="The OpenAI logo over a tectonic shift in the background." class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2024/09/openai_tectonic_shift-300x169.jpg" width="300" /&gt;
                  &lt;img alt="The OpenAI logo over a tectonic shift in the background." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/09/openai_tectonic_shift-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards / OpenAI

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Thursday, OpenAI released GPT-5.2, its newest family of AI models for ChatGPT, in three versions called Instant, Thinking, and Pro. The release follows CEO Sam Altman‚Äôs internal ‚Äúcode red‚Äù memo earlier this month, which directed company resources toward improving ChatGPT in response to competitive pressure from Google‚Äôs Gemini 3 AI model.&lt;/p&gt;
&lt;p&gt;‚ÄúWe designed 5.2 to unlock even more economic value for people,‚Äù Fidji Simo, OpenAI‚Äôs chief product officer, said during a press briefing with journalists on Thursday. ‚ÄúIt‚Äôs better at creating spreadsheets, building presentations, writing code, perceiving images, understanding long context, using tools and then linking complex, multi-step projects.‚Äù&lt;/p&gt;
&lt;p&gt;As with previous versions of GPT-5, the three model tiers serve different purposes: Instant handles faster tasks like writing and translation; Thinking spits out simulated reasoning ‚Äúthinking‚Äù text in an attempt to tackle more complex work like coding and math; and Pro spits out even more simulated reasoning text with the goal of delivering the highest-accuracy performance for difficult problems.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2131644 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="A chart of GPT-5.2 benchmark results taken from OpenAI's website." class="center large" height="762" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/gpt_52chart-1024x762.png" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A chart of GPT-5.2 Thinking benchmark results comparing it to its predecessor, taken from OpenAI‚Äôs website.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          OpenAI

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;GPT-5.2 features a 400,000-token context window, allowing it to process hundreds of documents at once, and a knowledge cutoff date of August 31, 2025.&lt;/p&gt;
&lt;p&gt;GPT-5.2 is rolling out to paid ChatGPT subscribers starting Thursday, with API access available to developers. Pricing in the API runs $1.75 per million input tokens for the standard model, a 40 percent increase over GPT-5.1. OpenAI says the older GPT-5.1 will remain available in ChatGPT for paid users for three months under a legacy models dropdown.&lt;/p&gt;
&lt;h2&gt;Playing catch-up with Google&lt;/h2&gt;
&lt;p&gt;The release follows a tricky month for OpenAI. In early December, Altman issued an internal ‚Äúcode red‚Äù directive after Google‚Äôs Gemini 3 model topped multiple AI benchmarks and gained market share. The memo called for delaying other initiatives, including advertising plans for ChatGPT, to focus on improving the chatbot‚Äôs core experience.&lt;/p&gt;
&lt;p&gt;The stakes for OpenAI are substantial. The company has made commitments totaling $1.4 trillion for AI infrastructure buildouts over the next several years, bets it made when it had a more obvious technology lead among AI companies. Google‚Äôs Gemini app now has more than 650 million monthly active users, while OpenAI reports 800 million weekly active users for ChatGPT.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In attempting to keep up with (or ahead of) the competition, model releases proceed at a steady clip: GPT-5.2 represents OpenAI‚Äôs third major model release since August. GPT-5 launched that month with a new routing system that toggles between instant-response and simulated reasoning modes, though users complained about responses that felt cold and clinical. November‚Äôs GPT-5.1 update added eight preset ‚Äúpersonality‚Äù options and focused on making the system more conversational.&lt;/p&gt;
&lt;h2&gt;Numbers go up&lt;/h2&gt;
&lt;p&gt;Oddly, even though the GPT-5.2 model release is ostensibly a response to Gemini 3‚Äôs performance, OpenAI chose not to list any benchmarks on its promotional website comparing the two models. Instead, the official blog post focuses on GPT-5.2‚Äôs improvements over its predecessors and its performance on OpenAI‚Äôs new GDPval benchmark, which attempts to measure professional knowledge work tasks across 44 occupations.&lt;/p&gt;
&lt;p&gt;During the press briefing, OpenAI did share some competition comparison benchmarks that included Gemini 3 Pro and Claude Opus 4.5 but pushed back on the narrative that GPT-5.2 was rushed to market in response to Google. ‚ÄúIt is important to note this has been in the works for many, many months,‚Äù Simo told reporters, although choosing when to release it, we‚Äôll note, is a strategic decision.&lt;/p&gt;
&lt;p&gt;According to the shared numbers, GPT-5.2 Thinking scored 55.6 percent on SWE-Bench Pro, a software engineering benchmark, compared to 43.3 percent for Gemini 3 Pro and 52.0 percent for Claude Opus 4.5. On GPQA Diamond, a graduate-level science benchmark, GPT-5.2 scored 92.4 percent versus Gemini 3 Pro‚Äôs 91.9 percent.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2131641 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="GPT-5.2 benchmarks that OpenAI shared with the press." class="fullwidth full" height="563" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/G76Fh4WagAAE_Ec.avif" width="1000" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      GPT-5.2 benchmarks that OpenAI shared with the press.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          OpenAI / Venturebeat

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;OpenAI says GPT-5.2 Thinking beats or ties ‚Äúhuman professionals‚Äù on 70.9 percent of tasks in the GDPval benchmark (compared to 53.3 percent for Gemini 3 Pro). The company also claims the model completes these tasks at more than 11 times the speed and less than 1 percent of the cost of human experts.&lt;/p&gt;
&lt;p&gt;GPT-5.2 Thinking also reportedly generates responses with 38 percent fewer confabulations than GPT-5.1, according to Max Schwarzer, OpenAI‚Äôs post-training lead, who told VentureBeat that the model ‚Äúhallucinates substantially less‚Äù than its predecessor.&lt;/p&gt;
&lt;p&gt;However, we always take benchmarks with a grain of salt because it‚Äôs easy to present them in a way that is positive to a company, especially when the science of measuring AI performance objectively hasn‚Äôt quite caught up with corporate sales pitches for humanlike AI capabilities.&lt;/p&gt;
&lt;p&gt;Independent benchmark results from researchers outside OpenAI will take time to arrive. In the meantime, if you use ChatGPT for work tasks, expect competent models with incremental improvements and some better coding performance thrown in for good measure.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;


  &lt;div class="listing-credit my-2"&gt;
    &lt;p class="text-gray-350 font-impact text-sm font-semibold"&gt;
    Listing image:
    Benj Edwards / OpenAI
  &lt;/p&gt;
  &lt;/div&gt;




  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Company claims new AI model tops Gemini and matches humans on 70% of work tasks.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="The OpenAI logo over a tectonic shift in the background." class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2024/09/openai_tectonic_shift-300x169.jpg" width="300" /&gt;
                  &lt;img alt="The OpenAI logo over a tectonic shift in the background." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/09/openai_tectonic_shift-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards / OpenAI

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Thursday, OpenAI released GPT-5.2, its newest family of AI models for ChatGPT, in three versions called Instant, Thinking, and Pro. The release follows CEO Sam Altman‚Äôs internal ‚Äúcode red‚Äù memo earlier this month, which directed company resources toward improving ChatGPT in response to competitive pressure from Google‚Äôs Gemini 3 AI model.&lt;/p&gt;
&lt;p&gt;‚ÄúWe designed 5.2 to unlock even more economic value for people,‚Äù Fidji Simo, OpenAI‚Äôs chief product officer, said during a press briefing with journalists on Thursday. ‚ÄúIt‚Äôs better at creating spreadsheets, building presentations, writing code, perceiving images, understanding long context, using tools and then linking complex, multi-step projects.‚Äù&lt;/p&gt;
&lt;p&gt;As with previous versions of GPT-5, the three model tiers serve different purposes: Instant handles faster tasks like writing and translation; Thinking spits out simulated reasoning ‚Äúthinking‚Äù text in an attempt to tackle more complex work like coding and math; and Pro spits out even more simulated reasoning text with the goal of delivering the highest-accuracy performance for difficult problems.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2131644 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="A chart of GPT-5.2 benchmark results taken from OpenAI's website." class="center large" height="762" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/gpt_52chart-1024x762.png" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A chart of GPT-5.2 Thinking benchmark results comparing it to its predecessor, taken from OpenAI‚Äôs website.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          OpenAI

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;GPT-5.2 features a 400,000-token context window, allowing it to process hundreds of documents at once, and a knowledge cutoff date of August 31, 2025.&lt;/p&gt;
&lt;p&gt;GPT-5.2 is rolling out to paid ChatGPT subscribers starting Thursday, with API access available to developers. Pricing in the API runs $1.75 per million input tokens for the standard model, a 40 percent increase over GPT-5.1. OpenAI says the older GPT-5.1 will remain available in ChatGPT for paid users for three months under a legacy models dropdown.&lt;/p&gt;
&lt;h2&gt;Playing catch-up with Google&lt;/h2&gt;
&lt;p&gt;The release follows a tricky month for OpenAI. In early December, Altman issued an internal ‚Äúcode red‚Äù directive after Google‚Äôs Gemini 3 model topped multiple AI benchmarks and gained market share. The memo called for delaying other initiatives, including advertising plans for ChatGPT, to focus on improving the chatbot‚Äôs core experience.&lt;/p&gt;
&lt;p&gt;The stakes for OpenAI are substantial. The company has made commitments totaling $1.4 trillion for AI infrastructure buildouts over the next several years, bets it made when it had a more obvious technology lead among AI companies. Google‚Äôs Gemini app now has more than 650 million monthly active users, while OpenAI reports 800 million weekly active users for ChatGPT.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In attempting to keep up with (or ahead of) the competition, model releases proceed at a steady clip: GPT-5.2 represents OpenAI‚Äôs third major model release since August. GPT-5 launched that month with a new routing system that toggles between instant-response and simulated reasoning modes, though users complained about responses that felt cold and clinical. November‚Äôs GPT-5.1 update added eight preset ‚Äúpersonality‚Äù options and focused on making the system more conversational.&lt;/p&gt;
&lt;h2&gt;Numbers go up&lt;/h2&gt;
&lt;p&gt;Oddly, even though the GPT-5.2 model release is ostensibly a response to Gemini 3‚Äôs performance, OpenAI chose not to list any benchmarks on its promotional website comparing the two models. Instead, the official blog post focuses on GPT-5.2‚Äôs improvements over its predecessors and its performance on OpenAI‚Äôs new GDPval benchmark, which attempts to measure professional knowledge work tasks across 44 occupations.&lt;/p&gt;
&lt;p&gt;During the press briefing, OpenAI did share some competition comparison benchmarks that included Gemini 3 Pro and Claude Opus 4.5 but pushed back on the narrative that GPT-5.2 was rushed to market in response to Google. ‚ÄúIt is important to note this has been in the works for many, many months,‚Äù Simo told reporters, although choosing when to release it, we‚Äôll note, is a strategic decision.&lt;/p&gt;
&lt;p&gt;According to the shared numbers, GPT-5.2 Thinking scored 55.6 percent on SWE-Bench Pro, a software engineering benchmark, compared to 43.3 percent for Gemini 3 Pro and 52.0 percent for Claude Opus 4.5. On GPQA Diamond, a graduate-level science benchmark, GPT-5.2 scored 92.4 percent versus Gemini 3 Pro‚Äôs 91.9 percent.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2131641 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="GPT-5.2 benchmarks that OpenAI shared with the press." class="fullwidth full" height="563" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/G76Fh4WagAAE_Ec.avif" width="1000" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      GPT-5.2 benchmarks that OpenAI shared with the press.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          OpenAI / Venturebeat

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;OpenAI says GPT-5.2 Thinking beats or ties ‚Äúhuman professionals‚Äù on 70.9 percent of tasks in the GDPval benchmark (compared to 53.3 percent for Gemini 3 Pro). The company also claims the model completes these tasks at more than 11 times the speed and less than 1 percent of the cost of human experts.&lt;/p&gt;
&lt;p&gt;GPT-5.2 Thinking also reportedly generates responses with 38 percent fewer confabulations than GPT-5.1, according to Max Schwarzer, OpenAI‚Äôs post-training lead, who told VentureBeat that the model ‚Äúhallucinates substantially less‚Äù than its predecessor.&lt;/p&gt;
&lt;p&gt;However, we always take benchmarks with a grain of salt because it‚Äôs easy to present them in a way that is positive to a company, especially when the science of measuring AI performance objectively hasn‚Äôt quite caught up with corporate sales pitches for humanlike AI capabilities.&lt;/p&gt;
&lt;p&gt;Independent benchmark results from researchers outside OpenAI will take time to arrive. In the meantime, if you use ChatGPT for work tasks, expect competent models with incremental improvements and some better coding performance thrown in for good measure.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;


  &lt;div class="listing-credit my-2"&gt;
    &lt;p class="text-gray-350 font-impact text-sm font-semibold"&gt;
    Listing image:
    Benj Edwards / OpenAI
  &lt;/p&gt;
  &lt;/div&gt;




  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/information-technology/2025/12/openai-releases-gpt-5-2-after-code-red-google-threat-alert/</guid><pubDate>Thu, 11 Dec 2025 21:27:18 +0000</pubDate></item><item><title>[NEW] 1X¬†struck a deal to send its ‚Äòhome‚Äô humanoids to¬†factories and warehouses (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/11/1x-struck-a-deal-to-send-its-home-humanoids-to-factories-and-warehouses/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/1X_NEO-Home-Duster.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Robotics company 1X found some big potential buyers for its humanoid robots designed for consumers ‚Äî the portfolio companies of one of its investors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company announced a strategic partnership to make thousands of its humanoid robots available for EQT‚Äôs portfolio companies on Thursday. EQT is a large Swedish multi-asset investor, and its venture fund EQT Ventures, is one of 1X‚Äôs backers.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This deal involves shipping up to 10,000 1X Neo humanoid robots between 2026 and 2030 to EQT‚Äôs more than 300 portfolio companies with a concentration on manufacturing, warehousing, logistics, and other industrial use cases.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;1X will sign individual deals with each of EQT‚Äôs interested portfolio companies, 1X confirmed to TechCrunch. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This partnership is particularly interesting because 1X‚Äôs Neo has been marketed as a humanoid for personal use and tagged as the ‚Äúfirst consumer-ready humanoid robot designed to transform life at home.‚Äù Unlike some of 1X‚Äôs peers, like Figure, it has not been marketed as a bot for commercial purposes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;1X does have a robot designed for industrial purposes, Eve Industrial, but this deal specifically involves the Neo humanoid.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When the company opened up preorders for the $20,000 robot in October, the announcement was focused on how the robot would operate in someone‚Äôs home from descriptions of the different chores that the robot is able to perform and how it interacts with people.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;This deal is quite a different use case.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That‚Äôs likely because humanoids for the home will remain a hard sell for quite some time while industrial use cases are an easier sell. The $20,000 price tag automatically limits the potential pool of consumer customers too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Neo specifically also comes with a privacy element that would be hard to swallow for many people ‚Äî human operators from 1X are able to look through the robots eyes into someone‚Äôs home.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Humanoids also come with potential safety issues around pets and small children due to their size and instability. Multiple VCs and scientists in the robotics field told TechCrunch this summer that humanoid adoption wouldn‚Äôt be for multiple years, if not a decade away.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company declined to share how many preorders it received for its Neo bot but a spokesperson said preorders ‚Äúfar exceeded‚Äù the company‚Äôs goal.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Founded in 2014, 1x has since raised more than $130 million in venture capital from firms, including EQT Ventures, Tiger Global, and the OpenAI Startup Fund, among others.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/1X_NEO-Home-Duster.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Robotics company 1X found some big potential buyers for its humanoid robots designed for consumers ‚Äî the portfolio companies of one of its investors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company announced a strategic partnership to make thousands of its humanoid robots available for EQT‚Äôs portfolio companies on Thursday. EQT is a large Swedish multi-asset investor, and its venture fund EQT Ventures, is one of 1X‚Äôs backers.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This deal involves shipping up to 10,000 1X Neo humanoid robots between 2026 and 2030 to EQT‚Äôs more than 300 portfolio companies with a concentration on manufacturing, warehousing, logistics, and other industrial use cases.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;1X will sign individual deals with each of EQT‚Äôs interested portfolio companies, 1X confirmed to TechCrunch. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This partnership is particularly interesting because 1X‚Äôs Neo has been marketed as a humanoid for personal use and tagged as the ‚Äúfirst consumer-ready humanoid robot designed to transform life at home.‚Äù Unlike some of 1X‚Äôs peers, like Figure, it has not been marketed as a bot for commercial purposes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;1X does have a robot designed for industrial purposes, Eve Industrial, but this deal specifically involves the Neo humanoid.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When the company opened up preorders for the $20,000 robot in October, the announcement was focused on how the robot would operate in someone‚Äôs home from descriptions of the different chores that the robot is able to perform and how it interacts with people.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;This deal is quite a different use case.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That‚Äôs likely because humanoids for the home will remain a hard sell for quite some time while industrial use cases are an easier sell. The $20,000 price tag automatically limits the potential pool of consumer customers too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Neo specifically also comes with a privacy element that would be hard to swallow for many people ‚Äî human operators from 1X are able to look through the robots eyes into someone‚Äôs home.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Humanoids also come with potential safety issues around pets and small children due to their size and instability. Multiple VCs and scientists in the robotics field told TechCrunch this summer that humanoid adoption wouldn‚Äôt be for multiple years, if not a decade away.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company declined to share how many preorders it received for its Neo bot but a spokesperson said preorders ‚Äúfar exceeded‚Äù the company‚Äôs goal.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Founded in 2014, 1x has since raised more than $130 million in venture capital from firms, including EQT Ventures, Tiger Global, and the OpenAI Startup Fund, among others.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/11/1x-struck-a-deal-to-send-its-home-humanoids-to-factories-and-warehouses/</guid><pubDate>Thu, 11 Dec 2025 22:03:36 +0000</pubDate></item><item><title>[NEW] GPT-5.2 first impressions: a powerful update, especially for business tasks and workflows (AI | VentureBeat)</title><link>https://venturebeat.com/ai/gpt-5-2-first-impressions-a-powerful-update-especially-for-business-tasks</link><description>[unable to retrieve full-text content]&lt;p&gt;OpenAI has officially &lt;a href="https://venturebeat.com/ai/openais-gpt-5-2-is-here-what-enterprises-need-to-know"&gt;released GPT-5.2&lt;/a&gt;, and the reactions from early testers ‚Äî among whom OpenAI seeded the model several days prior to public release, in some cases weeks ago ‚Äî paints a two toned picture: it is a monumental leap forward for deep, autonomous reasoning and coding, yet potentially an underwhelming &amp;quot;incremental&amp;quot; update for casual conversationalists.&lt;/p&gt;&lt;p&gt;Following early access periods and today&amp;#x27;s broader rollout, executives, developers, and analysts have taken to X (formerly Twitter) and company blogs to share their first testing results. &lt;/p&gt;&lt;p&gt;Here is a roundup of the first reactions to OpenAI‚Äôs latest flagship model.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;&amp;quot;AI as a serious analyst&amp;quot;&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The strongest praise for GPT-5.2 centers on its ability to handle &amp;quot;hard problems&amp;quot; that require extended thinking time.&lt;/p&gt;&lt;p&gt;Matt Shumer, CEO of HyperWriteAI, did not mince words in &lt;a href="https://shumer.dev/gpt52review"&gt;his review&lt;/a&gt;, calling GPT-5.2 Pro &amp;quot;the best model in the world.&amp;quot; &lt;/p&gt;&lt;p&gt;Shumer highlighted the model&amp;#x27;s tenacity, noting that &amp;quot;it thinks for **over an hour** on hard problems. And it nails tasks no other model can touch.&amp;quot;&lt;/p&gt;&lt;p&gt;This sentiment was&lt;a href="https://x.com/alliekmiller/status/1999189893910790427"&gt; echoed by Allie K. Miller&lt;/a&gt;, an AI entrepreneur and former AWS executive. Miller described the model as a step toward &amp;quot;AI as a serious analyst&amp;quot; rather than a &amp;quot;friendly companion.&amp;quot;&lt;/p&gt;&lt;p&gt;&amp;quot;The thinking and problem-solving feel noticeably stronger,&amp;quot; Miller wrote on X. &amp;quot;It gives much deeper explanations than I‚Äôm used to seeing. At one point it literally wrote code to improve its own OCR in the middle of a task.&amp;quot;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Enterprise gains: Box reports distinct performance jumps&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;For the enterprise sector, the update appears to be even more significant. &lt;/p&gt;&lt;p&gt;&lt;a href="https://x.com/levie/status/1999191612321391058"&gt;Aaron Levie, CEO of Box, revealed on X&lt;/a&gt; that his company has been testing GPT-5.2 in early access. Levie reported that the model performs &amp;quot;7 points better than GPT-5.1&amp;quot; on their expanded reasoning tests, which approximate real-world knowledge work in financial services and life sciences.&lt;/p&gt;&lt;p&gt;&amp;quot;The model performed the majority of the tasks far faster than GPT-5.1 and GPT-5 as well,&amp;quot; Levie noted, confirming that Box AI will be rolling out GPT-5.2 integration shortly.&lt;/p&gt;&lt;p&gt;Rutuja Rajwade, a Senior Product Marketing Manager at Box, &lt;a href="https://blog.box.com/how-openais-gpt-52-delivers-lightning-fast-specialist-level-reasoning"&gt;expanded on this in a company blog post&lt;/a&gt;, citing specific latency improvements. &lt;/p&gt;&lt;p&gt;&amp;quot;Complex extraction&amp;quot; tasks dropped from 46 seconds on GPT-5 to just 12 seconds with GPT-5.2. &lt;/p&gt;&lt;p&gt;Rajwade also noted a jump in reasoning capabilities for the Media and Entertainment vertical, rising from 76% accuracy in GPT-5.1 to 81% in the new model.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;A &amp;quot;serious leap&amp;quot; for coding and simulation&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Developers are finding GPT-5.2 particularly potent for &amp;quot;one-shot&amp;quot; generation of complex code structures.&lt;/p&gt;&lt;p&gt;Pietro Schirano, CEO of magicpathai, &lt;a href="https://x.com/skirano/status/1999182295685644366"&gt;shared a video &lt;/a&gt;of the model building a full 3D graphics engine in a single file with interactive controls. &amp;quot;It‚Äôs a serious leap forward in complex reasoning, math, coding, and simulations,&amp;quot; Schirano posted. &amp;quot;The pace of progress is unreal.&amp;quot;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;S&lt;!-- --&gt;imilarly, Ethan Mollick, a professor at the Wharton School of Business at the University of Pennsylvania and longtime LLM and AI power user and writer, &lt;a href="https://x.com/emollick/status/1999185085719887978?s=20"&gt;demonstrated the model&amp;#x27;s ability to create a visually complex shader&lt;/a&gt;‚Äîan infinite neo-gothic city in a stormy ocean‚Äîvia a single prompt.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The Agentic Era: Long-running autonomy&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Perhaps the most functional shift is the model&amp;#x27;s ability to stay on task for hours without losing the thread.&lt;/p&gt;&lt;p&gt;&lt;a href="https://x.com/danshipper/status/1999180972995281298?s=20"&gt;Dan Shipper, CEO of thoughtful AI testing newsletter Every&lt;/a&gt;, reported that the model successfully performed a profit and loss (P&amp;amp;L) analysis that required it to work autonomously for two hours. &amp;quot;It did a P&amp;amp;L analysis where it worked for 2 hours and gave me great results,&amp;quot; Shipper wrote.&lt;/p&gt;&lt;p&gt;However, Shipper also noted that for day-to-day tasks, the update feels &amp;quot;mostly incremental.&amp;quot; &lt;/p&gt;&lt;p&gt;In &lt;a href="https://every.to/vibe-check/vibe-check-gpt-5-2-is-an-incremental-upgrade"&gt;an article for Every&lt;/a&gt;, Katie Parrott wrote that while GPT-5.2 excels at instruction following, it is &amp;quot;less resourceful&amp;quot; than competitors like Claude Opus 4.5 in certain contexts, such as deducing a user&amp;#x27;s location from email data.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The downsides: Speed and Rigidity&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Despite the reasoning capabilities, the &amp;quot;feel&amp;quot; of the model has drawn critique.&lt;/p&gt;&lt;p&gt;Shumer highlighted a significant &amp;quot;speed penalty&amp;quot; when using the model&amp;#x27;s Thinking mode. &amp;quot;In my experience the Thinking mode is very slow for most questions,&amp;quot; Shumer wrote in his deep-dive review. &amp;quot;I almost never use Instant.&amp;quot;&lt;/p&gt;&lt;p&gt;Allie Miller also pointed out issues with the model&amp;#x27;s default behavior. &amp;quot;The downside is tone and format,&amp;quot; she noted. &amp;quot;The default voice felt a bit more rigid, and the length/markdown behavior is extreme: a simple question turned into 58 bullets and numbered points.&amp;quot;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The Verdict&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The early reaction suggests that GPT-5.2 is a tool optimized for power users, developers, and enterprise agents rather than casual chat. As Shumer summarized in his review: &amp;quot;For deep research, complex reasoning, and tasks that benefit from careful thought, GPT-5.2 Pro is the best option available right now.&amp;quot;&lt;/p&gt;&lt;p&gt;However, for users seeking creative writing or quick, fluid answers, models like Claude Opus 4.5 remain strong competitors. &amp;quot;My favorite model remains Claude Opus 4.5,&amp;quot; Miller admitted, &amp;quot;but my complex ChatGPT work will get a nice incremental boost.&amp;quot;&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;OpenAI has officially &lt;a href="https://venturebeat.com/ai/openais-gpt-5-2-is-here-what-enterprises-need-to-know"&gt;released GPT-5.2&lt;/a&gt;, and the reactions from early testers ‚Äî among whom OpenAI seeded the model several days prior to public release, in some cases weeks ago ‚Äî paints a two toned picture: it is a monumental leap forward for deep, autonomous reasoning and coding, yet potentially an underwhelming &amp;quot;incremental&amp;quot; update for casual conversationalists.&lt;/p&gt;&lt;p&gt;Following early access periods and today&amp;#x27;s broader rollout, executives, developers, and analysts have taken to X (formerly Twitter) and company blogs to share their first testing results. &lt;/p&gt;&lt;p&gt;Here is a roundup of the first reactions to OpenAI‚Äôs latest flagship model.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;&amp;quot;AI as a serious analyst&amp;quot;&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The strongest praise for GPT-5.2 centers on its ability to handle &amp;quot;hard problems&amp;quot; that require extended thinking time.&lt;/p&gt;&lt;p&gt;Matt Shumer, CEO of HyperWriteAI, did not mince words in &lt;a href="https://shumer.dev/gpt52review"&gt;his review&lt;/a&gt;, calling GPT-5.2 Pro &amp;quot;the best model in the world.&amp;quot; &lt;/p&gt;&lt;p&gt;Shumer highlighted the model&amp;#x27;s tenacity, noting that &amp;quot;it thinks for **over an hour** on hard problems. And it nails tasks no other model can touch.&amp;quot;&lt;/p&gt;&lt;p&gt;This sentiment was&lt;a href="https://x.com/alliekmiller/status/1999189893910790427"&gt; echoed by Allie K. Miller&lt;/a&gt;, an AI entrepreneur and former AWS executive. Miller described the model as a step toward &amp;quot;AI as a serious analyst&amp;quot; rather than a &amp;quot;friendly companion.&amp;quot;&lt;/p&gt;&lt;p&gt;&amp;quot;The thinking and problem-solving feel noticeably stronger,&amp;quot; Miller wrote on X. &amp;quot;It gives much deeper explanations than I‚Äôm used to seeing. At one point it literally wrote code to improve its own OCR in the middle of a task.&amp;quot;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Enterprise gains: Box reports distinct performance jumps&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;For the enterprise sector, the update appears to be even more significant. &lt;/p&gt;&lt;p&gt;&lt;a href="https://x.com/levie/status/1999191612321391058"&gt;Aaron Levie, CEO of Box, revealed on X&lt;/a&gt; that his company has been testing GPT-5.2 in early access. Levie reported that the model performs &amp;quot;7 points better than GPT-5.1&amp;quot; on their expanded reasoning tests, which approximate real-world knowledge work in financial services and life sciences.&lt;/p&gt;&lt;p&gt;&amp;quot;The model performed the majority of the tasks far faster than GPT-5.1 and GPT-5 as well,&amp;quot; Levie noted, confirming that Box AI will be rolling out GPT-5.2 integration shortly.&lt;/p&gt;&lt;p&gt;Rutuja Rajwade, a Senior Product Marketing Manager at Box, &lt;a href="https://blog.box.com/how-openais-gpt-52-delivers-lightning-fast-specialist-level-reasoning"&gt;expanded on this in a company blog post&lt;/a&gt;, citing specific latency improvements. &lt;/p&gt;&lt;p&gt;&amp;quot;Complex extraction&amp;quot; tasks dropped from 46 seconds on GPT-5 to just 12 seconds with GPT-5.2. &lt;/p&gt;&lt;p&gt;Rajwade also noted a jump in reasoning capabilities for the Media and Entertainment vertical, rising from 76% accuracy in GPT-5.1 to 81% in the new model.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;A &amp;quot;serious leap&amp;quot; for coding and simulation&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Developers are finding GPT-5.2 particularly potent for &amp;quot;one-shot&amp;quot; generation of complex code structures.&lt;/p&gt;&lt;p&gt;Pietro Schirano, CEO of magicpathai, &lt;a href="https://x.com/skirano/status/1999182295685644366"&gt;shared a video &lt;/a&gt;of the model building a full 3D graphics engine in a single file with interactive controls. &amp;quot;It‚Äôs a serious leap forward in complex reasoning, math, coding, and simulations,&amp;quot; Schirano posted. &amp;quot;The pace of progress is unreal.&amp;quot;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;S&lt;!-- --&gt;imilarly, Ethan Mollick, a professor at the Wharton School of Business at the University of Pennsylvania and longtime LLM and AI power user and writer, &lt;a href="https://x.com/emollick/status/1999185085719887978?s=20"&gt;demonstrated the model&amp;#x27;s ability to create a visually complex shader&lt;/a&gt;‚Äîan infinite neo-gothic city in a stormy ocean‚Äîvia a single prompt.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The Agentic Era: Long-running autonomy&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Perhaps the most functional shift is the model&amp;#x27;s ability to stay on task for hours without losing the thread.&lt;/p&gt;&lt;p&gt;&lt;a href="https://x.com/danshipper/status/1999180972995281298?s=20"&gt;Dan Shipper, CEO of thoughtful AI testing newsletter Every&lt;/a&gt;, reported that the model successfully performed a profit and loss (P&amp;amp;L) analysis that required it to work autonomously for two hours. &amp;quot;It did a P&amp;amp;L analysis where it worked for 2 hours and gave me great results,&amp;quot; Shipper wrote.&lt;/p&gt;&lt;p&gt;However, Shipper also noted that for day-to-day tasks, the update feels &amp;quot;mostly incremental.&amp;quot; &lt;/p&gt;&lt;p&gt;In &lt;a href="https://every.to/vibe-check/vibe-check-gpt-5-2-is-an-incremental-upgrade"&gt;an article for Every&lt;/a&gt;, Katie Parrott wrote that while GPT-5.2 excels at instruction following, it is &amp;quot;less resourceful&amp;quot; than competitors like Claude Opus 4.5 in certain contexts, such as deducing a user&amp;#x27;s location from email data.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The downsides: Speed and Rigidity&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Despite the reasoning capabilities, the &amp;quot;feel&amp;quot; of the model has drawn critique.&lt;/p&gt;&lt;p&gt;Shumer highlighted a significant &amp;quot;speed penalty&amp;quot; when using the model&amp;#x27;s Thinking mode. &amp;quot;In my experience the Thinking mode is very slow for most questions,&amp;quot; Shumer wrote in his deep-dive review. &amp;quot;I almost never use Instant.&amp;quot;&lt;/p&gt;&lt;p&gt;Allie Miller also pointed out issues with the model&amp;#x27;s default behavior. &amp;quot;The downside is tone and format,&amp;quot; she noted. &amp;quot;The default voice felt a bit more rigid, and the length/markdown behavior is extreme: a simple question turned into 58 bullets and numbered points.&amp;quot;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The Verdict&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The early reaction suggests that GPT-5.2 is a tool optimized for power users, developers, and enterprise agents rather than casual chat. As Shumer summarized in his review: &amp;quot;For deep research, complex reasoning, and tasks that benefit from careful thought, GPT-5.2 Pro is the best option available right now.&amp;quot;&lt;/p&gt;&lt;p&gt;However, for users seeking creative writing or quick, fluid answers, models like Claude Opus 4.5 remain strong competitors. &amp;quot;My favorite model remains Claude Opus 4.5,&amp;quot; Miller admitted, &amp;quot;but my complex ChatGPT work will get a nice incremental boost.&amp;quot;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/gpt-5-2-first-impressions-a-powerful-update-especially-for-business-tasks</guid><pubDate>Thu, 11 Dec 2025 23:26:00 +0000</pubDate></item><item><title>[NEW] Runway claims its GWM-1 ‚Äúworld models‚Äù can stay coherent for minutes at a time (AI ‚Äì Ars Technica)</title><link>https://arstechnica.com/ai/2025/12/with-gwm-1-family-of-world-models-runway-shows-ambitions-beyond-hollywood/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Runway joins a competitive field alongside Google, Nvidia, and others.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A first-person view from a canoe on a lake in an AI-generated video" class="absolute inset-0 w-full h-full object-cover hidden" height="358" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Runway-GWM-Worlds-640x358.jpg" width="640" /&gt;
                  &lt;img alt="A first-person view from a canoe on a lake in an AI-generated video" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Runway-GWM-Worlds-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Runway claims GWM Worlds can simulate movement in vehicles or boats, not just on foot.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Runway

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;AI company Runway has announced what it calls its first world model, GWM-1. It‚Äôs a significant step in a new direction for a company that has made its name primarily on video generation, and it‚Äôs part of a wider gold rush to build a new frontier of models as large language models and image and video generation move into a refinement phase, no longer an untapped frontier.&lt;/p&gt;
&lt;p&gt;GWM-1 is a blanket term for a trio of autoregression models, each built on top Runway‚Äôs Gen-4.5 text-to-video generation model and then post-trained with domain-specific data for different kinds of applications. Here‚Äôs what each does.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Runway‚Äôs world model announcement livestream video.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;h2&gt;GWM Worlds&lt;/h2&gt;
&lt;p&gt;GWM Worlds offers an interface for digital environment exploration with real-time user input that affects the generation of coming frames, which Runway suggests can remain consistent and coherent ‚Äúacross long sequences of movement.‚Äù&lt;/p&gt;
&lt;p&gt;Users can define the nature of the world‚Äîwhat it contains and how it appears‚Äîas well as rules like physics. They can give it actions or changes that will be reflected in real-time, like camera movements or descriptions of changes to the environment or the objects in it. As the methodology here is basically an advanced form of frame prediction, it might be a stretch to say these are full-on world simulations, but the claim is that they‚Äôre reliable enough to be usable as such.&lt;/p&gt;
&lt;p&gt;Potential applications include pre-visualization and early iteration for game design and development, generation of virtual reality environments, or educational explorations of historical spaces.&lt;/p&gt;
&lt;p&gt;There‚Äôs also a major use case that takes this outside Runway‚Äôs usual area of focus: World models like this can be used to train AI agents of various types, including robots.&lt;/p&gt;
&lt;h2&gt;GWM Robotics&lt;/h2&gt;
&lt;p&gt;The second model, GWM Robotics, does just that. It can be used ‚Äúto generate synthetic training data that augments your existing robotics datasets across multiple dimensions, including novel objects, task instructions, and environmental variations.‚Äù&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2131712 align-none"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="none large" height="578" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Runway-GWM-Robotics-1024x578.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A video generated with Runway‚Äôs GWM Robotics.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Runway

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;There are a couple of key applications for this in the field of robotics. First, a world model could be used for training scenarios that are otherwise very hard to reliably reproduce in the physical world, such as varying weather conditions. There‚Äôs also policy evaluation‚Äîtesting control policies entirely in a simulated world before real-world testing, which is safer and cheaper.&lt;/p&gt;
&lt;p&gt;Runway has put together a Python SDK for its robotics world model API that is currently available on a per-request basis.&lt;/p&gt;
&lt;h2&gt;GWM Avatars&lt;/h2&gt;
&lt;p&gt;Lastly, GWM Avatars combines generative video and speech in a unified model to produce human-like avatars that emote and move naturally both while speaking and listening. Runway claims they can maintain ‚Äúextended conversations without quality degradation‚Äù‚Äîa mighty feat if true. It‚Äôs coming to the web app and the API in the future.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2131713 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="A simulated man speaks on the phone" class="fullwidth full" height="585" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Runway-GWM-Avatars.jpg" width="938" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Faces animated semi-realistically during extended conversation, Runway says.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Runway

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;‚ÄúGeneral‚Äù-ish&lt;/h2&gt;
&lt;p&gt;Those who have described ‚Äúgeneral‚Äù world models are aiming for something grand: a multi-purpose, foundational model that works out of the box to simulate many types of environments, usable for any tasks, agents, and applications across multiple domains.&lt;/p&gt;
&lt;p&gt;World models are definitely not new, but the idea that they can be that general is a relatively recent ambition, and it‚Äôs often framed as a stepping stone to artificial general intelligence (AGI)‚Äîthough there‚Äôs no evidence yet that they will in fact lead there for most definitions of the term.&lt;/p&gt;
&lt;p&gt;Runway didn‚Äôt use AGI framing in this announcement as others like Google‚Äôs DeepMind have. That said, CEO Crist√≥bal Valenzuela did take to X to describe GWM-1 as ‚Äúa major step toward universal simulation.‚Äù That itself is a lofty end point, as we don‚Äôt yet have any evidence the current path will lead to something that comprehensive, and you also have to consider that there‚Äôs not much consensus on the definition of ‚Äúuniversal.‚Äù&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Even using the word ‚Äúgeneral‚Äù has an air of aspiration to it. You would expect a general world model to be, well, one model‚Äîbut in this case, we‚Äôre looking at three distinct, post-trained models. That caveats the general-ness a bit, but Runway says that it‚Äôs ‚Äúworking toward unifying many different domains and action spaces under a single base world model.‚Äù&lt;/p&gt;
&lt;h2&gt;A competitive field&lt;/h2&gt;
&lt;p&gt;And that brings us to another important consideration: With GWM-1, Runway is entering a competitive gold-rush space where its differentiators and competitive advantages are less clear than they were for video. With video, Runway has been able to make major inroads in film/television, advertising, and other industries because its founders are perceived as being more rooted in those creative industries than most competitors, and they‚Äôve designed tools with those industries in mind.&lt;/p&gt;
&lt;p&gt;There are indeed hypothetical applications of world models in film, television, advertising, and game development‚Äîbut it was apparent from Runway‚Äôs livestream that the company is also looking at applications in robotics as well as physics and life sciences research, where competitors are already well-established and where we‚Äôve seen increasing investment in recent months.&lt;/p&gt;
&lt;p&gt;Many of those competitors are big tech companies with massive resource advantages over Runway. Runway was one of the first to market with a sellable product, and its aggressive efforts to court industry professionals directly has so far allowed it to overcome those advantages in video generation, but it remains to be seen how things will play out with world models, where it doesn‚Äôt enjoy either advantage any more than the other entrants.&lt;/p&gt;
&lt;p&gt;Regardless, the GWM-1 advancements are impressive‚Äîespecially if Runway‚Äôs claims about consistency and coherence over longer stretches of time are true.&lt;/p&gt;
&lt;p&gt;Runway also used its livestream to announce new Gen 4.5 video generation capabilities, including native audio, audio editing, and multi-shot video editing. Further, it announced a deal with CoreWeave, a cloud computing company with an AI focus. The deal will see Runway utilizing Nvidia‚Äôs GB300 NVL72 racks on CoreWeave‚Äôs cloud infrastructure for future training and inference.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Runway joins a competitive field alongside Google, Nvidia, and others.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A first-person view from a canoe on a lake in an AI-generated video" class="absolute inset-0 w-full h-full object-cover hidden" height="358" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Runway-GWM-Worlds-640x358.jpg" width="640" /&gt;
                  &lt;img alt="A first-person view from a canoe on a lake in an AI-generated video" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Runway-GWM-Worlds-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Runway claims GWM Worlds can simulate movement in vehicles or boats, not just on foot.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Runway

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;AI company Runway has announced what it calls its first world model, GWM-1. It‚Äôs a significant step in a new direction for a company that has made its name primarily on video generation, and it‚Äôs part of a wider gold rush to build a new frontier of models as large language models and image and video generation move into a refinement phase, no longer an untapped frontier.&lt;/p&gt;
&lt;p&gt;GWM-1 is a blanket term for a trio of autoregression models, each built on top Runway‚Äôs Gen-4.5 text-to-video generation model and then post-trained with domain-specific data for different kinds of applications. Here‚Äôs what each does.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Runway‚Äôs world model announcement livestream video.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;h2&gt;GWM Worlds&lt;/h2&gt;
&lt;p&gt;GWM Worlds offers an interface for digital environment exploration with real-time user input that affects the generation of coming frames, which Runway suggests can remain consistent and coherent ‚Äúacross long sequences of movement.‚Äù&lt;/p&gt;
&lt;p&gt;Users can define the nature of the world‚Äîwhat it contains and how it appears‚Äîas well as rules like physics. They can give it actions or changes that will be reflected in real-time, like camera movements or descriptions of changes to the environment or the objects in it. As the methodology here is basically an advanced form of frame prediction, it might be a stretch to say these are full-on world simulations, but the claim is that they‚Äôre reliable enough to be usable as such.&lt;/p&gt;
&lt;p&gt;Potential applications include pre-visualization and early iteration for game design and development, generation of virtual reality environments, or educational explorations of historical spaces.&lt;/p&gt;
&lt;p&gt;There‚Äôs also a major use case that takes this outside Runway‚Äôs usual area of focus: World models like this can be used to train AI agents of various types, including robots.&lt;/p&gt;
&lt;h2&gt;GWM Robotics&lt;/h2&gt;
&lt;p&gt;The second model, GWM Robotics, does just that. It can be used ‚Äúto generate synthetic training data that augments your existing robotics datasets across multiple dimensions, including novel objects, task instructions, and environmental variations.‚Äù&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2131712 align-none"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="none large" height="578" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Runway-GWM-Robotics-1024x578.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A video generated with Runway‚Äôs GWM Robotics.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Runway

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;There are a couple of key applications for this in the field of robotics. First, a world model could be used for training scenarios that are otherwise very hard to reliably reproduce in the physical world, such as varying weather conditions. There‚Äôs also policy evaluation‚Äîtesting control policies entirely in a simulated world before real-world testing, which is safer and cheaper.&lt;/p&gt;
&lt;p&gt;Runway has put together a Python SDK for its robotics world model API that is currently available on a per-request basis.&lt;/p&gt;
&lt;h2&gt;GWM Avatars&lt;/h2&gt;
&lt;p&gt;Lastly, GWM Avatars combines generative video and speech in a unified model to produce human-like avatars that emote and move naturally both while speaking and listening. Runway claims they can maintain ‚Äúextended conversations without quality degradation‚Äù‚Äîa mighty feat if true. It‚Äôs coming to the web app and the API in the future.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2131713 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="A simulated man speaks on the phone" class="fullwidth full" height="585" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Runway-GWM-Avatars.jpg" width="938" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Faces animated semi-realistically during extended conversation, Runway says.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Runway

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;‚ÄúGeneral‚Äù-ish&lt;/h2&gt;
&lt;p&gt;Those who have described ‚Äúgeneral‚Äù world models are aiming for something grand: a multi-purpose, foundational model that works out of the box to simulate many types of environments, usable for any tasks, agents, and applications across multiple domains.&lt;/p&gt;
&lt;p&gt;World models are definitely not new, but the idea that they can be that general is a relatively recent ambition, and it‚Äôs often framed as a stepping stone to artificial general intelligence (AGI)‚Äîthough there‚Äôs no evidence yet that they will in fact lead there for most definitions of the term.&lt;/p&gt;
&lt;p&gt;Runway didn‚Äôt use AGI framing in this announcement as others like Google‚Äôs DeepMind have. That said, CEO Crist√≥bal Valenzuela did take to X to describe GWM-1 as ‚Äúa major step toward universal simulation.‚Äù That itself is a lofty end point, as we don‚Äôt yet have any evidence the current path will lead to something that comprehensive, and you also have to consider that there‚Äôs not much consensus on the definition of ‚Äúuniversal.‚Äù&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Even using the word ‚Äúgeneral‚Äù has an air of aspiration to it. You would expect a general world model to be, well, one model‚Äîbut in this case, we‚Äôre looking at three distinct, post-trained models. That caveats the general-ness a bit, but Runway says that it‚Äôs ‚Äúworking toward unifying many different domains and action spaces under a single base world model.‚Äù&lt;/p&gt;
&lt;h2&gt;A competitive field&lt;/h2&gt;
&lt;p&gt;And that brings us to another important consideration: With GWM-1, Runway is entering a competitive gold-rush space where its differentiators and competitive advantages are less clear than they were for video. With video, Runway has been able to make major inroads in film/television, advertising, and other industries because its founders are perceived as being more rooted in those creative industries than most competitors, and they‚Äôve designed tools with those industries in mind.&lt;/p&gt;
&lt;p&gt;There are indeed hypothetical applications of world models in film, television, advertising, and game development‚Äîbut it was apparent from Runway‚Äôs livestream that the company is also looking at applications in robotics as well as physics and life sciences research, where competitors are already well-established and where we‚Äôve seen increasing investment in recent months.&lt;/p&gt;
&lt;p&gt;Many of those competitors are big tech companies with massive resource advantages over Runway. Runway was one of the first to market with a sellable product, and its aggressive efforts to court industry professionals directly has so far allowed it to overcome those advantages in video generation, but it remains to be seen how things will play out with world models, where it doesn‚Äôt enjoy either advantage any more than the other entrants.&lt;/p&gt;
&lt;p&gt;Regardless, the GWM-1 advancements are impressive‚Äîespecially if Runway‚Äôs claims about consistency and coherence over longer stretches of time are true.&lt;/p&gt;
&lt;p&gt;Runway also used its livestream to announce new Gen 4.5 video generation capabilities, including native audio, audio editing, and multi-shot video editing. Further, it announced a deal with CoreWeave, a cloud computing company with an AI focus. The deal will see Runway utilizing Nvidia‚Äôs GB300 NVL72 racks on CoreWeave‚Äôs cloud infrastructure for future training and inference.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/12/with-gwm-1-family-of-world-models-runway-shows-ambitions-beyond-hollywood/</guid><pubDate>Thu, 11 Dec 2025 23:47:33 +0000</pubDate></item><item><title>[NEW] Google launched its deepest AI research agent yet ‚Äî on the same day OpenAI dropped GPT-5.2 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/11/google-launched-its-deepest-ai-research-agent-yet-on-the-same-day-openai-dropped-gpt-5-2/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2022/05/GettyImages-1147600063.jpg?resize=1200,817" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google released on Thursday a ‚Äúreimagined‚Äù version of its research agent Gemini Deep Research based on its much-ballyhooed state-of-the-art foundation model, Gemini 3 Pro.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This new agent isn‚Äôt just designed to produce research reports ‚Äî although it can still do that. It now allows developers to embed Google‚Äôs SATA-model research capabilities into their own apps. That capability is made possible through Google‚Äôs new Interactions API, which is designed to give devs more control in the coming agentic AI era.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The new Gemini Deep Research tool is an agent equipped to synthesize mountains of information and handle a large context dump in the prompt. Google says it‚Äôs used by customers for tasks ranging from due diligence to drug toxicity safety research.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google also says it will soon be integrating this new deep research agent into services, including Google Search, Google Finance, its Gemini App, and its popular NotebookLM. This is another step toward preparing for a world where humans don‚Äôt Google anything anymore ‚Äî their AI agents do.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tech giant says that Deep Research benefits from Gemini 3 Pro‚Äôs status as its ‚Äúmost factual‚Äù model that is trained to minimize hallucinations during complex tasks. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI hallucinations ‚Äî where the LLM just makes stuff up ‚Äî are an especially crucial issue for long-running, deep reasoning agentic tasks, in which many autonomous decisions are made over minutes, hours, or longer. The more choices an LLM has to make, the greater the chance that even one hallucinated choice will invalidate the entire output.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To prove its progress claims, Google has also created yet another benchmark (as if the AI world needs another one). The new benchmark is unimaginatively named DeepSearchQA and is intended to test agents on complex, multi-step information-seeking tasks. Google has open sourced this benchmark.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;It also tested Deep Research on Humanity‚Äôs Last Exam, a much more interestingly named, independent benchmark of general knowledge filled with impossibly niche tasks; and BrowserComp, a benchmark for browser-based agentic tasks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As you might expect, Google‚Äôs new agent bested the competition on its own benchmark, and Humanity‚Äôs. However, OpenAI‚Äôs ChatGPT 5 Pro was a surprisingly close second all the way around and slightly bested Google on BrowserComp.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But those benchmark comparisons were obsolete almost the moment Google published them. Because on the same day, OpenAI launched its highly anticipated GPT 5.2 ‚Äî codenamed Garlic. OpenAI says its newest model bests its rivals ‚Äî especially Google ‚Äî on a suite of the typical benchmarks, including OpenAI‚Äôs homegrown one.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Perhaps one of the most interesting parts of this announcement was the timing. Knowing that the world was awaiting the release of Garlic, Google dropped some AI news of its own.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2022/05/GettyImages-1147600063.jpg?resize=1200,817" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google released on Thursday a ‚Äúreimagined‚Äù version of its research agent Gemini Deep Research based on its much-ballyhooed state-of-the-art foundation model, Gemini 3 Pro.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This new agent isn‚Äôt just designed to produce research reports ‚Äî although it can still do that. It now allows developers to embed Google‚Äôs SATA-model research capabilities into their own apps. That capability is made possible through Google‚Äôs new Interactions API, which is designed to give devs more control in the coming agentic AI era.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The new Gemini Deep Research tool is an agent equipped to synthesize mountains of information and handle a large context dump in the prompt. Google says it‚Äôs used by customers for tasks ranging from due diligence to drug toxicity safety research.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google also says it will soon be integrating this new deep research agent into services, including Google Search, Google Finance, its Gemini App, and its popular NotebookLM. This is another step toward preparing for a world where humans don‚Äôt Google anything anymore ‚Äî their AI agents do.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tech giant says that Deep Research benefits from Gemini 3 Pro‚Äôs status as its ‚Äúmost factual‚Äù model that is trained to minimize hallucinations during complex tasks. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI hallucinations ‚Äî where the LLM just makes stuff up ‚Äî are an especially crucial issue for long-running, deep reasoning agentic tasks, in which many autonomous decisions are made over minutes, hours, or longer. The more choices an LLM has to make, the greater the chance that even one hallucinated choice will invalidate the entire output.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To prove its progress claims, Google has also created yet another benchmark (as if the AI world needs another one). The new benchmark is unimaginatively named DeepSearchQA and is intended to test agents on complex, multi-step information-seeking tasks. Google has open sourced this benchmark.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;It also tested Deep Research on Humanity‚Äôs Last Exam, a much more interestingly named, independent benchmark of general knowledge filled with impossibly niche tasks; and BrowserComp, a benchmark for browser-based agentic tasks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As you might expect, Google‚Äôs new agent bested the competition on its own benchmark, and Humanity‚Äôs. However, OpenAI‚Äôs ChatGPT 5 Pro was a surprisingly close second all the way around and slightly bested Google on BrowserComp.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But those benchmark comparisons were obsolete almost the moment Google published them. Because on the same day, OpenAI launched its highly anticipated GPT 5.2 ‚Äî codenamed Garlic. OpenAI says its newest model bests its rivals ‚Äî especially Google ‚Äî on a suite of the typical benchmarks, including OpenAI‚Äôs homegrown one.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Perhaps one of the most interesting parts of this announcement was the timing. Knowing that the world was awaiting the release of Garlic, Google dropped some AI news of its own.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/11/google-launched-its-deepest-ai-research-agent-yet-on-the-same-day-openai-dropped-gpt-5-2/</guid><pubDate>Fri, 12 Dec 2025 00:18:56 +0000</pubDate></item></channel></rss>