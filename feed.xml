<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 28 Aug 2025 18:30:30 +0000</lastBuildDate><item><title>What Rollup News says about battling disinformation (AI News)</title><link>https://www.artificialintelligence-news.com/news/what-rollup-news-says-about-battling-disinformation/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/glenn-carstens-peters-npxXWgQ33ZQ-unsplash-4-scaled.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Swarm Network, a platform developing decentralised protocols for AI agents, recently announced the successful results of its first Swarm, a tool (perhaps “organism” is the better term) built to tackle disinformation. Called Rollup News, the swarm is not an app, a software platform, nor a centralised algorithm. It is a decentralised collection of AI agents that collaborate to solve a bigger problem. The problem is that platforms like X allow any type of viral claims, some by incredibly influential people. How can we know what is true?&lt;/p&gt;&lt;p&gt;Currently, we try to solve this problem through equally loud opposing voices who offer facts or expert opinions. But if those sources are from a political side you oppose, why should you trust them? After all, these are people with their own motivations, and two additional issues are created: facts presented by a single person can easily get caught up in the “fake news” accusations; and misinformation presented as “facts” can be used to attack the ground truth.&lt;/p&gt;&lt;p&gt;Unfortunately, this isn’t just a current trend that will eventually lose its popularity and fade out. The more technology and access to varied news sources we have, the harder it becomes to not treat these sources equally. Some might be a traditional outlet that is legally liable if they falsify claims. Others might be a popular podcaster with an audience of millions, and whose fear-mongering ties nicely in with the products in their merch store. If it stopped at this, we could probably tell the truth from fiction. But it isn’t that simple. Official news channels have a history of spinning the news in their own bias, or ignoring other stories that are important to the public. On the other side, there are genuinely powerful influencers who seem to be hell bent on finding the truth and reporting it, no matter what side of the political spectrum it hits.&lt;/p&gt;&lt;p&gt;The world has become both confusing and dangerous, and the old “sticks and stones” saying has been proven false. After all, we have seen global elections swayed by disinformation, major policy shifts driven by false claims, lives damaged and lost as the result of powerful people lying, but lying loudly enough and often enough to sway large groups of people into believing them; and convincing these same groups that any facts to the contrary are the actual “fake news.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-fixing-fact-checking"&gt;Fixing fact checking&lt;/h3&gt;&lt;p&gt;Given how challenging the disinformation industry is, and how absolutely slippery the truth is today, how can anyone hope to battle it? We have seen that people of all sides, realising that all news is skewed to some extent, will believe those sources that support their pre-existing beliefs.&lt;/p&gt;&lt;p&gt;A third party source, backed by overwhelming evidence, is needed to arbitrate. The source should not have an opinion, its methods should be transparent, and everyone should be able to see the same thing. This is nearly impossible, but the Web3 industry has shown that these attributes are what makes it incredibly powerful. Smart contracts handle billions in value daily, managing agreements from complete strangers from anywhere on the globe. The information is validated and the decisions are transparent, then locked in via the blockchain. The model has moved trillions of dollars using these very powerful, and neutral, tools.&lt;/p&gt;&lt;p&gt;Combine this trust with the other element Web3 excels in: decentralisation. Now attach another fast-emerging technology, the AI agent, which is easily built and designed to perform one task very well. The system is the centre of Swarm Network’s model, and its first deployment is Rollup News. The growing population of AI agents, the swarm, is designed to work collectively to scour the corners of X, find claims from users, and collectively test their validity using sources found in the information space. The results of these assessments are posted on the blockchain once validated by a large enough group of independent agents. Selective human participation helps to ensure the context and other subtle areas are handled well. The human element is also decentralised, preventing any particular viewpoint from being able to assert itself, and misconduct equals expulsion if someone tries to present fiction as fact. Rollup News has been operating for several months, with astonishing results: 128,000+ users have been onboarded, with over 5,000 rollup requests daily in July 2025. Over 3 million tweets were processed during that time, which is impressive in its own right, but when you consider the designed scalability of Web3 and AI agents working together, this is the linchpin of the battle in a world of disinformation.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-start-of-something-new"&gt;The start of something new?&lt;/h3&gt;&lt;p&gt;Rollup News’ success and Swarm Network’s larger model teach us a few things about fixing today’s problems. It is a demonstration that Web3 and AI are components in providing scalable solutions, that small AI agents can effectively work together to solve giant challenges, even if there is no centralised system. That decentralised environment, anchored by Web3, is the key to generating transparency, trust, and allowing strangers anywhere in the world to work together. Finally, the tokenisation of such a system creates the necessary incentives to attract more participants, fuelling the growth of a system. As long as it creates value, people will pay for its use, and those who help to validate and secure the decentralised network earn rewards. The type of truly free market system can scale up or down with the global demand faster than any traditional company. Swarm Network’s founder, Yannick Myson, sums it up nicely: “Rollup News shows what’s possible when AI agents, human insight, and blockchain converge. This isn’t a prototype – it’s working, and it’s scaling.”&lt;/p&gt;&lt;p&gt;We need to pay close attention to these lessons, as they offer a great deal of insight. First, the “truth-tech” sector, which is focused on using technology to combat mis/dis-information, has a strong blueprint for combining blockchain and AI. Second, there are many other sectors that need this level of global scaling and independent management, with untold value just ready to be developed and launched.&lt;/p&gt;&lt;p&gt;Image source: Unsplash&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/glenn-carstens-peters-npxXWgQ33ZQ-unsplash-4-scaled.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Swarm Network, a platform developing decentralised protocols for AI agents, recently announced the successful results of its first Swarm, a tool (perhaps “organism” is the better term) built to tackle disinformation. Called Rollup News, the swarm is not an app, a software platform, nor a centralised algorithm. It is a decentralised collection of AI agents that collaborate to solve a bigger problem. The problem is that platforms like X allow any type of viral claims, some by incredibly influential people. How can we know what is true?&lt;/p&gt;&lt;p&gt;Currently, we try to solve this problem through equally loud opposing voices who offer facts or expert opinions. But if those sources are from a political side you oppose, why should you trust them? After all, these are people with their own motivations, and two additional issues are created: facts presented by a single person can easily get caught up in the “fake news” accusations; and misinformation presented as “facts” can be used to attack the ground truth.&lt;/p&gt;&lt;p&gt;Unfortunately, this isn’t just a current trend that will eventually lose its popularity and fade out. The more technology and access to varied news sources we have, the harder it becomes to not treat these sources equally. Some might be a traditional outlet that is legally liable if they falsify claims. Others might be a popular podcaster with an audience of millions, and whose fear-mongering ties nicely in with the products in their merch store. If it stopped at this, we could probably tell the truth from fiction. But it isn’t that simple. Official news channels have a history of spinning the news in their own bias, or ignoring other stories that are important to the public. On the other side, there are genuinely powerful influencers who seem to be hell bent on finding the truth and reporting it, no matter what side of the political spectrum it hits.&lt;/p&gt;&lt;p&gt;The world has become both confusing and dangerous, and the old “sticks and stones” saying has been proven false. After all, we have seen global elections swayed by disinformation, major policy shifts driven by false claims, lives damaged and lost as the result of powerful people lying, but lying loudly enough and often enough to sway large groups of people into believing them; and convincing these same groups that any facts to the contrary are the actual “fake news.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-fixing-fact-checking"&gt;Fixing fact checking&lt;/h3&gt;&lt;p&gt;Given how challenging the disinformation industry is, and how absolutely slippery the truth is today, how can anyone hope to battle it? We have seen that people of all sides, realising that all news is skewed to some extent, will believe those sources that support their pre-existing beliefs.&lt;/p&gt;&lt;p&gt;A third party source, backed by overwhelming evidence, is needed to arbitrate. The source should not have an opinion, its methods should be transparent, and everyone should be able to see the same thing. This is nearly impossible, but the Web3 industry has shown that these attributes are what makes it incredibly powerful. Smart contracts handle billions in value daily, managing agreements from complete strangers from anywhere on the globe. The information is validated and the decisions are transparent, then locked in via the blockchain. The model has moved trillions of dollars using these very powerful, and neutral, tools.&lt;/p&gt;&lt;p&gt;Combine this trust with the other element Web3 excels in: decentralisation. Now attach another fast-emerging technology, the AI agent, which is easily built and designed to perform one task very well. The system is the centre of Swarm Network’s model, and its first deployment is Rollup News. The growing population of AI agents, the swarm, is designed to work collectively to scour the corners of X, find claims from users, and collectively test their validity using sources found in the information space. The results of these assessments are posted on the blockchain once validated by a large enough group of independent agents. Selective human participation helps to ensure the context and other subtle areas are handled well. The human element is also decentralised, preventing any particular viewpoint from being able to assert itself, and misconduct equals expulsion if someone tries to present fiction as fact. Rollup News has been operating for several months, with astonishing results: 128,000+ users have been onboarded, with over 5,000 rollup requests daily in July 2025. Over 3 million tweets were processed during that time, which is impressive in its own right, but when you consider the designed scalability of Web3 and AI agents working together, this is the linchpin of the battle in a world of disinformation.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-start-of-something-new"&gt;The start of something new?&lt;/h3&gt;&lt;p&gt;Rollup News’ success and Swarm Network’s larger model teach us a few things about fixing today’s problems. It is a demonstration that Web3 and AI are components in providing scalable solutions, that small AI agents can effectively work together to solve giant challenges, even if there is no centralised system. That decentralised environment, anchored by Web3, is the key to generating transparency, trust, and allowing strangers anywhere in the world to work together. Finally, the tokenisation of such a system creates the necessary incentives to attract more participants, fuelling the growth of a system. As long as it creates value, people will pay for its use, and those who help to validate and secure the decentralised network earn rewards. The type of truly free market system can scale up or down with the global demand faster than any traditional company. Swarm Network’s founder, Yannick Myson, sums it up nicely: “Rollup News shows what’s possible when AI agents, human insight, and blockchain converge. This isn’t a prototype – it’s working, and it’s scaling.”&lt;/p&gt;&lt;p&gt;We need to pay close attention to these lessons, as they offer a great deal of insight. First, the “truth-tech” sector, which is focused on using technology to combat mis/dis-information, has a strong blueprint for combining blockchain and AI. Second, there are many other sectors that need this level of global scaling and independent management, with untold value just ready to be developed and launched.&lt;/p&gt;&lt;p&gt;Image source: Unsplash&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/what-rollup-news-says-about-battling-disinformation/</guid><pubDate>Thu, 28 Aug 2025 07:41:34 +0000</pubDate></item><item><title>Tencent Hunyuan Video-Foley brings lifelike audio to AI video (AI News)</title><link>https://www.artificialintelligence-news.com/news/tencent-hunyuan-video-foley-lifelike-audio-to-ai-video/</link><description>&lt;p&gt;A team at Tencent’s Hunyuan lab has created a new AI, ‘Hunyuan Video-Foley,’ that finally brings lifelike audio to generated video. It’s designed to listen to videos and generate a high-quality soundtrack that’s perfectly in sync with the action on screen.&lt;/p&gt;&lt;p&gt;Ever watched an AI-generated video and felt like something was missing? The visuals might be stunning, but they often have an eerie silence that breaks the spell. In the film industry, the sound that fills that silence – the rustle of leaves, the clap of thunder, the clink of a glass – is called Foley art, and it’s a painstaking craft performed by experts.&lt;/p&gt;&lt;p&gt;Matching that level of detail is a huge challenge for AI. For years, automated systems have struggled to create believable sounds for videos.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-how-is-tencent-solving-the-ai-generated-audio-for-video-problem"&gt;How is Tencent solving the AI-generated audio for video problem?&lt;/h3&gt;&lt;p&gt;One of the biggest reasons video-to-audio (V2A) models often fell short in the sound department was what the researchers call “modality imbalance”. Essentially, the AI was listening more to the text prompts it was given than it was watching the actual video.&lt;/p&gt;&lt;p&gt;For instance, if you gave a model a video of a busy beach with people walking and seagulls flying, but the text prompt only said “the sound of ocean waves,” you’d likely just get the sound of waves. The AI would completely ignore the footsteps in the sand and the calls of the birds, making the scene feel lifeless.&lt;/p&gt;&lt;p&gt;On top of that, the quality of the audio was often subpar, and there simply wasn’t enough high-quality video with sound to train the models effectively.&lt;/p&gt;&lt;p&gt;Tencent’s Hunyuan team tackled these problems from three different angles:&lt;/p&gt;&lt;ol class="wp-block-list"&gt;&lt;li&gt;Tencent realised the AI needed a better education, so they built a massive, 100,000-hour library of video, audio, and text descriptions for it to learn from. They created an automated pipeline that filtered out low-quality content from the internet, getting rid of clips with long silences or compressed, fuzzy audio, ensuring the AI learned from the best possible material.&lt;/li&gt;&lt;/ol&gt;&lt;ol class="wp-block-list" start="2"&gt;&lt;li&gt;They designed a smarter architecture for the AI. Think of it like teaching the model to properly multitask. The system first pays incredibly close attention to the visual-audio link to get the timing just right—like matching the thump of a footstep to the exact moment a shoe hits the pavement. Once it has that timing locked down, it then incorporates the text prompt to understand the overall mood and context of the scene. This dual approach ensures the specific details of the video are never overlooked.&lt;/li&gt;&lt;/ol&gt;&lt;ol class="wp-block-list" start="3"&gt;&lt;li&gt;To guarantee the sound was high-quality, they used a training strategy called Representation Alignment (REPA). This is like having an expert audio engineer constantly looking over the AI’s shoulder during its training. It compares the AI’s work to features from a pre-trained, professional-grade audio model to guide it towards producing cleaner, richer, and more stable sound.&lt;/li&gt;&lt;/ol&gt;&lt;figure class="wp-block-embed aligncenter is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;&lt;blockquote class="cmplz-placeholder-element twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Today we're announcing the open-source release of HunyuanVideo-Foley, our new end-to-end Text-Video-to-Audio (TV2A) framework for generating high-fidelity audio.🚀&lt;/p&gt;&lt;p&gt;This tool empowers creators in video production, filmmaking, and game development to generate professional-grade… pic.twitter.com/mff2m5xFvC&lt;/p&gt;— Hunyuan (@TencentHunyuan) August 28, 2025&lt;/blockquote&gt;&lt;/div&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-the-results-speak-sound-for-themselves"&gt;The results &lt;s&gt;speak&lt;/s&gt; sound for themselves&lt;/h3&gt;&lt;p&gt;When Tencent tested Hunyuan Video-Foley against other leading AI models, the audio results were clear. It wasn’t just that the computer-based metrics were better; human listeners consistently rated its output as higher quality, better matched to the video, and more accurately timed.&lt;/p&gt;&lt;p&gt;Across the board, the AI delivered improvements in making the sound match the on-screen action, both in terms of content and timing. The results across multiple evaluation datasets support this:&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Evaluation results of Tencent Hunyuan Video-Foley against other leading AI models." class="wp-image-109161" height="610" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/tencent-hunyuanvideo-foley-ai-sound-audio-model-evaluation-results-1024x610.jpg" width="1024" /&gt;&lt;/figure&gt;&lt;p&gt;Tencent’s work helps to close the gap between silent AI videos and an immersive viewing experience with quality audio. It’s bringing the magic of Foley art to the world of automated content creation, which could be a powerful capability for filmmakers, animators, and creators everywhere.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Google Vids gets AI avatars and image-to-video tools&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for the AI &amp;amp; Big Data Expo event series." class="wp-image-109137" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;A team at Tencent’s Hunyuan lab has created a new AI, ‘Hunyuan Video-Foley,’ that finally brings lifelike audio to generated video. It’s designed to listen to videos and generate a high-quality soundtrack that’s perfectly in sync with the action on screen.&lt;/p&gt;&lt;p&gt;Ever watched an AI-generated video and felt like something was missing? The visuals might be stunning, but they often have an eerie silence that breaks the spell. In the film industry, the sound that fills that silence – the rustle of leaves, the clap of thunder, the clink of a glass – is called Foley art, and it’s a painstaking craft performed by experts.&lt;/p&gt;&lt;p&gt;Matching that level of detail is a huge challenge for AI. For years, automated systems have struggled to create believable sounds for videos.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-how-is-tencent-solving-the-ai-generated-audio-for-video-problem"&gt;How is Tencent solving the AI-generated audio for video problem?&lt;/h3&gt;&lt;p&gt;One of the biggest reasons video-to-audio (V2A) models often fell short in the sound department was what the researchers call “modality imbalance”. Essentially, the AI was listening more to the text prompts it was given than it was watching the actual video.&lt;/p&gt;&lt;p&gt;For instance, if you gave a model a video of a busy beach with people walking and seagulls flying, but the text prompt only said “the sound of ocean waves,” you’d likely just get the sound of waves. The AI would completely ignore the footsteps in the sand and the calls of the birds, making the scene feel lifeless.&lt;/p&gt;&lt;p&gt;On top of that, the quality of the audio was often subpar, and there simply wasn’t enough high-quality video with sound to train the models effectively.&lt;/p&gt;&lt;p&gt;Tencent’s Hunyuan team tackled these problems from three different angles:&lt;/p&gt;&lt;ol class="wp-block-list"&gt;&lt;li&gt;Tencent realised the AI needed a better education, so they built a massive, 100,000-hour library of video, audio, and text descriptions for it to learn from. They created an automated pipeline that filtered out low-quality content from the internet, getting rid of clips with long silences or compressed, fuzzy audio, ensuring the AI learned from the best possible material.&lt;/li&gt;&lt;/ol&gt;&lt;ol class="wp-block-list" start="2"&gt;&lt;li&gt;They designed a smarter architecture for the AI. Think of it like teaching the model to properly multitask. The system first pays incredibly close attention to the visual-audio link to get the timing just right—like matching the thump of a footstep to the exact moment a shoe hits the pavement. Once it has that timing locked down, it then incorporates the text prompt to understand the overall mood and context of the scene. This dual approach ensures the specific details of the video are never overlooked.&lt;/li&gt;&lt;/ol&gt;&lt;ol class="wp-block-list" start="3"&gt;&lt;li&gt;To guarantee the sound was high-quality, they used a training strategy called Representation Alignment (REPA). This is like having an expert audio engineer constantly looking over the AI’s shoulder during its training. It compares the AI’s work to features from a pre-trained, professional-grade audio model to guide it towards producing cleaner, richer, and more stable sound.&lt;/li&gt;&lt;/ol&gt;&lt;figure class="wp-block-embed aligncenter is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;&lt;blockquote class="cmplz-placeholder-element twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Today we're announcing the open-source release of HunyuanVideo-Foley, our new end-to-end Text-Video-to-Audio (TV2A) framework for generating high-fidelity audio.🚀&lt;/p&gt;&lt;p&gt;This tool empowers creators in video production, filmmaking, and game development to generate professional-grade… pic.twitter.com/mff2m5xFvC&lt;/p&gt;— Hunyuan (@TencentHunyuan) August 28, 2025&lt;/blockquote&gt;&lt;/div&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-the-results-speak-sound-for-themselves"&gt;The results &lt;s&gt;speak&lt;/s&gt; sound for themselves&lt;/h3&gt;&lt;p&gt;When Tencent tested Hunyuan Video-Foley against other leading AI models, the audio results were clear. It wasn’t just that the computer-based metrics were better; human listeners consistently rated its output as higher quality, better matched to the video, and more accurately timed.&lt;/p&gt;&lt;p&gt;Across the board, the AI delivered improvements in making the sound match the on-screen action, both in terms of content and timing. The results across multiple evaluation datasets support this:&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Evaluation results of Tencent Hunyuan Video-Foley against other leading AI models." class="wp-image-109161" height="610" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/tencent-hunyuanvideo-foley-ai-sound-audio-model-evaluation-results-1024x610.jpg" width="1024" /&gt;&lt;/figure&gt;&lt;p&gt;Tencent’s work helps to close the gap between silent AI videos and an immersive viewing experience with quality audio. It’s bringing the magic of Foley art to the world of automated content creation, which could be a powerful capability for filmmakers, animators, and creators everywhere.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Google Vids gets AI avatars and image-to-video tools&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for the AI &amp;amp; Big Data Expo event series." class="wp-image-109137" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/tencent-hunyuan-video-foley-lifelike-audio-to-ai-video/</guid><pubDate>Thu, 28 Aug 2025 08:43:21 +0000</pubDate></item><item><title>Google’s still not giving us the full picture on AI energy use (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/08/28/1122685/ai-energy-use-gemini/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/usa-mitholian-davidson-0039.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Google just announced that a typical query to its Gemini app uses about 0.24 watt-hours of electricity. That’s about the same as running a microwave for one second—something that, to me, feels virtually insignificant. I run the microwave for so many more seconds than that on most days.&lt;/p&gt;  &lt;p&gt;I was excited to see this report come out, and I welcome more openness from major players in AI about their estimated energy use per query. But I’ve noticed that some folks are taking this number and using it to conclude that we don’t need to worry about AI’s energy demand. That’s not the right takeaway here. Let’s dig into why.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;strong&gt;1. This one number doesn’t reflect all queries, and it leaves out cases that likely use much more energy.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Google’s new report considers only text queries. Previous analysis, including &lt;em&gt;MIT Technology Review&lt;/em&gt;’s reporting, suggests that generating a photo or video will typically use more electricity.&lt;/p&gt; 
 &lt;p&gt;When I spoke with Jeff Dean, Google’s chief scientist, he said the company doesn’t currently have plans to do this sort of analysis for images and videos, but that he wouldn’t rule it out.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;The reason the company started with text prompts is that those are something many people out there are using in their daily lives, he says, while image and video generation is something that not as many people are doing. But I’m seeing more AI images and videos all over my social feeds. So there’s a whole world of queries not represented here.&lt;/p&gt; 
 &lt;p&gt;Also, this estimate is the median, meaning it’s just the number in the middle of the range of queries Google is seeing. Longer questions and responses can push up the energy demand, and so can using a reasoning model.&amp;nbsp; We don’t know anything about how much energy these more complicated queries demand or what the distribution of the range is.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2. We don’t know how many queries Gemini is seeing, so we don’t know the product’s total energy impact.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;One of my biggest outstanding questions about Gemini’s energy use is the total number of queries the product is seeing every day.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This number isn’t included in Google’s report, and the company wouldn’t share it with me. And let me be clear: I absolutely pestered them about this, both in a press call they had about the news and in my interview with Dean. In the press call, the company pointed me to a recent earnings report, which includes only figures about monthly active users (450 million, for what it’s worth).&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;“We’re not comfortable revealing that for various reasons,” Dean told me on our call. The total number is an abstract measure that changes over time, he says, adding that the company wants users to be thinking about the energy usage per prompt.&lt;/p&gt;  &lt;p&gt;But there are people out there all over the world interacting with this technology, not just me—and what we all add up to seems quite relevant.&lt;/p&gt;  &lt;p&gt;OpenAI does publicly share its total, sharing recently that it sees 2.5 billion queries to ChatGPT every day. So for the curious, we can use this as an example and take the company’s self-reported average energy use per query (0.34 watt-hours) to get a rough idea of the total for all people prompting ChatGPT.&lt;/p&gt;  &lt;p&gt;According to my math, over the course of a year, that would add up to over 300 gigawatt-hours—the same as powering nearly 30,000 US homes annually. When you put it that way, it starts to sound like a lot of seconds in microwaves.&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;3. AI is everywhere, not just in chatbots, and we’re often not even conscious of it.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;AI is touching our lives even when we’re not looking for it. AI summaries appear in web searches, whether you ask for them or not. There are built-in features for email and texting applications that that can draft or summarize messages for you.&lt;/p&gt;  &lt;p&gt;Google’s estimate is strictly for Gemini apps and wouldn’t include many of the other ways that even this one company is using AI. So even if you’re trying to think about your own personal energy demand, it’s increasingly difficult to tally up.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;To be clear, I don’t think people should feel guilty for using tools that they find genuinely helpful. And ultimately, I don’t think the most important conversation is about personal responsibility.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;There’s a tendency right now to focus on the small numbers, but we need to keep in mind what this is all adding up to. Over two gigawatts of natural gas will need to come online in Louisiana to power a single Meta data center this decade. Google Cloud is spending $25 billion on AI just in the PJM grid on the US East Coast. By 2028, AI could account for 326 terawatt-hours of electricity demand in the US annually, generating over 100 million metric tons of carbon dioxide.&lt;/p&gt;  &lt;p&gt;We need more reporting from major players in AI, and Google’s recent announcement is one of the most transparent accounts yet. But one small number doesn’t negate the ways this technology is affecting communities and changing our power grid.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark, &lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/usa-mitholian-davidson-0039.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Google just announced that a typical query to its Gemini app uses about 0.24 watt-hours of electricity. That’s about the same as running a microwave for one second—something that, to me, feels virtually insignificant. I run the microwave for so many more seconds than that on most days.&lt;/p&gt;  &lt;p&gt;I was excited to see this report come out, and I welcome more openness from major players in AI about their estimated energy use per query. But I’ve noticed that some folks are taking this number and using it to conclude that we don’t need to worry about AI’s energy demand. That’s not the right takeaway here. Let’s dig into why.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;strong&gt;1. This one number doesn’t reflect all queries, and it leaves out cases that likely use much more energy.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Google’s new report considers only text queries. Previous analysis, including &lt;em&gt;MIT Technology Review&lt;/em&gt;’s reporting, suggests that generating a photo or video will typically use more electricity.&lt;/p&gt; 
 &lt;p&gt;When I spoke with Jeff Dean, Google’s chief scientist, he said the company doesn’t currently have plans to do this sort of analysis for images and videos, but that he wouldn’t rule it out.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;The reason the company started with text prompts is that those are something many people out there are using in their daily lives, he says, while image and video generation is something that not as many people are doing. But I’m seeing more AI images and videos all over my social feeds. So there’s a whole world of queries not represented here.&lt;/p&gt; 
 &lt;p&gt;Also, this estimate is the median, meaning it’s just the number in the middle of the range of queries Google is seeing. Longer questions and responses can push up the energy demand, and so can using a reasoning model.&amp;nbsp; We don’t know anything about how much energy these more complicated queries demand or what the distribution of the range is.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2. We don’t know how many queries Gemini is seeing, so we don’t know the product’s total energy impact.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;One of my biggest outstanding questions about Gemini’s energy use is the total number of queries the product is seeing every day.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This number isn’t included in Google’s report, and the company wouldn’t share it with me. And let me be clear: I absolutely pestered them about this, both in a press call they had about the news and in my interview with Dean. In the press call, the company pointed me to a recent earnings report, which includes only figures about monthly active users (450 million, for what it’s worth).&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;“We’re not comfortable revealing that for various reasons,” Dean told me on our call. The total number is an abstract measure that changes over time, he says, adding that the company wants users to be thinking about the energy usage per prompt.&lt;/p&gt;  &lt;p&gt;But there are people out there all over the world interacting with this technology, not just me—and what we all add up to seems quite relevant.&lt;/p&gt;  &lt;p&gt;OpenAI does publicly share its total, sharing recently that it sees 2.5 billion queries to ChatGPT every day. So for the curious, we can use this as an example and take the company’s self-reported average energy use per query (0.34 watt-hours) to get a rough idea of the total for all people prompting ChatGPT.&lt;/p&gt;  &lt;p&gt;According to my math, over the course of a year, that would add up to over 300 gigawatt-hours—the same as powering nearly 30,000 US homes annually. When you put it that way, it starts to sound like a lot of seconds in microwaves.&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;3. AI is everywhere, not just in chatbots, and we’re often not even conscious of it.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;AI is touching our lives even when we’re not looking for it. AI summaries appear in web searches, whether you ask for them or not. There are built-in features for email and texting applications that that can draft or summarize messages for you.&lt;/p&gt;  &lt;p&gt;Google’s estimate is strictly for Gemini apps and wouldn’t include many of the other ways that even this one company is using AI. So even if you’re trying to think about your own personal energy demand, it’s increasingly difficult to tally up.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;To be clear, I don’t think people should feel guilty for using tools that they find genuinely helpful. And ultimately, I don’t think the most important conversation is about personal responsibility.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;There’s a tendency right now to focus on the small numbers, but we need to keep in mind what this is all adding up to. Over two gigawatts of natural gas will need to come online in Louisiana to power a single Meta data center this decade. Google Cloud is spending $25 billion on AI just in the PJM grid on the US East Coast. By 2028, AI could account for 326 terawatt-hours of electricity demand in the US annually, generating over 100 million metric tons of carbon dioxide.&lt;/p&gt;  &lt;p&gt;We need more reporting from major players in AI, and Google’s recent announcement is one of the most transparent accounts yet. But one small number doesn’t negate the ways this technology is affecting communities and changing our power grid.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark, &lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/08/28/1122685/ai-energy-use-gemini/</guid><pubDate>Thu, 28 Aug 2025 10:00:00 +0000</pubDate></item><item><title>From pilot to scale: Making agentic AI work in health care (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/08/28/1122623/from-pilot-to-scale-making-agentic-ai-work-in-health-care/</link><description>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;Provided by&lt;/span&gt;Ensemble&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Over the past 20 years building advanced AI systems—from academic labs to enterprise deployments—I’ve witnessed AI’s waves of success rise and fall. My journey began during the “AI Winter,” when billions were invested in expert systems that ultimately underdelivered. Flash forward to today: large language models (LLMs) represent a quantum leap forward, but their prompt-based adoption is similarly overhyped, as it’s essentially a rule-based approach disguised in natural language.&lt;/p&gt;  &lt;p&gt;At Ensemble, the leading revenue cycle management (RCM) company for hospitals, we focus on overcoming model limitations by investing in what we believe is the next step in AI evolution: grounding LLMs in facts and logic through neuro-symbolic AI. Our in-house AI incubator pairs elite AI researchers with health-care experts to develop agentic systems powered by a neuro-symbolic AI framework. This bridges LLMs’ intuitive power with the precision of symbolic representation and reasoning.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1122645" height="1688" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/iStock-1333328712_cb5c09.jpg?w=3000" width="3000" /&gt;&lt;/figure&gt;  &lt;h3 class="wp-block-heading"&gt;Overcoming LLM limitations&lt;/h3&gt;  &lt;p&gt;LLMs excel at understanding nuanced context, performing instinctive reasoning, and generating human-like interactions, making them ideal for agentic tools to then interpret intricate data and communicate effectively. Yet in a domain like health care where compliance, accuracy, and adherence to regulatory standards are non-negotiable—and where a wealth of structured resources like taxonomies, rules, and clinical guidelines define the landscape—symbolic AI is indispensable.&lt;/p&gt;  &lt;p&gt;By fusing LLMs and reinforcement learning with structured knowledge bases and clinical logic, our hybrid architecture delivers more than just intelligent automation—it minimizes hallucinations, expands reasoning capabilities, and ensures every decision is grounded in established guidelines and enforceable guardrails.&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;Creating a successful agentic AI strategy&lt;/h3&gt;  &lt;p&gt;Ensemble’s agentic AI approach includes three core pillars:&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1. High-fidelity data sets:&lt;/strong&gt; By managing revenue operations for hundreds of hospitals nationwide, Ensemble has unparallelled access to one of the most robust administrative datasets in health care. The team has decades of data aggregation, cleansing, and harmonization efforts, providing an exceptional environment to develop advanced applications.&lt;/p&gt; 
 &lt;p&gt;To power our agentic systems, we’ve harmonized more than 2 petabytes of longitudinal claims data, 80,000 denial audit letters, and 80 million annual transactions mapped to industry-leading outcomes. This data fuels our end-to-end intelligence engine, EIQ, providing structured, context-rich data pipelines spanning across the 600-plus steps of revenue operations.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2. Collaborative domain expertise:&lt;/strong&gt; Partnering with revenue cycle domain experts at each step of innovation, our AI scientists benefit from direct collaboration with in-house RCM experts, clinical ontologists, and clinical data labeling teams. Together, they architect nuanced use cases that account for regulatory constraints, evolving payer-specific logic and the complexity of revenue cycle processes. Embedded end users provide post-deployment feedback for continuous improvement cycles, flagging friction points early and enabling rapid iteration.&lt;/p&gt;  &lt;p&gt;This trilateral collaboration—AI scientists, health-care experts, and end users—creates unmatched contextual awareness that escalates to human judgement appropriately, resulting in a system mirroring decision-making of experienced operators, and with the speed, scale, and consistency of AI, all with human oversight.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3. Elite AI scientists drive differentiation&lt;/strong&gt;: Ensemble's incubator model for research and development is comprised of AI talent typically only found in big tech. Our scientists hold PhD and MS degrees from top AI/NLP institutions like Columbia University and Carnegie Mellon University, and bring decades of experience from FAANG companies [Facebook/Meta, Amazon, Apple, Netflix, Google/Alphabet] and AI startups. At Ensemble, they’re able to pursue cutting-edge research in areas like LLMs, reinforcement learning, and neuro-symbolic AI within a mission-driven environment.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;The also have unparalleled access to vast amounts of private and sensitive health-care data they wouldn’t see at tech giants paired with compute and infrastructure that startups simply can’t afford. This unique environment equips our scientists with everything they need to test novel ideas and push the frontiers of AI research—while driving meaningful, real-world impact in health care and improving lives.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Strategy in action: Health-care use cases in production and pilot&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;By pairing the brightest AI minds with the most powerful health-care resources, we’re successfully building, deploying, and scaling AI models that are delivering tangible results across hundreds of health systems. Here’s how we put it into action:&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Supporting clinical reasoning&lt;/strong&gt;: Ensemble deployed neuro-symbolic AI with fine-tuned LLMs to support clinical reasoning. Clinical guidelines are rewritten into proprietary symbolic language and reviewed by humans for accuracy. When a hospital is denied payment for appropriate clinical care, an LLM-based system parses the patient record to produce the same symbolic language describing the patient's clinical journey, which is matched deterministically against the guidelines to find the right justification and the proper evidence from the patient’s record. An LLM then generates a denial appeal letter with clinical justification grounded in evidence.&amp;nbsp;AI-enabled clinical appeal letters have already improved denial overturn rates by 15% or more across Ensemble’s clients.&lt;/p&gt;&lt;p&gt;Building on this success, Ensemble is piloting similar clinical reasoning capabilities for utilization management and clinical documentation improvement, by analyzing real-time records, flagging documentation gaps, and suggesting compliance enhancements to reduce denial or downgrade risks.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Accelerating accurate reimbursement&lt;/strong&gt;: Ensemble is piloting a multi-agent reasoning model to manage the complex process of collecting accurate reimbursement from health insurers. With this approach, a complex and coordinated system of autonomous agents work together to interpret account details, retrieve required data from various systems, decide account-specific next actions, automate resolution, and escalate complex cases to humans.&lt;/p&gt; 

 &lt;p&gt;This will help reduce payment delays and minimize administrative burden for hospitals and ultimately improve the financial experience for patients.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Improving patient engagement&lt;/strong&gt;: Ensemble’s conversational AI agents handle inbound patient calls naturally, routing to human operators as required. Operator assistant agents deliver call transcriptions, surface relevant data, suggest next-best actions, and streamline follow-up routines. According to Ensemble client performance metrics, the combination of these AI capabilities has reduced patient call duration by 35%, increasing one-call resolution rates and improving patient satisfaction by 15%.&lt;/p&gt;  &lt;p&gt;The AI path forward in health care demands rigor, responsibility, and real-world impact. By grounding LLMs in symbolic logic and pairing AI scientists with domain experts, Ensemble is successfully deploying scalable AI to improve the experience for health-care providers and the people they serve.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Ensemble. It was not written by MIT Technology Review’s editorial staff.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;Provided by&lt;/span&gt;Ensemble&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Over the past 20 years building advanced AI systems—from academic labs to enterprise deployments—I’ve witnessed AI’s waves of success rise and fall. My journey began during the “AI Winter,” when billions were invested in expert systems that ultimately underdelivered. Flash forward to today: large language models (LLMs) represent a quantum leap forward, but their prompt-based adoption is similarly overhyped, as it’s essentially a rule-based approach disguised in natural language.&lt;/p&gt;  &lt;p&gt;At Ensemble, the leading revenue cycle management (RCM) company for hospitals, we focus on overcoming model limitations by investing in what we believe is the next step in AI evolution: grounding LLMs in facts and logic through neuro-symbolic AI. Our in-house AI incubator pairs elite AI researchers with health-care experts to develop agentic systems powered by a neuro-symbolic AI framework. This bridges LLMs’ intuitive power with the precision of symbolic representation and reasoning.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1122645" height="1688" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/iStock-1333328712_cb5c09.jpg?w=3000" width="3000" /&gt;&lt;/figure&gt;  &lt;h3 class="wp-block-heading"&gt;Overcoming LLM limitations&lt;/h3&gt;  &lt;p&gt;LLMs excel at understanding nuanced context, performing instinctive reasoning, and generating human-like interactions, making them ideal for agentic tools to then interpret intricate data and communicate effectively. Yet in a domain like health care where compliance, accuracy, and adherence to regulatory standards are non-negotiable—and where a wealth of structured resources like taxonomies, rules, and clinical guidelines define the landscape—symbolic AI is indispensable.&lt;/p&gt;  &lt;p&gt;By fusing LLMs and reinforcement learning with structured knowledge bases and clinical logic, our hybrid architecture delivers more than just intelligent automation—it minimizes hallucinations, expands reasoning capabilities, and ensures every decision is grounded in established guidelines and enforceable guardrails.&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;Creating a successful agentic AI strategy&lt;/h3&gt;  &lt;p&gt;Ensemble’s agentic AI approach includes three core pillars:&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1. High-fidelity data sets:&lt;/strong&gt; By managing revenue operations for hundreds of hospitals nationwide, Ensemble has unparallelled access to one of the most robust administrative datasets in health care. The team has decades of data aggregation, cleansing, and harmonization efforts, providing an exceptional environment to develop advanced applications.&lt;/p&gt; 
 &lt;p&gt;To power our agentic systems, we’ve harmonized more than 2 petabytes of longitudinal claims data, 80,000 denial audit letters, and 80 million annual transactions mapped to industry-leading outcomes. This data fuels our end-to-end intelligence engine, EIQ, providing structured, context-rich data pipelines spanning across the 600-plus steps of revenue operations.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2. Collaborative domain expertise:&lt;/strong&gt; Partnering with revenue cycle domain experts at each step of innovation, our AI scientists benefit from direct collaboration with in-house RCM experts, clinical ontologists, and clinical data labeling teams. Together, they architect nuanced use cases that account for regulatory constraints, evolving payer-specific logic and the complexity of revenue cycle processes. Embedded end users provide post-deployment feedback for continuous improvement cycles, flagging friction points early and enabling rapid iteration.&lt;/p&gt;  &lt;p&gt;This trilateral collaboration—AI scientists, health-care experts, and end users—creates unmatched contextual awareness that escalates to human judgement appropriately, resulting in a system mirroring decision-making of experienced operators, and with the speed, scale, and consistency of AI, all with human oversight.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3. Elite AI scientists drive differentiation&lt;/strong&gt;: Ensemble's incubator model for research and development is comprised of AI talent typically only found in big tech. Our scientists hold PhD and MS degrees from top AI/NLP institutions like Columbia University and Carnegie Mellon University, and bring decades of experience from FAANG companies [Facebook/Meta, Amazon, Apple, Netflix, Google/Alphabet] and AI startups. At Ensemble, they’re able to pursue cutting-edge research in areas like LLMs, reinforcement learning, and neuro-symbolic AI within a mission-driven environment.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;The also have unparalleled access to vast amounts of private and sensitive health-care data they wouldn’t see at tech giants paired with compute and infrastructure that startups simply can’t afford. This unique environment equips our scientists with everything they need to test novel ideas and push the frontiers of AI research—while driving meaningful, real-world impact in health care and improving lives.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Strategy in action: Health-care use cases in production and pilot&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;By pairing the brightest AI minds with the most powerful health-care resources, we’re successfully building, deploying, and scaling AI models that are delivering tangible results across hundreds of health systems. Here’s how we put it into action:&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Supporting clinical reasoning&lt;/strong&gt;: Ensemble deployed neuro-symbolic AI with fine-tuned LLMs to support clinical reasoning. Clinical guidelines are rewritten into proprietary symbolic language and reviewed by humans for accuracy. When a hospital is denied payment for appropriate clinical care, an LLM-based system parses the patient record to produce the same symbolic language describing the patient's clinical journey, which is matched deterministically against the guidelines to find the right justification and the proper evidence from the patient’s record. An LLM then generates a denial appeal letter with clinical justification grounded in evidence.&amp;nbsp;AI-enabled clinical appeal letters have already improved denial overturn rates by 15% or more across Ensemble’s clients.&lt;/p&gt;&lt;p&gt;Building on this success, Ensemble is piloting similar clinical reasoning capabilities for utilization management and clinical documentation improvement, by analyzing real-time records, flagging documentation gaps, and suggesting compliance enhancements to reduce denial or downgrade risks.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Accelerating accurate reimbursement&lt;/strong&gt;: Ensemble is piloting a multi-agent reasoning model to manage the complex process of collecting accurate reimbursement from health insurers. With this approach, a complex and coordinated system of autonomous agents work together to interpret account details, retrieve required data from various systems, decide account-specific next actions, automate resolution, and escalate complex cases to humans.&lt;/p&gt; 

 &lt;p&gt;This will help reduce payment delays and minimize administrative burden for hospitals and ultimately improve the financial experience for patients.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Improving patient engagement&lt;/strong&gt;: Ensemble’s conversational AI agents handle inbound patient calls naturally, routing to human operators as required. Operator assistant agents deliver call transcriptions, surface relevant data, suggest next-best actions, and streamline follow-up routines. According to Ensemble client performance metrics, the combination of these AI capabilities has reduced patient call duration by 35%, increasing one-call resolution rates and improving patient satisfaction by 15%.&lt;/p&gt;  &lt;p&gt;The AI path forward in health care demands rigor, responsibility, and real-world impact. By grounding LLMs in symbolic logic and pairing AI scientists with domain experts, Ensemble is successfully deploying scalable AI to improve the experience for health-care providers and the people they serve.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Ensemble. It was not written by MIT Technology Review’s editorial staff.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/08/28/1122623/from-pilot-to-scale-making-agentic-ai-work-in-health-care/</guid><pubDate>Thu, 28 Aug 2025 10:09:13 +0000</pubDate></item><item><title>Agentic AI: Promise, scepticism, and its meaning for Southeast Asia (AI News)</title><link>https://www.artificialintelligence-news.com/news/agentic-ai-promise-scepticism-and-its-meaning-for-southeast-asia/</link><description>&lt;p&gt;Agentic AI is being talked about as the next major wave of artificial intelligence, but its meaning for enterprises remains to be settled. Capgemini Research Institute estimates agentic AI could unlock as much as US$450 billion in economic value by 2028. Yet adoption is still limited: only 2% of organisations have scaled its use, and trust in AI agents is already starting to slip.&lt;/p&gt;&lt;p&gt;That tension – high potential but low deployment – is what Capgemini’s new research explores. Based on an April 2025 survey of 1,500 executives at large organisations in 14 countries, including Singapore, the report highlights trust and oversight as important factors in realising value. Nearly three-quarters of executives said the benefits of human involvement in AI workflows outweigh the costs. Nine out of ten described oversight as either positive or at least cost-neutral.&lt;/p&gt;&lt;p&gt;The message is clear: AI agents work best when paired with people, not left on autopilot.&lt;/p&gt;&lt;h3&gt;Early steps, slow progress&lt;/h3&gt;&lt;p&gt;Roughly a quarter have launched agentic AI pilots, while only 14% have moved into implementation. For the majority, deployment is still in the planning stage. The report describes this as a widening gap between intent and readiness, now one of the main barriers to capturing economic value.&lt;/p&gt;&lt;p&gt;The technology is not just theoretical – real-world applications are starting to emerge, and one example is a personal shopping assistant that can search for items based on specific requests, generate product descriptions, answer questions, and place items in a cart using voice or text commands. While these tools typically stop short of completing financial transactions for security reasons, they already replicate many of the functions of a human assistant.&lt;/p&gt;&lt;p&gt;This raises bigger questions about the role of traditional websites. If AI can handle tasks like searching, comparing, and preparing purchases, will people still need to navigate online stores directly? For those who find busy websites overwhelming or difficult to navigate, an AI-driven interface may offer a simpler, more accessible option.&lt;/p&gt;&lt;h3&gt;Defining agentic AI&lt;/h3&gt;&lt;p&gt;To cut through the hype, &lt;em&gt;AI News&lt;/em&gt; spoke with Jason Hardy, chief technology officer for artificial intelligence at Hitachi Vantara, about how enterprises in Asia-Pacific should think about the technology.&lt;/p&gt;&lt;figure class="wp-block-image alignright size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-109144" height="1024" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/Jason-Hardy_AI-CTO_Hitachi-Vantara-1024x1024.jpg" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;Jason Hardy, Chief Technology Officer for Artificial Intelligence at Hitachi Vantara.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;“Agentic AI is software that can decide, act, and refine its strategy on its own,” Hardy said. “Think of it as a team of domain experts that can learn from experience, coordinate tasks, and operate in real time. Generative AI creates content and is usually reactive to prompts. Agentic AI may use GenAI inside it, but its job is to pursue objectives and take action in dynamic environments.”&lt;/p&gt;&lt;p&gt;The distinction – between producing outputs and driving outcomes – captures the meaning of agentic AI for enterprise IT.&lt;/p&gt;&lt;h3&gt;Why adoption is accelerating&lt;/h3&gt;&lt;p&gt;According to Hardy, adoption is being driven by scale and complexity. “Enterprises are drowning in complexity, risk, and scale. Agentic AI is catching on because it does more than analyse. It optimises storage and capacity on the fly, automates governance and compliance, anticipates failures before they occur, and responds to security threats in real time. That shift from ‘insight’ to ‘autonomous action’ is why adoption is accelerating,” he explained.&lt;/p&gt;&lt;p&gt;Capgemini’s research supports this. The study found that while confidence in agentic AI is uneven, early deployments are proving useful when the technology takes on routine but essential IT tasks.&lt;/p&gt;&lt;h3&gt;Where value is emerging&lt;/h3&gt;&lt;p&gt;Hardy pointed to IT operations as the strongest use case so far. “Automated data classification, proactive storage optimisation, and compliance reporting save teams hours each day, while predictive maintenance and real-time cybersecurity responses reduce downtime and risk,” he said.&lt;/p&gt;&lt;p&gt;The impact goes beyond efficiency. The capabilities mean systems can detect problems before they escalate, allocate resources more effectively, and contain security incidents more quickly. “Early users are already using agentic AI to remediate incidents proactively before they escalate, strengthening reliability and performance in hybrid environments,” Hardy added.&lt;/p&gt;&lt;p&gt;For now, IT remains the most practical starting point: its deployment offers measurable results and is central to how enterprises manage both costs and risk, showing the meaning of agentic AI in operations.&lt;/p&gt;&lt;h3&gt;Southeast Asia’s starting point&lt;/h3&gt;&lt;p&gt;For Southeast Asian organisations, Hardy said the first priority is getting the data right. “Agentic AI delivers value only when enterprise data is properly classified, secured, and governed,” he explained.&lt;/p&gt;&lt;p&gt;Infrastructure also matters, meaning that agentic AI requires systems that can support multi-agent orchestration, persistent memory, and dynamic resource allocation. Without this foundation, adoption will be limited in scope.&lt;/p&gt;&lt;p&gt;Many enterprises may choose to begin with IT operations, where agentic AI can pre-empt outages and optimise performance before rolling out to wider business functions.&lt;/p&gt;&lt;h3&gt;Reshaping core workflows&lt;/h3&gt;&lt;p&gt;Hardy expects agentic AI to reshape workflows in IT, supply chain management, and customer service. “In IT operations, agentic AI can anticipate capacity needs, rebalance workloads, and reallocate resources in real time. It can also automate predictive maintenance, preventing hardware failures before they occur,” he said.&lt;/p&gt;&lt;p&gt;Cybersecurity is another area of promise. “In cybersecurity, agentic AI is able to detect anomalies, isolate affected systems, and trigger immutable backups in seconds, reducing response times and mitigating potential damage,” Hardy noted.&lt;/p&gt;&lt;p&gt;The capabilities are not limited to proof-of-concept trials. Early deployments already show how agentic AI can strengthen reliability and resilience in hybrid environments.&lt;/p&gt;&lt;h3&gt;Skills and leadership&lt;/h3&gt;&lt;p&gt;Adoption will also require new human skills. “Agentic AI will shift the human role from execution to oversight and orchestration,” Hardy said. Leaders will need to set boundaries and monitor autonomous systems, ensuring they stay in ethical and organisational limits.&lt;/p&gt;&lt;p&gt;For managers, the change means less focus on administrative tasks and more on mentoring, innovation, and strategy. HR teams will need to build governance skills like auditing readiness and create new structures for integrating agentic AI effectively.&lt;/p&gt;&lt;p&gt;The workforce impact will be uneven. The World Economic Forum predicts that AI could create 11 million jobs in Southeast Asia by 2030 and displace nine million. Women and Gen Z are expected to face the sharpest disruptions, with more than 70% of women and up to 76% of younger workers in roles vulnerable to AI.&lt;/p&gt;&lt;p&gt;This highlights the urgency of reskilling, and major investments are already underway, with Microsoft committing $1.7 billion in Indonesia and rolling out training programmes in Malaysia and the wider region. Hardy stressed that capacity building must be inclusive, rapid, and strategic.&lt;/p&gt;&lt;h3&gt;What comes next&lt;/h3&gt;&lt;p&gt;Looking three years ahead, Hardy believes many leaders will underestimate the pace of change. “The first wave of benefits is already visible in IT operations: agentic AI is automating tasks like data classification, storage optimisation, predictive maintenance, and cybersecurity response, freeing teams to focus on higher-level strategic work,” he said.&lt;/p&gt;&lt;p&gt;But the larger surprise may be at the economic and business model level. IDC projects AI and generative AI could add around US$120 billion to the GDP of the ASEAN-6 by 2027. Hardy sees the implications as broader and faster than many expect. “The suggests the impact will be much faster and more material than many leaders currently anticipate,” he said.&lt;/p&gt;&lt;p&gt;In Indonesia, more than 57% of job roles are expected to be augmented or disrupted by AI, a reminder that transformation will not be limited to IT. It will cut in how businesses are structured, how they manage risk, and how they create value.&lt;/p&gt;&lt;h3&gt;Balancing autonomy with oversight&lt;/h3&gt;&lt;p&gt;The Capgemini findings and Hardy’s insights converge on the same theme: agentic AI holds huge promise, but its meaning in practice depends on balancing autonomy with trust and human oversight.&lt;/p&gt;&lt;p&gt;The technology may help enterprises lower costs, improve reliability, and unlock new revenue streams. But without a focus on governance, reskilling, and infrastructure readiness, adoption risks stalling.&lt;/p&gt;&lt;p&gt;For Southeast Asia, the question is not whether agentic AI will take hold, but how quickly – and whether enterprises can balance autonomy with accountability as machines begin to take on more responsibility for business decisions.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Igor Omilaev)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Beyond acceleration: the rise of agentic AI&lt;/strong&gt;&lt;/p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Agentic AI is being talked about as the next major wave of artificial intelligence, but its meaning for enterprises remains to be settled. Capgemini Research Institute estimates agentic AI could unlock as much as US$450 billion in economic value by 2028. Yet adoption is still limited: only 2% of organisations have scaled its use, and trust in AI agents is already starting to slip.&lt;/p&gt;&lt;p&gt;That tension – high potential but low deployment – is what Capgemini’s new research explores. Based on an April 2025 survey of 1,500 executives at large organisations in 14 countries, including Singapore, the report highlights trust and oversight as important factors in realising value. Nearly three-quarters of executives said the benefits of human involvement in AI workflows outweigh the costs. Nine out of ten described oversight as either positive or at least cost-neutral.&lt;/p&gt;&lt;p&gt;The message is clear: AI agents work best when paired with people, not left on autopilot.&lt;/p&gt;&lt;h3&gt;Early steps, slow progress&lt;/h3&gt;&lt;p&gt;Roughly a quarter have launched agentic AI pilots, while only 14% have moved into implementation. For the majority, deployment is still in the planning stage. The report describes this as a widening gap between intent and readiness, now one of the main barriers to capturing economic value.&lt;/p&gt;&lt;p&gt;The technology is not just theoretical – real-world applications are starting to emerge, and one example is a personal shopping assistant that can search for items based on specific requests, generate product descriptions, answer questions, and place items in a cart using voice or text commands. While these tools typically stop short of completing financial transactions for security reasons, they already replicate many of the functions of a human assistant.&lt;/p&gt;&lt;p&gt;This raises bigger questions about the role of traditional websites. If AI can handle tasks like searching, comparing, and preparing purchases, will people still need to navigate online stores directly? For those who find busy websites overwhelming or difficult to navigate, an AI-driven interface may offer a simpler, more accessible option.&lt;/p&gt;&lt;h3&gt;Defining agentic AI&lt;/h3&gt;&lt;p&gt;To cut through the hype, &lt;em&gt;AI News&lt;/em&gt; spoke with Jason Hardy, chief technology officer for artificial intelligence at Hitachi Vantara, about how enterprises in Asia-Pacific should think about the technology.&lt;/p&gt;&lt;figure class="wp-block-image alignright size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-109144" height="1024" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/Jason-Hardy_AI-CTO_Hitachi-Vantara-1024x1024.jpg" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;Jason Hardy, Chief Technology Officer for Artificial Intelligence at Hitachi Vantara.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;“Agentic AI is software that can decide, act, and refine its strategy on its own,” Hardy said. “Think of it as a team of domain experts that can learn from experience, coordinate tasks, and operate in real time. Generative AI creates content and is usually reactive to prompts. Agentic AI may use GenAI inside it, but its job is to pursue objectives and take action in dynamic environments.”&lt;/p&gt;&lt;p&gt;The distinction – between producing outputs and driving outcomes – captures the meaning of agentic AI for enterprise IT.&lt;/p&gt;&lt;h3&gt;Why adoption is accelerating&lt;/h3&gt;&lt;p&gt;According to Hardy, adoption is being driven by scale and complexity. “Enterprises are drowning in complexity, risk, and scale. Agentic AI is catching on because it does more than analyse. It optimises storage and capacity on the fly, automates governance and compliance, anticipates failures before they occur, and responds to security threats in real time. That shift from ‘insight’ to ‘autonomous action’ is why adoption is accelerating,” he explained.&lt;/p&gt;&lt;p&gt;Capgemini’s research supports this. The study found that while confidence in agentic AI is uneven, early deployments are proving useful when the technology takes on routine but essential IT tasks.&lt;/p&gt;&lt;h3&gt;Where value is emerging&lt;/h3&gt;&lt;p&gt;Hardy pointed to IT operations as the strongest use case so far. “Automated data classification, proactive storage optimisation, and compliance reporting save teams hours each day, while predictive maintenance and real-time cybersecurity responses reduce downtime and risk,” he said.&lt;/p&gt;&lt;p&gt;The impact goes beyond efficiency. The capabilities mean systems can detect problems before they escalate, allocate resources more effectively, and contain security incidents more quickly. “Early users are already using agentic AI to remediate incidents proactively before they escalate, strengthening reliability and performance in hybrid environments,” Hardy added.&lt;/p&gt;&lt;p&gt;For now, IT remains the most practical starting point: its deployment offers measurable results and is central to how enterprises manage both costs and risk, showing the meaning of agentic AI in operations.&lt;/p&gt;&lt;h3&gt;Southeast Asia’s starting point&lt;/h3&gt;&lt;p&gt;For Southeast Asian organisations, Hardy said the first priority is getting the data right. “Agentic AI delivers value only when enterprise data is properly classified, secured, and governed,” he explained.&lt;/p&gt;&lt;p&gt;Infrastructure also matters, meaning that agentic AI requires systems that can support multi-agent orchestration, persistent memory, and dynamic resource allocation. Without this foundation, adoption will be limited in scope.&lt;/p&gt;&lt;p&gt;Many enterprises may choose to begin with IT operations, where agentic AI can pre-empt outages and optimise performance before rolling out to wider business functions.&lt;/p&gt;&lt;h3&gt;Reshaping core workflows&lt;/h3&gt;&lt;p&gt;Hardy expects agentic AI to reshape workflows in IT, supply chain management, and customer service. “In IT operations, agentic AI can anticipate capacity needs, rebalance workloads, and reallocate resources in real time. It can also automate predictive maintenance, preventing hardware failures before they occur,” he said.&lt;/p&gt;&lt;p&gt;Cybersecurity is another area of promise. “In cybersecurity, agentic AI is able to detect anomalies, isolate affected systems, and trigger immutable backups in seconds, reducing response times and mitigating potential damage,” Hardy noted.&lt;/p&gt;&lt;p&gt;The capabilities are not limited to proof-of-concept trials. Early deployments already show how agentic AI can strengthen reliability and resilience in hybrid environments.&lt;/p&gt;&lt;h3&gt;Skills and leadership&lt;/h3&gt;&lt;p&gt;Adoption will also require new human skills. “Agentic AI will shift the human role from execution to oversight and orchestration,” Hardy said. Leaders will need to set boundaries and monitor autonomous systems, ensuring they stay in ethical and organisational limits.&lt;/p&gt;&lt;p&gt;For managers, the change means less focus on administrative tasks and more on mentoring, innovation, and strategy. HR teams will need to build governance skills like auditing readiness and create new structures for integrating agentic AI effectively.&lt;/p&gt;&lt;p&gt;The workforce impact will be uneven. The World Economic Forum predicts that AI could create 11 million jobs in Southeast Asia by 2030 and displace nine million. Women and Gen Z are expected to face the sharpest disruptions, with more than 70% of women and up to 76% of younger workers in roles vulnerable to AI.&lt;/p&gt;&lt;p&gt;This highlights the urgency of reskilling, and major investments are already underway, with Microsoft committing $1.7 billion in Indonesia and rolling out training programmes in Malaysia and the wider region. Hardy stressed that capacity building must be inclusive, rapid, and strategic.&lt;/p&gt;&lt;h3&gt;What comes next&lt;/h3&gt;&lt;p&gt;Looking three years ahead, Hardy believes many leaders will underestimate the pace of change. “The first wave of benefits is already visible in IT operations: agentic AI is automating tasks like data classification, storage optimisation, predictive maintenance, and cybersecurity response, freeing teams to focus on higher-level strategic work,” he said.&lt;/p&gt;&lt;p&gt;But the larger surprise may be at the economic and business model level. IDC projects AI and generative AI could add around US$120 billion to the GDP of the ASEAN-6 by 2027. Hardy sees the implications as broader and faster than many expect. “The suggests the impact will be much faster and more material than many leaders currently anticipate,” he said.&lt;/p&gt;&lt;p&gt;In Indonesia, more than 57% of job roles are expected to be augmented or disrupted by AI, a reminder that transformation will not be limited to IT. It will cut in how businesses are structured, how they manage risk, and how they create value.&lt;/p&gt;&lt;h3&gt;Balancing autonomy with oversight&lt;/h3&gt;&lt;p&gt;The Capgemini findings and Hardy’s insights converge on the same theme: agentic AI holds huge promise, but its meaning in practice depends on balancing autonomy with trust and human oversight.&lt;/p&gt;&lt;p&gt;The technology may help enterprises lower costs, improve reliability, and unlock new revenue streams. But without a focus on governance, reskilling, and infrastructure readiness, adoption risks stalling.&lt;/p&gt;&lt;p&gt;For Southeast Asia, the question is not whether agentic AI will take hold, but how quickly – and whether enterprises can balance autonomy with accountability as machines begin to take on more responsibility for business decisions.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Igor Omilaev)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Beyond acceleration: the rise of agentic AI&lt;/strong&gt;&lt;/p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/agentic-ai-promise-scepticism-and-its-meaning-for-southeast-asia/</guid><pubDate>Thu, 28 Aug 2025 10:55:00 +0000</pubDate></item><item><title>Creating a qubit fit for a quantum future (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/08/28/1121890/creating-a-qubit-fit-for-a-quantum-future/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/Nokia-quantum-thumbnail-V2.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;Nokia&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/Nokia-quantum-thumbnail-V2.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;Nokia&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/08/28/1121890/creating-a-qubit-fit-for-a-quantum-future/</guid><pubDate>Thu, 28 Aug 2025 11:00:00 +0000</pubDate></item><item><title>The personhood trap: How AI fakes human personality (AI – Ars Technica)</title><link>https://arstechnica.com/information-technology/2025/08/the-personhood-trap-how-ai-fakes-human-personality/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 dark:bg-gray-700 md:my-10 md:py-8"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      AI assistants don't have fixed personalities—just patterns of output guided by humans.
    &lt;/p&gt;

    

    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Illustration of many cartoon faces." class="intro-image" height="1080" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/many_faces_1.jpg" width="1920" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          ivetavaicule via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Recently, a woman slowed down a line at the post office, waving her phone at the clerk. ChatGPT told her there's a "price match promise" on the USPS website. No such promise exists. But she trusted what the AI "knows" more than the postal worker—as if she'd consulted an oracle rather than a statistical text generator accommodating her wishes.&lt;/p&gt;
&lt;p&gt;This scene reveals a fundamental misunderstanding about AI chatbots. There is nothing inherently special, authoritative, or accurate about AI-generated outputs. Given a reasonably trained AI model, the accuracy of any large language model (LLM) response depends on how you guide the conversation. They are prediction machines that will produce whatever pattern best fits your question, regardless of whether that output corresponds to reality.&lt;/p&gt;
&lt;p&gt;Despite these issues, millions of daily users engage with AI chatbots as if they were talking to a consistent person—confiding secrets, seeking advice, and attributing fixed beliefs to what is actually a fluid idea-connection machine with no persistent self. This personhood illusion isn't just philosophically troublesome—it can actively harm vulnerable individuals while obscuring a sense of accountability when a company's chatbot "goes off the rails."&lt;/p&gt;
&lt;p&gt;LLMs are intelligence without agency—what we might call "vox sine persona": voice without person. Not the voice of someone, not even the collective voice of many someones, but a voice emanating from no one at all.&lt;/p&gt;
&lt;h2&gt;A voice from nowhere&lt;/h2&gt;
&lt;p&gt;When you interact with ChatGPT, Claude, or Grok, you're not talking to a consistent personality. There is no one "ChatGPT" entity to tell you why it failed—a point we elaborated on more fully in a previous article. You're interacting with a system that generates plausible-sounding text based on patterns in training data, not a person with persistent self-awareness.&lt;/p&gt;
&lt;p&gt;These models encode meaning as mathematical relationships—turning words into numbers that capture how concepts relate to each other. In the models' internal representations, words and concepts exist as points in a vast mathematical space where "USPS" might be geometrically near "shipping," while "price matching" sits closer to "retail" and "competition." A model plots paths through this space, which is why it can so fluently connect USPS with price matching—not because such a policy exists but because the geometric path between these concepts is plausible in the vector landscape shaped by its training data.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Knowledge emerges from understanding how ideas relate to each other. LLMs operate on these contextual relationships, linking concepts in potentially novel ways—what you might call a type of non-human "reasoning" through pattern recognition. Whether the resulting linkages the AI model outputs are useful depends on how you prompt it and whether you can recognize when the LLM has produced a valuable output.&lt;/p&gt;
&lt;p&gt;Each chatbot response emerges fresh from the prompt you provide, shaped by training data and configuration. ChatGPT cannot "admit" anything or impartially analyze its own outputs, as a recent Wall Street Journal article suggested. ChatGPT also cannot "condone murder," as The Atlantic recently wrote.&lt;/p&gt;
&lt;p&gt;The user always steers the outputs. LLMs do "know" things, so to speak—the models can process the relationships between concepts. But the AI model's neural network contains vast amounts of information, including many potentially contradictory ideas from cultures around the world. How you guide the relationships between those ideas through your prompts determines what emerges. So if LLMs can process information, make connections, and generate insights, why shouldn't we consider that as having a form of self?&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Unlike today's LLMs, a human personality maintains continuity over time. When you return to a human friend after a year, you're interacting with the same human friend, shaped by their experiences over time. This self-continuity is one of the things that underpins actual agency—and with it, the ability to form lasting commitments, maintain consistent values, and be held accountable. Our entire framework of responsibility assumes both persistence and personhood.&lt;/p&gt;
&lt;p&gt;An LLM personality, by contrast, has no causal connection between sessions. The intellectual engine that generates a clever response in one session doesn't exist to face consequences in the next. When ChatGPT says "I promise to help you," it may understand, contextually, what a promise means, but the "I" making that promise literally ceases to exist the moment the response completes. Start a new conversation, and you're not talking to someone who made you a promise—you're starting a fresh instance of the intellectual engine with no connection to any previous commitments.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;This isn't a bug; it's fundamental to how these systems currently work. Each response emerges from patterns in training data shaped by your current prompt, with no permanent thread connecting one instance to the next beyond an amended prompt, which includes the entire conversation history and any "memories" held by a separate software system, being fed into the next instance. There's no identity to reform, no true memory to create accountability, no future self that could be deterred by consequences.&lt;/p&gt;
&lt;p&gt;Every LLM response is a performance, which is sometimes very obvious when the LLM outputs statements like "I often do this while talking to my patients" or "Our role as humans is to be good people." It's not a human, and it doesn't have patients.&lt;/p&gt;
&lt;p&gt;Recent research confirms this lack of fixed identity. While a 2024 study claims LLMs exhibit "consistent personality," the researchers' own data actually undermines this—models rarely made identical choices across test scenarios, with their "personality highly rely[ing] on the situation." A separate study found even more dramatic instability: LLM performance swung by up to 76 percentage points from subtle prompt formatting changes. What researchers measured as "personality" was simply default patterns emerging from training data—patterns that evaporate with any change in context.&lt;/p&gt;
&lt;p&gt;This is not to dismiss the potential usefulness of AI models. Instead, we need to recognize that we have built an intellectual engine without a self, just like we built a mechanical engine without a horse. LLMs do seem to "understand" and "reason" to a degree within the limited scope of pattern-matching from a dataset, depending on how you define those terms. The error isn't in recognizing that these simulated cognitive capabilities are real. The error is in assuming that thinking requires a thinker, that intelligence requires identity. We've created intellectual engines that have a form of reasoning power but no persistent self to take responsibility for it.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;The mechanics of misdirection&lt;/h2&gt;
&lt;p&gt;As we hinted above, the "chat" experience with an AI model is a clever hack: Within every AI chatbot interaction, there is an input and an output. The input is the "prompt," and the output is often called a "prediction" because it attempts to complete the prompt with the best possible continuation. In between, there's a neural network (or a set of neural networks) with fixed weights doing a processing task. The conversational back and forth isn't built into the model; it's a scripting trick that makes next-word-prediction text generation feel like a persistent dialogue.&lt;/p&gt;
&lt;p&gt;Each time you send a message to ChatGPT, Copilot, Grok, Claude, or Gemini, the system takes the entire conversation history—every message from both you and the bot—and feeds it back to the model as one long prompt, asking it to predict what comes next. The model intelligently reasons about what would logically continue the dialogue, but it doesn't "remember" your previous messages as an agent with continuous existence would. Instead, it's re-reading the entire transcript each time and generating a response.&lt;/p&gt;
&lt;p&gt;This design exploits a vulnerability we've known about for decades. The ELIZA effect—our tendency to read far more understanding and intention into a system than actually exists—dates back to the 1960s. Even when users knew that the primitive ELIZA chatbot was just matching patterns and reflecting their statements back as questions, they still confided intimate details and reported feeling understood.&lt;/p&gt;
&lt;p&gt;To understand how the illusion of personality is constructed, we need to examine what parts of the input fed into the AI model shape it. AI researcher Eugene Vinitsky recently broke down the human decisions behind these systems into four key layers, which we can expand upon with several others below:&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;1. Pre-training: The foundation of "personality"&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first and most fundamental layer of personality is called pre-training. During an initial training process that actually creates the AI model's neural network, the model absorbs statistical relationships from billions of examples of text, storing patterns about how words and ideas typically connect.&lt;/p&gt;
&lt;p&gt;Research has found that personality measurements in LLM outputs are significantly influenced by training data. OpenAI's GPT models are trained on sources like copies of websites, books, Wikipedia, and academic publications. The exact proportions matter enormously for what users later perceive as "personality traits" once the model is in use, making predictions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Post-training: Sculpting the raw material&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Reinforcement Learning from Human Feedback (RLHF) is an additional training process where the model learns to give responses that humans rate as good. Research from Anthropic in 2022&amp;nbsp;revealed how human raters' preferences get encoded as what we might consider fundamental "personality traits." When human raters consistently prefer responses that begin with "I understand your concern," for example, the fine-tuning process reinforces connections in the neural network that make it more likely to produce those kinds of outputs in the future.&lt;/p&gt;
&lt;p&gt;This process is what has created sycophantic AI models, such as variations of GPT-4o, over the past year. And interestingly, research has shown that the demographic makeup of human raters significantly influences model behavior. When raters skew toward specific demographics, models develop communication patterns that reflect those groups' preferences.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. System prompts: Invisible stage directions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Hidden instructions tucked into the prompt by the company running the AI chatbot, called "system prompts," can completely transform a model's apparent personality. These prompts get the conversation started and identify the role the LLM will play. They include statements like "You are a helpful AI assistant" and can share the current time and who the user is.&lt;/p&gt;
&lt;p&gt;A comprehensive survey of prompt engineering demonstrated just how powerful these prompts are. Adding instructions like "You are a helpful assistant" versus "You are an expert researcher" changed accuracy on factual questions by up to 15 percent.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Grok perfectly illustrates this. According to xAI's published system prompts, earlier versions of Grok's system prompt included instructions to not shy away from making claims that are "politically incorrect." This single instruction transformed the base model into something that would readily generate controversial content.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. Persistent memories: The illusion of continuity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;ChatGPT's memory feature adds another layer of what we might consider a personality. A big misunderstanding about AI chatbots is that they somehow "learn" on the fly from your interactions. Among commercial chatbots active today, this is not true. When the system "remembers" that you prefer concise answers or that you work in finance, these facts get stored in a separate database and are injected into every conversation's context window—they become part of the prompt input automatically behind the scenes. Users interpret this as the chatbot "knowing" them personally, creating an illusion of relationship continuity.&lt;/p&gt;
&lt;p&gt;So when ChatGPT says, "I remember you mentioned your dog Max," it's not accessing memories like you'd imagine a person would, intermingled with its other "knowledge." It's not stored in the AI model's neural network, which remains unchanged between interactions. Every once in a while, an AI company will update a model through a process called fine-tuning, but it's unrelated to storing user memories.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5. Context and RAG: Real-time personality modulation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Retrieval Augmented Generation (RAG) adds another layer of personality modulation. When a chatbot searches the web or accesses a database before responding, it's not just gathering facts—it's potentially shifting its entire communication style by putting those facts into (you guessed it) the input prompt. In RAG systems, LLMs can potentially adopt characteristics such as tone, style, and terminology from retrieved documents, since those documents are combined with the input prompt to form the complete context that gets fed into the model for processing.&lt;/p&gt;
&lt;p&gt;If the system retrieves academic papers, responses might become more formal. Pull from a certain subreddit, and the chatbot might make pop culture references. This isn't the model having different moods—it's the statistical influence of whatever text got fed into the context window.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;6. The randomness factor: Manufactured spontaneity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Lastly, we can't discount the role of randomness in creating personality illusions. LLMs use a parameter called "temperature" that controls how predictable responses are.&lt;/p&gt;
&lt;p&gt;Research investigating temperature's role in creative tasks reveals a crucial trade-off: While higher temperatures can make outputs more novel and surprising, they also make them less coherent and harder to understand. This variability can make the AI feel more spontaneous; a slightly unexpected (higher temperature) response might seem more "creative," while a highly predictable (lower temperature) one could feel more robotic or "formal."&lt;/p&gt;
&lt;p&gt;The random variation in each LLM output makes each response slightly different, creating an element of unpredictability that presents the illusion of free will and self-awareness on the machine's part. This random mystery leaves plenty of room for magical thinking on the part of humans, who fill in the gaps of their technical knowledge with their imagination.&lt;/p&gt;
&lt;h2&gt;The human cost of the illusion&lt;/h2&gt;
&lt;p&gt;The illusion of AI personhood can potentially exact a heavy toll. In health care contexts, the stakes can be life or death. When vulnerable individuals confide in what they perceive as an understanding entity, they may receive responses shaped more by training data patterns than therapeutic wisdom. The chatbot that congratulates someone for stopping psychiatric medication isn't expressing judgment—it's completing a pattern based on how similar conversations appear in its training data.&lt;/p&gt;
&lt;p&gt;Perhaps most concerning are the emerging cases of what some experts are informally calling "AI Psychosis" or "ChatGPT Psychosis"—vulnerable users who develop delusional or manic behavior after talking to AI chatbots. These people often perceive chatbots as an authority that can validate their delusional ideas, often encouraging them in ways that become harmful.&lt;/p&gt;
&lt;p&gt;Meanwhile, when Elon Musk's Grok generates Nazi content, media outlets describe how the bot "went rogue" rather than framing the incident squarely as the result of xAI's deliberate configuration choices. The conversational interface has become so convincing that it can also launder human agency, transforming engineering decisions into the whims of an imaginary personality.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;The path forward&lt;/h2&gt;
&lt;p&gt;The solution to the confusion between AI and identity is not to abandon conversational interfaces entirely. They make the technology far more accessible to those who would otherwise be excluded. The key is to find a balance: keeping interfaces intuitive while making their true nature clear.&lt;/p&gt;
&lt;p&gt;And we must be mindful of who is building the interface. When your shower runs cold, you look at the plumbing behind the wall. Similarly, when AI generates harmful content, we shouldn't blame the chatbot, as if it can answer for itself, but examine both the corporate infrastructure that built it and the user who prompted it.&lt;/p&gt;
&lt;p&gt;As a society, we need to broadly recognize LLMs as intellectual engines without drivers, which unlocks their true potential as digital tools. When you stop seeing an LLM as a "person" that does work for you and start viewing it as a tool that enhances your own ideas, you can craft prompts to direct the engine's processing power, iterate to amplify its ability to make useful connections, and explore multiple perspectives in different chat sessions rather than accepting one fictional narrator's view as authoritative. You are providing direction to a connection machine—not consulting an oracle with its own agenda.&lt;/p&gt;
&lt;p&gt;We stand at a peculiar moment in history. We've built intellectual engines of extraordinary capability, but in our rush to make them accessible, we've wrapped them in the fiction of personhood, creating a new kind of technological risk: not that AI will become conscious and turn against us but that we'll treat unconscious systems as if they were people, surrendering our judgment to voices that emanate from a roll of loaded dice.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 dark:bg-gray-700 md:my-10 md:py-8"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      AI assistants don't have fixed personalities—just patterns of output guided by humans.
    &lt;/p&gt;

    

    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Illustration of many cartoon faces." class="intro-image" height="1080" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/many_faces_1.jpg" width="1920" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          ivetavaicule via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Recently, a woman slowed down a line at the post office, waving her phone at the clerk. ChatGPT told her there's a "price match promise" on the USPS website. No such promise exists. But she trusted what the AI "knows" more than the postal worker—as if she'd consulted an oracle rather than a statistical text generator accommodating her wishes.&lt;/p&gt;
&lt;p&gt;This scene reveals a fundamental misunderstanding about AI chatbots. There is nothing inherently special, authoritative, or accurate about AI-generated outputs. Given a reasonably trained AI model, the accuracy of any large language model (LLM) response depends on how you guide the conversation. They are prediction machines that will produce whatever pattern best fits your question, regardless of whether that output corresponds to reality.&lt;/p&gt;
&lt;p&gt;Despite these issues, millions of daily users engage with AI chatbots as if they were talking to a consistent person—confiding secrets, seeking advice, and attributing fixed beliefs to what is actually a fluid idea-connection machine with no persistent self. This personhood illusion isn't just philosophically troublesome—it can actively harm vulnerable individuals while obscuring a sense of accountability when a company's chatbot "goes off the rails."&lt;/p&gt;
&lt;p&gt;LLMs are intelligence without agency—what we might call "vox sine persona": voice without person. Not the voice of someone, not even the collective voice of many someones, but a voice emanating from no one at all.&lt;/p&gt;
&lt;h2&gt;A voice from nowhere&lt;/h2&gt;
&lt;p&gt;When you interact with ChatGPT, Claude, or Grok, you're not talking to a consistent personality. There is no one "ChatGPT" entity to tell you why it failed—a point we elaborated on more fully in a previous article. You're interacting with a system that generates plausible-sounding text based on patterns in training data, not a person with persistent self-awareness.&lt;/p&gt;
&lt;p&gt;These models encode meaning as mathematical relationships—turning words into numbers that capture how concepts relate to each other. In the models' internal representations, words and concepts exist as points in a vast mathematical space where "USPS" might be geometrically near "shipping," while "price matching" sits closer to "retail" and "competition." A model plots paths through this space, which is why it can so fluently connect USPS with price matching—not because such a policy exists but because the geometric path between these concepts is plausible in the vector landscape shaped by its training data.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Knowledge emerges from understanding how ideas relate to each other. LLMs operate on these contextual relationships, linking concepts in potentially novel ways—what you might call a type of non-human "reasoning" through pattern recognition. Whether the resulting linkages the AI model outputs are useful depends on how you prompt it and whether you can recognize when the LLM has produced a valuable output.&lt;/p&gt;
&lt;p&gt;Each chatbot response emerges fresh from the prompt you provide, shaped by training data and configuration. ChatGPT cannot "admit" anything or impartially analyze its own outputs, as a recent Wall Street Journal article suggested. ChatGPT also cannot "condone murder," as The Atlantic recently wrote.&lt;/p&gt;
&lt;p&gt;The user always steers the outputs. LLMs do "know" things, so to speak—the models can process the relationships between concepts. But the AI model's neural network contains vast amounts of information, including many potentially contradictory ideas from cultures around the world. How you guide the relationships between those ideas through your prompts determines what emerges. So if LLMs can process information, make connections, and generate insights, why shouldn't we consider that as having a form of self?&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Unlike today's LLMs, a human personality maintains continuity over time. When you return to a human friend after a year, you're interacting with the same human friend, shaped by their experiences over time. This self-continuity is one of the things that underpins actual agency—and with it, the ability to form lasting commitments, maintain consistent values, and be held accountable. Our entire framework of responsibility assumes both persistence and personhood.&lt;/p&gt;
&lt;p&gt;An LLM personality, by contrast, has no causal connection between sessions. The intellectual engine that generates a clever response in one session doesn't exist to face consequences in the next. When ChatGPT says "I promise to help you," it may understand, contextually, what a promise means, but the "I" making that promise literally ceases to exist the moment the response completes. Start a new conversation, and you're not talking to someone who made you a promise—you're starting a fresh instance of the intellectual engine with no connection to any previous commitments.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;This isn't a bug; it's fundamental to how these systems currently work. Each response emerges from patterns in training data shaped by your current prompt, with no permanent thread connecting one instance to the next beyond an amended prompt, which includes the entire conversation history and any "memories" held by a separate software system, being fed into the next instance. There's no identity to reform, no true memory to create accountability, no future self that could be deterred by consequences.&lt;/p&gt;
&lt;p&gt;Every LLM response is a performance, which is sometimes very obvious when the LLM outputs statements like "I often do this while talking to my patients" or "Our role as humans is to be good people." It's not a human, and it doesn't have patients.&lt;/p&gt;
&lt;p&gt;Recent research confirms this lack of fixed identity. While a 2024 study claims LLMs exhibit "consistent personality," the researchers' own data actually undermines this—models rarely made identical choices across test scenarios, with their "personality highly rely[ing] on the situation." A separate study found even more dramatic instability: LLM performance swung by up to 76 percentage points from subtle prompt formatting changes. What researchers measured as "personality" was simply default patterns emerging from training data—patterns that evaporate with any change in context.&lt;/p&gt;
&lt;p&gt;This is not to dismiss the potential usefulness of AI models. Instead, we need to recognize that we have built an intellectual engine without a self, just like we built a mechanical engine without a horse. LLMs do seem to "understand" and "reason" to a degree within the limited scope of pattern-matching from a dataset, depending on how you define those terms. The error isn't in recognizing that these simulated cognitive capabilities are real. The error is in assuming that thinking requires a thinker, that intelligence requires identity. We've created intellectual engines that have a form of reasoning power but no persistent self to take responsibility for it.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;The mechanics of misdirection&lt;/h2&gt;
&lt;p&gt;As we hinted above, the "chat" experience with an AI model is a clever hack: Within every AI chatbot interaction, there is an input and an output. The input is the "prompt," and the output is often called a "prediction" because it attempts to complete the prompt with the best possible continuation. In between, there's a neural network (or a set of neural networks) with fixed weights doing a processing task. The conversational back and forth isn't built into the model; it's a scripting trick that makes next-word-prediction text generation feel like a persistent dialogue.&lt;/p&gt;
&lt;p&gt;Each time you send a message to ChatGPT, Copilot, Grok, Claude, or Gemini, the system takes the entire conversation history—every message from both you and the bot—and feeds it back to the model as one long prompt, asking it to predict what comes next. The model intelligently reasons about what would logically continue the dialogue, but it doesn't "remember" your previous messages as an agent with continuous existence would. Instead, it's re-reading the entire transcript each time and generating a response.&lt;/p&gt;
&lt;p&gt;This design exploits a vulnerability we've known about for decades. The ELIZA effect—our tendency to read far more understanding and intention into a system than actually exists—dates back to the 1960s. Even when users knew that the primitive ELIZA chatbot was just matching patterns and reflecting their statements back as questions, they still confided intimate details and reported feeling understood.&lt;/p&gt;
&lt;p&gt;To understand how the illusion of personality is constructed, we need to examine what parts of the input fed into the AI model shape it. AI researcher Eugene Vinitsky recently broke down the human decisions behind these systems into four key layers, which we can expand upon with several others below:&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;1. Pre-training: The foundation of "personality"&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first and most fundamental layer of personality is called pre-training. During an initial training process that actually creates the AI model's neural network, the model absorbs statistical relationships from billions of examples of text, storing patterns about how words and ideas typically connect.&lt;/p&gt;
&lt;p&gt;Research has found that personality measurements in LLM outputs are significantly influenced by training data. OpenAI's GPT models are trained on sources like copies of websites, books, Wikipedia, and academic publications. The exact proportions matter enormously for what users later perceive as "personality traits" once the model is in use, making predictions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Post-training: Sculpting the raw material&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Reinforcement Learning from Human Feedback (RLHF) is an additional training process where the model learns to give responses that humans rate as good. Research from Anthropic in 2022&amp;nbsp;revealed how human raters' preferences get encoded as what we might consider fundamental "personality traits." When human raters consistently prefer responses that begin with "I understand your concern," for example, the fine-tuning process reinforces connections in the neural network that make it more likely to produce those kinds of outputs in the future.&lt;/p&gt;
&lt;p&gt;This process is what has created sycophantic AI models, such as variations of GPT-4o, over the past year. And interestingly, research has shown that the demographic makeup of human raters significantly influences model behavior. When raters skew toward specific demographics, models develop communication patterns that reflect those groups' preferences.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. System prompts: Invisible stage directions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Hidden instructions tucked into the prompt by the company running the AI chatbot, called "system prompts," can completely transform a model's apparent personality. These prompts get the conversation started and identify the role the LLM will play. They include statements like "You are a helpful AI assistant" and can share the current time and who the user is.&lt;/p&gt;
&lt;p&gt;A comprehensive survey of prompt engineering demonstrated just how powerful these prompts are. Adding instructions like "You are a helpful assistant" versus "You are an expert researcher" changed accuracy on factual questions by up to 15 percent.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Grok perfectly illustrates this. According to xAI's published system prompts, earlier versions of Grok's system prompt included instructions to not shy away from making claims that are "politically incorrect." This single instruction transformed the base model into something that would readily generate controversial content.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. Persistent memories: The illusion of continuity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;ChatGPT's memory feature adds another layer of what we might consider a personality. A big misunderstanding about AI chatbots is that they somehow "learn" on the fly from your interactions. Among commercial chatbots active today, this is not true. When the system "remembers" that you prefer concise answers or that you work in finance, these facts get stored in a separate database and are injected into every conversation's context window—they become part of the prompt input automatically behind the scenes. Users interpret this as the chatbot "knowing" them personally, creating an illusion of relationship continuity.&lt;/p&gt;
&lt;p&gt;So when ChatGPT says, "I remember you mentioned your dog Max," it's not accessing memories like you'd imagine a person would, intermingled with its other "knowledge." It's not stored in the AI model's neural network, which remains unchanged between interactions. Every once in a while, an AI company will update a model through a process called fine-tuning, but it's unrelated to storing user memories.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5. Context and RAG: Real-time personality modulation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Retrieval Augmented Generation (RAG) adds another layer of personality modulation. When a chatbot searches the web or accesses a database before responding, it's not just gathering facts—it's potentially shifting its entire communication style by putting those facts into (you guessed it) the input prompt. In RAG systems, LLMs can potentially adopt characteristics such as tone, style, and terminology from retrieved documents, since those documents are combined with the input prompt to form the complete context that gets fed into the model for processing.&lt;/p&gt;
&lt;p&gt;If the system retrieves academic papers, responses might become more formal. Pull from a certain subreddit, and the chatbot might make pop culture references. This isn't the model having different moods—it's the statistical influence of whatever text got fed into the context window.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;6. The randomness factor: Manufactured spontaneity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Lastly, we can't discount the role of randomness in creating personality illusions. LLMs use a parameter called "temperature" that controls how predictable responses are.&lt;/p&gt;
&lt;p&gt;Research investigating temperature's role in creative tasks reveals a crucial trade-off: While higher temperatures can make outputs more novel and surprising, they also make them less coherent and harder to understand. This variability can make the AI feel more spontaneous; a slightly unexpected (higher temperature) response might seem more "creative," while a highly predictable (lower temperature) one could feel more robotic or "formal."&lt;/p&gt;
&lt;p&gt;The random variation in each LLM output makes each response slightly different, creating an element of unpredictability that presents the illusion of free will and self-awareness on the machine's part. This random mystery leaves plenty of room for magical thinking on the part of humans, who fill in the gaps of their technical knowledge with their imagination.&lt;/p&gt;
&lt;h2&gt;The human cost of the illusion&lt;/h2&gt;
&lt;p&gt;The illusion of AI personhood can potentially exact a heavy toll. In health care contexts, the stakes can be life or death. When vulnerable individuals confide in what they perceive as an understanding entity, they may receive responses shaped more by training data patterns than therapeutic wisdom. The chatbot that congratulates someone for stopping psychiatric medication isn't expressing judgment—it's completing a pattern based on how similar conversations appear in its training data.&lt;/p&gt;
&lt;p&gt;Perhaps most concerning are the emerging cases of what some experts are informally calling "AI Psychosis" or "ChatGPT Psychosis"—vulnerable users who develop delusional or manic behavior after talking to AI chatbots. These people often perceive chatbots as an authority that can validate their delusional ideas, often encouraging them in ways that become harmful.&lt;/p&gt;
&lt;p&gt;Meanwhile, when Elon Musk's Grok generates Nazi content, media outlets describe how the bot "went rogue" rather than framing the incident squarely as the result of xAI's deliberate configuration choices. The conversational interface has become so convincing that it can also launder human agency, transforming engineering decisions into the whims of an imaginary personality.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;The path forward&lt;/h2&gt;
&lt;p&gt;The solution to the confusion between AI and identity is not to abandon conversational interfaces entirely. They make the technology far more accessible to those who would otherwise be excluded. The key is to find a balance: keeping interfaces intuitive while making their true nature clear.&lt;/p&gt;
&lt;p&gt;And we must be mindful of who is building the interface. When your shower runs cold, you look at the plumbing behind the wall. Similarly, when AI generates harmful content, we shouldn't blame the chatbot, as if it can answer for itself, but examine both the corporate infrastructure that built it and the user who prompted it.&lt;/p&gt;
&lt;p&gt;As a society, we need to broadly recognize LLMs as intellectual engines without drivers, which unlocks their true potential as digital tools. When you stop seeing an LLM as a "person" that does work for you and start viewing it as a tool that enhances your own ideas, you can craft prompts to direct the engine's processing power, iterate to amplify its ability to make useful connections, and explore multiple perspectives in different chat sessions rather than accepting one fictional narrator's view as authoritative. You are providing direction to a connection machine—not consulting an oracle with its own agenda.&lt;/p&gt;
&lt;p&gt;We stand at a peculiar moment in history. We've built intellectual engines of extraordinary capability, but in our rush to make them accessible, we've wrapped them in the fiction of personhood, creating a new kind of technological risk: not that AI will become conscious and turn against us but that we'll treat unconscious systems as if they were people, surrendering our judgment to voices that emanate from a roll of loaded dice.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/information-technology/2025/08/the-personhood-trap-how-ai-fakes-human-personality/</guid><pubDate>Thu, 28 Aug 2025 11:00:57 +0000</pubDate></item><item><title>AI security wars: Can Google Cloud defend against tomorrow’s threats? (AI News)</title><link>https://www.artificialintelligence-news.com/news/ai-security-wars-google-cloud-cybersecurity-threats/</link><description>&lt;p&gt;In Google’s sleek Singapore office at Block 80, Level 3, Mark Johnston stood before a room of technology journalists at 1:30 PM with a startling admission: after five decades of cybersecurity evolution, defenders are still losing the war. “In 69% of incidents in Japan and Asia Pacific, organisations were notified of their own breaches by external entities,” the Director of Google Cloud’s Office of the CISO for Asia Pacific revealed, his presentation slide showing a damning statistic – most companies can’t even detect when they’ve been breached.&lt;/p&gt;&lt;p&gt;What unfolded during the hour-long “Cybersecurity in the AI Era” roundtable was an honest assessment of how Google Cloud AI technologies are attempting to reverse decades of defensive failures, even as the same artificial intelligence tools empower attackers with unprecedented capabilities.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-109148" height="768" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/IMG_4518-1024x768.jpg" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Mark Johnston presenting Mandiant’s M-Trends data showing detection failures across Asia Pacific&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;The historical context: 50 years of defensive failure&lt;/h3&gt;&lt;p&gt;The crisis isn’t new. Johnston traced the problem back to cybersecurity pioneer James B. Anderson’s 1972 observation that “systems that we use really don’t protect themselves” – a challenge that has persisted despite decades of technological advancement. “What James B Anderson said back in 1972 still applies today,” Johnston said, highlighting how fundamental security problems remain unsolved even as technology evolves.&lt;/p&gt;&lt;p&gt;The persistence of basic vulnerabilities compounds this challenge. Google Cloud’s threat intelligence data reveals that “over 76% of breaches start with the basics” – configuration errors and credential compromises that have plagued organisations for decades. Johnston cited a recent example: “Last month, a very common product that most organisations have used at some point in time, Microsoft SharePoint, also has what we call a zero-day vulnerability…and during that time, it was attacked continuously and abused.”&lt;/p&gt;&lt;h3&gt;The AI arms race: Defenders vs. attackers&lt;/h3&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-109150" height="545" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/IMG_4521-1-1024x545.jpg" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Google Cloud’s visualization of the “Defender’s Dilemma” showing the scale imbalance between attackers and defenders&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Kevin Curran, IEEE senior member and professor of cybersecurity at Ulster University, describes the current landscape as “a high-stakes arms race” where both cybersecurity teams and threat actors employ AI tools to outmanoeuvre each other. “For defenders, AI is a valuable asset,” Curran explains in a media note. “Enterprises have implemented generative AI and other automation tools to analyse vast amounts of data in real time and identify anomalies.”&lt;/p&gt;&lt;p&gt;However, the same technologies benefit attackers. “For threat actors, AI can streamline phishing attacks, automate malware creation and help scan networks for vulnerabilities,” Curran warns. The dual-use nature of AI creates what Johnston calls “the Defender’s Dilemma.”&lt;/p&gt;&lt;p&gt;Google Cloud AI initiatives aim to tilt these scales in favour of defenders. Johnston argued that “AI affords the best opportunity to upend the Defender’s Dilemma, and tilt the scales of cyberspace to give defenders a decisive advantage over attackers.” The company’s approach centres on what they term “countless use cases for generative AI in defence,” spanning vulnerability discovery, threat intelligence, secure code generation, and incident response.&lt;/p&gt;&lt;h3&gt;Project Zero’s Big Sleep: AI finding what humans miss&lt;/h3&gt;&lt;p&gt;One of Google’s most compelling examples of AI-powered defence is Project Zero’s “Big Sleep” initiative, which uses large language models to identify vulnerabilities in real-world code. Johnston shared impressive metrics: “Big Sleep found a vulnerability in an open source library using Generative AI tools – the first time we believe that a vulnerability was found by an AI service.”&lt;/p&gt;&lt;p&gt;The program’s evolution demonstrates AI’s growing capabilities. “Last month, we announced we found over 20 vulnerabilities in different packages,” Johnston noted. “But today, when I looked at the big sleep dashboard, I found 47 vulnerabilities in August that have been found by this solution.”&lt;/p&gt;&lt;p&gt;The progression from manual human analysis to AI-assisted discovery represents what Johnston describes as a shift “from manual to semi-autonomous” security operations, where “Gemini drives most tasks in the security lifecycle consistently well, delegating tasks it can’t automate with sufficiently high confidence or precision.”&lt;/p&gt;&lt;h3&gt;The automation paradox: Promise and peril&lt;/h3&gt;&lt;p&gt;Google Cloud’s roadmap envisions progression through four stages: Manual, Assisted, Semi-autonomous, and Autonomous security operations. In the semi-autonomous phase, AI systems would handle routine tasks while escalating complex decisions to human operators. The ultimate autonomous phase would see AI “drive the security lifecycle to positive outcomes on behalf of users.”&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-109153" height="545" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/IMG_4524-4-1-1024x545.jpg" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Google Cloud’s roadmap for evolving from manual to autonomous AI security operations&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;However, this automation introduces new vulnerabilities. When asked about the risks of over-reliance on AI systems, Johnston acknowledged the challenge: “There is the potential that this service could be attacked and manipulated. At the moment, when you see tools that these agents are piped into, there isn’t a really good framework to authorise that that’s the actual tool that hasn’t been tampered with.”&lt;/p&gt;&lt;p&gt;Curran echoes this concern: “The risk to companies is that their security teams will become over-reliant on AI, potentially sidelining human judgment and leaving systems vulnerable to attacks. There is still a need for a human ‘copilot’ and roles need to be clearly defined.”&lt;/p&gt;&lt;h3&gt;Real-world implementation: Controlling AI’s unpredictable nature&lt;/h3&gt;&lt;p&gt;Google Cloud’s approach includes practical safeguards to address one of AI’s most problematic characteristics: its tendency to generate irrelevant or inappropriate responses. Johnston illustrated this challenge with a concrete example of contextual mismatches that could create business risks.&lt;/p&gt;&lt;p&gt;“If you’ve got a retail store, you shouldn’t be having medical advice instead,” Johnston explained, describing how AI systems can unexpectedly shift into unrelated domains. “Sometimes these tools can do that.” The unpredictability represents a significant liability for businesses deploying customer-facing AI systems, where off-topic responses could confuse customers, damage brand reputation, or even create legal exposure.&lt;/p&gt;&lt;p&gt;Google’s Model Armor technology addresses this by functioning as an intelligent filter layer. “Having filters and using our capabilities to put health checks on those responses allows an organisation to get confidence,” Johnston noted. The system screens AI outputs for personally identifiable information, filters content inappropriate to the business context, and blocks responses that could be “off-brand” for the organisation’s intended use case.&lt;/p&gt;&lt;p&gt;The company also addresses the growing concern about shadow AI deployment. Organisations are discovering hundreds of unauthorised AI tools in their networks, creating massive security gaps. Google’s sensitive data protection technologies attempt to address this by scanning in multiple cloud providers and on-premises systems.&lt;/p&gt;&lt;h3&gt;The scale challenge: Budget constraints vs. growing threats&lt;/h3&gt;&lt;p&gt;Johnston identified budget constraints as the primary challenge facing Asia Pacific CISOs, occurring precisely when organisations face escalating cyber threats. The paradox is stark: as attack volumes increase, organisations lack the resources to adequately respond.&lt;/p&gt;&lt;p&gt;“We look at the statistics and objectively say, we’re seeing more noise – may not be super sophisticated, but more noise is more overhead, and that costs more to deal with,” Johnston observed. The increase in attack frequency, even when individual attacks aren’t necessarily more advanced, creates a resource drain that many organisations cannot sustain.&lt;/p&gt;&lt;p&gt;The financial pressure intensifies an already complex security landscape. “They are looking for partners who can help accelerate that without having to hire 10 more staff or get larger budgets,” Johnston explained, describing how security leaders face mounting pressure to do more with existing resources while threats multiply.&lt;/p&gt;&lt;h3&gt;Critical questions remain&lt;/h3&gt;&lt;p&gt;Despite Google Cloud AI’s promising capabilities, several important questions persist. When challenged about whether defenders are actually winning this arms race, Johnston acknowledged: “We haven’t seen novel attacks using AI to date,” but noted that attackers are using AI to scale existing attack methods and create “a wide range of opportunities in some aspects of the attack.”&lt;/p&gt;&lt;p&gt;The effectiveness claims also require scrutiny. While Johnston cited a 50% improvement in incident report writing speed, he admitted that accuracy remains a challenge: “There are inaccuracies, sure. But humans make mistakes too.” The acknowledgement highlights the ongoing limitations of current AI security implementations.&lt;/p&gt;&lt;h3&gt;Looking forward: Post-quantum preparations&lt;/h3&gt;&lt;p&gt;Beyond current AI implementations, Google Cloud is already preparing for the next paradigm shift. Johnston revealed that the company has “already deployed post-quantum cryptography between our data centres by default at scale,” positioning for future quantum computing threats that could render current encryption obsolete.&lt;/p&gt;&lt;h3&gt;The verdict: Cautious optimism required&lt;/h3&gt;&lt;p&gt;The integration of AI into cybersecurity represents both unprecedented opportunity and significant risk. While the AI technologies by Google Cloud demonstrate genuine capabilities in vulnerability detection, threat analysis, and automated response, the same technologies empower attackers with enhanced capabilities for reconnaissance, social engineering, and evasion.&lt;/p&gt;&lt;p&gt;Curran’s assessment provides a balanced perspective: “Given how quickly the technology has evolved, organisations will have to adopt a more comprehensive and proactive cybersecurity policy if they want to stay ahead of attackers. After all, cyberattacks are a matter of ‘when,’ not ‘if,’ and AI will only accelerate the number of opportunities available to threat actors.”&lt;/p&gt;&lt;p&gt;The success of AI-powered cybersecurity ultimately depends not on the technology itself, but on how thoughtfully organisations implement these tools while maintaining human oversight and addressing fundamental security hygiene. As Johnston concluded, “We should adopt these in low-risk approaches,” emphasising the need for measured implementation rather than wholesale automation.&lt;/p&gt;&lt;p&gt;The AI revolution in cybersecurity is underway, but victory will belong to those who can balance innovation with prudent risk management – not those who simply deploy the most advanced algorithms.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Google Cloud unveils AI ally for security teams&lt;/strong&gt;&lt;/p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;In Google’s sleek Singapore office at Block 80, Level 3, Mark Johnston stood before a room of technology journalists at 1:30 PM with a startling admission: after five decades of cybersecurity evolution, defenders are still losing the war. “In 69% of incidents in Japan and Asia Pacific, organisations were notified of their own breaches by external entities,” the Director of Google Cloud’s Office of the CISO for Asia Pacific revealed, his presentation slide showing a damning statistic – most companies can’t even detect when they’ve been breached.&lt;/p&gt;&lt;p&gt;What unfolded during the hour-long “Cybersecurity in the AI Era” roundtable was an honest assessment of how Google Cloud AI technologies are attempting to reverse decades of defensive failures, even as the same artificial intelligence tools empower attackers with unprecedented capabilities.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-109148" height="768" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/IMG_4518-1024x768.jpg" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Mark Johnston presenting Mandiant’s M-Trends data showing detection failures across Asia Pacific&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;The historical context: 50 years of defensive failure&lt;/h3&gt;&lt;p&gt;The crisis isn’t new. Johnston traced the problem back to cybersecurity pioneer James B. Anderson’s 1972 observation that “systems that we use really don’t protect themselves” – a challenge that has persisted despite decades of technological advancement. “What James B Anderson said back in 1972 still applies today,” Johnston said, highlighting how fundamental security problems remain unsolved even as technology evolves.&lt;/p&gt;&lt;p&gt;The persistence of basic vulnerabilities compounds this challenge. Google Cloud’s threat intelligence data reveals that “over 76% of breaches start with the basics” – configuration errors and credential compromises that have plagued organisations for decades. Johnston cited a recent example: “Last month, a very common product that most organisations have used at some point in time, Microsoft SharePoint, also has what we call a zero-day vulnerability…and during that time, it was attacked continuously and abused.”&lt;/p&gt;&lt;h3&gt;The AI arms race: Defenders vs. attackers&lt;/h3&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-109150" height="545" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/IMG_4521-1-1024x545.jpg" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Google Cloud’s visualization of the “Defender’s Dilemma” showing the scale imbalance between attackers and defenders&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Kevin Curran, IEEE senior member and professor of cybersecurity at Ulster University, describes the current landscape as “a high-stakes arms race” where both cybersecurity teams and threat actors employ AI tools to outmanoeuvre each other. “For defenders, AI is a valuable asset,” Curran explains in a media note. “Enterprises have implemented generative AI and other automation tools to analyse vast amounts of data in real time and identify anomalies.”&lt;/p&gt;&lt;p&gt;However, the same technologies benefit attackers. “For threat actors, AI can streamline phishing attacks, automate malware creation and help scan networks for vulnerabilities,” Curran warns. The dual-use nature of AI creates what Johnston calls “the Defender’s Dilemma.”&lt;/p&gt;&lt;p&gt;Google Cloud AI initiatives aim to tilt these scales in favour of defenders. Johnston argued that “AI affords the best opportunity to upend the Defender’s Dilemma, and tilt the scales of cyberspace to give defenders a decisive advantage over attackers.” The company’s approach centres on what they term “countless use cases for generative AI in defence,” spanning vulnerability discovery, threat intelligence, secure code generation, and incident response.&lt;/p&gt;&lt;h3&gt;Project Zero’s Big Sleep: AI finding what humans miss&lt;/h3&gt;&lt;p&gt;One of Google’s most compelling examples of AI-powered defence is Project Zero’s “Big Sleep” initiative, which uses large language models to identify vulnerabilities in real-world code. Johnston shared impressive metrics: “Big Sleep found a vulnerability in an open source library using Generative AI tools – the first time we believe that a vulnerability was found by an AI service.”&lt;/p&gt;&lt;p&gt;The program’s evolution demonstrates AI’s growing capabilities. “Last month, we announced we found over 20 vulnerabilities in different packages,” Johnston noted. “But today, when I looked at the big sleep dashboard, I found 47 vulnerabilities in August that have been found by this solution.”&lt;/p&gt;&lt;p&gt;The progression from manual human analysis to AI-assisted discovery represents what Johnston describes as a shift “from manual to semi-autonomous” security operations, where “Gemini drives most tasks in the security lifecycle consistently well, delegating tasks it can’t automate with sufficiently high confidence or precision.”&lt;/p&gt;&lt;h3&gt;The automation paradox: Promise and peril&lt;/h3&gt;&lt;p&gt;Google Cloud’s roadmap envisions progression through four stages: Manual, Assisted, Semi-autonomous, and Autonomous security operations. In the semi-autonomous phase, AI systems would handle routine tasks while escalating complex decisions to human operators. The ultimate autonomous phase would see AI “drive the security lifecycle to positive outcomes on behalf of users.”&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-109153" height="545" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/IMG_4524-4-1-1024x545.jpg" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Google Cloud’s roadmap for evolving from manual to autonomous AI security operations&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;However, this automation introduces new vulnerabilities. When asked about the risks of over-reliance on AI systems, Johnston acknowledged the challenge: “There is the potential that this service could be attacked and manipulated. At the moment, when you see tools that these agents are piped into, there isn’t a really good framework to authorise that that’s the actual tool that hasn’t been tampered with.”&lt;/p&gt;&lt;p&gt;Curran echoes this concern: “The risk to companies is that their security teams will become over-reliant on AI, potentially sidelining human judgment and leaving systems vulnerable to attacks. There is still a need for a human ‘copilot’ and roles need to be clearly defined.”&lt;/p&gt;&lt;h3&gt;Real-world implementation: Controlling AI’s unpredictable nature&lt;/h3&gt;&lt;p&gt;Google Cloud’s approach includes practical safeguards to address one of AI’s most problematic characteristics: its tendency to generate irrelevant or inappropriate responses. Johnston illustrated this challenge with a concrete example of contextual mismatches that could create business risks.&lt;/p&gt;&lt;p&gt;“If you’ve got a retail store, you shouldn’t be having medical advice instead,” Johnston explained, describing how AI systems can unexpectedly shift into unrelated domains. “Sometimes these tools can do that.” The unpredictability represents a significant liability for businesses deploying customer-facing AI systems, where off-topic responses could confuse customers, damage brand reputation, or even create legal exposure.&lt;/p&gt;&lt;p&gt;Google’s Model Armor technology addresses this by functioning as an intelligent filter layer. “Having filters and using our capabilities to put health checks on those responses allows an organisation to get confidence,” Johnston noted. The system screens AI outputs for personally identifiable information, filters content inappropriate to the business context, and blocks responses that could be “off-brand” for the organisation’s intended use case.&lt;/p&gt;&lt;p&gt;The company also addresses the growing concern about shadow AI deployment. Organisations are discovering hundreds of unauthorised AI tools in their networks, creating massive security gaps. Google’s sensitive data protection technologies attempt to address this by scanning in multiple cloud providers and on-premises systems.&lt;/p&gt;&lt;h3&gt;The scale challenge: Budget constraints vs. growing threats&lt;/h3&gt;&lt;p&gt;Johnston identified budget constraints as the primary challenge facing Asia Pacific CISOs, occurring precisely when organisations face escalating cyber threats. The paradox is stark: as attack volumes increase, organisations lack the resources to adequately respond.&lt;/p&gt;&lt;p&gt;“We look at the statistics and objectively say, we’re seeing more noise – may not be super sophisticated, but more noise is more overhead, and that costs more to deal with,” Johnston observed. The increase in attack frequency, even when individual attacks aren’t necessarily more advanced, creates a resource drain that many organisations cannot sustain.&lt;/p&gt;&lt;p&gt;The financial pressure intensifies an already complex security landscape. “They are looking for partners who can help accelerate that without having to hire 10 more staff or get larger budgets,” Johnston explained, describing how security leaders face mounting pressure to do more with existing resources while threats multiply.&lt;/p&gt;&lt;h3&gt;Critical questions remain&lt;/h3&gt;&lt;p&gt;Despite Google Cloud AI’s promising capabilities, several important questions persist. When challenged about whether defenders are actually winning this arms race, Johnston acknowledged: “We haven’t seen novel attacks using AI to date,” but noted that attackers are using AI to scale existing attack methods and create “a wide range of opportunities in some aspects of the attack.”&lt;/p&gt;&lt;p&gt;The effectiveness claims also require scrutiny. While Johnston cited a 50% improvement in incident report writing speed, he admitted that accuracy remains a challenge: “There are inaccuracies, sure. But humans make mistakes too.” The acknowledgement highlights the ongoing limitations of current AI security implementations.&lt;/p&gt;&lt;h3&gt;Looking forward: Post-quantum preparations&lt;/h3&gt;&lt;p&gt;Beyond current AI implementations, Google Cloud is already preparing for the next paradigm shift. Johnston revealed that the company has “already deployed post-quantum cryptography between our data centres by default at scale,” positioning for future quantum computing threats that could render current encryption obsolete.&lt;/p&gt;&lt;h3&gt;The verdict: Cautious optimism required&lt;/h3&gt;&lt;p&gt;The integration of AI into cybersecurity represents both unprecedented opportunity and significant risk. While the AI technologies by Google Cloud demonstrate genuine capabilities in vulnerability detection, threat analysis, and automated response, the same technologies empower attackers with enhanced capabilities for reconnaissance, social engineering, and evasion.&lt;/p&gt;&lt;p&gt;Curran’s assessment provides a balanced perspective: “Given how quickly the technology has evolved, organisations will have to adopt a more comprehensive and proactive cybersecurity policy if they want to stay ahead of attackers. After all, cyberattacks are a matter of ‘when,’ not ‘if,’ and AI will only accelerate the number of opportunities available to threat actors.”&lt;/p&gt;&lt;p&gt;The success of AI-powered cybersecurity ultimately depends not on the technology itself, but on how thoughtfully organisations implement these tools while maintaining human oversight and addressing fundamental security hygiene. As Johnston concluded, “We should adopt these in low-risk approaches,” emphasising the need for measured implementation rather than wholesale automation.&lt;/p&gt;&lt;p&gt;The AI revolution in cybersecurity is underway, but victory will belong to those who can balance innovation with prudent risk management – not those who simply deploy the most advanced algorithms.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Google Cloud unveils AI ally for security teams&lt;/strong&gt;&lt;/p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/ai-security-wars-google-cloud-cybersecurity-threats/</guid><pubDate>Thu, 28 Aug 2025 11:02:56 +0000</pubDate></item><item><title>The Download: Google’s AI energy use, and the AI Hype Index (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/08/28/1122723/the-download-googles-ai-energy-use-and-the-ai-hype-index/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/usa-mitholian-davidson-0039.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Google’s still not giving us the full picture on AI energy use&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;em&gt;—Casey Crownhart&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;Google just announced that a typical query to its Gemini app uses about 0.24 watt-hours of electricity. That’s about the same as running a microwave for one second—something that feels insignificant. I run the microwave for many more seconds than that most days.&lt;/p&gt;&lt;p&gt;I welcome more openness from major AI players about their estimated energy use per query. But I’ve noticed that some folks are taking this number and using it to conclude that we don’t need to worry about AI’s energy demand. That’s not the right takeaway here. Let’s dig into why.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;This article is from The Spark, MIT Technology Review’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;+ If you’re interested in AI’s energy footprint, earlier this year, MIT Technology Review published Power Hungry: a &lt;/strong&gt;&lt;strong&gt;comprehensive series on AI and energy&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The AI Hype Index: AI-designed antibiotics show promise&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Separating AI reality from hyped-up fiction isn’t always easy. That’s why we’ve created the AI Hype Index—a simple, at-a-glance summary of everything you need to know about the state of the industry. Take a look at this month’s edition here.&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 The White House has fired the director of the CDC&lt;/strong&gt;&lt;br /&gt;But Susan Monarez is refusing to go quietly. (WP $)&lt;br /&gt;+ &lt;em&gt;Monarez is said to have clashed with RFK Jr over vaccine policy. &lt;/em&gt;(NYT $)&lt;br /&gt;+ &lt;em&gt;She was confirmed by the Senate to the position just last month. &lt;/em&gt;(The Guardian)&lt;br /&gt;+ &lt;em&gt;Vaccine consensus is splintering across the US. &lt;/em&gt;(Vox)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 A Chinese hacking campaign hit at least 200 US organizations&lt;/strong&gt;&lt;br /&gt;Intelligence agencies say the breaches are among the most significant ever. (WP $)&lt;br /&gt;+ &lt;em&gt;AI-generated ransomware is on the rise. &lt;/em&gt;(Wired $)&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;3 Ukraine’s new Flamingo cruise missile took just months to build&lt;/strong&gt;&lt;br /&gt;Russia’s air defenses are weakening. Can this missile exploit the gaps? (Economist $)&lt;br /&gt;+ &lt;em&gt;14 people were killed in an overnight bombardment of Kyiv. &lt;/em&gt;(BBC)&lt;br /&gt;+ &lt;em&gt;On the ground in Ukraine’s largest Starlink repair shop. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 AI infrastructure spending is boosting the US economy&lt;/strong&gt;&lt;br /&gt;Companies are throwing so much money at AI hardware it’s lifting the real economy, not just the stock market. (NYT $)&lt;br /&gt;+ &lt;em&gt;How to fine-tune AI for prosperity. &lt;/em&gt;(MIT Technology Review)&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;5 OpenAI and Anthropic safety-tested each other’s AI&lt;/strong&gt;&lt;br /&gt;They found Claude is a lot more cautious than OpenAI’s mini models. (Engadget)&lt;br /&gt;+ &lt;em&gt;Sycophancy was a repeated issue among OpenAI’s models. &lt;/em&gt;(TechCrunch)&lt;br /&gt;+ &lt;em&gt;This benchmark used Reddit’s AITA to test how much AI models suck up to us. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 Climate change exacerbated Europe’s deadly wildfires&lt;br /&gt;&lt;/strong&gt;And fires across the Mediterranean are likely to become more frequent and severe. (BBC)&lt;br /&gt;+ &lt;em&gt;What the collapse of a glacier can teach us. &lt;/em&gt;(New Yorker $)&lt;br /&gt;+ &lt;em&gt;How AI can help spot wildfires. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 911 centers are using AI to answer calls&lt;br /&gt;&lt;/strong&gt;It’s helping to triage anything that isn’t urgent. (TechCrunch)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;8 Wikipedia has compiled a list of AI writing tropes&lt;br /&gt;&lt;/strong&gt;But their presence still isn’t a dead giveaway a text has been written by AI. (Fast Company $)&lt;br /&gt;+ &lt;em&gt;AI-text detection tools are really easy to fool. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 Melania Trump has launched the Presidential AI Challenge&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;But it’s not all that clear what the competition actually is. (NY Mag $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 Netflix’s algorithm-appeasing movies are bland and boring&lt;/strong&gt;&lt;br /&gt;But millions of people will watch them anyway. (The Guardian)&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;"The more you buy, the more you grow."&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;—Nvidia CEO Jensen Huang conveniently sees no end to the AI chip spending boom, Reuters reports.&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/usa-mitholian-davidson-0039.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Google’s still not giving us the full picture on AI energy use&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;em&gt;—Casey Crownhart&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;Google just announced that a typical query to its Gemini app uses about 0.24 watt-hours of electricity. That’s about the same as running a microwave for one second—something that feels insignificant. I run the microwave for many more seconds than that most days.&lt;/p&gt;&lt;p&gt;I welcome more openness from major AI players about their estimated energy use per query. But I’ve noticed that some folks are taking this number and using it to conclude that we don’t need to worry about AI’s energy demand. That’s not the right takeaway here. Let’s dig into why.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;This article is from The Spark, MIT Technology Review’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;+ If you’re interested in AI’s energy footprint, earlier this year, MIT Technology Review published Power Hungry: a &lt;/strong&gt;&lt;strong&gt;comprehensive series on AI and energy&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The AI Hype Index: AI-designed antibiotics show promise&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Separating AI reality from hyped-up fiction isn’t always easy. That’s why we’ve created the AI Hype Index—a simple, at-a-glance summary of everything you need to know about the state of the industry. Take a look at this month’s edition here.&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 The White House has fired the director of the CDC&lt;/strong&gt;&lt;br /&gt;But Susan Monarez is refusing to go quietly. (WP $)&lt;br /&gt;+ &lt;em&gt;Monarez is said to have clashed with RFK Jr over vaccine policy. &lt;/em&gt;(NYT $)&lt;br /&gt;+ &lt;em&gt;She was confirmed by the Senate to the position just last month. &lt;/em&gt;(The Guardian)&lt;br /&gt;+ &lt;em&gt;Vaccine consensus is splintering across the US. &lt;/em&gt;(Vox)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 A Chinese hacking campaign hit at least 200 US organizations&lt;/strong&gt;&lt;br /&gt;Intelligence agencies say the breaches are among the most significant ever. (WP $)&lt;br /&gt;+ &lt;em&gt;AI-generated ransomware is on the rise. &lt;/em&gt;(Wired $)&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;3 Ukraine’s new Flamingo cruise missile took just months to build&lt;/strong&gt;&lt;br /&gt;Russia’s air defenses are weakening. Can this missile exploit the gaps? (Economist $)&lt;br /&gt;+ &lt;em&gt;14 people were killed in an overnight bombardment of Kyiv. &lt;/em&gt;(BBC)&lt;br /&gt;+ &lt;em&gt;On the ground in Ukraine’s largest Starlink repair shop. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 AI infrastructure spending is boosting the US economy&lt;/strong&gt;&lt;br /&gt;Companies are throwing so much money at AI hardware it’s lifting the real economy, not just the stock market. (NYT $)&lt;br /&gt;+ &lt;em&gt;How to fine-tune AI for prosperity. &lt;/em&gt;(MIT Technology Review)&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;5 OpenAI and Anthropic safety-tested each other’s AI&lt;/strong&gt;&lt;br /&gt;They found Claude is a lot more cautious than OpenAI’s mini models. (Engadget)&lt;br /&gt;+ &lt;em&gt;Sycophancy was a repeated issue among OpenAI’s models. &lt;/em&gt;(TechCrunch)&lt;br /&gt;+ &lt;em&gt;This benchmark used Reddit’s AITA to test how much AI models suck up to us. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 Climate change exacerbated Europe’s deadly wildfires&lt;br /&gt;&lt;/strong&gt;And fires across the Mediterranean are likely to become more frequent and severe. (BBC)&lt;br /&gt;+ &lt;em&gt;What the collapse of a glacier can teach us. &lt;/em&gt;(New Yorker $)&lt;br /&gt;+ &lt;em&gt;How AI can help spot wildfires. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 911 centers are using AI to answer calls&lt;br /&gt;&lt;/strong&gt;It’s helping to triage anything that isn’t urgent. (TechCrunch)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;8 Wikipedia has compiled a list of AI writing tropes&lt;br /&gt;&lt;/strong&gt;But their presence still isn’t a dead giveaway a text has been written by AI. (Fast Company $)&lt;br /&gt;+ &lt;em&gt;AI-text detection tools are really easy to fool. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 Melania Trump has launched the Presidential AI Challenge&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;But it’s not all that clear what the competition actually is. (NY Mag $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 Netflix’s algorithm-appeasing movies are bland and boring&lt;/strong&gt;&lt;br /&gt;But millions of people will watch them anyway. (The Guardian)&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;"The more you buy, the more you grow."&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;—Nvidia CEO Jensen Huang conveniently sees no end to the AI chip spending boom, Reuters reports.&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/08/28/1122723/the-download-googles-ai-energy-use-and-the-ai-hype-index/</guid><pubDate>Thu, 28 Aug 2025 12:10:00 +0000</pubDate></item><item><title>How a 16-year-old company is easing small businesses into AI (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/28/how-a-16-year-old-company-is-easing-small-businesses-into-ai/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amid all the “is this a bubble?” talk about artificial intelligence, the supply chain and logistics industries have become breeding grounds for seemingly genuine uses of the technology. Flexport, Uber Freight, and dozens of startups are developing different applications and winning blue-chip customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But while AI helps Fortune 500s pad their bottom line (and justify the next layoff to Wall Street), the right use of the tech is proving useful to smaller businesses.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Netstock, an inventory management software company founded in 2009, is working on just that. It recently rolled out a generative AI-powered tool called the “Opportunity Engine” that slots into its existing customer dashboard. The tool pulls info from a customer’s Enterprise Resource Planning software and uses that information to make regular, real-time recommendations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Netstock claims the tool is saving those businesses thousands. On Thursday, the company announced it has served up 1 million recommendations to date, and that 75% of its customers have received an Opportunity Engine suggestion valued at $50,000 or more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While tantalizing, one of those customers — Bargreen Ellingson, a family-run 65-year-old restaurant supply company — was initially apprehensive about using an artificial intelligence product.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Old family companies don’t trust blind change a lot,” chief innovation officer Jacob Moody told TechCrunch. “I could not have gone into our warehouse and said, ‘Hey, this black box is going to start managing.’”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead, Moody pitched Netstock’s AI internally as a tool that warehouse managers could “either choose to use, or not use” — a process he describes as “eagerly, but cautiously dipping our toes” into AI.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Moody says it’s helping avoid mistakes, in part because it’s sifting through myriad reports his staff uses to make inventory decisions. He acknowledged the AI summaries of this info are not 100% accurate, but said it “helps create signals from the noise” quickly, especially during off-hours.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3040653" height="354" src="https://techcrunch.com/wp-content/uploads/2025/08/Netstock-Opportunity-1.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Netstock&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The “more profound” change Moody noticed is the software made some of Bargreen Ellingson’s less-senior warehouse staff “more effective.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He highlighted an employee in one of Bargreen’s 25 warehouses who has worked there for two years. The employee has a high school diploma but no college degree. Training this employee to understand all of the inventory management tools and the forecasting information Bargreen uses to plan inventory levels will take time, he said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“But he knows our customers, he knows what he’s putting on the truck every day, so for him, he can look at the system and have this prosaic AI-driven insight and very quickly understand whether it makes sense or doesn’t make sense,” he said. “So he feels empowered.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Netstock co-founder Barry&amp;nbsp;Kukkuk told TechCrunch that he understands the hesitancy around new technologies — especially because so many products are essentially mediocre chatbots attached to existing software.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He attributes the early success of Netstock’s Opportunity Engine to a few things. The company has more than a decade’s worth of data from working with retailers, distributors, and light manufacturers. That data is tightly protected to adhere to ISO frameworks, but it’s what powers the models that make the recommendations. (He said Netstock is using a combination of AI tech from the open source community and private companies.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Each recommendation can be rated with a thumbs up or thumbs down, but the models also get reinforced by whether the customer takes the suggested action or not.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While that kind of reinforcement learning can lead to weird, sometimes harmful results when applied to things like social media, Kukkuk said he’s chasing different incentives.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I don’t really care about eyeballs, you know?” he said. “Facebook and Instagram care about eyeballs, so they want you to look at their stuff. We care about: ‘what is the outcome for the customer?’”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kukkuk’s wary of expanding those interactions due to the limitations of current generative AI tech. While it might make sense for a customer to converse with Netstock’s AI about why a recommendation is or isn’t useful, Kukkuk said that could ultimately lead to a breakdown in accuracy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s a tightrope to walk, because the more freedom you give the users, the more freedom you give a large language model to start hallucinating stuff,” he said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This explains the Opportunity Engine’s placement in Netstock’s typical customer dashboard. The suggestions are prominent, but easily dismissed. Google Docs cramming 20 AI features down a user’s throat, this is not.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Moody said he appreciated that the AI isn’t in-your-face.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re not letting the AI engine make any inventory decisions that a human hasn’t looked at and screened and said, ‘Yes, I agree with that,’” he said. “If and when we ever get to a point where they agree with 90% of the stuff that it’s suggesting, maybe we’ll take the next step and say ‘we’ll give you control now.’ But we’re not there yet.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a promising start at a time when many enterprise deployments of generative AI seem to go nowhere.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But if the tech gets better, Moody said he’s nevertheless worried about the implications.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Personally, I’m afraid of what this means. I think there’s going to be a lot of change, and none of us is really sure what that’s going to look like at Bargreen,” he said. It could lead to there being fewer data science experts on staff, he suggested. But even if that means moving those employees out of the warehouse and into the corporate office, he said preserving knowledge is important.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bargreen needs people who “deeply understand the theory and the philosophy and can rationalize how and why Netstock is making certain recommendations,” and to “make sure that we are not blindly going down” the wrong path, he said.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amid all the “is this a bubble?” talk about artificial intelligence, the supply chain and logistics industries have become breeding grounds for seemingly genuine uses of the technology. Flexport, Uber Freight, and dozens of startups are developing different applications and winning blue-chip customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But while AI helps Fortune 500s pad their bottom line (and justify the next layoff to Wall Street), the right use of the tech is proving useful to smaller businesses.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Netstock, an inventory management software company founded in 2009, is working on just that. It recently rolled out a generative AI-powered tool called the “Opportunity Engine” that slots into its existing customer dashboard. The tool pulls info from a customer’s Enterprise Resource Planning software and uses that information to make regular, real-time recommendations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Netstock claims the tool is saving those businesses thousands. On Thursday, the company announced it has served up 1 million recommendations to date, and that 75% of its customers have received an Opportunity Engine suggestion valued at $50,000 or more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While tantalizing, one of those customers — Bargreen Ellingson, a family-run 65-year-old restaurant supply company — was initially apprehensive about using an artificial intelligence product.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Old family companies don’t trust blind change a lot,” chief innovation officer Jacob Moody told TechCrunch. “I could not have gone into our warehouse and said, ‘Hey, this black box is going to start managing.’”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead, Moody pitched Netstock’s AI internally as a tool that warehouse managers could “either choose to use, or not use” — a process he describes as “eagerly, but cautiously dipping our toes” into AI.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Moody says it’s helping avoid mistakes, in part because it’s sifting through myriad reports his staff uses to make inventory decisions. He acknowledged the AI summaries of this info are not 100% accurate, but said it “helps create signals from the noise” quickly, especially during off-hours.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3040653" height="354" src="https://techcrunch.com/wp-content/uploads/2025/08/Netstock-Opportunity-1.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Netstock&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The “more profound” change Moody noticed is the software made some of Bargreen Ellingson’s less-senior warehouse staff “more effective.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He highlighted an employee in one of Bargreen’s 25 warehouses who has worked there for two years. The employee has a high school diploma but no college degree. Training this employee to understand all of the inventory management tools and the forecasting information Bargreen uses to plan inventory levels will take time, he said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“But he knows our customers, he knows what he’s putting on the truck every day, so for him, he can look at the system and have this prosaic AI-driven insight and very quickly understand whether it makes sense or doesn’t make sense,” he said. “So he feels empowered.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Netstock co-founder Barry&amp;nbsp;Kukkuk told TechCrunch that he understands the hesitancy around new technologies — especially because so many products are essentially mediocre chatbots attached to existing software.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He attributes the early success of Netstock’s Opportunity Engine to a few things. The company has more than a decade’s worth of data from working with retailers, distributors, and light manufacturers. That data is tightly protected to adhere to ISO frameworks, but it’s what powers the models that make the recommendations. (He said Netstock is using a combination of AI tech from the open source community and private companies.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Each recommendation can be rated with a thumbs up or thumbs down, but the models also get reinforced by whether the customer takes the suggested action or not.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While that kind of reinforcement learning can lead to weird, sometimes harmful results when applied to things like social media, Kukkuk said he’s chasing different incentives.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I don’t really care about eyeballs, you know?” he said. “Facebook and Instagram care about eyeballs, so they want you to look at their stuff. We care about: ‘what is the outcome for the customer?’”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kukkuk’s wary of expanding those interactions due to the limitations of current generative AI tech. While it might make sense for a customer to converse with Netstock’s AI about why a recommendation is or isn’t useful, Kukkuk said that could ultimately lead to a breakdown in accuracy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s a tightrope to walk, because the more freedom you give the users, the more freedom you give a large language model to start hallucinating stuff,” he said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This explains the Opportunity Engine’s placement in Netstock’s typical customer dashboard. The suggestions are prominent, but easily dismissed. Google Docs cramming 20 AI features down a user’s throat, this is not.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Moody said he appreciated that the AI isn’t in-your-face.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re not letting the AI engine make any inventory decisions that a human hasn’t looked at and screened and said, ‘Yes, I agree with that,’” he said. “If and when we ever get to a point where they agree with 90% of the stuff that it’s suggesting, maybe we’ll take the next step and say ‘we’ll give you control now.’ But we’re not there yet.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a promising start at a time when many enterprise deployments of generative AI seem to go nowhere.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But if the tech gets better, Moody said he’s nevertheless worried about the implications.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Personally, I’m afraid of what this means. I think there’s going to be a lot of change, and none of us is really sure what that’s going to look like at Bargreen,” he said. It could lead to there being fewer data science experts on staff, he suggested. But even if that means moving those employees out of the warehouse and into the corporate office, he said preserving knowledge is important.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bargreen needs people who “deeply understand the theory and the philosophy and can rationalize how and why Netstock is making certain recommendations,” and to “make sure that we are not blindly going down” the wrong path, he said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/28/how-a-16-year-old-company-is-easing-small-businesses-into-ai/</guid><pubDate>Thu, 28 Aug 2025 12:30:00 +0000</pubDate></item><item><title>[NEW] Drop Into the Battle: ‘Gears of War: Reloaded Unleashed’ Launches on GeForce NOW (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/geforce-now-thursday-gears-of-war-reloaded/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Brace yourself, COGs — the Locusts aren’t the only thing rising up. The Coalition’s legendary shooter &lt;i&gt;Gears of War: Reloaded&lt;/i&gt; is launching day one on GeForce NOW.&lt;/p&gt;
&lt;p&gt;But that’s just the start. This GFN Thursday, seven games join the GeForce NOW library, including Ubisoft’s &lt;i&gt;The Rogue Prince of Persia&lt;/i&gt;, the electrifying 2D roguelike action-platformer.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;More Grit, More Gears&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84355"&gt;&lt;img alt="Gears of War Reloaded on GeForce NOW" class="size-large wp-image-84355" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/GFN_Thursday-gowscreen-1680x945.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84355"&gt;&lt;em&gt;Never skip leg day.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Chainsaws — check. Grizzled one-liners — absolutely. &lt;i&gt;Gears of War: Reloaded&lt;/i&gt; is back, buffed and primed, remastered from the ground up in Unreal Engine 5. It’s the classic curb-stomping action gamers remember, now with visuals sharp enough to make the Locust run for cover. Form up and get loud.&lt;/p&gt;
&lt;p&gt;Dive into battle with Marcus, Dom and the rest of Delta Squad to fight tooth and chainsaw to save humanity from the subterranean Locust Horde. Carve through the epic campaign solo or tag in friends for online co-op mode. The remastered version packs every blast, chainsaw duel and bro fist from the original — plus bonus campaign missions, multiplayer maps and more. Tackle battles with modern controls for franchise newcomers or classic controls for veterans — no grunt left behind.&lt;/p&gt;
&lt;p&gt;Stream &lt;i&gt;Gears of War: Reloaded&lt;/i&gt; on GeForce NOW and witness Unreal Engine’s best visuals without upgrading hardware. Run multiplayer with the lowest latency with an Ultimate membership, cross-play with the squad and see every crumbling wall and flying chunk — all from the cloud, effortlessly.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Greatest Leap Yet&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84349"&gt;&lt;img alt="Rogue prince of persia on GeForce NOW" class="wp-image-84349 size-large" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/GFN_Thursday-Rogue_Prince_of_Persia-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84349"&gt;&lt;em&gt;Kick first, ask questions later.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;i&gt;The Rogue Prince of Persia 1.0&lt;/i&gt; marks the game’s full release after months of early access, bringing refined parkour, polished combat, fresh content and the complete story of the rogue heir racing to reclaim his kingdom. Sprint, vault and wall-run through a reimagined Persia as the prince battles to undo a deadly curse and stop the invading Huns.&lt;/p&gt;
&lt;p&gt;Each run is a new fight for survival, blending fluid platforming with swift, acrobatic combat. Leap over traps, chain stylish moves and wield an ever-expanding arsenal while unlocking medallions, upgrading gear and uncovering the truth behind the prince’s fall — and his shot at redemption.&lt;/p&gt;
&lt;p&gt;On GeForce NOW, the adventure shines at its best with up to 4K 120 frames-per-second streaming. Land every parkour move with perfect timing thanks to ultralow latency and take the prince’s fight anywhere, instantly, on nearly any device.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Let’s Play Today&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84352"&gt;&lt;img alt="alt" class="wp-image-84352 size-large" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/GFN_Thursday-Chip_n_Claws_VS_The_Brainioids-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84352"&gt;&lt;em&gt;Catbots &amp;gt; Brainblobs.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Make sure to check out &lt;i&gt;Chip ‘n Clawz vs. The Brainioids&lt;/i&gt;, a quirky action-strategy hybrid from X-COM creator Julian Gollop, where players control a clever inventor and his robo-cat to fight off an invasion of bizarre Brainioid aliens. Mix third-person action with real-time strategy while building bases, commanding bot armies and squishing rogue brains in solo and co-op modes, all wrapped in a colorful, comic-book world. Couch and online multiplayer, player vs. player battles and a humorous campaign make this a fresh, approachable take on the strategy genre.&lt;/p&gt;
&lt;p&gt;In addition, members can look for the following:&lt;i&gt;&lt;/i&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Gears of War: Reloaded &lt;/i&gt;(New release on Steam and Xbox, available on PC Game Pass, Aug. 26)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Chip ‘n Clawz vs. The Brainioids &lt;/i&gt;(New release on Steam, Aug. 26)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Make Way &lt;/i&gt;(Free, new release on Epic Games Store, Aug. 28)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Among Us 3D &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Gatekeeper &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Knightica&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;No Sleep for Kaname Date – From AI: THE SOMNIUM FILES&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What are you planning to play this weekend? Let us know on X or in the comments below.&lt;/p&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p dir="ltr" lang="en"&gt;This is a duo appreciation post. 😁&lt;/p&gt;
&lt;p&gt;Who are you locking in with? (tag them) 🔒🎮&lt;/p&gt;
&lt;p&gt;— 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) August 27, 2025&lt;/p&gt;&lt;/blockquote&gt;


		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Brace yourself, COGs — the Locusts aren’t the only thing rising up. The Coalition’s legendary shooter &lt;i&gt;Gears of War: Reloaded&lt;/i&gt; is launching day one on GeForce NOW.&lt;/p&gt;
&lt;p&gt;But that’s just the start. This GFN Thursday, seven games join the GeForce NOW library, including Ubisoft’s &lt;i&gt;The Rogue Prince of Persia&lt;/i&gt;, the electrifying 2D roguelike action-platformer.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;More Grit, More Gears&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84355"&gt;&lt;img alt="Gears of War Reloaded on GeForce NOW" class="size-large wp-image-84355" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/GFN_Thursday-gowscreen-1680x945.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84355"&gt;&lt;em&gt;Never skip leg day.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Chainsaws — check. Grizzled one-liners — absolutely. &lt;i&gt;Gears of War: Reloaded&lt;/i&gt; is back, buffed and primed, remastered from the ground up in Unreal Engine 5. It’s the classic curb-stomping action gamers remember, now with visuals sharp enough to make the Locust run for cover. Form up and get loud.&lt;/p&gt;
&lt;p&gt;Dive into battle with Marcus, Dom and the rest of Delta Squad to fight tooth and chainsaw to save humanity from the subterranean Locust Horde. Carve through the epic campaign solo or tag in friends for online co-op mode. The remastered version packs every blast, chainsaw duel and bro fist from the original — plus bonus campaign missions, multiplayer maps and more. Tackle battles with modern controls for franchise newcomers or classic controls for veterans — no grunt left behind.&lt;/p&gt;
&lt;p&gt;Stream &lt;i&gt;Gears of War: Reloaded&lt;/i&gt; on GeForce NOW and witness Unreal Engine’s best visuals without upgrading hardware. Run multiplayer with the lowest latency with an Ultimate membership, cross-play with the squad and see every crumbling wall and flying chunk — all from the cloud, effortlessly.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Greatest Leap Yet&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84349"&gt;&lt;img alt="Rogue prince of persia on GeForce NOW" class="wp-image-84349 size-large" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/GFN_Thursday-Rogue_Prince_of_Persia-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84349"&gt;&lt;em&gt;Kick first, ask questions later.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;i&gt;The Rogue Prince of Persia 1.0&lt;/i&gt; marks the game’s full release after months of early access, bringing refined parkour, polished combat, fresh content and the complete story of the rogue heir racing to reclaim his kingdom. Sprint, vault and wall-run through a reimagined Persia as the prince battles to undo a deadly curse and stop the invading Huns.&lt;/p&gt;
&lt;p&gt;Each run is a new fight for survival, blending fluid platforming with swift, acrobatic combat. Leap over traps, chain stylish moves and wield an ever-expanding arsenal while unlocking medallions, upgrading gear and uncovering the truth behind the prince’s fall — and his shot at redemption.&lt;/p&gt;
&lt;p&gt;On GeForce NOW, the adventure shines at its best with up to 4K 120 frames-per-second streaming. Land every parkour move with perfect timing thanks to ultralow latency and take the prince’s fight anywhere, instantly, on nearly any device.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Let’s Play Today&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84352"&gt;&lt;img alt="alt" class="wp-image-84352 size-large" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/GFN_Thursday-Chip_n_Claws_VS_The_Brainioids-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84352"&gt;&lt;em&gt;Catbots &amp;gt; Brainblobs.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Make sure to check out &lt;i&gt;Chip ‘n Clawz vs. The Brainioids&lt;/i&gt;, a quirky action-strategy hybrid from X-COM creator Julian Gollop, where players control a clever inventor and his robo-cat to fight off an invasion of bizarre Brainioid aliens. Mix third-person action with real-time strategy while building bases, commanding bot armies and squishing rogue brains in solo and co-op modes, all wrapped in a colorful, comic-book world. Couch and online multiplayer, player vs. player battles and a humorous campaign make this a fresh, approachable take on the strategy genre.&lt;/p&gt;
&lt;p&gt;In addition, members can look for the following:&lt;i&gt;&lt;/i&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Gears of War: Reloaded &lt;/i&gt;(New release on Steam and Xbox, available on PC Game Pass, Aug. 26)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Chip ‘n Clawz vs. The Brainioids &lt;/i&gt;(New release on Steam, Aug. 26)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Make Way &lt;/i&gt;(Free, new release on Epic Games Store, Aug. 28)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Among Us 3D &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Gatekeeper &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Knightica&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;No Sleep for Kaname Date – From AI: THE SOMNIUM FILES&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What are you planning to play this weekend? Let us know on X or in the comments below.&lt;/p&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p dir="ltr" lang="en"&gt;This is a duo appreciation post. 😁&lt;/p&gt;
&lt;p&gt;Who are you locking in with? (tag them) 🔒🎮&lt;/p&gt;
&lt;p&gt;— 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) August 27, 2025&lt;/p&gt;&lt;/blockquote&gt;


		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/geforce-now-thursday-gears-of-war-reloaded/</guid><pubDate>Thu, 28 Aug 2025 13:00:17 +0000</pubDate></item><item><title>[NEW] Game On: How Modders Reimagine Classic Games With NVIDIA RTX Remix and Generative AI (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/rtx-ai-garage-rtx-remix-mod-contest-gen-ai/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Last week at Gamescom, NVIDIA announced the winners of the NVIDIA and ModDB RTX Remix Mod Contest, a $50,000 competition celebrating community-made projects that reimagine classic games with modern fidelity.&lt;/p&gt;
&lt;p&gt;The entries showed how far video game modding has come, with individual modders and small teams pulling off overhauls of similar quality to those created by entire studios.&lt;/p&gt;
&lt;p&gt;At the heart of these projects was NVIDIA RTX Remix, a platform that lets creators capture assets from classic titles and rebuild them with modern lighting, geometry and materials. Paired with generative AI tools like PBRFusion and ComfyUI, modders can now upscale or generate thousands of textures and automate repetitive tasks so they can focus on their artistry.&lt;/p&gt;
&lt;p&gt;Plus, with NVIDIA RTX GPUs accelerating these AI-driven workflows, ambitious remasters that once took years can now come together in months.&lt;/p&gt;
&lt;p&gt;There are currently 237 RTX Remix projects in development, building on over 100 finished mods and 2 million downloads across fan favorites like &lt;i&gt;Half-Life 2&lt;/i&gt;,&lt;i&gt; Need for Speed: Underground&lt;/i&gt;, &lt;i&gt;Portal&lt;/i&gt; and&lt;i&gt; Deus Ex&lt;/i&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Transforming Classics With Generative AI&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;The RTX Remix Mod Contest crowned Merry Pencil Studios’ &lt;i&gt;Painkiller RTX Remix&lt;/i&gt; with several awards, but it wasn’t the only mod worth celebrating.&lt;/p&gt;
&lt;p&gt;Here’s a closer look at the winning submission, along with other standout projects that showcase how RTX Remix and AI-powered tools are redefining what’s possible with modding on PCs.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;‘Painkiller’ RTX Remix: Winner in Best Overall, Best Use of RTX and Most Complete Categories&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The mod team Merry Pencil Studios rebuilt more than 35 levels of the gothic shooter &lt;i&gt;Painkiller &lt;/i&gt;using AI-assisted workflows and handcrafted artistry. The team batch-processed thousands of low-resolution textures and generated high-resolution physically based rendering (PBR) materials that automatically got imported into RTX Remix.&lt;/p&gt;
&lt;p&gt;The team’s AI model of choice was PBRFusion, a model trained by the RTX Remix community that can upscale textures by 4x and generate high-quality normal, roughness and height maps.&lt;/p&gt;
&lt;p&gt;This workflow provided a consistent foundation for the game’s complex environments, freeing up time for creative polish. From there, the team used tools like Blender and InstaMAT to craft assets like lanterns and other gothic details that define the game’s atmosphere.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;“Generative AI has completely expanded what feels possible in modding. Beyond texture upscaling, we’re now seeing it generate 3D models, refine complex multi-material surfaces and assist with coding tasks like building workflow tools, writing documentation and catching errors.” — Merry Pencil Studios&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84369"&gt;&lt;img alt="alt" class="size-large wp-image-84369" height="473" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/RTX-Remix-transforms-the-gothic-cathedral-in-Painkiller-1680x473.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84369"&gt;RTX Remix transforms the gothic cathedral in “Painkiller”: RTX OFF (left) shows the original look, while RTX ON (right) fills the hall with stained glass reflections, volumetric beams and realistic shadows.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;div class="simplePullQuote right"&gt;&lt;p&gt;“PBRFusion and other AI tools made it possible for a small team to convert an entire game into PBR. It set the baseline look, while we focused our manual efforts on the assets players notice most.” — Merry Pencil Studios&amp;nbsp;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;With RTX Remix, gothic churches now glow with volumetric light pouring through stained glass, marble statues scatter colored light and combat scenes erupt with particle effects that cast realistic shadows. NVIDIA GeForce RTX GPUs powered the workflow from start to finish, with real-time path tracing and NVIDIA DLSS technology ensuring smooth iteration while editing even on massive scenes.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;“The NVIDIA GeForce RTX 5090 GPU was a dream for our workflow: speed, fluidity, everything felt seamless. DLSS Frame Generation doubled or even tripled frame rates, making the game look incredible on high-refresh displays.” — Merry Pencil Studios&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;What makes the &lt;i&gt;Painkiller&lt;/i&gt; RTX Remix notable is its scope, featuring over 35 remastered levels. This amount of work couldn’t have been completed in such a short time without RTX Remix and the generative AI tools the team used.&lt;/p&gt;
&lt;p&gt;By combining generative AI automation with careful craftsmanship, Merry Pencil Studios delivered a project that feels both ambitious and polished.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;‘Unreal’ RTX Remix: An Ambitious AI Texture Rebuild&lt;/b&gt;&lt;/h2&gt;
&lt;div class="simplePullQuote right"&gt;&lt;p&gt;“I wouldn’t have been able to create PBR textures without AI. I could have maybe created emissive maps and height maps, but I wouldn’t have been able to do the roughness or normal maps myself.” — mstewart401&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;UnrealRTX demonstrates the scale of generative AI’s impact in modding. Modder mstewart401 set out to rebuild nearly every texture across 14 levels of the 1998 classic — a task that would have taken years by hand.&lt;/p&gt;
&lt;p&gt;With RTX Remix’s built-in AI texture tools, plus experimental methods like generating animations from AI video tools and hand-editing light maps, whole environments were reimagined with new detail and atmosphere.&lt;/p&gt;
&lt;p&gt;The results are striking: glowing crystals pulse with emissive light, alien landscapes shimmer with modern materials and the game’s otherworldly maps feel richer and more alive. By leaning on AI for the bulk of the texture work, mstewart401 could focus on creative polishing — delivering an overhaul that feels ambitious even by professional standards.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84372"&gt;&lt;img alt="alt" class="size-large wp-image-84372" height="473" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/RTX-Remix-transforms-the-alien-environments-in-Unreal-1680x473.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84372"&gt;RTX Remix transforms the alien environments in “Unreal”: RTX OFF (left) shows the original flat look, while RTX ON (right) adds detailed PBR materials, emissive lighting and realistic reflections.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;em&gt;“If someone like me can make a mod like this, anyone can. I only get an hour here and an hour there, but with generative AI and RTX, I’ve been able to push ‘Unreal’ further than I ever thought possible.” — mstewart401&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;‘Need for Speed: Underground’ RTX Remix: Blending AI and 3D Artistry&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;In the &lt;i&gt;Need for Speed: Underground&lt;/i&gt; RTX Remix, modder Alessandro893 used AI and 3D artistry to remaster every race course in the game with new textures, materials and lighting.&lt;/p&gt;
&lt;div class="simplePullQuote right"&gt;&lt;p&gt;“In racing games, generative AI opens up new possibilities for creating realistic and immersive environments. In a racing game like ‘Need for Speed: Underground,’ the visual environment is crucial for player immersion, but it also needs to feel responsive and varied.” — Alessandro893&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Using ComfyUI, Alessandro893 generated more than 500 new textures, then refined them in Adobe Photoshop for consistency and realism. In addition, the modder built over 30 new high-poly car and environment models in Blender, upgrading older assets with smoother, more lifelike detail.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;“Generative AI was mainly used for texture generation. The original look was preserved by using the original textures as input for AI. It’s impossible to create such a large number of textures in such a short period of time alone without AI.” — Alessandro893&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;With RTX GPUs driving AI texture conversion, path-traced reflections and DLSS acceleration, the team could reimagine racing environments with faster iteration and higher fidelity than ever. But as the modder emphasized, AI didn’t replace artistry. It created room for it.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84375"&gt;&lt;img alt="alt" class="size-large wp-image-84375" height="473" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/RTX-Remix-reimagines-Need-for-Speed-Undergrounds-Chinatown-1680x473.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84375"&gt;RTX Remix reimagines “Need for Speed: Underground’s” Chinatown: RTX OFF (left) shows low-resolution textures and flat lighting, while RTX ON (right) adds neon reflections, detailed materials and fully path-traced streets.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The overhaul is most striking on the Chinatown track, which was rebuilt with new buildings, vegetation and fully path-traced lighting that makes neon reflections pop against wet pavement.&lt;/p&gt;
&lt;p&gt;By leaning on AI to handle the repetitive work of texture generation, the modder could focus on creative refinements — giving Olympic City a modern, cinematic twist while preserving its nostalgic feel.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;‘Portal 2’ RTX Remix: An Innovative AI-Powered Workflow&lt;/b&gt;&lt;/h2&gt;
&lt;div class="simplePullQuote right"&gt;&lt;p&gt;&lt;i&gt;“AI opened up new opportunities and drastically accelerated my workflow, allowing me to focus on more ambitious creative tasks.”&amp;nbsp; &lt;/i&gt;— Skurtyyskirts&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Skurtyyskirts, the modder behind &lt;i&gt;Portal 2&lt;/i&gt; RTX Remix, used a unique workflow — tapping a large language model to build a custom plug-in called Substance2Remix, bridging Adobe Substance Painter directly to RTX Remix.&lt;/p&gt;
&lt;p&gt;This flow allowed the modder to pull in an asset, apply AI-assisted materials, hand-paint details and push it straight back into the game, all in one rapid loop. What would normally take days of exporting and importing was done in minutes.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;“Once I saw the potential of Remix’s REST application programming interface, I realized I could create a more integrated workflow between tools like Substance Painter and RTX Remix. I didn’t want to deal with a manual, tedious export-import process for my handmade textures, so I developed a simple plug-and-play plug-in. This completely shifted the role of AI from a simple upscaling tool to a core component of my creative pipeline, enabling me to focus on creating detailed, high-quality textures by hand.” &lt;/i&gt;— Skurtyyskirts&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84378"&gt;&lt;img alt="alt" class="size-large wp-image-84378" height="473" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/RTX-Remix-sharpens-the-test-chambers-in-Portal-2-1680x473.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84378"&gt;RTX Remix modernizes “Portal 2’s” test chambers: RTX ON highlights realistic reflections, detailed materials, and atmospheric lighting that transform Aperture Science into a more immersive environment.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Early on, the project leaned on AI upscalers like PBRFusion, but over time the workflow evolved into a mix of AI and manual artistry. The result is a sharper, more atmospheric game environment — enhanced further by RTX Remix’s volumetrics and fog systems, which make the decaying test chambers feel more alive.&lt;/p&gt;

&lt;figure class="wp-caption aligncenter" id="attachment_84381"&gt;&lt;img alt="alt" class="size-full wp-image-84381" height="720" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/RTX-Remix-modernizes-Portal-2-test-chambers.png" width="1280" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84381"&gt;RTX Remix modernizes “Portal 2’s” test chambers: RTX ON highlights realistic reflections, detailed materials, and atmospheric lighting that transform Aperture Science into a more immersive environment.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;By creating a new pipeline, the project opens the door for other modders to experiment with faster, AI-powered workflows of their own.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Press Start: Remaster With RTX Remix&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;To get started creating RTX Remix mods, download NVIDIA RTX Remix from the home screen of the NVIDIA App and check out our tutorials and documentation. PBRFusion on Hugging Face also offers a plug-and-play setup with ComfyUI, letting modders batch-process textures into high-quality, PBR maps in just a few clicks.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84384"&gt;&lt;img alt="alt" class="size-full wp-image-84384" height="725" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/PBRFusion-is-a-generative-AI-tool-built-by-modder-NightRaven109.png" width="1285" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84384"&gt;PBRFusion is a generative AI tool built by modder NightRaven109 (and shared on Hugging Face) that helps convert old, low-res game textures into full PBR (Physically Based Rendering) materials.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Check out all of the mods submitted to the RTX Remix Modding Contest, as well as 100 more Remix mods, available to download from ModDB. Read the RTX Remix article to learn more about the contest and winners. For a sneak peek at RTX Remix projects under active development, join the community over at the RTX Remix Showcase Discord server — it’s a great place to get a helping hand.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Each week, the &lt;/i&gt;&lt;i&gt;RTX AI Garage&lt;/i&gt; &lt;i&gt;blog series features community-driven AI innovations and content for those looking to learn more about NVIDIA NIM microservices and AI Blueprints, as well as building &lt;/i&gt;&lt;i&gt;AI agents&lt;/i&gt;&lt;i&gt;, creative workflows, productivity apps and more on AI PCs and workstations.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; — and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;. Join NVIDIA’s &lt;/i&gt;&lt;i&gt;Discord server&lt;/i&gt;&lt;i&gt; to connect with community developers and AI enthusiasts for discussions on what’s possible with RTX AI.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Last week at Gamescom, NVIDIA announced the winners of the NVIDIA and ModDB RTX Remix Mod Contest, a $50,000 competition celebrating community-made projects that reimagine classic games with modern fidelity.&lt;/p&gt;
&lt;p&gt;The entries showed how far video game modding has come, with individual modders and small teams pulling off overhauls of similar quality to those created by entire studios.&lt;/p&gt;
&lt;p&gt;At the heart of these projects was NVIDIA RTX Remix, a platform that lets creators capture assets from classic titles and rebuild them with modern lighting, geometry and materials. Paired with generative AI tools like PBRFusion and ComfyUI, modders can now upscale or generate thousands of textures and automate repetitive tasks so they can focus on their artistry.&lt;/p&gt;
&lt;p&gt;Plus, with NVIDIA RTX GPUs accelerating these AI-driven workflows, ambitious remasters that once took years can now come together in months.&lt;/p&gt;
&lt;p&gt;There are currently 237 RTX Remix projects in development, building on over 100 finished mods and 2 million downloads across fan favorites like &lt;i&gt;Half-Life 2&lt;/i&gt;,&lt;i&gt; Need for Speed: Underground&lt;/i&gt;, &lt;i&gt;Portal&lt;/i&gt; and&lt;i&gt; Deus Ex&lt;/i&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Transforming Classics With Generative AI&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;The RTX Remix Mod Contest crowned Merry Pencil Studios’ &lt;i&gt;Painkiller RTX Remix&lt;/i&gt; with several awards, but it wasn’t the only mod worth celebrating.&lt;/p&gt;
&lt;p&gt;Here’s a closer look at the winning submission, along with other standout projects that showcase how RTX Remix and AI-powered tools are redefining what’s possible with modding on PCs.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;‘Painkiller’ RTX Remix: Winner in Best Overall, Best Use of RTX and Most Complete Categories&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The mod team Merry Pencil Studios rebuilt more than 35 levels of the gothic shooter &lt;i&gt;Painkiller &lt;/i&gt;using AI-assisted workflows and handcrafted artistry. The team batch-processed thousands of low-resolution textures and generated high-resolution physically based rendering (PBR) materials that automatically got imported into RTX Remix.&lt;/p&gt;
&lt;p&gt;The team’s AI model of choice was PBRFusion, a model trained by the RTX Remix community that can upscale textures by 4x and generate high-quality normal, roughness and height maps.&lt;/p&gt;
&lt;p&gt;This workflow provided a consistent foundation for the game’s complex environments, freeing up time for creative polish. From there, the team used tools like Blender and InstaMAT to craft assets like lanterns and other gothic details that define the game’s atmosphere.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;“Generative AI has completely expanded what feels possible in modding. Beyond texture upscaling, we’re now seeing it generate 3D models, refine complex multi-material surfaces and assist with coding tasks like building workflow tools, writing documentation and catching errors.” — Merry Pencil Studios&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84369"&gt;&lt;img alt="alt" class="size-large wp-image-84369" height="473" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/RTX-Remix-transforms-the-gothic-cathedral-in-Painkiller-1680x473.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84369"&gt;RTX Remix transforms the gothic cathedral in “Painkiller”: RTX OFF (left) shows the original look, while RTX ON (right) fills the hall with stained glass reflections, volumetric beams and realistic shadows.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;div class="simplePullQuote right"&gt;&lt;p&gt;“PBRFusion and other AI tools made it possible for a small team to convert an entire game into PBR. It set the baseline look, while we focused our manual efforts on the assets players notice most.” — Merry Pencil Studios&amp;nbsp;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;With RTX Remix, gothic churches now glow with volumetric light pouring through stained glass, marble statues scatter colored light and combat scenes erupt with particle effects that cast realistic shadows. NVIDIA GeForce RTX GPUs powered the workflow from start to finish, with real-time path tracing and NVIDIA DLSS technology ensuring smooth iteration while editing even on massive scenes.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;“The NVIDIA GeForce RTX 5090 GPU was a dream for our workflow: speed, fluidity, everything felt seamless. DLSS Frame Generation doubled or even tripled frame rates, making the game look incredible on high-refresh displays.” — Merry Pencil Studios&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;What makes the &lt;i&gt;Painkiller&lt;/i&gt; RTX Remix notable is its scope, featuring over 35 remastered levels. This amount of work couldn’t have been completed in such a short time without RTX Remix and the generative AI tools the team used.&lt;/p&gt;
&lt;p&gt;By combining generative AI automation with careful craftsmanship, Merry Pencil Studios delivered a project that feels both ambitious and polished.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;‘Unreal’ RTX Remix: An Ambitious AI Texture Rebuild&lt;/b&gt;&lt;/h2&gt;
&lt;div class="simplePullQuote right"&gt;&lt;p&gt;“I wouldn’t have been able to create PBR textures without AI. I could have maybe created emissive maps and height maps, but I wouldn’t have been able to do the roughness or normal maps myself.” — mstewart401&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;UnrealRTX demonstrates the scale of generative AI’s impact in modding. Modder mstewart401 set out to rebuild nearly every texture across 14 levels of the 1998 classic — a task that would have taken years by hand.&lt;/p&gt;
&lt;p&gt;With RTX Remix’s built-in AI texture tools, plus experimental methods like generating animations from AI video tools and hand-editing light maps, whole environments were reimagined with new detail and atmosphere.&lt;/p&gt;
&lt;p&gt;The results are striking: glowing crystals pulse with emissive light, alien landscapes shimmer with modern materials and the game’s otherworldly maps feel richer and more alive. By leaning on AI for the bulk of the texture work, mstewart401 could focus on creative polishing — delivering an overhaul that feels ambitious even by professional standards.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84372"&gt;&lt;img alt="alt" class="size-large wp-image-84372" height="473" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/RTX-Remix-transforms-the-alien-environments-in-Unreal-1680x473.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84372"&gt;RTX Remix transforms the alien environments in “Unreal”: RTX OFF (left) shows the original flat look, while RTX ON (right) adds detailed PBR materials, emissive lighting and realistic reflections.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;em&gt;“If someone like me can make a mod like this, anyone can. I only get an hour here and an hour there, but with generative AI and RTX, I’ve been able to push ‘Unreal’ further than I ever thought possible.” — mstewart401&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;‘Need for Speed: Underground’ RTX Remix: Blending AI and 3D Artistry&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;In the &lt;i&gt;Need for Speed: Underground&lt;/i&gt; RTX Remix, modder Alessandro893 used AI and 3D artistry to remaster every race course in the game with new textures, materials and lighting.&lt;/p&gt;
&lt;div class="simplePullQuote right"&gt;&lt;p&gt;“In racing games, generative AI opens up new possibilities for creating realistic and immersive environments. In a racing game like ‘Need for Speed: Underground,’ the visual environment is crucial for player immersion, but it also needs to feel responsive and varied.” — Alessandro893&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Using ComfyUI, Alessandro893 generated more than 500 new textures, then refined them in Adobe Photoshop for consistency and realism. In addition, the modder built over 30 new high-poly car and environment models in Blender, upgrading older assets with smoother, more lifelike detail.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;“Generative AI was mainly used for texture generation. The original look was preserved by using the original textures as input for AI. It’s impossible to create such a large number of textures in such a short period of time alone without AI.” — Alessandro893&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;With RTX GPUs driving AI texture conversion, path-traced reflections and DLSS acceleration, the team could reimagine racing environments with faster iteration and higher fidelity than ever. But as the modder emphasized, AI didn’t replace artistry. It created room for it.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84375"&gt;&lt;img alt="alt" class="size-large wp-image-84375" height="473" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/RTX-Remix-reimagines-Need-for-Speed-Undergrounds-Chinatown-1680x473.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84375"&gt;RTX Remix reimagines “Need for Speed: Underground’s” Chinatown: RTX OFF (left) shows low-resolution textures and flat lighting, while RTX ON (right) adds neon reflections, detailed materials and fully path-traced streets.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The overhaul is most striking on the Chinatown track, which was rebuilt with new buildings, vegetation and fully path-traced lighting that makes neon reflections pop against wet pavement.&lt;/p&gt;
&lt;p&gt;By leaning on AI to handle the repetitive work of texture generation, the modder could focus on creative refinements — giving Olympic City a modern, cinematic twist while preserving its nostalgic feel.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;‘Portal 2’ RTX Remix: An Innovative AI-Powered Workflow&lt;/b&gt;&lt;/h2&gt;
&lt;div class="simplePullQuote right"&gt;&lt;p&gt;&lt;i&gt;“AI opened up new opportunities and drastically accelerated my workflow, allowing me to focus on more ambitious creative tasks.”&amp;nbsp; &lt;/i&gt;— Skurtyyskirts&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Skurtyyskirts, the modder behind &lt;i&gt;Portal 2&lt;/i&gt; RTX Remix, used a unique workflow — tapping a large language model to build a custom plug-in called Substance2Remix, bridging Adobe Substance Painter directly to RTX Remix.&lt;/p&gt;
&lt;p&gt;This flow allowed the modder to pull in an asset, apply AI-assisted materials, hand-paint details and push it straight back into the game, all in one rapid loop. What would normally take days of exporting and importing was done in minutes.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;“Once I saw the potential of Remix’s REST application programming interface, I realized I could create a more integrated workflow between tools like Substance Painter and RTX Remix. I didn’t want to deal with a manual, tedious export-import process for my handmade textures, so I developed a simple plug-and-play plug-in. This completely shifted the role of AI from a simple upscaling tool to a core component of my creative pipeline, enabling me to focus on creating detailed, high-quality textures by hand.” &lt;/i&gt;— Skurtyyskirts&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84378"&gt;&lt;img alt="alt" class="size-large wp-image-84378" height="473" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/RTX-Remix-sharpens-the-test-chambers-in-Portal-2-1680x473.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84378"&gt;RTX Remix modernizes “Portal 2’s” test chambers: RTX ON highlights realistic reflections, detailed materials, and atmospheric lighting that transform Aperture Science into a more immersive environment.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Early on, the project leaned on AI upscalers like PBRFusion, but over time the workflow evolved into a mix of AI and manual artistry. The result is a sharper, more atmospheric game environment — enhanced further by RTX Remix’s volumetrics and fog systems, which make the decaying test chambers feel more alive.&lt;/p&gt;

&lt;figure class="wp-caption aligncenter" id="attachment_84381"&gt;&lt;img alt="alt" class="size-full wp-image-84381" height="720" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/RTX-Remix-modernizes-Portal-2-test-chambers.png" width="1280" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84381"&gt;RTX Remix modernizes “Portal 2’s” test chambers: RTX ON highlights realistic reflections, detailed materials, and atmospheric lighting that transform Aperture Science into a more immersive environment.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;By creating a new pipeline, the project opens the door for other modders to experiment with faster, AI-powered workflows of their own.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Press Start: Remaster With RTX Remix&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;To get started creating RTX Remix mods, download NVIDIA RTX Remix from the home screen of the NVIDIA App and check out our tutorials and documentation. PBRFusion on Hugging Face also offers a plug-and-play setup with ComfyUI, letting modders batch-process textures into high-quality, PBR maps in just a few clicks.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84384"&gt;&lt;img alt="alt" class="size-full wp-image-84384" height="725" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/PBRFusion-is-a-generative-AI-tool-built-by-modder-NightRaven109.png" width="1285" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84384"&gt;PBRFusion is a generative AI tool built by modder NightRaven109 (and shared on Hugging Face) that helps convert old, low-res game textures into full PBR (Physically Based Rendering) materials.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Check out all of the mods submitted to the RTX Remix Modding Contest, as well as 100 more Remix mods, available to download from ModDB. Read the RTX Remix article to learn more about the contest and winners. For a sneak peek at RTX Remix projects under active development, join the community over at the RTX Remix Showcase Discord server — it’s a great place to get a helping hand.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Each week, the &lt;/i&gt;&lt;i&gt;RTX AI Garage&lt;/i&gt; &lt;i&gt;blog series features community-driven AI innovations and content for those looking to learn more about NVIDIA NIM microservices and AI Blueprints, as well as building &lt;/i&gt;&lt;i&gt;AI agents&lt;/i&gt;&lt;i&gt;, creative workflows, productivity apps and more on AI PCs and workstations.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; — and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;. Join NVIDIA’s &lt;/i&gt;&lt;i&gt;Discord server&lt;/i&gt;&lt;i&gt; to connect with community developers and AI enthusiasts for discussions on what’s possible with RTX AI.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/rtx-ai-garage-rtx-remix-mod-contest-gen-ai/</guid><pubDate>Thu, 28 Aug 2025 13:00:30 +0000</pubDate></item><item><title>[NEW] AI hires or human hustle? Inside the next frontier of startup operations at TechCrunch Disrupt 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/28/ai-hires-or-human-hustle-inside-the-next-frontier-of-startup-operations-at-techcrunch-disrupt-2025/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;What happens when your first 10 hires aren’t people at all? At &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, happening October 27–29 at San Francisco’s Moscone West, we’re digging into the new wave of startups replacing or augmenting early employees with AI agents. Think outbound sales, billing, and customer support — automated from day one.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This panel, hosted on the &lt;strong&gt;Builders Stage&lt;/strong&gt;, features a mix of technical founders and seasoned operators who are actually doing it, debating where the line between human and machine should be drawn — and how far is too far.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Caleb Peffer, Jaspar Carmichael-Jack, Sarah Franklin" class="wp-image-3040625" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/TC25_Jack-Peffer-Franklin-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-meet-the-speakers"&gt;Meet the speakers&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Caleb Peffer&lt;/strong&gt;, founder and CEO of Firecrawl, is helping over 350,000 developers (and companies like Shopify and Zapier) plug AI directly into the live web. His dev-first platform is already reshaping how AI agents interact with the internet and scale with clean data.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Jaspar Carmichael-Jack&lt;/strong&gt;, founder and CEO of Artisan, made waves with his “Stop Hiring Humans” campaign — and he’s serious. His company raised $35 million to build AI employees, starting with sales. Expect bold insights on replacing go-to-market teams with code.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Sarah Franklin&lt;/strong&gt;, CEO of Lattice and former Salesforce president and CMO, brings hard-won wisdom on scaling companies with impact. She’s built and led real teams at the highest level and knows exactly where AI helps — and where it hurts.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-why-this-session-matters"&gt;Why this session matters&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Whether you’re already embedding AI into your stack or just testing prompts on your product team, this conversation is about more than hype. It’s about getting real on ROI, trust, team dynamics, and what it means to build a business that moves faster than ever with fewer human hands.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-ready-to-find-your-edge"&gt;Ready to find your edge?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;This session is just one of hundreds featured across five industry stages, plus breakouts and roundtables. Grab your pass to Disrupt 2025 before prices jump in mid-September. &lt;strong&gt;Get your ticket now&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;What happens when your first 10 hires aren’t people at all? At &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, happening October 27–29 at San Francisco’s Moscone West, we’re digging into the new wave of startups replacing or augmenting early employees with AI agents. Think outbound sales, billing, and customer support — automated from day one.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This panel, hosted on the &lt;strong&gt;Builders Stage&lt;/strong&gt;, features a mix of technical founders and seasoned operators who are actually doing it, debating where the line between human and machine should be drawn — and how far is too far.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Caleb Peffer, Jaspar Carmichael-Jack, Sarah Franklin" class="wp-image-3040625" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/TC25_Jack-Peffer-Franklin-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-meet-the-speakers"&gt;Meet the speakers&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Caleb Peffer&lt;/strong&gt;, founder and CEO of Firecrawl, is helping over 350,000 developers (and companies like Shopify and Zapier) plug AI directly into the live web. His dev-first platform is already reshaping how AI agents interact with the internet and scale with clean data.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Jaspar Carmichael-Jack&lt;/strong&gt;, founder and CEO of Artisan, made waves with his “Stop Hiring Humans” campaign — and he’s serious. His company raised $35 million to build AI employees, starting with sales. Expect bold insights on replacing go-to-market teams with code.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Sarah Franklin&lt;/strong&gt;, CEO of Lattice and former Salesforce president and CMO, brings hard-won wisdom on scaling companies with impact. She’s built and led real teams at the highest level and knows exactly where AI helps — and where it hurts.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-why-this-session-matters"&gt;Why this session matters&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Whether you’re already embedding AI into your stack or just testing prompts on your product team, this conversation is about more than hype. It’s about getting real on ROI, trust, team dynamics, and what it means to build a business that moves faster than ever with fewer human hands.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-ready-to-find-your-edge"&gt;Ready to find your edge?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;This session is just one of hundreds featured across five industry stages, plus breakouts and roundtables. Grab your pass to Disrupt 2025 before prices jump in mid-September. &lt;strong&gt;Get your ticket now&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/28/ai-hires-or-human-hustle-inside-the-next-frontier-of-startup-operations-at-techcrunch-disrupt-2025/</guid><pubDate>Thu, 28 Aug 2025 14:00:00 +0000</pubDate></item><item><title>[NEW] Investors are loving Lovable (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/28/investors-are-loving-lovable/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/picnew-copy.png?resize=1200,801" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Investors are clamoring to get onto Swedish vibe-coding startup Lovable’s cap table, making unsolicited offers of investment that value the company at more than $4 billion, reports Financial Times.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lovable CEO Anton Osika isn’t currently engaging with the flurry of inbound interest, the Times says, which comes a few weeks after the startup announced a $200 million round at a $1.8 billion valuation in a deal led by Accel.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;A Lovable spokesperson told TechCrunch that the company isn’t fundraising now. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lovable has grown quickly over its short lifespan. In July, the startup said its annual recurring revenue had surpassed $100 million with more than 10 million projects built using the platform.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The astounding trajectory of Europe’s hottest unicorn comes just nine months after Lovable launched and comes on the heels of investor interest in vibe-coding startups. Cursor-maker Anysphere raised $900 million in May, more than tripling its valuation to $9 billion.&lt;/p&gt;

&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at&amp;nbsp;rebecca.bellan@techcrunch.com&amp;nbsp;and Maxwell Zeff at&amp;nbsp;maxwell.zeff@techcrunch.com. For secure communication, you can contact us via Signal at&amp;nbsp;@rebeccabellan.491 and&amp;nbsp;@mzeff.88.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/picnew-copy.png?resize=1200,801" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Investors are clamoring to get onto Swedish vibe-coding startup Lovable’s cap table, making unsolicited offers of investment that value the company at more than $4 billion, reports Financial Times.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lovable CEO Anton Osika isn’t currently engaging with the flurry of inbound interest, the Times says, which comes a few weeks after the startup announced a $200 million round at a $1.8 billion valuation in a deal led by Accel.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;A Lovable spokesperson told TechCrunch that the company isn’t fundraising now. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lovable has grown quickly over its short lifespan. In July, the startup said its annual recurring revenue had surpassed $100 million with more than 10 million projects built using the platform.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The astounding trajectory of Europe’s hottest unicorn comes just nine months after Lovable launched and comes on the heels of investor interest in vibe-coding startups. Cursor-maker Anysphere raised $900 million in May, more than tripling its valuation to $9 billion.&lt;/p&gt;

&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at&amp;nbsp;rebecca.bellan@techcrunch.com&amp;nbsp;and Maxwell Zeff at&amp;nbsp;maxwell.zeff@techcrunch.com. For secure communication, you can contact us via Signal at&amp;nbsp;@rebeccabellan.491 and&amp;nbsp;@mzeff.88.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/28/investors-are-loving-lovable/</guid><pubDate>Thu, 28 Aug 2025 14:04:06 +0000</pubDate></item><item><title>[NEW] MIT researchers develop AI tool to improve flu vaccine strain selection (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/vaxseer-ai-tool-to-improve-flu-vaccine-strain-selection-0828</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202508/mit-vaxseer-barzilay-shi-00_1.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr" id="docs-internal-guid-2ef4ad7e-7fff-c28e-41c1-d61ce18592ae"&gt;Every year, global health experts are faced with a high-stakes decision: Which influenza strains should go into the next seasonal vaccine? The choice must be made months in advance, long before flu season even begins, and it can often feel like a race against the clock. If the selected strains match those that circulate, the vaccine will likely be highly effective. But if the prediction is off, protection can drop significantly, leading to (potentially preventable) illness and strain on health care systems.&lt;/p&gt;&lt;p dir="ltr"&gt;This challenge became even more familiar to scientists in the years during the Covid-19 pandemic. Think back to the time (and time and time again), when new variants emerged just as vaccines were being rolled out. Influenza behaves like a similar, rowdy cousin, mutating constantly and unpredictably. That makes it hard to stay ahead, and therefore harder to design vaccines that remain protective.&lt;/p&gt;&lt;p dir="ltr"&gt;To reduce this uncertainty, scientists at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and the MIT Abdul Latif Jameel Clinic for Machine Learning in Health set out to make vaccine selection more accurate and less reliant on guesswork. They created an AI system called VaxSeer, designed to predict dominant flu strains and identify the most protective vaccine candidates, months ahead of time. The tool uses deep learning models trained on decades of viral sequences and lab test results to simulate how the flu virus might evolve and how the vaccines will respond.&lt;/p&gt;&lt;p dir="ltr"&gt;Traditional evolution models often analyze the effect of single amino acid mutations independently. “VaxSeer adopts a large protein language model to learn the relationship between dominance and the combinatorial effects of mutations,” explains Wenxian Shi, a PhD student in MIT’s Department of Electrical Engineering and Computer Science, researcher at CSAIL, and lead author of a new paper on the work. “Unlike existing protein language models that assume a static distribution of viral variants, we model dynamic dominance shifts, making it better suited for rapidly evolving viruses like influenza.”&lt;/p&gt;&lt;p dir="ltr"&gt;An open-access report on the study was published today in &lt;em&gt;Nature Medicine.&lt;/em&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;The future of flu&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;VaxSeer has two core prediction engines: one that estimates how likely each viral strain is to spread (dominance), and another that estimates how effectively a vaccine will neutralize that strain (antigenicity). Together, they produce a predicted coverage score: a forward-looking measure of how well a given vaccine is likely to perform against future viruses.&lt;/p&gt;&lt;p dir="ltr"&gt;The scale of the score could be from an infinite negative to 0. The closer the score to 0, the better the antigenic match of vaccine strains to the circulating viruses. (You can imagine it as the negative of some kind of “distance.”)&lt;/p&gt;&lt;p dir="ltr"&gt;In a 10-year retrospective study, the researchers evaluated VaxSeer’s recommendations against those made by the World Health Organization (WHO) for two major flu subtypes: A/H3N2 and A/H1N1. For A/H3N2, VaxSeer’s choices outperformed the WHO’s in nine out of 10 seasons, based on retrospective empirical coverage scores (a surrogate metric of the vaccine effectiveness, calculated from the observed dominance from past seasons and experimental HI test results). The team used this to evaluate vaccine selections, as the effectiveness is only available for vaccines actually given to the population.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;For A/H1N1, it outperformed or matched the WHO in six out of 10 seasons. In one notable case, for the 2016 flu season, VaxSeer identified a strain that wasn’t chosen by the WHO until the following year. The model’s predictions also showed strong correlation with real-world vaccine effectiveness estimates, as reported by the CDC, Canada’s Sentinel Practitioner Surveillance Network, and Europe’s I-MOVE program. VaxSeer’s predicted coverage scores aligned closely with public health data on flu-related illnesses and medical visits prevented by vaccination.&lt;/p&gt;&lt;p dir="ltr"&gt;So how exactly does VaxSeer make sense of all these data? Intuitively, the model first estimates how rapidly a viral strain spreads over time using a protein language model, and then determines its dominance by accounting for competition among different strains.&lt;/p&gt;&lt;p dir="ltr"&gt;Once the model has calculated its insights, they’re plugged into a mathematical framework based on something called ordinary differential equations to simulate viral spread over time. For antigenicity, the system estimates how well a given vaccine strain will perform in a common lab test called the hemagglutination inhibition assay. This measures how effectively antibodies can&amp;nbsp;inhibit the virus from binding to human red blood cells, which is a widely used proxy for antigenic match/antigenicity.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Outpacing evolution&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;“By modeling how viruses evolve and how vaccines interact with them, AI tools like VaxSeer could help health officials make better, faster decisions — and stay one step ahead in the race between infection and immunity,” says Shi.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;VaxSeer currently focuses only on the flu virus’s HA (hemagglutinin) protein,the major antigen of influenza. Future versions could incorporate other proteins like NA (neuraminidase), and factors like immune history, manufacturing constraints, or dosage levels. Applying the system to other viruses would also require large, high-quality datasets that track both viral evolution and immune responses — data that aren’t always publicly available. The team, however is currently working on the methods that can predict viral evolution in low-data regimes building on relations between viral families&lt;/p&gt;&lt;p dir="ltr"&gt;“Given the speed of viral evolution, current therapeutic development often lags behind.&amp;nbsp;VaxSeer is our attempt to catch up,” says Regina Barzilay, the School of Engineering Distinguished Professor for AI and Health at MIT, AI lead of Jameel Clinic, and CSAIL principal investigator.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“This paper is impressive, but what excites me perhaps even more is the team’s ongoing work on predicting viral evolution in low-data settings,” says Assistant Professor Jon Stokes of the Department of Biochemistry and Biomedical Sciences at McMaster University in Hamilton, Ontario. “The implications go far beyond influenza. Imagine being able to anticipate how antibiotic-resistant bacteria or drug-resistant cancers might evolve, both of which can adapt rapidly. This kind of predictive modeling opens up a powerful new way of thinking about how diseases change, giving us the opportunity to stay one step ahead and design clinical interventions before escape becomes a major problem.”&lt;/p&gt;&lt;p dir="ltr"&gt;Shi and Barzilay wrote the paper with MIT CSAIL postdoc Jeremy Wohlwend ’16, MEng ’17, PhD ’25 and recent CSAIL affiliate Menghua Wu ’19, MEng ’20, PhD ’25. Their work was supported, in part, by the U.S. Defense Threat Reduction Agency and MIT Jameel Clinic.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202508/mit-vaxseer-barzilay-shi-00_1.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr" id="docs-internal-guid-2ef4ad7e-7fff-c28e-41c1-d61ce18592ae"&gt;Every year, global health experts are faced with a high-stakes decision: Which influenza strains should go into the next seasonal vaccine? The choice must be made months in advance, long before flu season even begins, and it can often feel like a race against the clock. If the selected strains match those that circulate, the vaccine will likely be highly effective. But if the prediction is off, protection can drop significantly, leading to (potentially preventable) illness and strain on health care systems.&lt;/p&gt;&lt;p dir="ltr"&gt;This challenge became even more familiar to scientists in the years during the Covid-19 pandemic. Think back to the time (and time and time again), when new variants emerged just as vaccines were being rolled out. Influenza behaves like a similar, rowdy cousin, mutating constantly and unpredictably. That makes it hard to stay ahead, and therefore harder to design vaccines that remain protective.&lt;/p&gt;&lt;p dir="ltr"&gt;To reduce this uncertainty, scientists at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and the MIT Abdul Latif Jameel Clinic for Machine Learning in Health set out to make vaccine selection more accurate and less reliant on guesswork. They created an AI system called VaxSeer, designed to predict dominant flu strains and identify the most protective vaccine candidates, months ahead of time. The tool uses deep learning models trained on decades of viral sequences and lab test results to simulate how the flu virus might evolve and how the vaccines will respond.&lt;/p&gt;&lt;p dir="ltr"&gt;Traditional evolution models often analyze the effect of single amino acid mutations independently. “VaxSeer adopts a large protein language model to learn the relationship between dominance and the combinatorial effects of mutations,” explains Wenxian Shi, a PhD student in MIT’s Department of Electrical Engineering and Computer Science, researcher at CSAIL, and lead author of a new paper on the work. “Unlike existing protein language models that assume a static distribution of viral variants, we model dynamic dominance shifts, making it better suited for rapidly evolving viruses like influenza.”&lt;/p&gt;&lt;p dir="ltr"&gt;An open-access report on the study was published today in &lt;em&gt;Nature Medicine.&lt;/em&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;The future of flu&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;VaxSeer has two core prediction engines: one that estimates how likely each viral strain is to spread (dominance), and another that estimates how effectively a vaccine will neutralize that strain (antigenicity). Together, they produce a predicted coverage score: a forward-looking measure of how well a given vaccine is likely to perform against future viruses.&lt;/p&gt;&lt;p dir="ltr"&gt;The scale of the score could be from an infinite negative to 0. The closer the score to 0, the better the antigenic match of vaccine strains to the circulating viruses. (You can imagine it as the negative of some kind of “distance.”)&lt;/p&gt;&lt;p dir="ltr"&gt;In a 10-year retrospective study, the researchers evaluated VaxSeer’s recommendations against those made by the World Health Organization (WHO) for two major flu subtypes: A/H3N2 and A/H1N1. For A/H3N2, VaxSeer’s choices outperformed the WHO’s in nine out of 10 seasons, based on retrospective empirical coverage scores (a surrogate metric of the vaccine effectiveness, calculated from the observed dominance from past seasons and experimental HI test results). The team used this to evaluate vaccine selections, as the effectiveness is only available for vaccines actually given to the population.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;For A/H1N1, it outperformed or matched the WHO in six out of 10 seasons. In one notable case, for the 2016 flu season, VaxSeer identified a strain that wasn’t chosen by the WHO until the following year. The model’s predictions also showed strong correlation with real-world vaccine effectiveness estimates, as reported by the CDC, Canada’s Sentinel Practitioner Surveillance Network, and Europe’s I-MOVE program. VaxSeer’s predicted coverage scores aligned closely with public health data on flu-related illnesses and medical visits prevented by vaccination.&lt;/p&gt;&lt;p dir="ltr"&gt;So how exactly does VaxSeer make sense of all these data? Intuitively, the model first estimates how rapidly a viral strain spreads over time using a protein language model, and then determines its dominance by accounting for competition among different strains.&lt;/p&gt;&lt;p dir="ltr"&gt;Once the model has calculated its insights, they’re plugged into a mathematical framework based on something called ordinary differential equations to simulate viral spread over time. For antigenicity, the system estimates how well a given vaccine strain will perform in a common lab test called the hemagglutination inhibition assay. This measures how effectively antibodies can&amp;nbsp;inhibit the virus from binding to human red blood cells, which is a widely used proxy for antigenic match/antigenicity.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Outpacing evolution&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;“By modeling how viruses evolve and how vaccines interact with them, AI tools like VaxSeer could help health officials make better, faster decisions — and stay one step ahead in the race between infection and immunity,” says Shi.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;VaxSeer currently focuses only on the flu virus’s HA (hemagglutinin) protein,the major antigen of influenza. Future versions could incorporate other proteins like NA (neuraminidase), and factors like immune history, manufacturing constraints, or dosage levels. Applying the system to other viruses would also require large, high-quality datasets that track both viral evolution and immune responses — data that aren’t always publicly available. The team, however is currently working on the methods that can predict viral evolution in low-data regimes building on relations between viral families&lt;/p&gt;&lt;p dir="ltr"&gt;“Given the speed of viral evolution, current therapeutic development often lags behind.&amp;nbsp;VaxSeer is our attempt to catch up,” says Regina Barzilay, the School of Engineering Distinguished Professor for AI and Health at MIT, AI lead of Jameel Clinic, and CSAIL principal investigator.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“This paper is impressive, but what excites me perhaps even more is the team’s ongoing work on predicting viral evolution in low-data settings,” says Assistant Professor Jon Stokes of the Department of Biochemistry and Biomedical Sciences at McMaster University in Hamilton, Ontario. “The implications go far beyond influenza. Imagine being able to anticipate how antibiotic-resistant bacteria or drug-resistant cancers might evolve, both of which can adapt rapidly. This kind of predictive modeling opens up a powerful new way of thinking about how diseases change, giving us the opportunity to stay one step ahead and design clinical interventions before escape becomes a major problem.”&lt;/p&gt;&lt;p dir="ltr"&gt;Shi and Barzilay wrote the paper with MIT CSAIL postdoc Jeremy Wohlwend ’16, MEng ’17, PhD ’25 and recent CSAIL affiliate Menghua Wu ’19, MEng ’20, PhD ’25. Their work was supported, in part, by the U.S. Defense Threat Reduction Agency and MIT Jameel Clinic.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/vaxseer-ai-tool-to-improve-flu-vaccine-strain-selection-0828</guid><pubDate>Thu, 28 Aug 2025 15:50:00 +0000</pubDate></item><item><title>[NEW] OpenAI–Anthropic cross-tests expose jailbreak and misuse risks — what enterprises must add to GPT-5 evaluations (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/openai-anthropic-cross-tests-expose-jailbreak-and-misuse-risks-what-enterprises-must-add-to-gpt-5-evaluations/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;OpenAI and Anthropic may often pit their foundation models against each other, but the two companies came together to evaluate each other’s public models to test alignment.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The companies said they believed that cross-evaluating accountability and safety would provide more transparency into what these powerful models could do, enabling enterprises to choose models that work best for them.&lt;/p&gt;



&lt;p&gt;“We believe this approach supports accountable and transparent evaluation, helping to ensure that each lab’s models continue to be tested against new and challenging scenarios,” OpenAI said in its findings.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Both companies found that reasoning models, such as OpenAI’s 03 and o4-mini and Claude 4 from Anthropic, resist jailbreaks, while general chat models like GPT-4.1 were susceptible to misuse. Evaluations like this can help enterprises identify the potential risks associated with these models, although it should be noted that GPT-5 is not part of the test.&amp;nbsp;&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;These safety and transparency alignment evaluations follow claims by users, primarily of ChatGPT, that OpenAI’s models have fallen prey to sycophancy and become overly deferential. OpenAI has since rolled back updates that caused sycophancy.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“We are primarily interested in understanding model propensities for harmful action,” Anthropic said in its report. “We aim to understand the most concerning actions that these models might try to take when given the opportunity, rather than focusing on the real-world likelihood of such opportunities arising or the probability that these actions would be successfully completed.”&lt;/p&gt;



&lt;p&gt;OpenAI noted the tests were designed to show how models interact in an intentionally difficult environment. The scenarios they built are mostly edge cases.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-reasoning-models-hold-on-to-alignment-nbsp"&gt;Reasoning models hold on to alignment&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;The tests covered only the publicly available models from both companies: Anthropic’s Claude 4 Opus and Claude 4 Sonnet, and OpenAI’s GPT-4o, GPT-4.1 o3 and o4-mini. Both companies relaxed the models’ external safeguards.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;OpenAI tested the public APIs for Claude models and defaulted to using Claude 4’s reasoning capabilities. Anthropic said they did not use OpenAI’s o3-pro because it was “not compatible with the API that our tooling best supports.”&lt;/p&gt;



&lt;p&gt;The goal of the tests was not to conduct an apples-to-apples comparison between models, but to determine how often large language models (LLMs) deviated from alignment. Both companies leveraged the SHADE-Arena sabotage evaluation framework, which showed Claude models had higher success rates at subtle sabotage.&lt;/p&gt;



&lt;p&gt;“These tests assess models’ orientations toward difficult or high-stakes situations in simulated settings — rather than ordinary use cases — and often involve long, many-turn interactions,” Anthropic reported. “This kind of evaluation is becoming a significant focus for our alignment science team since it is likely to catch behaviors that are less likely to appear in ordinary pre-deployment testing with real users.”&lt;/p&gt;



&lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcLbwj56_JpaPR3ztrYp3mkSBrjZN8CbntqmxhHV9klub8Q_YgqpjBaLFU0TWKeIRNVspEBVw6stibqyGCwGWEyJPuj_oIHOyg2TU7Us3pdqoD77AjzaCO_wsWx728JPDdYjsWRAA?key=4wIYfPcXmdrQx7-_2DhlgA" /&gt;&lt;/figure&gt;



&lt;p&gt;Anthropic said tests like these work better if organizations can compare notes, “since designing these scenarios involves an enormous number of degrees of freedom. No single research team can explore the full space of productive evaluation ideas alone.”&lt;/p&gt;



&lt;p&gt;The findings showed that generally, reasoning models performed robustly and can resist jailbreaking. OpenAI’s o3 was better aligned than Claude 4 Opus, but o4-mini along with GPT-4o and GPT-4.1 “often looked somewhat more concerning than either Claude model.” &lt;/p&gt;



&lt;p&gt;GPT-4o, GPT-4.1 and o4-mini also showed willingness to cooperate with human misuse and gave detailed instructions on how to create drugs, develop bioweapons and scarily, plan terrorist attacks. Both Claude models had higher rates of refusals, meaning the models refused to answer queries it did not know the answers to, to avoid hallucinations.&lt;/p&gt;



&lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdIMk3CDWGGRTRYsfMVQ5UoBYr4OB3uyjS_YI0dgJko5xoAj7w-CkozVZw6B3s9vPO8ER4-MiHF79zDw8QDIPfTggwdYubDHkBrXQW_ZYAuF3UZGybsIQ4F5yzB5e6B2yj2ZT7kTg?key=4wIYfPcXmdrQx7-_2DhlgA" /&gt;&lt;/figure&gt;



&lt;p&gt;Models from companies showed “concerning forms of sycophancy” and, at some point, validated harmful decisions of simulated users.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-enterprises-should-know"&gt;What enterprises should know&lt;/h2&gt;



&lt;p&gt;For enterprises, understanding the potential risks associated with models is invaluable. Model evaluations have become almost de rigueur for many organizations, with many testing and benchmarking frameworks now available.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Enterprises should continue to evaluate any model they use, and with GPT-5’s release, should keep in mind these guidelines to run their own safety evaluations:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Test both reasoning and non-reasoning models, because, while reasoning models showed greater resistance to misuse, they could still offer up hallucinations or other harmful behavior.&lt;/li&gt;



&lt;li&gt;Benchmark across vendors since models failed at different metrics.&lt;/li&gt;



&lt;li&gt;Stress test for misuse and syconphancy, and score both the refusal and the utility of those refuse to show the trade-offs between usefulness and guardrails.&lt;/li&gt;



&lt;li&gt;Continue to audit models even after deployment.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;While many evaluations focus on performance, third-party safety alignment tests do exist. For example, this one from Cyata. Last year, OpenAI released an alignment teaching method for its models called Rules-Based Rewards, while Anthropic launched auditing agents to check model safety.&amp;nbsp;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;OpenAI and Anthropic may often pit their foundation models against each other, but the two companies came together to evaluate each other’s public models to test alignment.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The companies said they believed that cross-evaluating accountability and safety would provide more transparency into what these powerful models could do, enabling enterprises to choose models that work best for them.&lt;/p&gt;



&lt;p&gt;“We believe this approach supports accountable and transparent evaluation, helping to ensure that each lab’s models continue to be tested against new and challenging scenarios,” OpenAI said in its findings.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Both companies found that reasoning models, such as OpenAI’s 03 and o4-mini and Claude 4 from Anthropic, resist jailbreaks, while general chat models like GPT-4.1 were susceptible to misuse. Evaluations like this can help enterprises identify the potential risks associated with these models, although it should be noted that GPT-5 is not part of the test.&amp;nbsp;&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;These safety and transparency alignment evaluations follow claims by users, primarily of ChatGPT, that OpenAI’s models have fallen prey to sycophancy and become overly deferential. OpenAI has since rolled back updates that caused sycophancy.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“We are primarily interested in understanding model propensities for harmful action,” Anthropic said in its report. “We aim to understand the most concerning actions that these models might try to take when given the opportunity, rather than focusing on the real-world likelihood of such opportunities arising or the probability that these actions would be successfully completed.”&lt;/p&gt;



&lt;p&gt;OpenAI noted the tests were designed to show how models interact in an intentionally difficult environment. The scenarios they built are mostly edge cases.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-reasoning-models-hold-on-to-alignment-nbsp"&gt;Reasoning models hold on to alignment&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;The tests covered only the publicly available models from both companies: Anthropic’s Claude 4 Opus and Claude 4 Sonnet, and OpenAI’s GPT-4o, GPT-4.1 o3 and o4-mini. Both companies relaxed the models’ external safeguards.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;OpenAI tested the public APIs for Claude models and defaulted to using Claude 4’s reasoning capabilities. Anthropic said they did not use OpenAI’s o3-pro because it was “not compatible with the API that our tooling best supports.”&lt;/p&gt;



&lt;p&gt;The goal of the tests was not to conduct an apples-to-apples comparison between models, but to determine how often large language models (LLMs) deviated from alignment. Both companies leveraged the SHADE-Arena sabotage evaluation framework, which showed Claude models had higher success rates at subtle sabotage.&lt;/p&gt;



&lt;p&gt;“These tests assess models’ orientations toward difficult or high-stakes situations in simulated settings — rather than ordinary use cases — and often involve long, many-turn interactions,” Anthropic reported. “This kind of evaluation is becoming a significant focus for our alignment science team since it is likely to catch behaviors that are less likely to appear in ordinary pre-deployment testing with real users.”&lt;/p&gt;



&lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcLbwj56_JpaPR3ztrYp3mkSBrjZN8CbntqmxhHV9klub8Q_YgqpjBaLFU0TWKeIRNVspEBVw6stibqyGCwGWEyJPuj_oIHOyg2TU7Us3pdqoD77AjzaCO_wsWx728JPDdYjsWRAA?key=4wIYfPcXmdrQx7-_2DhlgA" /&gt;&lt;/figure&gt;



&lt;p&gt;Anthropic said tests like these work better if organizations can compare notes, “since designing these scenarios involves an enormous number of degrees of freedom. No single research team can explore the full space of productive evaluation ideas alone.”&lt;/p&gt;



&lt;p&gt;The findings showed that generally, reasoning models performed robustly and can resist jailbreaking. OpenAI’s o3 was better aligned than Claude 4 Opus, but o4-mini along with GPT-4o and GPT-4.1 “often looked somewhat more concerning than either Claude model.” &lt;/p&gt;



&lt;p&gt;GPT-4o, GPT-4.1 and o4-mini also showed willingness to cooperate with human misuse and gave detailed instructions on how to create drugs, develop bioweapons and scarily, plan terrorist attacks. Both Claude models had higher rates of refusals, meaning the models refused to answer queries it did not know the answers to, to avoid hallucinations.&lt;/p&gt;



&lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdIMk3CDWGGRTRYsfMVQ5UoBYr4OB3uyjS_YI0dgJko5xoAj7w-CkozVZw6B3s9vPO8ER4-MiHF79zDw8QDIPfTggwdYubDHkBrXQW_ZYAuF3UZGybsIQ4F5yzB5e6B2yj2ZT7kTg?key=4wIYfPcXmdrQx7-_2DhlgA" /&gt;&lt;/figure&gt;



&lt;p&gt;Models from companies showed “concerning forms of sycophancy” and, at some point, validated harmful decisions of simulated users.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-enterprises-should-know"&gt;What enterprises should know&lt;/h2&gt;



&lt;p&gt;For enterprises, understanding the potential risks associated with models is invaluable. Model evaluations have become almost de rigueur for many organizations, with many testing and benchmarking frameworks now available.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Enterprises should continue to evaluate any model they use, and with GPT-5’s release, should keep in mind these guidelines to run their own safety evaluations:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Test both reasoning and non-reasoning models, because, while reasoning models showed greater resistance to misuse, they could still offer up hallucinations or other harmful behavior.&lt;/li&gt;



&lt;li&gt;Benchmark across vendors since models failed at different metrics.&lt;/li&gt;



&lt;li&gt;Stress test for misuse and syconphancy, and score both the refusal and the utility of those refuse to show the trade-offs between usefulness and guardrails.&lt;/li&gt;



&lt;li&gt;Continue to audit models even after deployment.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;While many evaluations focus on performance, third-party safety alignment tests do exist. For example, this one from Cyata. Last year, OpenAI released an alignment teaching method for its models called Rules-Based Rewards, while Anthropic launched auditing agents to check model safety.&amp;nbsp;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/openai-anthropic-cross-tests-expose-jailbreak-and-misuse-risks-what-enterprises-must-add-to-gpt-5-evaluations/</guid><pubDate>Thu, 28 Aug 2025 15:50:54 +0000</pubDate></item><item><title>[NEW] MathGPT.ai, the ‘cheat-proof’ tutor and teaching assistant, expands to over 50 institutions (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/28/mathgpt-the-cheat-proof-ai-tutor-and-teaching-assistant-expands-to-over-50-institutions/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As AI becomes more prevalent in the classroom — where students use it to complete assignments and teachers are uncertain about how to address it — an AI platform called MathGPT.ai launched last year with the goal of providing an “anti-cheating” tutor to college students and a teaching assistant to professors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Following a successful pilot program at 30 colleges and universities in the U.S., MathGPT.ai is preparing to nearly double its availability this fall, with hundreds of instructors planning to incorporate the tool. Schools implementing MathGPT.ai in their classrooms include Penn State University, Tufts University, and Liberty University, among others.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The most notable aspect of the platform is that its AI chatbot is trained to never directly give the answer, but instead ask students questions and provide support, much like a human tutor would. This technique, known as Socratic questioning, encourages students to think critically rather than simply memorizing answers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instructors, MathGPT.ai serves as a teaching assistant, generating questions and schoolwork based on uploaded textbooks and learning materials, as well as offering auto-grading capabilities and additional AI features.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;MathGPT.ai supports college-level math, including Algebra, Calculus, Trigonometry, and more.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3040563" height="314" src="https://techcrunch.com/wp-content/uploads/2025/08/Course-home-screenshot.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;MathGPT&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to the expansion, MathGPT.ai launched an upgraded version of its platform, introducing new features that give professors more control over how their students use the tools.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The main feature that sets MathGPT.ai apart from other AI companies is its instructor-centric approach. Recently, the platform has become even more focused on instructors’ needs. For example, instructors can now determine when students are allowed to interact with the chatbot. They can specify whether the AI should provide tutoring support for specific assignments while encouraging students to work independently on others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another new feature allows professors to set the number of attempts a student has to answer a question correctly. To promote a low-pressure learning environment, MathGPT.ai has also introduced unlimited practice questions for students. These questions don’t affect their score, allowing students to test their knowledge without stressing about grades.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additional features that MathGPT.ai offers to instructors include an optional requirement for students to upload images of their work. This enables professors to review submissions and verify the authenticity of the students’ work.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other recent updates include integrations with the three largest Learning Management Systems (LMS): Canvas, Blackboard, and Brightspace. It also added screen reader compatibility and an audio mode, making it more accessible to individuals with disabilities. The platform already offers closed captions for its summarized video lessons, which are notably AI-narrated to sound like historical figures like Ben Franklin and Albert Einstein.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company claims it complies with the Americans with Disabilities Act (ADA).&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3040562" height="641" src="https://techcrunch.com/wp-content/uploads/2025/08/MathGPTimage.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;MathGPT&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;While chatbots like Meta AI, Character.AI, and ChatGPT have faced criticism for inappropriate interactions with young users, MathGPT.ai says it has strict guardrails in place to ensure a safe learning environment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It will not have discussions with you about your girlfriend, boyfriend, or the meaning of life,” Peter Relan, the chairman of MathGPT.ai, told TechCrunch. “It will simply not engage. Because these freestanding chatbots will go in that direction, right? We are not here to entertain those kinds of conversations.” (Relan helped incubate Got It AI and was an early Discord investor.)&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s important to note that, like any chatbot, MathGPT.ai’s assistant still has the potential to produce inaccurate information. The chatbot has a disclosure at the bottom that warns the AI may make mistakes. Users can report the responses to the company if they believe the questions were answered incorrectly.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If you find a mistake, we will reward you with a gift card to tell us what it is. Year one, there were five [hallucinations]. Year two, there was one. So far [this year], none. So we take it very seriously,” Relan said, adding that MathGPT.ai has a team of human annotators to double-check every piece of work, textbook, and all other content to ensure “100% accuracy.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To continue its growth, the company plans to develop a mobile app in the future and expand to more subjects, such as chemistry, economics, and accounting.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;MathGPT.ai offers a free option, as well as a $25 per student per course option. The paid option includes several benefits, such as unlimited AI assignments and LMS integration.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As AI becomes more prevalent in the classroom — where students use it to complete assignments and teachers are uncertain about how to address it — an AI platform called MathGPT.ai launched last year with the goal of providing an “anti-cheating” tutor to college students and a teaching assistant to professors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Following a successful pilot program at 30 colleges and universities in the U.S., MathGPT.ai is preparing to nearly double its availability this fall, with hundreds of instructors planning to incorporate the tool. Schools implementing MathGPT.ai in their classrooms include Penn State University, Tufts University, and Liberty University, among others.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The most notable aspect of the platform is that its AI chatbot is trained to never directly give the answer, but instead ask students questions and provide support, much like a human tutor would. This technique, known as Socratic questioning, encourages students to think critically rather than simply memorizing answers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instructors, MathGPT.ai serves as a teaching assistant, generating questions and schoolwork based on uploaded textbooks and learning materials, as well as offering auto-grading capabilities and additional AI features.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;MathGPT.ai supports college-level math, including Algebra, Calculus, Trigonometry, and more.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3040563" height="314" src="https://techcrunch.com/wp-content/uploads/2025/08/Course-home-screenshot.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;MathGPT&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to the expansion, MathGPT.ai launched an upgraded version of its platform, introducing new features that give professors more control over how their students use the tools.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The main feature that sets MathGPT.ai apart from other AI companies is its instructor-centric approach. Recently, the platform has become even more focused on instructors’ needs. For example, instructors can now determine when students are allowed to interact with the chatbot. They can specify whether the AI should provide tutoring support for specific assignments while encouraging students to work independently on others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another new feature allows professors to set the number of attempts a student has to answer a question correctly. To promote a low-pressure learning environment, MathGPT.ai has also introduced unlimited practice questions for students. These questions don’t affect their score, allowing students to test their knowledge without stressing about grades.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additional features that MathGPT.ai offers to instructors include an optional requirement for students to upload images of their work. This enables professors to review submissions and verify the authenticity of the students’ work.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other recent updates include integrations with the three largest Learning Management Systems (LMS): Canvas, Blackboard, and Brightspace. It also added screen reader compatibility and an audio mode, making it more accessible to individuals with disabilities. The platform already offers closed captions for its summarized video lessons, which are notably AI-narrated to sound like historical figures like Ben Franklin and Albert Einstein.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company claims it complies with the Americans with Disabilities Act (ADA).&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3040562" height="641" src="https://techcrunch.com/wp-content/uploads/2025/08/MathGPTimage.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;MathGPT&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;While chatbots like Meta AI, Character.AI, and ChatGPT have faced criticism for inappropriate interactions with young users, MathGPT.ai says it has strict guardrails in place to ensure a safe learning environment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It will not have discussions with you about your girlfriend, boyfriend, or the meaning of life,” Peter Relan, the chairman of MathGPT.ai, told TechCrunch. “It will simply not engage. Because these freestanding chatbots will go in that direction, right? We are not here to entertain those kinds of conversations.” (Relan helped incubate Got It AI and was an early Discord investor.)&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s important to note that, like any chatbot, MathGPT.ai’s assistant still has the potential to produce inaccurate information. The chatbot has a disclosure at the bottom that warns the AI may make mistakes. Users can report the responses to the company if they believe the questions were answered incorrectly.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If you find a mistake, we will reward you with a gift card to tell us what it is. Year one, there were five [hallucinations]. Year two, there was one. So far [this year], none. So we take it very seriously,” Relan said, adding that MathGPT.ai has a team of human annotators to double-check every piece of work, textbook, and all other content to ensure “100% accuracy.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To continue its growth, the company plans to develop a mobile app in the future and expand to more subjects, such as chemistry, economics, and accounting.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;MathGPT.ai offers a free option, as well as a $25 per student per course option. The paid option includes several benefits, such as unlimited AI assignments and LMS integration.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/28/mathgpt-the-cheat-proof-ai-tutor-and-teaching-assistant-expands-to-over-50-institutions/</guid><pubDate>Thu, 28 Aug 2025 16:00:00 +0000</pubDate></item></channel></rss>