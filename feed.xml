<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 12 Nov 2025 06:34:20 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>Only 9% of developers think AI code can be used without human oversight, BairesDev survey reveals (AI | VentureBeat)</title><link>https://venturebeat.com/ai/only-9-of-developers-think-ai-code-can-be-used-without-human-oversight</link><description>[unable to retrieve full-text content]&lt;p&gt;Senior software developers are preparing for a major shift in how they work as artificial intelligence becomes central to their workflows, according to &lt;a href="https://www.bairesdev.com/"&gt;BairesDev’s&lt;/a&gt; latest &lt;i&gt;Dev Barometer&lt;/i&gt; report &lt;a href="https://www.bairesdev.com/blog/dev-barometer-q4-the-ai-native-workforce/"&gt;published today&lt;/a&gt;. VentureBeat was given an exclusive early look and the findings below come directly from that report. &lt;/p&gt;&lt;p&gt;The quarterly global survey, which polled 501 developers and 19 project managers across 92 software initiatives, finds that nearly two-thirds (65%) of senior developers expect their roles to be redefined by AI in 2026. &lt;/p&gt;&lt;p&gt;The data highlights a transformation underway in software development: fewer routine coding tasks, more emphasis on design and strategy, and a rising need for AI fluency.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;From Coders to Strategists&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Among those anticipating change, 74% say they expect to shift from hands-on coding to designing solutions. &lt;/p&gt;&lt;p&gt;Another 61% plan to integrate AI-generated code into their workflows, and half foresee spending more time on system strategy and architecture.&lt;/p&gt;&lt;p&gt;“It’s not about lines of code anymore,” said Justice Erolin, Chief Technology Officer at BairesDev, in a recent interview with &lt;i&gt;VentureBeat&lt;/i&gt; conducted over video call. “It’s about the quality and type of code, and the kind of work developers are doing.”&lt;/p&gt;&lt;p&gt;Erolin said the company is watching developers evolve from individual contributors into system thinkers.&lt;/p&gt;&lt;p&gt;“AI is great at code scaffolding and generating unit tests, saving developers around eight hours a week,” he explained. “That time can now be used for solution architecture and strategy work—areas where AI still falls short.”&lt;/p&gt;&lt;p&gt;The survey’s data reflects this shift. Developers are moving toward higher-value tasks while automation takes over much of the repetitive coding that once occupied junior engineers.&lt;/p&gt;&lt;p&gt;Erolin noted that BairesDev’s internal data mirrors these findings. “We’re seeing a shift where senior engineers with AI tools are outperforming, and even replacing, the traditional senior-plus-junior team setup,” he said.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Realism About AI’s Limits&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Despite widespread enthusiasm, developers remain cautious about AI’s reliability.&lt;/p&gt;&lt;p&gt;Over half (56%) describe AI-generated code as “somewhat reliable,” saying it still requires validation for accuracy and security.&lt;b&gt; Only 9%&lt;/b&gt; trust it enough to use without human oversight.&lt;/p&gt;&lt;p&gt;Erolin agreed with that sentiment. “AI doesn’t replace human oversight,” he said. “Even as tools improve, developers still need to understand how individual components fit into the bigger system.” &lt;/p&gt;&lt;p&gt;He added that the biggest constraint in large language models today is “their context window”—the limited ability to retain and reason across entire systems. “Engineers need to think holistically about architecture, not just individual lines of code,” he said.&lt;/p&gt;&lt;p&gt;The CTO described 2025 as a turning point for how engineers use AI tools like GitHub Copilot, Cursor, Claude, and OpenAI’s models. “We’re tracking what tools and models our engineers use,” he said. “But the bigger story is how those tools impact learning, productivity, and oversight.”&lt;/p&gt;&lt;p&gt;That tempered optimism aligns with BairesDev’s previous &lt;i&gt;Dev Barometer&lt;/i&gt; findings, which reported that 92% of developers were already using AI-assisted coding by Q3 2025, saving an average of 7.3 hours per week.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;A Year of Upskilling&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;In 2025, AI integration already brought tangible professional benefits. 74% of developers said the technology strengthened their technical skills, 50% reported better work-life balance, and 37% said AI tools expanded their career opportunities.&lt;/p&gt;&lt;p&gt;Erolin said the company is seeing AI emerge as “a top use case for upskilling.” Developers use it to “learn new technologies faster and fill knowledge gaps,” he noted. “When developers understand how AI works and its limitations, they can use it to enhance—not replace—their critical thinking. They prompt better and learn more efficiently.”&lt;/p&gt;&lt;p&gt;Still, he warned of a potential long-term risk in the industry’s current trajectory. “If junior engineers are being replaced or not hired, we’ll face a shortage of qualified senior engineers in ten years as current ones retire,” Erolin said.&lt;/p&gt;&lt;p&gt;The &lt;i&gt;Dev Barometer&lt;/i&gt; findings echo that concern. Developers expect leaner teams, but many also worry that fewer entry-level opportunities could lead to long-term talent pipeline issues.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Leaner Teams, New Priorities&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Developers expect 2026 to bring smaller, more specialized teams. 58% say automation will reduce entry-level tasks, while 63% expect new career paths to emerge as AI redefines team structures. 59% anticipate that AI will create entirely new specialized roles.&lt;/p&gt;&lt;p&gt;According to BairesDev’s data, developers currently divide their time between writing code (48%), debugging (42%), and documentation (35%). Only 19% report focusing primarily on creative problem-solving and innovation—a share that’s expected to grow as AI removes lower-level coding tasks.&lt;/p&gt;&lt;p&gt;The report also highlights where developers see the fastest-growing areas for 2026: AI/ML (67%), data analytics (46%), and cybersecurity (45%). In parallel, 63% of project managers said developers will need more training in AI, cloud, and security.&lt;/p&gt;&lt;p&gt;Erolin described the next generation of developers as “T-shaped engineers”—people with broad system knowledge and deep expertise in one or more areas. “The most important developer moving forward will be the T-shaped engineer,” he said. “Broad in understanding, deep in skill.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;AI as an Industry Standard&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The Q4 &lt;i&gt;Dev Barometer&lt;/i&gt; frames AI not as an experiment but as a foundation for how teams will operate in 2026. Developers are moving beyond using AI as a coding shortcut and instead incorporating it into architecture, validation, and design decisions.&lt;/p&gt;&lt;p&gt;Erolin emphasized that BairesDev is already adapting its internal teams to this new reality. “Our engineers are full-time with us, and we staff them out where they’re needed,” he said. “Some clients need help for six months to a year; others outsource their entire dev team to us.”&lt;/p&gt;&lt;p&gt;He said BairesDev provides “about 5,000 software engineers from Latin America, offering clients timezone-aligned, culturally aligned, and highly fluent English-speaking talent.”&lt;/p&gt;&lt;p&gt;As developers integrate AI deeper into their daily work, Erolin believes the competitive advantage will belong to those who understand both the technology’s capabilities and its constraints. “When developers learn to collaborate with AI instead of compete against it, that’s when the real productivity and creativity gains happen,” he said.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Background: Who BairesDev Is&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Founded in Buenos Aires in 2009 by Nacho De Marco and Paul Azorin, BairesDev began with a mission to connect what it describes as the “top 1%” of Latin American developers with global companies seeking high-quality software solutions. The company grew from those early roots into a major nearshore software development and staffing provider, offering everything from individual developer placements to full end-to-end project outsourcing.&lt;/p&gt;&lt;p&gt;Today, BairesDev claims to have delivered more than 1,200 projects across 130+ industries, serving hundreds of clients ranging from startups to Fortune 500 firms such as Google, Adobe, and Rolls-Royce. It operates with a remote-first model and a workforce of over 4,000 professionals across more than 40 countries, aligning its teams to North American time zones.&lt;/p&gt;&lt;p&gt;The company emphasizes three core advantages: access to elite technical talent across 100+ technologies, rapid scalability for project needs, and nearshore proximity for real-time collaboration. It reports client relationships averaging over three years and a satisfaction rate around 91%.&lt;/p&gt;&lt;p&gt;BairesDev’s unique position—bridging Latin American talent with global enterprise clients—gives it an unusually data-rich perspective on how AI is transforming software development at scale.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The Takeaway&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The &lt;i&gt;Dev Barometer&lt;/i&gt;’s Q4 2025 results suggest 2026 will mark a turning point for software engineering. Developers are becoming system architects rather than pure coders, AI literacy is becoming a baseline requirement, and traditional entry-level roles may give way to new, specialized positions.&lt;/p&gt;&lt;p&gt;As AI becomes embedded in every stage of development—from design to testing—developers who can combine technical fluency with strategic thinking are set to lead the next era of software creation.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Senior software developers are preparing for a major shift in how they work as artificial intelligence becomes central to their workflows, according to &lt;a href="https://www.bairesdev.com/"&gt;BairesDev’s&lt;/a&gt; latest &lt;i&gt;Dev Barometer&lt;/i&gt; report &lt;a href="https://www.bairesdev.com/blog/dev-barometer-q4-the-ai-native-workforce/"&gt;published today&lt;/a&gt;. VentureBeat was given an exclusive early look and the findings below come directly from that report. &lt;/p&gt;&lt;p&gt;The quarterly global survey, which polled 501 developers and 19 project managers across 92 software initiatives, finds that nearly two-thirds (65%) of senior developers expect their roles to be redefined by AI in 2026. &lt;/p&gt;&lt;p&gt;The data highlights a transformation underway in software development: fewer routine coding tasks, more emphasis on design and strategy, and a rising need for AI fluency.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;From Coders to Strategists&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Among those anticipating change, 74% say they expect to shift from hands-on coding to designing solutions. &lt;/p&gt;&lt;p&gt;Another 61% plan to integrate AI-generated code into their workflows, and half foresee spending more time on system strategy and architecture.&lt;/p&gt;&lt;p&gt;“It’s not about lines of code anymore,” said Justice Erolin, Chief Technology Officer at BairesDev, in a recent interview with &lt;i&gt;VentureBeat&lt;/i&gt; conducted over video call. “It’s about the quality and type of code, and the kind of work developers are doing.”&lt;/p&gt;&lt;p&gt;Erolin said the company is watching developers evolve from individual contributors into system thinkers.&lt;/p&gt;&lt;p&gt;“AI is great at code scaffolding and generating unit tests, saving developers around eight hours a week,” he explained. “That time can now be used for solution architecture and strategy work—areas where AI still falls short.”&lt;/p&gt;&lt;p&gt;The survey’s data reflects this shift. Developers are moving toward higher-value tasks while automation takes over much of the repetitive coding that once occupied junior engineers.&lt;/p&gt;&lt;p&gt;Erolin noted that BairesDev’s internal data mirrors these findings. “We’re seeing a shift where senior engineers with AI tools are outperforming, and even replacing, the traditional senior-plus-junior team setup,” he said.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Realism About AI’s Limits&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Despite widespread enthusiasm, developers remain cautious about AI’s reliability.&lt;/p&gt;&lt;p&gt;Over half (56%) describe AI-generated code as “somewhat reliable,” saying it still requires validation for accuracy and security.&lt;b&gt; Only 9%&lt;/b&gt; trust it enough to use without human oversight.&lt;/p&gt;&lt;p&gt;Erolin agreed with that sentiment. “AI doesn’t replace human oversight,” he said. “Even as tools improve, developers still need to understand how individual components fit into the bigger system.” &lt;/p&gt;&lt;p&gt;He added that the biggest constraint in large language models today is “their context window”—the limited ability to retain and reason across entire systems. “Engineers need to think holistically about architecture, not just individual lines of code,” he said.&lt;/p&gt;&lt;p&gt;The CTO described 2025 as a turning point for how engineers use AI tools like GitHub Copilot, Cursor, Claude, and OpenAI’s models. “We’re tracking what tools and models our engineers use,” he said. “But the bigger story is how those tools impact learning, productivity, and oversight.”&lt;/p&gt;&lt;p&gt;That tempered optimism aligns with BairesDev’s previous &lt;i&gt;Dev Barometer&lt;/i&gt; findings, which reported that 92% of developers were already using AI-assisted coding by Q3 2025, saving an average of 7.3 hours per week.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;A Year of Upskilling&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;In 2025, AI integration already brought tangible professional benefits. 74% of developers said the technology strengthened their technical skills, 50% reported better work-life balance, and 37% said AI tools expanded their career opportunities.&lt;/p&gt;&lt;p&gt;Erolin said the company is seeing AI emerge as “a top use case for upskilling.” Developers use it to “learn new technologies faster and fill knowledge gaps,” he noted. “When developers understand how AI works and its limitations, they can use it to enhance—not replace—their critical thinking. They prompt better and learn more efficiently.”&lt;/p&gt;&lt;p&gt;Still, he warned of a potential long-term risk in the industry’s current trajectory. “If junior engineers are being replaced or not hired, we’ll face a shortage of qualified senior engineers in ten years as current ones retire,” Erolin said.&lt;/p&gt;&lt;p&gt;The &lt;i&gt;Dev Barometer&lt;/i&gt; findings echo that concern. Developers expect leaner teams, but many also worry that fewer entry-level opportunities could lead to long-term talent pipeline issues.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Leaner Teams, New Priorities&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Developers expect 2026 to bring smaller, more specialized teams. 58% say automation will reduce entry-level tasks, while 63% expect new career paths to emerge as AI redefines team structures. 59% anticipate that AI will create entirely new specialized roles.&lt;/p&gt;&lt;p&gt;According to BairesDev’s data, developers currently divide their time between writing code (48%), debugging (42%), and documentation (35%). Only 19% report focusing primarily on creative problem-solving and innovation—a share that’s expected to grow as AI removes lower-level coding tasks.&lt;/p&gt;&lt;p&gt;The report also highlights where developers see the fastest-growing areas for 2026: AI/ML (67%), data analytics (46%), and cybersecurity (45%). In parallel, 63% of project managers said developers will need more training in AI, cloud, and security.&lt;/p&gt;&lt;p&gt;Erolin described the next generation of developers as “T-shaped engineers”—people with broad system knowledge and deep expertise in one or more areas. “The most important developer moving forward will be the T-shaped engineer,” he said. “Broad in understanding, deep in skill.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;AI as an Industry Standard&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The Q4 &lt;i&gt;Dev Barometer&lt;/i&gt; frames AI not as an experiment but as a foundation for how teams will operate in 2026. Developers are moving beyond using AI as a coding shortcut and instead incorporating it into architecture, validation, and design decisions.&lt;/p&gt;&lt;p&gt;Erolin emphasized that BairesDev is already adapting its internal teams to this new reality. “Our engineers are full-time with us, and we staff them out where they’re needed,” he said. “Some clients need help for six months to a year; others outsource their entire dev team to us.”&lt;/p&gt;&lt;p&gt;He said BairesDev provides “about 5,000 software engineers from Latin America, offering clients timezone-aligned, culturally aligned, and highly fluent English-speaking talent.”&lt;/p&gt;&lt;p&gt;As developers integrate AI deeper into their daily work, Erolin believes the competitive advantage will belong to those who understand both the technology’s capabilities and its constraints. “When developers learn to collaborate with AI instead of compete against it, that’s when the real productivity and creativity gains happen,” he said.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Background: Who BairesDev Is&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Founded in Buenos Aires in 2009 by Nacho De Marco and Paul Azorin, BairesDev began with a mission to connect what it describes as the “top 1%” of Latin American developers with global companies seeking high-quality software solutions. The company grew from those early roots into a major nearshore software development and staffing provider, offering everything from individual developer placements to full end-to-end project outsourcing.&lt;/p&gt;&lt;p&gt;Today, BairesDev claims to have delivered more than 1,200 projects across 130+ industries, serving hundreds of clients ranging from startups to Fortune 500 firms such as Google, Adobe, and Rolls-Royce. It operates with a remote-first model and a workforce of over 4,000 professionals across more than 40 countries, aligning its teams to North American time zones.&lt;/p&gt;&lt;p&gt;The company emphasizes three core advantages: access to elite technical talent across 100+ technologies, rapid scalability for project needs, and nearshore proximity for real-time collaboration. It reports client relationships averaging over three years and a satisfaction rate around 91%.&lt;/p&gt;&lt;p&gt;BairesDev’s unique position—bridging Latin American talent with global enterprise clients—gives it an unusually data-rich perspective on how AI is transforming software development at scale.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The Takeaway&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The &lt;i&gt;Dev Barometer&lt;/i&gt;’s Q4 2025 results suggest 2026 will mark a turning point for software engineering. Developers are becoming system architects rather than pure coders, AI literacy is becoming a baseline requirement, and traditional entry-level roles may give way to new, specialized positions.&lt;/p&gt;&lt;p&gt;As AI becomes embedded in every stage of development—from design to testing—developers who can combine technical fluency with strategic thinking are set to lead the next era of software creation.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/only-9-of-developers-think-ai-code-can-be-used-without-human-oversight</guid><pubDate>Tue, 11 Nov 2025 19:43:00 +0000</pubDate></item><item><title>SoftBank’s Nvidia sale rattles market, raises questions (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/11/softbanks-nvidia-sale-rattles-market-raises-questions/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/05/GettyImages-1142417807.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Masayoshi Son isn’t known for half measures. The SoftBank founder’s career has been studded with eyebrow-raising bets, each one seemingly more outrageous than the last. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His latest move is to cash out his entire $5.8 billion Nvidia stake to go all-in on AI. And while it surprised the business world on Tuesday, it maybe should not. At this point, it’s almost more surprising when the 68-year-old Son doesn’t push his chips to the center of the table.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Consider that during the late 1990s dot-com bubble, Son’s net worth soared to about $78 billion by February 2000, briefly making him the richest person in the world. Then came the ugly dot-com implosion months later. He lost $70 billion personally – which, at the time, was the largest financial loss by any individual in history — as SoftBank’s market cap plummeted 98% from $180 billion to just $2.5 billion.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amid that terribleness, Son made what would become his most legendary bet: a $20 million investment in Alibaba in 2000, one decided (the story goes) after just a six-minute meeting with Jack Ma. That stake would eventually grow to be worth $150 billion by 2020, transforming him into one of the venture industry’s most celebrated figures and funding his comeback.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That Alibaba success has often made it harder to see when Son has stayed too long at the table. When Son needed capital to launch his first Vision Fund in 2017, he didn’t hesitate to seek $45 billion from Saudi Arabia’s Public Investment Fund – long before taking Saudi money became acceptable in Silicon Valley. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After journalist Jamal Khashoggi was murdered in October 2018, Son condemned the killing as “horrific and deeply regrettable” but insisted SoftBank couldn’t “turn our backs on the Saudi people,” maintaining the firm’s commitment to managing the kingdom’s capital. In fact, the Vision Fund actually ramped up dealmaking soon after.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That didn’t turn out so well. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;A big bet on Uber generated paper losses for years. Then came WeWork. Son overrode his lieutenants’ objections, fell “in love” with founder Adam Neumann, and assigned the co-working company a dizzying valuation of $47 billion in early 2019 after making several previous investments in the company. But WeWork’s IPO plans collapsed after it published a famously troubling S-1 filing. The company never quite recovered – even after pushing out Neumann and instituting a series of belt-tightening measures – ultimately costing SoftBank $11.5 billion in equity losses and another $2.2 billion in debt. (Son reportedly later called it “a stain on my life.”)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Son has been mounting another comeback for years, and Tuesday will undoubtedly be remembered as an important moment in his turnaround tale. Indeed, it will likely be recalled as the day SoftBank revealed it had sold all 32.1 million of its Nvidia shares – not to diversify its bets but instead to double down elsewhere, including on a planned $30 billion commitment to OpenAI and to participate (it reportedly hopes) in a $1 trillion AI manufacturing hub in Arizona.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If selling that position still gives Son some heartburn, that’s understandable. At about $181.58 per share, SoftBank exited just 14% below Nvidia’s all-time high of $212.19, which is a strong look. That’s remarkably close to peak valuation for such a huge position. Still, the move marks SoftBank’s second complete exit from Nvidia, and the first one was exceedingly costly. (In 2019, SoftBank sold a $4 billion stake in the company for $3.6 billion, shares that would now be worth more than $150 billion.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The move also rattled the market. As of this writing, Nvidia shares are down nearly 3% following the disclosure, even as analysts emphasize that the sale “should not be seen as a cautious or negative stance on Nvidia,” but rather reflects SoftBank needing capital for its AI ambitions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Wall Street can’t help but wonder: does Son see something right now that others do not? Judging by his track record, maybe — and that ambiguity is all investors have to go on.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/05/GettyImages-1142417807.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Masayoshi Son isn’t known for half measures. The SoftBank founder’s career has been studded with eyebrow-raising bets, each one seemingly more outrageous than the last. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His latest move is to cash out his entire $5.8 billion Nvidia stake to go all-in on AI. And while it surprised the business world on Tuesday, it maybe should not. At this point, it’s almost more surprising when the 68-year-old Son doesn’t push his chips to the center of the table.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Consider that during the late 1990s dot-com bubble, Son’s net worth soared to about $78 billion by February 2000, briefly making him the richest person in the world. Then came the ugly dot-com implosion months later. He lost $70 billion personally – which, at the time, was the largest financial loss by any individual in history — as SoftBank’s market cap plummeted 98% from $180 billion to just $2.5 billion.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amid that terribleness, Son made what would become his most legendary bet: a $20 million investment in Alibaba in 2000, one decided (the story goes) after just a six-minute meeting with Jack Ma. That stake would eventually grow to be worth $150 billion by 2020, transforming him into one of the venture industry’s most celebrated figures and funding his comeback.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That Alibaba success has often made it harder to see when Son has stayed too long at the table. When Son needed capital to launch his first Vision Fund in 2017, he didn’t hesitate to seek $45 billion from Saudi Arabia’s Public Investment Fund – long before taking Saudi money became acceptable in Silicon Valley. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After journalist Jamal Khashoggi was murdered in October 2018, Son condemned the killing as “horrific and deeply regrettable” but insisted SoftBank couldn’t “turn our backs on the Saudi people,” maintaining the firm’s commitment to managing the kingdom’s capital. In fact, the Vision Fund actually ramped up dealmaking soon after.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That didn’t turn out so well. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;A big bet on Uber generated paper losses for years. Then came WeWork. Son overrode his lieutenants’ objections, fell “in love” with founder Adam Neumann, and assigned the co-working company a dizzying valuation of $47 billion in early 2019 after making several previous investments in the company. But WeWork’s IPO plans collapsed after it published a famously troubling S-1 filing. The company never quite recovered – even after pushing out Neumann and instituting a series of belt-tightening measures – ultimately costing SoftBank $11.5 billion in equity losses and another $2.2 billion in debt. (Son reportedly later called it “a stain on my life.”)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Son has been mounting another comeback for years, and Tuesday will undoubtedly be remembered as an important moment in his turnaround tale. Indeed, it will likely be recalled as the day SoftBank revealed it had sold all 32.1 million of its Nvidia shares – not to diversify its bets but instead to double down elsewhere, including on a planned $30 billion commitment to OpenAI and to participate (it reportedly hopes) in a $1 trillion AI manufacturing hub in Arizona.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If selling that position still gives Son some heartburn, that’s understandable. At about $181.58 per share, SoftBank exited just 14% below Nvidia’s all-time high of $212.19, which is a strong look. That’s remarkably close to peak valuation for such a huge position. Still, the move marks SoftBank’s second complete exit from Nvidia, and the first one was exceedingly costly. (In 2019, SoftBank sold a $4 billion stake in the company for $3.6 billion, shares that would now be worth more than $150 billion.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The move also rattled the market. As of this writing, Nvidia shares are down nearly 3% following the disclosure, even as analysts emphasize that the sale “should not be seen as a cautious or negative stance on Nvidia,” but rather reflects SoftBank needing capital for its AI ambitions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Wall Street can’t help but wonder: does Son see something right now that others do not? Judging by his track record, maybe — and that ambiguity is all investors have to go on.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/11/softbanks-nvidia-sale-rattles-market-raises-questions/</guid><pubDate>Tue, 11 Nov 2025 19:52:31 +0000</pubDate></item><item><title>Google says new cloud-based “Private AI Compute” is just as secure as local processing (AI – Ars Technica)</title><link>https://arstechnica.com/google/2025/11/google-says-new-cloud-based-private-ai-compute-is-just-as-secure-as-local-processing/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        New system allows devices to connect directly to secure space in Google’s AI servers.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="361" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google_Private_Inference-640x361.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google_Private_Inference-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google’s current mission is to weave generative AI into as many products as it can, getting everyone accustomed to, and maybe even dependent on, working with confabulatory robots. That means it needs to feed the bots a lot of your data, and that’s getting easier with the company’s new Private AI Compute. Google claims its new secure cloud environment will power better AI experiences without sacrificing your privacy.&lt;/p&gt;
&lt;p&gt;The pitch sounds a lot like Apple’s Private Cloud Compute. Google’s Private AI Compute runs on “one seamless Google stack” powered by the company’s custom Tensor Processing Units (TPUs). These chips have integrated secure elements, and the new system allows devices to connect directly to the protected space via an encrypted link.&lt;/p&gt;
&lt;p&gt;Google’s TPUs rely on an AMD-based Trusted Execution Environment (TEE) that encrypts and isolates memory from the host. Theoretically, that means no one else—not even Google itself—can access your data. Google says independent analysis by NCC Group shows that Private AI Compute meets its strict privacy guidelines.&lt;/p&gt;
&lt;p&gt;According to Google, the Private AI Compute service is just as secure as using local processing on your device. However, Google’s cloud has a lot more processing power than your laptop or phone, enabling the use of Google’s largest and most capable Gemini models.&lt;/p&gt;
&lt;h2&gt;Edge vs. Cloud&lt;/h2&gt;
&lt;p&gt;As Google has added more AI features to devices like Pixel phones, it has talked up the power of its on-device neural processing units (NPUs). Pixels and a few other phones run Gemini Nano models, allowing the phone to process AI workloads securely on “the edge” without sending any of your data to the Internet. With the release of the Pixel 10, Google upgraded Gemini Nano to handle even more data with the help of researchers from DeepMind.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;NPUs can’t do it all, though. While Gemini Nano is getting more capable, it can’t compete with models that run on massive, high-wattage servers. That might be why some AI features, like the temporarily unavailable Daily Brief, don’t do much on the Pixels. Magic Cue, which surfaces personal data based on screen context, is probably in a similar place. Google now says that Magic Cue will get “even more helpful” thanks to the Private AI Compute system.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2114347 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Pixel 10 flat" class="fullwidth full" height="1080" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/Google-Pixel-10-5.jpg" width="1920" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Magic Cue debuted on the Pixel 10, but it doesn’t do much yet.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Google has also released a Pixel feature drop today, but there aren’t many new features of note (unless you’ve been hankering for Wicked themes). As part of the update, Magic Cue will begin using the Private AI Compute system to generate suggestions. The more powerful model &lt;em&gt;might&lt;/em&gt; be able to tease out more actionable details from your data. Google also notes the Recorder app will be able to summarize in more languages thanks to the secure cloud.&lt;/p&gt;
&lt;p&gt;So what Google is saying here is that more of your data is being offloaded to the cloud so that Magic Cue can generate useful suggestions, which would be a change. Since launch, we’ve only seen Magic Cue appear a handful of times, and it’s not offering anything interesting when it does.&lt;/p&gt;
&lt;p&gt;There are still reasons to use local AI, even if the cloud system has “the same security and privacy assurances,” as Google claims. An NPU offers superior latency because your data doesn’t have to go anywhere, and it’s more reliable, as AI features will still work without an Internet connection. Google believes this hybrid approach is the way forward for generative AI, which requires significant processing even for seemingly simple tasks. We can expect to see more AI features reaching out to Google’s secure cloud soon.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        New system allows devices to connect directly to secure space in Google’s AI servers.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="361" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google_Private_Inference-640x361.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google_Private_Inference-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google’s current mission is to weave generative AI into as many products as it can, getting everyone accustomed to, and maybe even dependent on, working with confabulatory robots. That means it needs to feed the bots a lot of your data, and that’s getting easier with the company’s new Private AI Compute. Google claims its new secure cloud environment will power better AI experiences without sacrificing your privacy.&lt;/p&gt;
&lt;p&gt;The pitch sounds a lot like Apple’s Private Cloud Compute. Google’s Private AI Compute runs on “one seamless Google stack” powered by the company’s custom Tensor Processing Units (TPUs). These chips have integrated secure elements, and the new system allows devices to connect directly to the protected space via an encrypted link.&lt;/p&gt;
&lt;p&gt;Google’s TPUs rely on an AMD-based Trusted Execution Environment (TEE) that encrypts and isolates memory from the host. Theoretically, that means no one else—not even Google itself—can access your data. Google says independent analysis by NCC Group shows that Private AI Compute meets its strict privacy guidelines.&lt;/p&gt;
&lt;p&gt;According to Google, the Private AI Compute service is just as secure as using local processing on your device. However, Google’s cloud has a lot more processing power than your laptop or phone, enabling the use of Google’s largest and most capable Gemini models.&lt;/p&gt;
&lt;h2&gt;Edge vs. Cloud&lt;/h2&gt;
&lt;p&gt;As Google has added more AI features to devices like Pixel phones, it has talked up the power of its on-device neural processing units (NPUs). Pixels and a few other phones run Gemini Nano models, allowing the phone to process AI workloads securely on “the edge” without sending any of your data to the Internet. With the release of the Pixel 10, Google upgraded Gemini Nano to handle even more data with the help of researchers from DeepMind.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;NPUs can’t do it all, though. While Gemini Nano is getting more capable, it can’t compete with models that run on massive, high-wattage servers. That might be why some AI features, like the temporarily unavailable Daily Brief, don’t do much on the Pixels. Magic Cue, which surfaces personal data based on screen context, is probably in a similar place. Google now says that Magic Cue will get “even more helpful” thanks to the Private AI Compute system.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2114347 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Pixel 10 flat" class="fullwidth full" height="1080" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/Google-Pixel-10-5.jpg" width="1920" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Magic Cue debuted on the Pixel 10, but it doesn’t do much yet.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Google has also released a Pixel feature drop today, but there aren’t many new features of note (unless you’ve been hankering for Wicked themes). As part of the update, Magic Cue will begin using the Private AI Compute system to generate suggestions. The more powerful model &lt;em&gt;might&lt;/em&gt; be able to tease out more actionable details from your data. Google also notes the Recorder app will be able to summarize in more languages thanks to the secure cloud.&lt;/p&gt;
&lt;p&gt;So what Google is saying here is that more of your data is being offloaded to the cloud so that Magic Cue can generate useful suggestions, which would be a change. Since launch, we’ve only seen Magic Cue appear a handful of times, and it’s not offering anything interesting when it does.&lt;/p&gt;
&lt;p&gt;There are still reasons to use local AI, even if the cloud system has “the same security and privacy assurances,” as Google claims. An NPU offers superior latency because your data doesn’t have to go anywhere, and it’s more reliable, as AI features will still work without an Internet connection. Google believes this hybrid approach is the way forward for generative AI, which requires significant processing even for seemingly simple tasks. We can expect to see more AI features reaching out to Google’s secure cloud soon.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2025/11/google-says-new-cloud-based-private-ai-compute-is-just-as-secure-as-local-processing/</guid><pubDate>Tue, 11 Nov 2025 21:34:10 +0000</pubDate></item><item><title>Meta’s SPICE framework lets AI systems teach themselves to reason (AI | VentureBeat)</title><link>https://venturebeat.com/ai/metas-spice-framework-lets-ai-systems-teach-themselves-to-reason</link><description>[unable to retrieve full-text content]&lt;p&gt;Researchers at &lt;a href="https://ai.meta.com/research/"&gt;Meta FAIR&lt;/a&gt; and the &lt;a href="https://nus.edu.sg/"&gt;National University of Singapore&lt;/a&gt; have developed a new reinforcement learning framework for self-improving AI systems. &lt;/p&gt;&lt;p&gt;Called &lt;a href="https://arxiv.org/abs/2510.24684"&gt;Self-Play In Corpus Environments (SPICE)&lt;/a&gt;, the framework pits two AI agents against each other, creating its own challenges and gradually improving without human supervision.&lt;/p&gt;&lt;p&gt;While currently a proof-of-concept, this self-play mechanism could provide a basis for future AI systems that can dynamically adapt to their environments, making them more robust against the unpredictability of real-world applications.&lt;/p&gt;&lt;h2&gt;The challenge of self-improving AI&lt;/h2&gt;&lt;p&gt;The goal of self-improving AI is to create systems that can &lt;a href="https://venturebeat.com/ai/the-era-of-experience-will-unleash-self-learning-ai-agents-across-the-web-heres-how-to-prepare"&gt;enhance their capabilities by interacting with their environment&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;A common approach is reinforcement learning with verifiable rewards (RLVR), where models are rewarded for providing the correct answers to problems. This is often limited by its reliance on human-curated problem sets and domain-specific reward engineering, which makes it difficult to scale.&lt;/p&gt;&lt;p&gt;Self-play, where a model improves by competing against itself, is another promising paradigm. But existing self-play methods for language models are often limited by two critical factors. &lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;F&lt;!-- --&gt;actual errors in generated questions and answers compound, leading to a feedback loop of hallucinations. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;When the problem generator and solver have information symmetry (i.e., share the same knowledge base) they fail to generate genuinely new challenges and fall into repetitive patterns. &lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;As the researchers note in their paper, “These systematic empirical failures indicate that self-improvement requires interaction with an external source providing diverse, verifiable feedback, rather than closed-loop pure introspection.”&lt;/p&gt;&lt;h2&gt;How SPICE works&lt;/h2&gt;&lt;p&gt;SPICE is a self-play framework where a single model acts in two distinct roles. &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;A &amp;quot;Challenger&amp;quot; constructs a curriculum of challenging problems from a large corpus of documents. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A &amp;quot;Reasoner&amp;quot; then attempts to solve these problems without access to the source documents. &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This setup breaks the information symmetry that limits other self-play methods, as the Reasoner does not have access to the documents and knowledge that the Challenger uses to generate the problems.&lt;/p&gt;&lt;p&gt;Grounding the tasks in a vast and diverse corpus of documents prevents hallucination by anchoring questions and answers in real-world content. This is important because for AI systems to reliably self-improve, they need external grounding sources. Therefore, LLM agents should learn from interactions with humans and the real world, not just their own outputs, to avoid compounding errors.&lt;/p&gt;&lt;p&gt;The adversarial dynamic between the two roles creates an automatic curriculum. &lt;/p&gt;&lt;p&gt;The Challenger is rewarded for generating problems that are both diverse and at the frontier of the Reasoner&amp;#x27;s capability (not too easy and also not impossible). &lt;/p&gt;&lt;p&gt;The Reasoner is rewarded for answering correctly. This symbiotic interaction pushes both agents to continuously discover and overcome new challenges. &lt;/p&gt;&lt;p&gt;Because the system uses raw documents instead of pre-defined question-answer pairs, it can generate diverse task formats, such as multiple-choice and free-form questions. &lt;/p&gt;&lt;p&gt;This flexibility allows SPICE to be applied to any domain, breaking the bottleneck that has confined previous methods to narrow fields like math and code. It also reduces dependence on expensive human-curated datasets for specialized domains like legal or medical analysis.&lt;/p&gt;&lt;h2&gt;SPICE in action&lt;/h2&gt;&lt;p&gt;The researchers evaluated SPICE on several base models, including &lt;a href="https://venturebeat.com/ai/alibaba-launches-open-source-qwen3-model-that-surpasses-openai-o1-and-deepseek-r1"&gt;Qwen3-4B-Base&lt;/a&gt; and &lt;a href="https://github.com/GAIR-NLP/OctoThinker"&gt;OctoThinker-3B-Hybrid-Base&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;They compared its performance against baselines such as the base model with no training, a Reasoner model trained with a fixed &amp;quot;Strong Challenger&amp;quot; (Qwen3-32B-Instruct), and pure self-play methods like R-Zero and Absolute Zero. The evaluation covered a wide range of mathematical and general reasoning benchmarks.&lt;/p&gt;&lt;p&gt;Across all models, SPICE consistently outperformed the baselines, delivering significant improvements in both mathematical and general reasoning tasks. &lt;/p&gt;&lt;p&gt;The results show that the reasoning capabilities developed through corpus-grounded self-play transfer broadly across different models, thanks to the diverse external knowledge corpus they used.&lt;/p&gt;&lt;p&gt;A key finding is that the adversarial dynamic creates an effective automatic curriculum. As training progresses, the Challenger learns to generate increasingly difficult problems. &lt;/p&gt;&lt;p&gt;In one experiment, the Reasoner&amp;#x27;s pass rate on a fixed set of problems increased from 55% to 85% over time, showing its improved capabilities. &lt;/p&gt;&lt;p&gt;Meanwhile, later versions of the Challenger were able to generate questions that dropped the pass rate of an early-stage Reasoner from 55% to 35%, confirming that both roles co-evolve successfully.&lt;/p&gt;&lt;p&gt;The researchers conclude that this approach presents a paradigm shift in self-improving reasoning methods from “closed-loop self-play that often stagnates due to hallucination drift, to open-ended improvement through interaction with the vast, verifiable knowledge embedded in web document corpora.”&lt;/p&gt;&lt;p&gt;Currently, the corpus used for SPICE represents human experience captured in text. The ultimate goal is for self-improving systems to generate questions based on interactions with reality, including the physical world, the internet, and human interactions across multiple modalities like video, audio, and sensor data.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Researchers at &lt;a href="https://ai.meta.com/research/"&gt;Meta FAIR&lt;/a&gt; and the &lt;a href="https://nus.edu.sg/"&gt;National University of Singapore&lt;/a&gt; have developed a new reinforcement learning framework for self-improving AI systems. &lt;/p&gt;&lt;p&gt;Called &lt;a href="https://arxiv.org/abs/2510.24684"&gt;Self-Play In Corpus Environments (SPICE)&lt;/a&gt;, the framework pits two AI agents against each other, creating its own challenges and gradually improving without human supervision.&lt;/p&gt;&lt;p&gt;While currently a proof-of-concept, this self-play mechanism could provide a basis for future AI systems that can dynamically adapt to their environments, making them more robust against the unpredictability of real-world applications.&lt;/p&gt;&lt;h2&gt;The challenge of self-improving AI&lt;/h2&gt;&lt;p&gt;The goal of self-improving AI is to create systems that can &lt;a href="https://venturebeat.com/ai/the-era-of-experience-will-unleash-self-learning-ai-agents-across-the-web-heres-how-to-prepare"&gt;enhance their capabilities by interacting with their environment&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;A common approach is reinforcement learning with verifiable rewards (RLVR), where models are rewarded for providing the correct answers to problems. This is often limited by its reliance on human-curated problem sets and domain-specific reward engineering, which makes it difficult to scale.&lt;/p&gt;&lt;p&gt;Self-play, where a model improves by competing against itself, is another promising paradigm. But existing self-play methods for language models are often limited by two critical factors. &lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;F&lt;!-- --&gt;actual errors in generated questions and answers compound, leading to a feedback loop of hallucinations. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;When the problem generator and solver have information symmetry (i.e., share the same knowledge base) they fail to generate genuinely new challenges and fall into repetitive patterns. &lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;As the researchers note in their paper, “These systematic empirical failures indicate that self-improvement requires interaction with an external source providing diverse, verifiable feedback, rather than closed-loop pure introspection.”&lt;/p&gt;&lt;h2&gt;How SPICE works&lt;/h2&gt;&lt;p&gt;SPICE is a self-play framework where a single model acts in two distinct roles. &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;A &amp;quot;Challenger&amp;quot; constructs a curriculum of challenging problems from a large corpus of documents. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A &amp;quot;Reasoner&amp;quot; then attempts to solve these problems without access to the source documents. &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This setup breaks the information symmetry that limits other self-play methods, as the Reasoner does not have access to the documents and knowledge that the Challenger uses to generate the problems.&lt;/p&gt;&lt;p&gt;Grounding the tasks in a vast and diverse corpus of documents prevents hallucination by anchoring questions and answers in real-world content. This is important because for AI systems to reliably self-improve, they need external grounding sources. Therefore, LLM agents should learn from interactions with humans and the real world, not just their own outputs, to avoid compounding errors.&lt;/p&gt;&lt;p&gt;The adversarial dynamic between the two roles creates an automatic curriculum. &lt;/p&gt;&lt;p&gt;The Challenger is rewarded for generating problems that are both diverse and at the frontier of the Reasoner&amp;#x27;s capability (not too easy and also not impossible). &lt;/p&gt;&lt;p&gt;The Reasoner is rewarded for answering correctly. This symbiotic interaction pushes both agents to continuously discover and overcome new challenges. &lt;/p&gt;&lt;p&gt;Because the system uses raw documents instead of pre-defined question-answer pairs, it can generate diverse task formats, such as multiple-choice and free-form questions. &lt;/p&gt;&lt;p&gt;This flexibility allows SPICE to be applied to any domain, breaking the bottleneck that has confined previous methods to narrow fields like math and code. It also reduces dependence on expensive human-curated datasets for specialized domains like legal or medical analysis.&lt;/p&gt;&lt;h2&gt;SPICE in action&lt;/h2&gt;&lt;p&gt;The researchers evaluated SPICE on several base models, including &lt;a href="https://venturebeat.com/ai/alibaba-launches-open-source-qwen3-model-that-surpasses-openai-o1-and-deepseek-r1"&gt;Qwen3-4B-Base&lt;/a&gt; and &lt;a href="https://github.com/GAIR-NLP/OctoThinker"&gt;OctoThinker-3B-Hybrid-Base&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;They compared its performance against baselines such as the base model with no training, a Reasoner model trained with a fixed &amp;quot;Strong Challenger&amp;quot; (Qwen3-32B-Instruct), and pure self-play methods like R-Zero and Absolute Zero. The evaluation covered a wide range of mathematical and general reasoning benchmarks.&lt;/p&gt;&lt;p&gt;Across all models, SPICE consistently outperformed the baselines, delivering significant improvements in both mathematical and general reasoning tasks. &lt;/p&gt;&lt;p&gt;The results show that the reasoning capabilities developed through corpus-grounded self-play transfer broadly across different models, thanks to the diverse external knowledge corpus they used.&lt;/p&gt;&lt;p&gt;A key finding is that the adversarial dynamic creates an effective automatic curriculum. As training progresses, the Challenger learns to generate increasingly difficult problems. &lt;/p&gt;&lt;p&gt;In one experiment, the Reasoner&amp;#x27;s pass rate on a fixed set of problems increased from 55% to 85% over time, showing its improved capabilities. &lt;/p&gt;&lt;p&gt;Meanwhile, later versions of the Challenger were able to generate questions that dropped the pass rate of an early-stage Reasoner from 55% to 35%, confirming that both roles co-evolve successfully.&lt;/p&gt;&lt;p&gt;The researchers conclude that this approach presents a paradigm shift in self-improving reasoning methods from “closed-loop self-play that often stagnates due to hallucination drift, to open-ended improvement through interaction with the vast, verifiable knowledge embedded in web document corpora.”&lt;/p&gt;&lt;p&gt;Currently, the corpus used for SPICE represents human experience captured in text. The ultimate goal is for self-improving systems to generate questions based on interactions with reality, including the physical world, the internet, and human interactions across multiple modalities like video, audio, and sensor data.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/metas-spice-framework-lets-ai-systems-teach-themselves-to-reason</guid><pubDate>Tue, 11 Nov 2025 22:21:00 +0000</pubDate></item><item><title>Baidu just dropped an open-source multimodal AI that it claims beats GPT-5 and Gemini (AI | VentureBeat)</title><link>https://venturebeat.com/ai/baidu-just-dropped-an-open-source-multimodal-ai-that-it-claims-beats-gpt-5</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://www.baidu.com/"&gt;&lt;u&gt;Baidu Inc.&lt;/u&gt;&lt;/a&gt;, China&amp;#x27;s largest search engine company, released a new artificial intelligence model on Monday that its developers claim outperforms competitors from &lt;a href="https://www.google.com/"&gt;&lt;u&gt;Google&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://openai.com/"&gt;&lt;u&gt;OpenAI&lt;/u&gt;&lt;/a&gt; on several vision-related benchmarks despite using a fraction of the computing resources typically required for such systems.&lt;/p&gt;&lt;p&gt;The model, dubbed &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/u&gt;&lt;/a&gt;, is the latest salvo in an escalating competition among technology companies to build AI systems that can understand and reason about images, videos, and documents alongside traditional text — capabilities increasingly critical for enterprise applications ranging from automated document processing to industrial quality control.&lt;/p&gt;&lt;p&gt;What sets Baidu&amp;#x27;s release apart is its efficiency: the model activates just 3 billion parameters during operation while maintaining 28 billion total parameters through a sophisticated routing architecture. According to documentation released with the model, this design allows it to match or exceed the performance of much larger competing systems on tasks involving document understanding, chart analysis, and visual reasoning while consuming significantly less computational power and memory.&lt;/p&gt;&lt;p&gt;&amp;quot;Built upon the powerful ERNIE-4.5-VL-28B-A3B architecture, the newly upgraded ERNIE-4.5-VL-28B-A3B-Thinking achieves a remarkable leap forward in multimodal reasoning capabilities,&amp;quot; Baidu wrote in the model&amp;#x27;s &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;technical documentation&lt;/u&gt;&lt;/a&gt; on Hugging Face, the AI model repository where the system was released.&lt;/p&gt;&lt;p&gt;The company said the model underwent &amp;quot;an extensive mid-training phase&amp;quot; that incorporated &amp;quot;a vast and highly diverse corpus of premium visual-language reasoning data,&amp;quot; dramatically boosting its ability to align visual and textual information semantically.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;How the model mimics human visual problem-solving through dynamic image analysis&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Perhaps the model&amp;#x27;s most distinctive feature is what Baidu calls &amp;quot;&lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;Thinking with Images&lt;/u&gt;&lt;/a&gt;&amp;quot; — a capability that allows the AI to dynamically zoom in and out of images to examine fine-grained details, mimicking how humans approach visual problem-solving tasks.&lt;/p&gt;&lt;p&gt;&amp;quot;The model thinks like a human, capable of freely zooming in and out of images to grasp every detail and uncover all information,&amp;quot; according to the model card. When paired with tools like image search, Baidu claims this feature &amp;quot;dramatically elevates the model&amp;#x27;s ability to process fine-grained details and handle long-tail visual knowledge.&amp;quot;&lt;/p&gt;&lt;p&gt;This approach marks a departure from traditional vision-language models, which typically process images at a fixed resolution. By allowing dynamic image examination, the system can theoretically handle scenarios requiring both broad context and granular detail—such as analyzing complex technical diagrams or detecting subtle defects in manufacturing quality control.&lt;/p&gt;&lt;p&gt;The model also supports what Baidu describes as enhanced &amp;quot;visual grounding&amp;quot; capabilities with &amp;quot;more precise grounding and flexible instruction execution, easily triggering grounding functions in complex industrial scenarios,&amp;quot; suggesting potential applications in robotics, warehouse automation, and other settings where AI systems must identify and locate specific objects in visual scenes.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Baidu&amp;#x27;s performance claims draw scrutiny as independent testing remains pending&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Baidu&amp;#x27;s assertion that the model outperforms Google&amp;#x27;s &lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/"&gt;&lt;u&gt;Gemini 2.5 Pro&lt;/u&gt;&lt;/a&gt; and OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-gpt-5/"&gt;&lt;u&gt;GPT-5-High&lt;/u&gt;&lt;/a&gt; on various document and chart understanding benchmarks has drawn attention across social media, though independent verification of these claims remains pending.&lt;/p&gt;&lt;p&gt;The company released the model under the permissive &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking#license"&gt;&lt;u&gt;Apache 2.0 license&lt;/u&gt;&lt;/a&gt;, allowing unrestricted commercial use—a strategic decision that contrasts with the more restrictive licensing approaches of some competitors and could accelerate enterprise adoption.&lt;/p&gt;&lt;p&gt;&amp;quot;&lt;a href="https://x.com/orionintx/status/1988214142030037115"&gt;&lt;u&gt;Apache 2.0 is smart&lt;/u&gt;&lt;/a&gt;,&amp;quot; wrote one X user responding to Baidu&amp;#x27;s announcement, highlighting the competitive advantage of open licensing in the enterprise market.&lt;/p&gt;&lt;p&gt;According to Baidu&amp;#x27;s documentation, the model demonstrates six core capabilities beyond traditional text processing. In visual reasoning, the system can perform what Baidu describes as &amp;quot;multi-step reasoning, chart analysis, and causal reasoning capabilities in complex visual tasks,&amp;quot; aided by what the company characterizes as &amp;quot;large-scale reinforcement learning.&amp;quot; &lt;/p&gt;&lt;p&gt;For STEM problem solving, Baidu claims that &amp;quot;leveraging its powerful visual abilities, the model achieves a leap in performance on STEM tasks like solving problems from photos.&amp;quot; The visual grounding capability allows the model to identify and locate objects within images with what Baidu characterizes as industrial-grade precision. Through tool integration, the system can invoke external functions including image search capabilities to access information beyond its training data.&lt;/p&gt;&lt;p&gt;For video understanding, Baidu claims the model possesses &amp;quot;outstanding temporal awareness and event localization abilities, accurately identifying content changes across different time segments in a video.&amp;quot; Finally, the thinking with images feature enables the dynamic zoom functionality that distinguishes this model from competitors.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Inside the mixture-of-experts architecture that powers efficient multimodal processing&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Under the hood, &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/u&gt;&lt;/a&gt; employs a &lt;a href="https://huggingface.co/blog/moe"&gt;&lt;u&gt;Mixture-of-Experts (MoE) architecture&lt;/u&gt;&lt;/a&gt; — a design pattern that has become increasingly popular for building efficient large-scale AI systems. Rather than activating all 28 billion parameters for every task, the model uses a routing mechanism to selectively activate only the 3 billion parameters most relevant to each specific input.&lt;/p&gt;&lt;p&gt;This approach offers substantial practical advantages for enterprise deployments. According to Baidu&amp;#x27;s documentation, the model can run on a single 80GB GPU — hardware readily available in many corporate data centers — making it significantly more accessible than competing systems that may require multiple high-end accelerators.&lt;/p&gt;&lt;p&gt;The technical documentation reveals that Baidu employed several advanced training techniques to achieve the model&amp;#x27;s capabilities. The company used &amp;quot;cutting-edge multimodal reinforcement learning techniques on verifiable tasks, integrating GSPO and IcePop strategies to stabilize MoE training combined with dynamic difficulty sampling for exceptional learning efficiency.&amp;quot;&lt;/p&gt;&lt;p&gt;Baidu also notes that in response to &amp;quot;strong community demand,&amp;quot; the company &amp;quot;significantly strengthened the model&amp;#x27;s grounding performance with improved instruction-following capabilities.&amp;quot;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The new model fits into Baidu&amp;#x27;s ambitious multimodal AI ecosystem&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The new release is one component of Baidu&amp;#x27;s broader &lt;a href="https://ernie.baidu.com/blog/posts/ernie4.5/"&gt;&lt;u&gt;ERNIE 4.5 model family&lt;/u&gt;&lt;/a&gt;, which the company unveiled in June 2025. That family comprises 10 distinct variants, including Mixture-of-Experts models ranging from the flagship &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-424B-A47B-Base-PT"&gt;&lt;u&gt;ERNIE-4.5-VL-424B-A47B&lt;/u&gt;&lt;/a&gt; with 424 billion total parameters down to a compact 0.3 billion parameter dense model.&lt;/p&gt;&lt;p&gt;According to Baidu&amp;#x27;s &lt;a href="https://ernie.baidu.com/blog/posts/ernie4.5/"&gt;&lt;u&gt;technical report&lt;/u&gt;&lt;/a&gt; on the ERNIE 4.5 family, the models incorporate &amp;quot;a novel heterogeneous modality structure, which supports parameter sharing across modalities while also allowing dedicated parameters for each individual modality.&amp;quot;&lt;/p&gt;&lt;p&gt;This architectural choice addresses a longstanding challenge in multimodal AI development: training systems on both visual and textual data without one modality degrading the performance of the other. Baidu claims this design &amp;quot;has the advantage to enhance multimodal understanding without compromising, and even improving, performance on text-related tasks.&amp;quot;&lt;/p&gt;&lt;p&gt;The company reported achieving &lt;a href="https://ernie.baidu.com/blog/posts/ernie4.5/"&gt;&lt;u&gt;47% Model FLOPs Utilization (MFU)&lt;/u&gt;&lt;/a&gt; — a measure of training efficiency — during pre-training of its largest ERNIE 4.5 language model, using the &lt;a href="https://github.com/PaddlePaddle/Paddle"&gt;&lt;u&gt;PaddlePaddle&lt;/u&gt;&lt;/a&gt; deep learning framework developed in-house.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Comprehensive developer tools aim to simplify enterprise deployment and integration&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;For organizations looking to deploy the model, Baidu has released a comprehensive suite of development tools through &lt;a href="https://github.com/PaddlePaddle/ERNIE"&gt;&lt;u&gt;ERNIEKit&lt;/u&gt;&lt;/a&gt;, what the company describes as an &amp;quot;industrial-grade training and compression development toolkit.&amp;quot;&lt;/p&gt;&lt;p&gt;The model offers full compatibility with popular open-source frameworks including &lt;a href="https://huggingface.co/docs/transformers/en/index"&gt;&lt;u&gt;Hugging Face Transformers&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://github.com/vllm-project/vllm"&gt;&lt;u&gt;vLLM&lt;/u&gt;&lt;/a&gt; (a high-performance inference engine), and Baidu&amp;#x27;s own &lt;a href="https://github.com/PaddlePaddle/FastDeploy"&gt;&lt;u&gt;FastDeploy toolkit&lt;/u&gt;&lt;/a&gt;. This multi-platform support could prove critical for enterprise adoption, allowing organizations to integrate the model into existing AI infrastructure without wholesale platform changes.&lt;/p&gt;&lt;p&gt;Sample code released by Baidu shows a relatively straightforward implementation path. Using the Transformers library, developers can load and run the model with approximately 30 lines of Python code, according to the documentation on Hugging Face.&lt;/p&gt;&lt;p&gt;For production deployments requiring higher throughput, Baidu provides vLLM integration with specialized support for the model&amp;#x27;s &amp;quot;reasoning-parser&amp;quot; and &amp;quot;tool-call-parser&amp;quot; capabilities — features that enable the dynamic image examination and external tool integration that distinguish this model from earlier systems.&lt;/p&gt;&lt;p&gt;The company also offers &lt;a href="https://yiyan.baidu.com/blog/posts/fastdeploy2.0/"&gt;&lt;u&gt;FastDeploy&lt;/u&gt;&lt;/a&gt;, a proprietary inference toolkit that Baidu claims delivers &amp;quot;production-ready, easy-to-use multi-hardware deployment solutions&amp;quot; with support for various quantization schemes that can reduce memory requirements and increase inference speed.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Why this release matters for the enterprise AI market at a critical inflection point&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The release comes at a pivotal moment in the enterprise AI market. As organizations move &lt;a href="https://www.glean.com/perspectives/enterprise-insights-from-ai"&gt;&lt;u&gt;beyond experimental chatbot deployments&lt;/u&gt;&lt;/a&gt; toward production systems that process documents, analyze visual data, and automate complex workflows, demand for capable and cost-effective vision-language models has intensified.&lt;/p&gt;&lt;p&gt;Several enterprise use cases appear particularly well-suited to the model&amp;#x27;s capabilities. Document processing — extracting information from invoices, contracts, and forms — represents a massive market where accurate chart and table understanding directly translates to cost savings through automation. Manufacturing quality control, where AI systems must detect visual defects, could benefit from the model&amp;#x27;s grounding capabilities. Customer service applications that handle images from users could leverage the multi-step visual reasoning.&lt;/p&gt;&lt;p&gt;The model&amp;#x27;s efficiency profile may prove especially attractive to mid-market organizations and startups that lack the computing budgets of large technology companies. By fitting on a single 80GB GPU — hardware costing roughly $10,000 to $30,000 depending on the specific model — the system becomes economically viable for a much broader range of organizations than models requiring multi-GPU setups costing hundreds of thousands of dollars.&lt;/p&gt;&lt;p&gt;&amp;quot;With all these new models, where&amp;#x27;s the best place to actually build and scale? Access to compute is everything,&amp;quot; &lt;a href="https://x.com/orionintx/status/1988214142030037115"&gt;&lt;u&gt;wrote one X user&lt;/u&gt;&lt;/a&gt; in response to Baidu&amp;#x27;s announcement, highlighting the persistent infrastructure challenges facing organizations attempting to deploy advanced AI systems.&lt;/p&gt;&lt;p&gt;The Apache 2.0 licensing further lowers barriers to adoption. Unlike models released under more restrictive licenses that may limit commercial use or require revenue sharing, organizations can deploy &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/u&gt;&lt;/a&gt; in production applications without ongoing licensing fees or usage restrictions.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Competition intensifies as Chinese tech giant takes aim at Google and OpenAI&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Baidu&amp;#x27;s release intensifies competition in the vision-language model space, where &lt;a href="https://www.google.com/?zx=1762903123628&amp;amp;no_sw_cr=1"&gt;&lt;u&gt;Google&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://openai.com/"&gt;&lt;u&gt;OpenAI&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://anthropic.com/"&gt;&lt;u&gt;Anthropic&lt;/u&gt;&lt;/a&gt;, and Chinese companies including &lt;a href="https://www.alibaba.com/"&gt;&lt;u&gt;Alibaba&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://www.bytedance.com/en/"&gt;&lt;u&gt;ByteDance&lt;/u&gt;&lt;/a&gt; have all released capable systems in recent months.&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s performance claims — if validated by independent testing — would represent a significant achievement. Google&amp;#x27;s &lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/"&gt;&lt;u&gt;Gemini 2.5 Pro&lt;/u&gt;&lt;/a&gt; and OpenAI&amp;#x27;s &lt;a href="https://artificialanalysis.ai/models/gpt-5"&gt;&lt;u&gt;GPT-5-High&lt;/u&gt;&lt;/a&gt; are substantially larger models backed by the deep resources of two of the world&amp;#x27;s most valuable technology companies. That a more compact, openly available model could match or exceed their performance on specific tasks would suggest the field is advancing more rapidly than some analysts anticipated.&lt;/p&gt;&lt;p&gt;&amp;quot;Impressive that ERNIE is outperforming Gemini 2.5 Pro,&amp;quot; wrote one social media commenter, expressing surprise at the claimed results.&lt;/p&gt;&lt;p&gt;However, some observers counseled caution about benchmark comparisons. &amp;quot;It&amp;#x27;s fascinating to see how multimodal models are evolving, especially with features like &amp;#x27;Thinking with Images,&amp;#x27;&amp;quot; &lt;a href="https://x.com/_junaidkhalid1/status/1988259730871963652"&gt;&lt;u&gt;wrote one X user&lt;/u&gt;&lt;/a&gt;. &amp;quot;That said, I&amp;#x27;m curious if ERNIE-4.5&amp;#x27;s edge over competitors like Gemini-2.5-Pro and GPT-5-High primarily lies in specific use cases like document and chart&amp;quot; understanding rather than general-purpose vision tasks.&lt;/p&gt;&lt;p&gt;Industry analysts note that &lt;a href="https://www.nytimes.com/2024/04/15/technology/ai-models-measurement.html"&gt;&lt;u&gt;benchmark performance often fails to capture real-world behavior&lt;/u&gt;&lt;/a&gt; across the diverse scenarios enterprises encounter. A model that excels at document understanding may struggle with creative visual tasks or real-time video analysis. Organizations evaluating these systems typically conduct extensive internal testing on representative workloads before committing to production deployments.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Technical limitations and infrastructure requirements that enterprises must consider&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Despite its capabilities, the model faces several technical challenges common to large vision-language systems. The minimum requirement of 80GB of GPU memory, while more accessible than some competitors, still represents a significant infrastructure investment. Organizations without existing GPU infrastructure would need to procure specialized hardware or rely on cloud computing services, introducing ongoing operational costs.&lt;/p&gt;&lt;p&gt;The model&amp;#x27;s context window — the amount of text and visual information it can process simultaneously — is listed as 128K tokens in Baidu&amp;#x27;s documentation. While substantial, this may prove limiting for some document processing scenarios involving very long technical manuals or extensive video content.&lt;/p&gt;&lt;p&gt;Questions also remain about the model&amp;#x27;s behavior on adversarial inputs, out-of-distribution data, and edge cases. Baidu&amp;#x27;s documentation does not provide detailed information about safety testing, bias mitigation, or failure modes — considerations increasingly important for enterprise deployments where errors could have financial or safety implications.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;What technical decision-makers need to evaluate beyond the benchmark numbers&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;For technical decision-makers evaluating the model, several implementation factors warrant consideration beyond raw performance metrics.&lt;/p&gt;&lt;p&gt;The model&amp;#x27;s &lt;a href="https://huggingface.co/blog/moe"&gt;&lt;u&gt;MoE architecture&lt;/u&gt;&lt;/a&gt;, while efficient during inference, adds complexity to deployment and optimization. Organizations must ensure their infrastructure can properly route inputs to the appropriate expert subnetworks — a capability not universally supported across all deployment platforms.&lt;/p&gt;&lt;p&gt;The &amp;quot;&lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;Thinking with Images&lt;/u&gt;&lt;/a&gt;&amp;quot; feature, while innovative, requires integration with image manipulation tools to achieve its full potential. Baidu&amp;#x27;s documentation suggests this capability works best &amp;quot;when paired with tools like image zooming and image search,&amp;quot; implying that organizations may need to build additional infrastructure to fully leverage this functionality.&lt;/p&gt;&lt;p&gt;The model&amp;#x27;s video understanding capabilities, while highlighted in marketing materials, come with practical constraints. Processing video requires substantially more computational resources than static images, and the documentation does not specify maximum video length or optimal frame rates.&lt;/p&gt;&lt;p&gt;Organizations considering deployment should also evaluate Baidu&amp;#x27;s ongoing commitment to the model. Open-source AI models require continuing maintenance, security updates, and potential retraining as data distributions shift over time. While the &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking#license"&gt;&lt;u&gt;Apache 2.0 license&lt;/u&gt;&lt;/a&gt; ensures the model remains available, future improvements and support depend on Baidu&amp;#x27;s strategic priorities.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Developer community responds with enthusiasm tempered by practical requests&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Early response from the AI research and development community has been cautiously optimistic. Developers have requested versions of the model in additional formats including GGUF (a quantization format popular for local deployment) and MNN (a mobile neural network framework), suggesting interest in running the system on resource-constrained devices.&lt;/p&gt;&lt;p&gt;&amp;quot;Release MNN and GGUF so I can run it on my phone,&amp;quot; &lt;a href="https://x.com/Elaina43114880/status/1988327740496638345"&gt;&lt;u&gt;wrote one developer&lt;/u&gt;&lt;/a&gt;, highlighting demand for mobile deployment options.&lt;/p&gt;&lt;p&gt;Other developers praised Baidu&amp;#x27;s technical choices while requesting additional resources. &amp;quot;Fantastic model! Did you use discoveries from PaddleOCR?&amp;quot; &lt;a href="https://x.com/J3rryH0well/status/1988281431421055439"&gt;&lt;u&gt;asked one user&lt;/u&gt;&lt;/a&gt;, referencing Baidu&amp;#x27;s open-source optical character recognition toolkit.&lt;/p&gt;&lt;p&gt;The model&amp;#x27;s lengthy name—ERNIE-4.5-VL-28B-A3B-Thinking—drew lighthearted commentary. &amp;quot;ERNIE-4.5-VL-28B-A3B-Thinking might be the longest model name in history,&amp;quot; &lt;a href="https://x.com/Mr_Pratap_Singh/status/1988304605877596177"&gt;&lt;u&gt;joked one observer&lt;/u&gt;&lt;/a&gt;. &amp;quot;But hey, if you&amp;#x27;re outperforming Gemini-2.5-Pro with only 3B active params, you&amp;#x27;ve earned the right to a dramatic name!&amp;quot;&lt;/p&gt;&lt;p&gt;Baidu plans to showcase the ERNIE lineup during its &lt;a href="https://ir.baidu.com/news-releases/news-release-details/baidu-host-baidu-world-annual-flagship-technology-conference-nov"&gt;&lt;u&gt;Baidu World 2025 conference&lt;/u&gt;&lt;/a&gt; on November 13, where the company is expected to provide additional details about the model&amp;#x27;s development, performance validation, and future roadmap.&lt;/p&gt;&lt;p&gt;The release marks a strategic move by Baidu to establish itself as a major player in the global AI infrastructure market. While Chinese AI companies have historically focused primarily on domestic markets, the open-source release under a permissive license signals ambitions to compete internationally with Western AI giants.&lt;/p&gt;&lt;p&gt;For enterprises, the release adds another capable option to a rapidly expanding menu of AI models. Organizations no longer face a binary choice between building proprietary systems or licensing closed-source models from a handful of vendors. The proliferation of capable open-source alternatives like &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/u&gt;&lt;/a&gt; is reshaping the economics of AI deployment and accelerating adoption across industries.&lt;/p&gt;&lt;p&gt;Whether the model delivers on its performance promises in real-world deployments remains to be seen. But for organizations seeking powerful, cost-effective tools for visual understanding and reasoning, one thing is certain. As one developer succinctly summarized: &amp;quot;Open source plus commercial use equals chef&amp;#x27;s kiss. Baidu not playing around.&amp;quot;&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://www.baidu.com/"&gt;&lt;u&gt;Baidu Inc.&lt;/u&gt;&lt;/a&gt;, China&amp;#x27;s largest search engine company, released a new artificial intelligence model on Monday that its developers claim outperforms competitors from &lt;a href="https://www.google.com/"&gt;&lt;u&gt;Google&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://openai.com/"&gt;&lt;u&gt;OpenAI&lt;/u&gt;&lt;/a&gt; on several vision-related benchmarks despite using a fraction of the computing resources typically required for such systems.&lt;/p&gt;&lt;p&gt;The model, dubbed &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/u&gt;&lt;/a&gt;, is the latest salvo in an escalating competition among technology companies to build AI systems that can understand and reason about images, videos, and documents alongside traditional text — capabilities increasingly critical for enterprise applications ranging from automated document processing to industrial quality control.&lt;/p&gt;&lt;p&gt;What sets Baidu&amp;#x27;s release apart is its efficiency: the model activates just 3 billion parameters during operation while maintaining 28 billion total parameters through a sophisticated routing architecture. According to documentation released with the model, this design allows it to match or exceed the performance of much larger competing systems on tasks involving document understanding, chart analysis, and visual reasoning while consuming significantly less computational power and memory.&lt;/p&gt;&lt;p&gt;&amp;quot;Built upon the powerful ERNIE-4.5-VL-28B-A3B architecture, the newly upgraded ERNIE-4.5-VL-28B-A3B-Thinking achieves a remarkable leap forward in multimodal reasoning capabilities,&amp;quot; Baidu wrote in the model&amp;#x27;s &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;technical documentation&lt;/u&gt;&lt;/a&gt; on Hugging Face, the AI model repository where the system was released.&lt;/p&gt;&lt;p&gt;The company said the model underwent &amp;quot;an extensive mid-training phase&amp;quot; that incorporated &amp;quot;a vast and highly diverse corpus of premium visual-language reasoning data,&amp;quot; dramatically boosting its ability to align visual and textual information semantically.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;How the model mimics human visual problem-solving through dynamic image analysis&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Perhaps the model&amp;#x27;s most distinctive feature is what Baidu calls &amp;quot;&lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;Thinking with Images&lt;/u&gt;&lt;/a&gt;&amp;quot; — a capability that allows the AI to dynamically zoom in and out of images to examine fine-grained details, mimicking how humans approach visual problem-solving tasks.&lt;/p&gt;&lt;p&gt;&amp;quot;The model thinks like a human, capable of freely zooming in and out of images to grasp every detail and uncover all information,&amp;quot; according to the model card. When paired with tools like image search, Baidu claims this feature &amp;quot;dramatically elevates the model&amp;#x27;s ability to process fine-grained details and handle long-tail visual knowledge.&amp;quot;&lt;/p&gt;&lt;p&gt;This approach marks a departure from traditional vision-language models, which typically process images at a fixed resolution. By allowing dynamic image examination, the system can theoretically handle scenarios requiring both broad context and granular detail—such as analyzing complex technical diagrams or detecting subtle defects in manufacturing quality control.&lt;/p&gt;&lt;p&gt;The model also supports what Baidu describes as enhanced &amp;quot;visual grounding&amp;quot; capabilities with &amp;quot;more precise grounding and flexible instruction execution, easily triggering grounding functions in complex industrial scenarios,&amp;quot; suggesting potential applications in robotics, warehouse automation, and other settings where AI systems must identify and locate specific objects in visual scenes.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Baidu&amp;#x27;s performance claims draw scrutiny as independent testing remains pending&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Baidu&amp;#x27;s assertion that the model outperforms Google&amp;#x27;s &lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/"&gt;&lt;u&gt;Gemini 2.5 Pro&lt;/u&gt;&lt;/a&gt; and OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-gpt-5/"&gt;&lt;u&gt;GPT-5-High&lt;/u&gt;&lt;/a&gt; on various document and chart understanding benchmarks has drawn attention across social media, though independent verification of these claims remains pending.&lt;/p&gt;&lt;p&gt;The company released the model under the permissive &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking#license"&gt;&lt;u&gt;Apache 2.0 license&lt;/u&gt;&lt;/a&gt;, allowing unrestricted commercial use—a strategic decision that contrasts with the more restrictive licensing approaches of some competitors and could accelerate enterprise adoption.&lt;/p&gt;&lt;p&gt;&amp;quot;&lt;a href="https://x.com/orionintx/status/1988214142030037115"&gt;&lt;u&gt;Apache 2.0 is smart&lt;/u&gt;&lt;/a&gt;,&amp;quot; wrote one X user responding to Baidu&amp;#x27;s announcement, highlighting the competitive advantage of open licensing in the enterprise market.&lt;/p&gt;&lt;p&gt;According to Baidu&amp;#x27;s documentation, the model demonstrates six core capabilities beyond traditional text processing. In visual reasoning, the system can perform what Baidu describes as &amp;quot;multi-step reasoning, chart analysis, and causal reasoning capabilities in complex visual tasks,&amp;quot; aided by what the company characterizes as &amp;quot;large-scale reinforcement learning.&amp;quot; &lt;/p&gt;&lt;p&gt;For STEM problem solving, Baidu claims that &amp;quot;leveraging its powerful visual abilities, the model achieves a leap in performance on STEM tasks like solving problems from photos.&amp;quot; The visual grounding capability allows the model to identify and locate objects within images with what Baidu characterizes as industrial-grade precision. Through tool integration, the system can invoke external functions including image search capabilities to access information beyond its training data.&lt;/p&gt;&lt;p&gt;For video understanding, Baidu claims the model possesses &amp;quot;outstanding temporal awareness and event localization abilities, accurately identifying content changes across different time segments in a video.&amp;quot; Finally, the thinking with images feature enables the dynamic zoom functionality that distinguishes this model from competitors.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Inside the mixture-of-experts architecture that powers efficient multimodal processing&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Under the hood, &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/u&gt;&lt;/a&gt; employs a &lt;a href="https://huggingface.co/blog/moe"&gt;&lt;u&gt;Mixture-of-Experts (MoE) architecture&lt;/u&gt;&lt;/a&gt; — a design pattern that has become increasingly popular for building efficient large-scale AI systems. Rather than activating all 28 billion parameters for every task, the model uses a routing mechanism to selectively activate only the 3 billion parameters most relevant to each specific input.&lt;/p&gt;&lt;p&gt;This approach offers substantial practical advantages for enterprise deployments. According to Baidu&amp;#x27;s documentation, the model can run on a single 80GB GPU — hardware readily available in many corporate data centers — making it significantly more accessible than competing systems that may require multiple high-end accelerators.&lt;/p&gt;&lt;p&gt;The technical documentation reveals that Baidu employed several advanced training techniques to achieve the model&amp;#x27;s capabilities. The company used &amp;quot;cutting-edge multimodal reinforcement learning techniques on verifiable tasks, integrating GSPO and IcePop strategies to stabilize MoE training combined with dynamic difficulty sampling for exceptional learning efficiency.&amp;quot;&lt;/p&gt;&lt;p&gt;Baidu also notes that in response to &amp;quot;strong community demand,&amp;quot; the company &amp;quot;significantly strengthened the model&amp;#x27;s grounding performance with improved instruction-following capabilities.&amp;quot;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The new model fits into Baidu&amp;#x27;s ambitious multimodal AI ecosystem&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The new release is one component of Baidu&amp;#x27;s broader &lt;a href="https://ernie.baidu.com/blog/posts/ernie4.5/"&gt;&lt;u&gt;ERNIE 4.5 model family&lt;/u&gt;&lt;/a&gt;, which the company unveiled in June 2025. That family comprises 10 distinct variants, including Mixture-of-Experts models ranging from the flagship &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-424B-A47B-Base-PT"&gt;&lt;u&gt;ERNIE-4.5-VL-424B-A47B&lt;/u&gt;&lt;/a&gt; with 424 billion total parameters down to a compact 0.3 billion parameter dense model.&lt;/p&gt;&lt;p&gt;According to Baidu&amp;#x27;s &lt;a href="https://ernie.baidu.com/blog/posts/ernie4.5/"&gt;&lt;u&gt;technical report&lt;/u&gt;&lt;/a&gt; on the ERNIE 4.5 family, the models incorporate &amp;quot;a novel heterogeneous modality structure, which supports parameter sharing across modalities while also allowing dedicated parameters for each individual modality.&amp;quot;&lt;/p&gt;&lt;p&gt;This architectural choice addresses a longstanding challenge in multimodal AI development: training systems on both visual and textual data without one modality degrading the performance of the other. Baidu claims this design &amp;quot;has the advantage to enhance multimodal understanding without compromising, and even improving, performance on text-related tasks.&amp;quot;&lt;/p&gt;&lt;p&gt;The company reported achieving &lt;a href="https://ernie.baidu.com/blog/posts/ernie4.5/"&gt;&lt;u&gt;47% Model FLOPs Utilization (MFU)&lt;/u&gt;&lt;/a&gt; — a measure of training efficiency — during pre-training of its largest ERNIE 4.5 language model, using the &lt;a href="https://github.com/PaddlePaddle/Paddle"&gt;&lt;u&gt;PaddlePaddle&lt;/u&gt;&lt;/a&gt; deep learning framework developed in-house.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Comprehensive developer tools aim to simplify enterprise deployment and integration&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;For organizations looking to deploy the model, Baidu has released a comprehensive suite of development tools through &lt;a href="https://github.com/PaddlePaddle/ERNIE"&gt;&lt;u&gt;ERNIEKit&lt;/u&gt;&lt;/a&gt;, what the company describes as an &amp;quot;industrial-grade training and compression development toolkit.&amp;quot;&lt;/p&gt;&lt;p&gt;The model offers full compatibility with popular open-source frameworks including &lt;a href="https://huggingface.co/docs/transformers/en/index"&gt;&lt;u&gt;Hugging Face Transformers&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://github.com/vllm-project/vllm"&gt;&lt;u&gt;vLLM&lt;/u&gt;&lt;/a&gt; (a high-performance inference engine), and Baidu&amp;#x27;s own &lt;a href="https://github.com/PaddlePaddle/FastDeploy"&gt;&lt;u&gt;FastDeploy toolkit&lt;/u&gt;&lt;/a&gt;. This multi-platform support could prove critical for enterprise adoption, allowing organizations to integrate the model into existing AI infrastructure without wholesale platform changes.&lt;/p&gt;&lt;p&gt;Sample code released by Baidu shows a relatively straightforward implementation path. Using the Transformers library, developers can load and run the model with approximately 30 lines of Python code, according to the documentation on Hugging Face.&lt;/p&gt;&lt;p&gt;For production deployments requiring higher throughput, Baidu provides vLLM integration with specialized support for the model&amp;#x27;s &amp;quot;reasoning-parser&amp;quot; and &amp;quot;tool-call-parser&amp;quot; capabilities — features that enable the dynamic image examination and external tool integration that distinguish this model from earlier systems.&lt;/p&gt;&lt;p&gt;The company also offers &lt;a href="https://yiyan.baidu.com/blog/posts/fastdeploy2.0/"&gt;&lt;u&gt;FastDeploy&lt;/u&gt;&lt;/a&gt;, a proprietary inference toolkit that Baidu claims delivers &amp;quot;production-ready, easy-to-use multi-hardware deployment solutions&amp;quot; with support for various quantization schemes that can reduce memory requirements and increase inference speed.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Why this release matters for the enterprise AI market at a critical inflection point&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The release comes at a pivotal moment in the enterprise AI market. As organizations move &lt;a href="https://www.glean.com/perspectives/enterprise-insights-from-ai"&gt;&lt;u&gt;beyond experimental chatbot deployments&lt;/u&gt;&lt;/a&gt; toward production systems that process documents, analyze visual data, and automate complex workflows, demand for capable and cost-effective vision-language models has intensified.&lt;/p&gt;&lt;p&gt;Several enterprise use cases appear particularly well-suited to the model&amp;#x27;s capabilities. Document processing — extracting information from invoices, contracts, and forms — represents a massive market where accurate chart and table understanding directly translates to cost savings through automation. Manufacturing quality control, where AI systems must detect visual defects, could benefit from the model&amp;#x27;s grounding capabilities. Customer service applications that handle images from users could leverage the multi-step visual reasoning.&lt;/p&gt;&lt;p&gt;The model&amp;#x27;s efficiency profile may prove especially attractive to mid-market organizations and startups that lack the computing budgets of large technology companies. By fitting on a single 80GB GPU — hardware costing roughly $10,000 to $30,000 depending on the specific model — the system becomes economically viable for a much broader range of organizations than models requiring multi-GPU setups costing hundreds of thousands of dollars.&lt;/p&gt;&lt;p&gt;&amp;quot;With all these new models, where&amp;#x27;s the best place to actually build and scale? Access to compute is everything,&amp;quot; &lt;a href="https://x.com/orionintx/status/1988214142030037115"&gt;&lt;u&gt;wrote one X user&lt;/u&gt;&lt;/a&gt; in response to Baidu&amp;#x27;s announcement, highlighting the persistent infrastructure challenges facing organizations attempting to deploy advanced AI systems.&lt;/p&gt;&lt;p&gt;The Apache 2.0 licensing further lowers barriers to adoption. Unlike models released under more restrictive licenses that may limit commercial use or require revenue sharing, organizations can deploy &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/u&gt;&lt;/a&gt; in production applications without ongoing licensing fees or usage restrictions.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Competition intensifies as Chinese tech giant takes aim at Google and OpenAI&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Baidu&amp;#x27;s release intensifies competition in the vision-language model space, where &lt;a href="https://www.google.com/?zx=1762903123628&amp;amp;no_sw_cr=1"&gt;&lt;u&gt;Google&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://openai.com/"&gt;&lt;u&gt;OpenAI&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://anthropic.com/"&gt;&lt;u&gt;Anthropic&lt;/u&gt;&lt;/a&gt;, and Chinese companies including &lt;a href="https://www.alibaba.com/"&gt;&lt;u&gt;Alibaba&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://www.bytedance.com/en/"&gt;&lt;u&gt;ByteDance&lt;/u&gt;&lt;/a&gt; have all released capable systems in recent months.&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s performance claims — if validated by independent testing — would represent a significant achievement. Google&amp;#x27;s &lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/"&gt;&lt;u&gt;Gemini 2.5 Pro&lt;/u&gt;&lt;/a&gt; and OpenAI&amp;#x27;s &lt;a href="https://artificialanalysis.ai/models/gpt-5"&gt;&lt;u&gt;GPT-5-High&lt;/u&gt;&lt;/a&gt; are substantially larger models backed by the deep resources of two of the world&amp;#x27;s most valuable technology companies. That a more compact, openly available model could match or exceed their performance on specific tasks would suggest the field is advancing more rapidly than some analysts anticipated.&lt;/p&gt;&lt;p&gt;&amp;quot;Impressive that ERNIE is outperforming Gemini 2.5 Pro,&amp;quot; wrote one social media commenter, expressing surprise at the claimed results.&lt;/p&gt;&lt;p&gt;However, some observers counseled caution about benchmark comparisons. &amp;quot;It&amp;#x27;s fascinating to see how multimodal models are evolving, especially with features like &amp;#x27;Thinking with Images,&amp;#x27;&amp;quot; &lt;a href="https://x.com/_junaidkhalid1/status/1988259730871963652"&gt;&lt;u&gt;wrote one X user&lt;/u&gt;&lt;/a&gt;. &amp;quot;That said, I&amp;#x27;m curious if ERNIE-4.5&amp;#x27;s edge over competitors like Gemini-2.5-Pro and GPT-5-High primarily lies in specific use cases like document and chart&amp;quot; understanding rather than general-purpose vision tasks.&lt;/p&gt;&lt;p&gt;Industry analysts note that &lt;a href="https://www.nytimes.com/2024/04/15/technology/ai-models-measurement.html"&gt;&lt;u&gt;benchmark performance often fails to capture real-world behavior&lt;/u&gt;&lt;/a&gt; across the diverse scenarios enterprises encounter. A model that excels at document understanding may struggle with creative visual tasks or real-time video analysis. Organizations evaluating these systems typically conduct extensive internal testing on representative workloads before committing to production deployments.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Technical limitations and infrastructure requirements that enterprises must consider&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Despite its capabilities, the model faces several technical challenges common to large vision-language systems. The minimum requirement of 80GB of GPU memory, while more accessible than some competitors, still represents a significant infrastructure investment. Organizations without existing GPU infrastructure would need to procure specialized hardware or rely on cloud computing services, introducing ongoing operational costs.&lt;/p&gt;&lt;p&gt;The model&amp;#x27;s context window — the amount of text and visual information it can process simultaneously — is listed as 128K tokens in Baidu&amp;#x27;s documentation. While substantial, this may prove limiting for some document processing scenarios involving very long technical manuals or extensive video content.&lt;/p&gt;&lt;p&gt;Questions also remain about the model&amp;#x27;s behavior on adversarial inputs, out-of-distribution data, and edge cases. Baidu&amp;#x27;s documentation does not provide detailed information about safety testing, bias mitigation, or failure modes — considerations increasingly important for enterprise deployments where errors could have financial or safety implications.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;What technical decision-makers need to evaluate beyond the benchmark numbers&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;For technical decision-makers evaluating the model, several implementation factors warrant consideration beyond raw performance metrics.&lt;/p&gt;&lt;p&gt;The model&amp;#x27;s &lt;a href="https://huggingface.co/blog/moe"&gt;&lt;u&gt;MoE architecture&lt;/u&gt;&lt;/a&gt;, while efficient during inference, adds complexity to deployment and optimization. Organizations must ensure their infrastructure can properly route inputs to the appropriate expert subnetworks — a capability not universally supported across all deployment platforms.&lt;/p&gt;&lt;p&gt;The &amp;quot;&lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;Thinking with Images&lt;/u&gt;&lt;/a&gt;&amp;quot; feature, while innovative, requires integration with image manipulation tools to achieve its full potential. Baidu&amp;#x27;s documentation suggests this capability works best &amp;quot;when paired with tools like image zooming and image search,&amp;quot; implying that organizations may need to build additional infrastructure to fully leverage this functionality.&lt;/p&gt;&lt;p&gt;The model&amp;#x27;s video understanding capabilities, while highlighted in marketing materials, come with practical constraints. Processing video requires substantially more computational resources than static images, and the documentation does not specify maximum video length or optimal frame rates.&lt;/p&gt;&lt;p&gt;Organizations considering deployment should also evaluate Baidu&amp;#x27;s ongoing commitment to the model. Open-source AI models require continuing maintenance, security updates, and potential retraining as data distributions shift over time. While the &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking#license"&gt;&lt;u&gt;Apache 2.0 license&lt;/u&gt;&lt;/a&gt; ensures the model remains available, future improvements and support depend on Baidu&amp;#x27;s strategic priorities.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Developer community responds with enthusiasm tempered by practical requests&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Early response from the AI research and development community has been cautiously optimistic. Developers have requested versions of the model in additional formats including GGUF (a quantization format popular for local deployment) and MNN (a mobile neural network framework), suggesting interest in running the system on resource-constrained devices.&lt;/p&gt;&lt;p&gt;&amp;quot;Release MNN and GGUF so I can run it on my phone,&amp;quot; &lt;a href="https://x.com/Elaina43114880/status/1988327740496638345"&gt;&lt;u&gt;wrote one developer&lt;/u&gt;&lt;/a&gt;, highlighting demand for mobile deployment options.&lt;/p&gt;&lt;p&gt;Other developers praised Baidu&amp;#x27;s technical choices while requesting additional resources. &amp;quot;Fantastic model! Did you use discoveries from PaddleOCR?&amp;quot; &lt;a href="https://x.com/J3rryH0well/status/1988281431421055439"&gt;&lt;u&gt;asked one user&lt;/u&gt;&lt;/a&gt;, referencing Baidu&amp;#x27;s open-source optical character recognition toolkit.&lt;/p&gt;&lt;p&gt;The model&amp;#x27;s lengthy name—ERNIE-4.5-VL-28B-A3B-Thinking—drew lighthearted commentary. &amp;quot;ERNIE-4.5-VL-28B-A3B-Thinking might be the longest model name in history,&amp;quot; &lt;a href="https://x.com/Mr_Pratap_Singh/status/1988304605877596177"&gt;&lt;u&gt;joked one observer&lt;/u&gt;&lt;/a&gt;. &amp;quot;But hey, if you&amp;#x27;re outperforming Gemini-2.5-Pro with only 3B active params, you&amp;#x27;ve earned the right to a dramatic name!&amp;quot;&lt;/p&gt;&lt;p&gt;Baidu plans to showcase the ERNIE lineup during its &lt;a href="https://ir.baidu.com/news-releases/news-release-details/baidu-host-baidu-world-annual-flagship-technology-conference-nov"&gt;&lt;u&gt;Baidu World 2025 conference&lt;/u&gt;&lt;/a&gt; on November 13, where the company is expected to provide additional details about the model&amp;#x27;s development, performance validation, and future roadmap.&lt;/p&gt;&lt;p&gt;The release marks a strategic move by Baidu to establish itself as a major player in the global AI infrastructure market. While Chinese AI companies have historically focused primarily on domestic markets, the open-source release under a permissive license signals ambitions to compete internationally with Western AI giants.&lt;/p&gt;&lt;p&gt;For enterprises, the release adds another capable option to a rapidly expanding menu of AI models. Organizations no longer face a binary choice between building proprietary systems or licensing closed-source models from a handful of vendors. The proliferation of capable open-source alternatives like &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/u&gt;&lt;/a&gt; is reshaping the economics of AI deployment and accelerating adoption across industries.&lt;/p&gt;&lt;p&gt;Whether the model delivers on its performance promises in real-world deployments remains to be seen. But for organizations seeking powerful, cost-effective tools for visual understanding and reasoning, one thing is certain. As one developer succinctly summarized: &amp;quot;Open source plus commercial use equals chef&amp;#x27;s kiss. Baidu not playing around.&amp;quot;&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/baidu-just-dropped-an-open-source-multimodal-ai-that-it-claims-beats-gpt-5</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item></channel></rss>