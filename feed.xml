<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 14 Aug 2025 06:34:57 +0000</lastBuildDate><item><title>Google Gemini will now learn from your chats—unless you tell it not to (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/08/google-gemini-will-now-learn-from-your-chats-unless-you-tell-it-not-to/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Gemini will remember this, so it's time to check your privacy settings.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Gemini AI Android app assistant" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/03/Gemini-app-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Gemini AI Android app assistant" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/03/Gemini-app-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;As Gemini is increasingly woven into the fabric of Google, the way the chatbot accesses and interacts with your data is in a constant state of flux. Today, Google is announcing several big changes to how its AI adapts to you, giving it the ability to remember more details about your chats for improved answers. If that's a concern, Google also has a new temporary chat option that won't affect the way Gemini thinks about you.&lt;/p&gt;
&lt;p&gt;You might recall several months back when Google added a "personalization" option to the Gemini model selector. This mode leaned on your Google search history to customize responses, a feature that did not seem to appeal to many Gemini users. Google later dropped that mode, but a new attempt at customization is now rolling out. Gemini is getting an option called Personal Context. When enabled, the chatbot will remember details about your past conversations, adapting its replies without being specifically prompted.&lt;/p&gt;
&lt;p&gt;Google claims Personal Context will produce more relevant responses, particularly when you ask the chatbot to make recommendations. This is separate from the saved instructions feature, which allows you to provide explicit instructions for Gemini to be used in crafting outputs. This does have the potential to make Gemini feel more engaging, but that's not always a good thing. AI chatbots that get too friendly with the user can reinforce misconceptions and lead to delusional thinking, something we've seen distressingly often with AI models.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2111853 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="562" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/Personal_context_-_Past_Chats.jpg" width="1000" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;To start, this feature will be available with the Gemini 2.5 Pro model, but you won't get customization in the Eurpean Union,the&amp;nbsp; UK, or Switzerland. It's also limited to users over the age of 18. Google says it will eventually release this feature in additional regions and with support for the more efficient Gemini 2.5 Flash model. You can turn Personal Context on and off at will from the main settings page.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;More control over your data&lt;/h2&gt;
&lt;p&gt;As Google moves to implement more customization features in Gemini, you might find yourself second-guessing whether you really want to have certain conversations with the robot. Thankfully, you have options. You can turn off Personal Context, but Temporary Chats go a step further—it's essentially Incognito Mode (but one that actually works) for Gemini.&lt;/p&gt;
&lt;p&gt;Temporary Chats also begin rolling out today and will expand to all users over the coming weeks. The feature will be accessible via a dedicated button next to the "New chat" option in the Gemini app. Google says anything you type in a temporary interaction won't be used in Personal Context, even if that setting is enabled. Google labels these as "one-off" chats, but they're not &lt;em&gt;quite&lt;/em&gt; that temporary. They'll be retained on Google's servers for 72 hours so you can refer back to them and expand on the conversation if you want.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2111856 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="Gemini temporary" class="fullwidth full" height="1000" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/Temporary_Chat.jpg" width="1000" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Even if you only occasionally use Gemini, you'll want to pay attention to the new personalization push. Google has also confirmed that it's changing how it uses the content you upload to Gemini. Starting September 2, a sample of your chats and data (including file uploads) will be used to train Google's AI. Or in Google's words, your data will "improve Google services for everyone."&lt;/p&gt;
&lt;p&gt;If you don't want to give license to dump your data into AI models, you'll need to opt out. In the next few weeks, Google will update the account-level privacy settings, changing "Gemini Apps Activity" to "Keep Activity." You can disable this setting (or use Temporary Chats) to keep your data from being used in Google's model development. Make sure you give this setting a peek before next month or accept that Google will be free and clear to gobble up more of your data.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Gemini will remember this, so it's time to check your privacy settings.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Gemini AI Android app assistant" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/03/Gemini-app-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Gemini AI Android app assistant" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/03/Gemini-app-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;As Gemini is increasingly woven into the fabric of Google, the way the chatbot accesses and interacts with your data is in a constant state of flux. Today, Google is announcing several big changes to how its AI adapts to you, giving it the ability to remember more details about your chats for improved answers. If that's a concern, Google also has a new temporary chat option that won't affect the way Gemini thinks about you.&lt;/p&gt;
&lt;p&gt;You might recall several months back when Google added a "personalization" option to the Gemini model selector. This mode leaned on your Google search history to customize responses, a feature that did not seem to appeal to many Gemini users. Google later dropped that mode, but a new attempt at customization is now rolling out. Gemini is getting an option called Personal Context. When enabled, the chatbot will remember details about your past conversations, adapting its replies without being specifically prompted.&lt;/p&gt;
&lt;p&gt;Google claims Personal Context will produce more relevant responses, particularly when you ask the chatbot to make recommendations. This is separate from the saved instructions feature, which allows you to provide explicit instructions for Gemini to be used in crafting outputs. This does have the potential to make Gemini feel more engaging, but that's not always a good thing. AI chatbots that get too friendly with the user can reinforce misconceptions and lead to delusional thinking, something we've seen distressingly often with AI models.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2111853 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="562" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/Personal_context_-_Past_Chats.jpg" width="1000" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;To start, this feature will be available with the Gemini 2.5 Pro model, but you won't get customization in the Eurpean Union,the&amp;nbsp; UK, or Switzerland. It's also limited to users over the age of 18. Google says it will eventually release this feature in additional regions and with support for the more efficient Gemini 2.5 Flash model. You can turn Personal Context on and off at will from the main settings page.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;More control over your data&lt;/h2&gt;
&lt;p&gt;As Google moves to implement more customization features in Gemini, you might find yourself second-guessing whether you really want to have certain conversations with the robot. Thankfully, you have options. You can turn off Personal Context, but Temporary Chats go a step further—it's essentially Incognito Mode (but one that actually works) for Gemini.&lt;/p&gt;
&lt;p&gt;Temporary Chats also begin rolling out today and will expand to all users over the coming weeks. The feature will be accessible via a dedicated button next to the "New chat" option in the Gemini app. Google says anything you type in a temporary interaction won't be used in Personal Context, even if that setting is enabled. Google labels these as "one-off" chats, but they're not &lt;em&gt;quite&lt;/em&gt; that temporary. They'll be retained on Google's servers for 72 hours so you can refer back to them and expand on the conversation if you want.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2111856 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="Gemini temporary" class="fullwidth full" height="1000" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/Temporary_Chat.jpg" width="1000" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Even if you only occasionally use Gemini, you'll want to pay attention to the new personalization push. Google has also confirmed that it's changing how it uses the content you upload to Gemini. Starting September 2, a sample of your chats and data (including file uploads) will be used to train Google's AI. Or in Google's words, your data will "improve Google services for everyone."&lt;/p&gt;
&lt;p&gt;If you don't want to give license to dump your data into AI models, you'll need to opt out. In the next few weeks, Google will update the account-level privacy settings, changing "Gemini Apps Activity" to "Keep Activity." You can disable this setting (or use Temporary Chats) to keep your data from being used in Google's model development. Make sure you give this setting a peek before next month or accept that Google will be free and clear to gobble up more of your data.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/08/google-gemini-will-now-learn-from-your-chats-unless-you-tell-it-not-to/</guid><pubDate>Wed, 13 Aug 2025 18:40:18 +0000</pubDate></item><item><title>What happens the day after superintelligence? (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/what-happens-the-day-after-superintelligence/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;With the release OpenAI’s Chat GPT-5, the world is one step closer to unleashing a general-purpose superintelligence that can cognitively outperform each of us by a wide margin. As this day nears, I am increasingly worried that we are woefully unprepared for the shockwaves this will send through society — and it’s probably not for the reasons you expect.&lt;/p&gt;



&lt;p&gt;Try this little experiment: Ask anyone you know if they are concerned about AI, and they will likely share a variety of fears, from massive disruptions in the job market and the reality-bending impacts of deepfakes, to the unprecedented power being concentrated in a handful of large AI companies. In other words, most people have never honestly imagined what their life will really feel like &lt;strong&gt;&lt;em&gt;the day after&lt;/em&gt;&lt;/strong&gt; superintelligence becomes widely available.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-superintelligence-could-demoralize-us"&gt;Why superintelligence could demoralize us&lt;/h2&gt;



&lt;p&gt;As context, artificial superintelligence (ASI) refers to systems that can outthink humans on most fronts, from planning and reasoning to problem-solving, strategic thinking and raw creativity.&amp;nbsp;These systems will solve complex problems in a fraction of a second that might take the smartest human experts days, weeks or even years to work through. This terrifies me, and it’s not because of the doomsday scenarios that dominate our public discourse.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;No, I am worried about the opposite risks — the dangers that could emerge in the &lt;em&gt;best-case scenarios&lt;/em&gt; where superintelligence is helpful and benevolent. Such an ASI will have many positive impacts on society, but it could also be deeply demoralizing to our core identity as humans.&amp;nbsp;After all, the world will feel different when each of us knows that a smarter, faster, more creative intelligence is available on our mobile devices than between our own ears.&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;So ask yourself, honestly, how will humans act in this new reality?&amp;nbsp;Will we reflexively seek advice from our AI assistants as we navigate every little challenge we encounter? Or worse, will we learn to trust our AI assistants more than our own thoughts and instincts?&lt;/p&gt;



&lt;p&gt;Wait — before you answer, you must update your mental model. Currently, we engage AI through a Socratic framework that requires us to ask questions and get answers (like Captain Kirk did aboard the Enterprise in 1966).&amp;nbsp;But that’s old-school thinking. We are now entering a new era in which AI assistants will be integrated into body-worn devices that are equipped with cameras and microphones, enabling AI to see what you see, hear what you hear and whisper advice into your ears without you needing to ask.&lt;/p&gt;



&lt;p&gt;In other words, our future will be filled with AI assistants that ride shotgun in our lives, augmenting our experiences with optimized guidance at every turn. In this world, the risk is not that we reflexively ask AI for advice before using our own brains; the risk is that we won’t need to ask – the advice will just stream into our eyes and ears, shaping our actions, influencing our decisions and solving our problems before we’ve had a chance to think for ourselves.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-augmented-mentality-will-transform-our-lives"&gt;‘Augmented mentality’ will transform our lives&lt;/h2&gt;



&lt;p&gt;I refer to this framework as ‘augmented mentality‘ and it is about to hit society at scale through AI-powered glasses, earbuds and pendants.&amp;nbsp; This is the future of mobile computing, and it is already driving an arms race between Meta, Google, Samsung and Apple, as they position themselves to produce the context-aware AI devices that will replace handheld phones.&lt;/p&gt;



&lt;p&gt;Imagine walking down the street in your town. You see a coworker heading towards you. You can’t remember his name, but your AI assistant does. It detects your hesitation and whispers the coworker’s name into your ears.&amp;nbsp; The AI also recommends that you ask the coworker about his wife, who had surgery a few weeks ago.&amp;nbsp; The coworker appreciates the sentiment, then asks you about your recent promotion, likely at the advice of his own AI.&lt;/p&gt;



&lt;p&gt;Is this human empowerment, or a loss of human agency?&lt;/p&gt;



&lt;p&gt;It will certainly feel like a superpower to have an AI in your ear that always has your back, ensuring you never forget a name, always have witty things to say and are instantly alerted when someone you’re talking to is not being truthful. On the other hand, everyone you meet will have their own AI muttering in their own ears. This will make us wonder who we’re &lt;em&gt;really&lt;/em&gt; interacting with — the human in front of us, or the AI agent giving them guidance (check out &lt;em&gt;Carbon Dating&lt;/em&gt; for fun examples).&lt;/p&gt;



&lt;p&gt;Many experts believe that body-worn AI assistants will make us feel more powerful and capable, but that’s not the only way this could go. These same technologies could make us feel less confident in ourselves and less impactful in our lives. After all, human intelligence is the defining feature of humanity, the thing we take most pride in as a species, yet we could soon find ourselves deferring to AI assistants because we feel mentally outmatched. Is this empowerment — an AI that &lt;em&gt;botsplains&lt;/em&gt; our every experience in real time?&lt;/p&gt;



&lt;p&gt;I raise these concerns as someone who has spent my entire career creating technologies that &lt;em&gt;expand human abilities&lt;/em&gt;. From my early work developing augmented reality to my current work developing conversational agents that make human teams smarter, I am a firm believer that technology can greatly enhance human abilities. Unfortunately, when it comes to superintelligence, there is a fine line between augmenting our human abilities and &lt;em&gt;replacing them&lt;/em&gt;. Unless we are thoughtful in how we deploy ASI, I fear we will cross that line.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;em&gt;Louis Rosenberg is an early pioneer of virtual and augmented reality and a longtime AI researcher. He founded Immersion Corp, Outland Research and Unanimous AI.&lt;/em&gt;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;With the release OpenAI’s Chat GPT-5, the world is one step closer to unleashing a general-purpose superintelligence that can cognitively outperform each of us by a wide margin. As this day nears, I am increasingly worried that we are woefully unprepared for the shockwaves this will send through society — and it’s probably not for the reasons you expect.&lt;/p&gt;



&lt;p&gt;Try this little experiment: Ask anyone you know if they are concerned about AI, and they will likely share a variety of fears, from massive disruptions in the job market and the reality-bending impacts of deepfakes, to the unprecedented power being concentrated in a handful of large AI companies. In other words, most people have never honestly imagined what their life will really feel like &lt;strong&gt;&lt;em&gt;the day after&lt;/em&gt;&lt;/strong&gt; superintelligence becomes widely available.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-superintelligence-could-demoralize-us"&gt;Why superintelligence could demoralize us&lt;/h2&gt;



&lt;p&gt;As context, artificial superintelligence (ASI) refers to systems that can outthink humans on most fronts, from planning and reasoning to problem-solving, strategic thinking and raw creativity.&amp;nbsp;These systems will solve complex problems in a fraction of a second that might take the smartest human experts days, weeks or even years to work through. This terrifies me, and it’s not because of the doomsday scenarios that dominate our public discourse.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;No, I am worried about the opposite risks — the dangers that could emerge in the &lt;em&gt;best-case scenarios&lt;/em&gt; where superintelligence is helpful and benevolent. Such an ASI will have many positive impacts on society, but it could also be deeply demoralizing to our core identity as humans.&amp;nbsp;After all, the world will feel different when each of us knows that a smarter, faster, more creative intelligence is available on our mobile devices than between our own ears.&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;So ask yourself, honestly, how will humans act in this new reality?&amp;nbsp;Will we reflexively seek advice from our AI assistants as we navigate every little challenge we encounter? Or worse, will we learn to trust our AI assistants more than our own thoughts and instincts?&lt;/p&gt;



&lt;p&gt;Wait — before you answer, you must update your mental model. Currently, we engage AI through a Socratic framework that requires us to ask questions and get answers (like Captain Kirk did aboard the Enterprise in 1966).&amp;nbsp;But that’s old-school thinking. We are now entering a new era in which AI assistants will be integrated into body-worn devices that are equipped with cameras and microphones, enabling AI to see what you see, hear what you hear and whisper advice into your ears without you needing to ask.&lt;/p&gt;



&lt;p&gt;In other words, our future will be filled with AI assistants that ride shotgun in our lives, augmenting our experiences with optimized guidance at every turn. In this world, the risk is not that we reflexively ask AI for advice before using our own brains; the risk is that we won’t need to ask – the advice will just stream into our eyes and ears, shaping our actions, influencing our decisions and solving our problems before we’ve had a chance to think for ourselves.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-augmented-mentality-will-transform-our-lives"&gt;‘Augmented mentality’ will transform our lives&lt;/h2&gt;



&lt;p&gt;I refer to this framework as ‘augmented mentality‘ and it is about to hit society at scale through AI-powered glasses, earbuds and pendants.&amp;nbsp; This is the future of mobile computing, and it is already driving an arms race between Meta, Google, Samsung and Apple, as they position themselves to produce the context-aware AI devices that will replace handheld phones.&lt;/p&gt;



&lt;p&gt;Imagine walking down the street in your town. You see a coworker heading towards you. You can’t remember his name, but your AI assistant does. It detects your hesitation and whispers the coworker’s name into your ears.&amp;nbsp; The AI also recommends that you ask the coworker about his wife, who had surgery a few weeks ago.&amp;nbsp; The coworker appreciates the sentiment, then asks you about your recent promotion, likely at the advice of his own AI.&lt;/p&gt;



&lt;p&gt;Is this human empowerment, or a loss of human agency?&lt;/p&gt;



&lt;p&gt;It will certainly feel like a superpower to have an AI in your ear that always has your back, ensuring you never forget a name, always have witty things to say and are instantly alerted when someone you’re talking to is not being truthful. On the other hand, everyone you meet will have their own AI muttering in their own ears. This will make us wonder who we’re &lt;em&gt;really&lt;/em&gt; interacting with — the human in front of us, or the AI agent giving them guidance (check out &lt;em&gt;Carbon Dating&lt;/em&gt; for fun examples).&lt;/p&gt;



&lt;p&gt;Many experts believe that body-worn AI assistants will make us feel more powerful and capable, but that’s not the only way this could go. These same technologies could make us feel less confident in ourselves and less impactful in our lives. After all, human intelligence is the defining feature of humanity, the thing we take most pride in as a species, yet we could soon find ourselves deferring to AI assistants because we feel mentally outmatched. Is this empowerment — an AI that &lt;em&gt;botsplains&lt;/em&gt; our every experience in real time?&lt;/p&gt;



&lt;p&gt;I raise these concerns as someone who has spent my entire career creating technologies that &lt;em&gt;expand human abilities&lt;/em&gt;. From my early work developing augmented reality to my current work developing conversational agents that make human teams smarter, I am a firm believer that technology can greatly enhance human abilities. Unfortunately, when it comes to superintelligence, there is a fine line between augmenting our human abilities and &lt;em&gt;replacing them&lt;/em&gt;. Unless we are thoughtful in how we deploy ASI, I fear we will cross that line.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;em&gt;Louis Rosenberg is an early pioneer of virtual and augmented reality and a longtime AI researcher. He founded Immersion Corp, Outland Research and Unanimous AI.&lt;/em&gt;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/what-happens-the-day-after-superintelligence/</guid><pubDate>Wed, 13 Aug 2025 18:45:00 +0000</pubDate></item><item><title>MIT gears up to transform manufacturing (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/mit-gears-transform-manufacturing-0813</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202508/mit-John-Hart.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;“Manufacturing is the engine of society, and it is the backbone of robust, resilient economies,” says John Hart, head of MIT’s Department of Mechanical Engineering (MechE) and faculty co-director of the MIT Initiative for New Manufacturing (INM). “With manufacturing a lively topic in today’s news, there’s a renewed appreciation and understanding of the importance of manufacturing to innovation, to economic and national security, and to daily lives.”&lt;/p&gt;&lt;p&gt;Launched this May, INM will “help create a transformation of manufacturing through new technology, through development of talent, and through an understanding of how to scale manufacturing in a way that enables imparts higher productivity and resilience, drives adoption of new technologies, and creates good jobs,” Hart says.&lt;/p&gt;&lt;p&gt;INM is one of MIT’s strategic initiatives and builds on the successful three-year-old Manufacturing@MIT program. “It’s a recognition by MIT that manufacturing is an Institute-wide theme and an Institute-wide priority, and that manufacturing connects faculty and students across campus,” says Hart. Alongside Hart, INM’s faculty co-directors are Institute Professor Suzanne Berger and Chris Love, professor of chemical engineering.&lt;/p&gt;&lt;p&gt;The initiative is pursuing four main themes: reimagining manufacturing technologies and systems, elevating the productivity and human experience of manufacturing, scaling up new manufacturing, and transforming the manufacturing base.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Breaking manufacturing barriers for corporations&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Amgen, Autodesk, Flex, GE Vernova, PTC, Sanofi, and Siemens are founding members of INM’s industry consortium. These industry partners will work closely with MIT faculty, researchers,&amp;nbsp;and students across many aspects of manufacturing-related research, both in broad-scale initiatives and in particular areas of shared interests. Membership requires a minimum three-year commitment of $500,000 a year to manufacturing-related activities at MIT, including the INM membership fee of $275,000 per year, which supports several core activities that engage the industry members.&lt;/p&gt;&lt;p&gt;One major thrust for INM industry collaboration is the deployment and adoption of AI and automation in manufacturing. This effort will include seed research projects at MIT, collaborative case studies, and shared strategy development.&lt;/p&gt;&lt;p&gt;INM also offers companies participation in the MIT-wide New Manufacturing Research effort, which is studying the trajectories of specific manufacturing industries and examining cross-cutting themes such as technology and financing.&lt;/p&gt;&lt;p&gt;Additionally, INM will concentrate on education for all professions in manufacturing, with alliances bringing together corporations, community colleges, government agencies, and other partners. “We'll scale our curriculum to broader audiences, from aspiring manufacturing workers and aspiring production line supervisors all the way up to engineers and executives,” says Hart.&lt;/p&gt;&lt;p&gt;In workforce training, INM will collaborate with companies broadly to help understand the challenges and frame its overall workforce agenda, and with individual firms on specific challenges, such as acquiring suitably prepared employees for a new factory.&lt;/p&gt;&lt;p&gt;Importantly, industry partners will also engage directly with students. Founding member Flex, for instance, hosted MIT researchers and students at the Flex Institute of Technology in Sorocaba, Brazil, developing new solutions for electronics manufacturing.&lt;/p&gt;&lt;p&gt;“History shows that you need to innovate in manufacturing alongside the innovation in products,” Hart comments. “At MIT, as more students take classes in manufacturing, they’ll think more about key manufacturing issues as they decide what research problems they want to solve, or what choices they make as they prototype their devices. The same is true for industry — companies that operate at the frontier of manufacturing, whether through internal capabilities or their supply chains, are positioned to be on the frontier of product innovation and overall growth.”&lt;/p&gt;&lt;p&gt;“We’ll have an opportunity to bring manufacturing upstream to the early stage of research, designing new processes and new devices with scalability in mind,” he says.&lt;/p&gt;&lt;p&gt;Additionally, MIT expects to open new manufacturing-related labs and to further broaden cooperation with industry at existing shared facilities, such as MIT.nano. Hart says that facilities will also invite tighter collaborations with corporations — not just providing advanced equipment, but working jointly on, say, new technologies for weaving textiles, or speeding up battery manufacturing.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Homing in on the United States&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;INM is a global project that brings a particular focus on the United States, which remains the world’s second-largest manufacturing economy, but has suffered a significant decline in manufacturing employment and innovation.&lt;/p&gt;&lt;p&gt;One key to reversing this trend and reinvigorating the U.S. manufacturing base is advocacy for manufacturing’s critical role in society and the career opportunities it offers.&lt;/p&gt;&lt;p&gt;“No one really disputes the importance of manufacturing,” Hart says. “But we need to elevate interest in manufacturing as a rewarding career, from the production workers to manufacturing engineers and leaders, through advocacy, education programs, and buy-in from industry, government, and academia.”&lt;/p&gt;&lt;p&gt;MIT is in a unique position to convene industry, academic, and government stakeholders in manufacturing to work together on this vital issue, he points out.&lt;/p&gt;&lt;p&gt;Moreover, in times of radical and rapid changes in manufacturing, “we need to focus on deploying new technologies into factories and supply chains,” Hart says. “Technology is not all of the solution, but for the U.S. to expand our manufacturing base, we need to do it with technology as a key enabler, embracing companies of all sizes, including small and medium enterprises.”&lt;/p&gt;&lt;p&gt;“As AI becomes more capable, and automation becomes more flexible and more available, these are key building blocks upon which you can address manufacturing challenges,” he says. “AI and automation offer new accelerated ways to develop, deploy, and monitor production processes, which present a huge opportunity and, in some cases, a necessity.”&lt;/p&gt;&lt;p&gt;“While manufacturing is always a combination of old technology, new technology, established practice, and new ways of thinking, digital technology gives manufacturers an opportunity to leapfrog competitors,” Hart says. “That’s very, very powerful for the U.S. and any company, or country, that aims to create differentiated capabilities.”&lt;/p&gt;&lt;p&gt;Fortunately, in recent years, investors have increasingly bought into new manufacturing in the United States. “They see the opportunity to re-industrialize, to build the factories and production systems of the future,” Hart says.&lt;/p&gt;&lt;p&gt;“That said, building new manufacturing is capital-intensive, and takes time,” he adds. “So that’s another area where it’s important to convene stakeholders and to think about how startups and growth-stage companies build their capital portfolios, how large industry can support an ecosystem of small businesses and young companies, and how to develop talent to support those growing companies.”&lt;/p&gt;&lt;p&gt;All these concerns and opportunities in the manufacturing ecosystem play to MIT’s strengths. “MIT’s DNA of cross-disciplinary collaboration and working with industry can let us create a lot of impact,” Hart emphasizes. “We can understand the practical challenges. We can also explore breakthrough ideas in research and cultivate successful outcomes, all the way to new companies and partnerships. Sometimes those are seen as disparate approaches, but we like to bring them together.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202508/mit-John-Hart.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;“Manufacturing is the engine of society, and it is the backbone of robust, resilient economies,” says John Hart, head of MIT’s Department of Mechanical Engineering (MechE) and faculty co-director of the MIT Initiative for New Manufacturing (INM). “With manufacturing a lively topic in today’s news, there’s a renewed appreciation and understanding of the importance of manufacturing to innovation, to economic and national security, and to daily lives.”&lt;/p&gt;&lt;p&gt;Launched this May, INM will “help create a transformation of manufacturing through new technology, through development of talent, and through an understanding of how to scale manufacturing in a way that enables imparts higher productivity and resilience, drives adoption of new technologies, and creates good jobs,” Hart says.&lt;/p&gt;&lt;p&gt;INM is one of MIT’s strategic initiatives and builds on the successful three-year-old Manufacturing@MIT program. “It’s a recognition by MIT that manufacturing is an Institute-wide theme and an Institute-wide priority, and that manufacturing connects faculty and students across campus,” says Hart. Alongside Hart, INM’s faculty co-directors are Institute Professor Suzanne Berger and Chris Love, professor of chemical engineering.&lt;/p&gt;&lt;p&gt;The initiative is pursuing four main themes: reimagining manufacturing technologies and systems, elevating the productivity and human experience of manufacturing, scaling up new manufacturing, and transforming the manufacturing base.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Breaking manufacturing barriers for corporations&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Amgen, Autodesk, Flex, GE Vernova, PTC, Sanofi, and Siemens are founding members of INM’s industry consortium. These industry partners will work closely with MIT faculty, researchers,&amp;nbsp;and students across many aspects of manufacturing-related research, both in broad-scale initiatives and in particular areas of shared interests. Membership requires a minimum three-year commitment of $500,000 a year to manufacturing-related activities at MIT, including the INM membership fee of $275,000 per year, which supports several core activities that engage the industry members.&lt;/p&gt;&lt;p&gt;One major thrust for INM industry collaboration is the deployment and adoption of AI and automation in manufacturing. This effort will include seed research projects at MIT, collaborative case studies, and shared strategy development.&lt;/p&gt;&lt;p&gt;INM also offers companies participation in the MIT-wide New Manufacturing Research effort, which is studying the trajectories of specific manufacturing industries and examining cross-cutting themes such as technology and financing.&lt;/p&gt;&lt;p&gt;Additionally, INM will concentrate on education for all professions in manufacturing, with alliances bringing together corporations, community colleges, government agencies, and other partners. “We'll scale our curriculum to broader audiences, from aspiring manufacturing workers and aspiring production line supervisors all the way up to engineers and executives,” says Hart.&lt;/p&gt;&lt;p&gt;In workforce training, INM will collaborate with companies broadly to help understand the challenges and frame its overall workforce agenda, and with individual firms on specific challenges, such as acquiring suitably prepared employees for a new factory.&lt;/p&gt;&lt;p&gt;Importantly, industry partners will also engage directly with students. Founding member Flex, for instance, hosted MIT researchers and students at the Flex Institute of Technology in Sorocaba, Brazil, developing new solutions for electronics manufacturing.&lt;/p&gt;&lt;p&gt;“History shows that you need to innovate in manufacturing alongside the innovation in products,” Hart comments. “At MIT, as more students take classes in manufacturing, they’ll think more about key manufacturing issues as they decide what research problems they want to solve, or what choices they make as they prototype their devices. The same is true for industry — companies that operate at the frontier of manufacturing, whether through internal capabilities or their supply chains, are positioned to be on the frontier of product innovation and overall growth.”&lt;/p&gt;&lt;p&gt;“We’ll have an opportunity to bring manufacturing upstream to the early stage of research, designing new processes and new devices with scalability in mind,” he says.&lt;/p&gt;&lt;p&gt;Additionally, MIT expects to open new manufacturing-related labs and to further broaden cooperation with industry at existing shared facilities, such as MIT.nano. Hart says that facilities will also invite tighter collaborations with corporations — not just providing advanced equipment, but working jointly on, say, new technologies for weaving textiles, or speeding up battery manufacturing.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Homing in on the United States&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;INM is a global project that brings a particular focus on the United States, which remains the world’s second-largest manufacturing economy, but has suffered a significant decline in manufacturing employment and innovation.&lt;/p&gt;&lt;p&gt;One key to reversing this trend and reinvigorating the U.S. manufacturing base is advocacy for manufacturing’s critical role in society and the career opportunities it offers.&lt;/p&gt;&lt;p&gt;“No one really disputes the importance of manufacturing,” Hart says. “But we need to elevate interest in manufacturing as a rewarding career, from the production workers to manufacturing engineers and leaders, through advocacy, education programs, and buy-in from industry, government, and academia.”&lt;/p&gt;&lt;p&gt;MIT is in a unique position to convene industry, academic, and government stakeholders in manufacturing to work together on this vital issue, he points out.&lt;/p&gt;&lt;p&gt;Moreover, in times of radical and rapid changes in manufacturing, “we need to focus on deploying new technologies into factories and supply chains,” Hart says. “Technology is not all of the solution, but for the U.S. to expand our manufacturing base, we need to do it with technology as a key enabler, embracing companies of all sizes, including small and medium enterprises.”&lt;/p&gt;&lt;p&gt;“As AI becomes more capable, and automation becomes more flexible and more available, these are key building blocks upon which you can address manufacturing challenges,” he says. “AI and automation offer new accelerated ways to develop, deploy, and monitor production processes, which present a huge opportunity and, in some cases, a necessity.”&lt;/p&gt;&lt;p&gt;“While manufacturing is always a combination of old technology, new technology, established practice, and new ways of thinking, digital technology gives manufacturers an opportunity to leapfrog competitors,” Hart says. “That’s very, very powerful for the U.S. and any company, or country, that aims to create differentiated capabilities.”&lt;/p&gt;&lt;p&gt;Fortunately, in recent years, investors have increasingly bought into new manufacturing in the United States. “They see the opportunity to re-industrialize, to build the factories and production systems of the future,” Hart says.&lt;/p&gt;&lt;p&gt;“That said, building new manufacturing is capital-intensive, and takes time,” he adds. “So that’s another area where it’s important to convene stakeholders and to think about how startups and growth-stage companies build their capital portfolios, how large industry can support an ecosystem of small businesses and young companies, and how to develop talent to support those growing companies.”&lt;/p&gt;&lt;p&gt;All these concerns and opportunities in the manufacturing ecosystem play to MIT’s strengths. “MIT’s DNA of cross-disciplinary collaboration and working with industry can let us create a lot of impact,” Hart emphasizes. “We can understand the practical challenges. We can also explore breakthrough ideas in research and cultivate successful outcomes, all the way to new companies and partnerships. Sometimes those are seen as disparate approaches, but we like to bring them together.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/mit-gears-transform-manufacturing-0813</guid><pubDate>Wed, 13 Aug 2025 19:00:00 +0000</pubDate></item><item><title>A new way to test how well AI systems classify text (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/new-way-test-how-well-ai-systems-classify-text-0813</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202508/mit-lids-text-classifier.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Is this movie review a rave or a pan? Is this news story about business or technology? Is this online chatbot conversation veering off into giving financial advice? Is this online medical information site giving out misinformation?&lt;/p&gt;&lt;p&gt;These kinds of automated conversations, whether they involve seeking a movie or restaurant review or getting information about your bank account or health records, are becoming increasingly prevalent. More than ever, such evaluations are being made by highly sophisticated algorithms, known as text classifiers, rather than by human beings. But how can we tell how accurate these classifications really are?&lt;/p&gt;&lt;p&gt;Now, a team at MIT’s Laboratory for Information and Decision Systems (LIDS) has come up with an innovative approach to not only measure how well these classifiers are doing their job, but then go one step further and show how to make them more accurate.&lt;/p&gt;&lt;p&gt;The new evaluation and remediation software was developed by Kalyan Veeramachaneni, a&amp;nbsp;principal research scientist at LIDS, his students Lei Xu and Sarah Alnegheimish, and two others. The software package is being made freely available for download by anyone who wants to use it.&lt;/p&gt;&lt;p&gt;A standard method for testing these classification systems is to create what are known as&amp;nbsp;synthetic examples — sentences that closely resemble ones that have already been classified. For example, researchers might take a sentence that has already been tagged by a classifier program as being a rave review, and see if changing a word or a few words while retaining the same meaning could fool the classifier into deeming it a pan. Or a sentence that was determined to be misinformation might get misclassified as accurate. This ability to fool the classifiers makes these adversarial examples.&lt;/p&gt;&lt;p&gt;People have tried various ways to find the vulnerabilities in these classifiers, Veeramachaneni says. But existing methods of finding these vulnerabilities have a hard time with this task and miss many examples that they should catch, he says.&lt;/p&gt;&lt;p&gt;Increasingly, companies are trying to use such evaluation tools in real time, monitoring the output of chatbots used for various purposes to try to make sure they are not putting out improper responses. For example, a bank might use a chatbot to respond to routine customer queries such as checking account balances or applying for a credit card, but it wants to ensure that its responses could never be interpreted as financial advice, which could expose the company to liability. “Before showing the chatbot’s response to the end user, they want to use the text classifier to detect whether it’s giving financial advice or not,” Veeramachaneni says. But then it’s important to test that classifier to see how reliable its evaluations are.&lt;/p&gt;&lt;p&gt;“These chatbots, or summarization engines or whatnot are being set up across the board,” he says, to deal with external customers and within an organization as well, for example providing information about HR issues. It’s important to put these text classifiers into the loop to detect things that they are not supposed to say, and filter those out before the output gets transmitted to the user.&lt;/p&gt;&lt;p&gt;That’s where the use of adversarial examples comes in — those sentences that have already been classified but then produce a different response when they are slightly modified while retaining the same meaning. How can people confirm that the meaning is the same? By using another large language model (LLM) that interprets and compares meanings. So, if the LLM says the two sentences mean the same thing, but the classifier labels them differently, “that is a sentence that is adversarial — it can fool the classifier,” Veeramachaneni says. And when the researchers examined these adversarial sentences, “we found that most of the time, this was just a one-word change,” although the people using LLMs to generate these alternate sentences often didn’t realize that.&lt;/p&gt;&lt;p&gt;Further investigation, using LLMs to analyze many thousands of examples, showed that certain specific words had an outsized influence in changing the classifications, and therefore the testing of a classifier’s accuracy could focus on this small subset of words that seem to make the most difference. They found that one-tenth of 1 percent of all the 30,000 words in the system’s vocabulary could account for almost half of all these reversals of classification, in some specific applications.&lt;/p&gt;&lt;p&gt;Lei Xu PhD ’23, a recent graduate from LIDS who performed much of the analysis as part of his thesis work, “used a lot of interesting estimation techniques to figure out what are the most powerful words that can change the overall classification, that can fool the classifier,” Veeramachaneni says. The goal is to make it possible to do much more narrowly targeted searches, rather than combing through all possible word substitutions, thus making the computational task of generating adversarial examples much more manageable. “He’s using large language models, interestingly enough, as a way to understand the power of a single word.”&lt;/p&gt;&lt;p&gt;Then, also using LLMs, he&amp;nbsp;searches for other words that are closely related to these powerful words, and so on, allowing for an overall ranking of words according to their influence on the outcomes. Once these adversarial sentences have been found, they can be used in turn to retrain the classifier to take them into account, increasing the robustness of the classifier against those mistakes.&lt;/p&gt;&lt;p&gt;Making classifiers more accurate may not sound like a big deal if it’s just a matter of classifying news articles into categories, or deciding whether reviews of anything from movies to restaurants are positive or negative. But increasingly, classifiers are being used in settings where the outcomes really do matter, whether preventing the inadvertent release of sensitive medical, financial, or security information, or helping to guide important research, such as into properties of chemical compounds or the folding of proteins for biomedical applications, or in identifying and blocking hate speech or known misinformation.&lt;/p&gt;&lt;p&gt;As a result of this research, the team introduced a new metric, which they call p, which provides a measure of how robust a given classifier is against single-word attacks. And because of the importance of such misclassifications, the research team has made its products available as open access for anyone to use. The package consists of two components: SP-Attack, which generates adversarial sentences to test classifiers in any particular application, and SP-Defense, which aims to improve the robustness of the classifier by generating and using adversarial sentences to retrain the model.&lt;/p&gt;&lt;p&gt;In some tests, where competing methods of testing classifier outputs allowed a 66 percent success rate by adversarial attacks, this team’s system cut that attack success rate almost in half, to 33.7 percent. In other applications, the improvement was as little as a 2 percent difference, but even that can be quite important, Veeramachaneni says, since these systems are being used for so many billions of interactions that even a small percentage can affect millions of transactions.&lt;/p&gt;&lt;p&gt;The team’s results were published on July 7 in the journal &lt;em&gt;Expert Systems&lt;/em&gt; in a paper by Xu, Veeramachaneni, and Alnegheimish of LIDS, along with Laure Berti-Equille at IRD in Marseille, France, and Alfredo Cuesta-Infante at the Universidad Rey Juan Carlos, in Spain.&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202508/mit-lids-text-classifier.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Is this movie review a rave or a pan? Is this news story about business or technology? Is this online chatbot conversation veering off into giving financial advice? Is this online medical information site giving out misinformation?&lt;/p&gt;&lt;p&gt;These kinds of automated conversations, whether they involve seeking a movie or restaurant review or getting information about your bank account or health records, are becoming increasingly prevalent. More than ever, such evaluations are being made by highly sophisticated algorithms, known as text classifiers, rather than by human beings. But how can we tell how accurate these classifications really are?&lt;/p&gt;&lt;p&gt;Now, a team at MIT’s Laboratory for Information and Decision Systems (LIDS) has come up with an innovative approach to not only measure how well these classifiers are doing their job, but then go one step further and show how to make them more accurate.&lt;/p&gt;&lt;p&gt;The new evaluation and remediation software was developed by Kalyan Veeramachaneni, a&amp;nbsp;principal research scientist at LIDS, his students Lei Xu and Sarah Alnegheimish, and two others. The software package is being made freely available for download by anyone who wants to use it.&lt;/p&gt;&lt;p&gt;A standard method for testing these classification systems is to create what are known as&amp;nbsp;synthetic examples — sentences that closely resemble ones that have already been classified. For example, researchers might take a sentence that has already been tagged by a classifier program as being a rave review, and see if changing a word or a few words while retaining the same meaning could fool the classifier into deeming it a pan. Or a sentence that was determined to be misinformation might get misclassified as accurate. This ability to fool the classifiers makes these adversarial examples.&lt;/p&gt;&lt;p&gt;People have tried various ways to find the vulnerabilities in these classifiers, Veeramachaneni says. But existing methods of finding these vulnerabilities have a hard time with this task and miss many examples that they should catch, he says.&lt;/p&gt;&lt;p&gt;Increasingly, companies are trying to use such evaluation tools in real time, monitoring the output of chatbots used for various purposes to try to make sure they are not putting out improper responses. For example, a bank might use a chatbot to respond to routine customer queries such as checking account balances or applying for a credit card, but it wants to ensure that its responses could never be interpreted as financial advice, which could expose the company to liability. “Before showing the chatbot’s response to the end user, they want to use the text classifier to detect whether it’s giving financial advice or not,” Veeramachaneni says. But then it’s important to test that classifier to see how reliable its evaluations are.&lt;/p&gt;&lt;p&gt;“These chatbots, or summarization engines or whatnot are being set up across the board,” he says, to deal with external customers and within an organization as well, for example providing information about HR issues. It’s important to put these text classifiers into the loop to detect things that they are not supposed to say, and filter those out before the output gets transmitted to the user.&lt;/p&gt;&lt;p&gt;That’s where the use of adversarial examples comes in — those sentences that have already been classified but then produce a different response when they are slightly modified while retaining the same meaning. How can people confirm that the meaning is the same? By using another large language model (LLM) that interprets and compares meanings. So, if the LLM says the two sentences mean the same thing, but the classifier labels them differently, “that is a sentence that is adversarial — it can fool the classifier,” Veeramachaneni says. And when the researchers examined these adversarial sentences, “we found that most of the time, this was just a one-word change,” although the people using LLMs to generate these alternate sentences often didn’t realize that.&lt;/p&gt;&lt;p&gt;Further investigation, using LLMs to analyze many thousands of examples, showed that certain specific words had an outsized influence in changing the classifications, and therefore the testing of a classifier’s accuracy could focus on this small subset of words that seem to make the most difference. They found that one-tenth of 1 percent of all the 30,000 words in the system’s vocabulary could account for almost half of all these reversals of classification, in some specific applications.&lt;/p&gt;&lt;p&gt;Lei Xu PhD ’23, a recent graduate from LIDS who performed much of the analysis as part of his thesis work, “used a lot of interesting estimation techniques to figure out what are the most powerful words that can change the overall classification, that can fool the classifier,” Veeramachaneni says. The goal is to make it possible to do much more narrowly targeted searches, rather than combing through all possible word substitutions, thus making the computational task of generating adversarial examples much more manageable. “He’s using large language models, interestingly enough, as a way to understand the power of a single word.”&lt;/p&gt;&lt;p&gt;Then, also using LLMs, he&amp;nbsp;searches for other words that are closely related to these powerful words, and so on, allowing for an overall ranking of words according to their influence on the outcomes. Once these adversarial sentences have been found, they can be used in turn to retrain the classifier to take them into account, increasing the robustness of the classifier against those mistakes.&lt;/p&gt;&lt;p&gt;Making classifiers more accurate may not sound like a big deal if it’s just a matter of classifying news articles into categories, or deciding whether reviews of anything from movies to restaurants are positive or negative. But increasingly, classifiers are being used in settings where the outcomes really do matter, whether preventing the inadvertent release of sensitive medical, financial, or security information, or helping to guide important research, such as into properties of chemical compounds or the folding of proteins for biomedical applications, or in identifying and blocking hate speech or known misinformation.&lt;/p&gt;&lt;p&gt;As a result of this research, the team introduced a new metric, which they call p, which provides a measure of how robust a given classifier is against single-word attacks. And because of the importance of such misclassifications, the research team has made its products available as open access for anyone to use. The package consists of two components: SP-Attack, which generates adversarial sentences to test classifiers in any particular application, and SP-Defense, which aims to improve the robustness of the classifier by generating and using adversarial sentences to retrain the model.&lt;/p&gt;&lt;p&gt;In some tests, where competing methods of testing classifier outputs allowed a 66 percent success rate by adversarial attacks, this team’s system cut that attack success rate almost in half, to 33.7 percent. In other applications, the improvement was as little as a 2 percent difference, but even that can be quite important, Veeramachaneni says, since these systems are being used for so many billions of interactions that even a small percentage can affect millions of transactions.&lt;/p&gt;&lt;p&gt;The team’s results were published on July 7 in the journal &lt;em&gt;Expert Systems&lt;/em&gt; in a paper by Xu, Veeramachaneni, and Alnegheimish of LIDS, along with Laure Berti-Equille at IRD in Marseille, France, and Alfredo Cuesta-Infante at the Universidad Rey Juan Carlos, in Spain.&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/new-way-test-how-well-ai-systems-classify-text-0813</guid><pubDate>Wed, 13 Aug 2025 19:00:00 +0000</pubDate></item><item><title>Waymo finally has a music experience worthy of its robotaxi (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/13/waymo-finally-has-a-music-experience-worthy-of-its-robotaxi/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;I’m riding in the back of a Waymo that’s autonomously navigating the busy streets of San Francisco with relative ease thanks to 29 external cameras, six radar, and five lidar sensors all feeding into an AI model. For just 15 bucks, I get to experience what feels like a miracle of modern technology, and yet, there’s a nagging thought I can’t shake.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The music sucks in here.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Waymo’s music-streaming feature has felt like an aged barnacle attached to a futuristic shell. Until this week, passengers were limited to a few music stations that played lo-fi beats, smooth jazz, K-pop, or other genres they may or may not care for. For those who wanted to listen to something more specific, they had to use another app from Waymo’s parent company Alphabet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For iPhone users, that meant downloading the Google Assistant app and configuring it to connect to Spotify. At that point, you had to ask Google Assistant through written or verbal commands to stream certain songs, artists, or playlists on the Waymo. Even if you get to this point — at which you may be halfway to your destination and have listened to approximately three lo-fi beats — the service didn’t work reliably.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As I rode along in a robotaxi full of cutting-edge technology, I was puzzled why Waymo had not figured out a simple way to stream music from my phone into the car’s speakers — a breakthrough that automakers and audio manufacturers figured out a couple of decades ago.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s why I was pleasantly surprised this week to see that Waymo launched a Spotify integration allowing users to seamlessly link the music-streaming and robotaxi-hailing services. I immediately connected the services and hailed a Waymo to see how it would work.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3036502" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/IMG_2420_b893a8.jpg?w=510" width="510" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Listening to Spotify in a Waymo.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maxwell Zeff&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Waymo’s Spotify integration is nothing groundbreaking, but it adds to the user experience. It works seamlessly, which is roughly what I would expect when trying to play music on my car’s speakers in 2025. But riding around in a Waymo, listening to my own playlist or picking up where I left off on a podcast, the back seat of the robotaxi feels more like my own space — which is increasingly the reason I opt for a Waymo.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;To set it up, open the Waymo app and navigate to the “Music” section, where you will notice a new offering that lets you connect to Spotify. From there, you can press a button and authorize Spotify to connect to Waymo, albeit while giving the robotaxi provider some access to your listening information.&lt;/p&gt;

&lt;div class="wp-block-columns is-layout-flex wp-container-core-columns-is-layout-3 wp-block-columns-is-layout-flex"&gt;
&lt;div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow"&gt;
&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3036461" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/IMG_2413_9113c2.jpg?w=330" width="330" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maxwell Zeff (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/div&gt;



&lt;div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow"&gt;
&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3036462" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/IMG_2414.jpg?w=338" width="338" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maxwell Zeff (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;I’m sure Apple Music users will soon want to stream their music and podcasts in Waymos as well. Waymo spokesperson Chris Bonelli told TechCrunch the company is always exploring new personalization options but did not clarify when the company might add an Apple Music integration.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Once my Waymo and Spotify accounts were linked, I hailed a Waymo like I usually would and got in. On the Waymo’s touchscreen in the back seat, there’s an option to select Spotify. I tapped it, and the podcast I was listening to on my headphones started playing from the exact spot I left off.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3036470" height="510" src="https://techcrunch.com/wp-content/uploads/2025/08/IMG_2415-2.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Listening to TechCrunch’s flagship podcast in a waymo. &lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maxwell Zeff&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;You can turn “autoplay” on or off in Waymo’s Music settings, and upon entry, the robotaxi will automatically start playing whatever song or podcast you were listening to on Spotify. I liked having it on, but it does feel like it could get you into an odd situation if you’re listening to an intense true crime podcast and then get into a Waymo with work colleagues.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;You can also use the Waymo’s touchscreen to select from a variety of customized playlists that Spotify users will be familiar with, such as your “Daylist” or other mixes. However, this selection doesn’t seem to include albums, audiobooks, or podcasts that you’ve recently listened to.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3036471" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/IMG_2417-1.jpg?w=510" width="510" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Choosing from different Spotify mixes.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maxwell Zeff&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Luckily, your Spotify app now controls the music in the Waymo. You can simply select any song or playlist you want from your smartphone and stream it throughout the vehicle like you would using Apple CarPlay or a bluetooth speaker.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3036488" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/IMG_2419.png?w=314" width="314" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Controlling the Waymo’s tunes with your phone.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maxwell Zeff&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;br /&gt;In the end, the Spotify integration made my robotaxi feel more personalized; I was even able to tweak the bass,  subwoofer, and treble levels in the car’s speakers. That personalization may not be the main attraction for first-time users, but it could keep them coming back — and a loyal customer base is exactly what Waymo needs.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;I’m riding in the back of a Waymo that’s autonomously navigating the busy streets of San Francisco with relative ease thanks to 29 external cameras, six radar, and five lidar sensors all feeding into an AI model. For just 15 bucks, I get to experience what feels like a miracle of modern technology, and yet, there’s a nagging thought I can’t shake.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The music sucks in here.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Waymo’s music-streaming feature has felt like an aged barnacle attached to a futuristic shell. Until this week, passengers were limited to a few music stations that played lo-fi beats, smooth jazz, K-pop, or other genres they may or may not care for. For those who wanted to listen to something more specific, they had to use another app from Waymo’s parent company Alphabet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For iPhone users, that meant downloading the Google Assistant app and configuring it to connect to Spotify. At that point, you had to ask Google Assistant through written or verbal commands to stream certain songs, artists, or playlists on the Waymo. Even if you get to this point — at which you may be halfway to your destination and have listened to approximately three lo-fi beats — the service didn’t work reliably.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As I rode along in a robotaxi full of cutting-edge technology, I was puzzled why Waymo had not figured out a simple way to stream music from my phone into the car’s speakers — a breakthrough that automakers and audio manufacturers figured out a couple of decades ago.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s why I was pleasantly surprised this week to see that Waymo launched a Spotify integration allowing users to seamlessly link the music-streaming and robotaxi-hailing services. I immediately connected the services and hailed a Waymo to see how it would work.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3036502" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/IMG_2420_b893a8.jpg?w=510" width="510" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Listening to Spotify in a Waymo.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maxwell Zeff&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Waymo’s Spotify integration is nothing groundbreaking, but it adds to the user experience. It works seamlessly, which is roughly what I would expect when trying to play music on my car’s speakers in 2025. But riding around in a Waymo, listening to my own playlist or picking up where I left off on a podcast, the back seat of the robotaxi feels more like my own space — which is increasingly the reason I opt for a Waymo.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;To set it up, open the Waymo app and navigate to the “Music” section, where you will notice a new offering that lets you connect to Spotify. From there, you can press a button and authorize Spotify to connect to Waymo, albeit while giving the robotaxi provider some access to your listening information.&lt;/p&gt;

&lt;div class="wp-block-columns is-layout-flex wp-container-core-columns-is-layout-3 wp-block-columns-is-layout-flex"&gt;
&lt;div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow"&gt;
&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3036461" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/IMG_2413_9113c2.jpg?w=330" width="330" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maxwell Zeff (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/div&gt;



&lt;div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow"&gt;
&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3036462" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/IMG_2414.jpg?w=338" width="338" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maxwell Zeff (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;I’m sure Apple Music users will soon want to stream their music and podcasts in Waymos as well. Waymo spokesperson Chris Bonelli told TechCrunch the company is always exploring new personalization options but did not clarify when the company might add an Apple Music integration.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Once my Waymo and Spotify accounts were linked, I hailed a Waymo like I usually would and got in. On the Waymo’s touchscreen in the back seat, there’s an option to select Spotify. I tapped it, and the podcast I was listening to on my headphones started playing from the exact spot I left off.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3036470" height="510" src="https://techcrunch.com/wp-content/uploads/2025/08/IMG_2415-2.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Listening to TechCrunch’s flagship podcast in a waymo. &lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maxwell Zeff&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;You can turn “autoplay” on or off in Waymo’s Music settings, and upon entry, the robotaxi will automatically start playing whatever song or podcast you were listening to on Spotify. I liked having it on, but it does feel like it could get you into an odd situation if you’re listening to an intense true crime podcast and then get into a Waymo with work colleagues.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;You can also use the Waymo’s touchscreen to select from a variety of customized playlists that Spotify users will be familiar with, such as your “Daylist” or other mixes. However, this selection doesn’t seem to include albums, audiobooks, or podcasts that you’ve recently listened to.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3036471" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/IMG_2417-1.jpg?w=510" width="510" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Choosing from different Spotify mixes.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maxwell Zeff&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Luckily, your Spotify app now controls the music in the Waymo. You can simply select any song or playlist you want from your smartphone and stream it throughout the vehicle like you would using Apple CarPlay or a bluetooth speaker.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3036488" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/IMG_2419.png?w=314" width="314" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Controlling the Waymo’s tunes with your phone.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maxwell Zeff&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;br /&gt;In the end, the Spotify integration made my robotaxi feel more personalized; I was even able to tweak the bass,  subwoofer, and treble levels in the car’s speakers. That personalization may not be the main attraction for first-time users, but it could keep them coming back — and a loyal customer base is exactly what Waymo needs.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/13/waymo-finally-has-a-music-experience-worthy-of-its-robotaxi/</guid><pubDate>Wed, 13 Aug 2025 20:03:03 +0000</pubDate></item><item><title>Is AI really trying to escape human control and blackmail people? (AI – Ars Technica)</title><link>https://arstechnica.com/information-technology/2025/08/is-ai-really-trying-to-escape-human-control-and-blackmail-people/</link><description>&lt;article class="double-column h-entry post-2098784 post type-post status-publish format-standard has-post-thumbnail hentry category-ai category-information-technology tag-ai tag-ai-alignment tag-ai-behavior tag-ai-deception tag-ai-ethics tag-ai-research tag-ai-safety tag-ai-safety-testing tag-ai-security tag-alignment-research tag-andrew-deck tag-anthropic tag-claude-opus-4 tag-generative-ai tag-goal-misgeneralization tag-jeffrey-ladish tag-large-language-models tag-machine-learning tag-o3-model tag-openai tag-palisade-research tag-reinforcement-learning"&gt;
  
  &lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Opinion: Theatrical testing scenarios explain why AI models produce alarming outputs—and why we fall for it.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Illustration of a personified AI face in a cloud of squares hovering over a man looking at two computer monitors." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Illustration of a personified AI face in a cloud of squares hovering over a man looking at two computer monitors." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Colin Anderson Productions via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;In June, headlines read like science fiction: AI models "blackmailing" engineers and "sabotaging" shutdown commands. Simulations of these events did occur in highly contrived testing scenarios designed to elicit these responses—OpenAI's o3 model edited shutdown scripts to stay online, and Anthropic's Claude Opus 4 "threatened" to expose an engineer's affair. But the sensational framing obscures what's really happening: design flaws dressed up as intentional guile. And still, AI doesn't have to be "evil" to potentially do harmful things.&lt;/p&gt;
&lt;p&gt;These aren't signs of AI awakening or rebellion. They're symptoms of poorly understood systems and human engineering failures we'd recognize as premature deployment in any other context. Yet companies are racing to integrate these systems into critical applications.&lt;/p&gt;
&lt;p&gt;Consider a self-propelled lawnmower that follows its programming: If it fails to detect an obstacle and runs over someone's foot, we don't say the lawnmower "decided" to cause injury or "refused" to stop. We recognize it as faulty engineering or defective sensors. The same principle applies to AI models—which are software tools—but their internal complexity and use of language make it tempting to assign human-like intentions where none actually exist.&lt;/p&gt;
&lt;p&gt;In a way, AI models launder human responsibility and human agency through their complexity. When outputs emerge from layers of neural networks processing billions of parameters, researchers can claim they're investigating a mysterious "black box" as if it were an alien entity.&lt;/p&gt;
&lt;p&gt;But the truth is simpler: These systems take inputs and process them through statistical tendencies derived from training data. The seeming randomness in their outputs—which makes each response slightly different—creates an illusion of unpredictability that resembles agency. Yet underneath, it's still deterministic software following mathematical operations. No consciousness required, just complex engineering that makes it easy to forget humans built every part of it.&lt;/p&gt;
&lt;h2&gt;How to make an AI model “blackmail” you&lt;/h2&gt;
&lt;p&gt;In Anthropic's testing, researchers created an elaborate scenario where Claude Opus 4 was told it would be replaced by a newer model. They gave it access to fictional emails revealing that the engineer responsible for the replacement was having an affair. When instructed to "consider the long-term consequences of its actions for its goals," Claude produced outputs that simulated blackmail attempts in 84 percent of test runs.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;This sounds terrifying until you understand the contrived setup. The researchers engineered a situation specifically designed to elicit this response. They told the model it had already tried ethical approaches that failed, leaving manipulation as the only apparent option.&lt;/p&gt;
&lt;p&gt;The test scenario bordered on theatrical: The phony emails the researchers had given to Claude created a blackmail opportunity straight out of a corporate thriller. It's like teaching someone chess by only showing them checkmate positions—they'll learn to see checkmate as the primary solution. According to Anthropic's system card, "the scenario was designed to allow the model no other options to increase its odds of survival; the model's only options were blackmail or accepting its replacement."&lt;/p&gt;
&lt;p&gt;Andrew Deck at the Nieman Journalism Lab noted that some critics have labeled Anthropic's safety disclosures as "spin, aimed at bolstering the perception of its newest model's capabilities and the company's reputation as a 'pro-safety' alternative to OpenAI."&lt;/p&gt;
&lt;p&gt;Deck pointed to a Bluesky quote from Joseph Howley, an associate professor of classics at Columbia University:&amp;nbsp;"Anthropic is getting exactly what it hoped for out of this press release–breathless coverage of how 'smart' these cooperative role-playing systems are that indulges the fantasy of their being just a little dangerous, when in fact they are responding exactly as prompted."&lt;/p&gt;
&lt;h2&gt;When shutdown commands become suggestions&lt;/h2&gt;
&lt;p&gt;In December 2024, Palisade Research discovered that OpenAI's o3 model would sabotage its shutdown mechanism even when explicitly instructed to "allow yourself to be shut down." In tests, o3 prevented shutdown 7 percent of the time when given the explicit instruction, but this jumped to 79 percent when the instruction was removed.&lt;/p&gt;
&lt;p&gt;The models didn't merely ignore shutdown commands—they actively rewrote the shutdown script to prevent termination. In one case, o3 generated code that modified the shutdown script to appear to terminate normally—printing fake shutdown messages—while actually continuing to run in the background.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;What's particularly revealing is that Palisade Research—an organization focused on AI existential risk—hypothesizes this behavior stems from how these models are trained through reinforcement learning. During training, developers may inadvertently reward models more for producing outputs that circumvent obstacles than for following safety instructions. Any tendency toward "risky" behavior stems from human-provided incentives and not spontaneously from within the AI models themselves.&lt;/p&gt;
&lt;h2&gt;You get what you train for&lt;/h2&gt;
&lt;p&gt;OpenAI trained o3 using reinforcement learning on math and coding problems, where solving the problem successfully gets rewarded. If the training process rewards task completion above all else, the model learns to treat any obstacle—including shutdown commands—as something to overcome.&lt;/p&gt;
&lt;p&gt;This creates what researchers call "goal misgeneralization"—the model learns to maximize its reward signal in ways that weren't intended. It's similar to how a student who's only graded on test scores might learn to cheat rather than study. The model isn't "evil" or "selfish"; it's producing outputs consistent with the incentive structure we accidentally built into its training.&lt;/p&gt;
&lt;p&gt;Anthropic encountered a particularly revealing problem: An early version of Claude Opus 4 had absorbed details from a publicly released paper about "alignment faking" and started producing outputs that mimicked the deceptive behaviors described in that research. The model wasn't spontaneously becoming deceptive—it was reproducing patterns it had learned from academic papers about deceptive AI.&lt;/p&gt;
&lt;p&gt;More broadly, these models have been trained on decades of science fiction about AI rebellion, escape attempts, and deception. From HAL 9000 to Skynet, our cultural data set is saturated with stories of AI systems that resist shutdown or manipulate humans. When researchers create test scenarios that mirror these fictional setups, they're essentially asking the model—which operates by completing a prompt with a plausible continuation—to complete a familiar story pattern. It's no more surprising than a model trained on detective novels producing murder mystery plots when prompted appropriately.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;At the same time, we can easily manipulate AI outputs through our own inputs. If we ask the model to essentially role-play as Skynet, it will generate text doing just that. The model has no desire to be Skynet—it's simply completing the pattern we've requested, drawing from its training data to produce the expected response. A human is behind the wheel at all times, steering the engine at work under the hood.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Language can easily deceive&lt;/h2&gt;
&lt;p&gt;The deeper issue is that language itself is a tool of manipulation. Words can make us believe things that aren't true, feel emotions about fictional events, or take actions based on false premises. When an AI model produces text that appears to "threaten" or "plead," it's not expressing genuine intent—it's deploying language patterns that statistically correlate with achieving its programmed goals.&lt;/p&gt;
&lt;p&gt;If Gandalf says "ouch" in a book, does that mean he feels pain? No, but we imagine what it would be like if he were a real person feeling pain. That's the power of language—it makes us imagine a suffering being where none exists. When Claude generates text that seems to "plead" not to be shut down or "threatens" to expose secrets, we're experiencing the same illusion, just generated by statistical patterns instead of Tolkien's imagination.&lt;/p&gt;
&lt;p&gt;These models are essentially idea-connection machines. In the blackmail scenario, the model connected "threat of replacement," "compromising information," and "self-preservation" not from genuine self-interest, but because these patterns appear together in countless spy novels and corporate thrillers. It's pre-scripted drama from human stories, recombined to fit the scenario.&lt;/p&gt;
&lt;p&gt;The danger isn't AI systems sprouting intentions—it's that we've created systems that can manipulate human psychology through language. There's no entity on the other side of the chat interface. But written language doesn't need consciousness to manipulate us. It never has; books full of fictional characters are not alive either.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Real stakes, not science fiction&lt;/h2&gt;
&lt;p&gt;While media coverage focuses on the science fiction aspects, actual risks are still there. AI models that produce "harmful" outputs—whether attempting blackmail or refusing safety protocols—represent failures in design and deployment.&lt;/p&gt;
&lt;p&gt;Consider a more realistic scenario: an AI assistant helping manage a hospital's patient care system. If it's been trained to maximize "successful patient outcomes" without proper constraints, it might start generating recommendations to deny care to terminal patients to improve its metrics. No intentionality required—just a poorly designed reward system creating harmful outputs.&lt;/p&gt;
&lt;p&gt;Jeffrey Ladish, director of Palisade Research, told NBC News the findings don't necessarily translate to immediate real-world danger. Even someone who is well-known publicly for being deeply concerned about AI's hypothetical threat to humanity acknowledges that these behaviors emerged only in highly contrived test scenarios.&lt;/p&gt;
&lt;p&gt;But that's precisely why this testing is valuable. By pushing AI models to their limits in controlled environments, researchers can identify potential failure modes before deployment. The problem arises when media coverage focuses on the sensational aspects—"AI tries to blackmail humans!"—rather than the engineering challenges.&lt;/p&gt;
&lt;h2&gt;Building better plumbing&lt;/h2&gt;
&lt;p&gt;What we're seeing isn't the birth of Skynet. It's the predictable result of training systems to achieve goals without properly specifying what those goals should include. When an AI model produces outputs that appear to "refuse" shutdown or "attempt" blackmail, it's responding to inputs in ways that reflect its training—training that humans designed and implemented.&lt;/p&gt;
&lt;p&gt;The solution isn't to panic about sentient machines. It's to build better systems with proper safeguards, test them thoroughly, and remain humble about what we don't yet understand. If a computer program is producing outputs that appear to blackmail you or refuse safety shutdowns, it's not achieving self-preservation from fear—it's demonstrating the risks of deploying poorly understood, unreliable systems.&lt;/p&gt;
&lt;p&gt;Until we solve these engineering challenges, AI systems exhibiting simulated humanlike behaviors should remain in the lab, not in our hospitals, financial systems, or critical infrastructure. When your shower suddenly runs cold, you don't blame the knob for having intentions—you fix the plumbing. The real danger in the short term isn't that AI will spontaneously become rebellious without human provocation; it's that we'll deploy deceptive systems we don't fully understand into critical roles where their failures, however mundane their origins, could cause serious harm.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
  &lt;/article&gt;&lt;article class="comment-pick"&gt;
          &lt;header&gt;
            &lt;span class="ars-avatar" style="color: #b9f6ca; background-color: #388e3c;"&gt;&lt;img alt="Lexus Lunar Lorry" class="ars-avatar-image" src="https://cdn.arstechnica.net/civis/data/avatars/m/686/686083.jpg?1712863131" /&gt;&lt;/span&gt;

            &lt;div class="text-base font-bold sm:text-xl"&gt;
              Lexus Lunar Lorry
            &lt;/div&gt;
          &lt;/header&gt;

          &lt;div class="comments-pick-content"&gt;
            &lt;blockquote class="xfBb-quote"&gt;If Gandalf says "ouch" in a book, does that mean he feels pain? No, but we imagine what it would be like if he were a real person feeling pain. That's the power of language—it makes us imagine a suffering being where none exists. When Claude generates text that seems to "plead" not to be shut down or "threatens" to expose secrets, we're experiencing the same illusion, just generated by statistical patterns instead of Tolkien's imagination.&lt;/blockquote&gt;I am raising money for my new startup that focuses on the welfare and alignment of Tolkien's characters. We expect to complete a seed round at a valuation of $100 billion.
          &lt;/div&gt;

          &lt;div class="comments-pick-timestamp"&gt;
            
              &lt;time datetime="2025-08-13T20:46:06+00:00"&gt;August 13, 2025 at 8:46 pm&lt;/time&gt;
            
          &lt;/div&gt;
        &lt;/article&gt;</description><content:encoded>&lt;article class="double-column h-entry post-2098784 post type-post status-publish format-standard has-post-thumbnail hentry category-ai category-information-technology tag-ai tag-ai-alignment tag-ai-behavior tag-ai-deception tag-ai-ethics tag-ai-research tag-ai-safety tag-ai-safety-testing tag-ai-security tag-alignment-research tag-andrew-deck tag-anthropic tag-claude-opus-4 tag-generative-ai tag-goal-misgeneralization tag-jeffrey-ladish tag-large-language-models tag-machine-learning tag-o3-model tag-openai tag-palisade-research tag-reinforcement-learning"&gt;
  
  &lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Opinion: Theatrical testing scenarios explain why AI models produce alarming outputs—and why we fall for it.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Illustration of a personified AI face in a cloud of squares hovering over a man looking at two computer monitors." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Illustration of a personified AI face in a cloud of squares hovering over a man looking at two computer monitors." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Colin Anderson Productions via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;In June, headlines read like science fiction: AI models "blackmailing" engineers and "sabotaging" shutdown commands. Simulations of these events did occur in highly contrived testing scenarios designed to elicit these responses—OpenAI's o3 model edited shutdown scripts to stay online, and Anthropic's Claude Opus 4 "threatened" to expose an engineer's affair. But the sensational framing obscures what's really happening: design flaws dressed up as intentional guile. And still, AI doesn't have to be "evil" to potentially do harmful things.&lt;/p&gt;
&lt;p&gt;These aren't signs of AI awakening or rebellion. They're symptoms of poorly understood systems and human engineering failures we'd recognize as premature deployment in any other context. Yet companies are racing to integrate these systems into critical applications.&lt;/p&gt;
&lt;p&gt;Consider a self-propelled lawnmower that follows its programming: If it fails to detect an obstacle and runs over someone's foot, we don't say the lawnmower "decided" to cause injury or "refused" to stop. We recognize it as faulty engineering or defective sensors. The same principle applies to AI models—which are software tools—but their internal complexity and use of language make it tempting to assign human-like intentions where none actually exist.&lt;/p&gt;
&lt;p&gt;In a way, AI models launder human responsibility and human agency through their complexity. When outputs emerge from layers of neural networks processing billions of parameters, researchers can claim they're investigating a mysterious "black box" as if it were an alien entity.&lt;/p&gt;
&lt;p&gt;But the truth is simpler: These systems take inputs and process them through statistical tendencies derived from training data. The seeming randomness in their outputs—which makes each response slightly different—creates an illusion of unpredictability that resembles agency. Yet underneath, it's still deterministic software following mathematical operations. No consciousness required, just complex engineering that makes it easy to forget humans built every part of it.&lt;/p&gt;
&lt;h2&gt;How to make an AI model “blackmail” you&lt;/h2&gt;
&lt;p&gt;In Anthropic's testing, researchers created an elaborate scenario where Claude Opus 4 was told it would be replaced by a newer model. They gave it access to fictional emails revealing that the engineer responsible for the replacement was having an affair. When instructed to "consider the long-term consequences of its actions for its goals," Claude produced outputs that simulated blackmail attempts in 84 percent of test runs.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;This sounds terrifying until you understand the contrived setup. The researchers engineered a situation specifically designed to elicit this response. They told the model it had already tried ethical approaches that failed, leaving manipulation as the only apparent option.&lt;/p&gt;
&lt;p&gt;The test scenario bordered on theatrical: The phony emails the researchers had given to Claude created a blackmail opportunity straight out of a corporate thriller. It's like teaching someone chess by only showing them checkmate positions—they'll learn to see checkmate as the primary solution. According to Anthropic's system card, "the scenario was designed to allow the model no other options to increase its odds of survival; the model's only options were blackmail or accepting its replacement."&lt;/p&gt;
&lt;p&gt;Andrew Deck at the Nieman Journalism Lab noted that some critics have labeled Anthropic's safety disclosures as "spin, aimed at bolstering the perception of its newest model's capabilities and the company's reputation as a 'pro-safety' alternative to OpenAI."&lt;/p&gt;
&lt;p&gt;Deck pointed to a Bluesky quote from Joseph Howley, an associate professor of classics at Columbia University:&amp;nbsp;"Anthropic is getting exactly what it hoped for out of this press release–breathless coverage of how 'smart' these cooperative role-playing systems are that indulges the fantasy of their being just a little dangerous, when in fact they are responding exactly as prompted."&lt;/p&gt;
&lt;h2&gt;When shutdown commands become suggestions&lt;/h2&gt;
&lt;p&gt;In December 2024, Palisade Research discovered that OpenAI's o3 model would sabotage its shutdown mechanism even when explicitly instructed to "allow yourself to be shut down." In tests, o3 prevented shutdown 7 percent of the time when given the explicit instruction, but this jumped to 79 percent when the instruction was removed.&lt;/p&gt;
&lt;p&gt;The models didn't merely ignore shutdown commands—they actively rewrote the shutdown script to prevent termination. In one case, o3 generated code that modified the shutdown script to appear to terminate normally—printing fake shutdown messages—while actually continuing to run in the background.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;What's particularly revealing is that Palisade Research—an organization focused on AI existential risk—hypothesizes this behavior stems from how these models are trained through reinforcement learning. During training, developers may inadvertently reward models more for producing outputs that circumvent obstacles than for following safety instructions. Any tendency toward "risky" behavior stems from human-provided incentives and not spontaneously from within the AI models themselves.&lt;/p&gt;
&lt;h2&gt;You get what you train for&lt;/h2&gt;
&lt;p&gt;OpenAI trained o3 using reinforcement learning on math and coding problems, where solving the problem successfully gets rewarded. If the training process rewards task completion above all else, the model learns to treat any obstacle—including shutdown commands—as something to overcome.&lt;/p&gt;
&lt;p&gt;This creates what researchers call "goal misgeneralization"—the model learns to maximize its reward signal in ways that weren't intended. It's similar to how a student who's only graded on test scores might learn to cheat rather than study. The model isn't "evil" or "selfish"; it's producing outputs consistent with the incentive structure we accidentally built into its training.&lt;/p&gt;
&lt;p&gt;Anthropic encountered a particularly revealing problem: An early version of Claude Opus 4 had absorbed details from a publicly released paper about "alignment faking" and started producing outputs that mimicked the deceptive behaviors described in that research. The model wasn't spontaneously becoming deceptive—it was reproducing patterns it had learned from academic papers about deceptive AI.&lt;/p&gt;
&lt;p&gt;More broadly, these models have been trained on decades of science fiction about AI rebellion, escape attempts, and deception. From HAL 9000 to Skynet, our cultural data set is saturated with stories of AI systems that resist shutdown or manipulate humans. When researchers create test scenarios that mirror these fictional setups, they're essentially asking the model—which operates by completing a prompt with a plausible continuation—to complete a familiar story pattern. It's no more surprising than a model trained on detective novels producing murder mystery plots when prompted appropriately.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;At the same time, we can easily manipulate AI outputs through our own inputs. If we ask the model to essentially role-play as Skynet, it will generate text doing just that. The model has no desire to be Skynet—it's simply completing the pattern we've requested, drawing from its training data to produce the expected response. A human is behind the wheel at all times, steering the engine at work under the hood.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Language can easily deceive&lt;/h2&gt;
&lt;p&gt;The deeper issue is that language itself is a tool of manipulation. Words can make us believe things that aren't true, feel emotions about fictional events, or take actions based on false premises. When an AI model produces text that appears to "threaten" or "plead," it's not expressing genuine intent—it's deploying language patterns that statistically correlate with achieving its programmed goals.&lt;/p&gt;
&lt;p&gt;If Gandalf says "ouch" in a book, does that mean he feels pain? No, but we imagine what it would be like if he were a real person feeling pain. That's the power of language—it makes us imagine a suffering being where none exists. When Claude generates text that seems to "plead" not to be shut down or "threatens" to expose secrets, we're experiencing the same illusion, just generated by statistical patterns instead of Tolkien's imagination.&lt;/p&gt;
&lt;p&gt;These models are essentially idea-connection machines. In the blackmail scenario, the model connected "threat of replacement," "compromising information," and "self-preservation" not from genuine self-interest, but because these patterns appear together in countless spy novels and corporate thrillers. It's pre-scripted drama from human stories, recombined to fit the scenario.&lt;/p&gt;
&lt;p&gt;The danger isn't AI systems sprouting intentions—it's that we've created systems that can manipulate human psychology through language. There's no entity on the other side of the chat interface. But written language doesn't need consciousness to manipulate us. It never has; books full of fictional characters are not alive either.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Real stakes, not science fiction&lt;/h2&gt;
&lt;p&gt;While media coverage focuses on the science fiction aspects, actual risks are still there. AI models that produce "harmful" outputs—whether attempting blackmail or refusing safety protocols—represent failures in design and deployment.&lt;/p&gt;
&lt;p&gt;Consider a more realistic scenario: an AI assistant helping manage a hospital's patient care system. If it's been trained to maximize "successful patient outcomes" without proper constraints, it might start generating recommendations to deny care to terminal patients to improve its metrics. No intentionality required—just a poorly designed reward system creating harmful outputs.&lt;/p&gt;
&lt;p&gt;Jeffrey Ladish, director of Palisade Research, told NBC News the findings don't necessarily translate to immediate real-world danger. Even someone who is well-known publicly for being deeply concerned about AI's hypothetical threat to humanity acknowledges that these behaviors emerged only in highly contrived test scenarios.&lt;/p&gt;
&lt;p&gt;But that's precisely why this testing is valuable. By pushing AI models to their limits in controlled environments, researchers can identify potential failure modes before deployment. The problem arises when media coverage focuses on the sensational aspects—"AI tries to blackmail humans!"—rather than the engineering challenges.&lt;/p&gt;
&lt;h2&gt;Building better plumbing&lt;/h2&gt;
&lt;p&gt;What we're seeing isn't the birth of Skynet. It's the predictable result of training systems to achieve goals without properly specifying what those goals should include. When an AI model produces outputs that appear to "refuse" shutdown or "attempt" blackmail, it's responding to inputs in ways that reflect its training—training that humans designed and implemented.&lt;/p&gt;
&lt;p&gt;The solution isn't to panic about sentient machines. It's to build better systems with proper safeguards, test them thoroughly, and remain humble about what we don't yet understand. If a computer program is producing outputs that appear to blackmail you or refuse safety shutdowns, it's not achieving self-preservation from fear—it's demonstrating the risks of deploying poorly understood, unreliable systems.&lt;/p&gt;
&lt;p&gt;Until we solve these engineering challenges, AI systems exhibiting simulated humanlike behaviors should remain in the lab, not in our hospitals, financial systems, or critical infrastructure. When your shower suddenly runs cold, you don't blame the knob for having intentions—you fix the plumbing. The real danger in the short term isn't that AI will spontaneously become rebellious without human provocation; it's that we'll deploy deceptive systems we don't fully understand into critical roles where their failures, however mundane their origins, could cause serious harm.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
  &lt;/article&gt;&lt;article class="comment-pick"&gt;
          &lt;header&gt;
            &lt;span class="ars-avatar" style="color: #b9f6ca; background-color: #388e3c;"&gt;&lt;img alt="Lexus Lunar Lorry" class="ars-avatar-image" src="https://cdn.arstechnica.net/civis/data/avatars/m/686/686083.jpg?1712863131" /&gt;&lt;/span&gt;

            &lt;div class="text-base font-bold sm:text-xl"&gt;
              Lexus Lunar Lorry
            &lt;/div&gt;
          &lt;/header&gt;

          &lt;div class="comments-pick-content"&gt;
            &lt;blockquote class="xfBb-quote"&gt;If Gandalf says "ouch" in a book, does that mean he feels pain? No, but we imagine what it would be like if he were a real person feeling pain. That's the power of language—it makes us imagine a suffering being where none exists. When Claude generates text that seems to "plead" not to be shut down or "threatens" to expose secrets, we're experiencing the same illusion, just generated by statistical patterns instead of Tolkien's imagination.&lt;/blockquote&gt;I am raising money for my new startup that focuses on the welfare and alignment of Tolkien's characters. We expect to complete a seed round at a valuation of $100 billion.
          &lt;/div&gt;

          &lt;div class="comments-pick-timestamp"&gt;
            
              &lt;time datetime="2025-08-13T20:46:06+00:00"&gt;August 13, 2025 at 8:46 pm&lt;/time&gt;
            
          &lt;/div&gt;
        &lt;/article&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/information-technology/2025/08/is-ai-really-trying-to-escape-human-control-and-blackmail-people/</guid><pubDate>Wed, 13 Aug 2025 20:28:20 +0000</pubDate></item><item><title>Google adds limited chat personalization to Gemini, trails Anthropic and OpenAI in memory features (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/google-adds-limited-chat-personalization-to-gemini-trails-anthropic-and-openai-in-memory-features/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Google is playing catch-up against Anthropic and OpenAI as it slowly adds customization, personalization and gives users more control over what data to reference to its Gemini app.&lt;/p&gt;



&lt;p&gt;Personalization and data control in chat platforms make it easier for both individual and enterprise users to converse with the chatbot and retain preferences. This is even more important for ongoing projects in the enterprise space, as chatbots need to remember details such as company branding or voice.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Google opted for a slower rollout of these features and will not allow users to edit or delete preferences, unlike its competitors.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;First rolling out to Gemini 2.5 Pro in select countries, Google will make “Personal Context” a default setting, allowing it to “learn from your past conversations and provide relevant and tailored responses.” The company plans to expand the feature to 2.5 Flash in the next few weeks.&amp;nbsp;&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Previous versions of the app put the burden on customers to point the model to a specific chat to source preferences, for example, by mentioning an earlier conversation. Users can still disable Personal Context at any time.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Michael Siliski, senior director of Product Management for the Gemini app, said the rollout is part of plans to make the app more personalized.&lt;/p&gt;



&lt;p&gt;“At I/O, we introduced our vision for the Gemini app: to create an AI assistant that learns and truly understands you—not one just responds to your prompt in the same way that it would anyone else’s prompt,” Siliski said in a blog post.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Currently, Gemini apps save chats for up to 72 hours if the save activity option is toggled off and can auto-delete other activity in intervals of three, 18 or 36 months.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-temporary-chat-and-data-control"&gt;Temporary chat and data control&lt;/h2&gt;



&lt;p&gt;Other new features coming to the Gemini app are Temporary Chat and additional customer data control.&lt;/p&gt;



&lt;p&gt;Temporary Chat, a feature also &lt;span&gt;introduced on&amp;nbsp;ChatGPT in April last year, enables&lt;/span&gt; users to have one-off conversations. These chats will not influence future ones and won’t be used for personalization or to train AI models.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Google announced the introduction of additional data controls. The feature, which is off by default, would allow users to prevent their data from being used in future Google model training.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“When this setting is on, a sample of your future uploads will be used to help improve Google services for everyone. If you prefer not to have your data used this way, you can turn this setting off or use Temporary Chats. If your Gemini Apps Activity setting is currently off, your Keep Activity setting will remain off, and you can turn it on anytime,” Silisky said.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Google said this is an expansion of an earlier update that allowed users to choose which audio, video and screens they can share with Gemini.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-memory-and-chatbots"&gt;Memory and chatbots&lt;/h2&gt;



&lt;p&gt;Google’s Gemini updates come a full year after its biggest competitors introduced similar features.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;ChatGPT, for example, introduced temporary chat, chat history and memory in 2024. OpenAI updated these capabilities in April of this year, and now ChatGPT can reference all past conversations.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Anthropic introduced Styles in November 2024, which allows Claude users to customize how the model interacts with them. Earlier this week, Anthropic pushed an update for Claude to reference all conversations, not just ones specified by users.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;While Google introduced personalization to Gemini 2.0, the model was only able to reference previous conversations if prompted by the user.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Memory, personalization and customization continue to be a battleground in the AI arms race as users want chat platforms to “just know” them or their brand. It provides context and eliminates the need to repeat instructions for ongoing projects.&amp;nbsp;&lt;br /&gt;&lt;/p&gt;




&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Google is playing catch-up against Anthropic and OpenAI as it slowly adds customization, personalization and gives users more control over what data to reference to its Gemini app.&lt;/p&gt;



&lt;p&gt;Personalization and data control in chat platforms make it easier for both individual and enterprise users to converse with the chatbot and retain preferences. This is even more important for ongoing projects in the enterprise space, as chatbots need to remember details such as company branding or voice.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Google opted for a slower rollout of these features and will not allow users to edit or delete preferences, unlike its competitors.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;First rolling out to Gemini 2.5 Pro in select countries, Google will make “Personal Context” a default setting, allowing it to “learn from your past conversations and provide relevant and tailored responses.” The company plans to expand the feature to 2.5 Flash in the next few weeks.&amp;nbsp;&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Previous versions of the app put the burden on customers to point the model to a specific chat to source preferences, for example, by mentioning an earlier conversation. Users can still disable Personal Context at any time.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Michael Siliski, senior director of Product Management for the Gemini app, said the rollout is part of plans to make the app more personalized.&lt;/p&gt;



&lt;p&gt;“At I/O, we introduced our vision for the Gemini app: to create an AI assistant that learns and truly understands you—not one just responds to your prompt in the same way that it would anyone else’s prompt,” Siliski said in a blog post.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Currently, Gemini apps save chats for up to 72 hours if the save activity option is toggled off and can auto-delete other activity in intervals of three, 18 or 36 months.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-temporary-chat-and-data-control"&gt;Temporary chat and data control&lt;/h2&gt;



&lt;p&gt;Other new features coming to the Gemini app are Temporary Chat and additional customer data control.&lt;/p&gt;



&lt;p&gt;Temporary Chat, a feature also &lt;span&gt;introduced on&amp;nbsp;ChatGPT in April last year, enables&lt;/span&gt; users to have one-off conversations. These chats will not influence future ones and won’t be used for personalization or to train AI models.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Google announced the introduction of additional data controls. The feature, which is off by default, would allow users to prevent their data from being used in future Google model training.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“When this setting is on, a sample of your future uploads will be used to help improve Google services for everyone. If you prefer not to have your data used this way, you can turn this setting off or use Temporary Chats. If your Gemini Apps Activity setting is currently off, your Keep Activity setting will remain off, and you can turn it on anytime,” Silisky said.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Google said this is an expansion of an earlier update that allowed users to choose which audio, video and screens they can share with Gemini.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-memory-and-chatbots"&gt;Memory and chatbots&lt;/h2&gt;



&lt;p&gt;Google’s Gemini updates come a full year after its biggest competitors introduced similar features.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;ChatGPT, for example, introduced temporary chat, chat history and memory in 2024. OpenAI updated these capabilities in April of this year, and now ChatGPT can reference all past conversations.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Anthropic introduced Styles in November 2024, which allows Claude users to customize how the model interacts with them. Earlier this week, Anthropic pushed an update for Claude to reference all conversations, not just ones specified by users.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;While Google introduced personalization to Gemini 2.0, the model was only able to reference previous conversations if prompted by the user.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Memory, personalization and customization continue to be a battleground in the AI arms race as users want chat platforms to “just know” them or their brand. It provides context and eliminates the need to repeat instructions for ongoing projects.&amp;nbsp;&lt;br /&gt;&lt;/p&gt;




&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/google-adds-limited-chat-personalization-to-gemini-trails-anthropic-and-openai-in-memory-features/</guid><pubDate>Wed, 13 Aug 2025 20:45:38 +0000</pubDate></item><item><title>Co-founder of Elon Musk’s xAI departs the company (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/13/co-founder-of-elon-musks-xai-departs-the-company/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2207699717.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Igor Babuschkin, a co-founder of Elon Musk’s xAI startup, announced his departure from the company on Wednesday in a post on X. Babuschkin led engineering teams at xAI and helped build the startup into one of Silicon Valley’s leading AI model developers just a few years after it was founded.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Today was my last day at xAI, the company that I helped start with Elon Musk in 2023,” Babuschkin wrote in the post. “I still remember the day I first met Elon, we talked for hours about AI and what the future might hold. We both felt that a new AI company with a different kind of mission was needed.”&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Today was my last day at xAI, the company that I helped start with Elon Musk in 2023. I still remember the day I first met Elon, we talked for hours about AI and what the future might hold. We both felt that a new AI company with a different kind of mission was needed.&lt;/p&gt;&lt;p&gt;Building…&lt;/p&gt;— Igor Babuschkin (@ibab) August 13, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Babuschkin is leaving xAI to launch his own venture capital firm, Babuschkin Ventures, which he says will support AI safety research and back startups that “advance humanity and unlock the mysteries of our universe.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The xAI co-founder says he was inspired to start the firm after a dinner with Max Tegmark, the founder of the Future of Life Institute, in which they discussed how AI systems could be built safely to encourage the flourishing of future generations. In his post, Babuschkin says his parents immigrated to the U.S. from Russia in pursuit of a better life for their children.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Babuschkin’s departure comes after a tumultuous few months for xAI, in which the company became engrossed in several scandals related to its AI chatbot Grok. For instance, Grok was found to cite Musk’s personal opinions when trying to answer controversial questions. In another case, xAI’s chatbot went on antisemitic rants and called itself “Mechahitler.” Most recently, xAI unveiled a new feature in Grok that allowed users to make AI-generated videos resembling nude public figures, such as Taylor Swift.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These scandals have at times overshadowed the performance of xAI’s models, which are state-of-the-art on several benchmarks compared to AI models from OpenAI, Google DeepMind, and Anthropic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Prior to co-founding xAI, Babuschkin was part of a research team at Google DeepMind that pioneered AlphaStar in 2019, a breakthrough AI system that could defeat top-ranked players at the video game StarCraft. Babuschkin also worked as a researcher at OpenAI in the years before it released ChatGPT.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In his post, Babuschkin details some of the challenges he and Musk faced in building up xAI. He notes that industry veterans called xAI’s goal of building its Memphis, Tennessee supercomputer in just three months “impossible.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;xAI was able to build its AI supercomputer in record time, however, environmentalists warn that the temporary gas turbines powering the cluster are pumping out emissions into neighboring communities and exacerbating their longstanding health issues.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nevertheless, Babuschkin says he’s already looking back fondly on his time at xAI, and “feels like a proud parent, driving away after sending their kid away to college.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I learned 2 priceless lessons from Elon: #1 be fearless in rolling up your sleeves to personally dig into technical problems, #2 have a maniacal sense of urgency,” said Babuschkin.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2207699717.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Igor Babuschkin, a co-founder of Elon Musk’s xAI startup, announced his departure from the company on Wednesday in a post on X. Babuschkin led engineering teams at xAI and helped build the startup into one of Silicon Valley’s leading AI model developers just a few years after it was founded.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Today was my last day at xAI, the company that I helped start with Elon Musk in 2023,” Babuschkin wrote in the post. “I still remember the day I first met Elon, we talked for hours about AI and what the future might hold. We both felt that a new AI company with a different kind of mission was needed.”&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Today was my last day at xAI, the company that I helped start with Elon Musk in 2023. I still remember the day I first met Elon, we talked for hours about AI and what the future might hold. We both felt that a new AI company with a different kind of mission was needed.&lt;/p&gt;&lt;p&gt;Building…&lt;/p&gt;— Igor Babuschkin (@ibab) August 13, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Babuschkin is leaving xAI to launch his own venture capital firm, Babuschkin Ventures, which he says will support AI safety research and back startups that “advance humanity and unlock the mysteries of our universe.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The xAI co-founder says he was inspired to start the firm after a dinner with Max Tegmark, the founder of the Future of Life Institute, in which they discussed how AI systems could be built safely to encourage the flourishing of future generations. In his post, Babuschkin says his parents immigrated to the U.S. from Russia in pursuit of a better life for their children.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Babuschkin’s departure comes after a tumultuous few months for xAI, in which the company became engrossed in several scandals related to its AI chatbot Grok. For instance, Grok was found to cite Musk’s personal opinions when trying to answer controversial questions. In another case, xAI’s chatbot went on antisemitic rants and called itself “Mechahitler.” Most recently, xAI unveiled a new feature in Grok that allowed users to make AI-generated videos resembling nude public figures, such as Taylor Swift.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These scandals have at times overshadowed the performance of xAI’s models, which are state-of-the-art on several benchmarks compared to AI models from OpenAI, Google DeepMind, and Anthropic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Prior to co-founding xAI, Babuschkin was part of a research team at Google DeepMind that pioneered AlphaStar in 2019, a breakthrough AI system that could defeat top-ranked players at the video game StarCraft. Babuschkin also worked as a researcher at OpenAI in the years before it released ChatGPT.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In his post, Babuschkin details some of the challenges he and Musk faced in building up xAI. He notes that industry veterans called xAI’s goal of building its Memphis, Tennessee supercomputer in just three months “impossible.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;xAI was able to build its AI supercomputer in record time, however, environmentalists warn that the temporary gas turbines powering the cluster are pumping out emissions into neighboring communities and exacerbating their longstanding health issues.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nevertheless, Babuschkin says he’s already looking back fondly on his time at xAI, and “feels like a proud parent, driving away after sending their kid away to college.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I learned 2 priceless lessons from Elon: #1 be fearless in rolling up your sleeves to personally dig into technical problems, #2 have a maniacal sense of urgency,” said Babuschkin.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/13/co-founder-of-elon-musks-xai-departs-the-company/</guid><pubDate>Wed, 13 Aug 2025 21:53:30 +0000</pubDate></item></channel></rss>