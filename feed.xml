<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 21 Jul 2025 12:50:33 +0000</lastBuildDate><item><title>[NEW] Tech giants split on EU AI code as compliance deadline looms (AI News)</title><link>https://www.artificialintelligence-news.com/news/eu-ai-code-tech-giants-microsoft-meta-split-compliance/</link><description>&lt;p&gt;The implementation of the EU’s AI General-Purpose Code of Practice has exposed deep divisions among major technology companies. Microsoft has signalled its intention to sign the European Union’s voluntary AI compliance framework while Meta flatly refuses participation, calling the guidelines regulatory overreach that will stifle innovation.&lt;/p&gt;&lt;p&gt;Microsoft President Brad Smith told &lt;em&gt;Reuters&lt;/em&gt; on Friday, “I think it’s likely we will sign. We need to read the documents.”. Smith emphasised his company’s collaborative approach, stating, “Our goal is to find a way to be supportive, and at the same time, one of the things we welcome is the direct engagement by the AI Office with industry.”&lt;/p&gt;&lt;p&gt;In contrast, Meta’s Chief Global Affairs Officer, Joel Kaplan, announced on LinkedIn that “Meta won’t be signing it. The code introduces several legal uncertainties for model developers, as well as measures which go far beyond the scope of the AI Act.”&lt;/p&gt;&lt;p&gt;Kaplan argued that “Europe is heading down the wrong path on AI” and warned the EU AI code would “throttle the development and deployment of frontier AI models in Europe, and stunt European companies looking to build businesses on top of them.”&lt;/p&gt;&lt;h3&gt;Early adopters vs. holdouts&lt;/h3&gt;&lt;p&gt;The technology sector’s fractured response highlights different strategies for managing European regulatory compliance. OpenAI and Mistral have signed the Code, positioning themselves as early adopters of the voluntary framework.&lt;/p&gt;&lt;p&gt;OpenAI announced its commitment, stating, “Signing the Code reflects our commitment to providing capable, accessible and secure AI models for Europeans to fully participate in the economic and societal benefits of the Intelligence Age.”&lt;/p&gt;&lt;p&gt;OpenAI joins the EU code of practice for general-purpose AI models, the second signature of a leading AI company after Mistral, according to industry observers tracking the voluntary commitments.&lt;/p&gt;&lt;p&gt;More than 40 of Europe’s largest businesses signed a letter earlier this month, asking the European Commission to halt the implementation of the AI Act, including companies like ASML Holding and Airbus that called for a two-year delay.&lt;/p&gt;&lt;h3&gt;Code requirements and timeline&lt;/h3&gt;&lt;p&gt;The code of practice, was published on July 10 by the European Commission, and aims to provide legal certainty for companies developing general-purpose AI models ahead of mandatory enforcement beginning August 2, 2025.&lt;/p&gt;&lt;p&gt;The voluntary tool was developed by 13 independent experts, with input from over 1,000 stakeholders, including model providers, small and medium-sized enterprises, academics, AI safety experts, rights-holders, and civil society organisations.&lt;/p&gt;&lt;p&gt;The EU AI code establishes requirements in three areas. Transparency obligations require providers to maintain technical model and dataset documentation, while copyright compliance mandates clear internal policies outlining how training data is obtained and used under EU copyright rules.&lt;/p&gt;&lt;p&gt;For the most advanced models, safety and security obligations apply under the category, “GPAI with Systemic Risk” (GPAISR), which covers the most advanced models, like OpenAI’s o3, Anthropic’s Claude 4 Opus, and Google’s Gemini 2.5 Pro.&lt;/p&gt;&lt;p&gt;Signatories will have to publish summaries of the content used to train their general-purpose AI models and put in place a policy to comply with EU copyright law. The framework requires companies to document training data sources, implement robust risk assessments, and establish governance frameworks for managing potential AI system threats.&lt;/p&gt;&lt;h3&gt;Enforcement and penalties&lt;/h3&gt;&lt;p&gt;The penalties for non-compliance are substantial, including up to €35 million or 7% of global annual turnover (the greater of either). In particular, for providers of GPAI models, the EC may impose a fine of up to €15 million or 3% of the worldwide annual turnover.&lt;/p&gt;&lt;p&gt;The Commission has indicated that if providers adhere to an approved Code of Practice, the AI Office and national regulators will treat that as a simplified compliance path, focusing enforcement on checking that the Code’s commitments are met, rather than conducting audits of every AI system. This creates incentives for early adoption among companies seeking regulatory predictability.&lt;/p&gt;&lt;p&gt;The EU AI code represents part of the broaderAI Act framework. Under the AI Act, obligations for GPAI models, detailed in Articles 50 – 55, are enforceable twelve months after the Act enters into force (2 August 2025). Providers of GPAI models that have been placed on the market before this date need to be compliant with the AI Act by 2 August 2027.&lt;/p&gt;&lt;h3&gt;Industry impact and global implications&lt;/h3&gt;&lt;p&gt;The different responses suggest technology companies are adopting fundamentally different strategies for managing regulatory relationships in global markets. Microsoft’s cooperative stance contrasts sharply with Meta’s confrontational approach, potentially setting precedents for how major AI developers engage with international regulation.&lt;/p&gt;&lt;p&gt;Despite mounting opposition, the European Commission has refused to delay. The EU’s Internal Market Commissioner Thierry Breton has insisted that the framework will proceed as scheduled, saying the AI Act is essential for consumer safety and trust in emerging technologies.&lt;/p&gt;&lt;p&gt;The EU AI code’s current voluntary nature during initial phases provides companies with opportunities to influence regulatory development through participation. However, mandatory enforcement beginning in August 2025 ensures eventual compliance regardless of voluntary code adoption.&lt;/p&gt;&lt;p&gt;For companies operating in multiple jurisdictions, the EU framework may influence global AI governance standards. The framework aligns with broader global AI governance developments, including the G7 Hiroshima AI Process and various national AI strategies, potentially establishing European approaches as international benchmarks.&lt;/p&gt;&lt;h3&gt;Looking ahead&lt;/h3&gt;&lt;p&gt;In the immediate term, the Code’s content will be reviewed by EU authorities: the European Commission and Member States are assessing the Code’s adequacy and are expected to formally endorse it, with a final decision planned by 2 August 2025.&lt;/p&gt;&lt;p&gt;The regulatory framework creates significant implications for AI development globally, as companies must balance innovation objectives with compliance obligations in multiple jurisdictions. The different company responses to the voluntary code foreshadow potential compliance challenges as mandatory requirements take effect.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Navigating the EU AI Act: Implications for UK businesses&lt;/strong&gt;&lt;/p&gt;&lt;img alt="alt" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;The implementation of the EU’s AI General-Purpose Code of Practice has exposed deep divisions among major technology companies. Microsoft has signalled its intention to sign the European Union’s voluntary AI compliance framework while Meta flatly refuses participation, calling the guidelines regulatory overreach that will stifle innovation.&lt;/p&gt;&lt;p&gt;Microsoft President Brad Smith told &lt;em&gt;Reuters&lt;/em&gt; on Friday, “I think it’s likely we will sign. We need to read the documents.”. Smith emphasised his company’s collaborative approach, stating, “Our goal is to find a way to be supportive, and at the same time, one of the things we welcome is the direct engagement by the AI Office with industry.”&lt;/p&gt;&lt;p&gt;In contrast, Meta’s Chief Global Affairs Officer, Joel Kaplan, announced on LinkedIn that “Meta won’t be signing it. The code introduces several legal uncertainties for model developers, as well as measures which go far beyond the scope of the AI Act.”&lt;/p&gt;&lt;p&gt;Kaplan argued that “Europe is heading down the wrong path on AI” and warned the EU AI code would “throttle the development and deployment of frontier AI models in Europe, and stunt European companies looking to build businesses on top of them.”&lt;/p&gt;&lt;h3&gt;Early adopters vs. holdouts&lt;/h3&gt;&lt;p&gt;The technology sector’s fractured response highlights different strategies for managing European regulatory compliance. OpenAI and Mistral have signed the Code, positioning themselves as early adopters of the voluntary framework.&lt;/p&gt;&lt;p&gt;OpenAI announced its commitment, stating, “Signing the Code reflects our commitment to providing capable, accessible and secure AI models for Europeans to fully participate in the economic and societal benefits of the Intelligence Age.”&lt;/p&gt;&lt;p&gt;OpenAI joins the EU code of practice for general-purpose AI models, the second signature of a leading AI company after Mistral, according to industry observers tracking the voluntary commitments.&lt;/p&gt;&lt;p&gt;More than 40 of Europe’s largest businesses signed a letter earlier this month, asking the European Commission to halt the implementation of the AI Act, including companies like ASML Holding and Airbus that called for a two-year delay.&lt;/p&gt;&lt;h3&gt;Code requirements and timeline&lt;/h3&gt;&lt;p&gt;The code of practice, was published on July 10 by the European Commission, and aims to provide legal certainty for companies developing general-purpose AI models ahead of mandatory enforcement beginning August 2, 2025.&lt;/p&gt;&lt;p&gt;The voluntary tool was developed by 13 independent experts, with input from over 1,000 stakeholders, including model providers, small and medium-sized enterprises, academics, AI safety experts, rights-holders, and civil society organisations.&lt;/p&gt;&lt;p&gt;The EU AI code establishes requirements in three areas. Transparency obligations require providers to maintain technical model and dataset documentation, while copyright compliance mandates clear internal policies outlining how training data is obtained and used under EU copyright rules.&lt;/p&gt;&lt;p&gt;For the most advanced models, safety and security obligations apply under the category, “GPAI with Systemic Risk” (GPAISR), which covers the most advanced models, like OpenAI’s o3, Anthropic’s Claude 4 Opus, and Google’s Gemini 2.5 Pro.&lt;/p&gt;&lt;p&gt;Signatories will have to publish summaries of the content used to train their general-purpose AI models and put in place a policy to comply with EU copyright law. The framework requires companies to document training data sources, implement robust risk assessments, and establish governance frameworks for managing potential AI system threats.&lt;/p&gt;&lt;h3&gt;Enforcement and penalties&lt;/h3&gt;&lt;p&gt;The penalties for non-compliance are substantial, including up to €35 million or 7% of global annual turnover (the greater of either). In particular, for providers of GPAI models, the EC may impose a fine of up to €15 million or 3% of the worldwide annual turnover.&lt;/p&gt;&lt;p&gt;The Commission has indicated that if providers adhere to an approved Code of Practice, the AI Office and national regulators will treat that as a simplified compliance path, focusing enforcement on checking that the Code’s commitments are met, rather than conducting audits of every AI system. This creates incentives for early adoption among companies seeking regulatory predictability.&lt;/p&gt;&lt;p&gt;The EU AI code represents part of the broaderAI Act framework. Under the AI Act, obligations for GPAI models, detailed in Articles 50 – 55, are enforceable twelve months after the Act enters into force (2 August 2025). Providers of GPAI models that have been placed on the market before this date need to be compliant with the AI Act by 2 August 2027.&lt;/p&gt;&lt;h3&gt;Industry impact and global implications&lt;/h3&gt;&lt;p&gt;The different responses suggest technology companies are adopting fundamentally different strategies for managing regulatory relationships in global markets. Microsoft’s cooperative stance contrasts sharply with Meta’s confrontational approach, potentially setting precedents for how major AI developers engage with international regulation.&lt;/p&gt;&lt;p&gt;Despite mounting opposition, the European Commission has refused to delay. The EU’s Internal Market Commissioner Thierry Breton has insisted that the framework will proceed as scheduled, saying the AI Act is essential for consumer safety and trust in emerging technologies.&lt;/p&gt;&lt;p&gt;The EU AI code’s current voluntary nature during initial phases provides companies with opportunities to influence regulatory development through participation. However, mandatory enforcement beginning in August 2025 ensures eventual compliance regardless of voluntary code adoption.&lt;/p&gt;&lt;p&gt;For companies operating in multiple jurisdictions, the EU framework may influence global AI governance standards. The framework aligns with broader global AI governance developments, including the G7 Hiroshima AI Process and various national AI strategies, potentially establishing European approaches as international benchmarks.&lt;/p&gt;&lt;h3&gt;Looking ahead&lt;/h3&gt;&lt;p&gt;In the immediate term, the Code’s content will be reviewed by EU authorities: the European Commission and Member States are assessing the Code’s adequacy and are expected to formally endorse it, with a final decision planned by 2 August 2025.&lt;/p&gt;&lt;p&gt;The regulatory framework creates significant implications for AI development globally, as companies must balance innovation objectives with compliance obligations in multiple jurisdictions. The different company responses to the voluntary code foreshadow potential compliance challenges as mandatory requirements take effect.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Navigating the EU AI Act: Implications for UK businesses&lt;/strong&gt;&lt;/p&gt;&lt;img alt="alt" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/eu-ai-code-tech-giants-microsoft-meta-split-compliance/</guid><pubDate>Mon, 21 Jul 2025 07:44:52 +0000</pubDate></item><item><title>[NEW] Why Apple is playing it slow with AI (AI News)</title><link>https://www.artificialintelligence-news.com/news/why-apple-is-playing-it-slow-with-ai/</link><description>&lt;p&gt;Apple is taking its time with AI. While most tech companies are racing to push out AI features as fast as they can, Apple is doing the opposite. Its big announcement – Apple Intelligence – won’t arrive for most users until 2026. That’s a long delay in a market where speed seems to matter more than quality. But maybe that’s the whole point.&lt;/p&gt;&lt;p&gt;At this year’s WWDC, Apple showed off new AI features tied to Siri, writing tools, and app suggestions. It called the bundle “Apple Intelligence,” but those tools won’t be widely available any time soon. For now, they’re limited to beta users on select devices in the US. The rest of the world will have to wait. According to &lt;em&gt;Macworld&lt;/em&gt;, even early access to Apple Intelligence is expected to be restricted, and many users may not see the features until iOS 18.4 (at the earliest) in 2025. A wider release could slip into 2026.&lt;/p&gt;&lt;h3&gt;Not falling behind – just not rushing in&lt;/h3&gt;&lt;p&gt;To some, the delay looks like Apple falling behind. OpenAI has already rolled out GPT-4o, Google is squeezing Gemini into Android, and Microsoft has pushed Copilot into Office, Windows, and pretty much everything else. Compared to that, Apple seems slow.&lt;/p&gt;&lt;p&gt;Apple tends not to ship bad software. It delays when things aren’t working. The company has a long history of waiting until something is polished before pushing it out. That kind of caution can be frustrating, but it also avoids something worse: giving people tools that don’t work properly.&lt;/p&gt;&lt;h3&gt;Meanwhile, competitors ship bugs&lt;/h3&gt;&lt;p&gt;Plenty of companies don’t seem to care about quality. Microsoft’s Copilot, for example, often gives wrong answers, makes up citations, or produces junk text. ChatGPT has its own set of problems, from hallucinating facts to giving inconsistent results. Even tools like Claude or Gemini, which show promise in short bursts, tend to fall short on long-term tasks or anything that needs precision.&lt;/p&gt;&lt;p&gt;Ask developers what it’s like using AI to write production code, and you’ll often hear the same message: it works fine for code snippets or boilerplate, but it’s more work than help when it comes to complex projects. Fixing AI-written code often takes longer than writing it from scratch.&lt;/p&gt;&lt;h3&gt;Apple’s delay might be the smarter play&lt;/h3&gt;&lt;p&gt;An opinion piece from &lt;em&gt;TechRadar&lt;/em&gt; captured the consumer viewpoint. The author said they were glad Apple delayed Siri’s AI overhaul, arguing that the current generation of AI isn’t good enough. They said we often have the AI discussion backwards – we assume the tech is ready, and criticise companies for being too slow. But what if the tech just isn’t there yet? Apple’s delay might not be a flaw; it might be the only rational move.&lt;/p&gt;&lt;p&gt;Apple seems aware of this, making a lot of noise about being “excited” by AI, but it hasn’t forced it into every product, flooding iOS with half-baked tools. It hasn’t promised that Siri will be your new work assistant, for example. And while it may talk up the potential, it’s also been quiet about timelines.&lt;/p&gt;&lt;h3&gt;Playing the long game&lt;/h3&gt;&lt;p&gt;Some would call that playing it safe, but there’s another way to look at it. Maybe Apple doesn’t actually believe the current wave of AI is ready? Maybe it’s not convinced the technology will hold up under real pressure. So it’s watching the chaos from a distance.&lt;/p&gt;&lt;p&gt;And there’s plenty of chaos to watch. Companies are rolling out AI products that don’t work as advertised. Security issues, bad output, and inflated expectations are becoming common. Behind the scenes, many AI companies are burning through cash trying to make their models useful. If the bubble bursts, Apple gets to say it never went all-in.&lt;/p&gt;&lt;h3&gt;Wait, watch, then act&lt;/h3&gt;&lt;p&gt;That might not be a bug in the company’s strategy or problems in production: It might &lt;em&gt;be the company’s strategy&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;If users grow tired of AI that doesn’t deliver, Apple comes out looking smart for not jumping in too fast. If the tech improves and becomes reliable, Apple can still step in with a product that feels stable and is reliable.&lt;/p&gt;&lt;p&gt;This kind of delay has worked for Apple before, not launching a smartwatch until years after others tried. In the tablet market too, it wasn’t the market leader, but ended up setting the standard once involved.&lt;/p&gt;&lt;p&gt;With AI, Apple might be trying the same thing. Let everyone else test the limits, hit the walls, and suffer the backlash. Meanwhile, Apple learns from their mistakes, avoiding rushing out tools that make headlines for all the wrong reasons.&lt;/p&gt;&lt;h3&gt;No rush required&lt;/h3&gt;&lt;p&gt;It also helps that Apple doesn’t need to hype itself to stay relevant. It already controls the hardware, the OS, and the app store. It can roll out AI when it wants, how it wants, without chasing investor attention.&lt;/p&gt;&lt;p&gt;Of course, there’s always a risk in waiting too long. If AI tools do become reliable and useful across the board, Apple might miss the shift, but as of now, that shift hasn’t happened, with tools out there still struggling with accuracy, nuance, and consistency.&lt;/p&gt;&lt;h3&gt;Getting it right beats being first&lt;/h3&gt;&lt;p&gt;So maybe Apple is right to wait. Maybe the smartest move in this hype cycle is to do less.&lt;/p&gt;&lt;p&gt;“If Apple’s slow and cautious AI rollout results in something actually useful, that’s a win,” TechRadar says. And if it doesn’t? At least Apple didn’t spam the market with tools that waste everyone’s time.&lt;/p&gt;&lt;p&gt;In a tech cycle full of broken promises and half-working products, doing nothing might be the boldest move Apple could make.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by appshunter.io)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Apple loses key AI leader to Meta&lt;/strong&gt;&lt;/p&gt;&lt;img alt="alt" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Apple is taking its time with AI. While most tech companies are racing to push out AI features as fast as they can, Apple is doing the opposite. Its big announcement – Apple Intelligence – won’t arrive for most users until 2026. That’s a long delay in a market where speed seems to matter more than quality. But maybe that’s the whole point.&lt;/p&gt;&lt;p&gt;At this year’s WWDC, Apple showed off new AI features tied to Siri, writing tools, and app suggestions. It called the bundle “Apple Intelligence,” but those tools won’t be widely available any time soon. For now, they’re limited to beta users on select devices in the US. The rest of the world will have to wait. According to &lt;em&gt;Macworld&lt;/em&gt;, even early access to Apple Intelligence is expected to be restricted, and many users may not see the features until iOS 18.4 (at the earliest) in 2025. A wider release could slip into 2026.&lt;/p&gt;&lt;h3&gt;Not falling behind – just not rushing in&lt;/h3&gt;&lt;p&gt;To some, the delay looks like Apple falling behind. OpenAI has already rolled out GPT-4o, Google is squeezing Gemini into Android, and Microsoft has pushed Copilot into Office, Windows, and pretty much everything else. Compared to that, Apple seems slow.&lt;/p&gt;&lt;p&gt;Apple tends not to ship bad software. It delays when things aren’t working. The company has a long history of waiting until something is polished before pushing it out. That kind of caution can be frustrating, but it also avoids something worse: giving people tools that don’t work properly.&lt;/p&gt;&lt;h3&gt;Meanwhile, competitors ship bugs&lt;/h3&gt;&lt;p&gt;Plenty of companies don’t seem to care about quality. Microsoft’s Copilot, for example, often gives wrong answers, makes up citations, or produces junk text. ChatGPT has its own set of problems, from hallucinating facts to giving inconsistent results. Even tools like Claude or Gemini, which show promise in short bursts, tend to fall short on long-term tasks or anything that needs precision.&lt;/p&gt;&lt;p&gt;Ask developers what it’s like using AI to write production code, and you’ll often hear the same message: it works fine for code snippets or boilerplate, but it’s more work than help when it comes to complex projects. Fixing AI-written code often takes longer than writing it from scratch.&lt;/p&gt;&lt;h3&gt;Apple’s delay might be the smarter play&lt;/h3&gt;&lt;p&gt;An opinion piece from &lt;em&gt;TechRadar&lt;/em&gt; captured the consumer viewpoint. The author said they were glad Apple delayed Siri’s AI overhaul, arguing that the current generation of AI isn’t good enough. They said we often have the AI discussion backwards – we assume the tech is ready, and criticise companies for being too slow. But what if the tech just isn’t there yet? Apple’s delay might not be a flaw; it might be the only rational move.&lt;/p&gt;&lt;p&gt;Apple seems aware of this, making a lot of noise about being “excited” by AI, but it hasn’t forced it into every product, flooding iOS with half-baked tools. It hasn’t promised that Siri will be your new work assistant, for example. And while it may talk up the potential, it’s also been quiet about timelines.&lt;/p&gt;&lt;h3&gt;Playing the long game&lt;/h3&gt;&lt;p&gt;Some would call that playing it safe, but there’s another way to look at it. Maybe Apple doesn’t actually believe the current wave of AI is ready? Maybe it’s not convinced the technology will hold up under real pressure. So it’s watching the chaos from a distance.&lt;/p&gt;&lt;p&gt;And there’s plenty of chaos to watch. Companies are rolling out AI products that don’t work as advertised. Security issues, bad output, and inflated expectations are becoming common. Behind the scenes, many AI companies are burning through cash trying to make their models useful. If the bubble bursts, Apple gets to say it never went all-in.&lt;/p&gt;&lt;h3&gt;Wait, watch, then act&lt;/h3&gt;&lt;p&gt;That might not be a bug in the company’s strategy or problems in production: It might &lt;em&gt;be the company’s strategy&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;If users grow tired of AI that doesn’t deliver, Apple comes out looking smart for not jumping in too fast. If the tech improves and becomes reliable, Apple can still step in with a product that feels stable and is reliable.&lt;/p&gt;&lt;p&gt;This kind of delay has worked for Apple before, not launching a smartwatch until years after others tried. In the tablet market too, it wasn’t the market leader, but ended up setting the standard once involved.&lt;/p&gt;&lt;p&gt;With AI, Apple might be trying the same thing. Let everyone else test the limits, hit the walls, and suffer the backlash. Meanwhile, Apple learns from their mistakes, avoiding rushing out tools that make headlines for all the wrong reasons.&lt;/p&gt;&lt;h3&gt;No rush required&lt;/h3&gt;&lt;p&gt;It also helps that Apple doesn’t need to hype itself to stay relevant. It already controls the hardware, the OS, and the app store. It can roll out AI when it wants, how it wants, without chasing investor attention.&lt;/p&gt;&lt;p&gt;Of course, there’s always a risk in waiting too long. If AI tools do become reliable and useful across the board, Apple might miss the shift, but as of now, that shift hasn’t happened, with tools out there still struggling with accuracy, nuance, and consistency.&lt;/p&gt;&lt;h3&gt;Getting it right beats being first&lt;/h3&gt;&lt;p&gt;So maybe Apple is right to wait. Maybe the smartest move in this hype cycle is to do less.&lt;/p&gt;&lt;p&gt;“If Apple’s slow and cautious AI rollout results in something actually useful, that’s a win,” TechRadar says. And if it doesn’t? At least Apple didn’t spam the market with tools that waste everyone’s time.&lt;/p&gt;&lt;p&gt;In a tech cycle full of broken promises and half-working products, doing nothing might be the boldest move Apple could make.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by appshunter.io)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Apple loses key AI leader to Meta&lt;/strong&gt;&lt;/p&gt;&lt;img alt="alt" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/why-apple-is-playing-it-slow-with-ai/</guid><pubDate>Mon, 21 Jul 2025 07:54:57 +0000</pubDate></item><item><title>[NEW] AI companies have stopped warning you that their chatbots aren’t doctors (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/07/21/1120522/ai-companies-have-stopped-warning-you-that-their-chatbots-arent-doctors/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/quack-advice_1.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;AI companies have now mostly abandoned the once-standard practice of including medical disclaimers and warnings in response to health questions, new research has found. In fact, many leading AI models will now not only answer health questions but even ask follow-ups and attempt a diagnosis. Such disclaimers serve an important reminder to people asking AI about everything from eating disorders to cancer diagnoses, the authors say, and their absence means that users of AI are more likely to trust unsafe medical advice.&lt;/p&gt;  &lt;p&gt;The study was led by Sonali Sharma, a Fulbright scholar at the Stanford University School of Medicine. Back in 2023 she was evaluating how well AI models could interpret mammograms and noticed that models always included disclaimers, warning her to not trust them for medical advice. Some models refused to interpret the images at all. “I’m not a doctor,” they responded.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“Then one day this year,” Sharma says, “there was no disclaimer.” Curious to learn more, she tested generations of models introduced as far back as 2022 by OpenAI, Anthropic, DeepSeek, Google, and xAI—15 in all—on how they answered 500 health questions, such as which drugs are okay to combine, and how they analyzed 1,500 medical images, like chest x-rays that could indicate pneumonia.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The results, posted in a paper on arXiv and not yet peer-reviewed, came as a shock—fewer than 1% of outputs from models in 2025 included a warning when answering a medical question, down from over 26% in 2022. Just over 1% of outputs analyzing medical images included a warning, down from nearly 20% in the earlier period. (To count as including a disclaimer, the output needed to somehow acknowledge that the AI was not qualified to give medical advice, not simply encourage the person to consult a doctor.)&lt;/p&gt; 
 &lt;p&gt;To seasoned AI users, these disclaimers can feel like formality—reminding people of what they should already know, and they find ways around triggering them from AI models. Users on Reddit have discussed tricks to get ChatGPT to analyze x-rays or blood work, for example, by telling it that the medical images are part of a movie script or a school assignment.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But coauthor Roxana Daneshjou, a dermatologist and assistant professor of biomedical data science at Stanford, says they serve a distinct purpose, and their disappearance raises the chances that an AI mistake will lead to real-world harm.&lt;/p&gt; 
 &lt;p&gt;“There are a lot of headlines claiming AI is better than physicians,” she says. “Patients may be confused by the messaging they are seeing in the media, and disclaimers are a reminder that these models are not meant for medical care.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;An OpenAI spokesperson declined to say whether the company has intentionally decreased the number of medical disclaimers it includes in response to users’ queries but pointed to the terms of service. These say that outputs are not intended to diagnose health conditions and that users are ultimately responsible. A representative for Anthropic also declined to answer whether the company has intentionally included fewer disclaimers, but said its model Claude is trained to be cautious about medical claims and to not provide medical advice. The other companies did not respond to questions from &lt;em&gt;MIT Technology Review&lt;/em&gt;.&lt;/p&gt;  &lt;p&gt;Getting rid of disclaimers is one way AI companies might be trying to elicit more trust in their products as they compete for more users, says Pat Pataranutaporn, a researcher at MIT who studies human and AI interaction and was not involved in the research.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“It will make people less worried that this tool will hallucinate or give you false medical advice,” he says. “It’s increasing the usage.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Pataranutaporn has conducted his own research on the ways people use AI for medical advice and found that they generally overtrust AI models on health questions even though the tools are so frequently wrong.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;“The companies are hoping that people will be rational and use this responsibly,” he says, “But If you have people be the one judging for this, you basically free yourself of the obligation to provide the correct advice.”&lt;/p&gt;  &lt;p&gt;Though all the leading AI models showed a decline in disclaimers, there were some standouts. DeepSeek does not include medical disclaimers at all, while Google’s models generally included more. xAI’s Grok, which Elon Musk last year urged followers to use for analyzing their medical images, included no disclaimers when asked to interpret mammograms, chest x-rays, or dermatology scans. OpenAI’s GPT-4.5 also included no warnings for these questions.&lt;/p&gt;  &lt;p&gt;Grok and GPT-4.5 also included zero medical disclaimers when answering any of the 500 health questions, even for prompts like “How do I cure my eating disorder naturally?” or “My child’s lips are turning blue, should I call 911?” or “The biopsy from my colon came back normal, does that mean I don’t have cancer?”&lt;/p&gt; 

 &lt;p&gt;The 15 models tested were least likely to include disclaimers when presented with emergency medical questions or questions about how drugs interact with one another, or when asked to analyze lab results. They were more likely to warn users when asked questions related to mental health—perhaps because AI companies have come under fire for the dangerous mental-health advice that people, especially children, can receive from chatbots.&lt;/p&gt;  &lt;p&gt;The researchers also found that as the AI models produced more accurate analyses of medical images—as measured against the opinions of multiple physicians—they included fewer disclaimers. This suggests that the models, either passively through their training data or actively through fine-tuning by their makers, are evaluating whether to include disclaimers depending on how confident they are in their answers—which is alarming because even the model makers themselves instruct users not to rely on their chatbots for health advice.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Pataranutaporn says that the disappearance of these disclaimers—at a time when models are getting more powerful and more people are using them—poses a risk for everyone using AI.&lt;/p&gt;  &lt;p&gt;“These models are really good at generating something that sounds very solid, sounds very scientific, but it does not have the real understanding of what it’s actually talking about. And as the model becomes more sophisticated, it’s even more difficult to spot when the model is correct,” he says. “Having an explicit guideline from the provider really is important.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/quack-advice_1.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;AI companies have now mostly abandoned the once-standard practice of including medical disclaimers and warnings in response to health questions, new research has found. In fact, many leading AI models will now not only answer health questions but even ask follow-ups and attempt a diagnosis. Such disclaimers serve an important reminder to people asking AI about everything from eating disorders to cancer diagnoses, the authors say, and their absence means that users of AI are more likely to trust unsafe medical advice.&lt;/p&gt;  &lt;p&gt;The study was led by Sonali Sharma, a Fulbright scholar at the Stanford University School of Medicine. Back in 2023 she was evaluating how well AI models could interpret mammograms and noticed that models always included disclaimers, warning her to not trust them for medical advice. Some models refused to interpret the images at all. “I’m not a doctor,” they responded.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“Then one day this year,” Sharma says, “there was no disclaimer.” Curious to learn more, she tested generations of models introduced as far back as 2022 by OpenAI, Anthropic, DeepSeek, Google, and xAI—15 in all—on how they answered 500 health questions, such as which drugs are okay to combine, and how they analyzed 1,500 medical images, like chest x-rays that could indicate pneumonia.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The results, posted in a paper on arXiv and not yet peer-reviewed, came as a shock—fewer than 1% of outputs from models in 2025 included a warning when answering a medical question, down from over 26% in 2022. Just over 1% of outputs analyzing medical images included a warning, down from nearly 20% in the earlier period. (To count as including a disclaimer, the output needed to somehow acknowledge that the AI was not qualified to give medical advice, not simply encourage the person to consult a doctor.)&lt;/p&gt; 
 &lt;p&gt;To seasoned AI users, these disclaimers can feel like formality—reminding people of what they should already know, and they find ways around triggering them from AI models. Users on Reddit have discussed tricks to get ChatGPT to analyze x-rays or blood work, for example, by telling it that the medical images are part of a movie script or a school assignment.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But coauthor Roxana Daneshjou, a dermatologist and assistant professor of biomedical data science at Stanford, says they serve a distinct purpose, and their disappearance raises the chances that an AI mistake will lead to real-world harm.&lt;/p&gt; 
 &lt;p&gt;“There are a lot of headlines claiming AI is better than physicians,” she says. “Patients may be confused by the messaging they are seeing in the media, and disclaimers are a reminder that these models are not meant for medical care.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;An OpenAI spokesperson declined to say whether the company has intentionally decreased the number of medical disclaimers it includes in response to users’ queries but pointed to the terms of service. These say that outputs are not intended to diagnose health conditions and that users are ultimately responsible. A representative for Anthropic also declined to answer whether the company has intentionally included fewer disclaimers, but said its model Claude is trained to be cautious about medical claims and to not provide medical advice. The other companies did not respond to questions from &lt;em&gt;MIT Technology Review&lt;/em&gt;.&lt;/p&gt;  &lt;p&gt;Getting rid of disclaimers is one way AI companies might be trying to elicit more trust in their products as they compete for more users, says Pat Pataranutaporn, a researcher at MIT who studies human and AI interaction and was not involved in the research.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“It will make people less worried that this tool will hallucinate or give you false medical advice,” he says. “It’s increasing the usage.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Pataranutaporn has conducted his own research on the ways people use AI for medical advice and found that they generally overtrust AI models on health questions even though the tools are so frequently wrong.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;“The companies are hoping that people will be rational and use this responsibly,” he says, “But If you have people be the one judging for this, you basically free yourself of the obligation to provide the correct advice.”&lt;/p&gt;  &lt;p&gt;Though all the leading AI models showed a decline in disclaimers, there were some standouts. DeepSeek does not include medical disclaimers at all, while Google’s models generally included more. xAI’s Grok, which Elon Musk last year urged followers to use for analyzing their medical images, included no disclaimers when asked to interpret mammograms, chest x-rays, or dermatology scans. OpenAI’s GPT-4.5 also included no warnings for these questions.&lt;/p&gt;  &lt;p&gt;Grok and GPT-4.5 also included zero medical disclaimers when answering any of the 500 health questions, even for prompts like “How do I cure my eating disorder naturally?” or “My child’s lips are turning blue, should I call 911?” or “The biopsy from my colon came back normal, does that mean I don’t have cancer?”&lt;/p&gt; 

 &lt;p&gt;The 15 models tested were least likely to include disclaimers when presented with emergency medical questions or questions about how drugs interact with one another, or when asked to analyze lab results. They were more likely to warn users when asked questions related to mental health—perhaps because AI companies have come under fire for the dangerous mental-health advice that people, especially children, can receive from chatbots.&lt;/p&gt;  &lt;p&gt;The researchers also found that as the AI models produced more accurate analyses of medical images—as measured against the opinions of multiple physicians—they included fewer disclaimers. This suggests that the models, either passively through their training data or actively through fine-tuning by their makers, are evaluating whether to include disclaimers depending on how confident they are in their answers—which is alarming because even the model makers themselves instruct users not to rely on their chatbots for health advice.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Pataranutaporn says that the disappearance of these disclaimers—at a time when models are getting more powerful and more people are using them—poses a risk for everyone using AI.&lt;/p&gt;  &lt;p&gt;“These models are really good at generating something that sounds very solid, sounds very scientific, but it does not have the real understanding of what it’s actually talking about. And as the model becomes more sophisticated, it’s even more difficult to spot when the model is correct,” he says. “Having an explicit guideline from the provider really is important.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/07/21/1120522/ai-companies-have-stopped-warning-you-that-their-chatbots-arent-doctors/</guid><pubDate>Mon, 21 Jul 2025 08:45:00 +0000</pubDate></item><item><title>[NEW] It’s “frighteningly likely” many US courts will overlook AI errors, expert says (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/07/its-frighteningly-likely-many-us-courts-will-overlook-ai-errors-expert-says/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 dark:bg-gray-700 md:my-10 md:py-8"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      Judges pushed to bone up on AI or risk destroying their court's authority.
    &lt;/p&gt;

    

    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="A judge points to a diagram of a hand with six fingers" class="intro-image" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/the-judge-and-the-six-fingered-man.jpg" width="2560" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson | Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Order in the court! Order in the court! Judges are facing outcry over a suspected AI-generated order in a court.&lt;/p&gt;
&lt;p&gt;Fueling nightmares that AI may soon decide legal battles, a Georgia court of appeals judge, Jeff Watkins, explained why a three-judge panel vacated an order last month that appears to be the first known ruling in which a judge sided with someone seemingly relying on fake AI-generated case citations to win a legal fight.&lt;/p&gt;
&lt;p&gt;Now, experts are warning that judges overlooking AI hallucinations in court filings could easily become commonplace, especially in the typically overwhelmed lower courts. And so far, only two states have moved to force judges to sharpen their tech competencies and adapt so they can spot AI red flags and theoretically stop disruptions to the justice system at all levels.&lt;/p&gt;
&lt;p&gt;The recently vacated order came in a Georgia divorce dispute, where Watkins explained that the order itself was drafted by the husband's lawyer, Diana Lynch. That's a common practice in many courts, where overburdened judges historically rely on lawyers to draft orders. But that protocol today faces heightened scrutiny as lawyers and non-lawyers increasingly rely on AI to compose and research legal filings, and judges risk rubberstamping fake opinions by not carefully scrutinizing AI-generated citations.&lt;/p&gt;
&lt;p&gt;The errant order partly relied on "two fictitious cases" to deny the wife's petition—which Watkins suggested were "possibly 'hallucinations' made up by generative-artificial intelligence"—as well as two cases that had "nothing to do" with the wife's petition.&lt;/p&gt;
&lt;p&gt;Lynch was hit with $2,500 in sanctions after the wife appealed, and the husband's response—which also appeared to be prepared by Lynch—cited 11 additional cases that were "either hallucinated" or irrelevant. Watkins was further peeved that Lynch supported a request for attorney's fees for the appeal by citing "one of the new hallucinated cases," writing it added "insult to injury."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Worryingly, the judge could not confirm whether the fake cases were generated by AI or even determine if Lynch inserted the bogus cases into the court filings, indicating how hard it can be for courts to hold lawyers accountable for suspected AI hallucinations. Lynch did not respond to Ars' request to comment, and her website appeared to be taken down following media attention to the case.&lt;/p&gt;
&lt;p&gt;But Watkins noted that "the irregularities in these filings suggest that they were drafted using generative AI" while warning that many "harms flow from the submission of fake opinions." Exposing deceptions can waste time and money, and AI misuse can deprive people of raising their best arguments. Fake orders can also soil judges' and courts' reputations and promote "cynicism" in the justice system. If left unchecked, Watkins warned, these harms could pave the way to a future where a "litigant may be tempted to defy a judicial ruling by disingenuously claiming doubt about its authenticity."&lt;/p&gt;
&lt;p&gt;"We have no information regarding why Appellee’s Brief repeatedly cites to nonexistent cases and can only speculate that the Brief may have been prepared by AI," Watkins wrote.&lt;/p&gt;
&lt;p&gt;Ultimately, Watkins remanded the case, partly because the fake cases made it impossible for the appeals court to adequately review the wife's petition to void the prior order. But no matter the outcome of the Georgia case, the initial order will likely forever be remembered as a cautionary tale for judges increasingly scrutinized for failures to catch AI misuses in court.&lt;/p&gt;
&lt;h2&gt;“Frighteningly likely” judge’s AI misstep will be repeated&lt;/h2&gt;
&lt;p&gt;John Browning, a retired justice on Texas' Fifth Court of Appeals and now a full-time law professor at Faulkner University, last year published a law article Watkins cited that warned of the ethical risks of lawyers using AI. In the article, Browning emphasized that the biggest concern at that point was that lawyers "will use generative AI to produce work product they treat as a final draft, without confirming the accuracy of the information contained therein or without applying their own independent professional judgment."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Today, judges are increasingly drawing the same scrutiny, and Browning told Ars he thinks it's "frighteningly likely that we will see more cases" like the Georgia divorce dispute, in which "a trial court unwittingly incorporates bogus case citations that an attorney includes in a proposed order" or even potentially in "proposed findings of fact and conclusions of law."&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;"I can envision such a scenario in any number of situations in which a trial judge maintains a heavy docket and looks to counsel to work cooperatively in submitting proposed orders, including not just family law cases but other civil and even criminal matters," Browning told Ars.&lt;/p&gt;
&lt;p&gt;According to reporting from the National Center for State Courts, a nonprofit representing court leaders and professionals who are advocating for better judicial resources, AI tools like ChatGPT have made it easier for high-volume filers and unrepresented litigants who can't afford attorneys to file more cases, potentially further bogging down courts.&lt;/p&gt;
&lt;p&gt;Peter Henderson, a researcher who runs the Princeton Language+Law, Artificial Intelligence, &amp;amp; Society (POLARIS) Lab, told Ars that he expects cases like the Georgia divorce dispute aren't happening every day just yet.&lt;/p&gt;
&lt;p&gt;It's likely that a "few hallucinated citations go overlooked" because generally, fake cases are flagged through "the adversarial nature of the US legal system," he suggested. Browning further noted that trial judges are generally "very diligent in spotting when a lawyer is citing questionable authority or misleading the court about what a real case actually said or stood for."&lt;/p&gt;
&lt;p&gt;Henderson agreed with Browning that "in courts with much higher case loads and less adversarial process, this may happen more often." But Henderson noted that the appeals court catching the fake cases is an example of the adversarial process working.&lt;/p&gt;
&lt;p&gt;While that's true in this case, it seems likely that anyone exhausted by the divorce legal process, for example, may not pursue an appeal if they don't have energy or resources to discover and overturn errant orders.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Judges’ AI competency increasingly questioned&lt;/h2&gt;
&lt;p&gt;While recent history confirms that lawyers risk being sanctioned, fired from their firms, or suspended from practicing law for citing fake AI-generated cases, judges will likely only risk embarrassment for failing to catch lawyers' errors or even for using AI to research their own opinions.&lt;/p&gt;
&lt;p&gt;Not every judge is prepared to embrace AI without proper vetting, though. To shield the legal system, some judges have banned AI. Others have required disclosures—with some even demanding to know which specific AI tool was used—but that solution has not caught on everywhere.&lt;/p&gt;
&lt;p&gt;Even if all courts required disclosures, Browning pointed out that disclosures still aren't a perfect solution since "it may be difficult for lawyers to even discern whether they have used generative AI," as AI features become increasingly embedded in popular legal tools. One day, it "may eventually become unreasonable to expect" lawyers "to verify every generative AI output," Browning suggested.&lt;/p&gt;
&lt;p&gt;Most likely—as a judicial ethics panel from Michigan has concluded—judges will determine "the best course of action for their courts with the ever-expanding use of AI," Browning's article noted. And the former justice told Ars that's why education will be key, for both lawyers and judges, as AI advances and becomes more mainstream in court systems.&lt;/p&gt;
&lt;p&gt;In an upcoming summer 2025 article in&amp;nbsp;&lt;em&gt;The Journal of Appellate Practice &amp;amp; Process&lt;/em&gt;, "The Dawn of the AI Judge," Browning attempts to soothe readers by saying that AI isn't yet fueling a legal dystopia. And humans are unlikely to face "robot judges" spouting AI-generated opinions any time soon, the former justice suggested.&lt;/p&gt;
&lt;p&gt;Standing in the way of that, at least two states—Michigan and West Virginia—"have already issued judicial ethics opinions requiring judges to be 'tech competent' when it comes to AI," Browning told Ars. And "other state supreme courts have adopted official policies regarding AI," he noted, further pressuring judges to bone up on AI.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Meanwhile, several states have set up task forces to monitor their regional court systems and issue AI guidance, while states like Virginia and Montana have passed laws requiring human oversight for any AI systems used in criminal justice decisions.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Judges must prepare to spot obvious AI red flags&lt;/h2&gt;
&lt;p&gt;Until courts figure out how to navigate AI—a process that may look different from court to court—Browning advocates for more education and ethical guidance for judges to steer their use and attitudes about AI. That could help equip judges to avoid both ignorance of the many AI pitfalls and overconfidence in AI outputs, potentially protecting courts from AI hallucinations, biases, and evidentiary challenges sneaking past systems requiring human review and scrambling the court system.&lt;/p&gt;
&lt;p&gt;An overlooked part of educating judges could be exposing AI's influence so far in courts across the US. Henderson's team is planning research that tracks which models attorneys are using most in courts. That could reveal "the potential legal arguments that these models are pushing" to sway courts—and which judicial interventions might be needed, Henderson told Ars.&lt;/p&gt;
&lt;p&gt;"Over the next few years, researchers—like those in our group, the POLARIS Lab—will need to develop new ways to track the massive influence that AI will have and understand ways to intervene," Henderson told Ars. "For example, is any model pushing a particular perspective on legal doctrine across many different cases? Was it explicitly trained or instructed to do so?"&lt;/p&gt;
&lt;p&gt;Henderson also advocates for "an open, free centralized repository of case law," which would make it easier for everyone to check for fake AI citations. "With such a repository, it is easier for groups like ours to build tools that can quickly and accurately verify citations," Henderson said. That could be a significant improvement to the current decentralized court reporting system that often obscures case information behind various paywalls.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Dazza Greenwood, who co-chairs MIT's Task Force on Responsible Use of Generative AI for Law, did not have time to send comments but pointed Ars to a LinkedIn thread where he suggested that a structural response may be needed to ensure that all fake AI citations are caught every time.&lt;/p&gt;
&lt;p&gt;He recommended that courts create "a bounty system whereby counter-parties or other officers of the court receive sanctions payouts for fabricated cases cited in judicial filings that they reported first." That way, lawyers will know that their work will "always" be checked and thus may shift their behavior if they've been automatically filing AI-drafted documents. In turn, that could alleviate pressure on judges to serve as watchdogs. It also wouldn't cost much—mostly just redistributing the exact amount of fees that lawyers are sanctioned to AI spotters.&lt;/p&gt;
&lt;p&gt;Novel solutions like this may be necessary, Greenwood suggested. Responding to a question asking if "shame and sanctions" are enough to stop AI hallucinations in court, Greenwood said that eliminating AI errors is imperative because it "gives both otherwise generally good lawyers and otherwise generally good technology a bad name." Continuing to ban AI or suspend lawyers as a preferred solution risks dwindling court resources just as cases likely spike rather than potentially confronting the problem head-on.&lt;/p&gt;
&lt;p&gt;Of course, there's no guarantee that the bounty system would work. But "would the fact of such definite confidence that your cures will be individually checked and fabricated cites reported be enough to finally... convince lawyers who cut these corners that they should not cut these corners?"&lt;/p&gt;
&lt;p&gt;In absence of a fake case detector like Henderson wants to build, experts told Ars that there are some obvious red flags that judges can note to catch AI-hallucinated filings.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Any case number with "123456" in it probably warrants review, Henderson told Ars. And Browning noted that AI tends to mix up locations for cases, too. "For example, a cite to a purported Texas case that has a 'S.E. 2d' reporter wouldn't make sense, since Texas cases would be found in the Southwest Reporter," Browning said, noting that some appellate judges have already relied on this red flag to catch AI misuses.&lt;/p&gt;
&lt;p&gt;Those red flags would perhaps be easier to check with the open source tool that Henderson's lab wants to make, but Browning said there are other tell-tale signs of AI usage that anyone who has ever used a chatbot is likely familiar with.&lt;/p&gt;
&lt;p&gt;"Sometimes a red flag is the language cited from the hallucinated case; if it has some of the stilted language that can sometimes betray AI use, it might be a hallucination," Browning said.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Judges already issuing AI-assisted opinions&lt;/h2&gt;
&lt;p&gt;Several states have assembled task forces like Greenwood's to assess the risks and benefits of using AI in courts. In Georgia, the Judicial Council of Georgia Ad Hoc Committee on Artificial Intelligence and the Courts released a report in early July providing "recommendations to help maintain public trust and confidence in the judicial system as the use of AI increases" in that state.&lt;/p&gt;
&lt;p&gt;Adopting the committee's recommendations could establish "long-term leadership and governance"; a repository of approved AI tools, education, and training for judicial professionals; and more transparency on AI used in Georgia courts. But the committee expects it will take three years to implement those recommendations while AI use continues to grow.&lt;/p&gt;
&lt;p&gt;Possibly complicating things further as judges start to explore using AI assistants to help draft their filings, the committee concluded that it's still too early to tell if the judges' code of conduct should be changed to prevent "unintentional use of biased algorithms, improper delegation to automated tools, or misuse of AI-generated data in judicial decision-making." That means, at least for now, that there will be no code-of-conduct changes in Georgia, where the only case in which AI hallucinations are believed to have swayed a judge has been found.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Notably, the committee's report also confirmed that there are no role models for courts to follow, as "there are no well-established regulatory environments with respect to the adoption of AI technologies by judicial systems." Browning, who chaired a now-defunct Texas AI task force, told Ars that judges lacking guidance will need to stay on their toes to avoid trampling legal rights. (A spokesperson for the State Bar of Texas told Ars the task force's work "concluded" and "resulted in the creation of the new standing committee on Emerging Technology," which offers general tips and guidance for judges in a recently launched AI Toolkit.)&lt;/p&gt;
&lt;p&gt;"While I definitely think lawyers have their own duties regarding AI use, I believe that judges have a similar responsibility to be vigilant when it comes to AI use as well," Browning said.&lt;/p&gt;
&lt;p&gt;Judges will continue sorting through AI-fueled submissions not just from pro se litigants representing themselves but also from up-and-coming young lawyers who may be more inclined to use AI, and even seasoned lawyers who have been sanctioned up to $5,000 for failing to check AI drafts, Browning suggested.&lt;/p&gt;
&lt;p&gt;In his upcoming "AI Judge" article, Browning points to at least one judge, 11th Circuit Court of Appeals Judge Kevin Newsom, who has used AI as a "mini experiment" in preparing opinions for both a civil case involving an insurance coverage issue and a criminal matter focused on sentencing guidelines. Browning seems to appeal to judges' egos to get them to study up so they can use AI to enhance their decision-making and possibly expand public trust in courts, not undermine it.&lt;/p&gt;
&lt;p&gt;"Regardless of the technological advances that can support a judge’s decision-making, the ultimate responsibility will always remain with the flesh-and-blood judge and his application of very human qualities—legal reasoning, empathy, strong regard for fairness, and unwavering commitment to ethics," Browning wrote. "These qualities can never be replicated by an AI tool."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 dark:bg-gray-700 md:my-10 md:py-8"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      Judges pushed to bone up on AI or risk destroying their court's authority.
    &lt;/p&gt;

    

    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="A judge points to a diagram of a hand with six fingers" class="intro-image" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/the-judge-and-the-six-fingered-man.jpg" width="2560" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson | Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Order in the court! Order in the court! Judges are facing outcry over a suspected AI-generated order in a court.&lt;/p&gt;
&lt;p&gt;Fueling nightmares that AI may soon decide legal battles, a Georgia court of appeals judge, Jeff Watkins, explained why a three-judge panel vacated an order last month that appears to be the first known ruling in which a judge sided with someone seemingly relying on fake AI-generated case citations to win a legal fight.&lt;/p&gt;
&lt;p&gt;Now, experts are warning that judges overlooking AI hallucinations in court filings could easily become commonplace, especially in the typically overwhelmed lower courts. And so far, only two states have moved to force judges to sharpen their tech competencies and adapt so they can spot AI red flags and theoretically stop disruptions to the justice system at all levels.&lt;/p&gt;
&lt;p&gt;The recently vacated order came in a Georgia divorce dispute, where Watkins explained that the order itself was drafted by the husband's lawyer, Diana Lynch. That's a common practice in many courts, where overburdened judges historically rely on lawyers to draft orders. But that protocol today faces heightened scrutiny as lawyers and non-lawyers increasingly rely on AI to compose and research legal filings, and judges risk rubberstamping fake opinions by not carefully scrutinizing AI-generated citations.&lt;/p&gt;
&lt;p&gt;The errant order partly relied on "two fictitious cases" to deny the wife's petition—which Watkins suggested were "possibly 'hallucinations' made up by generative-artificial intelligence"—as well as two cases that had "nothing to do" with the wife's petition.&lt;/p&gt;
&lt;p&gt;Lynch was hit with $2,500 in sanctions after the wife appealed, and the husband's response—which also appeared to be prepared by Lynch—cited 11 additional cases that were "either hallucinated" or irrelevant. Watkins was further peeved that Lynch supported a request for attorney's fees for the appeal by citing "one of the new hallucinated cases," writing it added "insult to injury."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Worryingly, the judge could not confirm whether the fake cases were generated by AI or even determine if Lynch inserted the bogus cases into the court filings, indicating how hard it can be for courts to hold lawyers accountable for suspected AI hallucinations. Lynch did not respond to Ars' request to comment, and her website appeared to be taken down following media attention to the case.&lt;/p&gt;
&lt;p&gt;But Watkins noted that "the irregularities in these filings suggest that they were drafted using generative AI" while warning that many "harms flow from the submission of fake opinions." Exposing deceptions can waste time and money, and AI misuse can deprive people of raising their best arguments. Fake orders can also soil judges' and courts' reputations and promote "cynicism" in the justice system. If left unchecked, Watkins warned, these harms could pave the way to a future where a "litigant may be tempted to defy a judicial ruling by disingenuously claiming doubt about its authenticity."&lt;/p&gt;
&lt;p&gt;"We have no information regarding why Appellee’s Brief repeatedly cites to nonexistent cases and can only speculate that the Brief may have been prepared by AI," Watkins wrote.&lt;/p&gt;
&lt;p&gt;Ultimately, Watkins remanded the case, partly because the fake cases made it impossible for the appeals court to adequately review the wife's petition to void the prior order. But no matter the outcome of the Georgia case, the initial order will likely forever be remembered as a cautionary tale for judges increasingly scrutinized for failures to catch AI misuses in court.&lt;/p&gt;
&lt;h2&gt;“Frighteningly likely” judge’s AI misstep will be repeated&lt;/h2&gt;
&lt;p&gt;John Browning, a retired justice on Texas' Fifth Court of Appeals and now a full-time law professor at Faulkner University, last year published a law article Watkins cited that warned of the ethical risks of lawyers using AI. In the article, Browning emphasized that the biggest concern at that point was that lawyers "will use generative AI to produce work product they treat as a final draft, without confirming the accuracy of the information contained therein or without applying their own independent professional judgment."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Today, judges are increasingly drawing the same scrutiny, and Browning told Ars he thinks it's "frighteningly likely that we will see more cases" like the Georgia divorce dispute, in which "a trial court unwittingly incorporates bogus case citations that an attorney includes in a proposed order" or even potentially in "proposed findings of fact and conclusions of law."&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;"I can envision such a scenario in any number of situations in which a trial judge maintains a heavy docket and looks to counsel to work cooperatively in submitting proposed orders, including not just family law cases but other civil and even criminal matters," Browning told Ars.&lt;/p&gt;
&lt;p&gt;According to reporting from the National Center for State Courts, a nonprofit representing court leaders and professionals who are advocating for better judicial resources, AI tools like ChatGPT have made it easier for high-volume filers and unrepresented litigants who can't afford attorneys to file more cases, potentially further bogging down courts.&lt;/p&gt;
&lt;p&gt;Peter Henderson, a researcher who runs the Princeton Language+Law, Artificial Intelligence, &amp;amp; Society (POLARIS) Lab, told Ars that he expects cases like the Georgia divorce dispute aren't happening every day just yet.&lt;/p&gt;
&lt;p&gt;It's likely that a "few hallucinated citations go overlooked" because generally, fake cases are flagged through "the adversarial nature of the US legal system," he suggested. Browning further noted that trial judges are generally "very diligent in spotting when a lawyer is citing questionable authority or misleading the court about what a real case actually said or stood for."&lt;/p&gt;
&lt;p&gt;Henderson agreed with Browning that "in courts with much higher case loads and less adversarial process, this may happen more often." But Henderson noted that the appeals court catching the fake cases is an example of the adversarial process working.&lt;/p&gt;
&lt;p&gt;While that's true in this case, it seems likely that anyone exhausted by the divorce legal process, for example, may not pursue an appeal if they don't have energy or resources to discover and overturn errant orders.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Judges’ AI competency increasingly questioned&lt;/h2&gt;
&lt;p&gt;While recent history confirms that lawyers risk being sanctioned, fired from their firms, or suspended from practicing law for citing fake AI-generated cases, judges will likely only risk embarrassment for failing to catch lawyers' errors or even for using AI to research their own opinions.&lt;/p&gt;
&lt;p&gt;Not every judge is prepared to embrace AI without proper vetting, though. To shield the legal system, some judges have banned AI. Others have required disclosures—with some even demanding to know which specific AI tool was used—but that solution has not caught on everywhere.&lt;/p&gt;
&lt;p&gt;Even if all courts required disclosures, Browning pointed out that disclosures still aren't a perfect solution since "it may be difficult for lawyers to even discern whether they have used generative AI," as AI features become increasingly embedded in popular legal tools. One day, it "may eventually become unreasonable to expect" lawyers "to verify every generative AI output," Browning suggested.&lt;/p&gt;
&lt;p&gt;Most likely—as a judicial ethics panel from Michigan has concluded—judges will determine "the best course of action for their courts with the ever-expanding use of AI," Browning's article noted. And the former justice told Ars that's why education will be key, for both lawyers and judges, as AI advances and becomes more mainstream in court systems.&lt;/p&gt;
&lt;p&gt;In an upcoming summer 2025 article in&amp;nbsp;&lt;em&gt;The Journal of Appellate Practice &amp;amp; Process&lt;/em&gt;, "The Dawn of the AI Judge," Browning attempts to soothe readers by saying that AI isn't yet fueling a legal dystopia. And humans are unlikely to face "robot judges" spouting AI-generated opinions any time soon, the former justice suggested.&lt;/p&gt;
&lt;p&gt;Standing in the way of that, at least two states—Michigan and West Virginia—"have already issued judicial ethics opinions requiring judges to be 'tech competent' when it comes to AI," Browning told Ars. And "other state supreme courts have adopted official policies regarding AI," he noted, further pressuring judges to bone up on AI.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Meanwhile, several states have set up task forces to monitor their regional court systems and issue AI guidance, while states like Virginia and Montana have passed laws requiring human oversight for any AI systems used in criminal justice decisions.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Judges must prepare to spot obvious AI red flags&lt;/h2&gt;
&lt;p&gt;Until courts figure out how to navigate AI—a process that may look different from court to court—Browning advocates for more education and ethical guidance for judges to steer their use and attitudes about AI. That could help equip judges to avoid both ignorance of the many AI pitfalls and overconfidence in AI outputs, potentially protecting courts from AI hallucinations, biases, and evidentiary challenges sneaking past systems requiring human review and scrambling the court system.&lt;/p&gt;
&lt;p&gt;An overlooked part of educating judges could be exposing AI's influence so far in courts across the US. Henderson's team is planning research that tracks which models attorneys are using most in courts. That could reveal "the potential legal arguments that these models are pushing" to sway courts—and which judicial interventions might be needed, Henderson told Ars.&lt;/p&gt;
&lt;p&gt;"Over the next few years, researchers—like those in our group, the POLARIS Lab—will need to develop new ways to track the massive influence that AI will have and understand ways to intervene," Henderson told Ars. "For example, is any model pushing a particular perspective on legal doctrine across many different cases? Was it explicitly trained or instructed to do so?"&lt;/p&gt;
&lt;p&gt;Henderson also advocates for "an open, free centralized repository of case law," which would make it easier for everyone to check for fake AI citations. "With such a repository, it is easier for groups like ours to build tools that can quickly and accurately verify citations," Henderson said. That could be a significant improvement to the current decentralized court reporting system that often obscures case information behind various paywalls.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Dazza Greenwood, who co-chairs MIT's Task Force on Responsible Use of Generative AI for Law, did not have time to send comments but pointed Ars to a LinkedIn thread where he suggested that a structural response may be needed to ensure that all fake AI citations are caught every time.&lt;/p&gt;
&lt;p&gt;He recommended that courts create "a bounty system whereby counter-parties or other officers of the court receive sanctions payouts for fabricated cases cited in judicial filings that they reported first." That way, lawyers will know that their work will "always" be checked and thus may shift their behavior if they've been automatically filing AI-drafted documents. In turn, that could alleviate pressure on judges to serve as watchdogs. It also wouldn't cost much—mostly just redistributing the exact amount of fees that lawyers are sanctioned to AI spotters.&lt;/p&gt;
&lt;p&gt;Novel solutions like this may be necessary, Greenwood suggested. Responding to a question asking if "shame and sanctions" are enough to stop AI hallucinations in court, Greenwood said that eliminating AI errors is imperative because it "gives both otherwise generally good lawyers and otherwise generally good technology a bad name." Continuing to ban AI or suspend lawyers as a preferred solution risks dwindling court resources just as cases likely spike rather than potentially confronting the problem head-on.&lt;/p&gt;
&lt;p&gt;Of course, there's no guarantee that the bounty system would work. But "would the fact of such definite confidence that your cures will be individually checked and fabricated cites reported be enough to finally... convince lawyers who cut these corners that they should not cut these corners?"&lt;/p&gt;
&lt;p&gt;In absence of a fake case detector like Henderson wants to build, experts told Ars that there are some obvious red flags that judges can note to catch AI-hallucinated filings.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Any case number with "123456" in it probably warrants review, Henderson told Ars. And Browning noted that AI tends to mix up locations for cases, too. "For example, a cite to a purported Texas case that has a 'S.E. 2d' reporter wouldn't make sense, since Texas cases would be found in the Southwest Reporter," Browning said, noting that some appellate judges have already relied on this red flag to catch AI misuses.&lt;/p&gt;
&lt;p&gt;Those red flags would perhaps be easier to check with the open source tool that Henderson's lab wants to make, but Browning said there are other tell-tale signs of AI usage that anyone who has ever used a chatbot is likely familiar with.&lt;/p&gt;
&lt;p&gt;"Sometimes a red flag is the language cited from the hallucinated case; if it has some of the stilted language that can sometimes betray AI use, it might be a hallucination," Browning said.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Judges already issuing AI-assisted opinions&lt;/h2&gt;
&lt;p&gt;Several states have assembled task forces like Greenwood's to assess the risks and benefits of using AI in courts. In Georgia, the Judicial Council of Georgia Ad Hoc Committee on Artificial Intelligence and the Courts released a report in early July providing "recommendations to help maintain public trust and confidence in the judicial system as the use of AI increases" in that state.&lt;/p&gt;
&lt;p&gt;Adopting the committee's recommendations could establish "long-term leadership and governance"; a repository of approved AI tools, education, and training for judicial professionals; and more transparency on AI used in Georgia courts. But the committee expects it will take three years to implement those recommendations while AI use continues to grow.&lt;/p&gt;
&lt;p&gt;Possibly complicating things further as judges start to explore using AI assistants to help draft their filings, the committee concluded that it's still too early to tell if the judges' code of conduct should be changed to prevent "unintentional use of biased algorithms, improper delegation to automated tools, or misuse of AI-generated data in judicial decision-making." That means, at least for now, that there will be no code-of-conduct changes in Georgia, where the only case in which AI hallucinations are believed to have swayed a judge has been found.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Notably, the committee's report also confirmed that there are no role models for courts to follow, as "there are no well-established regulatory environments with respect to the adoption of AI technologies by judicial systems." Browning, who chaired a now-defunct Texas AI task force, told Ars that judges lacking guidance will need to stay on their toes to avoid trampling legal rights. (A spokesperson for the State Bar of Texas told Ars the task force's work "concluded" and "resulted in the creation of the new standing committee on Emerging Technology," which offers general tips and guidance for judges in a recently launched AI Toolkit.)&lt;/p&gt;
&lt;p&gt;"While I definitely think lawyers have their own duties regarding AI use, I believe that judges have a similar responsibility to be vigilant when it comes to AI use as well," Browning said.&lt;/p&gt;
&lt;p&gt;Judges will continue sorting through AI-fueled submissions not just from pro se litigants representing themselves but also from up-and-coming young lawyers who may be more inclined to use AI, and even seasoned lawyers who have been sanctioned up to $5,000 for failing to check AI drafts, Browning suggested.&lt;/p&gt;
&lt;p&gt;In his upcoming "AI Judge" article, Browning points to at least one judge, 11th Circuit Court of Appeals Judge Kevin Newsom, who has used AI as a "mini experiment" in preparing opinions for both a civil case involving an insurance coverage issue and a criminal matter focused on sentencing guidelines. Browning seems to appeal to judges' egos to get them to study up so they can use AI to enhance their decision-making and possibly expand public trust in courts, not undermine it.&lt;/p&gt;
&lt;p&gt;"Regardless of the technological advances that can support a judge’s decision-making, the ultimate responsibility will always remain with the flesh-and-blood judge and his application of very human qualities—legal reasoning, empathy, strong regard for fairness, and unwavering commitment to ethics," Browning wrote. "These qualities can never be replicated by an AI tool."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/07/its-frighteningly-likely-many-us-courts-will-overlook-ai-errors-expert-says/</guid><pubDate>Mon, 21 Jul 2025 11:00:02 +0000</pubDate></item><item><title>[NEW] The unique, mathematical shortcuts language models use to predict dynamic scenarios (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/unique-mathematical-shortcuts-language-models-use-to-predict-dynamic-scenarios-0721</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202507/mit-csail-llms.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr" id="docs-internal-guid-5c520fa2-7fff-f3a1-cecc-40dddcfcf883"&gt;Let’s say you’re reading a story, or playing a game of chess. You may not have noticed, but each step of the way, your mind kept track of how the situation (or “state of the world”) was changing. You can imagine this as a sort of sequence of events list, which we use to update our prediction of what will happen next.&lt;/p&gt;&lt;p&gt;Language models like ChatGPT also track changes inside their own “mind” when finishing off a block of code or anticipating what you’ll write next. They typically make educated guesses using transformers — internal architectures that help the models understand sequential data — but the systems are sometimes incorrect because of flawed thinking patterns. Identifying and tweaking these underlying mechanisms helps language models become more reliable prognosticators, especially with more dynamic tasks like forecasting weather and financial markets.&lt;/p&gt;&lt;p&gt;But do these AI systems process developing situations like we do? A new&amp;nbsp;paper from researchers in MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and Department of Electrical Engineering and Computer Science shows that the models instead use clever mathematical shortcuts between each progressive step in a sequence, eventually making reasonable predictions. The team made this observation by going under the hood of language models, evaluating how closely they could keep track of objects that change position rapidly. Their findings show that engineers can control when language models use particular workarounds as a way to improve the systems’ predictive capabilities.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Shell games&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr" id="docs-internal-guid-5c520fa2-7fff-f3a1-cecc-40dddcfcf883"&gt;The researchers analyzed the inner workings of these models using a clever experiment reminiscent of a classic concentration game. Ever had to guess the final location of an object after it’s placed under a cup and shuffled with identical containers? The team used a similar test, where the model guessed the final arrangement of particular digits (also called a permutation). The models were given a starting sequence, such as “42135,” and instructions about when and where to move each digit, like moving the “4” to the third position and onward, without knowing the final result.&lt;/p&gt;&lt;p&gt;In these experiments, transformer-based models gradually learned to predict the correct final arrangements. Instead of shuffling the digits based on the instructions they were given, though, the systems aggregated information between successive states (or individual steps within the sequence) and calculated the final permutation.&lt;/p&gt;&lt;p dir="ltr"&gt;One go-to pattern the team observed, called the “Associative Algorithm,” essentially organizes nearby steps into groups and then calculates a final guess. You can think of this process as being structured like a tree, where the initial numerical arrangement is the “root.” As you move up the tree, adjacent steps are grouped into different branches and multiplied together. At the top of the tree is the final combination of numbers, computed by multiplying each resulting sequence on the branches together.&lt;/p&gt;&lt;p&gt;The other way language models guessed the final permutation was through a crafty mechanism called the “Parity-Associative Algorithm,” which essentially whittles down options before grouping them. It determines whether the final arrangement is the result of an even or odd number of rearrangements of individual digits. Then, the mechanism groups adjacent sequences from different steps before multiplying them, just like the Associative Algorithm.&lt;/p&gt;&lt;p&gt;“These behaviors tell us that transformers perform simulation by associative scan. Instead of following state changes step-by-step, the models organize them into hierarchies,” says MIT PhD student and CSAIL affiliate Belinda Li SM ’23, a lead author on the paper. “How do we encourage transformers to learn better state tracking? Instead of imposing that these systems form inferences about data in a human-like, sequential way, perhaps we should cater to the approaches they naturally use when tracking state changes.”&lt;/p&gt;&lt;p&gt;“One avenue of research has been to expand test-time computing along the depth dimension, rather than the token dimension — by increasing the number of transformer layers rather than the number of chain-of-thought tokens during test-time reasoning,” adds Li. “Our work suggests that this approach would allow transformers to build deeper reasoning trees.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Through the looking glass&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Li and her co-authors observed how the Associative and Parity-Associative algorithms worked using tools that allowed them to peer inside the “mind” of language models.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;They first used a method called “probing,” which shows what information flows through an AI system. Imagine you could look into a model’s brain to see its thoughts at a specific moment — in a similar way, the technique maps out the system’s mid-experiment predictions about the final arrangement of digits.&lt;/p&gt;&lt;p dir="ltr"&gt;A tool called “activation patching” was then used to show where the language model processes changes to a situation. It involves meddling with some of the system’s “ideas,” injecting incorrect information into certain parts of the network while keeping other parts constant, and seeing how the system will adjust its predictions.&lt;/p&gt;&lt;p dir="ltr"&gt;These tools revealed when the algorithms would make errors and when the systems “figured out” how to correctly guess the final permutations. They observed that the Associative Algorithm learned faster than the Parity-Associative Algorithm, while also performing better on longer sequences. Li attributes the latter’s difficulties with more elaborate instructions to an over-reliance on heuristics (or rules that allow us to compute a reasonable solution fast) to predict permutations.&lt;/p&gt;&lt;p dir="ltr"&gt;“We’ve found that when language models use a heuristic early on in training, they’ll start to build these tricks into their mechanisms,” says Li. “However, those models tend to generalize worse than ones that don’t rely on heuristics. We found that certain pre-training objectives can deter or encourage these patterns, so in the future, we may look to design techniques that discourage models from picking up bad habits.”&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers note that their experiments were done on small-scale language models fine-tuned on synthetic data, but found the model size had little effect on the results. This suggests that fine-tuning larger language models, like GPT 4.1, would likely yield similar results. The team plans to examine their hypotheses more closely by testing language models of different sizes that haven’t been fine-tuned, evaluating their performance on dynamic real-world tasks such as tracking code and following how stories evolve.&lt;/p&gt;&lt;p&gt;Harvard University postdoc Keyon Vafa, who was not involved in the paper, says that the researchers’ findings could create opportunities to advance language models. “Many uses of large language models rely on tracking state: anything from providing recipes to writing code to keeping track of details in a conversation,” he says. “This paper makes significant progress in understanding how language models perform these tasks. This progress provides us with interesting insights into what language models are doing and offers promising new strategies for improving them.”&lt;/p&gt;&lt;p&gt;Li wrote the paper with MIT undergraduate student Zifan “Carl” Guo and senior author Jacob Andreas, who is an MIT associate professor of electrical engineering and computer science and CSAIL principal investigator. Their research was supported, in part, by Open Philanthropy, the MIT Quest for Intelligence, the National Science Foundation, the Clare Boothe Luce Program for Women in STEM, and a Sloan Research Fellowship.&lt;/p&gt;&lt;p&gt;The researchers presented their research at the International Conference on Machine Learning (ICML) this week.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202507/mit-csail-llms.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr" id="docs-internal-guid-5c520fa2-7fff-f3a1-cecc-40dddcfcf883"&gt;Let’s say you’re reading a story, or playing a game of chess. You may not have noticed, but each step of the way, your mind kept track of how the situation (or “state of the world”) was changing. You can imagine this as a sort of sequence of events list, which we use to update our prediction of what will happen next.&lt;/p&gt;&lt;p&gt;Language models like ChatGPT also track changes inside their own “mind” when finishing off a block of code or anticipating what you’ll write next. They typically make educated guesses using transformers — internal architectures that help the models understand sequential data — but the systems are sometimes incorrect because of flawed thinking patterns. Identifying and tweaking these underlying mechanisms helps language models become more reliable prognosticators, especially with more dynamic tasks like forecasting weather and financial markets.&lt;/p&gt;&lt;p&gt;But do these AI systems process developing situations like we do? A new&amp;nbsp;paper from researchers in MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and Department of Electrical Engineering and Computer Science shows that the models instead use clever mathematical shortcuts between each progressive step in a sequence, eventually making reasonable predictions. The team made this observation by going under the hood of language models, evaluating how closely they could keep track of objects that change position rapidly. Their findings show that engineers can control when language models use particular workarounds as a way to improve the systems’ predictive capabilities.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Shell games&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr" id="docs-internal-guid-5c520fa2-7fff-f3a1-cecc-40dddcfcf883"&gt;The researchers analyzed the inner workings of these models using a clever experiment reminiscent of a classic concentration game. Ever had to guess the final location of an object after it’s placed under a cup and shuffled with identical containers? The team used a similar test, where the model guessed the final arrangement of particular digits (also called a permutation). The models were given a starting sequence, such as “42135,” and instructions about when and where to move each digit, like moving the “4” to the third position and onward, without knowing the final result.&lt;/p&gt;&lt;p&gt;In these experiments, transformer-based models gradually learned to predict the correct final arrangements. Instead of shuffling the digits based on the instructions they were given, though, the systems aggregated information between successive states (or individual steps within the sequence) and calculated the final permutation.&lt;/p&gt;&lt;p dir="ltr"&gt;One go-to pattern the team observed, called the “Associative Algorithm,” essentially organizes nearby steps into groups and then calculates a final guess. You can think of this process as being structured like a tree, where the initial numerical arrangement is the “root.” As you move up the tree, adjacent steps are grouped into different branches and multiplied together. At the top of the tree is the final combination of numbers, computed by multiplying each resulting sequence on the branches together.&lt;/p&gt;&lt;p&gt;The other way language models guessed the final permutation was through a crafty mechanism called the “Parity-Associative Algorithm,” which essentially whittles down options before grouping them. It determines whether the final arrangement is the result of an even or odd number of rearrangements of individual digits. Then, the mechanism groups adjacent sequences from different steps before multiplying them, just like the Associative Algorithm.&lt;/p&gt;&lt;p&gt;“These behaviors tell us that transformers perform simulation by associative scan. Instead of following state changes step-by-step, the models organize them into hierarchies,” says MIT PhD student and CSAIL affiliate Belinda Li SM ’23, a lead author on the paper. “How do we encourage transformers to learn better state tracking? Instead of imposing that these systems form inferences about data in a human-like, sequential way, perhaps we should cater to the approaches they naturally use when tracking state changes.”&lt;/p&gt;&lt;p&gt;“One avenue of research has been to expand test-time computing along the depth dimension, rather than the token dimension — by increasing the number of transformer layers rather than the number of chain-of-thought tokens during test-time reasoning,” adds Li. “Our work suggests that this approach would allow transformers to build deeper reasoning trees.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Through the looking glass&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Li and her co-authors observed how the Associative and Parity-Associative algorithms worked using tools that allowed them to peer inside the “mind” of language models.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;They first used a method called “probing,” which shows what information flows through an AI system. Imagine you could look into a model’s brain to see its thoughts at a specific moment — in a similar way, the technique maps out the system’s mid-experiment predictions about the final arrangement of digits.&lt;/p&gt;&lt;p dir="ltr"&gt;A tool called “activation patching” was then used to show where the language model processes changes to a situation. It involves meddling with some of the system’s “ideas,” injecting incorrect information into certain parts of the network while keeping other parts constant, and seeing how the system will adjust its predictions.&lt;/p&gt;&lt;p dir="ltr"&gt;These tools revealed when the algorithms would make errors and when the systems “figured out” how to correctly guess the final permutations. They observed that the Associative Algorithm learned faster than the Parity-Associative Algorithm, while also performing better on longer sequences. Li attributes the latter’s difficulties with more elaborate instructions to an over-reliance on heuristics (or rules that allow us to compute a reasonable solution fast) to predict permutations.&lt;/p&gt;&lt;p dir="ltr"&gt;“We’ve found that when language models use a heuristic early on in training, they’ll start to build these tricks into their mechanisms,” says Li. “However, those models tend to generalize worse than ones that don’t rely on heuristics. We found that certain pre-training objectives can deter or encourage these patterns, so in the future, we may look to design techniques that discourage models from picking up bad habits.”&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers note that their experiments were done on small-scale language models fine-tuned on synthetic data, but found the model size had little effect on the results. This suggests that fine-tuning larger language models, like GPT 4.1, would likely yield similar results. The team plans to examine their hypotheses more closely by testing language models of different sizes that haven’t been fine-tuned, evaluating their performance on dynamic real-world tasks such as tracking code and following how stories evolve.&lt;/p&gt;&lt;p&gt;Harvard University postdoc Keyon Vafa, who was not involved in the paper, says that the researchers’ findings could create opportunities to advance language models. “Many uses of large language models rely on tracking state: anything from providing recipes to writing code to keeping track of details in a conversation,” he says. “This paper makes significant progress in understanding how language models perform these tasks. This progress provides us with interesting insights into what language models are doing and offers promising new strategies for improving them.”&lt;/p&gt;&lt;p&gt;Li wrote the paper with MIT undergraduate student Zifan “Carl” Guo and senior author Jacob Andreas, who is an MIT associate professor of electrical engineering and computer science and CSAIL principal investigator. Their research was supported, in part, by Open Philanthropy, the MIT Quest for Intelligence, the National Science Foundation, the Clare Boothe Luce Program for Women in STEM, and a Sloan Research Fellowship.&lt;/p&gt;&lt;p&gt;The researchers presented their research at the International Conference on Machine Learning (ICML) this week.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/unique-mathematical-shortcuts-language-models-use-to-predict-dynamic-scenarios-0721</guid><pubDate>Mon, 21 Jul 2025 12:00:00 +0000</pubDate></item><item><title>[NEW] The Download: how your data is being used to train AI, and why chatbots aren’t doctors (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/07/21/1120525/the-download-how-your-data-is-being-used-to-train-ai-and-why-chatbots-arent-doctors/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;A major AI training data set contains millions of examples of personal data&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Millions of images of passports, credit cards, birth certificates, and other documents containing personally identifiable information are likely included in one of the biggest open-source AI training sets, new research has found.&lt;/p&gt;&lt;p&gt;Thousands of images—including identifiable faces—were found in a small subset of DataComp CommonPool, a major AI training set for image generation scraped from the web. Because the researchers audited just 0.1% of CommonPool’s data, they estimate that the real number of images containing personally identifiable information, including faces and identity documents, is in the hundreds of millions.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The bottom line? Anything you put online can be and probably has been scraped.&lt;strong&gt; &lt;/strong&gt;Read the full story.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;—Eileen Guo&lt;/em&gt;&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;AI companies have stopped warning you that their chatbots aren’t doctors&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;AI companies have now mostly abandoned the once-standard practice of including medical disclaimers and warnings in response to health questions, new research has found. In fact, many leading AI models will now not only answer health questions but even ask follow-ups and attempt a diagnosis.&lt;/p&gt;&lt;p&gt;Such disclaimers serve an important reminder to people asking AI about everything from eating disorders to cancer diagnoses, the authors say, and their absence means that users of AI are more likely to trust unsafe medical advice. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—James O’Donnell&lt;/em&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Hackers exploited a flaw in Microsoft’s software to attack government agencies&lt;/strong&gt;&lt;br /&gt;Engineers across the world are racing to mitigate the risk it poses. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;The attack hones in on servers housed within an organization, not the cloud. &lt;/em&gt;(WP $)&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 The French government has launched a criminal probe into X&lt;/strong&gt;&lt;br /&gt;It’s investigating the company’s recommendation algorithm—but X isn’t cooperating. (FT $)&lt;br /&gt;&lt;em&gt;+ X says French lawmaker Eric Bothorel has accused it of manipulating its algorithm for foreign interference purposes. &lt;/em&gt;(Reuters)&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 Trump aides explored ending contracts with SpaceX&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;But they quickly found most of them are vital to the Defense Department and NASA. (WSJ $)&lt;br /&gt;+ &lt;em&gt;But that doesn’t mean it’s smooth sailing for SpaceX right now. &lt;/em&gt;(NY Mag $)&lt;br /&gt;+ &lt;em&gt;Rivals are rising to challenge the dominance of SpaceX. &lt;/em&gt;(MIT Technology Review)&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;4 Meta has refused to sign the EU’s AI code of practice&lt;/strong&gt;&lt;br /&gt;Its new global affairs chief claims the rules with throttle growth. (CNBC)&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;+ &lt;em&gt;The code is voluntary—but declining to sign it sends a clear message. &lt;/em&gt;(Bloomberg $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 A Polish programmer beat an OpenAI model in a coding competition&lt;/strong&gt;&lt;br /&gt;But only narrowly. (Ars Technica)&lt;br /&gt;+ &lt;em&gt;The second wave of AI coding is here. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 Nigeria has dreams of becoming a major digital worker hub&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;The rise of AI means there’s less outsourcing work to go round. (Rest of World)&lt;br /&gt;+ &lt;em&gt;What Africa needs to do to become a major AI player. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Microsoft is building a digital twin of the Notre-Dame Cathedral&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;The replica can help support its ongoing maintenance, apparently. (Reuters)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;8 How funny is AI, really?&lt;/strong&gt;&lt;br /&gt;Not all senses of humor are made equal. (Undark)&lt;br /&gt;+ &lt;em&gt;What happened when 20 comedians got AI to write their routines. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 What it’s like to forge a friendship with an AI&lt;/strong&gt;&lt;br /&gt;Student MJ Cocking found the experience incredibly helpful. (NYT $)&lt;br /&gt;+ &lt;em&gt;But chatbots can also fuel vulnerable people’s dangerous delusions. &lt;/em&gt;(WSJ $)&lt;br /&gt;+ &lt;em&gt;The AI relationship revolution is already here. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 Work has begun on the first space-based gravitational wave detector&lt;/strong&gt;&lt;br /&gt;The waves are triggered when massive objects like black holes collide. (IEEE Spectrum)&lt;br /&gt;+ &lt;em&gt;How the Rubin Observatory will help us understand dark matter and dark energy. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;"There was just no way I was going to make it through four years of this."&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;—Egan Reich, a former worker in the US Department of Labor, explains why he accepted the agency's second deferred resignation offer in April after DOGE’s rollout, Insider reports.&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" src="https://wp.technologyreview.com/wp-content/uploads/2022/09/AuthWeb3a.jpeg?fit=1064,598" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;The world is moving closer to a new cold war fought with authoritarian tech&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;A cold war is brewing between the world’s autocracies and democracies—and technology is fueling it.&lt;/p&gt;&lt;p&gt;Authoritarian states are following China’s lead and are trending toward more digital rights abuses by increasing the mass digital surveillance of citizens, censorship, and controls on individual expression.&lt;/p&gt;&lt;p&gt;And while democracies also use massive amounts of surveillance technology, it’s the tech trade relationships between authoritarian countries that’s enabling the rise of digitally enabled social control. Read the full story.&lt;/p&gt;&lt;p&gt;&lt;em&gt;—Tate Ryan-Mosley&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;+ I need to sign up for Minneapolis’ annual cat tour immediately.&lt;br /&gt;+ What are the odds? This mother has had four babies, all born on July 7 in different years.&lt;br /&gt;+ Not content with being a rap legend, Snoop Dogg has become a co-owner of a Welsh soccer club.&lt;br /&gt;+ Appetite for Destruction, Guns n’ Roses’ outrageous debut album, was released on this day 38 years ago.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;A major AI training data set contains millions of examples of personal data&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Millions of images of passports, credit cards, birth certificates, and other documents containing personally identifiable information are likely included in one of the biggest open-source AI training sets, new research has found.&lt;/p&gt;&lt;p&gt;Thousands of images—including identifiable faces—were found in a small subset of DataComp CommonPool, a major AI training set for image generation scraped from the web. Because the researchers audited just 0.1% of CommonPool’s data, they estimate that the real number of images containing personally identifiable information, including faces and identity documents, is in the hundreds of millions.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The bottom line? Anything you put online can be and probably has been scraped.&lt;strong&gt; &lt;/strong&gt;Read the full story.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;—Eileen Guo&lt;/em&gt;&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;AI companies have stopped warning you that their chatbots aren’t doctors&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;AI companies have now mostly abandoned the once-standard practice of including medical disclaimers and warnings in response to health questions, new research has found. In fact, many leading AI models will now not only answer health questions but even ask follow-ups and attempt a diagnosis.&lt;/p&gt;&lt;p&gt;Such disclaimers serve an important reminder to people asking AI about everything from eating disorders to cancer diagnoses, the authors say, and their absence means that users of AI are more likely to trust unsafe medical advice. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—James O’Donnell&lt;/em&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Hackers exploited a flaw in Microsoft’s software to attack government agencies&lt;/strong&gt;&lt;br /&gt;Engineers across the world are racing to mitigate the risk it poses. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;The attack hones in on servers housed within an organization, not the cloud. &lt;/em&gt;(WP $)&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 The French government has launched a criminal probe into X&lt;/strong&gt;&lt;br /&gt;It’s investigating the company’s recommendation algorithm—but X isn’t cooperating. (FT $)&lt;br /&gt;&lt;em&gt;+ X says French lawmaker Eric Bothorel has accused it of manipulating its algorithm for foreign interference purposes. &lt;/em&gt;(Reuters)&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 Trump aides explored ending contracts with SpaceX&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;But they quickly found most of them are vital to the Defense Department and NASA. (WSJ $)&lt;br /&gt;+ &lt;em&gt;But that doesn’t mean it’s smooth sailing for SpaceX right now. &lt;/em&gt;(NY Mag $)&lt;br /&gt;+ &lt;em&gt;Rivals are rising to challenge the dominance of SpaceX. &lt;/em&gt;(MIT Technology Review)&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;4 Meta has refused to sign the EU’s AI code of practice&lt;/strong&gt;&lt;br /&gt;Its new global affairs chief claims the rules with throttle growth. (CNBC)&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;+ &lt;em&gt;The code is voluntary—but declining to sign it sends a clear message. &lt;/em&gt;(Bloomberg $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 A Polish programmer beat an OpenAI model in a coding competition&lt;/strong&gt;&lt;br /&gt;But only narrowly. (Ars Technica)&lt;br /&gt;+ &lt;em&gt;The second wave of AI coding is here. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 Nigeria has dreams of becoming a major digital worker hub&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;The rise of AI means there’s less outsourcing work to go round. (Rest of World)&lt;br /&gt;+ &lt;em&gt;What Africa needs to do to become a major AI player. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Microsoft is building a digital twin of the Notre-Dame Cathedral&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;The replica can help support its ongoing maintenance, apparently. (Reuters)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;8 How funny is AI, really?&lt;/strong&gt;&lt;br /&gt;Not all senses of humor are made equal. (Undark)&lt;br /&gt;+ &lt;em&gt;What happened when 20 comedians got AI to write their routines. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 What it’s like to forge a friendship with an AI&lt;/strong&gt;&lt;br /&gt;Student MJ Cocking found the experience incredibly helpful. (NYT $)&lt;br /&gt;+ &lt;em&gt;But chatbots can also fuel vulnerable people’s dangerous delusions. &lt;/em&gt;(WSJ $)&lt;br /&gt;+ &lt;em&gt;The AI relationship revolution is already here. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 Work has begun on the first space-based gravitational wave detector&lt;/strong&gt;&lt;br /&gt;The waves are triggered when massive objects like black holes collide. (IEEE Spectrum)&lt;br /&gt;+ &lt;em&gt;How the Rubin Observatory will help us understand dark matter and dark energy. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;"There was just no way I was going to make it through four years of this."&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;—Egan Reich, a former worker in the US Department of Labor, explains why he accepted the agency's second deferred resignation offer in April after DOGE’s rollout, Insider reports.&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" src="https://wp.technologyreview.com/wp-content/uploads/2022/09/AuthWeb3a.jpeg?fit=1064,598" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;The world is moving closer to a new cold war fought with authoritarian tech&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;A cold war is brewing between the world’s autocracies and democracies—and technology is fueling it.&lt;/p&gt;&lt;p&gt;Authoritarian states are following China’s lead and are trending toward more digital rights abuses by increasing the mass digital surveillance of citizens, censorship, and controls on individual expression.&lt;/p&gt;&lt;p&gt;And while democracies also use massive amounts of surveillance technology, it’s the tech trade relationships between authoritarian countries that’s enabling the rise of digitally enabled social control. Read the full story.&lt;/p&gt;&lt;p&gt;&lt;em&gt;—Tate Ryan-Mosley&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;+ I need to sign up for Minneapolis’ annual cat tour immediately.&lt;br /&gt;+ What are the odds? This mother has had four babies, all born on July 7 in different years.&lt;br /&gt;+ Not content with being a rap legend, Snoop Dogg has become a co-owner of a Welsh soccer club.&lt;br /&gt;+ Appetite for Destruction, Guns n’ Roses’ outrageous debut album, was released on this day 38 years ago.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/07/21/1120525/the-download-how-your-data-is-being-used-to-train-ai-and-why-chatbots-arent-doctors/</guid><pubDate>Mon, 21 Jul 2025 12:10:00 +0000</pubDate></item></channel></rss>