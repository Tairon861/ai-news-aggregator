<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 04 Feb 2026 13:05:44 +0000</lastBuildDate><item><title>[NEW] Exclusive: Positron raises $230M Series B to take on Nvidia’s AI chips (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/04/exclusive-positron-raises-230m-series-b-to-take-on-nvidias-ai-chips/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/positron.jpg?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Semiconductor startup Positron has secured $230 million in Series B funding, TechCrunch has exclusively learned. The outfit plans to use the capital to speed up deployment of its high-speed memory chips, a critical component for the chips used for AI workloads, sources familiar with the matter told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Investors in the round include Qatar Investment Authority (QIA), the country’s sovereign wealth fund, which has been increasingly focused on building out AI infrastructure, the sources said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Reno-based startup’s Series B comes as hyperscalers and AI firms push to reduce their reliance on longstanding leader Nvidia. These firms include OpenAI, which, despite being one of Nvidia’s largest and most important customers, is reportedly unsatisfied with some of the firm’s latest AI chips and has been seeking alternatives since last year.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, Qatar, through QIA, has been accelerating a broader push into so-called “sovereign” AI infrastructure – a priority repeatedly underscored at Web Summit Qatar in Doha this week. Several sources told TechCrunch the country views compute capacity as critical to staying competitive on the global economic stage, and is positioning itself as a leading AI services hub in the Middle East, fueling interest in startups like Positron. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The strategy is already taking shape through major commitments, including a $20 billion AI infrastructure joint venture with Brookfield Asset Management that was announced in September.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Positron’s fundraise brings the three-year-old startup’s total capital raised to just over $300 million. The startup previously raised $75 million last year from investors including Valor Equity Partners, Atreides Management, DFJ Growth, Flume Ventures and Resilience Reserve.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company claims its first-generation chip, Atlas, manufactured in Arizona, can match the performance of Nvidia’s H100 GPUs for less than a third of the power. Positron is focused on inference – computing needed to run AI models for real-world applications – rather than training large language models, positioning the company as demand surges for inference hardware as businesses increasingly shift focus from building large models to deploying them at scale.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Sources tell TechCrunch that beyond its memory capabilities, Positron’s chips also perform strongly in high-frequency and video-processing workloads.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to Positron for more information.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/positron.jpg?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Semiconductor startup Positron has secured $230 million in Series B funding, TechCrunch has exclusively learned. The outfit plans to use the capital to speed up deployment of its high-speed memory chips, a critical component for the chips used for AI workloads, sources familiar with the matter told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Investors in the round include Qatar Investment Authority (QIA), the country’s sovereign wealth fund, which has been increasingly focused on building out AI infrastructure, the sources said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Reno-based startup’s Series B comes as hyperscalers and AI firms push to reduce their reliance on longstanding leader Nvidia. These firms include OpenAI, which, despite being one of Nvidia’s largest and most important customers, is reportedly unsatisfied with some of the firm’s latest AI chips and has been seeking alternatives since last year.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, Qatar, through QIA, has been accelerating a broader push into so-called “sovereign” AI infrastructure – a priority repeatedly underscored at Web Summit Qatar in Doha this week. Several sources told TechCrunch the country views compute capacity as critical to staying competitive on the global economic stage, and is positioning itself as a leading AI services hub in the Middle East, fueling interest in startups like Positron. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The strategy is already taking shape through major commitments, including a $20 billion AI infrastructure joint venture with Brookfield Asset Management that was announced in September.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Positron’s fundraise brings the three-year-old startup’s total capital raised to just over $300 million. The startup previously raised $75 million last year from investors including Valor Equity Partners, Atreides Management, DFJ Growth, Flume Ventures and Resilience Reserve.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company claims its first-generation chip, Atlas, manufactured in Arizona, can match the performance of Nvidia’s H100 GPUs for less than a third of the power. Positron is focused on inference – computing needed to run AI models for real-world applications – rather than training large language models, positioning the company as demand surges for inference hardware as businesses increasingly shift focus from building large models to deploying them at scale.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Sources tell TechCrunch that beyond its memory capabilities, Positron’s chips also perform strongly in high-frequency and video-processing workloads.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to Positron for more information.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/04/exclusive-positron-raises-230m-series-b-to-take-on-nvidias-ai-chips/</guid><pubDate>Wed, 04 Feb 2026 08:18:51 +0000</pubDate></item><item><title>[NEW] How Cisco builds smart systems for the AI era (AI News)</title><link>https://www.artificialintelligence-news.com/news/how-cisco-builds-smart-systems-for-the-ai-era/</link><description>&lt;p&gt; Among the big players in technology, Cisco is one of the sector’s leaders that’s advancing operational deployments of AI internally to its own operations, and the tools it sells to its customers around the world. As a large company, its activities encompass many areas of the typical IT stack, including infrastructure, services, security, and the design of entire enterprise-scale networks.&lt;/p&gt;&lt;p&gt; Cisco’s internal teams use a blend of machine learning and agentic AI to help them improve their own service delivery and personalise user experiences for its customers. It’s built a shared AI fabric built on patterns of compute and networking that are the product of years spent checking and validating its systems – battle-hardened solutions it then has the confidence to offer to customers. The infrastructure in play relies on high-performance GPUs, of course, but it’s not just raw horse-power. The detail is in the careful integration between compute and network stacks used in model training and the quite different demands from the ongoing load of inference.&lt;/p&gt;&lt;p&gt; Having made its name as the &lt;i&gt;de facto&lt;/i&gt; supplier of networking infrastructure for the enterprise, it comes as no shock that it’s in network automation that some of its better-known uses of AI finds their place. Automated configuration workflows and identity management combine into access solutions that are focused on rapid network deployments generated by natural language.&lt;/p&gt;&lt;p&gt; For organisations looking to develop into the next generation of AI users, Cisco has been rolling out hardware and orchestration tools that are aimed explicitly to support AI workloads. A recent collaboration with chip giant NVIDIA led to the emergence of a new line of switches and the Nexus Hyperfabric line of AI network controllers. These aim to simplify the deployment of the complex clusters needed for top-end, high-performance artificial intelligence clusters.&lt;/p&gt;&lt;p&gt; Cisco’s Secure AI Factory framework with partners like NVIDIA and Run:ai is aimed at production-grade AI pipelines. It uses distributed orchestration, GPU utilisation governance, Kubernetes microservice optimisation, and storage, under the umbrella product description Intersight. For more local deployments, Cisco Unified Edge brings all the necessary elements – compute, networking, security, and storage – close to where data gets generated and processed.&lt;/p&gt;&lt;p&gt; In environments where latency metrics are critically important, AI processing at the edge is the answer. But Cisco’s approach is not necessarily to offer dedicated IIoT-specific solutions. Instead, it tries to extend the operational models typically found in a data centre and applies the same technology (if not the same exact methodology) to edge sites. It’s like data centre-grade security policies and configurations available to remote installations. Having the same precepts and standards in cloud and edge mean that Cisco accredited engineers can manage and maintain data centres or small edge deployments using the same skills, accreditation, knowledge, and experience.&lt;/p&gt;&lt;p&gt; Security and risk management figure prominently in the Cisco AI narrative. Its Integrated AI Security and Safety Framework applies high standards of safety and security throughout the life-cycle of AI systems. It considers adversarial threats, supply chain weakness, the risk profiles of multi-agent interactions, and multi-modal vulnerabilities as issues that have to be addressed regardless of the nature or size of any deployment.&lt;/p&gt;&lt;p&gt; Cisco’s work on operational AI also reflects broader ecosystem conversations. The company markets products for organisations wanting to make the transition from generative to agentic AI, where autonomous software agents carry out operational tasks. In most cases, this requires new tooling and new operational protocols.&lt;/p&gt;&lt;p&gt; Cisco’s future AI plans include continuing its central work in infrastructure provision for AI workloads. It’s also pursuing broader adoption of AI-ready networks, including next-gen wireless and unified management systems that will control systems across campus, branch, and cloud environments. The company is also expanding its software and platform investments, including its most recent acquisition (NeuralFabric), to help it build a more comprehensive software stack and product portfolio.&lt;/p&gt;&lt;p&gt; In summary, Cisco’s AI deployment strategy combines hardware, software, and service elements that embed AI into operations, giving organisations a route to production-grade systems. Its work can be found in large-scale infrastructure, systems for unified management, risk mitigation, and anywhere that connects distributed, cloud, and edge computing.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: Pixabay)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt; Among the big players in technology, Cisco is one of the sector’s leaders that’s advancing operational deployments of AI internally to its own operations, and the tools it sells to its customers around the world. As a large company, its activities encompass many areas of the typical IT stack, including infrastructure, services, security, and the design of entire enterprise-scale networks.&lt;/p&gt;&lt;p&gt; Cisco’s internal teams use a blend of machine learning and agentic AI to help them improve their own service delivery and personalise user experiences for its customers. It’s built a shared AI fabric built on patterns of compute and networking that are the product of years spent checking and validating its systems – battle-hardened solutions it then has the confidence to offer to customers. The infrastructure in play relies on high-performance GPUs, of course, but it’s not just raw horse-power. The detail is in the careful integration between compute and network stacks used in model training and the quite different demands from the ongoing load of inference.&lt;/p&gt;&lt;p&gt; Having made its name as the &lt;i&gt;de facto&lt;/i&gt; supplier of networking infrastructure for the enterprise, it comes as no shock that it’s in network automation that some of its better-known uses of AI finds their place. Automated configuration workflows and identity management combine into access solutions that are focused on rapid network deployments generated by natural language.&lt;/p&gt;&lt;p&gt; For organisations looking to develop into the next generation of AI users, Cisco has been rolling out hardware and orchestration tools that are aimed explicitly to support AI workloads. A recent collaboration with chip giant NVIDIA led to the emergence of a new line of switches and the Nexus Hyperfabric line of AI network controllers. These aim to simplify the deployment of the complex clusters needed for top-end, high-performance artificial intelligence clusters.&lt;/p&gt;&lt;p&gt; Cisco’s Secure AI Factory framework with partners like NVIDIA and Run:ai is aimed at production-grade AI pipelines. It uses distributed orchestration, GPU utilisation governance, Kubernetes microservice optimisation, and storage, under the umbrella product description Intersight. For more local deployments, Cisco Unified Edge brings all the necessary elements – compute, networking, security, and storage – close to where data gets generated and processed.&lt;/p&gt;&lt;p&gt; In environments where latency metrics are critically important, AI processing at the edge is the answer. But Cisco’s approach is not necessarily to offer dedicated IIoT-specific solutions. Instead, it tries to extend the operational models typically found in a data centre and applies the same technology (if not the same exact methodology) to edge sites. It’s like data centre-grade security policies and configurations available to remote installations. Having the same precepts and standards in cloud and edge mean that Cisco accredited engineers can manage and maintain data centres or small edge deployments using the same skills, accreditation, knowledge, and experience.&lt;/p&gt;&lt;p&gt; Security and risk management figure prominently in the Cisco AI narrative. Its Integrated AI Security and Safety Framework applies high standards of safety and security throughout the life-cycle of AI systems. It considers adversarial threats, supply chain weakness, the risk profiles of multi-agent interactions, and multi-modal vulnerabilities as issues that have to be addressed regardless of the nature or size of any deployment.&lt;/p&gt;&lt;p&gt; Cisco’s work on operational AI also reflects broader ecosystem conversations. The company markets products for organisations wanting to make the transition from generative to agentic AI, where autonomous software agents carry out operational tasks. In most cases, this requires new tooling and new operational protocols.&lt;/p&gt;&lt;p&gt; Cisco’s future AI plans include continuing its central work in infrastructure provision for AI workloads. It’s also pursuing broader adoption of AI-ready networks, including next-gen wireless and unified management systems that will control systems across campus, branch, and cloud environments. The company is also expanding its software and platform investments, including its most recent acquisition (NeuralFabric), to help it build a more comprehensive software stack and product portfolio.&lt;/p&gt;&lt;p&gt; In summary, Cisco’s AI deployment strategy combines hardware, software, and service elements that embed AI into operations, giving organisations a route to production-grade systems. Its work can be found in large-scale infrastructure, systems for unified management, risk mitigation, and anywhere that connects distributed, cloud, and edge computing.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: Pixabay)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/how-cisco-builds-smart-systems-for-the-ai-era/</guid><pubDate>Wed, 04 Feb 2026 10:00:00 +0000</pubDate></item><item><title>[NEW] Combing the Rackspace blogfiles for operational AI pointers (AI News)</title><link>https://www.artificialintelligence-news.com/news/combing-the-rackspace-blogfiles-for-operational-ai-pointers/</link><description>&lt;p&gt; In a recent blog output, Rackspace refers to the bottlenecks familiar to many readers: messy data, unclear ownership, governance gaps, and the cost of running models once they become part of production. The company frames them through the lens of service delivery, security operations, and cloud modernisation, which tells you where it is putting its own effort.&lt;/p&gt;&lt;p&gt; One of the clearest examples of operational AI inside Rackspace sits in its security business. In late January, the company described RAIDER (Rackspace Advanced Intelligence, Detection and Event Research) as a custom back-end platform built for its internal cyber defense centre. With security teams working amid many alerts and logs, standard detection engineering doesn’t scale if dependent on the manual writing of security rules. Rackspace says its RAIDER system unifies threat intelligence with detection engineering workflows and uses its AI Security Engine (RAISE) and LLMs to automate detection rule creation, generating detection criteria  it describes as “platform-ready” in line with known frameworks such as MITRE ATT&amp;amp;CK. The company claims it’s cut detection development time by more than half and reduced mean time to detect and respond. This is just the kind of internal process change that matters.&lt;/p&gt;&lt;p&gt; The company also positions agentic AI as a way of taking the friction out of complex engineering programmes. A January post on modernising VMware environments on AWS describes a model in which AI agents handle data-intensive analysis and many repeating tasks, yet it keeps “architectural judgement, governance and business decisions” remain in the human domain. Rackspace presents this workflow as stopping senior engineers being sidelined into migration projects. The article states the target is to keep day two operations in scope –  where many migration plans fail as teams discover they have modernised infrastructure but not operating practices.&lt;/p&gt;&lt;p&gt; Elsewhere the company sets out a picture of AI-supported operations where monitoring becomes more predictive, routine incidents are handled by bots and automation scripts, and telemetry (plus historical data) are used to spot patterns and, it turn, recommend fixes. This is conventional AIOps language, but it Rackspace is tying such language to managed services delivery, suggesting the company uses AI to reduce the cost of labour in operational pipelines in addition to the more familiar use of AI in customer-facing environments.&lt;/p&gt;&lt;p&gt; In a post describing AI-enabled operations, the company stresses the importance of focus strategy, governance and operating models. It specifies the machinery it needed to industrialise AI, such as choosing infrastructure based on whether workloads involve training, fine-tuning or inference. Many tasks are relatively lightweight and can run inference locally on existing hardware.&lt;/p&gt;&lt;p&gt; The company’s noted four recurring barriers to AI adoption, most notably that of fragmented and inconsistent data, and it recommends investment in integration and data management so models have consistent foundations. This is not an opinion unique to Rackspace, of course, but having it writ large by a technology-first, big player is illustrative of the issues faced by many enterprise-scale AI deployments.&lt;/p&gt;&lt;p&gt; A company of even greater size, Microsoft, is working to coordinate autonomous agents’ work across systems. Copilot has evolved into an orchestration layer, and in Microsoft’s ecosystem, multi-step task execution and broader model choice do exist. However, it’s noteworthy that Redmond is called out by Rackspace on the fact that productivity gains only arrive when identity, data access, and oversight are firmly ensconced into operations.&lt;/p&gt;&lt;p&gt; Rackspace’s near-term AI plan comprises of AI-assisted security engineering, agent-supported modernisation, and AI-augmented service management. Its future plans can perhaps be discerned in a January article published on the company’s blog that concerns private cloud AI trends. In it, the author  argues inference economics and governance will drive architecture decisions well into 2026. It anticipates ‘bursty’ exploration in public clouds, while moving inference tasks into private clouds on the grounds of cost stability, and compliance. That’s a roadmap for operational AI grounded in budget and audit requirements, not novelty.&lt;/p&gt;&lt;p&gt; For decision-makers trying to accelerate their own deployments, the useful takeaway is  that Rackspace has treats AI as an operational discipline. The concrete, published examples it gives are those that reduce cycle time in repeatable work. Readers may accept the company’s direction and still be wary of the company’s claimed metrics. The steps to take inside a growing business are to discover repeating processes, examine where strict oversight is necessary because of data  governance, and where inference costs might be reduced by bringing some processing in-house.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: Pixabay)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt; In a recent blog output, Rackspace refers to the bottlenecks familiar to many readers: messy data, unclear ownership, governance gaps, and the cost of running models once they become part of production. The company frames them through the lens of service delivery, security operations, and cloud modernisation, which tells you where it is putting its own effort.&lt;/p&gt;&lt;p&gt; One of the clearest examples of operational AI inside Rackspace sits in its security business. In late January, the company described RAIDER (Rackspace Advanced Intelligence, Detection and Event Research) as a custom back-end platform built for its internal cyber defense centre. With security teams working amid many alerts and logs, standard detection engineering doesn’t scale if dependent on the manual writing of security rules. Rackspace says its RAIDER system unifies threat intelligence with detection engineering workflows and uses its AI Security Engine (RAISE) and LLMs to automate detection rule creation, generating detection criteria  it describes as “platform-ready” in line with known frameworks such as MITRE ATT&amp;amp;CK. The company claims it’s cut detection development time by more than half and reduced mean time to detect and respond. This is just the kind of internal process change that matters.&lt;/p&gt;&lt;p&gt; The company also positions agentic AI as a way of taking the friction out of complex engineering programmes. A January post on modernising VMware environments on AWS describes a model in which AI agents handle data-intensive analysis and many repeating tasks, yet it keeps “architectural judgement, governance and business decisions” remain in the human domain. Rackspace presents this workflow as stopping senior engineers being sidelined into migration projects. The article states the target is to keep day two operations in scope –  where many migration plans fail as teams discover they have modernised infrastructure but not operating practices.&lt;/p&gt;&lt;p&gt; Elsewhere the company sets out a picture of AI-supported operations where monitoring becomes more predictive, routine incidents are handled by bots and automation scripts, and telemetry (plus historical data) are used to spot patterns and, it turn, recommend fixes. This is conventional AIOps language, but it Rackspace is tying such language to managed services delivery, suggesting the company uses AI to reduce the cost of labour in operational pipelines in addition to the more familiar use of AI in customer-facing environments.&lt;/p&gt;&lt;p&gt; In a post describing AI-enabled operations, the company stresses the importance of focus strategy, governance and operating models. It specifies the machinery it needed to industrialise AI, such as choosing infrastructure based on whether workloads involve training, fine-tuning or inference. Many tasks are relatively lightweight and can run inference locally on existing hardware.&lt;/p&gt;&lt;p&gt; The company’s noted four recurring barriers to AI adoption, most notably that of fragmented and inconsistent data, and it recommends investment in integration and data management so models have consistent foundations. This is not an opinion unique to Rackspace, of course, but having it writ large by a technology-first, big player is illustrative of the issues faced by many enterprise-scale AI deployments.&lt;/p&gt;&lt;p&gt; A company of even greater size, Microsoft, is working to coordinate autonomous agents’ work across systems. Copilot has evolved into an orchestration layer, and in Microsoft’s ecosystem, multi-step task execution and broader model choice do exist. However, it’s noteworthy that Redmond is called out by Rackspace on the fact that productivity gains only arrive when identity, data access, and oversight are firmly ensconced into operations.&lt;/p&gt;&lt;p&gt; Rackspace’s near-term AI plan comprises of AI-assisted security engineering, agent-supported modernisation, and AI-augmented service management. Its future plans can perhaps be discerned in a January article published on the company’s blog that concerns private cloud AI trends. In it, the author  argues inference economics and governance will drive architecture decisions well into 2026. It anticipates ‘bursty’ exploration in public clouds, while moving inference tasks into private clouds on the grounds of cost stability, and compliance. That’s a roadmap for operational AI grounded in budget and audit requirements, not novelty.&lt;/p&gt;&lt;p&gt; For decision-makers trying to accelerate their own deployments, the useful takeaway is  that Rackspace has treats AI as an operational discipline. The concrete, published examples it gives are those that reduce cycle time in repeatable work. Readers may accept the company’s direction and still be wary of the company’s claimed metrics. The steps to take inside a growing business are to discover repeating processes, examine where strict oversight is necessary because of data  governance, and where inference costs might be reduced by bringing some processing in-house.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: Pixabay)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/combing-the-rackspace-blogfiles-for-operational-ai-pointers/</guid><pubDate>Wed, 04 Feb 2026 10:01:00 +0000</pubDate></item><item><title>[NEW] Accel doubles down on Fibr AI as agents turn static websites into one-to-one experiences (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/04/accel-doubles-down-on-fibr-ai-as-agents-turn-static-websites-into-one-to-one-experiences/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;While advertising and targeting have become increasingly personalized, the website — the final destination for that traffic — has remained largely static. Fibr AI aims to bridge that gap by using AI agents to turn generic webpages into one-to-one experiences tailored to each visitor, a thesis that has prompted Accel to double down on the company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Accel has led Fibr AI’s $5.7 million seed round following an earlier $1.8 million pre-seed investment in 2024. The fresh funding also included participation from WillowTree Ventures and MVP Ventures, alongside Fortune 100 operators joining as angel investors and advisors, bringing the startup’s total funding to $7.5 million.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For large companies, the gap between increasingly personalized ads and largely generic website experiences has traditionally been filled by a mix of personalization software, engineering teams, and marketing agencies — a model that is slow, expensive, and difficult to scale. While ads can be tailored instantly for different audiences, changing what happens once a visitor lands on a site often requires weeks of coordination and limits teams to running only a handful of experiments each year. Fibr AI argues that this human-heavy operating model no longer works. Instead, the startup uses autonomous AI agents to infer intent, generate variations, and continuously optimize pages in real time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fibr AI replaces the agency- and engineering-heavy model with autonomous systems that operate continuously, Ankur Goyal (pictured above, right), the co-founder and chief executive, said in an interview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are [the] software, and the agency is the workforce of agents we are deploying,” Goyal told TechCrunch, adding that this allows Fibr AI to run thousands of experiments in parallel rather than a few dozen each year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Adoption was initially slow. Founded in early 2023 by Goyal and Pritam Roy (pictured above, left), Fibr AI had just one or two customers for much of its first two years as enterprises took time to evaluate the approach. That began to change last year, Goyal said, with adoption picking up among large U.S. companies, including banks and healthcare providers, bringing the total number of customers to 12.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are an infra afterthought layer,” Goyal told TechCrunch. “Once it’s set up, nobody wants to think about it again.” That dynamic, he added, has led Fibr AI to sign three- to five-year contracts with large enterprises, which tend to treat website infrastructure as something to standardize rather than continuously revisit.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;At a technical level, Fibr AI operates as a layer on top of an existing website, connecting to a company’s advertising, analytics, and customer data systems to understand how visitors arrive and what they are likely looking for. Its AI agents then assemble and adjust page content, such as copy, imagery, and layout, treating each URL as a system that learns and optimizes continuously rather than a fixed page. Instead of relying on manually configured rules or sequential A/B tests, the platform runs large numbers of micro-experiments in parallel and updates experiences systematically as traffic flows in from different channels.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-3089322" height="1080" src="https://techcrunch.com/wp-content/uploads/2026/02/fibr-ai-personalization_b82c81.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Fibr AI personalizes webpages using AI agents&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Fibr AI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;That shift has direct cost implications for large enterprises. Traditional website personalization typically combines software licenses with agency retainers and engineering time, tying costs to people rather than outcomes. Goyal said enterprises are increasingly evaluating Fibr AI’s platform based on cost per experiment and conversion impact, rather than the number of tools or people involved.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Accel, that operating model — rather than the AI buzz — was central to the decision to invest again. “Advertising today is one-to-one, but when users land on a website it becomes one-to-many,” said Prayank Swaroop, a partner at Accel. “You can create hundreds of ads for different audiences, but they all still land on the same page.” Fibr’s ability to turn that dynamic into one-to-one personalization, he said, stood out because it removed the agency and engineering bottlenecks that typically limit how far enterprises can push experimentation.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Swaroop added that early enterprise adoption, particularly among banks and healthcare companies, helped validate the thesis. “These are regulated, conservative industries,” he said. “When they start saying, ‘We need this, and we’re willing to pay for it,’ that’s when we feel confident doubling down.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-future-proofing-for-the-agentic-commerce-era"&gt;Future-proofing for the agentic-commerce era&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;While most of Fibr AI’s business today is driven by personalizing experiences for human visitors, Accel and Fibr AI also see potential in how AI agents are beginning to mediate online discovery. As users increasingly research, compare, and shortlist products using large language models and AI chatbots, including OpenAI’s ChatGPT, before visiting a website, Swaroop said, the ability for sites to adapt based on what a visitor — or an AI system acting on their behalf — already knows could become more important over time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“That part is still early,” Swaroop said, “but the companies building for today’s needs while being ready for that shift tomorrow are the ones we want to back.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-3089266" height="984" src="https://techcrunch.com/wp-content/uploads/2026/02/fibr-ai-llm-optimization.jpg" width="1298" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Fibr AI’s dynamic experience for discovery through LLMs and AI chatbots&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Fibr AI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;With the new funding, Fibr AI plans to focus on expanding its sales and customer-facing teams in the U.S., while continuing to build out its technical base in India. The San Francisco-headquartered startup maintains an office in Bengaluru, with 17 of its roughly 23 employees based in India and the remaining six in the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Goyal said the startup targets about $5 million in annual recurring revenue by the end of this year and around 50 enterprise customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fibr AI is entering a space long dominated by incumbents such as Adobe and Optimizely, which offer experimentation and personalization tools to large enterprises. But both Goyal and Swaroop argued that those platforms are constrained by how they are built and sold, typically relying on marketing agencies and engineering teams to configure and operate them. That model, they said, makes it difficult to move quickly or scale experimentation, even as customer acquisition and messaging have become increasingly dynamic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Incumbents have been slow in bringing out products,” Swaroop said, adding that even when new features arrive, they often come years after demand has shifted.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;While advertising and targeting have become increasingly personalized, the website — the final destination for that traffic — has remained largely static. Fibr AI aims to bridge that gap by using AI agents to turn generic webpages into one-to-one experiences tailored to each visitor, a thesis that has prompted Accel to double down on the company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Accel has led Fibr AI’s $5.7 million seed round following an earlier $1.8 million pre-seed investment in 2024. The fresh funding also included participation from WillowTree Ventures and MVP Ventures, alongside Fortune 100 operators joining as angel investors and advisors, bringing the startup’s total funding to $7.5 million.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For large companies, the gap between increasingly personalized ads and largely generic website experiences has traditionally been filled by a mix of personalization software, engineering teams, and marketing agencies — a model that is slow, expensive, and difficult to scale. While ads can be tailored instantly for different audiences, changing what happens once a visitor lands on a site often requires weeks of coordination and limits teams to running only a handful of experiments each year. Fibr AI argues that this human-heavy operating model no longer works. Instead, the startup uses autonomous AI agents to infer intent, generate variations, and continuously optimize pages in real time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fibr AI replaces the agency- and engineering-heavy model with autonomous systems that operate continuously, Ankur Goyal (pictured above, right), the co-founder and chief executive, said in an interview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are [the] software, and the agency is the workforce of agents we are deploying,” Goyal told TechCrunch, adding that this allows Fibr AI to run thousands of experiments in parallel rather than a few dozen each year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Adoption was initially slow. Founded in early 2023 by Goyal and Pritam Roy (pictured above, left), Fibr AI had just one or two customers for much of its first two years as enterprises took time to evaluate the approach. That began to change last year, Goyal said, with adoption picking up among large U.S. companies, including banks and healthcare providers, bringing the total number of customers to 12.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are an infra afterthought layer,” Goyal told TechCrunch. “Once it’s set up, nobody wants to think about it again.” That dynamic, he added, has led Fibr AI to sign three- to five-year contracts with large enterprises, which tend to treat website infrastructure as something to standardize rather than continuously revisit.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;At a technical level, Fibr AI operates as a layer on top of an existing website, connecting to a company’s advertising, analytics, and customer data systems to understand how visitors arrive and what they are likely looking for. Its AI agents then assemble and adjust page content, such as copy, imagery, and layout, treating each URL as a system that learns and optimizes continuously rather than a fixed page. Instead of relying on manually configured rules or sequential A/B tests, the platform runs large numbers of micro-experiments in parallel and updates experiences systematically as traffic flows in from different channels.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-3089322" height="1080" src="https://techcrunch.com/wp-content/uploads/2026/02/fibr-ai-personalization_b82c81.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Fibr AI personalizes webpages using AI agents&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Fibr AI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;That shift has direct cost implications for large enterprises. Traditional website personalization typically combines software licenses with agency retainers and engineering time, tying costs to people rather than outcomes. Goyal said enterprises are increasingly evaluating Fibr AI’s platform based on cost per experiment and conversion impact, rather than the number of tools or people involved.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Accel, that operating model — rather than the AI buzz — was central to the decision to invest again. “Advertising today is one-to-one, but when users land on a website it becomes one-to-many,” said Prayank Swaroop, a partner at Accel. “You can create hundreds of ads for different audiences, but they all still land on the same page.” Fibr’s ability to turn that dynamic into one-to-one personalization, he said, stood out because it removed the agency and engineering bottlenecks that typically limit how far enterprises can push experimentation.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Swaroop added that early enterprise adoption, particularly among banks and healthcare companies, helped validate the thesis. “These are regulated, conservative industries,” he said. “When they start saying, ‘We need this, and we’re willing to pay for it,’ that’s when we feel confident doubling down.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-future-proofing-for-the-agentic-commerce-era"&gt;Future-proofing for the agentic-commerce era&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;While most of Fibr AI’s business today is driven by personalizing experiences for human visitors, Accel and Fibr AI also see potential in how AI agents are beginning to mediate online discovery. As users increasingly research, compare, and shortlist products using large language models and AI chatbots, including OpenAI’s ChatGPT, before visiting a website, Swaroop said, the ability for sites to adapt based on what a visitor — or an AI system acting on their behalf — already knows could become more important over time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“That part is still early,” Swaroop said, “but the companies building for today’s needs while being ready for that shift tomorrow are the ones we want to back.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-3089266" height="984" src="https://techcrunch.com/wp-content/uploads/2026/02/fibr-ai-llm-optimization.jpg" width="1298" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Fibr AI’s dynamic experience for discovery through LLMs and AI chatbots&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Fibr AI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;With the new funding, Fibr AI plans to focus on expanding its sales and customer-facing teams in the U.S., while continuing to build out its technical base in India. The San Francisco-headquartered startup maintains an office in Bengaluru, with 17 of its roughly 23 employees based in India and the remaining six in the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Goyal said the startup targets about $5 million in annual recurring revenue by the end of this year and around 50 enterprise customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fibr AI is entering a space long dominated by incumbents such as Adobe and Optimizely, which offer experimentation and personalization tools to large enterprises. But both Goyal and Swaroop argued that those platforms are constrained by how they are built and sold, typically relying on marketing agencies and engineering teams to configure and operate them. That model, they said, makes it difficult to move quickly or scale experimentation, even as customer acquisition and messaging have become increasingly dynamic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Incumbents have been slow in bringing out products,” Swaroop said, adding that even when new features arrive, they often come years after demand has shifted.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/04/accel-doubles-down-on-fibr-ai-as-agents-turn-static-websites-into-one-to-one-experiences/</guid><pubDate>Wed, 04 Feb 2026 13:00:00 +0000</pubDate></item></channel></rss>