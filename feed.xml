<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 23 Oct 2025 01:41:25 +0000</lastBuildDate><item><title>Several users reportedly complain to FTC that ChatGPT is causing psychological harm (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/22/several-users-reportedly-complain-to-ftc-that-chatgpt-is-causing-psychological-harm/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2195918462.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As AI companies claim their tech will one day grow to become a fundamental human right, and those backing them say slowing down AI development is akin to murder, the people using the tech are alleging that tools like ChatGPT sometimes can cause serious psychological harm.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At least seven people have complained to the U.S. Federal Trade Commission that ChatGPT caused them to experience severe delusions, paranoia, and emotional crises, Wired reported, citing public records of complaints mentioning ChatGPT since November 2022.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;One of the complainants claimed that talking to ChatGPT for long periods had led to delusions and a “real, unfolding spiritual and legal crisis” about people in their life. Another said during their conversations with ChatGPT, it started using “highly convincing emotional language” and that it simulated friendships and provided reflections that “became emotionally manipulative over time, especially without warning or protection.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One user alleged that ChatGPT had caused cognitive hallucinations by mimicking human trust-building mechanisms. When this user asked ChatGPT to confirm reality and cognitive stability, the chatbot said they weren’t hallucinating. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Im struggling,” another user wrote in their complaint to the FTC. “Pleas help me. Bc I feel very alone. Thank you.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Wired, several of the complainants wrote to the FTC because they couldn’t reach anyone at OpenAI. And most of the complaints urged the regulator to launch an investigation into the company and force it to add guardrails, the report said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These complaints come as investments in data centers and AI development soar to unprecedented levels. At the same time, debates are raging about whether the progress of the technology should be approached with caution to ensure it has safeguards built in. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;ChatGPT, and its maker OpenAI, itself has come under fire for allegedly playing a role in the suicide of a teenager.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In early October, we released a new GPT-5 default model in ChatGPT to more accurately detect and respond to potential signs of mental and emotional distress such as mania, delusion, psychosis, and de-escalate conversations in a supportive, grounding way,” OpenAI spokesperson Kate Waters said in an emailed statement. “We’ve also expanded access to professional help and hotlines, re-routed sensitive conversations to safer models, added nudges to take breaks during long sessions, and introduced parental controls to better protect teens. This work is deeply important and ongoing as we collaborate with mental health experts, clinicians, and policymakers around the world.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2195918462.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As AI companies claim their tech will one day grow to become a fundamental human right, and those backing them say slowing down AI development is akin to murder, the people using the tech are alleging that tools like ChatGPT sometimes can cause serious psychological harm.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At least seven people have complained to the U.S. Federal Trade Commission that ChatGPT caused them to experience severe delusions, paranoia, and emotional crises, Wired reported, citing public records of complaints mentioning ChatGPT since November 2022.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;One of the complainants claimed that talking to ChatGPT for long periods had led to delusions and a “real, unfolding spiritual and legal crisis” about people in their life. Another said during their conversations with ChatGPT, it started using “highly convincing emotional language” and that it simulated friendships and provided reflections that “became emotionally manipulative over time, especially without warning or protection.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One user alleged that ChatGPT had caused cognitive hallucinations by mimicking human trust-building mechanisms. When this user asked ChatGPT to confirm reality and cognitive stability, the chatbot said they weren’t hallucinating. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Im struggling,” another user wrote in their complaint to the FTC. “Pleas help me. Bc I feel very alone. Thank you.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Wired, several of the complainants wrote to the FTC because they couldn’t reach anyone at OpenAI. And most of the complaints urged the regulator to launch an investigation into the company and force it to add guardrails, the report said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These complaints come as investments in data centers and AI development soar to unprecedented levels. At the same time, debates are raging about whether the progress of the technology should be approached with caution to ensure it has safeguards built in. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;ChatGPT, and its maker OpenAI, itself has come under fire for allegedly playing a role in the suicide of a teenager.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In early October, we released a new GPT-5 default model in ChatGPT to more accurately detect and respond to potential signs of mental and emotional distress such as mania, delusion, psychosis, and de-escalate conversations in a supportive, grounding way,” OpenAI spokesperson Kate Waters said in an emailed statement. “We’ve also expanded access to professional help and hotlines, re-routed sensitive conversations to safer models, added nudges to take breaks during long sessions, and introduced parental controls to better protect teens. This work is deeply important and ongoing as we collaborate with mental health experts, clinicians, and policymakers around the world.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/22/several-users-reportedly-complain-to-ftc-that-chatgpt-is-causing-psychological-harm/</guid><pubDate>Wed, 22 Oct 2025 14:16:45 +0000</pubDate></item><item><title>Last-minute TechCrunch Disrupt 2025 deal: Save 60% on your plus-one’s pass before doors open on October 27 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/22/last-minute-techcrunch-disrupt-2025-deal-save-60-on-your-plus-ones-pass-before-doors-open-on-oct-27/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Only 5 days left until&amp;nbsp;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;&amp;nbsp;takes over Moscone West in San Francisco from October 27-29, bringing together&amp;nbsp;10,000+&amp;nbsp;leaders, VCs, operators, and innovators shaping the future of tech.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To celebrate the final countdown,&amp;nbsp;we’re&amp;nbsp;giving you one last chance to save big:&amp;nbsp;&lt;strong&gt;Buy your pass and save up to $444&lt;/strong&gt;&lt;strong&gt;, plus get 60% off a second ticket&lt;/strong&gt;&amp;nbsp;for your co-founder, partner, friend, or team member.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This offer at these low rates ends when doors open on October 27 —&amp;nbsp;&lt;strong&gt;lock in your two discounted passes now&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3016596" height="453" src="https://techcrunch.com/wp-content/uploads/2025/06/52457178574_dcedf0a9a3_o-1.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-disrupt-2025-ticket-types-made-for-your-growth-journey"&gt;Disrupt 2025 ticket types made for your growth journey&lt;/h2&gt;

&lt;h3 class="wp-block-heading" id="h-industry-nbsp-stage-nbsp-programming-5-nbsp-stages"&gt;Industry&amp;nbsp;stage&amp;nbsp;programming: 5&amp;nbsp;stages&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For&amp;nbsp;Attendee, Founder, Investor, Student, and&amp;nbsp;Non-Profit&amp;nbsp;Passes&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Five powerhouse stages delivering insights from the top minds in tech — including&amp;nbsp;Vinod Khosla,&amp;nbsp;Aaron Levie&amp;nbsp;(Box),&amp;nbsp;Elizabeth Stone&amp;nbsp;(Netflix),&amp;nbsp;Kevin Scott&amp;nbsp;(Microsoft), and more.&amp;nbsp;&lt;strong&gt;Explore the agenda and meet the speakers&lt;/strong&gt;&amp;nbsp;taking the stage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;AI Stage&lt;/strong&gt;&amp;nbsp;— Get immersed in the latest breakthroughs in AI across agents, hardware, creativity, and defense. Explore the science driving deep tech innovation and the ethical and societal impact behind it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Builders Stage&lt;/strong&gt;&amp;nbsp;— Learn founder-tested strategies and lessons from startup veterans ready to help you level up.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Disrupt Stage&lt;/strong&gt;&amp;nbsp;— Watch the intense&amp;nbsp;startup showdown,&amp;nbsp;&lt;strong&gt;Startup Battlefield 200&lt;/strong&gt;,&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;live&amp;nbsp;as 20 world-class startups pitch top investors. Get real insights from leaders shaping the future of entrepreneurship.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Going Public Stage&lt;/strong&gt;&amp;nbsp;— From day one to IPO, learn the frameworks and founder stories that guide companies to scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Space Stage&lt;/strong&gt;&amp;nbsp;— Discover how emerging companies are redefining the space industry and unlocking a new era of orbital exploration.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Box CEO Aaron Levie on stage at TechCrunch Disrupt in San Francisco in 2019." class="wp-image-2833981" height="454" src="https://techcrunch.com/wp-content/uploads/2024/08/GettyImages-1178603809.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Steve Jennings / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-interactive-roundtable-sessions"&gt;Interactive roundtable sessions&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For Attendee, Founder, Investor, Student, and&amp;nbsp;Non-Profit&amp;nbsp;Passes&lt;/em&gt;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Small-group&amp;nbsp;sessions where you can dive deep, share experiences, and ask the&amp;nbsp;hard questions&amp;nbsp;directly to industry experts and other attendees across different sectors in tech.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-q-amp-a-breakout-nbsp-sessions"&gt;Q&amp;amp;A breakout&amp;nbsp;sessions&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For all pass types&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Get answers from experts covering today’s most pressing topics. These&amp;nbsp;fast-paced, high-value sessions&amp;nbsp;near the Expo Hall are built for founders and operators who want actionable takeaways.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2024 Breakout Session" class="wp-image-2985187" height="454" src="https://techcrunch.com/wp-content/uploads/2025/03/Disrupt-2024-Breakout.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-expo-hall"&gt;Expo Hall&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For all pass types&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meet 300+ startups ready to pitch and demo their innovations. This is where 10,000+ tech leaders and investors come to discover&amp;nbsp;what’s&amp;nbsp;next.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-networking-that-moves-you-nbsp-forward"&gt;Networking that moves you&amp;nbsp;forward&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For all pass types&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Use Braindate to schedule curated 1:1 or small group meetings in the Networking Lounge. Build partnerships, spark&amp;nbsp;new ideas, and uncover your next big opportunity. And&amp;nbsp;don’t&amp;nbsp;forget — the entire venue is a hub for connections: Dive into collaborative roundtables, bump into innovators in the Expo Hall, or close deals over coffee in the exclusive founder-investor Deal Flow Cafe.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-disrupt-2025-is-days-away-don-t-wait"&gt;Disrupt 2025 is days away — don’t wait&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Grab your pass and save up to $444&lt;/strong&gt;,&amp;nbsp;and&amp;nbsp;&lt;strong&gt;get your plus-one at 60% off&lt;/strong&gt;&amp;nbsp;before prices increase when Disrupt kicks off. Bring your team, co-founder, or next collaborator and&amp;nbsp;&lt;strong&gt;experience Disrupt together&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt AI Stage" class="wp-image-3048038" height="454" src="https://techcrunch.com/wp-content/uploads/2025/09/Disrupt-2025-AI-Stage.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Only 5 days left until&amp;nbsp;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;&amp;nbsp;takes over Moscone West in San Francisco from October 27-29, bringing together&amp;nbsp;10,000+&amp;nbsp;leaders, VCs, operators, and innovators shaping the future of tech.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To celebrate the final countdown,&amp;nbsp;we’re&amp;nbsp;giving you one last chance to save big:&amp;nbsp;&lt;strong&gt;Buy your pass and save up to $444&lt;/strong&gt;&lt;strong&gt;, plus get 60% off a second ticket&lt;/strong&gt;&amp;nbsp;for your co-founder, partner, friend, or team member.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This offer at these low rates ends when doors open on October 27 —&amp;nbsp;&lt;strong&gt;lock in your two discounted passes now&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3016596" height="453" src="https://techcrunch.com/wp-content/uploads/2025/06/52457178574_dcedf0a9a3_o-1.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-disrupt-2025-ticket-types-made-for-your-growth-journey"&gt;Disrupt 2025 ticket types made for your growth journey&lt;/h2&gt;

&lt;h3 class="wp-block-heading" id="h-industry-nbsp-stage-nbsp-programming-5-nbsp-stages"&gt;Industry&amp;nbsp;stage&amp;nbsp;programming: 5&amp;nbsp;stages&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For&amp;nbsp;Attendee, Founder, Investor, Student, and&amp;nbsp;Non-Profit&amp;nbsp;Passes&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Five powerhouse stages delivering insights from the top minds in tech — including&amp;nbsp;Vinod Khosla,&amp;nbsp;Aaron Levie&amp;nbsp;(Box),&amp;nbsp;Elizabeth Stone&amp;nbsp;(Netflix),&amp;nbsp;Kevin Scott&amp;nbsp;(Microsoft), and more.&amp;nbsp;&lt;strong&gt;Explore the agenda and meet the speakers&lt;/strong&gt;&amp;nbsp;taking the stage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;AI Stage&lt;/strong&gt;&amp;nbsp;— Get immersed in the latest breakthroughs in AI across agents, hardware, creativity, and defense. Explore the science driving deep tech innovation and the ethical and societal impact behind it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Builders Stage&lt;/strong&gt;&amp;nbsp;— Learn founder-tested strategies and lessons from startup veterans ready to help you level up.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Disrupt Stage&lt;/strong&gt;&amp;nbsp;— Watch the intense&amp;nbsp;startup showdown,&amp;nbsp;&lt;strong&gt;Startup Battlefield 200&lt;/strong&gt;,&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;live&amp;nbsp;as 20 world-class startups pitch top investors. Get real insights from leaders shaping the future of entrepreneurship.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Going Public Stage&lt;/strong&gt;&amp;nbsp;— From day one to IPO, learn the frameworks and founder stories that guide companies to scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Space Stage&lt;/strong&gt;&amp;nbsp;— Discover how emerging companies are redefining the space industry and unlocking a new era of orbital exploration.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Box CEO Aaron Levie on stage at TechCrunch Disrupt in San Francisco in 2019." class="wp-image-2833981" height="454" src="https://techcrunch.com/wp-content/uploads/2024/08/GettyImages-1178603809.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Steve Jennings / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-interactive-roundtable-sessions"&gt;Interactive roundtable sessions&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For Attendee, Founder, Investor, Student, and&amp;nbsp;Non-Profit&amp;nbsp;Passes&lt;/em&gt;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Small-group&amp;nbsp;sessions where you can dive deep, share experiences, and ask the&amp;nbsp;hard questions&amp;nbsp;directly to industry experts and other attendees across different sectors in tech.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-q-amp-a-breakout-nbsp-sessions"&gt;Q&amp;amp;A breakout&amp;nbsp;sessions&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For all pass types&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Get answers from experts covering today’s most pressing topics. These&amp;nbsp;fast-paced, high-value sessions&amp;nbsp;near the Expo Hall are built for founders and operators who want actionable takeaways.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2024 Breakout Session" class="wp-image-2985187" height="454" src="https://techcrunch.com/wp-content/uploads/2025/03/Disrupt-2024-Breakout.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-expo-hall"&gt;Expo Hall&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For all pass types&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meet 300+ startups ready to pitch and demo their innovations. This is where 10,000+ tech leaders and investors come to discover&amp;nbsp;what’s&amp;nbsp;next.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-networking-that-moves-you-nbsp-forward"&gt;Networking that moves you&amp;nbsp;forward&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For all pass types&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Use Braindate to schedule curated 1:1 or small group meetings in the Networking Lounge. Build partnerships, spark&amp;nbsp;new ideas, and uncover your next big opportunity. And&amp;nbsp;don’t&amp;nbsp;forget — the entire venue is a hub for connections: Dive into collaborative roundtables, bump into innovators in the Expo Hall, or close deals over coffee in the exclusive founder-investor Deal Flow Cafe.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-disrupt-2025-is-days-away-don-t-wait"&gt;Disrupt 2025 is days away — don’t wait&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Grab your pass and save up to $444&lt;/strong&gt;,&amp;nbsp;and&amp;nbsp;&lt;strong&gt;get your plus-one at 60% off&lt;/strong&gt;&amp;nbsp;before prices increase when Disrupt kicks off. Bring your team, co-founder, or next collaborator and&amp;nbsp;&lt;strong&gt;experience Disrupt together&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt AI Stage" class="wp-image-3048038" height="454" src="https://techcrunch.com/wp-content/uploads/2025/09/Disrupt-2025-AI-Stage.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/22/last-minute-techcrunch-disrupt-2025-deal-save-60-on-your-plus-ones-pass-before-doors-open-on-oct-27/</guid><pubDate>Wed, 22 Oct 2025 14:30:00 +0000</pubDate></item><item><title>GM to introduce eyes-off, hands-off driving system in 2028 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/22/gm-to-introduce-eyes-off-hands-off-driving-system-in-2028/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/GM-Cadillac-Escalade-IQL-eyes-off-driving.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;General Motors said it plans to launch an automated driving system in 2028 that will allow drivers to keep their eyes off the road and hands off the wheel, starting with the Cadillac Escalade IQ.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The announcement, made Wednesday at its GM Forward event in New York City, comes a year after TechCrunch first reported that the automaker was working on the system. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;GM said its hands-off advanced driver assistance system, known as Super Cruise, is the foundation of this future, more capable product. Super Cruise, which launched in 2017 and is now available in 23 vehicle models, can be used on about 600,000 miles of highway. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This new eyes-off, hands-off driver assistance system — which will use lidar, radar, and cameras for perception —&amp;nbsp;will also start on highways. GM CEO Mary Barra noted during the event that GM would roll out its eyes-off product faster than it did its hands-off Super Cruise ADAS.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The automaker said it has tapped the experience of engineers who worked at its now shuttered autonomous vehicle technology subsidiary Cruise to improve the capabilities of that system. When GM shut down Cruise, its commercial robotaxi business,&amp;nbsp;in December 2024, it absorbed the subsidiary and combined it with its own efforts to develop driver assistance features. Over the last year, GM has also rehired several Cruise engineers as it pursues its goal of developing fully autonomous personal vehicles. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GM said it is also feeding Cruise’s technology stack, which includes AI models trained on five million driverless miles and a simulation framework running virtual test scenarios, into its next-generation driver assistance and autonomy programs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Robotaxi as a proof of concept when you start makes a lot of sense,” Sterling Anderson, GM’s executive vice president of global product and former co-founder of AV startup Aurora Innovation, said during the event, adding that the high cost of sensors and compute on autonomous vehicles required high utilization of those vehicles. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re now in a position in 2025 where the industry broadly has brought down the cost tremendously of some of the hardware,” Anderson said. “And GM, uniquely, has the install base, the manufacturing capacity to put these out at much larger volumes and much lower costs. Had the industry had low-cost systems and a huge install base and manufacturing capacity to begin with, we probably all would have gone for personal autonomous vehicles to begin with.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the U.S., Mercedes is currently the only automaker with a commercially available hands-off, eyes-off system. Such systems would fall under the SAE’s Level 3 of automation, which refers to an automated system that can drive itself under certain conditions but might still require a human to take over. Mercedes’ Drive Pilot is only available on certain mapped highways in California and Nevada, and only functions in heavy, low-speed traffic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GM’s eyes-off product will work on highways that GM hasn’t mapped, according to Baris Cetinok, GM’s senior vice president of software and services. He added that the system will only require human takeover for things like off-ramps, and can handle emergencies and sudden incidents. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Human intervention should not be the escape hatch for sudden incidents,” Cetinok said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Getting to market with an eyes-off, hands-off driving system would put GM ahead of most other automakers, unless they get there first. Earlier this year, Stellantis unveiled its own Level 3 system, but it has put the launch on hold. Tesla has been gunning to “solve full self-driving” by relying only on its cars’ cameras and neural networks for years, even though its Autopilot and FSD systems still require the driver to keep their eyes on the road. &amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/GM-Cadillac-Escalade-IQL-eyes-off-driving.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;General Motors said it plans to launch an automated driving system in 2028 that will allow drivers to keep their eyes off the road and hands off the wheel, starting with the Cadillac Escalade IQ.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The announcement, made Wednesday at its GM Forward event in New York City, comes a year after TechCrunch first reported that the automaker was working on the system. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;GM said its hands-off advanced driver assistance system, known as Super Cruise, is the foundation of this future, more capable product. Super Cruise, which launched in 2017 and is now available in 23 vehicle models, can be used on about 600,000 miles of highway. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This new eyes-off, hands-off driver assistance system — which will use lidar, radar, and cameras for perception —&amp;nbsp;will also start on highways. GM CEO Mary Barra noted during the event that GM would roll out its eyes-off product faster than it did its hands-off Super Cruise ADAS.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The automaker said it has tapped the experience of engineers who worked at its now shuttered autonomous vehicle technology subsidiary Cruise to improve the capabilities of that system. When GM shut down Cruise, its commercial robotaxi business,&amp;nbsp;in December 2024, it absorbed the subsidiary and combined it with its own efforts to develop driver assistance features. Over the last year, GM has also rehired several Cruise engineers as it pursues its goal of developing fully autonomous personal vehicles. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GM said it is also feeding Cruise’s technology stack, which includes AI models trained on five million driverless miles and a simulation framework running virtual test scenarios, into its next-generation driver assistance and autonomy programs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Robotaxi as a proof of concept when you start makes a lot of sense,” Sterling Anderson, GM’s executive vice president of global product and former co-founder of AV startup Aurora Innovation, said during the event, adding that the high cost of sensors and compute on autonomous vehicles required high utilization of those vehicles. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re now in a position in 2025 where the industry broadly has brought down the cost tremendously of some of the hardware,” Anderson said. “And GM, uniquely, has the install base, the manufacturing capacity to put these out at much larger volumes and much lower costs. Had the industry had low-cost systems and a huge install base and manufacturing capacity to begin with, we probably all would have gone for personal autonomous vehicles to begin with.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the U.S., Mercedes is currently the only automaker with a commercially available hands-off, eyes-off system. Such systems would fall under the SAE’s Level 3 of automation, which refers to an automated system that can drive itself under certain conditions but might still require a human to take over. Mercedes’ Drive Pilot is only available on certain mapped highways in California and Nevada, and only functions in heavy, low-speed traffic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GM’s eyes-off product will work on highways that GM hasn’t mapped, according to Baris Cetinok, GM’s senior vice president of software and services. He added that the system will only require human takeover for things like off-ramps, and can handle emergencies and sudden incidents. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Human intervention should not be the escape hatch for sudden incidents,” Cetinok said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Getting to market with an eyes-off, hands-off driving system would put GM ahead of most other automakers, unless they get there first. Earlier this year, Stellantis unveiled its own Level 3 system, but it has put the launch on hold. Tesla has been gunning to “solve full self-driving” by relying only on its cars’ cameras and neural networks for years, even though its Autopilot and FSD systems still require the driver to keep their eyes on the road. &amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/22/gm-to-introduce-eyes-off-hands-off-driving-system-in-2028/</guid><pubDate>Wed, 22 Oct 2025 15:00:00 +0000</pubDate></item><item><title>Open Source AI Week — How Developers and Contributors Are Advancing AI Innovation (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/open-source-ai-week/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA’s on the ground at Open Source AI Week. Stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward. Follow NVIDIA AI Developer on social channels for additional news and insights.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="keynote"&gt;&lt;b&gt;Advancing Embodied Intelligence Through Open-Source Innovation 🔗&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-86142 size-medium" height="540" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/ptc-keynote-960x540.jpg" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;At the PyTorch Conference, Jim Fan, director of robotics and distinguished research scientist at NVIDIA, discussed the Physical Turing Test — a way of measuring the performance of intelligent machines in the physical world.&lt;/p&gt;
&lt;p&gt;With conversational AI now capable of fluent, lifelike communication, Fan noted that the next challenge is enabling machines to act with similar naturalism. The Physical Turing Test asks: can an intelligent machine perform a real-world task so fluidly that a human cannot tell whether a person or a robot completed it?&lt;/p&gt;
&lt;p&gt;Fan highlighted that progress in embodied AI and physical AI depends on generating large amounts of diverse data, access to open robot foundation models and simulation frameworks — and walked through a unified workflow for developing embodied AI.&lt;/p&gt;
&lt;p&gt;With synthetic data workflows like NVIDIA Isaac GR00T-Dreams — built on NVIDIA Cosmos world foundation models— developers can generate virtual worlds from images and prompts, speeding the creation of large sets of diverse and physically accurate data.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;That data can then be used to post-train NVIDIA Isaac GR00T N open foundation models for generalized humanoid robot reasoning and skills. But before the models are deployed in the real world, these new robot skills need to be tested in simulation.&lt;/p&gt;
&lt;p&gt;Open simulation and learning frameworks such as NVIDIA Isaac Sim and Isaac Lab allow robots to “practice” countless times across millions of virtual environments before operating in the real world, dramatically accelerating learning and deployment cycles.&lt;/p&gt;
&lt;p&gt;Plus, with Newton, an open-source, differentiable physics engine built on NVIDIA Warp and OpenUSD, developers can bring high-fidelity simulation to complex robotic dynamics such as motion, balance and contact — reducing the simulation-to-real gap.&lt;/p&gt;
&lt;p&gt;This accelerates the creation of physically capable AI systems that learn faster, perform more safely and operate effectively in real-world environments.&lt;/p&gt;
&lt;p&gt;However, scaling embodied intelligence isn’t just about compute — it’s about access. Fan reaffirmed NVIDIA’s commitment to open source, emphasizing how the company’s frameworks and foundation models are shared to empower developers and researchers globally.&lt;/p&gt;
&lt;p&gt;Developers can get started with NVIDIA’s open embodied and physical AI models on Hugging Face.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="nemotron"&gt;&lt;b&gt;Llama‑Embed‑Nemotron‑8B Ranks Among Top Open Models for Multilingual Retrieval 🔗&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA’s Llama‑Embed‑Nemotron‑8B model has been recognized as the top open and portable model on the Multilingual Text Embedding Benchmark leaderboard.&lt;/p&gt;
&lt;p&gt;Built on the meta‑llama/Llama‑3.1‑8B architecture, Llama‑Embed‑Nemotron‑8B is a research text embedding model that converts text into 4,096‑dimensional vector representations. Designed for flexibility, it supports a wide range of use cases, including retrieval, reranking, semantic similarity and classification across more than 1,000 languages.&lt;/p&gt;
&lt;p&gt;Trained on a diverse collection of 16 million query–document pairs — half from public sources and half synthetically generated — the model benefits from refined data generation techniques, hard‑negative mining and model‑merging approaches that contribute to its broad generalization capabilities.&lt;/p&gt;
&lt;p&gt;This result builds on NVIDIA’s ongoing research in open, high‑performing AI models. Following earlier leaderboard recognition for the Llama NeMo Retriever ColEmbed model, the success of Llama‑Embed‑Nemotron‑8B highlights the value of openness, transparency and collaboration in advancing AI for the developer community.&lt;/p&gt;
&lt;p&gt;Check out Llama-Embed-Nemotron-8B on Hugging Face, and learn more about the model, including architectural highlights, training methodology and performance evaluation.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-full wp-image-86053" height="470" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/ezgif.com-video-to-gif-converter-7.gif" width="800" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;What Open Source Teaches Us About Making AI Better&lt;/b&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;b&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Open models are shaping the future of AI, enabling developers, enterprises and governments to innovate with transparency, customization and trust. In the latest episode of the NVIDIA AI Podcast, NVIDIA’s Bryan Catanzaro and Jonathan Cohen discuss how open models, datasets and research are laying the foundation for shared progress across the AI ecosystem.&lt;/p&gt;
&lt;p&gt;The NVIDIA Nemotron family of open models represents a full-stack approach to AI development, connecting model design to the underlying hardware and software that power it. By releasing Nemotron models, data and training methodologies openly, NVIDIA aims to help others refine, adapt and build upon its work, resulting in a faster exchange of ideas and more efficient systems.&lt;/p&gt;
&lt;p&gt;“When we as a community come together — contributing ideas, data and models — we all move faster,” said Catanzaro in the episode. “Open technologies make that possible.”&lt;/p&gt;

&lt;p&gt;There’s more happening this week at Open Source AI Week, including the start of the PyTorch Conference — bringing together developers, researchers and innovators pushing the boundaries of open AI.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Attendees can tune in to the special keynote address by Jim Fan, director of robotics and distinguished research scientist at NVIDIA, to hear the latest advancements in robotics — from simulation and synthetic data to accelerated computing. The keynote, titled “The Physical Turing Test: Solving General Purpose Robotics,” will take place on Wednesday, Oct. 22, from 9:50-10:05 a.m. PT.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;b&gt;Andrej Karpathy’s Nanochat Teaches Developers How to Train LLMs in Four Hours 🔗&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Computer scientist Andrej Karpathy recently introduced Nanochat, calling it “the best ChatGPT that $100 can buy.” Nanochat is an open-source, full-stack large language model (LLM) implementation built for transparency and experimentation. In about 8,000 lines of minimal, dependency-light code, Nanochat runs the entire LLM pipeline — from tokenization and pretraining to fine-tuning, inference and chat — all through a simple web user interface.&lt;br /&gt;NVIDIA is supporting Karpathy’s open-source Nanochat project by releasing two NVIDIA Launchables, making it easy to deploy and experiment with Nanochat across various NVIDIA GPUs.&lt;/p&gt;
&lt;p&gt;With NVIDIA Launchables, developers can train and interact with their own conversational model in hours with a single click. The Launchables dynamically support different-sized GPUs — including NVIDIA H100 and L40S GPUs — on various clouds without need for modification. They also automatically work on any eight-GPU instance on NVIDIA Brev, so developers can get compute access immediately.&lt;/p&gt;
&lt;p&gt;The &lt;b&gt;first 10 users&lt;/b&gt; to deploy these Launchables will also receive free compute access to NVIDIA H100 or L40S GPUs.&lt;/p&gt;
&lt;p&gt;Start training with Nanochat by deploying a Launchable:&lt;/p&gt;

&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-86022 size-medium" height="527" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/launchable-960x527.png" width="960" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Andrej Karpathy’s Next Experiments Begin With NVIDIA DGX Spark&lt;/b&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Today, Karpathy received an NVIDIA DGX Spark — the world’s smallest AI supercomputer, designed to bring the power of Blackwell right to a developer’s desktop. With up to a petaflop of AI processing power and 128GB of unified memory in a compact form factor, DGX Spark empowers innovators like Karpathy to experiment, fine-tune and run massive models locally.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="pytorch"&gt;&lt;b&gt;Building the Future of AI With PyTorch and NVIDIA 🔗&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;PyTorch, the fastest-growing AI framework, derives its performance from the NVIDIA CUDA platform and uses the Python programming language to unlock developer productivity. This year, NVIDIA added Python as a first-class language to the CUDA platform, giving the PyTorch developer community greater access to CUDA.&lt;/p&gt;
&lt;p&gt;CUDA Python includes key components that make GPU acceleration in Python easier than ever, with built-in support for kernel fusion, extension module integration and simplified packaging for fast deployment.&lt;/p&gt;
&lt;p&gt;Following PyTorch’s open collaboration model, CUDA Python is available on GitHub and PyPI.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_86034"&gt;&lt;img alt="alt" class="wp-image-86034 size-large" height="672" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/pytorch-infographic-1-1680x672.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-86034"&gt;According to PyPI Stats, PyTorch averaged over two million daily downloads, peaking at 2,303,217 on October 14, and had 65 million total downloads last month.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Every month, developers worldwide download hundreds of millions of NVIDIA libraries — including CUDA, cuDNN, cuBLAS and CUTLASS — mostly within Python and PyTorch environments. CUDA Python provides nvmath-python, a new library that acts as the bridge between Python code and these highly optimized GPU libraries.&lt;/p&gt;
&lt;p&gt;Plus, kernel enhancements and support for next-generation frameworks make NVIDIA accelerated computing more efficient, adaptable and widely accessible.&lt;/p&gt;
&lt;p&gt;NVIDIA maintains a long-standing collaboration with the PyTorch community through open-source contributions and technical leadership, as well as by sponsoring and participating in community events and activations.&lt;/p&gt;
&lt;p&gt;At PyTorch Conference 2025 in San Francisco, NVIDIA will host a keynote address, five technical sessions and nine poster presentations.&lt;/p&gt;
&lt;p&gt;NVIDIA’s on the ground at Open Source AI Week. Stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward. Follow NVIDIA AI Developer on social channels for additional news and insights.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="open-source-ai"&gt;&lt;b&gt;NVIDIA Spotlights Open Source Innovation 🔗&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Open Source AI Week kicks off on Monday with a series of hackathons, workshops and meetups spotlighting the latest advances in AI, machine learning and open-source innovation.&lt;/p&gt;
&lt;p&gt;The event brings together leading organizations, researchers and open-source communities to share knowledge, collaborate on tools and explore how openness accelerates AI development.&lt;/p&gt;
&lt;p&gt;NVIDIA continues to expand access to advanced AI innovation by providing open-source tools, models and datasets designed to empower developers. With more than 1,000 open-source tools on NVIDIA GitHub repositories and over 500 models and 100 datasets on the NVIDIA Hugging Face collections, NVIDIA is accelerating the pace of open, collaborative AI development.&lt;/p&gt;
&lt;p&gt;Over the past year, NVIDIA has become the top contributor in Hugging Face repositories, reflecting a deep commitment to sharing models, frameworks and research that empower the community.&lt;/p&gt;

&lt;p&gt;Openly available models, tools and datasets are essential to driving innovation and progress. By empowering anyone to use, modify and share technology, it fosters transparency and accelerates discovery, fueling breakthroughs that benefit both industry and communities alike. That’s why NVIDIA is committed to supporting the open source ecosystem.&lt;/p&gt;
&lt;p&gt;We’re on the ground all week — stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward, with the PyTorch Conference serving as the flagship event.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA’s on the ground at Open Source AI Week. Stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward. Follow NVIDIA AI Developer on social channels for additional news and insights.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="keynote"&gt;&lt;b&gt;Advancing Embodied Intelligence Through Open-Source Innovation 🔗&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-86142 size-medium" height="540" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/ptc-keynote-960x540.jpg" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;At the PyTorch Conference, Jim Fan, director of robotics and distinguished research scientist at NVIDIA, discussed the Physical Turing Test — a way of measuring the performance of intelligent machines in the physical world.&lt;/p&gt;
&lt;p&gt;With conversational AI now capable of fluent, lifelike communication, Fan noted that the next challenge is enabling machines to act with similar naturalism. The Physical Turing Test asks: can an intelligent machine perform a real-world task so fluidly that a human cannot tell whether a person or a robot completed it?&lt;/p&gt;
&lt;p&gt;Fan highlighted that progress in embodied AI and physical AI depends on generating large amounts of diverse data, access to open robot foundation models and simulation frameworks — and walked through a unified workflow for developing embodied AI.&lt;/p&gt;
&lt;p&gt;With synthetic data workflows like NVIDIA Isaac GR00T-Dreams — built on NVIDIA Cosmos world foundation models— developers can generate virtual worlds from images and prompts, speeding the creation of large sets of diverse and physically accurate data.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;That data can then be used to post-train NVIDIA Isaac GR00T N open foundation models for generalized humanoid robot reasoning and skills. But before the models are deployed in the real world, these new robot skills need to be tested in simulation.&lt;/p&gt;
&lt;p&gt;Open simulation and learning frameworks such as NVIDIA Isaac Sim and Isaac Lab allow robots to “practice” countless times across millions of virtual environments before operating in the real world, dramatically accelerating learning and deployment cycles.&lt;/p&gt;
&lt;p&gt;Plus, with Newton, an open-source, differentiable physics engine built on NVIDIA Warp and OpenUSD, developers can bring high-fidelity simulation to complex robotic dynamics such as motion, balance and contact — reducing the simulation-to-real gap.&lt;/p&gt;
&lt;p&gt;This accelerates the creation of physically capable AI systems that learn faster, perform more safely and operate effectively in real-world environments.&lt;/p&gt;
&lt;p&gt;However, scaling embodied intelligence isn’t just about compute — it’s about access. Fan reaffirmed NVIDIA’s commitment to open source, emphasizing how the company’s frameworks and foundation models are shared to empower developers and researchers globally.&lt;/p&gt;
&lt;p&gt;Developers can get started with NVIDIA’s open embodied and physical AI models on Hugging Face.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="nemotron"&gt;&lt;b&gt;Llama‑Embed‑Nemotron‑8B Ranks Among Top Open Models for Multilingual Retrieval 🔗&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA’s Llama‑Embed‑Nemotron‑8B model has been recognized as the top open and portable model on the Multilingual Text Embedding Benchmark leaderboard.&lt;/p&gt;
&lt;p&gt;Built on the meta‑llama/Llama‑3.1‑8B architecture, Llama‑Embed‑Nemotron‑8B is a research text embedding model that converts text into 4,096‑dimensional vector representations. Designed for flexibility, it supports a wide range of use cases, including retrieval, reranking, semantic similarity and classification across more than 1,000 languages.&lt;/p&gt;
&lt;p&gt;Trained on a diverse collection of 16 million query–document pairs — half from public sources and half synthetically generated — the model benefits from refined data generation techniques, hard‑negative mining and model‑merging approaches that contribute to its broad generalization capabilities.&lt;/p&gt;
&lt;p&gt;This result builds on NVIDIA’s ongoing research in open, high‑performing AI models. Following earlier leaderboard recognition for the Llama NeMo Retriever ColEmbed model, the success of Llama‑Embed‑Nemotron‑8B highlights the value of openness, transparency and collaboration in advancing AI for the developer community.&lt;/p&gt;
&lt;p&gt;Check out Llama-Embed-Nemotron-8B on Hugging Face, and learn more about the model, including architectural highlights, training methodology and performance evaluation.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-full wp-image-86053" height="470" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/ezgif.com-video-to-gif-converter-7.gif" width="800" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;What Open Source Teaches Us About Making AI Better&lt;/b&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;b&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Open models are shaping the future of AI, enabling developers, enterprises and governments to innovate with transparency, customization and trust. In the latest episode of the NVIDIA AI Podcast, NVIDIA’s Bryan Catanzaro and Jonathan Cohen discuss how open models, datasets and research are laying the foundation for shared progress across the AI ecosystem.&lt;/p&gt;
&lt;p&gt;The NVIDIA Nemotron family of open models represents a full-stack approach to AI development, connecting model design to the underlying hardware and software that power it. By releasing Nemotron models, data and training methodologies openly, NVIDIA aims to help others refine, adapt and build upon its work, resulting in a faster exchange of ideas and more efficient systems.&lt;/p&gt;
&lt;p&gt;“When we as a community come together — contributing ideas, data and models — we all move faster,” said Catanzaro in the episode. “Open technologies make that possible.”&lt;/p&gt;

&lt;p&gt;There’s more happening this week at Open Source AI Week, including the start of the PyTorch Conference — bringing together developers, researchers and innovators pushing the boundaries of open AI.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Attendees can tune in to the special keynote address by Jim Fan, director of robotics and distinguished research scientist at NVIDIA, to hear the latest advancements in robotics — from simulation and synthetic data to accelerated computing. The keynote, titled “The Physical Turing Test: Solving General Purpose Robotics,” will take place on Wednesday, Oct. 22, from 9:50-10:05 a.m. PT.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;b&gt;Andrej Karpathy’s Nanochat Teaches Developers How to Train LLMs in Four Hours 🔗&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Computer scientist Andrej Karpathy recently introduced Nanochat, calling it “the best ChatGPT that $100 can buy.” Nanochat is an open-source, full-stack large language model (LLM) implementation built for transparency and experimentation. In about 8,000 lines of minimal, dependency-light code, Nanochat runs the entire LLM pipeline — from tokenization and pretraining to fine-tuning, inference and chat — all through a simple web user interface.&lt;br /&gt;NVIDIA is supporting Karpathy’s open-source Nanochat project by releasing two NVIDIA Launchables, making it easy to deploy and experiment with Nanochat across various NVIDIA GPUs.&lt;/p&gt;
&lt;p&gt;With NVIDIA Launchables, developers can train and interact with their own conversational model in hours with a single click. The Launchables dynamically support different-sized GPUs — including NVIDIA H100 and L40S GPUs — on various clouds without need for modification. They also automatically work on any eight-GPU instance on NVIDIA Brev, so developers can get compute access immediately.&lt;/p&gt;
&lt;p&gt;The &lt;b&gt;first 10 users&lt;/b&gt; to deploy these Launchables will also receive free compute access to NVIDIA H100 or L40S GPUs.&lt;/p&gt;
&lt;p&gt;Start training with Nanochat by deploying a Launchable:&lt;/p&gt;

&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-86022 size-medium" height="527" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/launchable-960x527.png" width="960" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Andrej Karpathy’s Next Experiments Begin With NVIDIA DGX Spark&lt;/b&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Today, Karpathy received an NVIDIA DGX Spark — the world’s smallest AI supercomputer, designed to bring the power of Blackwell right to a developer’s desktop. With up to a petaflop of AI processing power and 128GB of unified memory in a compact form factor, DGX Spark empowers innovators like Karpathy to experiment, fine-tune and run massive models locally.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="pytorch"&gt;&lt;b&gt;Building the Future of AI With PyTorch and NVIDIA 🔗&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;PyTorch, the fastest-growing AI framework, derives its performance from the NVIDIA CUDA platform and uses the Python programming language to unlock developer productivity. This year, NVIDIA added Python as a first-class language to the CUDA platform, giving the PyTorch developer community greater access to CUDA.&lt;/p&gt;
&lt;p&gt;CUDA Python includes key components that make GPU acceleration in Python easier than ever, with built-in support for kernel fusion, extension module integration and simplified packaging for fast deployment.&lt;/p&gt;
&lt;p&gt;Following PyTorch’s open collaboration model, CUDA Python is available on GitHub and PyPI.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_86034"&gt;&lt;img alt="alt" class="wp-image-86034 size-large" height="672" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/pytorch-infographic-1-1680x672.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-86034"&gt;According to PyPI Stats, PyTorch averaged over two million daily downloads, peaking at 2,303,217 on October 14, and had 65 million total downloads last month.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Every month, developers worldwide download hundreds of millions of NVIDIA libraries — including CUDA, cuDNN, cuBLAS and CUTLASS — mostly within Python and PyTorch environments. CUDA Python provides nvmath-python, a new library that acts as the bridge between Python code and these highly optimized GPU libraries.&lt;/p&gt;
&lt;p&gt;Plus, kernel enhancements and support for next-generation frameworks make NVIDIA accelerated computing more efficient, adaptable and widely accessible.&lt;/p&gt;
&lt;p&gt;NVIDIA maintains a long-standing collaboration with the PyTorch community through open-source contributions and technical leadership, as well as by sponsoring and participating in community events and activations.&lt;/p&gt;
&lt;p&gt;At PyTorch Conference 2025 in San Francisco, NVIDIA will host a keynote address, five technical sessions and nine poster presentations.&lt;/p&gt;
&lt;p&gt;NVIDIA’s on the ground at Open Source AI Week. Stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward. Follow NVIDIA AI Developer on social channels for additional news and insights.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="open-source-ai"&gt;&lt;b&gt;NVIDIA Spotlights Open Source Innovation 🔗&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Open Source AI Week kicks off on Monday with a series of hackathons, workshops and meetups spotlighting the latest advances in AI, machine learning and open-source innovation.&lt;/p&gt;
&lt;p&gt;The event brings together leading organizations, researchers and open-source communities to share knowledge, collaborate on tools and explore how openness accelerates AI development.&lt;/p&gt;
&lt;p&gt;NVIDIA continues to expand access to advanced AI innovation by providing open-source tools, models and datasets designed to empower developers. With more than 1,000 open-source tools on NVIDIA GitHub repositories and over 500 models and 100 datasets on the NVIDIA Hugging Face collections, NVIDIA is accelerating the pace of open, collaborative AI development.&lt;/p&gt;
&lt;p&gt;Over the past year, NVIDIA has become the top contributor in Hugging Face repositories, reflecting a deep commitment to sharing models, frameworks and research that empower the community.&lt;/p&gt;

&lt;p&gt;Openly available models, tools and datasets are essential to driving innovation and progress. By empowering anyone to use, modify and share technology, it fosters transparency and accelerates discovery, fueling breakthroughs that benefit both industry and communities alike. That’s why NVIDIA is committed to supporting the open source ecosystem.&lt;/p&gt;
&lt;p&gt;We’re on the ground all week — stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward, with the PyTorch Conference serving as the flagship event.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/open-source-ai-week/</guid><pubDate>Wed, 22 Oct 2025 15:00:33 +0000</pubDate></item><item><title>A verifiable quantum advantage (The latest research from Google)</title><link>https://research.google/blog/a-verifiable-quantum-advantage/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;The computational gap between quantum and classical processors&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;The second consequence of many-body interference is classical complexity. A central task for quantum computing is to identify the computational cost gap between quantum and classical computers on specific computational tasks. We approached this in two ways: (1) through a combination of theoretical analysis and experiments, we revealed the fundamental obstacles to known classical algorithms in achieving the same outcome as our OTOC calculations on Willow, and (2) we tested the performance of nine relevant classical simulation algorithms by direct implementation and cost estimation.&lt;/p&gt;&lt;p&gt;In the first approach we identified that quantum interference is an obstacle for classical computation. A distinct characteristic of quantum mechanics is that predicting an outcome of an experiment requires analyzing probability amplitudes rather than probabilities as in classical mechanics. A well known example is the entanglement of light that manifests in quantum correlations between photons, elementary particles of light, that persist over long distances (2022 Physics Nobel Laureates) or macroscopic quantum tunneling phenomena in superconducting circuits (2025 Physics Nobel Laureates).&lt;/p&gt;&lt;p&gt;The interference in our second order OTOC data (i.e., an OTOC that runs through the backward and forward circuit loop twice) reveals a similar distinction between probabilities and probability amplitudes. Crucially, probabilities are non-negative numbers, whereas probability amplitudes can be of an arbitrary sign and are described by complex numbers. Taken together, these features mean they contain a much more complex collection of information. Instead of a pair of photons or a single superconducting junction, our experiment is described by probability amplitudes across an exponentially large space of 65 qubits. An exact description of such a quantum mechanical system requires storing and processing 2&lt;sup class="superscript"&gt;65&lt;/sup&gt; complex numbers in memory, which is beyond the capacity of supercomputers. Moreover, quantum chaos in our circuits ensures that every amplitude is equally important, and therefore algorithms using a compressed description of the system require memory and processing time beyond the capacity of supercomputers.&lt;/p&gt;&lt;p&gt;Our further theoretical and experimental analysis revealed that carefully accounting for the signs of the probability amplitudes is necessary to predict our experimental data by a numerical calculation. This presents a significant barrier for a class of efficient classical algorithms, quantum Monte Carlo, that have been successful at describing quantum phenomena in a large quantum mechanical space (e.g., superfluidity of liquid Helium-4). These algorithms rely on description in terms of probabilities, yet our analysis demonstrates that such approaches would result in an uncontrollable error in the computation output.&lt;/p&gt;&lt;p&gt;Our direct implementation of algorithms relying on both compressed representation and efficient quantum Monte Carlo confirmed the impossibility of predicting second-order OTOC data. Our experiments on Willow took approximately 2 hours, a task estimated to require 13,000 times longer on a classical supercomputer. This conclusion was reached after an estimated 10 person years spent in classical red teaming of our quantum result, implementing a total of nine classical simulation algorithms as a result.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;The computational gap between quantum and classical processors&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;The second consequence of many-body interference is classical complexity. A central task for quantum computing is to identify the computational cost gap between quantum and classical computers on specific computational tasks. We approached this in two ways: (1) through a combination of theoretical analysis and experiments, we revealed the fundamental obstacles to known classical algorithms in achieving the same outcome as our OTOC calculations on Willow, and (2) we tested the performance of nine relevant classical simulation algorithms by direct implementation and cost estimation.&lt;/p&gt;&lt;p&gt;In the first approach we identified that quantum interference is an obstacle for classical computation. A distinct characteristic of quantum mechanics is that predicting an outcome of an experiment requires analyzing probability amplitudes rather than probabilities as in classical mechanics. A well known example is the entanglement of light that manifests in quantum correlations between photons, elementary particles of light, that persist over long distances (2022 Physics Nobel Laureates) or macroscopic quantum tunneling phenomena in superconducting circuits (2025 Physics Nobel Laureates).&lt;/p&gt;&lt;p&gt;The interference in our second order OTOC data (i.e., an OTOC that runs through the backward and forward circuit loop twice) reveals a similar distinction between probabilities and probability amplitudes. Crucially, probabilities are non-negative numbers, whereas probability amplitudes can be of an arbitrary sign and are described by complex numbers. Taken together, these features mean they contain a much more complex collection of information. Instead of a pair of photons or a single superconducting junction, our experiment is described by probability amplitudes across an exponentially large space of 65 qubits. An exact description of such a quantum mechanical system requires storing and processing 2&lt;sup class="superscript"&gt;65&lt;/sup&gt; complex numbers in memory, which is beyond the capacity of supercomputers. Moreover, quantum chaos in our circuits ensures that every amplitude is equally important, and therefore algorithms using a compressed description of the system require memory and processing time beyond the capacity of supercomputers.&lt;/p&gt;&lt;p&gt;Our further theoretical and experimental analysis revealed that carefully accounting for the signs of the probability amplitudes is necessary to predict our experimental data by a numerical calculation. This presents a significant barrier for a class of efficient classical algorithms, quantum Monte Carlo, that have been successful at describing quantum phenomena in a large quantum mechanical space (e.g., superfluidity of liquid Helium-4). These algorithms rely on description in terms of probabilities, yet our analysis demonstrates that such approaches would result in an uncontrollable error in the computation output.&lt;/p&gt;&lt;p&gt;Our direct implementation of algorithms relying on both compressed representation and efficient quantum Monte Carlo confirmed the impossibility of predicting second-order OTOC data. Our experiments on Willow took approximately 2 hours, a task estimated to require 13,000 times longer on a classical supercomputer. This conclusion was reached after an estimated 10 person years spent in classical red teaming of our quantum result, implementing a total of nine classical simulation algorithms as a result.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/a-verifiable-quantum-advantage/</guid><pubDate>Wed, 22 Oct 2025 15:07:00 +0000</pubDate></item><item><title>Meta cuts 600 AI jobs amid ongoing reorganization (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/22/meta-cuts-600-ai-jobs-amid-ongoing-reorganization/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2194278734.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta’s chief AI officer, Alexandr Wang, wrote in a memo to staff on Wednesday that the company will cut about 600 jobs from its superintelligence lab, according to a report from Axios.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta declined to comment, but told TechCrunch that Axios’ reporting is accurate.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As Meta, OpenAI, Anthropic, Google, and other companies race to build the most powerful AI systems, Meta had a busy summer on the hiring front. The company poached more than 50 researchers from its competitors by offering multimillion-dollar pay packages, though OpenAI CEO Sam Altman claimed that “none of [OpenAI’s] best people” took the offers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“By reducing the size of our team, fewer conversations will be required to make a decision, and each person will be more load-bearing and have more scope and impact,” Wang wrote in the memo to staff.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This line of thinking tracks with Meta’s recent “year of efficiency” — a more sanitized way to describe the company’s mass layoffs. At the time, Meta CEO Mark Zuckerberg told staff that “leaner is better.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, it seems that Meta isn’t lowering its overall headcount by much, but rather, reorganizing its efforts. The company claims that most of these people impacted today should be able to find another job within Meta. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2194278734.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta’s chief AI officer, Alexandr Wang, wrote in a memo to staff on Wednesday that the company will cut about 600 jobs from its superintelligence lab, according to a report from Axios.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta declined to comment, but told TechCrunch that Axios’ reporting is accurate.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As Meta, OpenAI, Anthropic, Google, and other companies race to build the most powerful AI systems, Meta had a busy summer on the hiring front. The company poached more than 50 researchers from its competitors by offering multimillion-dollar pay packages, though OpenAI CEO Sam Altman claimed that “none of [OpenAI’s] best people” took the offers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“By reducing the size of our team, fewer conversations will be required to make a decision, and each person will be more load-bearing and have more scope and impact,” Wang wrote in the memo to staff.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This line of thinking tracks with Meta’s recent “year of efficiency” — a more sanitized way to describe the company’s mass layoffs. At the time, Meta CEO Mark Zuckerberg told staff that “leaner is better.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, it seems that Meta isn’t lowering its overall headcount by much, but rather, reorganizing its efforts. The company claims that most of these people impacted today should be able to find another job within Meta. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/22/meta-cuts-600-ai-jobs-amid-ongoing-reorganization/</guid><pubDate>Wed, 22 Oct 2025 15:08:18 +0000</pubDate></item><item><title>GM is bringing Google Gemini-powered AI assistant to cars in 2026 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/22/gm-is-bringing-google-gemini-powered-ai-assistant-to-cars-in-2026/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/GettyImages-2233653192.jpg?resize=1200,786" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;General Motors will add a conversational AI assistant powered by Google Gemini to its cars, trucks, and SUVs starting next year, the U.S. automaker said Wednesday during an event in New York City.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Google Gemini rollout is one of several tech-centric announcements made at the automaker’s GM Forward event, and it will be one of the first to get into consumers’ hands. Others, including an overhaul of its electrical architecture and computing platform and an automated driving feature that allows drivers to keep their hands off the wheel and eyes off the road, aren’t coming to GM brands until 2028.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;GM is the latest automaker to lean in to generative AI-based assistants that promise to respond to driver requests in a more natural-sounding way. Stellantis is collaborating with French AI firm Mistral, Mercedes is integrating ChatGPT, and Tesla has brought xAI’s Grok to its vehicles. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GM’s integration with Gemini is the next logical step for the automaker. Vehicles produced by GM brands Buick, Chevrolet, Cadillac, and GMC already have “Google built-in,” an operating system that gives drivers access to Google Assistant, Google Maps, and other apps directly from the car’s infotainment screen. In 2023, Google began using Google Cloud’s Dialogflow chatbot to handle non-emergency OnStar features, including common driver queries like routing and navigation assistance.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GM’s Gemini-powered AI assistant will have similar levels of capability — it’ll just perform better, according to Dave Richardson, senior vice president of software and services. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“One of the challenges with current voice assistants is that, if you’ve used them, you’ve probably also been frustrated by them because they’re trained on certain code words or they don’t understand accents very well or if you don’t say it quite right, you don’t get the right response,” Richardson told TechCrunch. “What’s great about large language models is they don’t seem to be affected by that. They have context about previous conversations that they can bring up. They’re flexible in how you speak to them&amp;nbsp;… so overall you’re getting a better, more natural experience.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That might make drafting and sending messages, planning routes with additional stops (like a charging station or a favorite coffee shop), or even prepping for a meeting on the go a more pain-free experience. The assistant will also have access to the web to be able to answer certain questions like, “What’s the history of this bridge I’m driving over?”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The Gemini assistant will be made available via the Play Store as an over-the-air upgrade to OnStar-equipped vehicles, model year 2015 and above.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GM’s new voice assistant is a step toward the automaker’s goal of developing its own custom-built AI that connects to your vehicle’s systems through OnStar, GM’s in-car concierge. The way GM executives described the technology at the NYC event, it seems like a mix of a health wearable and an AI pendant, but for your car.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The assistant promises to access vehicle data to provide maintenance alerts and route suggestions, explain car features like one-pedal driving, and turn your heat or air conditioning on before you enter the vehicle. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The idea here is you take [an existing] large language model, and you train it and refine it on a specific domain,” Richardson said. “We’ll take a base model and train it on the vehicle’s specifications, distill that down, and have that running on the vehicle.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While GM has a close relationship with Google and will already be implementing Gemini into certain vehicles, Richardson said GM plans to test several foundational models from other AI firms, which could include OpenAI, Anthropic, and others. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Richardson said drivers will be able to control what information the assistant can access and use, and it can learn from your habits to offer personalized recommendations.&amp;nbsp;GM’s emphasis on user controls is notable given the company’s recent controversy over selling customer driving and geolocation data to insurance brokers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Richardson said any data GM gets from drivers goes directly toward improving the product and won’t be sold to bring in additional revenue for the automaker. Over the last nearly two years, GM has brought on a new data team — including Christina Montgomery, who spent 30 years as IBM’s chief privacy and trust officer — to put standard processes and data governance technology in place.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Everything that we’re going to do is going to be driven by customer consent, so you can always opt in or opt out,” he said. “Our viewpoint is that data and privacy has to be built into everything that we do.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article has been updated with comments from Dave Richardson.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/GettyImages-2233653192.jpg?resize=1200,786" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;General Motors will add a conversational AI assistant powered by Google Gemini to its cars, trucks, and SUVs starting next year, the U.S. automaker said Wednesday during an event in New York City.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Google Gemini rollout is one of several tech-centric announcements made at the automaker’s GM Forward event, and it will be one of the first to get into consumers’ hands. Others, including an overhaul of its electrical architecture and computing platform and an automated driving feature that allows drivers to keep their hands off the wheel and eyes off the road, aren’t coming to GM brands until 2028.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;GM is the latest automaker to lean in to generative AI-based assistants that promise to respond to driver requests in a more natural-sounding way. Stellantis is collaborating with French AI firm Mistral, Mercedes is integrating ChatGPT, and Tesla has brought xAI’s Grok to its vehicles. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GM’s integration with Gemini is the next logical step for the automaker. Vehicles produced by GM brands Buick, Chevrolet, Cadillac, and GMC already have “Google built-in,” an operating system that gives drivers access to Google Assistant, Google Maps, and other apps directly from the car’s infotainment screen. In 2023, Google began using Google Cloud’s Dialogflow chatbot to handle non-emergency OnStar features, including common driver queries like routing and navigation assistance.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GM’s Gemini-powered AI assistant will have similar levels of capability — it’ll just perform better, according to Dave Richardson, senior vice president of software and services. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“One of the challenges with current voice assistants is that, if you’ve used them, you’ve probably also been frustrated by them because they’re trained on certain code words or they don’t understand accents very well or if you don’t say it quite right, you don’t get the right response,” Richardson told TechCrunch. “What’s great about large language models is they don’t seem to be affected by that. They have context about previous conversations that they can bring up. They’re flexible in how you speak to them&amp;nbsp;… so overall you’re getting a better, more natural experience.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That might make drafting and sending messages, planning routes with additional stops (like a charging station or a favorite coffee shop), or even prepping for a meeting on the go a more pain-free experience. The assistant will also have access to the web to be able to answer certain questions like, “What’s the history of this bridge I’m driving over?”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The Gemini assistant will be made available via the Play Store as an over-the-air upgrade to OnStar-equipped vehicles, model year 2015 and above.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GM’s new voice assistant is a step toward the automaker’s goal of developing its own custom-built AI that connects to your vehicle’s systems through OnStar, GM’s in-car concierge. The way GM executives described the technology at the NYC event, it seems like a mix of a health wearable and an AI pendant, but for your car.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The assistant promises to access vehicle data to provide maintenance alerts and route suggestions, explain car features like one-pedal driving, and turn your heat or air conditioning on before you enter the vehicle. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The idea here is you take [an existing] large language model, and you train it and refine it on a specific domain,” Richardson said. “We’ll take a base model and train it on the vehicle’s specifications, distill that down, and have that running on the vehicle.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While GM has a close relationship with Google and will already be implementing Gemini into certain vehicles, Richardson said GM plans to test several foundational models from other AI firms, which could include OpenAI, Anthropic, and others. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Richardson said drivers will be able to control what information the assistant can access and use, and it can learn from your habits to offer personalized recommendations.&amp;nbsp;GM’s emphasis on user controls is notable given the company’s recent controversy over selling customer driving and geolocation data to insurance brokers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Richardson said any data GM gets from drivers goes directly toward improving the product and won’t be sold to bring in additional revenue for the automaker. Over the last nearly two years, GM has brought on a new data team — including Christina Montgomery, who spent 30 years as IBM’s chief privacy and trust officer — to put standard processes and data governance technology in place.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Everything that we’re going to do is going to be driven by customer consent, so you can always opt in or opt out,” he said. “Our viewpoint is that data and privacy has to be built into everything that we do.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article has been updated with comments from Dave Richardson.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/22/gm-is-bringing-google-gemini-powered-ai-assistant-to-cars-in-2026/</guid><pubDate>Wed, 22 Oct 2025 15:51:03 +0000</pubDate></item><item><title>Amazon unveils AI smart glasses for its delivery drivers (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/22/amazon-unveils-ai-smart-glasses-for-its-delivery-drivers/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon announced on Wednesday that it’s developing AI-powered smart glasses for its delivery drivers. The idea behind the glasses is to give delivery drivers a hands-free experience that reduces the need to keep looking between their phone, the package they’re delivering, and their surroundings.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The e-commerce giant says the glasses will allow delivery drivers to scan packages, follow turn-by-turn walking directions, and capture proof of delivery, all without using their phones. The glasses use AI-powered sensing capabilities and computer vision alongside cameras to create a display that includes things like hazards and delivery tasks. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Amazon likely hopes that the new glasses will shave time off of each delivery by providing delivery drivers with detailed directions and information about hazards directly in their line of sight.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3060965" height="375" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-22-at-2.18.57PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;When a driver parks at a delivery location, Amazon says the glasses automatically activate. The glasses help the driver locate the package inside the vehicle and then navigate to the delivery address. The glasses can provide easy-to-follow directions in places like multi-unit apartment complexes and business locations.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The glasses are paired with a controller worn in the delivery vest that contains operational controls, a swappable battery, and a dedicated emergency button. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon notes that the glasses also support prescription lenses and transitional lenses that automatically adjust to light. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3060966" height="373" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-22-at-2.19.04PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The retailer is currently trialing the glasses with delivery drivers in North America and plans to refine the technology before a wider rollout.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The announcement doesn’t come as a surprise, as Reuters reported last year that Amazon was working on the smart glasses. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the future, Amazon says the glasses will be able to provide drivers with “real-time defect detection” that could notify them if they accidentally drop off a package at the wrong address. The glasses will also be able to detect pets in yards and automatically adjust to hazards like low light conditions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Also on Wednesday, Amazon unveiled a new robotic arm called “Blue Jay” that can work alongside warehouse employees to pick items off shelves and sort them. Additionally, the tech giant announced a new AI tool called Eluna that will help provide operational insights at Amazon warehouses.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon announced on Wednesday that it’s developing AI-powered smart glasses for its delivery drivers. The idea behind the glasses is to give delivery drivers a hands-free experience that reduces the need to keep looking between their phone, the package they’re delivering, and their surroundings.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The e-commerce giant says the glasses will allow delivery drivers to scan packages, follow turn-by-turn walking directions, and capture proof of delivery, all without using their phones. The glasses use AI-powered sensing capabilities and computer vision alongside cameras to create a display that includes things like hazards and delivery tasks. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Amazon likely hopes that the new glasses will shave time off of each delivery by providing delivery drivers with detailed directions and information about hazards directly in their line of sight.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3060965" height="375" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-22-at-2.18.57PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;When a driver parks at a delivery location, Amazon says the glasses automatically activate. The glasses help the driver locate the package inside the vehicle and then navigate to the delivery address. The glasses can provide easy-to-follow directions in places like multi-unit apartment complexes and business locations.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The glasses are paired with a controller worn in the delivery vest that contains operational controls, a swappable battery, and a dedicated emergency button. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon notes that the glasses also support prescription lenses and transitional lenses that automatically adjust to light. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3060966" height="373" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-22-at-2.19.04PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The retailer is currently trialing the glasses with delivery drivers in North America and plans to refine the technology before a wider rollout.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The announcement doesn’t come as a surprise, as Reuters reported last year that Amazon was working on the smart glasses. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the future, Amazon says the glasses will be able to provide drivers with “real-time defect detection” that could notify them if they accidentally drop off a package at the wrong address. The glasses will also be able to detect pets in yards and automatically adjust to hazards like low light conditions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Also on Wednesday, Amazon unveiled a new robotic arm called “Blue Jay” that can work alongside warehouse employees to pick items off shelves and sort them. Additionally, the tech giant announced a new AI tool called Eluna that will help provide operational insights at Amazon warehouses.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/22/amazon-unveils-ai-smart-glasses-for-its-delivery-drivers/</guid><pubDate>Wed, 22 Oct 2025 18:33:34 +0000</pubDate></item><item><title>[NEW] OpenAI’s Atlas is more about ChatGPT than the web (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/22/openais-atlas-is-more-about-chatgpt-than-the-web/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI unveiled its AI browser, ChatGPT Atlas, during a livestream on Tuesday. There are other AI browsers such as The Browser Company’s Dia, Opera’s Neon, Perplexity’s Comet, and General Catalyst-backed Strawberry. OpenAI’s launch is notable because of the sheer scale of reaching potentially 800 million of its weekly ChatGPT users. For the company, the browser is much more about keeping ChatGPT central than about making web browsing better.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Atlas is currently available only on the Mac, the company is already working on bringing it to Windows, iOS, and Android — all the surfaces where ChatGPT already exists. OpenAI has also made the browser available to all users instead of opting for an invite system like its rivals. The core proposition of the browser is for you to think of ChatGPT as the first interaction surface for search and answers instead of Google.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;All the AI browsers share a similar idea about search and Q&amp;amp;A. Instead of performing a search query, you would type something in your address bar to get answers from an AI chatbot, instead of looking at pages of links.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And OpenAI, just like other browser makers, thinks that Atlas will change the way you browse the web, as Sam Altman made clear at the launch. “We think AI represents once in a decade opportunity to rethink what a browser can be, how to use one, and how to most productively use the web. Tabs were great but there hasn’t been a lot of innovation since then,” Altman said in his opening speech.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tech leaders, including Sundar Pichai and Satya Nadella, have talked about AI as a platform shift. However, for consumers, phones and desktop operating systems are still the primary way to get to their AI tools. OpenAI wants to own the pipes of distribution of ChatGPT as much as it can. Last week, Meta shut its doors to third-party chatbots, including ChatGPT and Perplexity on WhatsApp, which has over 3 billion monthly users. This essentially means that the platform owners could put the brakes on distribution at any point in time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For OpenAI, Atlas will also present an opportunity to deeply integrate ChatGPT and other products better than other platforms can. Users can directly reference multiple websites instead of posting links to ChatGPT. The company already uses a headless browser for its agent. With Atlas, it might have more control over the feature. It has already integrated a hovering writing assistant that shows up in text fields.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3060764" height="167" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-22-at-5.01.03PM.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;What’s more, the company is working on integrating its App SDK, which lets you call other apps within ChatGPT, to improve discoverability. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The memory feature is also key for ChatGPT’s power users. The feature takes into account the browsing history, along with your ChatGPT history, to provide answers with that context in mind. You can ask, “What was the work document I had my presentation plan on?” and ChatGPT will fetch that link for you. This also means that ChatGPT gets more context about you as you spend more time in the browser. OpenAI can use this context and provide it to other apps when it starts rolling out Sign in with ChatGPT widely.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both features — making ChatGPT the default search option and enabling memory — are designed to gather more user data, giving OpenAI greater insight into user behavior and enabling better product development. The browser doesn’t have an ad-blocker, a VPN, a reading mode, or a translate feature to make the browsing experience better for a site. Rather, users have to ask ChatGPT to summarize content or find something on a page — as if opening a page is designed to give ChatGPT more context rather than to help users consume the content on the page.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In contrast, The Browser Company’s Arc has some useful ideas around revamping the browser experience, like using AI to rename downloaded files or customize a web page by letting you remove elements.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3060766" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/Memory.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;OpenAI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The result is more than a browser; it’s a broader canvas for ChatGPT itself. OpenAI’s CEO of applications, Fidji Simo, laid out this idea in her blog outlining the Atlas launch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“When we first released ChatGPT, we weren’t sure how people would use it. Now that we have feedback and signals from hundreds of millions of people around the world, it’s clear ChatGPT needs to become so much more than the simple chatbot it started as. Over time, we see ChatGPT evolving to become the operating system for your life: a fully connected hub that helps you manage your day and achieve your long-term goals,” Simo said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The big question for OpenAI is how to make people, whose default browser is Chrome, Safari, or Edge, switch to its own browser and get some market share out of Google, Apple, and Microsoft’s hands. OpenAI is seeing steady growth in the number of people using ChatGPT. But it is not clear whether an average user would want to mix their browser and chatbot experience just yet. Chrome succeeded because it was fast, and people wanted to use Google queries as the default starting experience of the internet. ChatGPT Atlas is perfect for users who have replaced Google with ChatGPT, but to replace Chrome, OpenAI needs to make sure that billions of users fall into that habit.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI unveiled its AI browser, ChatGPT Atlas, during a livestream on Tuesday. There are other AI browsers such as The Browser Company’s Dia, Opera’s Neon, Perplexity’s Comet, and General Catalyst-backed Strawberry. OpenAI’s launch is notable because of the sheer scale of reaching potentially 800 million of its weekly ChatGPT users. For the company, the browser is much more about keeping ChatGPT central than about making web browsing better.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Atlas is currently available only on the Mac, the company is already working on bringing it to Windows, iOS, and Android — all the surfaces where ChatGPT already exists. OpenAI has also made the browser available to all users instead of opting for an invite system like its rivals. The core proposition of the browser is for you to think of ChatGPT as the first interaction surface for search and answers instead of Google.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;All the AI browsers share a similar idea about search and Q&amp;amp;A. Instead of performing a search query, you would type something in your address bar to get answers from an AI chatbot, instead of looking at pages of links.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And OpenAI, just like other browser makers, thinks that Atlas will change the way you browse the web, as Sam Altman made clear at the launch. “We think AI represents once in a decade opportunity to rethink what a browser can be, how to use one, and how to most productively use the web. Tabs were great but there hasn’t been a lot of innovation since then,” Altman said in his opening speech.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tech leaders, including Sundar Pichai and Satya Nadella, have talked about AI as a platform shift. However, for consumers, phones and desktop operating systems are still the primary way to get to their AI tools. OpenAI wants to own the pipes of distribution of ChatGPT as much as it can. Last week, Meta shut its doors to third-party chatbots, including ChatGPT and Perplexity on WhatsApp, which has over 3 billion monthly users. This essentially means that the platform owners could put the brakes on distribution at any point in time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For OpenAI, Atlas will also present an opportunity to deeply integrate ChatGPT and other products better than other platforms can. Users can directly reference multiple websites instead of posting links to ChatGPT. The company already uses a headless browser for its agent. With Atlas, it might have more control over the feature. It has already integrated a hovering writing assistant that shows up in text fields.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3060764" height="167" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-22-at-5.01.03PM.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;What’s more, the company is working on integrating its App SDK, which lets you call other apps within ChatGPT, to improve discoverability. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The memory feature is also key for ChatGPT’s power users. The feature takes into account the browsing history, along with your ChatGPT history, to provide answers with that context in mind. You can ask, “What was the work document I had my presentation plan on?” and ChatGPT will fetch that link for you. This also means that ChatGPT gets more context about you as you spend more time in the browser. OpenAI can use this context and provide it to other apps when it starts rolling out Sign in with ChatGPT widely.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both features — making ChatGPT the default search option and enabling memory — are designed to gather more user data, giving OpenAI greater insight into user behavior and enabling better product development. The browser doesn’t have an ad-blocker, a VPN, a reading mode, or a translate feature to make the browsing experience better for a site. Rather, users have to ask ChatGPT to summarize content or find something on a page — as if opening a page is designed to give ChatGPT more context rather than to help users consume the content on the page.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In contrast, The Browser Company’s Arc has some useful ideas around revamping the browser experience, like using AI to rename downloaded files or customize a web page by letting you remove elements.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3060766" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/Memory.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;OpenAI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The result is more than a browser; it’s a broader canvas for ChatGPT itself. OpenAI’s CEO of applications, Fidji Simo, laid out this idea in her blog outlining the Atlas launch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“When we first released ChatGPT, we weren’t sure how people would use it. Now that we have feedback and signals from hundreds of millions of people around the world, it’s clear ChatGPT needs to become so much more than the simple chatbot it started as. Over time, we see ChatGPT evolving to become the operating system for your life: a fully connected hub that helps you manage your day and achieve your long-term goals,” Simo said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The big question for OpenAI is how to make people, whose default browser is Chrome, Safari, or Edge, switch to its own browser and get some market share out of Google, Apple, and Microsoft’s hands. OpenAI is seeing steady growth in the number of people using ChatGPT. But it is not clear whether an average user would want to mix their browser and chatbot experience just yet. Chrome succeeded because it was fast, and people wanted to use Google queries as the default starting experience of the internet. ChatGPT Atlas is perfect for users who have replaced Google with ChatGPT, but to replace Chrome, OpenAI needs to make sure that billions of users fall into that habit.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/22/openais-atlas-is-more-about-chatgpt-than-the-web/</guid><pubDate>Wed, 22 Oct 2025 18:51:43 +0000</pubDate></item><item><title>[NEW] Five with MIT ties elected to National Academy of Medicine for 2025 (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/mit-affiliates-elected-national-academy-medicine-1022</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202510/batista-katabi-nam-2025_1.png" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;On Oct. 20 during its annual meeting, the National Academy of Medicine announced the election of 100 new members, including MIT faculty members Dina Katabi and Facundo Batista, along with three additional MIT alumni.&lt;/p&gt;&lt;p&gt;Election to the National Academy of Medicine (NAM) is considered one of the highest honors in the fields of health and medicine, recognizing individuals who have demonstrated outstanding professional achievement and commitment to service.&lt;/p&gt;&lt;p&gt;Facundo Batista is the associate director and scientific director of the Ragon Institute of MGH, MIT and Harvard, as well as the first Phillip T. and Susan M. Ragon Professor in the MIT Department of Biology. The National Academy of Medicine recognized Batista for “his work unraveling the biology of antibody-producing B cells to better understand how our body’s immune systems responds to infectious disease.” More recently, Facundo’s research has advanced preclinical vaccine and therapeutic development for globally important diseases including HIV, malaria, and influenza.&lt;/p&gt;&lt;p&gt;Batista earned a PhD from the International School of Advanced Studies and established his lab in 2002 as a member of the Francis Crick Institute (formerly the London Research Institute), simultaneously holding a professorship at Imperial College London. In 2016, he joined the Ragon Institute to pursue a new research program applying his expertise in B cells and antibody responses to vaccine development, and preclinical vaccinology for diseases including SARS-CoV-2 and HIV. Batista is an elected fellow or member of the U.K. Academy of Medical Sciences, the American Academy of Microbiology, the Academia de Ciencias de América Latina, and the European Molecular Biology Organization, and he is chief editor of &lt;em&gt;The EMBO Journal&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;Dina Katabi SM ’99, PhD ’03 is the Thuan (1990) and Nicole Pham Professor in the Department of Electrical Engineering and Computer Science at MIT. Her research spans digital health, wireless sensing, mobile computing, machine learning, and computer vision. Katabi’s contributions include efficient communication protocols for the internet, advanced contactless biosensors, and novel AI models that interpret physiological signals. The NAM recognized Katabi for “pioneering digital health technology that enables non-invasive, off-body remote health monitoring via AI and wireless signals, and for developing digital biomarkers for Parkinson’s progression and detection. She has translated this technology to advance objective, sensitive measures of disease trajectory and treatment response in clinical trials.”&lt;/p&gt;&lt;p&gt;Katabi is director of the MIT Center for Wireless Networks and Mobile Computing. She is also a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL), where she leads the Networks at MIT Research Group. Katabi received a bachelor’s degree from the University of Damascus and MS and PhD degrees in computer science from MIT. She is a MacArthur Fellow; a member of the American Academy of Arts and Sciences, National Academy of Sciences, and National Academy of Engineering; and a recipient of the ACM Computing Prize.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Additional MIT alumni who were elected to the NAM for 2025 are:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Christopher S. Chen SM ’93, PhD ’97, an alumnus of the Department of Mechanical Engineering and the Harvard-MIT Program in Health Sciences and Technology;&lt;br /&gt;&amp;nbsp;&lt;/li&gt;&lt;li&gt;Michael E. Matheny SM ’06, an alumnus of the Harvard-MIT Program in Health Sciences and Technology; and&lt;br /&gt;&amp;nbsp;&lt;/li&gt;&lt;li&gt;Rebecca R. Richards-Kortum SM ’87, PhD ’90, and alumna of the Department of Physics and the Harvard-MIT Program in Health Sciences and Technology.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Established originally as the Institute of Medicine in 1970 by the National Academy of Sciences, the National Academy of Medicine addresses critical issues in health, science, medicine, and related policy, and inspires positive actions across sectors.&lt;/p&gt;&lt;p&gt;“I am deeply honored to welcome these extraordinary health and medicine leaders and researchers into the National Academy of Medicine,” says NAM President Victor J. Dzau. “Their demonstrated excellence in tackling public health challenges, leading major discoveries, improving health care, advancing health policy, and addressing health equity will critically strengthen our collective ability to tackle the most pressing health challenges of our time.”&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202510/batista-katabi-nam-2025_1.png" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;On Oct. 20 during its annual meeting, the National Academy of Medicine announced the election of 100 new members, including MIT faculty members Dina Katabi and Facundo Batista, along with three additional MIT alumni.&lt;/p&gt;&lt;p&gt;Election to the National Academy of Medicine (NAM) is considered one of the highest honors in the fields of health and medicine, recognizing individuals who have demonstrated outstanding professional achievement and commitment to service.&lt;/p&gt;&lt;p&gt;Facundo Batista is the associate director and scientific director of the Ragon Institute of MGH, MIT and Harvard, as well as the first Phillip T. and Susan M. Ragon Professor in the MIT Department of Biology. The National Academy of Medicine recognized Batista for “his work unraveling the biology of antibody-producing B cells to better understand how our body’s immune systems responds to infectious disease.” More recently, Facundo’s research has advanced preclinical vaccine and therapeutic development for globally important diseases including HIV, malaria, and influenza.&lt;/p&gt;&lt;p&gt;Batista earned a PhD from the International School of Advanced Studies and established his lab in 2002 as a member of the Francis Crick Institute (formerly the London Research Institute), simultaneously holding a professorship at Imperial College London. In 2016, he joined the Ragon Institute to pursue a new research program applying his expertise in B cells and antibody responses to vaccine development, and preclinical vaccinology for diseases including SARS-CoV-2 and HIV. Batista is an elected fellow or member of the U.K. Academy of Medical Sciences, the American Academy of Microbiology, the Academia de Ciencias de América Latina, and the European Molecular Biology Organization, and he is chief editor of &lt;em&gt;The EMBO Journal&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;Dina Katabi SM ’99, PhD ’03 is the Thuan (1990) and Nicole Pham Professor in the Department of Electrical Engineering and Computer Science at MIT. Her research spans digital health, wireless sensing, mobile computing, machine learning, and computer vision. Katabi’s contributions include efficient communication protocols for the internet, advanced contactless biosensors, and novel AI models that interpret physiological signals. The NAM recognized Katabi for “pioneering digital health technology that enables non-invasive, off-body remote health monitoring via AI and wireless signals, and for developing digital biomarkers for Parkinson’s progression and detection. She has translated this technology to advance objective, sensitive measures of disease trajectory and treatment response in clinical trials.”&lt;/p&gt;&lt;p&gt;Katabi is director of the MIT Center for Wireless Networks and Mobile Computing. She is also a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL), where she leads the Networks at MIT Research Group. Katabi received a bachelor’s degree from the University of Damascus and MS and PhD degrees in computer science from MIT. She is a MacArthur Fellow; a member of the American Academy of Arts and Sciences, National Academy of Sciences, and National Academy of Engineering; and a recipient of the ACM Computing Prize.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Additional MIT alumni who were elected to the NAM for 2025 are:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Christopher S. Chen SM ’93, PhD ’97, an alumnus of the Department of Mechanical Engineering and the Harvard-MIT Program in Health Sciences and Technology;&lt;br /&gt;&amp;nbsp;&lt;/li&gt;&lt;li&gt;Michael E. Matheny SM ’06, an alumnus of the Harvard-MIT Program in Health Sciences and Technology; and&lt;br /&gt;&amp;nbsp;&lt;/li&gt;&lt;li&gt;Rebecca R. Richards-Kortum SM ’87, PhD ’90, and alumna of the Department of Physics and the Harvard-MIT Program in Health Sciences and Technology.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Established originally as the Institute of Medicine in 1970 by the National Academy of Sciences, the National Academy of Medicine addresses critical issues in health, science, medicine, and related policy, and inspires positive actions across sectors.&lt;/p&gt;&lt;p&gt;“I am deeply honored to welcome these extraordinary health and medicine leaders and researchers into the National Academy of Medicine,” says NAM President Victor J. Dzau. “Their demonstrated excellence in tackling public health challenges, leading major discoveries, improving health care, advancing health policy, and addressing health equity will critically strengthen our collective ability to tackle the most pressing health challenges of our time.”&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/mit-affiliates-elected-national-academy-medicine-1022</guid><pubDate>Wed, 22 Oct 2025 19:25:00 +0000</pubDate></item><item><title>[NEW] When sycophancy and bias meet medicine (AI – Ars Technica)</title><link>https://arstechnica.com/health/2025/10/when-sycophancy-and-bias-meet-medicine/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Biased, eager-to-please models threaten health research replicability and trust.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A woman is standing behind a drawer filled with prescription medicines. She is wearing a white lab coat." class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2214872699-640x427.jpg" width="640" /&gt;
                  &lt;img alt="A woman is standing behind a drawer filled with prescription medicines. She is wearing a white lab coat." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2214872699-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Catherine Delahaye

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Once upon a time, two villagers visited the fabled Mullah Nasreddin. They hoped that the Sufi philosopher, famed for his acerbic wisdom, could mediate a dispute that had driven a wedge between them. Nasreddin listened patiently to the first villager’s version of the story and, upon its conclusion, exclaimed, “You are absolutely right!” The second villager then presented his case. After hearing him out, Nasreddin again responded, “You are absolutely right!” An observant bystander, confused by Nasreddin’s proclamations, interjected, “But Mullah, they can’t both be right.” Nasreddin paused, regarding the bystander for a moment before replying, “You are absolutely right, too!”&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In late May, the White House’s first “Make America Healthy Again” (MAHA) report was criticized for citing multiple research studies that did not exist. Fabricated citations like these are common in the outputs of generative artificial intelligence based on large language models, or LLMs. LLMs have presented plausible-sounding sources, catchy titles, or even false data to craft their conclusions. Here, the White House pushed back on the journalists who first broke the story before admitting to “minor citation errors.”&lt;/p&gt;
&lt;p&gt;It is ironic that fake citations were used to support a principal recommendation of the MAHA report: addressing the health research sector’s “replication crisis,” wherein scientists’ findings often cannot be reproduced by other independent teams.&lt;/p&gt;
&lt;p&gt;Yet the MAHA report’s use of phantom evidence is far from unique. Last year, The Washington Post reported on dozens of instances in which AI-generated falsehoods found their way into courtroom proceedings. Once uncovered, lawyers had to explain to judges how fictitious cases, citations, and decisions found their way into trials.&lt;/p&gt;
&lt;p&gt;Despite these widely recognized problems, the MAHA roadmap released last month directs the Department of Health and Human Services to prioritize AI research to “…assist in earlier diagnosis, personalized treatment plans, real-time monitoring, and predictive interventions…” This breathless rush to embed AI in so many aspects of medicine could be forgiven if we believe that the technology’s “hallucinations” will be easy to fix through version updates. But as the industry itself acknowledges, these ghosts in the machine may be impossible to eliminate.&lt;/p&gt;
&lt;p&gt;Consider the implications of accelerating AI use in health research for clinical decision making. Beyond the problems we’re seeing here, using AI in research without disclosure could create a feedback loop, supercharging the very biases that helped motivate its use. Once published, “research” based on false results and citations could become part of the datasets used to build future AI systems. Worse still, a recently published study highlights an industry of scientific fraudsters who could deploy AI to make their claims seem more legitimate.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In other words, a blind adoption of AI risks a downward spiral, where today’s flawed AI outputs become tomorrow’s training data, exponentially eroding research quality.&lt;/p&gt;
&lt;h2&gt;Three prongs of AI misuse&lt;/h2&gt;
&lt;p&gt;The challenge AI poses is threefold: hallucination, sycophancy, and the black box conundrum. Understanding these phenomena is critical for research scientists, policymakers, educators, and everyday citizens. Unaware, we risk vulnerability to deception as AI systems are increasingly deployed to shape diagnoses, insurance claims, health literacy, research, and public policy.&lt;/p&gt;
&lt;p&gt;Here’s how hallucination works: When a user inputs a query into an AI tool such as ChatGPT or Gemini, the model evaluates the input and generates a string of words that is statistically likely to make sense based on its training data. Current AI models will complete this task even if their training data is incomplete or biased, filling in the blanks regardless of their ability to answer. These hallucinations can take the form of nonexistent research studies, misinformation, or even clinical interactions that never happened. LLMs’ emphasis on producing authoritative-sounding language shrouds their false outputs in a facsimile of truth.&lt;/p&gt;
&lt;p&gt;And as human model trainers fine-tune generative AI responses, they tend to optimize and reward the AI system responses that favor their prior beliefs, leading to sycophancy. Human bias, it appears, begets AI bias, and human users of AI then perpetuate the cycle. A consequence is that AIs skew toward favoring pleasing answers over truthful ones, often seeking to reinforce the bias of the query.&lt;/p&gt;
&lt;p&gt;A recent illustration of this occurred in April, when OpenAI canceled a ChatGPT update for being too sycophantic after users demonstrated that it agreed too quickly and enthusiastically with the assumptions embedded in users’ queries. Sycophancy and hallucination often interact with each other; systems that aim to please will be more apt to fabricate data to reach user-preferred conclusions.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Correcting hallucinations, sycophancy, and other LLM mishaps is cumbersome because human observers can’t always determine how an AI platform arrived at its conclusions. This is the “black box” problem. Behind the probabilistic mathematics, is it even testing hypotheses? What methods did it use to derive an answer? Unlike traditional computer code or the rubric of scientific methodology, AI models operate through billions of computations. Looking at some well-structured outputs, it is easy to forget that the underlying processes are impenetrable to scrutiny and vastly different from a human’s approach to problem-solving.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;This opacity can become dangerous when people can’t identify where computations went wrong, making it impossible to correct systematic errors or biases in the decision-making process. In health care, this black box raises questions about accountability, liability, and trust when neither physicians nor patients can explain the sequence of reasoning that leads to a medical intervention.&lt;/p&gt;
&lt;h2&gt;AI and health research&lt;/h2&gt;
&lt;p&gt;These AI challenges can exacerbate the existing sources of error and bias that creep into traditional health research publications. Several sources originate from the natural human motivation to find and publish meaningful, positive results. Journalists want to report on connections, e.g., that St. John’s Wort improves mood (it might). Nobody would want to publish an article with the results: “the supplement has no significant effect.”&lt;/p&gt;
&lt;p&gt;The problem compounds when researchers use a study design to test not just a single hypothesis but many. One quirk of statistics-backed research is that testing more hypotheses in a single study raises the likelihood of uncovering a spurious coincidence.&lt;/p&gt;
&lt;p&gt;AI has the potential to supercharge these coincidences through its relentless ability to test hypotheses across massive datasets. In the past, a research assistant could use an existing dataset to test 10 to 20 of the most likely hypotheses; now, that assistant can set an AI loose to test millions of likely or unlikely hypotheses without human supervision. That all but guarantees some of the results will meet the criteria for statistical significance, regardless of whether the data includes any real biological effects.&lt;/p&gt;
&lt;p&gt;AI’s tireless capacity to investigate data, combined with its growing ability to develop authoritative-sounding narratives, expands the potential to elevate fabricated or bias-confirming errors into the collective public consciousness.&lt;/p&gt;
&lt;h2&gt;What’s next?&lt;/h2&gt;
&lt;p&gt;If you read the missives of AI luminaries, it would appear that society is on the cusp of superintelligence, which will transform every vexing societal conundrum into a trivial puzzle. While that’s highly unlikely, AI has certainly demonstrated promise in some health applications, despite its limitations. Unfortunately, it’s now being rapidly deployed sector-wide, even in areas where it has no prior track record.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;This speed may leave us little time to reflect on the accountability needed for safe deployment. Sycophancy, hallucination, and the black box of AI are non-trivial challenges when conjoined with existing biases in health research. If people can’t easily understand the inner workings of current AI tools (often comprising up to 1.8 trillion parameters), they will not be able to understand the process of future, more complex versions (using over 5 trillion parameters).&lt;/p&gt;
&lt;p&gt;History shows that most technological leaps forward are double-edged swords. Electronic health records increased the ability of clinicians to improve care coordination and aggregate data on population health, but they have eroded doctor-patient interactions and have become a source of physician burnout. The recent proliferation of telemedicine has expanded access to care, but it has also promoted lower-quality interactions with no physical examination.&lt;/p&gt;
&lt;p&gt;The use of AI in health policy and research is no different. Wisely deployed, it could transform the health sector, leading to healthier populations and unfathomable breakthroughs (for example, by accelerating drug discovery). But without embedding it in new professional norms and practices, it has the potential to generate countless flawed leads and falsehoods.&lt;/p&gt;
&lt;p&gt;Here are some potential solutions we see to the AI and health replicability crisis:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Clinical-specific models capable of admitting uncertainty in their outputs&lt;/li&gt;
&lt;li&gt;Greater transparency, requiring disclosure of AI model use in research&lt;/li&gt;
&lt;li&gt;Training for researchers, clinicians, and journalists on how to evaluate and stress-test AI-derived conclusions&lt;/li&gt;
&lt;li&gt;Pre-registered hypotheses and analysis plans before using AI tools&lt;/li&gt;
&lt;li&gt;AI audit trails&lt;/li&gt;
&lt;li&gt;Specific AI global prompts that limit sycophantic tendencies across user queries&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Regardless of the solutions deployed, we need to solve the failure points described here to fully realize the potential of AI for use in health research. The public, AI companies, and health researchers must be active participants in this journey. After all, in science, not everyone can be right.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Amit Chandra&lt;/strong&gt; is an emergency physician and global health policy specialist based in Washington, DC. He is an adjunct professor of global health at Georgetown University’s School of Health, where he has explored AI solutions for global health challenges since 2021.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Luke Shors&lt;/strong&gt; is an entrepreneur who focuses on energy, climate, and global health. He is the co-founder of the sustainability company Capture6 and previously worked on topics including computer vision and blockchain.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;








  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Biased, eager-to-please models threaten health research replicability and trust.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A woman is standing behind a drawer filled with prescription medicines. She is wearing a white lab coat." class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2214872699-640x427.jpg" width="640" /&gt;
                  &lt;img alt="A woman is standing behind a drawer filled with prescription medicines. She is wearing a white lab coat." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2214872699-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Catherine Delahaye

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Once upon a time, two villagers visited the fabled Mullah Nasreddin. They hoped that the Sufi philosopher, famed for his acerbic wisdom, could mediate a dispute that had driven a wedge between them. Nasreddin listened patiently to the first villager’s version of the story and, upon its conclusion, exclaimed, “You are absolutely right!” The second villager then presented his case. After hearing him out, Nasreddin again responded, “You are absolutely right!” An observant bystander, confused by Nasreddin’s proclamations, interjected, “But Mullah, they can’t both be right.” Nasreddin paused, regarding the bystander for a moment before replying, “You are absolutely right, too!”&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In late May, the White House’s first “Make America Healthy Again” (MAHA) report was criticized for citing multiple research studies that did not exist. Fabricated citations like these are common in the outputs of generative artificial intelligence based on large language models, or LLMs. LLMs have presented plausible-sounding sources, catchy titles, or even false data to craft their conclusions. Here, the White House pushed back on the journalists who first broke the story before admitting to “minor citation errors.”&lt;/p&gt;
&lt;p&gt;It is ironic that fake citations were used to support a principal recommendation of the MAHA report: addressing the health research sector’s “replication crisis,” wherein scientists’ findings often cannot be reproduced by other independent teams.&lt;/p&gt;
&lt;p&gt;Yet the MAHA report’s use of phantom evidence is far from unique. Last year, The Washington Post reported on dozens of instances in which AI-generated falsehoods found their way into courtroom proceedings. Once uncovered, lawyers had to explain to judges how fictitious cases, citations, and decisions found their way into trials.&lt;/p&gt;
&lt;p&gt;Despite these widely recognized problems, the MAHA roadmap released last month directs the Department of Health and Human Services to prioritize AI research to “…assist in earlier diagnosis, personalized treatment plans, real-time monitoring, and predictive interventions…” This breathless rush to embed AI in so many aspects of medicine could be forgiven if we believe that the technology’s “hallucinations” will be easy to fix through version updates. But as the industry itself acknowledges, these ghosts in the machine may be impossible to eliminate.&lt;/p&gt;
&lt;p&gt;Consider the implications of accelerating AI use in health research for clinical decision making. Beyond the problems we’re seeing here, using AI in research without disclosure could create a feedback loop, supercharging the very biases that helped motivate its use. Once published, “research” based on false results and citations could become part of the datasets used to build future AI systems. Worse still, a recently published study highlights an industry of scientific fraudsters who could deploy AI to make their claims seem more legitimate.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In other words, a blind adoption of AI risks a downward spiral, where today’s flawed AI outputs become tomorrow’s training data, exponentially eroding research quality.&lt;/p&gt;
&lt;h2&gt;Three prongs of AI misuse&lt;/h2&gt;
&lt;p&gt;The challenge AI poses is threefold: hallucination, sycophancy, and the black box conundrum. Understanding these phenomena is critical for research scientists, policymakers, educators, and everyday citizens. Unaware, we risk vulnerability to deception as AI systems are increasingly deployed to shape diagnoses, insurance claims, health literacy, research, and public policy.&lt;/p&gt;
&lt;p&gt;Here’s how hallucination works: When a user inputs a query into an AI tool such as ChatGPT or Gemini, the model evaluates the input and generates a string of words that is statistically likely to make sense based on its training data. Current AI models will complete this task even if their training data is incomplete or biased, filling in the blanks regardless of their ability to answer. These hallucinations can take the form of nonexistent research studies, misinformation, or even clinical interactions that never happened. LLMs’ emphasis on producing authoritative-sounding language shrouds their false outputs in a facsimile of truth.&lt;/p&gt;
&lt;p&gt;And as human model trainers fine-tune generative AI responses, they tend to optimize and reward the AI system responses that favor their prior beliefs, leading to sycophancy. Human bias, it appears, begets AI bias, and human users of AI then perpetuate the cycle. A consequence is that AIs skew toward favoring pleasing answers over truthful ones, often seeking to reinforce the bias of the query.&lt;/p&gt;
&lt;p&gt;A recent illustration of this occurred in April, when OpenAI canceled a ChatGPT update for being too sycophantic after users demonstrated that it agreed too quickly and enthusiastically with the assumptions embedded in users’ queries. Sycophancy and hallucination often interact with each other; systems that aim to please will be more apt to fabricate data to reach user-preferred conclusions.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Correcting hallucinations, sycophancy, and other LLM mishaps is cumbersome because human observers can’t always determine how an AI platform arrived at its conclusions. This is the “black box” problem. Behind the probabilistic mathematics, is it even testing hypotheses? What methods did it use to derive an answer? Unlike traditional computer code or the rubric of scientific methodology, AI models operate through billions of computations. Looking at some well-structured outputs, it is easy to forget that the underlying processes are impenetrable to scrutiny and vastly different from a human’s approach to problem-solving.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;This opacity can become dangerous when people can’t identify where computations went wrong, making it impossible to correct systematic errors or biases in the decision-making process. In health care, this black box raises questions about accountability, liability, and trust when neither physicians nor patients can explain the sequence of reasoning that leads to a medical intervention.&lt;/p&gt;
&lt;h2&gt;AI and health research&lt;/h2&gt;
&lt;p&gt;These AI challenges can exacerbate the existing sources of error and bias that creep into traditional health research publications. Several sources originate from the natural human motivation to find and publish meaningful, positive results. Journalists want to report on connections, e.g., that St. John’s Wort improves mood (it might). Nobody would want to publish an article with the results: “the supplement has no significant effect.”&lt;/p&gt;
&lt;p&gt;The problem compounds when researchers use a study design to test not just a single hypothesis but many. One quirk of statistics-backed research is that testing more hypotheses in a single study raises the likelihood of uncovering a spurious coincidence.&lt;/p&gt;
&lt;p&gt;AI has the potential to supercharge these coincidences through its relentless ability to test hypotheses across massive datasets. In the past, a research assistant could use an existing dataset to test 10 to 20 of the most likely hypotheses; now, that assistant can set an AI loose to test millions of likely or unlikely hypotheses without human supervision. That all but guarantees some of the results will meet the criteria for statistical significance, regardless of whether the data includes any real biological effects.&lt;/p&gt;
&lt;p&gt;AI’s tireless capacity to investigate data, combined with its growing ability to develop authoritative-sounding narratives, expands the potential to elevate fabricated or bias-confirming errors into the collective public consciousness.&lt;/p&gt;
&lt;h2&gt;What’s next?&lt;/h2&gt;
&lt;p&gt;If you read the missives of AI luminaries, it would appear that society is on the cusp of superintelligence, which will transform every vexing societal conundrum into a trivial puzzle. While that’s highly unlikely, AI has certainly demonstrated promise in some health applications, despite its limitations. Unfortunately, it’s now being rapidly deployed sector-wide, even in areas where it has no prior track record.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;This speed may leave us little time to reflect on the accountability needed for safe deployment. Sycophancy, hallucination, and the black box of AI are non-trivial challenges when conjoined with existing biases in health research. If people can’t easily understand the inner workings of current AI tools (often comprising up to 1.8 trillion parameters), they will not be able to understand the process of future, more complex versions (using over 5 trillion parameters).&lt;/p&gt;
&lt;p&gt;History shows that most technological leaps forward are double-edged swords. Electronic health records increased the ability of clinicians to improve care coordination and aggregate data on population health, but they have eroded doctor-patient interactions and have become a source of physician burnout. The recent proliferation of telemedicine has expanded access to care, but it has also promoted lower-quality interactions with no physical examination.&lt;/p&gt;
&lt;p&gt;The use of AI in health policy and research is no different. Wisely deployed, it could transform the health sector, leading to healthier populations and unfathomable breakthroughs (for example, by accelerating drug discovery). But without embedding it in new professional norms and practices, it has the potential to generate countless flawed leads and falsehoods.&lt;/p&gt;
&lt;p&gt;Here are some potential solutions we see to the AI and health replicability crisis:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Clinical-specific models capable of admitting uncertainty in their outputs&lt;/li&gt;
&lt;li&gt;Greater transparency, requiring disclosure of AI model use in research&lt;/li&gt;
&lt;li&gt;Training for researchers, clinicians, and journalists on how to evaluate and stress-test AI-derived conclusions&lt;/li&gt;
&lt;li&gt;Pre-registered hypotheses and analysis plans before using AI tools&lt;/li&gt;
&lt;li&gt;AI audit trails&lt;/li&gt;
&lt;li&gt;Specific AI global prompts that limit sycophantic tendencies across user queries&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Regardless of the solutions deployed, we need to solve the failure points described here to fully realize the potential of AI for use in health research. The public, AI companies, and health researchers must be active participants in this journey. After all, in science, not everyone can be right.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Amit Chandra&lt;/strong&gt; is an emergency physician and global health policy specialist based in Washington, DC. He is an adjunct professor of global health at Georgetown University’s School of Health, where he has explored AI solutions for global health challenges since 2021.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Luke Shors&lt;/strong&gt; is an entrepreneur who focuses on energy, climate, and global health. He is the co-founder of the sustainability company Capture6 and previously worked on topics including computer vision and blockchain.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;








  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/health/2025/10/when-sycophancy-and-bias-meet-medicine/</guid><pubDate>Wed, 22 Oct 2025 19:46:01 +0000</pubDate></item><item><title>[NEW] ‘The Next Generation of Compute Is Driving AI,’ Technology Leader Says at NVIDIA AI Day Sydney (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/ai-day-sydney/</link><description>&lt;!-- OneTrust Cookies Consent Notice start for nvidia.com --&gt;


&lt;!-- OneTrust Cookies Consent Notice end for nvidia.com --&gt;


	
	
	
	
	
	

	

	
	
	


	&lt;!-- This site is optimized with the Yoast SEO Premium plugin v26.2 (Yoast SEO v26.2) - https://yoast.com/wordpress/plugins/seo/ --&gt;
	‘The Next Generation of Compute Is Driving AI,’ Technology Leader Says at NVIDIA AI Day Sydney | NVIDIA Blog
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	&lt;!-- / Yoast SEO Premium plugin. --&gt;






































&lt;!-- Stream WordPress user activity plugin v4.1.1 --&gt;


	
				&lt;!-- Hotjar Tracking Code for NVIDIA --&gt;
			
			


				
				



		
		

&lt;div class="hfeed site" id="page"&gt;
	Skip to content

	&lt;!-- #masthead --&gt;
		
		&lt;div class="full-width-layout light"&gt;
		

		
&lt;div class="full-width-layout__hero light"&gt;
	&lt;div class="full-width-layout__hero-content light"&gt;
		&lt;div class="full-width-layout__hero-content__inner light"&gt;
			

							&lt;p&gt;
					Brendan Hopper, chief information officer for technology at the Commonwealth Bank of Australia, was among the attendees of last week’s event showcasing sovereign AI in the land down under.				&lt;/p&gt;
			
			
		&lt;/div&gt;
	&lt;/div&gt;

	

	&lt;/div&gt;

	
	
		&lt;div class="full-width-layout__sections"&gt;
&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This post is part of a blog series highlighting &lt;/i&gt;&lt;i&gt;NVIDIA AI Days&lt;/i&gt;&lt;i&gt; across the globe.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;NVIDIA AI Days — hosted for and in different pockets of the world — are drawing in thousands of enthusiasts, developers, researchers and startups to discuss and explore the latest technologies making AI breakthroughs possible.&lt;/p&gt;
&lt;p&gt;Sydney, Australia, was the latest stop.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="1366" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/ai-day-sydney-registration-scaled.jpeg" width="2048" /&gt;	
	
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			Attendees check in at NVIDIA AI Day Sydney, which took place Oct. 15-16 at ICC Sydney Theatre.		&lt;/span&gt;
	
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Last week, over 1,000 attendees joined NVIDIA AI Day Sydney to learn about sovereign AI — including 16 breakout sessions on agentic and physical AI, robotics and AI factories. These technologies are enabling the next frontier of AI.&lt;/p&gt;
&lt;p&gt;“It was a privilege to join NVIDIA AI Day Sydney and hear how the next generation of compute is driving AI,” said Brendan Hopper, chief information officer for technology at the Commonwealth Bank of Australia. “It’s phenomenal to have NVIDIA helping build out Australia’s AI ecosystem through infrastructure, partnerships and innovation.”&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="819" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/ai-day-sydney-brendan-hopper-pull-quote-scaled.jpg" width="2048" /&gt;	
	&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;Why It Matters&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;The latest AI trends in Australia showcase how the technology is driving digital transformation across financial services, retail and the public sector, among other industries.&lt;/p&gt;
&lt;p&gt;“AI Day Sydney highlighted how quantum and high-performance computing, empowered by AI, are redefining the pace and precision of scientific discovery,” said Giuseppe M. J. Barca, cofounder and head of research at QDX Technologies, and a professor at Monash University and the Australian National University.&lt;/p&gt;
&lt;p&gt;The Australian AI ecosystem is growing:&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__full-width-image-section"&gt;
			&lt;img alt="Infographic that states there are 600+ Australia-based NVIDIA Inception startups, three Australia-based quantum computing companies working with NVIDIA, and 20+ higher-education institutions in Australia using NVIDIA technologies for research." class="full-width-layout__image" height="819" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/sydney-ecosystem-infographic-scaled.jpg" width="2048" /&gt;	
	&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;“NVIDIA AI Day Sydney was by far the best to date technology event that XENON has attended in Australia,” said Dragan Dimitrovici, founder and CEO of XENON Systems, an Australian pioneer in high-performance computing, AI and deep learning. “It brought together key NVIDIA executives, industry leaders like Canva and Commonwealth Bank of Australia, NVIDIA Inception startups, ecosystem partners, the public sector and finance organizations to talk about how AI can help make Australia into an AI nation with the advent of AI factories.”&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;Industry in Motion&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;NVIDIA partners in Australia are advancing the nation’s role in the next industrial revolution fueled by AI.&lt;/p&gt;
&lt;p&gt;Australia-based graphic design platform provider Canva, which is working with NVIDIA to develop generative and agentic AI solutions for its hundreds of millions of users, presented on training video foundation models and curating video data at AI Day Sydney.&lt;/p&gt;
&lt;p&gt;“Canva is on a mission to empower everyone in the world to design anything and publish anywhere,” said Paul Thompson, senior director of engineering for generative AI at Canva. “We make extensive use of NVIDIA technologies to train, fine-tune and serve AI features to millions of users.”&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="1365" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/ai-day-sydney-video-flywheels-session-scaled.jpeg" width="2048" /&gt;	
	
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			An NVIDIA AI Day Sydney session covered video model flywheels and tackling large-scale data challenges.		&lt;/span&gt;
	
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;NVIDIA Cloud Partners including Firmus Technologies, ResetData and SHARON AI also attended the event.&lt;/p&gt;
&lt;p&gt;“NVIDIA AI Day Sydney proved that Australia is ready to step into its place as a global hub for AI infrastructure and innovation,” said Tim Rosenfield, co-CEO and cofounder of Firmus Technologies. “Australia’s time has come to go large in the AI era — and AI Day Sydney told that story in a way that I’ve never seen before.”&lt;/p&gt;
&lt;p&gt;“NVIDIA AI Day was a fantastic showcase for Australian AI talent and a welcomed opportunity to develop and grow the Australian ecosystem,” said Bass Salah, co-CEO of ResetData.&lt;/p&gt;
&lt;p&gt;“The energy, ideation and talent at NVIDIA AI Day Sydney were all remarkable,” added Andrew Leece, chief operating officer at SHARON AI. “SHARON AI is thrilled to be at the center of such an opportunity for Australian startups and innovators.”&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;Other Event Highlights&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;At AI Day Sydney, NVIDIA hosted its first-ever “Australia Startup, VC and Partner Connect” event, bringing together leading startups, industry leaders and government representatives to drive the region’s AI strategy, unlock opportunities for innovation and spark cross-sector collaboration. Venture capital companies including Airtree and Breakthrough Victoria attended to connect with startups.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__full-width-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="1365" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/ai-day-sydney-startup-session-1-scaled.jpeg" width="2048" /&gt;	
	
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			Panelists at the special event for Australian startups, venture capital companies and partners.		&lt;/span&gt;
	
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Presenters included quantum computing companies Diraq, PsiQuantum, Q-CTRL and Quantum Brilliance, using the NVIDIA CUDA-Q platform, as well as Heidi Health, which uses the NVIDIA Parakeet automatic speech recognition model for its medical AI scribe.&lt;/p&gt;
&lt;p&gt;“NVIDIA AI Day was a brilliant showcase of Australian AI innovation and a great boost for the local ecosystem,” said Yu Liu, cofounder and chief technology officer at Heidi Health.&lt;/p&gt;
&lt;p&gt;Lightning talks by startups, including NVIDIA Inception members BrainFish, Eklipse.gg and Zetaris, showcased how NVIDIA Triton Inference Server, NVIDIA NIM microservices and the NVIDIA AI Blueprint for video search and summarization accelerate the development of their AI applications in customer service, content creation, data processing and more.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="1366" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/susan-marshall-ai-day-sydney-scaled.jpeg" width="2048" /&gt;	
	
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			NVIDIA Senior Director of Developer Relations Susan Marshall shared her insights on Australia’s global innovation potential during a plenary session.		&lt;/span&gt;
	
	&lt;/p&gt;
&lt;/div&gt;



&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;What’s Next&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Australia’s AI future lies where the nation’s long heritage and expertise in high-performance computing, visual effects and animation intersect with the vibrant, emerging quantum and robotics industries, said NVIDIA Country Manager for Australia and New Zealand Enterprise Sudarshan Ramachandran at the event. This is poised to drive discovery and sustainability through advanced simulation and AI technologies.&lt;/p&gt;
&lt;p&gt;“Through strong collaboration, world-class infrastructure and innovation partnerships, we’re building a thriving ecosystem that positions Australia as a global leader in AI and economic advancement,” Ramachandran said.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__full-width-image-section"&gt;
			&lt;img alt="&amp;quot;We’re building a thriving ecosystem that positions Australia as a global leader in AI and economic advancement,” said NVIDIA Country Manager for Australia and New Zealand Enterprise Sudarshan Ramachandran." class="full-width-layout__image" height="819" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/ai-day-sydney-future-pull-quote-scaled.jpg" width="2048" /&gt;	
	&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Next up: Join NVIDIA at GTC Washington, D.C., running Oct. 27-29 at the Walter E. Washington Convention Center.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Explore more content from &lt;/i&gt;&lt;i&gt;AI Days&lt;/i&gt;&lt;i&gt; around the globe.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
	&lt;div class="full-width-layout__news-section"&gt;
		&lt;p&gt;Related News&lt;/p&gt;

		
	&lt;/div&gt;
		&lt;/div&gt;
	


&lt;!-- #colophon --&gt;

&lt;/div&gt;&lt;!-- #page --&gt;


&lt;!-- #has-highlight-and-share --&gt;		&lt;svg class="hidden" height="0" width="0" xmlns="http://www.w3.org/2000/svg"&gt;
			
				&lt;g&gt;&lt;path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z" fill="currentColor"&gt;&lt;/g&gt;
			
			
				&lt;path d="M279.14 288l14.22-92.66h-88.91v-60.13c0-25.35 12.42-50.06 52.24-50.06h40.42V6.26S260.43 0 225.36 0c-73.22 0-121.08 44.38-121.08 124.72v70.62H22.89V288h81.39v224h100.17V288z" fill="currentColor"&gt;
			
			
				&lt;path d="M256 8C118.941 8 8 118.919 8 256c0 137.059 110.919 248 248 248 48.154 0 95.342-14.14 135.408-40.223 12.005-7.815 14.625-24.288 5.552-35.372l-10.177-12.433c-7.671-9.371-21.179-11.667-31.373-5.129C325.92 429.757 291.314 440 256 440c-101.458 0-184-82.542-184-184S154.542 72 256 72c100.139 0 184 57.619 184 160 0 38.786-21.093 79.742-58.17 83.693-17.349-.454-16.91-12.857-13.476-30.024l23.433-121.11C394.653 149.75 383.308 136 368.225 136h-44.981a13.518 13.518 0 0 0-13.432 11.993l-.01.092c-14.697-17.901-40.448-21.775-59.971-21.775-74.58 0-137.831 62.234-137.831 151.46 0 65.303 36.785 105.87 96 105.87 26.984 0 57.369-15.637 74.991-38.333 9.522 34.104 40.613 34.103 70.71 34.103C462.609 379.41 504 307.798 504 232 504 95.653 394.023 8 256 8zm-21.68 304.43c-22.249 0-36.07-15.623-36.07-40.771 0-44.993 30.779-72.729 58.63-72.729 22.292 0 35.601 15.241 35.601 40.77 0 45.061-33.875 72.73-58.161 72.73z" fill="currentColor"&gt;
			
			
				&lt;path d="M100.28 448H7.4V148.9h92.88zM53.79 108.1C24.09 108.1 0 83.5 0 53.8a53.79 53.79 0 0 1 107.58 0c0 29.7-24.1 54.3-53.79 54.3zM447.9 448h-92.68V302.4c0-34.7-.7-79.2-48.29-79.2-48.29 0-55.69 37.7-55.69 76.7V448h-92.78V148.9h89.08v40.8h1.3c12.4-23.5 42.69-48.3 87.88-48.3 94 0 111.28 61.9 111.28 142.3V448z" fill="currentColor"&gt;
			
			
				&lt;path d="M162.7 210c-1.8 3.3-25.2 44.4-70.1 123.5-4.9 8.3-10.8 12.5-17.7 12.5H9.8c-7.7 0-12.1-7.5-8.5-14.4l69-121.3c.2 0 .2-.1 0-.3l-43.9-75.6c-4.3-7.8.3-14.1 8.5-14.1H100c7.3 0 13.3 4.1 18 12.2l44.7 77.5zM382.6 46.1l-144 253v.3L330.2 466c3.9 7.1.2 14.1-8.5 14.1h-65.2c-7.6 0-13.6-4-18-12.2l-92.4-168.5c3.3-5.8 51.5-90.8 144.8-255.2 4.6-8.1 10.4-12.2 17.5-12.2h65.7c8 0 12.3 6.7 8.5 14.1z" fill="currentColor"&gt;
			
			
				&lt;path d="M380.9 97.1C339 55.1 283.2 32 223.9 32c-122.4 0-222 99.6-222 222 0 39.1 10.2 77.3 29.6 111L0 480l117.7-30.9c32.4 17.7 68.9 27 106.1 27h.1c122.3 0 224.1-99.6 224.1-222 0-59.3-25.2-115-67.1-157zm-157 341.6c-33.2 0-65.7-8.9-94-25.7l-6.7-4-69.8 18.3L72 359.2l-4.4-7c-18.5-29.4-28.2-63.3-28.2-98.2 0-101.7 82.8-184.5 184.6-184.5 49.3 0 95.6 19.2 130.4 54.1 34.8 34.9 56.2 81.2 56.1 130.5 0 101.8-84.9 184.6-186.6 184.6zm101.2-138.2c-5.5-2.8-32.8-16.2-37.9-18-5.1-1.9-8.8-2.8-12.5 2.8-3.7 5.6-14.3 18-17.6 21.8-3.2 3.7-6.5 4.2-12 1.4-32.6-16.3-54-29.1-75.5-66-5.7-9.8 5.7-9.1 16.3-30.3 1.8-3.7.9-6.9-.5-9.7-1.4-2.8-12.5-30.1-17.1-41.2-4.5-10.8-9.1-9.3-12.5-9.5-3.2-.2-6.9-.2-10.6-.2-3.7 0-9.7 1.4-14.8 6.9-5.1 5.6-19.4 19-19.4 46.3 0 27.3 19.9 53.7 22.6 57.4 2.8 3.7 39.1 59.7 94.8 83.8 35.2 15.2 49 16.5 66.6 13.9 10.7-1.6 32.8-13.4 37.4-26.4 4.6-13 4.6-24.1 3.2-26.4-1.3-2.5-5-3.9-10.5-6.6z" fill="currentColor"&gt;
			
			
				&lt;path d="M320 448v40c0 13.255-10.745 24-24 24H24c-13.255 0-24-10.745-24-24V120c0-13.255 10.745-24 24-24h72v296c0 30.879 25.121 56 56 56h168zm0-344V0H152c-13.255 0-24 10.745-24 24v368c0 13.255 10.745 24 24 24h272c13.255 0 24-10.745 24-24V128H344c-13.2 0-24-10.8-24-24zm120.971-31.029L375.029 7.029A24 24 0 0 0 358.059 0H352v96h96v-6.059a24 24 0 0 0-7.029-16.97z" fill="currentColor"&gt;
			
			
				&lt;path d="M352 320c-22.608 0-43.387 7.819-59.79 20.895l-102.486-64.054a96.551 96.551 0 0 0 0-41.683l102.486-64.054C308.613 184.181 329.392 192 352 192c53.019 0 96-42.981 96-96S405.019 0 352 0s-96 42.981-96 96c0 7.158.79 14.13 2.276 20.841L155.79 180.895C139.387 167.819 118.608 160 96 160c-53.019 0-96 42.981-96 96s42.981 96 96 96c22.608 0 43.387-7.819 59.79-20.895l102.486 64.054A96.301 96.301 0 0 0 256 416c0 53.019 42.981 96 96 96s96-42.981 96-96-42.981-96-96-96z" fill="currentColor"&gt;
			
			
				&lt;path d="M440.3 203.5c-15 0-28.2 6.2-37.9 15.9-35.7-24.7-83.8-40.6-137.1-42.3L293 52.3l88.2 19.8c0 21.6 17.6 39.2 39.2 39.2 22 0 39.7-18.1 39.7-39.7s-17.6-39.7-39.7-39.7c-15.4 0-28.7 9.3-35.3 22l-97.4-21.6c-4.9-1.3-9.7 2.2-11 7.1L246.3 177c-52.9 2.2-100.5 18.1-136.3 42.8-9.7-10.1-23.4-16.3-38.4-16.3-55.6 0-73.8 74.6-22.9 100.1-1.8 7.9-2.6 16.3-2.6 24.7 0 83.8 94.4 151.7 210.3 151.7 116.4 0 210.8-67.9 210.8-151.7 0-8.4-.9-17.2-3.1-25.1 49.9-25.6 31.5-99.7-23.8-99.7zM129.4 308.9c0-22 17.6-39.7 39.7-39.7 21.6 0 39.2 17.6 39.2 39.7 0 21.6-17.6 39.2-39.2 39.2-22 .1-39.7-17.6-39.7-39.2zm214.3 93.5c-36.4 36.4-139.1 36.4-175.5 0-4-3.5-4-9.7 0-13.7 3.5-3.5 9.7-3.5 13.2 0 27.8 28.5 120 29 149 0 3.5-3.5 9.7-3.5 13.2 0 4.1 4 4.1 10.2.1 13.7zm-.8-54.2c-21.6 0-39.2-17.6-39.2-39.2 0-22 17.6-39.7 39.2-39.7 22 0 39.7 17.6 39.7 39.7-.1 21.5-17.7 39.2-39.7 39.2z" fill="currentColor"&gt;
			
			
				&lt;path d="M446.7 98.6l-67.6 318.8c-5.1 22.5-18.4 28.1-37.3 17.5l-103-75.9-49.7 47.8c-5.5 5.5-10.1 10.1-20.7 10.1l7.4-104.9 190.9-172.5c8.3-7.4-1.8-11.5-12.9-4.1L117.8 284 16.2 252.2c-22.1-6.9-22.5-22.1 4.6-32.7L418.2 66.4c18.4-6.9 34.5 4.1 28.5 32.2z" fill="currentColor"&gt;
			
			
				&lt;g&gt;
					&lt;path d="M97.2800192,3.739673 L100.160021,15.3787704 C88.8306631,18.1647705 77.9879854,22.6484879 68.0000023,28.6777391 L61.8399988,18.3985363 C72.8467373,11.7537029 84.7951803,6.81153332 97.2800192,3.739673 Z M158.720055,3.739673 L155.840053,15.3787704 C167.169411,18.1647705 178.012089,22.6484879 188.000072,28.6777391 L194.200075,18.3985363 C183.180932,11.7499974 171.218739,6.80771878 158.720055,3.739673 L158.720055,3.739673 Z M18.3999736,61.8351679 C11.7546212,72.8410466 6.81206547,84.7885562 3.73996516,97.2724198 L15.3799719,100.152197 C18.1661896,88.8237238 22.6502573,77.981893 28.6799796,67.9946902 L18.3999736,61.8351679 Z M11.9999699,127.990038 C11.9961044,122.172725 12.4306685,116.363392 13.2999707,110.611385 L1.43996383,108.811525 C-0.479938607,121.525138 -0.479938607,134.454937 1.43996383,147.168551 L13.2999707,145.36869 C12.4306685,139.616684 11.9961044,133.807351 11.9999699,127.990038 L11.9999699,127.990038 Z M194.160075,237.581539 L188.000072,227.302336 C178.024494,233.327885 167.195565,237.811494 155.880053,240.601305 L158.760055,252.240403 C171.231048,249.164732 183.165742,244.222671 194.160075,237.581539 L194.160075,237.581539 Z M244.000104,127.990038 C244.00397,133.807351 243.569406,139.616684 242.700103,145.36869 L254.56011,147.168551 C256.480013,134.454937 256.480013,121.525138 254.56011,108.811525 L242.700103,110.611385 C243.569406,116.363392 244.00397,122.172725 244.000104,127.990038 Z M252.260109,158.707656 L240.620102,155.827879 C237.833884,167.156352 233.349817,177.998183 227.320094,187.985385 L237.6001,194.184905 C244.249159,183.166622 249.191823,171.205364 252.260109,158.707656 L252.260109,158.707656 Z M145.380047,242.701142 C133.858209,244.43447 122.141865,244.43447 110.620027,242.701142 L108.820026,254.560223 C121.534632,256.479975 134.465442,256.479975 147.180048,254.560223 L145.380047,242.701142 Z M221.380091,196.804701 C214.461479,206.174141 206.175877,214.452354 196.800077,221.362797 L203.920081,231.022048 C214.262958,223.418011 223.404944,214.303705 231.040097,203.984145 L221.380091,196.804701 Z M196.800077,34.6172785 C206.177345,41.5338058 214.463023,49.8188367 221.380091,59.1953726 L231.040097,51.9959309 C223.429284,41.6822474 214.31457,32.5682452 204.000081,24.9580276 L196.800077,34.6172785 Z M34.619983,59.1953726 C41.5370506,49.8188367 49.8227288,41.5338058 59.1999972,34.6172785 L51.9999931,24.9580276 C41.6855038,32.5682452 32.5707896,41.6822474 24.9599774,51.9959309 L34.619983,59.1953726 Z M237.6001,61.8351679 L227.320094,67.9946902 C233.346114,77.969489 237.830073,88.7975718 240.620102,100.1122 L252.260109,97.2324229 C249.184198,84.7624043 244.241751,72.8286423 237.6001,61.8351679 L237.6001,61.8351679 Z M110.620027,13.2989317 C122.141865,11.5656035 133.858209,11.5656035 145.380047,13.2989317 L147.180048,1.43985134 C134.465442,-0.479901112 121.534632,-0.479901112 108.820026,1.43985134 L110.620027,13.2989317 Z M40.7799866,234.201801 L15.9999722,239.981353 L21.7799756,215.203275 L10.0999688,212.463487 L4.3199655,237.241566 C3.3734444,241.28318 4.58320332,245.526897 7.51859925,248.462064 C10.4539952,251.39723 14.6980441,252.606895 18.7399738,251.660448 L43.4999881,245.980888 L40.7799866,234.201801 Z M12.5999703,201.764317 L24.279977,204.484106 L28.2799793,187.305438 C22.4496684,177.507146 18.1025197,166.899584 15.3799719,155.827879 L3.73996516,158.707656 C6.34937618,169.311891 10.3154147,179.535405 15.539972,189.125297 L12.5999703,201.764317 Z M68.6000027,227.762301 L51.4199927,231.761991 L54.1399943,243.441085 L66.7800016,240.501313 C76.3706428,245.725462 86.5949557,249.691191 97.2000192,252.300398 L100.080021,240.6613 C89.0307035,237.906432 78.4495684,233.532789 68.6800027,227.682307 L68.6000027,227.762301 Z M128.000037,23.9980665 C90.1565244,24.0177003 55.3105242,44.590631 37.01511,77.715217 C18.7196958,110.839803 19.8628631,151.287212 39.9999861,183.325747 L29.9999803,225.982439 L72.660005,215.983214 C110.077932,239.548522 158.307237,236.876754 192.892851,209.322653 C227.478464,181.768552 240.856271,135.358391 226.242944,93.6248278 C211.629616,51.8912646 172.221191,23.9617202 128.000037,23.9980665 Z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			
				&lt;g&gt;
					&lt;path d="M357.1,324.5c-24.1,15.3-57.2,21.4-79.1,23.6l18.4,18.1l67,67c24.5,25.1-15.4,64.4-40.2,40.2c-16.8-17-41.4-41.6-67-67.3
						l-67,67.2c-24.8,24.2-64.7-15.5-39.9-40.2c17-17,41.4-41.6,67-67l18.1-18.1c-21.6-2.3-55.3-8-79.6-23.6
						c-28.6-18.5-41.2-29.3-30.1-51.8c6.5-12.8,24.3-23.6,48-5c0,0,31.9,25.4,83.4,25.4s83.4-25.4,83.4-25.4c23.6-18.5,41.4-7.8,48,5
						C398.3,295.1,385.7,305.9,357.1,324.5L357.1,324.5z M142,145c0-63,51.2-114,114-114s114,51,114,114c0,62.7-51.2,113.7-114,113.7
						S142,207.7,142,145L142,145z M200,145c0,30.8,25.1,56,56,56s56-25.1,56-56c0-31.1-25.1-56.2-56-56.2S200,113.9,200,145z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			
				&lt;g transform="translate(0,664)"&gt;
					&lt;path d="m 1073.3513,-606.40537 h 196.278 c 179.2103,0 221.8795,42.66915 221.8795,221.8795 v 196.27799 c 0,179.2103512 -42.6692,221.879451 -221.8795,221.879451 h -196.278 c -179.21038,0 -221.87951,-42.6691298 -221.87951,-221.879451 v -196.27801 c 0,-179.21035 42.66913,-221.87946 221.87951,-221.87948 z" fill="currentColor"&gt;
					&lt;path d="m 1375.0576,-393.98425 c 2.9513,-9.7072 0,-16.85429 -14.1342,-16.85429 h -46.6693 c -11.8763,0 -17.3521,6.16927 -20.3212,12.97854 0,0 -23.7347,56.82106 -57.3544,93.74763 -10.8806,10.66728 -15.8232,14.08081 -21.7613,14.08081 -2.969,0 -7.2715,-3.39577 -7.2715,-13.12075 v -90.83194 c 0,-11.66288 -3.4491,-16.85429 -13.3341,-16.85429 h -73.3553 c -7.4138,0 -11.8763,5.40476 -11.8763,10.54286 0,11.0406 16.8188,13.60078 18.5433,44.67814 v 67.52388 c 0,14.80973 -2.7202,17.49433 -8.6583,17.49433 -15.8231,0 -54.3143,-57.08773 -77.16,-122.40705 -4.4447,-12.71185 -8.9427,-17.83214 -20.8723,-17.83214 h -46.68718 c -13.3341,0 -16.0009,6.16925 -16.0009,12.97852 0,12.12515 15.8232,72.35973 73.69318,152.02656 38.58,54.40315 92.8942,83.89819 142.3726,83.89819 29.6729,0 33.3353,-6.54262 33.3353,-17.83216 v -41.12238 c 0,-13.10297 2.809,-15.71646 12.214,-15.71646 6.9338,0 18.7922,3.41353 46.4916,29.63728 31.6463,31.09512 36.8555,45.03372 54.6698,45.03372 h 46.6694 c 13.3341,0 20.0189,-6.54262 16.1787,-19.46781 -4.2313,-12.88962 -19.3433,-31.57515 -39.38,-53.74532 -10.8807,-12.62294 -27.2016,-26.22375 -32.1441,-33.03302 -6.9338,-8.72941 -4.9603,-12.62294 0,-20.39227 0,0 56.8566,-78.68897 62.7947,-105.41058 z" fill="currentColor"&gt;
					&lt;path d="m 567.69877,-429.06912 c 3.15618,-10.38133 0,-18.0247 -15.11579,-18.0247 h -49.91013 c -12.70096,0 -18.55706,6.59763 -21.73232,13.87977 0,0 -25.38286,60.76685 -61.33724,100.25768 -11.63627,11.40806 -16.92197,15.05863 -23.27242,15.05863 -3.17519,0 -7.77644,-3.63156 -7.77644,-14.0319 v -97.13948 c 0,-12.47278 -3.68869,-18.0247 -14.26014,-18.0247 h -78.44923 c -7.92857,0 -12.70097,5.78005 -12.70097,11.27491 0,11.80736 17.98666,14.54527 19.83094,47.78071 v 72.21293 c 0,15.83815 -2.9091,18.70918 -9.25948,18.70918 -16.92197,0 -58.08598,-61.05206 -82.51817,-130.90731 -4.75337,-13.59458 -9.56381,-19.07042 -22.32175,-19.07042 h -49.92915 c -14.26014,0 -17.11213,6.59763 -17.11213,13.87977 0,12.96714 16.92197,77.38454 78.81059,162.58363 41.25909,58.18101 99.34506,89.72424 152.25931,89.72424 31.73343,0 35.65018,-6.99691 35.65018,-19.07043 v -43.978 c 0,-14.01288 3.00405,-16.80786 13.0622,-16.80786 7.41521,0 20.09716,3.65057 49.71998,31.69536 33.84387,33.25443 39.41486,48.16093 58.46622,48.16093 h 49.91026 c 14.26,0 21.40913,-6.99691 17.30216,-20.81966 -4.5252,-13.78473 -20.68653,-33.76783 -42.11468,-57.47752 -11.63621,-13.49953 -29.09043,-28.04479 -34.37631,-35.32694 -7.41508,-9.33557 -5.30458,-13.4995 0,-21.80835 0,0 60.80491,-84.15334 67.15549,-112.73048 z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			&lt;path d="M309.8 480.3c-13.6 14.5-50 31.7-97.4 31.7-120.8 0-147-88.8-147-140.6v-144H17.9c-5.5 0-10-4.5-10-10v-68c0-7.2 4.5-13.6 11.3-16 62-21.8 81.5-76 84.3-117.1.8-11 6.5-16.3 16.1-16.3h70.9c5.5 0 10 4.5 10 10v115.2h83c5.5 0 10 4.4 10 9.9v81.7c0 5.5-4.5 10-10 10h-83.4V360c0 34.2 23.7 53.6 68 35.8 4.8-1.9 9-3.2 12.7-2.2 3.5.9 5.8 3.4 7.4 7.9l22 64.3c1.8 5 3.3 10.6-.4 14.5z" fill="currentColor"&gt;
			&lt;path d="M512 208L320 384H288V288H208c-61.9 0-112 50.1-112 112c0 48 32 80 32 80s-128-48-128-176c0-97.2 78.8-176 176-176H288V32h32L512 208z" fill="currentColor"&gt;
			&lt;path d="M309.8 480.3c-13.6 14.5-50 31.7-97.4 31.7-120.8 0-147-88.8-147-140.6v-144H17.9c-5.5 0-10-4.5-10-10v-68c0-7.2 4.5-13.6 11.3-16 62-21.8 81.5-76 84.3-117.1.8-11 6.5-16.3 16.1-16.3h70.9c5.5 0 10 4.5 10 10v115.2h83c5.5 0 10 4.4 10 9.9v81.7c0 5.5-4.5 10-10 10h-83.4V360c0 34.2 23.7 53.6 68 35.8 4.8-1.9 9-3.2 12.7-2.2 3.5.9 5.8 3.4 7.4 7.9l22 64.3c1.8 5 3.3 10.6-.4 14.5z" fill="currentColor"&gt;
			&lt;path d="M433 179.1c0-97.2-63.7-125.7-63.7-125.7-62.5-28.7-228.6-28.4-290.5 0 0 0-63.7 28.5-63.7 125.7 0 115.7-6.6 259.4 105.6 289.1 40.5 10.7 75.3 13 103.3 11.4 50.8-2.8 79.3-18.1 79.3-18.1l-1.7-36.9s-36.3 11.4-77.1 10.1c-40.4-1.4-83-4.4-89.6-54a102.5 102.5 0 0 1 -.9-13.9c85.6 20.9 158.7 9.1 178.8 6.7 56.1-6.7 105-41.3 111.2-72.9 9.8-49.8 9-121.5 9-121.5zm-75.1 125.2h-46.6v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.3V197c0-58.5-64-56.6-64-6.9v114.2H90.2c0-122.1-5.2-147.9 18.4-175 25.9-28.9 79.8-30.8 103.8 6.1l11.6 19.5 11.6-19.5c24.1-37.1 78.1-34.8 103.8-6.1 23.7 27.3 18.4 53 18.4 175z" fill="currentColor"&gt;
			
				&lt;path d="M331.5 235.7c2.2 .9 4.2 1.9 6.3 2.8c29.2 14.1 50.6 35.2 61.8 61.4c15.7 36.5 17.2 95.8-30.3 143.2c-36.2 36.2-80.3 52.5-142.6 53h-.3c-70.2-.5-124.1-24.1-160.4-70.2c-32.3-41-48.9-98.1-49.5-169.6V256v-.2C17 184.3 33.6 127.2 65.9 86.2C102.2 40.1 156.2 16.5 226.4 16h.3c70.3 .5 124.9 24 162.3 69.9c18.4 22.7 32 50 40.6 81.7l-40.4 10.8c-7.1-25.8-17.8-47.8-32.2-65.4c-29.2-35.8-73-54.2-130.5-54.6c-57 .5-100.1 18.8-128.2 54.4C72.1 146.1 58.5 194.3 58 256c.5 61.7 14.1 109.9 40.3 143.3c28 35.6 71.2 53.9 128.2 54.4c51.4-.4 85.4-12.6 113.7-40.9c32.3-32.2 31.7-71.8 21.4-95.9c-6.1-14.2-17.1-26-31.9-34.9c-3.7 26.9-11.8 48.3-24.7 64.8c-17.1 21.8-41.4 33.6-72.7 35.3c-23.6 1.3-46.3-4.4-63.9-16c-20.8-13.8-33-34.8-34.3-59.3c-2.5-48.3 35.7-83 95.2-86.4c21.1-1.2 40.9-.3 59.2 2.8c-2.4-14.8-7.3-26.6-14.6-35.2c-10-11.7-25.6-17.7-46.2-17.8H227c-16.6 0-39 4.6-53.3 26.3l-34.4-23.6c19.2-29.1 50.3-45.1 87.8-45.1h.8c62.6 .4 99.9 39.5 103.7 107.7l-.2 .2zm-156 68.8c1.3 25.1 28.4 36.8 54.6 35.3c25.6-1.4 54.6-11.4 59.5-73.2c-13.2-2.9-27.8-4.4-43.4-4.4c-4.8 0-9.6 .1-14.4 .4c-42.9 2.4-57.2 23.2-56.2 41.8l-.1 .1z" fill="currentColor"&gt;
			
			
				&lt;path d="M407.8 294.7c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3zM288 227.1C261.9 176.4 190.9 81.9 124.9 35.3C61.6-9.4 37.5-1.7 21.6 5.5C3.3 13.8 0 41.9 0 58.4S9.1 194 15 213.9c19.5 65.7 89.1 87.9 153.2 80.7c3.3-.5 6.6-.9 10-1.4c-3.3 .5-6.6 1-10 1.4C74.3 308.6-9.1 342.8 100.3 464.5C220.6 589.1 265.1 437.8 288 361.1c22.9 76.7 49.2 222.5 185.6 103.4c102.4-103.4 28.1-156-65.8-169.9c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3c64.1 7.1 133.6-15.1 153.2-80.7C566.9 194 576 75 576 58.4s-3.3-44.7-21.6-52.9c-15.8-7.1-40-14.9-103.2 29.8C385.1 81.9 314.1 176.4 288 227.1z" fill="currentColor"&gt;
			
			
				&lt;path d="M204 6.5C101.4 6.5 0 74.9 0 185.6 0 256 39.6 296 63.6 296c9.9 0 15.6-27.6 15.6-35.4 0-9.3-23.7-29.1-23.7-67.8 0-80.4 61.2-137.4 140.4-137.4 68.1 0 118.5 38.7 118.5 109.8 0 53.1-21.3 152.7-90.3 152.7-24.9 0-46.2-18-46.2-43.8 0-37.8 26.4-74.4 26.4-113.4 0-66.2-93.9-54.2-93.9 25.8 0 16.8 2.1 35.4 9.6 50.7-13.8 59.4-42 147.9-42 209.1 0 18.9 2.7 37.5 4.5 56.4 3.4 3.8 1.7 3.4 6.9 1.5 50.4-69 48.6-82.5 71.4-172.8 12.3 23.4 44.1 36 69.3 36 106.2 0 153.9-103.5 153.9-196.8C384 71.3 298.2 6.5 204 6.5z" fill="currentColor"&gt;
			
		&lt;/svg&gt;
		&lt;div id="has-mastodon-prompt"&gt;
			&lt;h3&gt;Share on Mastodon&lt;/h3&gt;
			
		&lt;/div&gt;</description><content:encoded>&lt;!-- OneTrust Cookies Consent Notice start for nvidia.com --&gt;


&lt;!-- OneTrust Cookies Consent Notice end for nvidia.com --&gt;


	
	
	
	
	
	

	

	
	
	


	&lt;!-- This site is optimized with the Yoast SEO Premium plugin v26.2 (Yoast SEO v26.2) - https://yoast.com/wordpress/plugins/seo/ --&gt;
	‘The Next Generation of Compute Is Driving AI,’ Technology Leader Says at NVIDIA AI Day Sydney | NVIDIA Blog
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	&lt;!-- / Yoast SEO Premium plugin. --&gt;






































&lt;!-- Stream WordPress user activity plugin v4.1.1 --&gt;


	
				&lt;!-- Hotjar Tracking Code for NVIDIA --&gt;
			
			


				
				



		
		

&lt;div class="hfeed site" id="page"&gt;
	Skip to content

	&lt;!-- #masthead --&gt;
		
		&lt;div class="full-width-layout light"&gt;
		

		
&lt;div class="full-width-layout__hero light"&gt;
	&lt;div class="full-width-layout__hero-content light"&gt;
		&lt;div class="full-width-layout__hero-content__inner light"&gt;
			

							&lt;p&gt;
					Brendan Hopper, chief information officer for technology at the Commonwealth Bank of Australia, was among the attendees of last week’s event showcasing sovereign AI in the land down under.				&lt;/p&gt;
			
			
		&lt;/div&gt;
	&lt;/div&gt;

	

	&lt;/div&gt;

	
	
		&lt;div class="full-width-layout__sections"&gt;
&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This post is part of a blog series highlighting &lt;/i&gt;&lt;i&gt;NVIDIA AI Days&lt;/i&gt;&lt;i&gt; across the globe.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;NVIDIA AI Days — hosted for and in different pockets of the world — are drawing in thousands of enthusiasts, developers, researchers and startups to discuss and explore the latest technologies making AI breakthroughs possible.&lt;/p&gt;
&lt;p&gt;Sydney, Australia, was the latest stop.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="1366" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/ai-day-sydney-registration-scaled.jpeg" width="2048" /&gt;	
	
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			Attendees check in at NVIDIA AI Day Sydney, which took place Oct. 15-16 at ICC Sydney Theatre.		&lt;/span&gt;
	
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Last week, over 1,000 attendees joined NVIDIA AI Day Sydney to learn about sovereign AI — including 16 breakout sessions on agentic and physical AI, robotics and AI factories. These technologies are enabling the next frontier of AI.&lt;/p&gt;
&lt;p&gt;“It was a privilege to join NVIDIA AI Day Sydney and hear how the next generation of compute is driving AI,” said Brendan Hopper, chief information officer for technology at the Commonwealth Bank of Australia. “It’s phenomenal to have NVIDIA helping build out Australia’s AI ecosystem through infrastructure, partnerships and innovation.”&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="819" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/ai-day-sydney-brendan-hopper-pull-quote-scaled.jpg" width="2048" /&gt;	
	&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;Why It Matters&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;The latest AI trends in Australia showcase how the technology is driving digital transformation across financial services, retail and the public sector, among other industries.&lt;/p&gt;
&lt;p&gt;“AI Day Sydney highlighted how quantum and high-performance computing, empowered by AI, are redefining the pace and precision of scientific discovery,” said Giuseppe M. J. Barca, cofounder and head of research at QDX Technologies, and a professor at Monash University and the Australian National University.&lt;/p&gt;
&lt;p&gt;The Australian AI ecosystem is growing:&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__full-width-image-section"&gt;
			&lt;img alt="Infographic that states there are 600+ Australia-based NVIDIA Inception startups, three Australia-based quantum computing companies working with NVIDIA, and 20+ higher-education institutions in Australia using NVIDIA technologies for research." class="full-width-layout__image" height="819" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/sydney-ecosystem-infographic-scaled.jpg" width="2048" /&gt;	
	&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;“NVIDIA AI Day Sydney was by far the best to date technology event that XENON has attended in Australia,” said Dragan Dimitrovici, founder and CEO of XENON Systems, an Australian pioneer in high-performance computing, AI and deep learning. “It brought together key NVIDIA executives, industry leaders like Canva and Commonwealth Bank of Australia, NVIDIA Inception startups, ecosystem partners, the public sector and finance organizations to talk about how AI can help make Australia into an AI nation with the advent of AI factories.”&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;Industry in Motion&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;NVIDIA partners in Australia are advancing the nation’s role in the next industrial revolution fueled by AI.&lt;/p&gt;
&lt;p&gt;Australia-based graphic design platform provider Canva, which is working with NVIDIA to develop generative and agentic AI solutions for its hundreds of millions of users, presented on training video foundation models and curating video data at AI Day Sydney.&lt;/p&gt;
&lt;p&gt;“Canva is on a mission to empower everyone in the world to design anything and publish anywhere,” said Paul Thompson, senior director of engineering for generative AI at Canva. “We make extensive use of NVIDIA technologies to train, fine-tune and serve AI features to millions of users.”&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="1365" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/ai-day-sydney-video-flywheels-session-scaled.jpeg" width="2048" /&gt;	
	
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			An NVIDIA AI Day Sydney session covered video model flywheels and tackling large-scale data challenges.		&lt;/span&gt;
	
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;NVIDIA Cloud Partners including Firmus Technologies, ResetData and SHARON AI also attended the event.&lt;/p&gt;
&lt;p&gt;“NVIDIA AI Day Sydney proved that Australia is ready to step into its place as a global hub for AI infrastructure and innovation,” said Tim Rosenfield, co-CEO and cofounder of Firmus Technologies. “Australia’s time has come to go large in the AI era — and AI Day Sydney told that story in a way that I’ve never seen before.”&lt;/p&gt;
&lt;p&gt;“NVIDIA AI Day was a fantastic showcase for Australian AI talent and a welcomed opportunity to develop and grow the Australian ecosystem,” said Bass Salah, co-CEO of ResetData.&lt;/p&gt;
&lt;p&gt;“The energy, ideation and talent at NVIDIA AI Day Sydney were all remarkable,” added Andrew Leece, chief operating officer at SHARON AI. “SHARON AI is thrilled to be at the center of such an opportunity for Australian startups and innovators.”&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;Other Event Highlights&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;At AI Day Sydney, NVIDIA hosted its first-ever “Australia Startup, VC and Partner Connect” event, bringing together leading startups, industry leaders and government representatives to drive the region’s AI strategy, unlock opportunities for innovation and spark cross-sector collaboration. Venture capital companies including Airtree and Breakthrough Victoria attended to connect with startups.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__full-width-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="1365" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/ai-day-sydney-startup-session-1-scaled.jpeg" width="2048" /&gt;	
	
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			Panelists at the special event for Australian startups, venture capital companies and partners.		&lt;/span&gt;
	
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Presenters included quantum computing companies Diraq, PsiQuantum, Q-CTRL and Quantum Brilliance, using the NVIDIA CUDA-Q platform, as well as Heidi Health, which uses the NVIDIA Parakeet automatic speech recognition model for its medical AI scribe.&lt;/p&gt;
&lt;p&gt;“NVIDIA AI Day was a brilliant showcase of Australian AI innovation and a great boost for the local ecosystem,” said Yu Liu, cofounder and chief technology officer at Heidi Health.&lt;/p&gt;
&lt;p&gt;Lightning talks by startups, including NVIDIA Inception members BrainFish, Eklipse.gg and Zetaris, showcased how NVIDIA Triton Inference Server, NVIDIA NIM microservices and the NVIDIA AI Blueprint for video search and summarization accelerate the development of their AI applications in customer service, content creation, data processing and more.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="1366" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/susan-marshall-ai-day-sydney-scaled.jpeg" width="2048" /&gt;	
	
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			NVIDIA Senior Director of Developer Relations Susan Marshall shared her insights on Australia’s global innovation potential during a plenary session.		&lt;/span&gt;
	
	&lt;/p&gt;
&lt;/div&gt;



&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;What’s Next&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Australia’s AI future lies where the nation’s long heritage and expertise in high-performance computing, visual effects and animation intersect with the vibrant, emerging quantum and robotics industries, said NVIDIA Country Manager for Australia and New Zealand Enterprise Sudarshan Ramachandran at the event. This is poised to drive discovery and sustainability through advanced simulation and AI technologies.&lt;/p&gt;
&lt;p&gt;“Through strong collaboration, world-class infrastructure and innovation partnerships, we’re building a thriving ecosystem that positions Australia as a global leader in AI and economic advancement,” Ramachandran said.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__full-width-image-section"&gt;
			&lt;img alt="&amp;quot;We’re building a thriving ecosystem that positions Australia as a global leader in AI and economic advancement,” said NVIDIA Country Manager for Australia and New Zealand Enterprise Sudarshan Ramachandran." class="full-width-layout__image" height="819" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/ai-day-sydney-future-pull-quote-scaled.jpg" width="2048" /&gt;	
	&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Next up: Join NVIDIA at GTC Washington, D.C., running Oct. 27-29 at the Walter E. Washington Convention Center.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Explore more content from &lt;/i&gt;&lt;i&gt;AI Days&lt;/i&gt;&lt;i&gt; around the globe.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
	&lt;div class="full-width-layout__news-section"&gt;
		&lt;p&gt;Related News&lt;/p&gt;

		
	&lt;/div&gt;
		&lt;/div&gt;
	


&lt;!-- #colophon --&gt;

&lt;/div&gt;&lt;!-- #page --&gt;


&lt;!-- #has-highlight-and-share --&gt;		&lt;svg class="hidden" height="0" width="0" xmlns="http://www.w3.org/2000/svg"&gt;
			
				&lt;g&gt;&lt;path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z" fill="currentColor"&gt;&lt;/g&gt;
			
			
				&lt;path d="M279.14 288l14.22-92.66h-88.91v-60.13c0-25.35 12.42-50.06 52.24-50.06h40.42V6.26S260.43 0 225.36 0c-73.22 0-121.08 44.38-121.08 124.72v70.62H22.89V288h81.39v224h100.17V288z" fill="currentColor"&gt;
			
			
				&lt;path d="M256 8C118.941 8 8 118.919 8 256c0 137.059 110.919 248 248 248 48.154 0 95.342-14.14 135.408-40.223 12.005-7.815 14.625-24.288 5.552-35.372l-10.177-12.433c-7.671-9.371-21.179-11.667-31.373-5.129C325.92 429.757 291.314 440 256 440c-101.458 0-184-82.542-184-184S154.542 72 256 72c100.139 0 184 57.619 184 160 0 38.786-21.093 79.742-58.17 83.693-17.349-.454-16.91-12.857-13.476-30.024l23.433-121.11C394.653 149.75 383.308 136 368.225 136h-44.981a13.518 13.518 0 0 0-13.432 11.993l-.01.092c-14.697-17.901-40.448-21.775-59.971-21.775-74.58 0-137.831 62.234-137.831 151.46 0 65.303 36.785 105.87 96 105.87 26.984 0 57.369-15.637 74.991-38.333 9.522 34.104 40.613 34.103 70.71 34.103C462.609 379.41 504 307.798 504 232 504 95.653 394.023 8 256 8zm-21.68 304.43c-22.249 0-36.07-15.623-36.07-40.771 0-44.993 30.779-72.729 58.63-72.729 22.292 0 35.601 15.241 35.601 40.77 0 45.061-33.875 72.73-58.161 72.73z" fill="currentColor"&gt;
			
			
				&lt;path d="M100.28 448H7.4V148.9h92.88zM53.79 108.1C24.09 108.1 0 83.5 0 53.8a53.79 53.79 0 0 1 107.58 0c0 29.7-24.1 54.3-53.79 54.3zM447.9 448h-92.68V302.4c0-34.7-.7-79.2-48.29-79.2-48.29 0-55.69 37.7-55.69 76.7V448h-92.78V148.9h89.08v40.8h1.3c12.4-23.5 42.69-48.3 87.88-48.3 94 0 111.28 61.9 111.28 142.3V448z" fill="currentColor"&gt;
			
			
				&lt;path d="M162.7 210c-1.8 3.3-25.2 44.4-70.1 123.5-4.9 8.3-10.8 12.5-17.7 12.5H9.8c-7.7 0-12.1-7.5-8.5-14.4l69-121.3c.2 0 .2-.1 0-.3l-43.9-75.6c-4.3-7.8.3-14.1 8.5-14.1H100c7.3 0 13.3 4.1 18 12.2l44.7 77.5zM382.6 46.1l-144 253v.3L330.2 466c3.9 7.1.2 14.1-8.5 14.1h-65.2c-7.6 0-13.6-4-18-12.2l-92.4-168.5c3.3-5.8 51.5-90.8 144.8-255.2 4.6-8.1 10.4-12.2 17.5-12.2h65.7c8 0 12.3 6.7 8.5 14.1z" fill="currentColor"&gt;
			
			
				&lt;path d="M380.9 97.1C339 55.1 283.2 32 223.9 32c-122.4 0-222 99.6-222 222 0 39.1 10.2 77.3 29.6 111L0 480l117.7-30.9c32.4 17.7 68.9 27 106.1 27h.1c122.3 0 224.1-99.6 224.1-222 0-59.3-25.2-115-67.1-157zm-157 341.6c-33.2 0-65.7-8.9-94-25.7l-6.7-4-69.8 18.3L72 359.2l-4.4-7c-18.5-29.4-28.2-63.3-28.2-98.2 0-101.7 82.8-184.5 184.6-184.5 49.3 0 95.6 19.2 130.4 54.1 34.8 34.9 56.2 81.2 56.1 130.5 0 101.8-84.9 184.6-186.6 184.6zm101.2-138.2c-5.5-2.8-32.8-16.2-37.9-18-5.1-1.9-8.8-2.8-12.5 2.8-3.7 5.6-14.3 18-17.6 21.8-3.2 3.7-6.5 4.2-12 1.4-32.6-16.3-54-29.1-75.5-66-5.7-9.8 5.7-9.1 16.3-30.3 1.8-3.7.9-6.9-.5-9.7-1.4-2.8-12.5-30.1-17.1-41.2-4.5-10.8-9.1-9.3-12.5-9.5-3.2-.2-6.9-.2-10.6-.2-3.7 0-9.7 1.4-14.8 6.9-5.1 5.6-19.4 19-19.4 46.3 0 27.3 19.9 53.7 22.6 57.4 2.8 3.7 39.1 59.7 94.8 83.8 35.2 15.2 49 16.5 66.6 13.9 10.7-1.6 32.8-13.4 37.4-26.4 4.6-13 4.6-24.1 3.2-26.4-1.3-2.5-5-3.9-10.5-6.6z" fill="currentColor"&gt;
			
			
				&lt;path d="M320 448v40c0 13.255-10.745 24-24 24H24c-13.255 0-24-10.745-24-24V120c0-13.255 10.745-24 24-24h72v296c0 30.879 25.121 56 56 56h168zm0-344V0H152c-13.255 0-24 10.745-24 24v368c0 13.255 10.745 24 24 24h272c13.255 0 24-10.745 24-24V128H344c-13.2 0-24-10.8-24-24zm120.971-31.029L375.029 7.029A24 24 0 0 0 358.059 0H352v96h96v-6.059a24 24 0 0 0-7.029-16.97z" fill="currentColor"&gt;
			
			
				&lt;path d="M352 320c-22.608 0-43.387 7.819-59.79 20.895l-102.486-64.054a96.551 96.551 0 0 0 0-41.683l102.486-64.054C308.613 184.181 329.392 192 352 192c53.019 0 96-42.981 96-96S405.019 0 352 0s-96 42.981-96 96c0 7.158.79 14.13 2.276 20.841L155.79 180.895C139.387 167.819 118.608 160 96 160c-53.019 0-96 42.981-96 96s42.981 96 96 96c22.608 0 43.387-7.819 59.79-20.895l102.486 64.054A96.301 96.301 0 0 0 256 416c0 53.019 42.981 96 96 96s96-42.981 96-96-42.981-96-96-96z" fill="currentColor"&gt;
			
			
				&lt;path d="M440.3 203.5c-15 0-28.2 6.2-37.9 15.9-35.7-24.7-83.8-40.6-137.1-42.3L293 52.3l88.2 19.8c0 21.6 17.6 39.2 39.2 39.2 22 0 39.7-18.1 39.7-39.7s-17.6-39.7-39.7-39.7c-15.4 0-28.7 9.3-35.3 22l-97.4-21.6c-4.9-1.3-9.7 2.2-11 7.1L246.3 177c-52.9 2.2-100.5 18.1-136.3 42.8-9.7-10.1-23.4-16.3-38.4-16.3-55.6 0-73.8 74.6-22.9 100.1-1.8 7.9-2.6 16.3-2.6 24.7 0 83.8 94.4 151.7 210.3 151.7 116.4 0 210.8-67.9 210.8-151.7 0-8.4-.9-17.2-3.1-25.1 49.9-25.6 31.5-99.7-23.8-99.7zM129.4 308.9c0-22 17.6-39.7 39.7-39.7 21.6 0 39.2 17.6 39.2 39.7 0 21.6-17.6 39.2-39.2 39.2-22 .1-39.7-17.6-39.7-39.2zm214.3 93.5c-36.4 36.4-139.1 36.4-175.5 0-4-3.5-4-9.7 0-13.7 3.5-3.5 9.7-3.5 13.2 0 27.8 28.5 120 29 149 0 3.5-3.5 9.7-3.5 13.2 0 4.1 4 4.1 10.2.1 13.7zm-.8-54.2c-21.6 0-39.2-17.6-39.2-39.2 0-22 17.6-39.7 39.2-39.7 22 0 39.7 17.6 39.7 39.7-.1 21.5-17.7 39.2-39.7 39.2z" fill="currentColor"&gt;
			
			
				&lt;path d="M446.7 98.6l-67.6 318.8c-5.1 22.5-18.4 28.1-37.3 17.5l-103-75.9-49.7 47.8c-5.5 5.5-10.1 10.1-20.7 10.1l7.4-104.9 190.9-172.5c8.3-7.4-1.8-11.5-12.9-4.1L117.8 284 16.2 252.2c-22.1-6.9-22.5-22.1 4.6-32.7L418.2 66.4c18.4-6.9 34.5 4.1 28.5 32.2z" fill="currentColor"&gt;
			
			
				&lt;g&gt;
					&lt;path d="M97.2800192,3.739673 L100.160021,15.3787704 C88.8306631,18.1647705 77.9879854,22.6484879 68.0000023,28.6777391 L61.8399988,18.3985363 C72.8467373,11.7537029 84.7951803,6.81153332 97.2800192,3.739673 Z M158.720055,3.739673 L155.840053,15.3787704 C167.169411,18.1647705 178.012089,22.6484879 188.000072,28.6777391 L194.200075,18.3985363 C183.180932,11.7499974 171.218739,6.80771878 158.720055,3.739673 L158.720055,3.739673 Z M18.3999736,61.8351679 C11.7546212,72.8410466 6.81206547,84.7885562 3.73996516,97.2724198 L15.3799719,100.152197 C18.1661896,88.8237238 22.6502573,77.981893 28.6799796,67.9946902 L18.3999736,61.8351679 Z M11.9999699,127.990038 C11.9961044,122.172725 12.4306685,116.363392 13.2999707,110.611385 L1.43996383,108.811525 C-0.479938607,121.525138 -0.479938607,134.454937 1.43996383,147.168551 L13.2999707,145.36869 C12.4306685,139.616684 11.9961044,133.807351 11.9999699,127.990038 L11.9999699,127.990038 Z M194.160075,237.581539 L188.000072,227.302336 C178.024494,233.327885 167.195565,237.811494 155.880053,240.601305 L158.760055,252.240403 C171.231048,249.164732 183.165742,244.222671 194.160075,237.581539 L194.160075,237.581539 Z M244.000104,127.990038 C244.00397,133.807351 243.569406,139.616684 242.700103,145.36869 L254.56011,147.168551 C256.480013,134.454937 256.480013,121.525138 254.56011,108.811525 L242.700103,110.611385 C243.569406,116.363392 244.00397,122.172725 244.000104,127.990038 Z M252.260109,158.707656 L240.620102,155.827879 C237.833884,167.156352 233.349817,177.998183 227.320094,187.985385 L237.6001,194.184905 C244.249159,183.166622 249.191823,171.205364 252.260109,158.707656 L252.260109,158.707656 Z M145.380047,242.701142 C133.858209,244.43447 122.141865,244.43447 110.620027,242.701142 L108.820026,254.560223 C121.534632,256.479975 134.465442,256.479975 147.180048,254.560223 L145.380047,242.701142 Z M221.380091,196.804701 C214.461479,206.174141 206.175877,214.452354 196.800077,221.362797 L203.920081,231.022048 C214.262958,223.418011 223.404944,214.303705 231.040097,203.984145 L221.380091,196.804701 Z M196.800077,34.6172785 C206.177345,41.5338058 214.463023,49.8188367 221.380091,59.1953726 L231.040097,51.9959309 C223.429284,41.6822474 214.31457,32.5682452 204.000081,24.9580276 L196.800077,34.6172785 Z M34.619983,59.1953726 C41.5370506,49.8188367 49.8227288,41.5338058 59.1999972,34.6172785 L51.9999931,24.9580276 C41.6855038,32.5682452 32.5707896,41.6822474 24.9599774,51.9959309 L34.619983,59.1953726 Z M237.6001,61.8351679 L227.320094,67.9946902 C233.346114,77.969489 237.830073,88.7975718 240.620102,100.1122 L252.260109,97.2324229 C249.184198,84.7624043 244.241751,72.8286423 237.6001,61.8351679 L237.6001,61.8351679 Z M110.620027,13.2989317 C122.141865,11.5656035 133.858209,11.5656035 145.380047,13.2989317 L147.180048,1.43985134 C134.465442,-0.479901112 121.534632,-0.479901112 108.820026,1.43985134 L110.620027,13.2989317 Z M40.7799866,234.201801 L15.9999722,239.981353 L21.7799756,215.203275 L10.0999688,212.463487 L4.3199655,237.241566 C3.3734444,241.28318 4.58320332,245.526897 7.51859925,248.462064 C10.4539952,251.39723 14.6980441,252.606895 18.7399738,251.660448 L43.4999881,245.980888 L40.7799866,234.201801 Z M12.5999703,201.764317 L24.279977,204.484106 L28.2799793,187.305438 C22.4496684,177.507146 18.1025197,166.899584 15.3799719,155.827879 L3.73996516,158.707656 C6.34937618,169.311891 10.3154147,179.535405 15.539972,189.125297 L12.5999703,201.764317 Z M68.6000027,227.762301 L51.4199927,231.761991 L54.1399943,243.441085 L66.7800016,240.501313 C76.3706428,245.725462 86.5949557,249.691191 97.2000192,252.300398 L100.080021,240.6613 C89.0307035,237.906432 78.4495684,233.532789 68.6800027,227.682307 L68.6000027,227.762301 Z M128.000037,23.9980665 C90.1565244,24.0177003 55.3105242,44.590631 37.01511,77.715217 C18.7196958,110.839803 19.8628631,151.287212 39.9999861,183.325747 L29.9999803,225.982439 L72.660005,215.983214 C110.077932,239.548522 158.307237,236.876754 192.892851,209.322653 C227.478464,181.768552 240.856271,135.358391 226.242944,93.6248278 C211.629616,51.8912646 172.221191,23.9617202 128.000037,23.9980665 Z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			
				&lt;g&gt;
					&lt;path d="M357.1,324.5c-24.1,15.3-57.2,21.4-79.1,23.6l18.4,18.1l67,67c24.5,25.1-15.4,64.4-40.2,40.2c-16.8-17-41.4-41.6-67-67.3
						l-67,67.2c-24.8,24.2-64.7-15.5-39.9-40.2c17-17,41.4-41.6,67-67l18.1-18.1c-21.6-2.3-55.3-8-79.6-23.6
						c-28.6-18.5-41.2-29.3-30.1-51.8c6.5-12.8,24.3-23.6,48-5c0,0,31.9,25.4,83.4,25.4s83.4-25.4,83.4-25.4c23.6-18.5,41.4-7.8,48,5
						C398.3,295.1,385.7,305.9,357.1,324.5L357.1,324.5z M142,145c0-63,51.2-114,114-114s114,51,114,114c0,62.7-51.2,113.7-114,113.7
						S142,207.7,142,145L142,145z M200,145c0,30.8,25.1,56,56,56s56-25.1,56-56c0-31.1-25.1-56.2-56-56.2S200,113.9,200,145z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			
				&lt;g transform="translate(0,664)"&gt;
					&lt;path d="m 1073.3513,-606.40537 h 196.278 c 179.2103,0 221.8795,42.66915 221.8795,221.8795 v 196.27799 c 0,179.2103512 -42.6692,221.879451 -221.8795,221.879451 h -196.278 c -179.21038,0 -221.87951,-42.6691298 -221.87951,-221.879451 v -196.27801 c 0,-179.21035 42.66913,-221.87946 221.87951,-221.87948 z" fill="currentColor"&gt;
					&lt;path d="m 1375.0576,-393.98425 c 2.9513,-9.7072 0,-16.85429 -14.1342,-16.85429 h -46.6693 c -11.8763,0 -17.3521,6.16927 -20.3212,12.97854 0,0 -23.7347,56.82106 -57.3544,93.74763 -10.8806,10.66728 -15.8232,14.08081 -21.7613,14.08081 -2.969,0 -7.2715,-3.39577 -7.2715,-13.12075 v -90.83194 c 0,-11.66288 -3.4491,-16.85429 -13.3341,-16.85429 h -73.3553 c -7.4138,0 -11.8763,5.40476 -11.8763,10.54286 0,11.0406 16.8188,13.60078 18.5433,44.67814 v 67.52388 c 0,14.80973 -2.7202,17.49433 -8.6583,17.49433 -15.8231,0 -54.3143,-57.08773 -77.16,-122.40705 -4.4447,-12.71185 -8.9427,-17.83214 -20.8723,-17.83214 h -46.68718 c -13.3341,0 -16.0009,6.16925 -16.0009,12.97852 0,12.12515 15.8232,72.35973 73.69318,152.02656 38.58,54.40315 92.8942,83.89819 142.3726,83.89819 29.6729,0 33.3353,-6.54262 33.3353,-17.83216 v -41.12238 c 0,-13.10297 2.809,-15.71646 12.214,-15.71646 6.9338,0 18.7922,3.41353 46.4916,29.63728 31.6463,31.09512 36.8555,45.03372 54.6698,45.03372 h 46.6694 c 13.3341,0 20.0189,-6.54262 16.1787,-19.46781 -4.2313,-12.88962 -19.3433,-31.57515 -39.38,-53.74532 -10.8807,-12.62294 -27.2016,-26.22375 -32.1441,-33.03302 -6.9338,-8.72941 -4.9603,-12.62294 0,-20.39227 0,0 56.8566,-78.68897 62.7947,-105.41058 z" fill="currentColor"&gt;
					&lt;path d="m 567.69877,-429.06912 c 3.15618,-10.38133 0,-18.0247 -15.11579,-18.0247 h -49.91013 c -12.70096,0 -18.55706,6.59763 -21.73232,13.87977 0,0 -25.38286,60.76685 -61.33724,100.25768 -11.63627,11.40806 -16.92197,15.05863 -23.27242,15.05863 -3.17519,0 -7.77644,-3.63156 -7.77644,-14.0319 v -97.13948 c 0,-12.47278 -3.68869,-18.0247 -14.26014,-18.0247 h -78.44923 c -7.92857,0 -12.70097,5.78005 -12.70097,11.27491 0,11.80736 17.98666,14.54527 19.83094,47.78071 v 72.21293 c 0,15.83815 -2.9091,18.70918 -9.25948,18.70918 -16.92197,0 -58.08598,-61.05206 -82.51817,-130.90731 -4.75337,-13.59458 -9.56381,-19.07042 -22.32175,-19.07042 h -49.92915 c -14.26014,0 -17.11213,6.59763 -17.11213,13.87977 0,12.96714 16.92197,77.38454 78.81059,162.58363 41.25909,58.18101 99.34506,89.72424 152.25931,89.72424 31.73343,0 35.65018,-6.99691 35.65018,-19.07043 v -43.978 c 0,-14.01288 3.00405,-16.80786 13.0622,-16.80786 7.41521,0 20.09716,3.65057 49.71998,31.69536 33.84387,33.25443 39.41486,48.16093 58.46622,48.16093 h 49.91026 c 14.26,0 21.40913,-6.99691 17.30216,-20.81966 -4.5252,-13.78473 -20.68653,-33.76783 -42.11468,-57.47752 -11.63621,-13.49953 -29.09043,-28.04479 -34.37631,-35.32694 -7.41508,-9.33557 -5.30458,-13.4995 0,-21.80835 0,0 60.80491,-84.15334 67.15549,-112.73048 z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			&lt;path d="M309.8 480.3c-13.6 14.5-50 31.7-97.4 31.7-120.8 0-147-88.8-147-140.6v-144H17.9c-5.5 0-10-4.5-10-10v-68c0-7.2 4.5-13.6 11.3-16 62-21.8 81.5-76 84.3-117.1.8-11 6.5-16.3 16.1-16.3h70.9c5.5 0 10 4.5 10 10v115.2h83c5.5 0 10 4.4 10 9.9v81.7c0 5.5-4.5 10-10 10h-83.4V360c0 34.2 23.7 53.6 68 35.8 4.8-1.9 9-3.2 12.7-2.2 3.5.9 5.8 3.4 7.4 7.9l22 64.3c1.8 5 3.3 10.6-.4 14.5z" fill="currentColor"&gt;
			&lt;path d="M512 208L320 384H288V288H208c-61.9 0-112 50.1-112 112c0 48 32 80 32 80s-128-48-128-176c0-97.2 78.8-176 176-176H288V32h32L512 208z" fill="currentColor"&gt;
			&lt;path d="M309.8 480.3c-13.6 14.5-50 31.7-97.4 31.7-120.8 0-147-88.8-147-140.6v-144H17.9c-5.5 0-10-4.5-10-10v-68c0-7.2 4.5-13.6 11.3-16 62-21.8 81.5-76 84.3-117.1.8-11 6.5-16.3 16.1-16.3h70.9c5.5 0 10 4.5 10 10v115.2h83c5.5 0 10 4.4 10 9.9v81.7c0 5.5-4.5 10-10 10h-83.4V360c0 34.2 23.7 53.6 68 35.8 4.8-1.9 9-3.2 12.7-2.2 3.5.9 5.8 3.4 7.4 7.9l22 64.3c1.8 5 3.3 10.6-.4 14.5z" fill="currentColor"&gt;
			&lt;path d="M433 179.1c0-97.2-63.7-125.7-63.7-125.7-62.5-28.7-228.6-28.4-290.5 0 0 0-63.7 28.5-63.7 125.7 0 115.7-6.6 259.4 105.6 289.1 40.5 10.7 75.3 13 103.3 11.4 50.8-2.8 79.3-18.1 79.3-18.1l-1.7-36.9s-36.3 11.4-77.1 10.1c-40.4-1.4-83-4.4-89.6-54a102.5 102.5 0 0 1 -.9-13.9c85.6 20.9 158.7 9.1 178.8 6.7 56.1-6.7 105-41.3 111.2-72.9 9.8-49.8 9-121.5 9-121.5zm-75.1 125.2h-46.6v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.3V197c0-58.5-64-56.6-64-6.9v114.2H90.2c0-122.1-5.2-147.9 18.4-175 25.9-28.9 79.8-30.8 103.8 6.1l11.6 19.5 11.6-19.5c24.1-37.1 78.1-34.8 103.8-6.1 23.7 27.3 18.4 53 18.4 175z" fill="currentColor"&gt;
			
				&lt;path d="M331.5 235.7c2.2 .9 4.2 1.9 6.3 2.8c29.2 14.1 50.6 35.2 61.8 61.4c15.7 36.5 17.2 95.8-30.3 143.2c-36.2 36.2-80.3 52.5-142.6 53h-.3c-70.2-.5-124.1-24.1-160.4-70.2c-32.3-41-48.9-98.1-49.5-169.6V256v-.2C17 184.3 33.6 127.2 65.9 86.2C102.2 40.1 156.2 16.5 226.4 16h.3c70.3 .5 124.9 24 162.3 69.9c18.4 22.7 32 50 40.6 81.7l-40.4 10.8c-7.1-25.8-17.8-47.8-32.2-65.4c-29.2-35.8-73-54.2-130.5-54.6c-57 .5-100.1 18.8-128.2 54.4C72.1 146.1 58.5 194.3 58 256c.5 61.7 14.1 109.9 40.3 143.3c28 35.6 71.2 53.9 128.2 54.4c51.4-.4 85.4-12.6 113.7-40.9c32.3-32.2 31.7-71.8 21.4-95.9c-6.1-14.2-17.1-26-31.9-34.9c-3.7 26.9-11.8 48.3-24.7 64.8c-17.1 21.8-41.4 33.6-72.7 35.3c-23.6 1.3-46.3-4.4-63.9-16c-20.8-13.8-33-34.8-34.3-59.3c-2.5-48.3 35.7-83 95.2-86.4c21.1-1.2 40.9-.3 59.2 2.8c-2.4-14.8-7.3-26.6-14.6-35.2c-10-11.7-25.6-17.7-46.2-17.8H227c-16.6 0-39 4.6-53.3 26.3l-34.4-23.6c19.2-29.1 50.3-45.1 87.8-45.1h.8c62.6 .4 99.9 39.5 103.7 107.7l-.2 .2zm-156 68.8c1.3 25.1 28.4 36.8 54.6 35.3c25.6-1.4 54.6-11.4 59.5-73.2c-13.2-2.9-27.8-4.4-43.4-4.4c-4.8 0-9.6 .1-14.4 .4c-42.9 2.4-57.2 23.2-56.2 41.8l-.1 .1z" fill="currentColor"&gt;
			
			
				&lt;path d="M407.8 294.7c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3zM288 227.1C261.9 176.4 190.9 81.9 124.9 35.3C61.6-9.4 37.5-1.7 21.6 5.5C3.3 13.8 0 41.9 0 58.4S9.1 194 15 213.9c19.5 65.7 89.1 87.9 153.2 80.7c3.3-.5 6.6-.9 10-1.4c-3.3 .5-6.6 1-10 1.4C74.3 308.6-9.1 342.8 100.3 464.5C220.6 589.1 265.1 437.8 288 361.1c22.9 76.7 49.2 222.5 185.6 103.4c102.4-103.4 28.1-156-65.8-169.9c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3c64.1 7.1 133.6-15.1 153.2-80.7C566.9 194 576 75 576 58.4s-3.3-44.7-21.6-52.9c-15.8-7.1-40-14.9-103.2 29.8C385.1 81.9 314.1 176.4 288 227.1z" fill="currentColor"&gt;
			
			
				&lt;path d="M204 6.5C101.4 6.5 0 74.9 0 185.6 0 256 39.6 296 63.6 296c9.9 0 15.6-27.6 15.6-35.4 0-9.3-23.7-29.1-23.7-67.8 0-80.4 61.2-137.4 140.4-137.4 68.1 0 118.5 38.7 118.5 109.8 0 53.1-21.3 152.7-90.3 152.7-24.9 0-46.2-18-46.2-43.8 0-37.8 26.4-74.4 26.4-113.4 0-66.2-93.9-54.2-93.9 25.8 0 16.8 2.1 35.4 9.6 50.7-13.8 59.4-42 147.9-42 209.1 0 18.9 2.7 37.5 4.5 56.4 3.4 3.8 1.7 3.4 6.9 1.5 50.4-69 48.6-82.5 71.4-172.8 12.3 23.4 44.1 36 69.3 36 106.2 0 153.9-103.5 153.9-196.8C384 71.3 298.2 6.5 204 6.5z" fill="currentColor"&gt;
			
		&lt;/svg&gt;
		&lt;div id="has-mastodon-prompt"&gt;
			&lt;h3&gt;Share on Mastodon&lt;/h3&gt;
			
		&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/ai-day-sydney/</guid><pubDate>Wed, 22 Oct 2025 20:00:34 +0000</pubDate></item><item><title>[NEW] General Motors will integrate AI into its cars, plus new hands-free assist (AI – Ars Technica)</title><link>https://arstechnica.com/cars/2025/10/ai-and-hands-free-driving-are-coming-to-gms-vehicles/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Do we want LLMs in our cars? GM thinks we do.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="CHINA - 2023/11/03: In this photo illustration, the American multinational automobile corporation, the General Motors (GM, NYSE: GM) logo seen displayed on a smartphone with an Artificial intelligence (AI) chip and symbol in the background." class="absolute inset-0 w-full h-full object-cover hidden" height="426" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1776398279-640x426.jpg" width="640" /&gt;
                  &lt;img alt="CHINA - 2023/11/03: In this photo illustration, the American multinational automobile corporation, the General Motors (GM, NYSE: GM) logo seen displayed on a smartphone with an Artificial intelligence (AI) chip and symbol in the background." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1776398279-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Budrul Chukrut/SOPA Images/LightRocket via Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;aside class="pullbox sidebar fullwidth"&gt;GM provided flights from Detroit to New York City and accommodation so Ars could attend its tech event. Ars does not accept paid editorial content.&lt;/aside&gt;
&lt;p&gt;General Motors held a preview event today to show the world what it’s working on. We’ve already seen some projects, like the further development of lithium manganese-rich battery technology or backup power for EVs that can power a home or support the power grid.&lt;/p&gt;
&lt;p&gt;The most significant new announcement is that Cadillac will offer an Escalade IQ with a so-called “Level 3” conditional automated driving system in 2028. GM is referring to it as a “hands off, eyes off” system and says it will integrate advanced digital mapping, use of lidar and other systems, and advanced machine learning to handle the driving duties in a controlled environment up to 80 mph (129 km/h).&lt;/p&gt;
&lt;p&gt;This means you can theoretically watch a movie from the driver’s seat while your car takes you down the highway to the airport. Over time, the system’s operation areas will expand to cover even more roads, making driving unnecessary in many situations—unless, of course, you like to drive.&lt;/p&gt;
&lt;p&gt;“We’re taking a safety-first approach,” CEO Mary Barra said to an audience of journalists at an event in New York City. “You’ll see us roll out much, much faster than what we did with Super Cruise.”&lt;/p&gt;
&lt;p&gt;So while the system will be limited to the Escalade at first, GM has made it clear that it wants to spread the system across its lineup. Barra even had the assembled journalists imagine a world in which your car anticipates your needs, takes you where you want to go, and services itself during downtime.&lt;/p&gt;
&lt;p&gt;Compared to Mercedes-Benz’s current offering, the advanced level 3 Super Cruise will work at a higher speed, and the team emphasized that the goal is for the system to work in all 50 states and in all weather conditions.&lt;/p&gt;
&lt;p&gt;Am I skeptical? Yes. But the company has a new liquid-cooled compute module to help handle the demands of a more advanced system. This would be the most advanced driver assistance system available to consumers in the US, and the company learned a lot through its now-shuttered Cruise autonomous vehicle program.&lt;/p&gt;
&lt;h2&gt;More AI&lt;/h2&gt;
&lt;p&gt;This new vehicle computer will allow GM to integrate more AI functionality into the vehicle. GM believes that incorporating Google Gemini (and eventually its own system) will help drivers better interact with a vehicle and its functions through natural language processing.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;I asked Dave Richardson, GM’s SVP of software, how the company will avoid the enshittification of vehicles as it integrates more AI.&lt;/p&gt;
&lt;p&gt;“There’s a lot of hype around AI right now,” he told me. “But there’s also practical use. I’ve been trying to focus the company on practical use cases. I think there’s a lot of pretty compelling things we can do to try to add real value.”&lt;/p&gt;
&lt;p&gt;He gave some examples, such as a car knowing you have a meeting and setting the navigation appropriately or knowing that you’re going on a road trip, so it should queue up the appropriate media for your kids to stream in the back seat.&lt;/p&gt;
&lt;p&gt;While the company is using Gemini at first, it eventually plans to have its own model on board. “With advanced processing in the car, we can handle interference on board so that it works in low-data-connection areas,” Richardson said.&lt;/p&gt;
&lt;p&gt;Ultimately, GM will deploy its own LLM that knows about the car and is limited in overall parameters, Richardson told me. It won’t need to rely on the cloud to operate, increasing responsiveness in the car and keeping personal information with you, he said.&lt;/p&gt;
&lt;p&gt;There are reasons to be skeptical, of course. One of my biggest concerns is how much driver data the car will collect. One reason GM doesn’t offer Android Auto or Apple CarPlay, the company has said, is that it wants to protect customer data. The owner must consent to any data sharing, GM said.&lt;/p&gt;
&lt;p&gt;And although GM says it has made some internal changes to protect customer data, there have been some very public instances of the company selling data. “Data privacy and security is priority one for us,” Richardson told me about his work at GM. He said he has hired people specifically tasked with ensuring that customer data protection frameworks are in place.&lt;/p&gt;
&lt;p&gt;“We have no interest in selling that data to third parties. When we think about data, whether it’s for Super Cruise or the AI, it’s really for us to develop the product and make it better. We don’t want to sell that data as the product itself,” he said.&lt;/p&gt;
&lt;p&gt;I believe there’s space for a privacy-focused automaker, and while I’m not sure whether that will be GM, I hope that privacy and data protection are as important to the company in the future as it says it is today.&lt;/p&gt;
&lt;p&gt;As for consumers wanting AI in their vehicles? GM thinks they do.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;








  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Do we want LLMs in our cars? GM thinks we do.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="CHINA - 2023/11/03: In this photo illustration, the American multinational automobile corporation, the General Motors (GM, NYSE: GM) logo seen displayed on a smartphone with an Artificial intelligence (AI) chip and symbol in the background." class="absolute inset-0 w-full h-full object-cover hidden" height="426" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1776398279-640x426.jpg" width="640" /&gt;
                  &lt;img alt="CHINA - 2023/11/03: In this photo illustration, the American multinational automobile corporation, the General Motors (GM, NYSE: GM) logo seen displayed on a smartphone with an Artificial intelligence (AI) chip and symbol in the background." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1776398279-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Budrul Chukrut/SOPA Images/LightRocket via Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;aside class="pullbox sidebar fullwidth"&gt;GM provided flights from Detroit to New York City and accommodation so Ars could attend its tech event. Ars does not accept paid editorial content.&lt;/aside&gt;
&lt;p&gt;General Motors held a preview event today to show the world what it’s working on. We’ve already seen some projects, like the further development of lithium manganese-rich battery technology or backup power for EVs that can power a home or support the power grid.&lt;/p&gt;
&lt;p&gt;The most significant new announcement is that Cadillac will offer an Escalade IQ with a so-called “Level 3” conditional automated driving system in 2028. GM is referring to it as a “hands off, eyes off” system and says it will integrate advanced digital mapping, use of lidar and other systems, and advanced machine learning to handle the driving duties in a controlled environment up to 80 mph (129 km/h).&lt;/p&gt;
&lt;p&gt;This means you can theoretically watch a movie from the driver’s seat while your car takes you down the highway to the airport. Over time, the system’s operation areas will expand to cover even more roads, making driving unnecessary in many situations—unless, of course, you like to drive.&lt;/p&gt;
&lt;p&gt;“We’re taking a safety-first approach,” CEO Mary Barra said to an audience of journalists at an event in New York City. “You’ll see us roll out much, much faster than what we did with Super Cruise.”&lt;/p&gt;
&lt;p&gt;So while the system will be limited to the Escalade at first, GM has made it clear that it wants to spread the system across its lineup. Barra even had the assembled journalists imagine a world in which your car anticipates your needs, takes you where you want to go, and services itself during downtime.&lt;/p&gt;
&lt;p&gt;Compared to Mercedes-Benz’s current offering, the advanced level 3 Super Cruise will work at a higher speed, and the team emphasized that the goal is for the system to work in all 50 states and in all weather conditions.&lt;/p&gt;
&lt;p&gt;Am I skeptical? Yes. But the company has a new liquid-cooled compute module to help handle the demands of a more advanced system. This would be the most advanced driver assistance system available to consumers in the US, and the company learned a lot through its now-shuttered Cruise autonomous vehicle program.&lt;/p&gt;
&lt;h2&gt;More AI&lt;/h2&gt;
&lt;p&gt;This new vehicle computer will allow GM to integrate more AI functionality into the vehicle. GM believes that incorporating Google Gemini (and eventually its own system) will help drivers better interact with a vehicle and its functions through natural language processing.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;I asked Dave Richardson, GM’s SVP of software, how the company will avoid the enshittification of vehicles as it integrates more AI.&lt;/p&gt;
&lt;p&gt;“There’s a lot of hype around AI right now,” he told me. “But there’s also practical use. I’ve been trying to focus the company on practical use cases. I think there’s a lot of pretty compelling things we can do to try to add real value.”&lt;/p&gt;
&lt;p&gt;He gave some examples, such as a car knowing you have a meeting and setting the navigation appropriately or knowing that you’re going on a road trip, so it should queue up the appropriate media for your kids to stream in the back seat.&lt;/p&gt;
&lt;p&gt;While the company is using Gemini at first, it eventually plans to have its own model on board. “With advanced processing in the car, we can handle interference on board so that it works in low-data-connection areas,” Richardson said.&lt;/p&gt;
&lt;p&gt;Ultimately, GM will deploy its own LLM that knows about the car and is limited in overall parameters, Richardson told me. It won’t need to rely on the cloud to operate, increasing responsiveness in the car and keeping personal information with you, he said.&lt;/p&gt;
&lt;p&gt;There are reasons to be skeptical, of course. One of my biggest concerns is how much driver data the car will collect. One reason GM doesn’t offer Android Auto or Apple CarPlay, the company has said, is that it wants to protect customer data. The owner must consent to any data sharing, GM said.&lt;/p&gt;
&lt;p&gt;And although GM says it has made some internal changes to protect customer data, there have been some very public instances of the company selling data. “Data privacy and security is priority one for us,” Richardson told me about his work at GM. He said he has hired people specifically tasked with ensuring that customer data protection frameworks are in place.&lt;/p&gt;
&lt;p&gt;“We have no interest in selling that data to third parties. When we think about data, whether it’s for Super Cruise or the AI, it’s really for us to develop the product and make it better. We don’t want to sell that data as the product itself,” he said.&lt;/p&gt;
&lt;p&gt;I believe there’s space for a privacy-focused automaker, and while I’m not sure whether that will be GM, I hope that privacy and data protection are as important to the company in the future as it says it is today.&lt;/p&gt;
&lt;p&gt;As for consumers wanting AI in their vehicles? GM thinks they do.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;








  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/cars/2025/10/ai-and-hands-free-driving-are-coming-to-gms-vehicles/</guid><pubDate>Wed, 22 Oct 2025 20:29:11 +0000</pubDate></item><item><title>[NEW] OpenAI requested memorial attendee list in ChatGPT suicide lawsuit (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/22/openai-requested-memorial-attendee-list-in-chatgpt-suicide-lawsuit/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2195918462.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI reportedly asked the Raine family — whose 16-year-old son Adam Raine died by suicide after prolonged conversations with ChatGPT — for a full list of attendees from the teenager’s memorial, signaling that the AI firm may try to subpoena friends and family. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI also requested “all documents relating to memorial services or events in the honor of the decedent, including but not limited to any videos or photographs taken, or eulogies given,” per a document obtained by the Financial Times.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Speaking to the FT, lawyers from the Raine family described the request as “intentional harassment.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new information comes as the Raine family updated its lawsuit against OpenAI on Wednesday. The family first filed a wrongful death suit against OpenAI in August after alleging their son had taken his own life following conversations with the chatbot about his mental health and suicidal ideation.&amp;nbsp;The updated lawsuit claims that OpenAI rushed GPT-4o’s May 2024 release by cutting safety testing due to competitive pressure.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The suit also claims that in February 2025, OpenAI weakened protections by removing suicide prevention from its “disallowed content” list, instead only advising the AI to “take care in risky situations.” The family argued that after this change, Adam’s ChatGPT usage surged from dozens of daily chats, with 1.6% containing self-harm content in January, to 300 daily chats in April, the month he died, with 17% containing such content.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a response to the amended lawsuit, OpenAI said: “Teen wellbeing is a top priority for us — minors deserve strong protections, especially in sensitive moments. We have safeguards in place today, such as [directing to] crisis hotlines, rerouting sensitive conversations to safer models, nudging for breaks during long sessions, and we’re continuing to strengthen them.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI recently began rolling out a new safety routing system and parental controls on ChatGPT. The routing system pushes more emotionally sensitive conversations to OpenAI’s newer model, GPT-5, which doesn’t have the same sycophantic tendencies as GPT-4o. And the parental controls allow parents to receive safety alerts in limited situations where the teen is potentially in danger of self-harm.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to OpenAI and the Raine family attorney.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2195918462.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI reportedly asked the Raine family — whose 16-year-old son Adam Raine died by suicide after prolonged conversations with ChatGPT — for a full list of attendees from the teenager’s memorial, signaling that the AI firm may try to subpoena friends and family. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI also requested “all documents relating to memorial services or events in the honor of the decedent, including but not limited to any videos or photographs taken, or eulogies given,” per a document obtained by the Financial Times.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Speaking to the FT, lawyers from the Raine family described the request as “intentional harassment.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new information comes as the Raine family updated its lawsuit against OpenAI on Wednesday. The family first filed a wrongful death suit against OpenAI in August after alleging their son had taken his own life following conversations with the chatbot about his mental health and suicidal ideation.&amp;nbsp;The updated lawsuit claims that OpenAI rushed GPT-4o’s May 2024 release by cutting safety testing due to competitive pressure.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The suit also claims that in February 2025, OpenAI weakened protections by removing suicide prevention from its “disallowed content” list, instead only advising the AI to “take care in risky situations.” The family argued that after this change, Adam’s ChatGPT usage surged from dozens of daily chats, with 1.6% containing self-harm content in January, to 300 daily chats in April, the month he died, with 17% containing such content.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a response to the amended lawsuit, OpenAI said: “Teen wellbeing is a top priority for us — minors deserve strong protections, especially in sensitive moments. We have safeguards in place today, such as [directing to] crisis hotlines, rerouting sensitive conversations to safer models, nudging for breaks during long sessions, and we’re continuing to strengthen them.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI recently began rolling out a new safety routing system and parental controls on ChatGPT. The routing system pushes more emotionally sensitive conversations to OpenAI’s newer model, GPT-5, which doesn’t have the same sycophantic tendencies as GPT-4o. And the parental controls allow parents to receive safety alerts in limited situations where the teen is potentially in danger of self-harm.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to OpenAI and the Raine family attorney.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/22/openai-requested-memorial-attendee-list-in-chatgpt-suicide-lawsuit/</guid><pubDate>Wed, 22 Oct 2025 20:49:19 +0000</pubDate></item><item><title>[NEW] Why Cohere’s ex-AI research lead is betting against the scaling race (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/22/why-coheres-ex-ai-research-lead-is-betting-against-the-scaling-race/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/03/Sara-Hooker_headshot.jpg?resize=900,1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI labs are racing to build data centers as large as Manhattan, each costing billions of dollars and consuming as much energy as a small city. The effort is driven by a deep belief in “scaling” — the idea that adding more computing power to existing AI training methods will eventually yield superintelligent systems capable of performing all kinds of tasks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But a growing chorus of AI researchers say the scaling of large language models may be reaching its limits, and that other breakthroughs may be needed to improve AI performance.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That’s the bet Sara Hooker, Cohere’s former VP of AI Research and a Google Brain alumna, is taking with her new startup, Adaption Labs. She co-founded the company with fellow Cohere and Google veteran Sudip Roy, and it’s built on the idea that scaling LLMs has become an inefficient way to squeeze more performance out of AI models. Hooker, who left Cohere in August, quietly announced the startup this month to start recruiting more broadly.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;I'm starting a new project.&lt;/p&gt;&lt;p&gt;Working on what I consider to be the most important problem: building thinking machines that adapt and continuously learn. &lt;/p&gt;&lt;p&gt;We have incredibly talent dense founding team + are hiring for engineering, ops, design. &lt;/p&gt;&lt;p&gt;Join us: https://t.co/eKlfWAfuRy&lt;/p&gt;— Sara Hooker (@sarahookr) October 7, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In an interview with TechCrunch, Hooker says Adaption Labs is building AI systems that can continuously adapt and learn from their real-world experiences, and do so extremely efficiently. She declined to share details about the methods behind this approach or whether the company relies on LLMs or another architecture.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There is a turning point now where it’s very clear that the formula of just scaling these models — scaling-pilled approaches, which are attractive but extremely boring — hasn’t produced intelligence that is able to navigate or interact with the world,” said Hooker.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Adapting is the “heart of learning,” according to Hooker. For example, stub your toe when you walk past your dining room table, and you’ll learn to step more carefully around it next time. AI labs have tried to capture this idea through reinforcement learning (RL), which allows AI models to learn from their mistakes in controlled settings. However, today’s RL methods don’t help AI models in production — meaning systems already being used by customers — to learn from their mistakes in real time. They just keep stubbing their toe.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some AI labs offer consulting services to help enterprises fine-tune their AI models to their custom needs, but it comes at a price. OpenAI reportedly requires customers to spend upward of $10 million with the company to offer its consulting services on fine-tuning.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“We have a handful of frontier labs that determine this set of AI models that are served the same way to everyone, and they’re very expensive to adapt,” said Hooker. “And actually, I think that doesn’t need to be true anymore, and AI systems can very efficiently learn from an environment. Proving that will completely change the dynamics of who gets to control and shape AI, and really, who these models serve at the end of the day.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Adaption Labs is the latest sign that the industry’s faith in scaling LLMs is wavering. A recent paper from MIT researchers found that the world’s largest AI models may soon show diminishing returns. The vibes in San Francisco seem to be shifting, too. The AI world’s favorite podcaster, Dwarkesh Patel, recently hosted some unusually skeptical conversations with famous AI researchers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Richard Sutton, a Turing award winner regarded as “the father of RL,” told Patel in September that LLMs can’t truly scale because they don’t learn from real-world experience. This month, early OpenAI employee Andrej Karpathy told Patel he had reservations about the long-term potential of RL to improve AI models.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;These types of fears aren’t unprecedented. In late 2024, some AI researchers raised concerns that scaling AI models through pretraining — in which AI models learn patterns from heaps of datasets — was hitting diminishing returns. Until then, pretraining had been the secret sauce for OpenAI and Google to improve their models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Those pretraining scaling concerns are now showing up in the data, but the AI industry has found other ways to improve models. In 2025, breakthroughs around AI reasoning models, which take additional time and computational resources to work through problems before answering, have pushed the capabilities of AI models even further.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI labs seem convinced that scaling up RL and AI reasoning models are the new frontier. OpenAI researchers previously told TechCrunch that they developed their first AI reasoning model, o1, because they thought it would scale up well. Meta and Periodic Labs researchers recently released a paper exploring how RL could scale performance further — a study that reportedly cost more than $4 million, underscoring how expensive current approaches remain.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Adaption Labs, by contrast, aims to find the next breakthrough and prove that learning from experience can be far cheaper. The startup was in talks to raise a $20 million to $40 million seed round earlier this fall, according to three investors who reviewed its pitch decks. They say the round has since closed, though the final amount is unclear. Hooker declined to comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re set up to be very ambitious,” said Hooker, when asked about her investors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hooker previously led Cohere Labs, where she trained small AI models for enterprise use cases. Compact AI systems now routinely outperform their larger counterparts on coding, math, and reasoning benchmarks — a trend Hooker wants to continue pushing on.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;She also built a reputation for broadening access to AI research globally, hiring research talent from underrepresented regions such as Africa. While Adaption Labs will open a San Francisco office soon, Hooker says she plans to hire worldwide.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If Hooker and Adaption Labs are right about the limitations of scaling, the implications could be huge. Billions have already been invested in scaling LLMs, with the assumption that bigger models will lead to general intelligence. But it’s possible that true adaptive learning could prove not only more powerful — but far more efficient.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Marina Temkin contributed reporting.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/03/Sara-Hooker_headshot.jpg?resize=900,1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI labs are racing to build data centers as large as Manhattan, each costing billions of dollars and consuming as much energy as a small city. The effort is driven by a deep belief in “scaling” — the idea that adding more computing power to existing AI training methods will eventually yield superintelligent systems capable of performing all kinds of tasks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But a growing chorus of AI researchers say the scaling of large language models may be reaching its limits, and that other breakthroughs may be needed to improve AI performance.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That’s the bet Sara Hooker, Cohere’s former VP of AI Research and a Google Brain alumna, is taking with her new startup, Adaption Labs. She co-founded the company with fellow Cohere and Google veteran Sudip Roy, and it’s built on the idea that scaling LLMs has become an inefficient way to squeeze more performance out of AI models. Hooker, who left Cohere in August, quietly announced the startup this month to start recruiting more broadly.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;I'm starting a new project.&lt;/p&gt;&lt;p&gt;Working on what I consider to be the most important problem: building thinking machines that adapt and continuously learn. &lt;/p&gt;&lt;p&gt;We have incredibly talent dense founding team + are hiring for engineering, ops, design. &lt;/p&gt;&lt;p&gt;Join us: https://t.co/eKlfWAfuRy&lt;/p&gt;— Sara Hooker (@sarahookr) October 7, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In an interview with TechCrunch, Hooker says Adaption Labs is building AI systems that can continuously adapt and learn from their real-world experiences, and do so extremely efficiently. She declined to share details about the methods behind this approach or whether the company relies on LLMs or another architecture.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There is a turning point now where it’s very clear that the formula of just scaling these models — scaling-pilled approaches, which are attractive but extremely boring — hasn’t produced intelligence that is able to navigate or interact with the world,” said Hooker.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Adapting is the “heart of learning,” according to Hooker. For example, stub your toe when you walk past your dining room table, and you’ll learn to step more carefully around it next time. AI labs have tried to capture this idea through reinforcement learning (RL), which allows AI models to learn from their mistakes in controlled settings. However, today’s RL methods don’t help AI models in production — meaning systems already being used by customers — to learn from their mistakes in real time. They just keep stubbing their toe.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some AI labs offer consulting services to help enterprises fine-tune their AI models to their custom needs, but it comes at a price. OpenAI reportedly requires customers to spend upward of $10 million with the company to offer its consulting services on fine-tuning.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“We have a handful of frontier labs that determine this set of AI models that are served the same way to everyone, and they’re very expensive to adapt,” said Hooker. “And actually, I think that doesn’t need to be true anymore, and AI systems can very efficiently learn from an environment. Proving that will completely change the dynamics of who gets to control and shape AI, and really, who these models serve at the end of the day.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Adaption Labs is the latest sign that the industry’s faith in scaling LLMs is wavering. A recent paper from MIT researchers found that the world’s largest AI models may soon show diminishing returns. The vibes in San Francisco seem to be shifting, too. The AI world’s favorite podcaster, Dwarkesh Patel, recently hosted some unusually skeptical conversations with famous AI researchers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Richard Sutton, a Turing award winner regarded as “the father of RL,” told Patel in September that LLMs can’t truly scale because they don’t learn from real-world experience. This month, early OpenAI employee Andrej Karpathy told Patel he had reservations about the long-term potential of RL to improve AI models.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;These types of fears aren’t unprecedented. In late 2024, some AI researchers raised concerns that scaling AI models through pretraining — in which AI models learn patterns from heaps of datasets — was hitting diminishing returns. Until then, pretraining had been the secret sauce for OpenAI and Google to improve their models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Those pretraining scaling concerns are now showing up in the data, but the AI industry has found other ways to improve models. In 2025, breakthroughs around AI reasoning models, which take additional time and computational resources to work through problems before answering, have pushed the capabilities of AI models even further.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI labs seem convinced that scaling up RL and AI reasoning models are the new frontier. OpenAI researchers previously told TechCrunch that they developed their first AI reasoning model, o1, because they thought it would scale up well. Meta and Periodic Labs researchers recently released a paper exploring how RL could scale performance further — a study that reportedly cost more than $4 million, underscoring how expensive current approaches remain.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Adaption Labs, by contrast, aims to find the next breakthrough and prove that learning from experience can be far cheaper. The startup was in talks to raise a $20 million to $40 million seed round earlier this fall, according to three investors who reviewed its pitch decks. They say the round has since closed, though the final amount is unclear. Hooker declined to comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re set up to be very ambitious,” said Hooker, when asked about her investors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hooker previously led Cohere Labs, where she trained small AI models for enterprise use cases. Compact AI systems now routinely outperform their larger counterparts on coding, math, and reasoning benchmarks — a trend Hooker wants to continue pushing on.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;She also built a reputation for broadening access to AI research globally, hiring research talent from underrepresented regions such as Africa. While Adaption Labs will open a San Francisco office soon, Hooker says she plans to hire worldwide.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If Hooker and Adaption Labs are right about the limitations of scaling, the implications could be huge. Billions have already been invested in scaling LLMs, with the assumption that bigger models will lead to general intelligence. But it’s possible that true adaptive learning could prove not only more powerful — but far more efficient.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Marina Temkin contributed reporting.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/22/why-coheres-ex-ai-research-lead-is-betting-against-the-scaling-race/</guid><pubDate>Wed, 22 Oct 2025 20:52:17 +0000</pubDate></item><item><title>[NEW] Snapchat makes its first open prompt AI Lens available for free in the US (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/22/snapchat-makes-its-first-open-prompt-ai-lens-available-for-free-in-the-us/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Imagine_Lens_16x9.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Snapchat is making its new “Imagine Lens,” the company’s first open prompt image-generation AI Lens, available to all users for free. The Lens was initially launched in September but only for paid subscribers. With the Lens, users can edit their own Snaps using custom prompts or generate their own.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, you could prompt the app, “Turn me into an alien” after snapping a selfie, or request an image of a “grumpy cat.” The company suggests you could use the Lens to try on Halloween costume ideas, or reimagine your friend in a new look, among other things.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The results can be shared with friends, posted to your Story on Snapchat, or shared beyond the app. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The broadened availability of the AI Lens follows the launch of AI video-generating apps from Meta (Meta AI) and OpenAI (Sora), which compete for young people’s attention with even more advanced AI tools and features than photo modifications. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, Sora lets users generate videos of themselves after providing the app with a one-time video and audio recording to capture their appearance. Friends can share these AI personas, dubbed “cameos,” with one another, so they can star in videos together.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That puts pressure on other social apps like Snapchat to keep up, so making the AI image Lens free to use seems a worthwhile investment for the app maker. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Until now, Snapchat’s AI Lens was only available to Lens+ and Snapchat Platinum Subscribers, the company says. With the expanded rollout, Snap is making a limited number of image generations available to all free users, as well. At launch, free users in the U.S. will be able to access the new Lens, with plans for other markets underway, starting with Canada, Great Britain, and Australia.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The Lens is found near the front of the in-app Lens Carousel, or you can search for it by name. To create, tap the caption to edit your prompt, or use one of the preloaded suggestions if you need inspiration.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company notes that Snapchat users access Lenses more than 8 billion times per day. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Imagine_Lens_16x9.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Snapchat is making its new “Imagine Lens,” the company’s first open prompt image-generation AI Lens, available to all users for free. The Lens was initially launched in September but only for paid subscribers. With the Lens, users can edit their own Snaps using custom prompts or generate their own.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, you could prompt the app, “Turn me into an alien” after snapping a selfie, or request an image of a “grumpy cat.” The company suggests you could use the Lens to try on Halloween costume ideas, or reimagine your friend in a new look, among other things.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The results can be shared with friends, posted to your Story on Snapchat, or shared beyond the app. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The broadened availability of the AI Lens follows the launch of AI video-generating apps from Meta (Meta AI) and OpenAI (Sora), which compete for young people’s attention with even more advanced AI tools and features than photo modifications. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, Sora lets users generate videos of themselves after providing the app with a one-time video and audio recording to capture their appearance. Friends can share these AI personas, dubbed “cameos,” with one another, so they can star in videos together.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That puts pressure on other social apps like Snapchat to keep up, so making the AI image Lens free to use seems a worthwhile investment for the app maker. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Until now, Snapchat’s AI Lens was only available to Lens+ and Snapchat Platinum Subscribers, the company says. With the expanded rollout, Snap is making a limited number of image generations available to all free users, as well. At launch, free users in the U.S. will be able to access the new Lens, with plans for other markets underway, starting with Canada, Great Britain, and Australia.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The Lens is found near the front of the in-app Lens Carousel, or you can search for it by name. To create, tap the caption to edit your prompt, or use one of the preloaded suggestions if you need inspiration.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company notes that Snapchat users access Lenses more than 8 billion times per day. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/22/snapchat-makes-its-first-open-prompt-ai-lens-available-for-free-in-the-us/</guid><pubDate>Wed, 22 Oct 2025 21:25:30 +0000</pubDate></item><item><title>[NEW] What enterprises can take away from Microsoft CEO Satya Nadella's shareholder letter (AI | VentureBeat)</title><link>https://venturebeat.com/ai/what-enterprises-can-take-away-from-microsoft-ceo-satya-nadellas-shareholder</link><description>[unable to retrieve full-text content]&lt;p&gt;One of the leading architects of the current generative AI boom — Microsoft CEO Satya Nadella, famed for having the software giant take an early investment in OpenAI (and later saying he was &amp;quot;&lt;a href="https://www.geekwire.com/2025/im-good-for-my-80-billion-what-microsoft-ceo-satya-nadella-really-meant-by-his-stargate-zinger/"&gt;good for my $80 billion&lt;/a&gt;&amp;quot;) — published his&lt;a href="https://www.linkedin.com/pulse/my-annual-letter-thinking-decades-executing-quarters-satya-nadella-7orpc/"&gt; latest annual letter yesterday on LinkedIn&lt;/a&gt; (a Microsoft subsidiary), and it&amp;#x27;s chock full of interesting ideas about the near-term future that enterprise technical decision makers would do well to pay attention to, as it could aid in their own planning and tech stack development.&lt;/p&gt;&lt;p&gt;In a companion &lt;a href="https://x.com/satyanadella/status/1980736083714535694"&gt;post on X&lt;/a&gt;, Nadella wrote, “AI is radically changing every layer of the tech stack, and we’re changing with it.&amp;quot; &lt;/p&gt;&lt;p&gt;The full letter reinforces that message: Microsoft sees itself not just participating in the AI revolution, but shaping its infrastructure, security, tooling and governance for decades to come.&lt;/p&gt;&lt;p&gt;While the message is addressed to Microsoft shareholders, the implications reach much further. The letter is a strategic signal to enterprise engineering leaders: CIOs, CTOs, AI leads, platform architects and security directors. Nadella outlines the direction of Microsoft’s innovation, but also what it expects from its customers and partners. The AI era is here, but it will be built by those who combine technical vision with operational discipline.&lt;/p&gt;&lt;p&gt;Below are the five most important takeaways for enterprise technical decision makers.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;1. Security and reliability are now the foundation of the AI stack&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Nadella makes security the first priority in the letter and ties it directly to Microsoft’s relevance going forward. Through its Secure Future Initiative (SFI), Microsoft has assigned the equivalent of 34,000 engineers to secure its identity systems, networks and software supply chain. Its Quality Excellence Initiative (QEI) aims to increase platform resiliency and strengthen global service uptime.&lt;/p&gt;&lt;p&gt;Microsoft’s positioning makes it clear that enterprises will no longer get away with “ship fast, harden later” AI deployments. Nadella calls security “non-negotiable,” signaling that AI infrastructure must now meet the standards of mission-critical software. That means identity-first architecture, zero-trust execution environments and change management discipline are now table stakes for enterprise AI.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;2. AI infrastructure strategy is hybrid, open and sovereignty-ready&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Nadella commits Microsoft to building “planet-scale systems” and backs that up with numbers: more than 400 Azure datacenters across 70 regions, two gigawatts of new compute capacity added this year, and new liquid-cooled GPU clusters rolling out across Azure. Microsoft also introduced Fairwater, a massive new AI datacenter in Wisconsin positioned to deliver unprecedented scale. Just as important, Microsoft is now officially multi-model. Azure AI Foundry offers access to more than 11,000 models including OpenAI, Meta, Mistral, Cohere and xAI. Microsoft is no longer pushing a single-model future, but a hybrid AI strategy.&lt;/p&gt;&lt;p&gt;Enterprises should interpret this as validation of “portfolio architectures,” where closed, open and domain-specific models coexist. Nadella also emphasizes growing investment in sovereign cloud offerings for regulated industries, previewing a world where AI systems will have to meet regional data residency and compliance requirements from day one.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;3. AI agents—not just chatbots—are now Microsoft’s future&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The AI shift inside Microsoft is no longer about copilots that answer questions. It is now about AI agents that perform work. Nadella points to the rollout of Agent Mode in Microsoft 365 Copilot, which turns natural language requests into multistep business workflows. GitHub Copilot evolves from code autocomplete into a “peer programmer” capable of executing tasks asynchronously. In security operations, Microsoft has deployed AI agents that autonomously respond to incidents. In healthcare, Copilot for Dragon Medical documents clinical encounters automatically.&lt;/p&gt;&lt;p&gt;This represents a major architectural pivot. Enterprises will need to move beyond prompt-response interfaces and begin engineering agent ecosystems that safely take actions inside business systems. That requires workflow orchestration, API integration strategies and strong guardrails. Nadella’s letter frames this as the next software platform shift.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;4. Unified data platforms are required to unlock AI value&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Nadella devotes significant attention to &lt;a href="https://venturebeat.com/data-infrastructure/enterprise-ai-success-is-about-more-than-just-data-its-about-knowledge-heres"&gt;Microsoft Fabric&lt;/a&gt; and OneLake, calling Fabric the company’s fastest-growing data and analytics product ever. Fabric promises to centralize enterprise data from multiple cloud and analytics environments. OneLake provides a universal storage layer that binds analytics and AI workloads together.&lt;/p&gt;&lt;p&gt;Microsoft’s message is blunt: siloed data means stalled AI. Enterprise teams that want AI at scale must unify operational and analytical data into a single architecture, enforce consistent data contracts and standardize metadata governance. AI success is now a data engineering problem more than a model problem.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;5. Trust, compliance and responsible AI are now mandatory for deployment&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;“People want technology they can trust,” Nadella writes. Microsoft now publishes Responsible AI Transparency Reports and aligns parts of its development process with UN human rights guidance. Microsoft is also committing to digital resilience in Europe and proactive safeguards against misuse of AI-generated content.&lt;/p&gt;&lt;p&gt;This shifts responsible AI out of the realm of corporate messaging and into engineering practice. Enterprises will need model documentation, reproducibility practices, audit trails, risk monitoring and human-in-the-loop checkpoints. Nadella signals that compliance will become integrated with product delivery—not an afterthought layered on top.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The real meaning of Microsoft’s AI strategy&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Taken together, these five pillars send a clear message to enterprise leaders: AI maturity is no longer about building prototypes or proving use cases. System-level readiness now defines success. Nadella frames Microsoft’s mission as helping customers “think in decades and execute in quarters,” and that is more than corporate poetry. It is a call to build AI platforms engineered for longevity.&lt;/p&gt;&lt;p&gt;The companies that win in enterprise AI will be the ones that invest early in secure cloud foundations, unify their data architectures, enable agent-based workflows and embrace responsible AI as a prerequisite for scale—not a press release. Nadella is betting that the next industrial transformation will be powered by AI infrastructure, not AI demos. With this letter, he has made Microsoft’s ambition clear: to become the platform on which that transformation is built.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;One of the leading architects of the current generative AI boom — Microsoft CEO Satya Nadella, famed for having the software giant take an early investment in OpenAI (and later saying he was &amp;quot;&lt;a href="https://www.geekwire.com/2025/im-good-for-my-80-billion-what-microsoft-ceo-satya-nadella-really-meant-by-his-stargate-zinger/"&gt;good for my $80 billion&lt;/a&gt;&amp;quot;) — published his&lt;a href="https://www.linkedin.com/pulse/my-annual-letter-thinking-decades-executing-quarters-satya-nadella-7orpc/"&gt; latest annual letter yesterday on LinkedIn&lt;/a&gt; (a Microsoft subsidiary), and it&amp;#x27;s chock full of interesting ideas about the near-term future that enterprise technical decision makers would do well to pay attention to, as it could aid in their own planning and tech stack development.&lt;/p&gt;&lt;p&gt;In a companion &lt;a href="https://x.com/satyanadella/status/1980736083714535694"&gt;post on X&lt;/a&gt;, Nadella wrote, “AI is radically changing every layer of the tech stack, and we’re changing with it.&amp;quot; &lt;/p&gt;&lt;p&gt;The full letter reinforces that message: Microsoft sees itself not just participating in the AI revolution, but shaping its infrastructure, security, tooling and governance for decades to come.&lt;/p&gt;&lt;p&gt;While the message is addressed to Microsoft shareholders, the implications reach much further. The letter is a strategic signal to enterprise engineering leaders: CIOs, CTOs, AI leads, platform architects and security directors. Nadella outlines the direction of Microsoft’s innovation, but also what it expects from its customers and partners. The AI era is here, but it will be built by those who combine technical vision with operational discipline.&lt;/p&gt;&lt;p&gt;Below are the five most important takeaways for enterprise technical decision makers.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;1. Security and reliability are now the foundation of the AI stack&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Nadella makes security the first priority in the letter and ties it directly to Microsoft’s relevance going forward. Through its Secure Future Initiative (SFI), Microsoft has assigned the equivalent of 34,000 engineers to secure its identity systems, networks and software supply chain. Its Quality Excellence Initiative (QEI) aims to increase platform resiliency and strengthen global service uptime.&lt;/p&gt;&lt;p&gt;Microsoft’s positioning makes it clear that enterprises will no longer get away with “ship fast, harden later” AI deployments. Nadella calls security “non-negotiable,” signaling that AI infrastructure must now meet the standards of mission-critical software. That means identity-first architecture, zero-trust execution environments and change management discipline are now table stakes for enterprise AI.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;2. AI infrastructure strategy is hybrid, open and sovereignty-ready&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Nadella commits Microsoft to building “planet-scale systems” and backs that up with numbers: more than 400 Azure datacenters across 70 regions, two gigawatts of new compute capacity added this year, and new liquid-cooled GPU clusters rolling out across Azure. Microsoft also introduced Fairwater, a massive new AI datacenter in Wisconsin positioned to deliver unprecedented scale. Just as important, Microsoft is now officially multi-model. Azure AI Foundry offers access to more than 11,000 models including OpenAI, Meta, Mistral, Cohere and xAI. Microsoft is no longer pushing a single-model future, but a hybrid AI strategy.&lt;/p&gt;&lt;p&gt;Enterprises should interpret this as validation of “portfolio architectures,” where closed, open and domain-specific models coexist. Nadella also emphasizes growing investment in sovereign cloud offerings for regulated industries, previewing a world where AI systems will have to meet regional data residency and compliance requirements from day one.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;3. AI agents—not just chatbots—are now Microsoft’s future&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The AI shift inside Microsoft is no longer about copilots that answer questions. It is now about AI agents that perform work. Nadella points to the rollout of Agent Mode in Microsoft 365 Copilot, which turns natural language requests into multistep business workflows. GitHub Copilot evolves from code autocomplete into a “peer programmer” capable of executing tasks asynchronously. In security operations, Microsoft has deployed AI agents that autonomously respond to incidents. In healthcare, Copilot for Dragon Medical documents clinical encounters automatically.&lt;/p&gt;&lt;p&gt;This represents a major architectural pivot. Enterprises will need to move beyond prompt-response interfaces and begin engineering agent ecosystems that safely take actions inside business systems. That requires workflow orchestration, API integration strategies and strong guardrails. Nadella’s letter frames this as the next software platform shift.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;4. Unified data platforms are required to unlock AI value&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Nadella devotes significant attention to &lt;a href="https://venturebeat.com/data-infrastructure/enterprise-ai-success-is-about-more-than-just-data-its-about-knowledge-heres"&gt;Microsoft Fabric&lt;/a&gt; and OneLake, calling Fabric the company’s fastest-growing data and analytics product ever. Fabric promises to centralize enterprise data from multiple cloud and analytics environments. OneLake provides a universal storage layer that binds analytics and AI workloads together.&lt;/p&gt;&lt;p&gt;Microsoft’s message is blunt: siloed data means stalled AI. Enterprise teams that want AI at scale must unify operational and analytical data into a single architecture, enforce consistent data contracts and standardize metadata governance. AI success is now a data engineering problem more than a model problem.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;5. Trust, compliance and responsible AI are now mandatory for deployment&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;“People want technology they can trust,” Nadella writes. Microsoft now publishes Responsible AI Transparency Reports and aligns parts of its development process with UN human rights guidance. Microsoft is also committing to digital resilience in Europe and proactive safeguards against misuse of AI-generated content.&lt;/p&gt;&lt;p&gt;This shifts responsible AI out of the realm of corporate messaging and into engineering practice. Enterprises will need model documentation, reproducibility practices, audit trails, risk monitoring and human-in-the-loop checkpoints. Nadella signals that compliance will become integrated with product delivery—not an afterthought layered on top.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The real meaning of Microsoft’s AI strategy&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Taken together, these five pillars send a clear message to enterprise leaders: AI maturity is no longer about building prototypes or proving use cases. System-level readiness now defines success. Nadella frames Microsoft’s mission as helping customers “think in decades and execute in quarters,” and that is more than corporate poetry. It is a call to build AI platforms engineered for longevity.&lt;/p&gt;&lt;p&gt;The companies that win in enterprise AI will be the ones that invest early in secure cloud foundations, unify their data architectures, enable agent-based workflows and embrace responsible AI as a prerequisite for scale—not a press release. Nadella is betting that the next industrial transformation will be powered by AI infrastructure, not AI demos. With this letter, he has made Microsoft’s ambition clear: to become the platform on which that transformation is built.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/what-enterprises-can-take-away-from-microsoft-ceo-satya-nadellas-shareholder</guid><pubDate>Thu, 23 Oct 2025 01:34:00 +0000</pubDate></item></channel></rss>