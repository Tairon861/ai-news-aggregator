<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 22 Jul 2025 01:57:06 +0000</lastBuildDate><item><title>The Gradient (The Gradient)</title><link>https://thegradient.pub/rss/</link><description>&amp;lt;![CDATA[The Gradient]]&amp;gt;https://thegradient.pub/https://thegradient.pub/favicon.pngThe Gradienthttps://thegradient.pub/Ghost 5.33Tue, 22 Jul 2025 02:01:08 GMT60&amp;lt;![CDATA[AGI Is Not Multimodal]]&amp;gt;"In projecting language back as the model for thought, we lose sight of the tacit embodied understanding that undergirds our intelligence." –Terry Winograd&lt;p&gt;The recent successes of generative AI models have convinced some that AGI is imminent. While these models appear to capture the essence of human&lt;/p&gt;]]&amp;gt;https://thegradient.pub/agi-is-not-multimodal/683fb98b77c3d76051ac142cWed, 04 Jun 2025 14:00:29 GMT"In projecting language back as the model for thought, we lose sight of the tacit embodied understanding that undergirds our intelligence." –Terry Winograd&lt;img alt="AGI Is Not Multimodal" src="https://thegradient.pub/content/images/2025/06/Gradient_Article_Art3-downscaled.png" /&gt;&lt;p&gt;The recent successes of generative AI models have convinced some that AGI is imminent. While these models appear to capture the essence of human intelligence, they defy even our most basic intuitions about it. They have emerged not because they are thoughtful solutions to the problem of intelligence, but because they &lt;em&gt;scaled&lt;/em&gt; effectively on hardware we already had. Seduced by the fruits of scale, some have come to believe that it provides a clear pathway to AGI. The most emblematic case of this is the multimodal approach, in which massive modular networks are optimized for an array of modalities that, taken together, &lt;em&gt;appear&lt;/em&gt; general. However, I argue that this strategy is sure to fail in the near term; it will not lead to human-level AGI that can, e.g., perform sensorimotor reasoning, motion planning, and social coordination. Instead of trying to glue modalities together into a patchwork AGI, &lt;strong&gt;we should pursue approaches to intelligence that treat embodiment and interaction with the environment as primary&lt;/strong&gt;, and see modality-centered processing as emergent phenomena.&lt;/p&gt;&lt;p&gt;Preface: Disembodied definitions of Artificial General Intelligence — emphasis on &lt;em&gt;general&lt;/em&gt; — exclude crucial problem spaces that we should expect AGI to be able to solve. &lt;strong&gt;A true AGI must be general across all domains.&lt;/strong&gt; Any &lt;em&gt;complete&lt;/em&gt; definition must at least include the ability to solve problems that originate in physical reality, e.g. repairing a car, untying a knot, preparing food, etc. As I will discuss in the next section, &lt;strong&gt;what is needed for these problems is a form of intelligence that is fundamentally situated in something like a physical world model&lt;/strong&gt;. For more discussion on this, look out for &lt;em&gt;Designing an Intelligence&lt;/em&gt;. Edited by George Konidaris, MIT Press, forthcoming.&lt;br /&gt;&lt;/p&gt;&lt;h2 id="why-we-need-the-world-and-how-llms-pretend-to-understand-it"&gt;Why We Need the World, and How LLMs Pretend to Understand It&lt;/h2&gt;&lt;p&gt;TLDR: I first argue that &lt;strong&gt;true AGI needs a physical understanding of the world&lt;/strong&gt;, as many problems cannot be converted into a problem of symbol manipulation. It has been suggested by some that LLMs are learning a model of the world through next token prediction, but it is more likely that LLMs are learning bags of heuristics to predict tokens. This leaves them with a superficial understanding of reality and contributes to false impressions of their intelligence.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;The most shocking result of the predict-next-token objective is that it yields AI models that reflect a deeply human-like understanding of the world, despite having never observed it like we have.&lt;/strong&gt; This result has led to confusion about what it means to &lt;em&gt;understand language&lt;/em&gt; and even to &lt;em&gt;understand the world&lt;/em&gt; — something we have long believed to be a prerequisite for language understanding. &lt;strong&gt;One explanation for the capabilities of LLMs comes from &lt;/strong&gt;&lt;strong&gt;an emerging theory&lt;/strong&gt;&lt;strong&gt; suggesting that they induce models of the world through next-token prediction&lt;/strong&gt;. Proponents of this theory cite the prowess of SOTA LLMs on various benchmarks, the convergence of large models to similar internal representations, and their favorite rendition of the idea that “language mirrors the structure of reality,” a notion that has been espoused at least by Plato, Wittgenstein, Foucault, and Eco. While I’m generally in support of digging up esoteric texts for research inspiration, I’m worried that this metaphor has been taken too literally. Do LLMs really learn implicit models of the world? How could they otherwise be so proficient at language?&lt;/p&gt;&lt;p&gt;One source of evidence in favor of the LLM world modeling hypothesis is the Othello paper, wherein researchers were able to predict the board of an Othello game from the hidden states of a transformer model trained on sequences of &lt;em&gt;legal&lt;/em&gt; &lt;em&gt;moves&lt;/em&gt;. However, there are &lt;em&gt;many&lt;/em&gt; issues with generalizing these results to models of natural language. For one, whereas Othello moves can &lt;em&gt;provably&lt;/em&gt; be used to deduce the full state of an Othello board,&lt;strong&gt; we have no reason to believe that a complete picture of the physical world can be inferred by a linguistic description. &lt;/strong&gt;What sets the game of Othello apart from many tasks in the physical world is that &lt;strong&gt;Othello fundamentally resides in the land of symbols, and is merely implemented using physical tokens to make it easier for humans to play.&lt;/strong&gt; A full game of Othello can be played with just pen and paper, but one can’t, e.g., sweep a floor, do dishes, or drive a car with just pen and paper. To solve such tasks, you need some physical conception of the world beyond what humans can merely &lt;em&gt;say&lt;/em&gt; about it. Whether that conception of the world is encoded in a formal world model or, e.g., a value function is up for debate, but it is clear that &lt;strong&gt;there are many problems in the physical world that &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;cannot&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; be &lt;/strong&gt;&lt;strong&gt;fully represented by a system of symbols&lt;/strong&gt;&lt;strong&gt; and solved with mere symbol manipulation.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Another issue stated in Melanie Mitchell’s recent piece and supported by this paper, is that there is evidence that &lt;strong&gt;generative models can score remarkably well on sequence prediction tasks while failing to learn models of the worlds that created such sequence data&lt;/strong&gt;, e.g. by learning comprehensive sets of idiosyncratic heuristics. E.g., it was pointed out in this blog post that OthelloGPT learned sequence prediction rules that don’t actually hold for all possible Othello games, like “if the token for B4 does not appear before A4 in the input string, then B4 is empty.” While one can argue that it doesn’t matter &lt;em&gt;how&lt;/em&gt; a world model predicts the next state of the world, it should raise suspicion when that prediction reflects a better understanding of the training data than the underlying world that led to such data. This, unfortunately, is the central fault of the predict-next-token objective, which seeks only to retain information relevant to the prediction of the next token. &lt;strong&gt;If it can be done with something easier to learn than a world model, it likely will be.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;To claim without caveat that predicting the &lt;em&gt;effects of earlier symbols on later symbols&lt;/em&gt; requires a model of the world like the ones humans generate from perception would be to abuse the “world model” notion. Unless we disagree on what the world is, it should be clear that a &lt;em&gt;true&lt;/em&gt; world model can be used to predict the next state of the &lt;em&gt;physical&lt;/em&gt; world given a history of states. Similar world models, which predict high fidelity observations of the physical world, are leveraged in many subfields of AI including model-based reinforcement learning, task and motion planning in robotics, causal world modeling, and areas of computer vision to solve problems instantiated in physical reality. LLMs are simply not running physics simulations in their latent next-token calculus when they ask you if your person, place, or thing is bigger than a breadbox. In fact, I conjecture that &lt;strong&gt;the behavior of LLMs is not thanks to a learned world model, but to brute force memorization of incomprehensibly abstract rules governing the behavior of symbols, i.e. a model of &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;syntax&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Quick primer:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Syntax&lt;/strong&gt; is a subfield of linguistics that studies how words of various grammatical categories (e.g. parts of speech) are arranged together into sentences, which can be parsed into syntax trees. &lt;em&gt;Syntax studies the structure of sentences and the atomic parts of speech that compose them.&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;strong&gt;Semantics&lt;/strong&gt; is another subfield concerned with the literal meaning of sentences, e.g., compiling “I am feeling chilly” into the &lt;em&gt;idea&lt;/em&gt; that you are experiencing cold. &lt;em&gt;Semantics boils language down to literal meaning, which is information about the world or human experience.&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Pragmatics&lt;/strong&gt; studies the interplay of physical and conversational context on speech interactions, like when someone knows to close an ajar window when you tell them “I am feeling chilly.” &lt;em&gt;Pragmatics involves interpreting speech while reasoning about the environment and the intentions and hidden knowledge of other agents.&lt;/em&gt;&lt;/li&gt;&lt;/ul&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;Without getting too technical, there is intuitive evidence that somewhat separate systems of cognition are responsible for each of these linguistic faculties. Look no further than the capability for humans to generate syntactically well-formed sentences that have no semantic meaning, e.g. Chomsky’s famous sentence “Colorless green ideas sleep furiously,” or sentences with well-formed semantics that make no pragmatic sense, e.g. responding merely with “Yes, I can” when asked, “Can you pass the salt?” Crucially, &lt;strong&gt;it is the fusion of the disparate cognitive abilities underpinning them that coalesce into human language understanding.&lt;/strong&gt; For example, there isn’t anything syntactically wrong with the sentence, “The fridge is in the apple,” as a syntactic account of “the fridge” and “the apple” would categorize them as noun phrases that can be used to produce a sentence with the production rule, S → (NP “is in” NP). However, &lt;strong&gt;humans recognize an obvious semantic failure in the sentence that becomes apparent after attempting to reconcile its meaning with our understanding of reality&lt;/strong&gt;: we know that fridges are larger than apples, and could not be fit into them.&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;But what if you have never perceived the real world, yet still were trying to figure out whether the sentence was ill-formed? &lt;strong&gt;One solution could be to embed semantic information at the level of syntax&lt;/strong&gt;, e.g., by inventing new syntactic categories, NP&lt;sub&gt;the fridge&lt;/sub&gt; and NP&lt;sub&gt;the apple &lt;/sub&gt;, and a single new production rule that prevents semantic misuse: S → (NP&lt;sub&gt;the apple&lt;/sub&gt; “is in” NP&lt;sub&gt;the fridge &lt;/sub&gt;). While this strategy would no longer require grounded world knowledge about fridges and apples, e.g., &lt;strong&gt;it would require special grammar rules for every semantically well-formed construction… which is actually possible to learn given a massive corpus of natural language.&lt;/strong&gt; Crucially, this would not be the same thing as grasping semantics, which in my view is fundamentally about understanding the nature of the world.&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;Finding that LLMs have reduced problems of semantics and pragmatics into syntax would have profound implications on how we should view their intelligence. People often treat language proficiency as a proxy for general intelligence by, e.g., strongly associating pragmatic and semantic understanding with the cognitive abilities that undergird them in humans. For example, someone who appears well-read and graceful in navigating social interactions is likely to score high in traits like sustained attention and theory of mind, which lie closer to measures of raw cognitive ability. In general, these proxies are reasonable for assessing a &lt;em&gt;person’s&lt;/em&gt; general intelligence, but not an LLM’s, as the apparent linguistic skills of LLMs could come from entirely separate mechanisms of cognition.&lt;/p&gt;&lt;h2 id="the-bitter-lesson-revisited"&gt;The Bitter Lesson Revisited&lt;/h2&gt;&lt;p&gt;TLDR: Sutton’s Bitter Lesson has sometimes been interpreted as meaning that making &lt;em&gt;any&lt;/em&gt; assumptions about the structure of AI is a mistake. This is both unproductive and a misinterpretation; it is precisely when humans think deeply about the structure of intelligence that major advancements occur. Despite this, scale maximalists have implicitly suggested that multimodal models can be a structure-agnostic framework for AGI. Ironically, today’s multimodal models contradict Sutton’s Bitter Lesson by making implicit assumptions about the structure of individual modalities and how they should be sewn together. &lt;strong&gt;In order to build AGI, we must either think deeply about how to unite existing modalities, or dispense with them altogether in favor of an interactive and embodied cognitive process.&lt;/strong&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="AGI Is Not Multimodal" class="kg-image" height="733" src="https://thegradient.pub/content/images/2025/06/ChatGPT-Image-May-21--2025--11_31_12-AM-copy.png" width="1024" /&gt;&lt;/figure&gt;&lt;p&gt;The paradigm that led to the success of LLMs is marked primarily by &lt;em&gt;scale&lt;/em&gt;, not efficiency. We have effectively trained a pile of one trillion ants for one billion years to mimic the form and function of a Formula 1 race car; eventually it gets there, but wow was the process inefficient. This analogy nicely captures a debate between structuralists, who want to build things like "wheels" and "axles" into AI systems, and scale maximalists, who want more ants, years, and F1 races to train on. Despite many decades of structuralist study in linguistics, the unstructured approaches of scale maximalism have yielded far better ant-racecars in recent years. This was most notably articulated by Rich Sutton — a recent recipient of the Turing Award along with Andy Barto for their work in Reinforcement Learning — in his piece “The Bitter Lesson.”&lt;/p&gt;&lt;p&gt;[W]e should build in only the meta-methods that can find and capture this arbitrary complexity… Essential to these methods is that they can find good approximations, but the search for them should be by our methods, not by us. We want AI agents that can discover like we can, not which contain what we have discovered. - Rich Sutton&lt;/p&gt;&lt;p&gt;Sutton’s argument is that methods that leverage computational resources will outpace methods that do not, and that any structure for problem-solving built as an inductive bias into AI will hinder it from learning better solutions. &lt;strong&gt;This is a compelling argument&lt;/strong&gt; &lt;strong&gt;that I believe has been seriously misinterpreted by some as implying that making &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;any&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; assumptions about structure is a false step.&lt;/strong&gt; It is, in fact, human intuition that was responsible for many significant advancements in the development of SOTA neural network architectures. For example, Convolutional Neural Networks made an assumption about translation invariance for pattern recognition in images and kickstarted the modern field of deep learning for computer vision; the attention mechanism of Transformers made an assumption about the long-distance relationships between symbols in a sentence that made ChatGPT possible and had nearly everyone drop their RNNs; and 3D Gaussian Splatting made an assumption about the solidity of physical objects that made it more performant than NeRFs. Potentially none of these methodological assumptions apply to the entire domain of &lt;em&gt;possible&lt;/em&gt; scenes, images, or token streams, but they do for the specific ones that humans have curated and formed structural intuitions about. Let’s not forget that humans have co-evolved with the environments that these datasets are drawn from.&lt;/p&gt;&lt;p&gt;The real question is how we might heed Sutton’s Bitter Lesson in our development of AGI. The scale maximalist approach worked for LLMs and LVMs (large vision models) because we had natural deposits of text and image data, but &lt;strong&gt;an analogous application of scale maximalism to AGI would require forms of embodiment data that we simply don’t have. &lt;/strong&gt;One solution to this data scarcity issue extends the generative modeling paradigm to multimodal modeling — encompassing language, vision, and action — &lt;strong&gt;with the hope that a general intelligence can be built by summing together general models of narrow modalities.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;There are multiple issues with this approach. First, &lt;strong&gt;there are deep connections between modalities that are unnaturally severed in the multimodal setting&lt;/strong&gt;, making the problem of concept synthesis ever more difficult. In practice, uniting modalities often involves pre-training dedicated neural modules for each modality and joining them together into a joint embedding space. In the early days, this was achieved by nudging the embeddings of, e.g. (language, vision, action) tuples to converge to similar latent vectors of meaning, a vast oversimplification of the kinds of relationships that may exist between modalities. One can imagine, e.g., captioning an image at various levels of abstraction, or implementing the same linguistic instruction with different sets of physical actions. Such one-to-many relationships suggest that a contrastive embedding objective is not suitable.&lt;/p&gt;&lt;p&gt;While modern approaches do not make such stringent assumptions about how modalities should be united, they still universally encode percepts from all modalities (e.g. text, images) into the same latent space. Intuitively, it would seem that such latent spaces could serve as common conceptual ground across modalities, analogous to a space of human concepts. However, these latent spaces do not cogently capture all information relevant to a concept, and instead rely on modality-specific decoders to flesh out important details. &lt;strong&gt;The “meaning” of a percept is not &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;in&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; the vector it is encoded as, but in the way relevant decoders process this vector into meaningful outputs. &lt;/strong&gt;As long as various encoders and decoders are subject to modality-specific training objectives, “meaning” will be decentralized and potentially inconsistent across modalities, especially as a result of pre-training. This is not a recipe for the formation of coherent concepts.&lt;/p&gt;&lt;p&gt;Furthermore, it is not clear that today’s modalities are an appropriate partitioning of the observation and action spaces for an embodied agent. It is not obvious that, e.g., images and text should be represented as separate observation streams, nor text production and motion planning as separate action capabilities. &lt;strong&gt;The human capacities for reading, seeing, speaking, and moving are ultimately mediated by overlapping cognitive structures.&lt;/strong&gt; &lt;strong&gt;Making structural assumptions about how modalities ought to be processed is likely to hinder the discovery of more fundamental cognition &lt;/strong&gt;that is responsible for processing data in all modalities. One solution would be to consolidate unnaturally partitioned modalities into a unified data representation. This would encourage networks to learn intelligent processes that generalize across modalities. Intuitively, &lt;strong&gt;a model that can understand the visual world as well as humans can&lt;/strong&gt; — including everything from human writing to traffic signs to visual art — &lt;strong&gt;should not make a serious architectural distinction between images and text. &lt;/strong&gt;Part of the reason why VLMs can’t, e.g., count the number of letters in a word is because they can’t &lt;em&gt;see&lt;/em&gt; what they are writing.&lt;/p&gt;&lt;p&gt;Finally, the &lt;strong&gt;learn-from-scale approach trains models to copy the conceptual structure of humans instead of learning the general capability to form novel concepts on their own.&lt;/strong&gt; Humans have spent hundreds of thousands of years refining concepts and passing them memetically through culture and language. Today’s models are trained only on the end result of this process: the present-day conceptual structures that make it into the corpus. By optimizing for the ultimate products of our intelligence, we have ignored the question of how those products were invented and discovered. Humans have a unique ability to form durable concepts from few examples and ascribe names to them, reason about them analogically, etc. While the in-context capabilities of today’s models can be impressive, they grow increasingly limited as tasks become more complex and stray further from the training data. &lt;strong&gt;The flexibility to form new concepts from experience is a foundational attribute of general intelligence&lt;/strong&gt;, we should think carefully about how it arises.&lt;/p&gt;&lt;p&gt;While structure-agnostic scale maximalism has succeeded in producing LLMs and LVMs that pass Turing tests, a multimodal scale maximalist approach to AGI will not bear similar fruit. &lt;strong&gt;Instead of pre-supposing structure in individual modalities, we should design a setting in which modality-specific processing emerges naturally.&lt;/strong&gt; For example, my recent paper on visual theory of mind saw abstract symbols naturally emerge from communication between image-classifying agents, blurring the lines between text and image processing. Eventually, we should hope to reintegrate as many features of intelligence as possible under the same umbrella. However, it is not clear whether there is genuine commercial viability in such an approach as long as scaling and fine-tuning narrow intelligence models solves commercial use-cases.&lt;/p&gt;&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;&lt;p&gt;The overall promise of scale maximalism is that a Frankenstein AGI can be sewed together using general models of narrow domains. I argue that this is extremely unlikely to yield an AGI that feels complete in its intelligence. If we intend to continue reaping the streamlined efficiency of modality-specific processing, we must be intentional in how modalities are united — ideally drawing from human intuition and classical fields of study, e.g. this work from MIT. Alternatively, we can re-formulate learning as an embodied and interactive process where disparate modalities naturally fuse together. We could do this by, e.g., processing images, text, and video using the same perception system and producing actions for generating text, manipulating objects, and navigating environments using the same action system. What we will lose in efficiency we will gain in flexible cognitive ability.&lt;/p&gt;&lt;p&gt;In a sense, the most challenging mathematical piece of the AGI puzzle has already been solved: the discovery of universal function approximators. What’s left is to inventory the functions we need and determine how they ought to be arranged into a coherent whole. This is a conceptual problem, not a mathematical one.&lt;/p&gt;&lt;hr /&gt;&lt;h2 id="acknowledgements"&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;I would like to thank Lucas Gelfond, Daniel Bashir, George Konidaris, and my father, Joseph Spiegel, for their thoughtful and thorough feedback on this work. Thanks to Alina Pringle for the wonderful illustration made for this piece.&lt;/p&gt;&lt;h2 id="author-bio"&gt;Author Bio&lt;/h2&gt;&lt;p&gt;Benjamin is a PhD candidate in Computer Science at Brown University. He is interested in models of language understanding that ground meaning to elements of structured decision-making. For more info see his personal website.&lt;/p&gt;&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;pre&gt;&lt;code&gt;Benjamin A. Spiegel, "AGI Is Not Multimodal", The Gradient, 2025.
&lt;/code&gt;&lt;/pre&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;pre&gt;&lt;code&gt;@article{spiegel2025agi,
    author = {Benjamin A. Spiegel},
    title = {AGI Is Not Multimodal},
    journal = {The Gradient},
    year = {2025},
    howpublished = {\url{https://thegradient.pub/agi-is-not-multimodal},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;h2 id="references"&gt;References&lt;/h2&gt;&lt;p&gt;Andreas, Jacob. “Language Models, World Models, and Human Model-Building.” &lt;em&gt;Mit.edu&lt;/em&gt;, 2024, lingo.csail.mit.edu/blog/world_models/.&lt;/p&gt;&lt;p&gt;Belkin, Mikhail, et al. "Reconciling modern machine-learning practice and the classical bias–variance trade-off." &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt; 116.32 (2019): 15849-15854.&lt;/p&gt;&lt;p&gt;Bernhard Kerbl, et al. “3D Gaussian Splatting for Real-Time Radiance Field Rendering.” &lt;em&gt;ACM Transactions on Graphics&lt;/em&gt;, vol. 42, no. 4, 26 July 2023, pp. 1–14, https://doi.org/10.1145/3592433.&lt;/p&gt;&lt;p&gt;Chomsky, Noam. 1965. Aspects of the theory of syntax. Cambridge, Massachusetts: MIT Press.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Designing an Intelligence&lt;/em&gt;. Edited by George Konidaris, MIT Press, 2026.&lt;/p&gt;&lt;p&gt;Emily M. Bender and Alexander Koller. 2020. Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data. In &lt;em&gt;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&lt;/em&gt;, pages 5185–5198, Online. Association for Computational Linguistics.&lt;/p&gt;&lt;p&gt;Eye on AI. “The Mastermind behind GPT-4 and the Future of AI | Ilya Sutskever.” &lt;em&gt;YouTube&lt;/em&gt;, 15 Mar. 2023, www.youtube.com/watch?v=SjhIlw3Iffs&amp;amp;list=PLpdlTIkm0-jJ4gJyeLvH1PJCEHp3NAYf4&amp;amp;index=64. Accessed 18 May 2025.&lt;/p&gt;&lt;p&gt;Frank, Michael C. “Bridging the data gap between children and large language models.” &lt;em&gt;Trends in cognitive sciences&lt;/em&gt; vol. 27,11 (2023): 990-992. doi:10.1016/j.tics.2023.08.007&lt;/p&gt;&lt;p&gt;Garrett, Caelan Reed, et al. "Integrated task and motion planning." &lt;em&gt;Annual review of control, robotics, and autonomous systems&lt;/em&gt; 4.1 (2021): 265-293.APA&lt;/p&gt;&lt;p&gt;Goodhart, C.A.E. (1984). Problems of Monetary Management: The UK Experience. In: Monetary Theory and Practice. Palgrave, London. https://doi.org/10.1007/978-1-349-17295-5_4&lt;/p&gt;&lt;p&gt;Hooker, Sara. The hardware lottery. Commun. ACM 64, 12 (December 2021), 58–65. https://doi.org/10.1145/3467017&lt;/p&gt;&lt;p&gt;Huh, Minyoung, et al. "The Platonic Representation Hypothesis." &lt;em&gt;Forty-first International Conference on Machine Learning&lt;/em&gt;. 2024.&lt;/p&gt;&lt;p&gt;Kaplan, Jared, et al. "Scaling laws for neural language models." &lt;em&gt;arXiv preprint arXiv:2001.08361&lt;/em&gt; (2020).&lt;/p&gt;&lt;p&gt;Lake, Brenden M. et al. “Building Machines That Learn and Think like People.” &lt;em&gt;Behavioral and Brain Sciences&lt;/em&gt; 40 (2017): e253. Web.&lt;/p&gt;&lt;p&gt;Li, Kenneth, et al. "Emergent world representations: Exploring a sequence model trained on a synthetic task." &lt;em&gt;ICLR&lt;/em&gt; (2023).&lt;/p&gt;&lt;p&gt;Luiten, Jonathon, Georgios, Kopanas, Bastian, Leibe, Deva, Ramanan. "Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis." &lt;em&gt;3DV&lt;/em&gt;. 2024.&lt;/p&gt;&lt;p&gt;Mao, Jiayuan, Chuang, Gan, Pushmeet, Kohli, Joshua B., Tenenbaum, Jiajun, Wu. "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision." &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. 2019.&lt;/p&gt;&lt;p&gt;Mitchell, Melanie. “LLMs and World Models, Part 1.” &lt;em&gt;Substack.com&lt;/em&gt;, AI: A Guide for Thinking Humans, 13 Feb. 2025, aiguide.substack.com/p/llms-and-world-models-part-1. Accessed 18 May 2025.&lt;/p&gt;&lt;p&gt;Mu, Norman. “Norman Mu | the Myth of Data Inefficiency in Large Language Models.” &lt;em&gt;Normanmu.com&lt;/em&gt;, 14 Feb. 2025, www.normanmu.com/2025/02/14/data-inefficiency-llms.html. Accessed 18 May 2025.&lt;/p&gt;&lt;p&gt;Newell, Allen, and Herbert A. Simon. “Computer Science as Empirical Inquiry: Symbols and Search.” &lt;em&gt;Communications of the ACM&lt;/em&gt;, vol. 19, no. 3, 1 Mar. 1976, pp. 113–126, https://doi.org/10.1145/360018.360022.&lt;/p&gt;&lt;p&gt;Peng, Hao, et al. “When Does In-Context Learning Fall Short and Why? A Study on Specification-Heavy Tasks.” &lt;em&gt;ArXiv.org&lt;/em&gt;, 2023, arxiv.org/abs/2311.08993.&lt;/p&gt;&lt;p&gt;Spiegel, Benjamin, et al. “Visual Theory of Mind Enables the Invention of Early Writing Systems.” &lt;em&gt;CogSci&lt;/em&gt;, 2025, arxiv.org/abs/2502.01568.&lt;/p&gt;&lt;p&gt;Sutton, Richard S. &lt;em&gt;Introduction to Reinforcement Learning&lt;/em&gt;. Cambridge, Mass, Mit Press, 04-98, 1998.&lt;/p&gt;&lt;p&gt;Vafa, Keyon, et al. "Evaluating the world model implicit in a generative model." &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 37 (2024): 26941-26975.&lt;/p&gt;&lt;p&gt;Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia (December 2017). "Attention is All you Need". In I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett (ed.). &lt;em&gt;31st Conference on Neural Information Processing Systems (NIPS)&lt;/em&gt;. Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc. arXiv:1706.03762.&lt;/p&gt;&lt;p&gt;Winograd, Terry. “Thinking Machines: Can There Be? Are We?” &lt;em&gt;The Boundaries of Humanity: Humans, Animals, Machines&lt;/em&gt;, edited by James Sheehan and Morton Sosna, Berkeley: University of California Press, 1991, pp. 198–223.&lt;/p&gt;&lt;p&gt;Wu, Shangda, et al. "Beyond language models: Byte models are digital world simulators." &lt;em&gt;arXiv preprint arXiv:2402.19155&lt;/em&gt; (2024). &lt;/p&gt;]]&amp;gt;&amp;lt;![CDATA[Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research]]&amp;gt;What is the Role of Mathematics in Modern Machine Learning?&lt;p&gt;The past decade has witnessed a shift in how progress is made in machine learning. Research involving carefully designed and mathematically principled architectures result in only marginal improvements while compute-intensive and engineering-first efforts that scale to ever larger training sets&lt;/p&gt;]]&amp;gt;https://thegradient.pub/shape-symmetry-structure/673686c693571d5c8c155078Sat, 16 Nov 2024 16:46:15 GMTWhat is the Role of Mathematics in Modern Machine Learning?&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" src="https://thegradient.pub/content/images/2024/11/DALL-E-2024-11-15-15.40.52---Create-an-abstract-image-that-illustrates-how-ReLU-based-neural-networks-shatter-input-space-into-numerous-polygonal-regions--each-behaving-like-a-lin.webp" /&gt;&lt;p&gt;The past decade has witnessed a shift in how progress is made in machine learning. Research involving carefully designed and mathematically principled architectures result in only marginal improvements while compute-intensive and engineering-first efforts that scale to ever larger training sets and model parameter counts result in remarkable new capabilities unpredicted by existing theory. Mathematics and statistics, once the primary guides of machine learning research, now struggle to provide immediate insight into the latest breakthroughs. This is not the first time that empirical progress in machine learning has outpaced more theory-motivated approaches, yet the magnitude of recent advances has forced us to swallow the bitter pill of the “Bitter Lesson” yet again [1].&lt;/p&gt;&lt;p&gt;This shift has prompted speculation about mathematics’ diminished role in machine learning research moving forward. It is already evident that mathematics will have to share the stage with a broader range of perspectives (for instance, biology which has deep experience drawing conclusions about irreducibly complex systems or the social sciences as AI is integrated ever more deeply into society). The increasingly interdisciplinary nature of machine learning should be welcomed as a positive development by all researchers.&lt;/p&gt;&lt;p&gt;However, we argue that mathematics remains as relevant as ever; its role is simply evolving. For example, whereas mathematics might once have primarily provided theoretical guarantees on model performance, it may soon be more commonly used for post-hoc explanations of empirical phenomena observed in model training and performance–a role analogous to one that it plays in physics. Similarly, while mathematical intuition might once have guided the design of handcrafted features or architectural details at a granular level, its use may shift to higher-level design choices such as matching architecture to underlying task structure or data symmetries.&lt;/p&gt;&lt;p&gt;None of this is completely new. Mathematics has always served multiple purposes in machine learning. After all, the translation equivariant convolutional neural network, which exemplifies the idea of architecture matching data symmetries mentioned above is now over 40 years old. What’s changing are the kinds of problems where mathematics will have the greatest impact and the ways it will most commonly be applied.&lt;/p&gt;&lt;p&gt;An intriguing consequence of the shift towards scale is that it has broadened the scope of the fields of mathematics applicable to machine learning. “Pure” mathematical domains such as topology, algebra, and geometry, are now joining the more traditionally applied fields of probability theory, analysis, and linear algebra. These pure fields have grown and developed over the last century to handle high levels of abstraction and complexity, helping mathematicians make discoveries about spaces, algebraic objects, and combinatorial processes that at first glance seem beyond human intuition. These capabilities promise to address many of the biggest challenges in modern deep learning. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;In this article we will explore several areas of current research that demonstrate the enduring ability of mathematics to guide the process of discovery and understanding in machine learning.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="372" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcr6ZUCNH3oKGK9XQzvDZOs1kJhvjym4RrMAlENx0OHrK-IBsQcBQQW2wDKu2_g2tNJxXVd32BI5llBopCmD-IAFV9zfhjvQ2ek5rIOgUMqwhK-qFhri2iaU718yl4BzISTanzZt9a2Rix04c6GUpdFR4Co?key=h8RUuDFEFKnzGnrKx9gMkg" width="529" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 1: Mathematics can illuminate the ways that ReLU-based neural networks shatter input space into countless polygonal regions, in each of which the model behaves like a linear map [2, 3, 4]. These decompositions create beautiful patterns. (Figure made with SplineCam [5]).&lt;/em&gt;&lt;/p&gt;&lt;h3 id="describing-an-elephant-from-a-pin-prick"&gt;Describing an Elephant from a Pin Prick&lt;/h3&gt;&lt;p&gt;Suppose you are given a 7 billion parameter neural network with 50 layers and are asked to analyze it; how would you begin? The standard procedure would be to calculate relevant performance statistics. For instance, the accuracy on a suite of evaluation benchmarks. In certain situations, this may be sufficient. However, deep learning models are complex and multifaceted. Two computer vision models with the same accuracy may have very different generalization properties to out-of-distribution data, calibration, adversarial robustness, and other “secondary statistics” that are critical in many real-world applications. Beyond this, all evidence suggests that to build a complete scientific understanding of deep learning, we will need to venture beyond evaluation scores. Indeed, just as it is impossible to capture all the dimensions of humanity with a single numerical quantity (e.g., IQ, height), trying to understand a model by one or even several statistics alone is fundamentally limiting.&lt;/p&gt;&lt;p&gt;One difference between understanding a human and understanding a model is that we have easy access to all model parameters and all the individual computations that occur in a model. Indeed, by extracting a model’s hidden activations we can directly trace the process by which a model converts raw input into a prediction. Unfortunately, the world of hidden activations is far less hospitable than that of simple model performance statistics. Like the initial input, hidden activations are usually high dimensional, but unlike input data they are not structured in a form that humans can understand. If we venture into even higher dimensions, we can try to understand a model through its weights directly. Here, in the space of model weights, we have the freedom to move in millions to billions of orthogonal directions from a single starting point. How do we even begin to make sense of these worlds?&lt;/p&gt;&lt;p&gt;There is a well-known fable in which three blind men each feel a different part of an elephant. The description that each gives of the animal is completely different, reflecting only the body part that that man felt. We argue that unlike the blind men who can at least use their hand to feel a substantial part of one of the elephant’s body parts, current methods of analyzing the hidden activations and weights of a model are akin to trying to describe the elephant from the touch of a single pin.&lt;/p&gt;&lt;h3 id="tools-to-characterize-what-we-cannot-visualize"&gt;Tools to Characterize What We Cannot Visualize&lt;/h3&gt;&lt;p&gt;Despite the popular perception that mathematicians exclusively focus on solving problems, much of research mathematics involves understanding the right questions to ask in the first place. This is natural since many of the objects that mathematicians study are so far removed from everyday experience that we start with very limited intuition for what we can hope to actually understand. Substantial effort is often required to build up tools that will enable us to leverage our existing intuition and achieve tractable results that increase our understanding. The concept of a &lt;em&gt;rotation &lt;/em&gt;provides a nice example of this situation since these are very familiar in 2- and 3-dimensions, but become less and less accessible to everyday intuition as their dimension grows larger. In this latter case, the differing perspectives provided by pure mathematics become more and more important to gaining a more holistic perspective on what these actually are. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;Those who know a little linear algebra will remember that rotations generalize to higher dimensions and that in $n$-dimensions they can be realized by $n \times n$ orthogonal matrices with determinant $1$. The set of these are commonly written as $SO(n)$ and called the &lt;em&gt;special orthogonal group&lt;/em&gt;. Suppose we want to understand the set of all $n$-dimensional rotations. There are many complementary approaches to doing this. We can explore the linear algebraic structure of all matrices in $SO(n)$ or study $SO(n)$ based on how each element behaves as an operator acting on $\mathbb{R}^n$.&lt;/p&gt;&lt;p&gt;Alternatively, we can also try to use our innate spatial intuition to understand $SO(n)$. This turns out to be a powerful perspective in math. In any dimension $n$, $SO(n)$ is a geometric object called a &lt;em&gt;manifold&lt;/em&gt;. Very roughly, a space that locally looks like Euclidean space, but which may have twists, holes, and other non-Euclidean features when we zoom out. Indeed, whether we make it precise or not, we all have a sense of whether two rotations are “close” to each other. For example, the reader would probably agree that $2$-dimensional rotations of $90^\circ$ and $91^\circ$ “feel” closer than rotations of $90^\circ$ and $180^\circ$. When $n=2$, one can show that the set of all rotations is geometrically “equivalent” to a $1$-dimensional circle. So, much of what we know about the circle can be translated to $SO(2)$.&lt;/p&gt;&lt;p&gt;What happens when we want to study the geometry of rotations in $n$-dimensions for $n &amp;gt; 3$? If $n = 512$ (a latent space for instance), this amounts to studying a manifold in $512^2$-dimensional space. Our visual intuition is seemingly useless here since it is not clear how concepts that are familiar in 2- and 3-dimensions can be utilized in $512^2$-dimensions. Mathematicians have been confronting the problem of understanding the un-visualizable for hundreds of years. One strategy is to find generalizations of familiar spatial concepts from $2$ and $3$-dimensions to $n$-dimensions that connect with our intuition.&lt;/p&gt;&lt;p&gt;This approach is already being used to better understand and characterize experimental observations about the space of model weights, hidden activations, and input data of deep learning models. We provide a taste of such tools and applications here:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;Intrinsic Dimension: &lt;/em&gt;Dimension is a concept that is familiar not only from our experience in the spatial dimensions that we can readily access, 1-, 2-, and 3-dimensions, but also from more informal notions of “degrees of freedom” in everyday systems such as driving a car (forward/back, turning the steering wheel either left or right). The notion of dimension arises naturally in the context of machine learning where we may want to capture the number of independent ways in which a dataset, learned representation, or collection of weight matrices actually vary.&lt;p&gt;In formal mathematics, the definitions of dimension depend on the kind of space one is studying but they all capture some aspect of this everyday intuition. As a simple example, if I walk along the perimeter of a circle, I am only able to move forward and backward, and thus the dimension of this space is $1$. For spaces like the circle which are manifolds, dimension can be formally defined by the fact that a sufficiently small neighborhood around each point looks like a subset of some Euclidean space $\mathbb{R}^k$. We then say that the manifold is $k$-dimensional. If we zoom in on a small segment of the circle, it almost looks like a segment of $\mathbb{R} = \mathbb{R}^1$, and hence the circle is $1$-dimensional.&lt;/p&gt;&lt;p&gt;The manifold hypothesis posits that many types of data (at least approximately) live on a low-dimensional manifold even though they are embedded in a high-dimensional space. If we assume that this is true, it makes sense that the dimension of this underlying manifold, called the intrinsic dimension of the data, is one way to describe the complexity of the dataset. Researchers have estimated intrinsic dimension for common benchmark datasets, showing that intrinsic dimension appears to be correlated to the ease with which models generalize from training to test sets [6], and can explain differences in model performance and robustness in different domains such as medical images [7]. Intrinsic dimension is also a fundamental ingredient in some proposed explanations of data scaling laws [8, 9], which underlie the race to build ever bigger generative models.&lt;/p&gt;&lt;p&gt;Researchers have also noted that the intrinsic dimension of hidden activations tend to change in a characteristic way as information passes through the model [10, 11] or over the course of the diffusion process [12]. These and other insights have led to the use of intrinsic dimension in detection of adversarial examples [13], AI-generated content [14], layers where hidden activations contain the richest semantic content [11], and hallucinations in generative models [15].&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;Curvature&lt;/em&gt;: While segments of the circle may look “straight” when we zoom up close enough, their curvature means that they will never be exactly linear as a straight line is. The notion of curvature is a familiar one and once formalized, it offers a way of rigorously measuring the extent to which the area around a point deviates from being linear. Care must be taken, however. Much of our everyday intuition about curvature assumes a single dimension. On manifolds with dimension $2$ or greater, there are multiple, linearly independent directions that we can travel away from a point and each of these may have a different curvature (in the $1$-dimensional sense). As a result, there are a range of different generalizations of curvature for higher-dimensional spaces, each with slightly different properties.&lt;p&gt;The notion of curvature has played a central role in deep learning, especially with respect to the loss landscape where changes in curvature have been used to analyze training trajectories [16]. Curvature is also central to an intriguing phenomenon known as the ‘edge of stability’, wherein the curvature of the loss landscape over the course of training increases as a function of learning rate until it hovers around the point where the training run is close to becoming unstable [17]. In another direction, curvature has been used to calculate the extent that model predictions change as input changes. For instance, [18] provided evidence that higher curvature in decision boundaries correlates with higher vulnerability to adversarial examples and suggested a new regularization term to reduce this. Finally, motivated by work in neuroscience, [19] presented a method that uses curvature to highlight interesting differences in representation between the raw training data and a neural network’s internal representation. A network may stretch and expand parts of the input space, generating regions of high curvature as it magnifies the representation of training examples that have a higher impact on the loss function.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;Topology&lt;/em&gt;: Both dimension and curvature capture local properties of a space that can be measured by looking at the neighborhood around a single point. On the other hand, the most notable feature of our running example, the circle, is neither its dimension nor its curvature, but rather the fact that it is circular. We can only see this aspect by analyzing the whole space at once. Topology is the field of mathematics that focuses on such “global” properties.&lt;p&gt;Topological tools such as homology, which counts the number of holes in a space, has been used to illuminate the way that neural networks process data, with [20] showing that deep learning models “untangle” data distributions, reducing their complexity layer by layer. Versions of homology have also been applied to the weights of networks to better understand their structural features, with [21] showing that such topological statistics can reliably predict optimal early-stopping times. Finally, since topology provides frameworks that capture the global aspects of a space, it has proved a rich source of ideas for how to design networks that capture higher order relationships within data, leading to a range of generalizations of graph neural networks built on top of topological constructions [22, 23, 24, 25].&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;While the examples above have each been useful for gaining insight into phenomena related to deep learning, they were all developed to address challenges in other fields. We believe that a bigger payoff will come when the community uses the geometric paradigm described here to build new tools specifically designed to address the challenges that deep learning poses. Progress in this direction has already begun. Think for instance of linear mode connectivity which has helped us to better understand the loss landscape of neural networks [26] or work around the linear representation hypothesis which has helped to illuminate the way that concepts are encoded in the latent space of large language models [27]. One of the most exciting occurrences in mathematics is when the tools from one domain provide unexpected insight in another. Think of the discovery that Riemannian geometry provides some of the mathematical language needed for general relativity. We hope that a similar story will eventually be told for geometry and topology’s role in deep learning.&lt;/p&gt;&lt;h3 id="symmetries-in-data-symmetries-in-models"&gt;Symmetries in data, symmetries in models&lt;br /&gt;&lt;/h3&gt;&lt;p&gt;Symmetry is a central theme in mathematics, allowing us to break a problem into simpler components that are easier to solve. Symmetry has long played an important role in machine learning, particularly computer vision. In the classic dog vs. cat classification task for instance, an image that contains a dog continues to contain a dog regardless of whether we move the dog from one part of the image to another, whether we rotate the dog, or whether we reflect it. We say that the task is &lt;em&gt;invariant&lt;/em&gt; to image translation, rotation, and reflection.&lt;/p&gt;&lt;p&gt;The notion of symmetry is mathematically encoded in the concept of a &lt;em&gt;group&lt;/em&gt;, which is a set $G$ equipped with a binary operation $\star$ that takes two elements of $G$, $g_1$, $g_2$ as input and produces a third $g_1\star g_2$ as output. You can think of the integers $\mathbb{Z}$ with the binary operation of addition ($\star = +$) or the non-zero real numbers with the binary operation of multiplication ($\star = \times$). The set of $n$-dimensional rotations, $SO(n)$, also forms a group. The binary operation takes two rotations and returns a third rotation that is defined by simply applying the first rotation and then applying the second.&lt;/p&gt;&lt;p&gt;Groups satisfy axioms that ensure that they capture familiar properties of symmetries. For example, for any symmetry transformation, there should be an inverse operation that undoes the symmetry. If I rotate a circle by $90^{\circ}$, then I can rotate it back by $-90^{\circ}$ and return to where I started. Notice that not all transformations satisfy this property. For instance, there isn’t a well-defined inverse for downsampling an image. Many different images downsample to the same (smaller) image.&lt;/p&gt;&lt;p&gt;In the previous section we gave two definitions of $SO(n)$: the first was the geometric definition, as rotations of $\mathbb{R}^n$, and the second was as a specific subset of $n \times n$ matrices. While the former definition may be convenient for our intuition, the latter has the benefit that linear algebra is something that we understand quite well at a computational level. The realization of an abstract group as a set of matrices is called a &lt;em&gt;linear representation&lt;/em&gt; and it has proven to be one of the most fruitful methods of studying symmetry. It is also the way that symmetries are usually leveraged when performing computations (for example, in machine learning).&lt;/p&gt;&lt;p&gt;We saw a few examples of symmetries that can be found in the data of a machine learning task, such as the translation, rotation, and reflection symmetries in computer vision problems. Consider the case of a segmentation model. If one rotates an input image by $45^{\circ}$ and then puts it through the model, we will hope that we get a $45^{\circ}$ rotation of the segmentation prediction for the un-rotated image (this is illustrated in 1). After all, we haven’t changed the content of the image.&lt;br /&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="323" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXd1UiuD880gmdVrtjEKvGPBHIr0dvdBsrLXAnxUFz6_KQNLyMekhxrSR2ROn-H8O3780yoKbJvF0tUEVZSEdsuDfbB7kSGw_CFCqsKzjC6-wpxN5dxLQd-e4g7qMsKnc8BCX1pw7Qh0-I9hsgY9EInhpROs?key=h8RUuDFEFKnzGnrKx9gMkg" width="534" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 2: The concept of rotation equivariance illustrated for a segmentation model. One gets the same output regardless of whether one rotates first and then applies the network or applies the network and then rotates.&lt;/em&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="273" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXe_QPixL6292p6Bz5yj_Hep_ypc6qrj-3q3Y7uIte0R5Nsc2ZPxNmSJOheOHohJY_0VbDi3LlyNSR61t94bHDfgTnJx0ssvyzU9KMtGLUKoqoiviKKTxpZR77Bb8VIzhkzd0Vxhspif10w8DnS3eWbjqwhW?key=h8RUuDFEFKnzGnrKx9gMkg" width="522" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 3: Equivariance holds when taking the top path (applying the network first and then the symmetry action) gives the same result as taking the bottom path (applying the symmetry transformation and then the network).&lt;/em&gt;&lt;/p&gt;&lt;p&gt;This property of a function (including neural networks), that applying a symmetry transformation before the function yields the same result as applying the symmetry transformation after the function is called &lt;em&gt;equivariance&lt;/em&gt; and can be captured by the diagram in Figure 3. The key point is that we get the same result whether we follow the upper path (applying the network first and then applying the group action) as when we follow the lower path (applying the group first and then applying the network). Conveniently, the concept of invariance, where applying a symmetry operation to input has no effect on the output of the function is a special case of equivariance where the action on the output space is defined to be trivial (applying symmetry actions does nothing).&lt;/p&gt;&lt;p&gt;Invariance and equivariance in deep learning models can be beneficial for a few reasons. Firstly, such a model will yield more predictable and consistent results across symmetry transformations. Secondly, through equivariance we can sometimes simplify the learning process with fewer parameters (compare the number of parameters in a convolutional neural network and an MLP of similar performance) and fewer modes of variation to learn in the data (a rotation invariant image classifier only needs to learn one orientation of each object rather than all possible orientations).&lt;/p&gt;&lt;p&gt;But how do we ensure that our model is equivariant? One way is to build our network with layers that are equivariant by design. By far the most well-known example of this is the convolutional neural network, whose layers are (approximately) equivariant to image translation. This is one reason why using a convolutional neural network for dog vs cat classification doesn’t require learning to recognize a dog at every location in an image as it might with an MLP. With a little thought, one can often come up with layers which are equivariant to a specific group. Unfortunately, being constrained to equivariant layers that we find in an ad-hoc manner often leaves us with a network with built-in equivariance but limited expressivity.&lt;/p&gt;&lt;p&gt;Fortunately, for most symmetry groups arising in machine learning, representation theory offers a comprehensive description of all possible linear equivariant maps. Indeed, it is a beautiful mathematical fact that all such maps are built from atomic building blocks called &lt;em&gt;irreducible representations&lt;/em&gt;. Happily, in many cases, the number of these irreducible representations is finite. Understanding the irreducible representations of a group can be quite powerful. Those familiar with the ubiquitous discrete Fourier transform (DFT) of a sequence of length $n$ are already familiar with the irreducible representations of one group, the cyclic group generated by a rotation by $360 ^{\circ}/n$ (though we note that moving between the description we give here and the description of the DFT found in the signal processing literature takes a little thought).&lt;/p&gt;&lt;p&gt;There is now a rich field of research in deep learning that uses group representations to systematically build expressive equivariant architectures. Some examples of symmetries that have been particularly well-studied include: rotation and reflection of images [28, 29, 30, 31], 3-dimensional rotation and translation of molecular structures [32] or point clouds [33], and permutations for learning on sets [34] or nodes of a graph [35]. Encoding equivariance to more exotic symmetries has also proven useful for areas such as theoretical physics [36] and data-driven optimization [37].&lt;/p&gt;&lt;p&gt;Equivariant layers and other architectural approaches to symmetry awareness are a prime example of using mathematics to inject high-level priors into a model. Do these approaches represent the future of learning in the face of data symmetries? Anecdotally, the most common approach to learning on data with symmetries continues to be using enough training data and enough data augmentation for the model to learn to handle the symmetries on its own. Two years ago, the author would have speculated that these latter approaches only work for simple cases, such as symmetries in 2-dimensions, and will be outperformed by models which are equivariant by design when symmetries become more complex. Yet, we continue to be surprised by the power of scale. After all, AlphaFold3 [38] uses a non-equivariant architecture despite learning on data with several basic symmetries. We speculate that there may be a threshold on the ratio of symmetry complexity on the one hand and the amount of training data on the other, that determines whether built-in equivariance will outperform learned equivariance [39, 40].&lt;br /&gt;&lt;/p&gt;&lt;p&gt;If this is true, we can expect to see models move away from bespoke equivariant architectures as larger datasets become available for a specific application. At the same time, since compute will always be finite, we predict that there will be some applications with exceptionally complex symmetries that will always require some built-in priors (for example, AI for math or algorithmic problems). Regardless of where we land on this spectrum, mathematicians can look forward to an interesting comparison of the ways humans inject symmetry into models vs the way that models learn symmetries on their own [41, 42].&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="129" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcJa_1Ow2zrMVuk1hJiOJtTJBq5bH7zyogibZ5fqQu85ERGFEjcX4jRn7r_rnZvTrCdpN5OzeVQUBLu60DJP_aIR4uoHq33tRMcoPAUf7qumOeJLreCkttvqtQEssCh90UwlbWkzBoK79FV54R6ncO2c_Ij?key=h8RUuDFEFKnzGnrKx9gMkg" width="572" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 4: A cartoon illustrating why adding a permutation and its inverse before and after a pointwise nonlinearity produces an equivalent model (even though the weights will be different). Since permutations can be realized by permutation matrices, the crossed arrows on the right can be merged into the fully-connected layer.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Of course, symmetry is not only present in data but also in the models themselves. For instance, the activations of hidden layers of a network are invariant to permutation. We can permute activations before entering the non-linearity and if we un-permute them afterward, the model (as a function) does not change (Figure 4). This means that we have an easy recipe for generating an exponentially large number of networks that have different weights but behave identically on data. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;While simple, this observation produces some unexpected results. There is evidence, for instance, that while the loss landscape of neural networks is highly non-convex, it may be much less non-convex when we consider all networks that can be produced through this permutation operation as equivalent [43, 44]. This means that your network and my network may not be connected by a linear path of low loss, but such a path may exist between your network and a permutation of my network. Other research has looked at whether it may be possible to use symmetries to accelerate optimization by ‘teleporting’ a model to a more favorable location in the loss landscape [45, 46]. Finally, permutation symmetries also provide one type of justification for an empirical phenomenon where individual neurons in a network tend to encode more semantically meaningful information than arbitrary linear combinations of such neurons [47].&lt;/p&gt;&lt;h3 id="taming-complexity-with-abstraction"&gt;Taming Complexity with Abstraction&lt;/h3&gt;&lt;p&gt;When discussing symmetry, we used the diagram in Figure 3 to define equivariance. One of the virtues of this approach is that we never had to specify details about the input data or architecture that we used. The spaces could be vector spaces and the maps linear transformations, they could be neural networks of a specific architecture, or they could just be sets and arbitrary functions between them–the definition is valid for each. This &lt;em&gt;diagrammatic&lt;/em&gt; point of view, which looks at mathematical constructions in terms of the composition of maps between objects rather than the objects themselves, has been very fruitful in mathematics and is one gateway to the subject known as &lt;em&gt;category theory&lt;/em&gt;. Category theory is now the lingua franca in many areas of mathematics since it allows mathematicians to translate definitions and results across a wide range of contexts.&lt;/p&gt;&lt;p&gt;Of course, deep learning is at its core all about function composition, so it is no great leap to try and connect it to the diagrammatic tradition in mathematics. The focus of function composition in the two disciplines is different, however. In deep learning we take simple layers that alone lack expressivity and compose them together to build a model capable of capturing the complexity of real-world data. With this comes the tongue-in-cheek demand to “stack more layers!”. Category theory instead tries to find a universal framework that captures the essence of structures appearing throughout mathematics. This allows mathematicians to uncover connections between things that look very different at first glance. For instance, category theory gives us the language to describe how the topological structure of a manifold can be encoded in groups via homology or homotopy theory.&lt;/p&gt;&lt;p&gt;It can be an interesting exercise to try to find a diagrammatic description of familiar constructions like the product of two sets $X$ and $Y$. Focusing our attention on maps rather than objects we find that what characterizes $X \times Y$ is the existence of the two canonical projections $\pi_1$ and $\pi_2$, the former sending $(x,y) \mapsto x$ and $(x,y) \mapsto y$ (at least in more familiar settings where $X$ and $Y$ are, for example, sets). Indeed, the &lt;em&gt;product &lt;/em&gt;$X \times Y$ (regardless of whether $X$ and $Y$ are sets, vectors spaces, etc.) is the unique object such that for any $Z$ with maps $f_1: Z \rightarrow X$ and $f_2: Z \rightarrow Y$, there is a map $h: Z \rightarrow X \times Y$ that satisfies the commutative diagram in Figure 5.&lt;/p&gt;&lt;p&gt;While this construction is a little involved for something as familiar as a product it has the remarkable property that it allows us to define a “product” even when there is no &amp;nbsp;underlying set structure (that is, those settings where we cannot resort to defining $X \times Y$ as the set of pairs of $(x,y)$ for $x \in X$ and $y \in Y$).&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="277" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXd0afUER-JEqT9BBW0z5ip7HSPD_ORKlpxTaQQFpep7MZF3DKfhgca3XbrZ2aGGTTnxcyOD3csHF1hdODeSXFx-nC63Mlw2etuY9xtM-AUvec4aIZKJK0hl2QiuxyJPzmlr18GbJA?key=h8RUuDFEFKnzGnrKx9gMkg" width="624" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 5: The commutative diagram that describes a product $X \times Y$. For any $Z$ with maps $f_1: Z \rightarrow X$ and $f_2: Z \rightarrow Y$, there exists a unique map $h: Z \rightarrow X \times Y$ such that $f_1 = \pi_1 \circ h$ and $f_2 = \pi_2 \circ h$ where $\pi_1$ and $\pi_2$ are the usual projection maps from $X \times Y$ to $X$ and $X \times Y$ to $Y$ respectively.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;One can reasonably argue that diagrammatic descriptions of well-known constructions, like products, are not useful for the machine learning researcher. After all, we already know how to form products in all of the spaces that come up in machine learning. On the other hand, there are more complicated examples where diagrammatics mesh well with the way we build neural network architectures in practice.&lt;br /&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="352" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXf30eZdcoTcrjBuYEo6BUjm4gmw8fvcY9kLpDsspW0qPoIVu6LN5mfd1ks5qiMtf9J1DyPNDtzDDLDpxVi7n5j62DxlIfkwyo5V4gAC7MeeMpUaDzOMgsU4Mqjrs7fUXL-hc_BeqPd9Upu6L0wmKnDzkop4?key=h8RUuDFEFKnzGnrKx9gMkg" width="624" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 6: Fiber bundles capture the notion that a space might locally look like a product but globally have twists in it.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Fiber bundles are a central construction in geometry and topology that capture the notion that a space may locally look like a product but may have twists that break this product structure globally. Compare the cylinder with the Möbius band. We can build both of these by starting with a circle and taking a product with the line segment $(0,1)$. In the case of the cylinder, this really is just (topologically) the product of the circle and the segment $(0,1)$, but to form the Möbius band we must add an additional twist that breaks the product structure. In these examples, the circle is called the &lt;em&gt;base &lt;/em&gt;space and $(0,1)$ is called the &lt;em&gt;fiber&lt;/em&gt;. While only the cylinder is a true product, both the cylinder and the Möbius band are fiber bundles. Here is another way of thinking about a fiber bundle. A fiber bundle is a union of many copies of the fiber parametrized by the base space. In the Möbius band/cylinder example, each point on the circle carries its own copy of $(0,1)$.&lt;/p&gt;&lt;p&gt;We drew inspiration from this latter description of fiber bundles when we were considering a conditional generation task in the context of a problem in materials science. Since the materials background is somewhat involved, we’ll illustrate the construction via a more pedestrian, animal-classification analogue. Let $M$ be the manifold of all possible images containing a single animal. We can propose to decompose the variation in elements of $M$ into two parts, the species of animal in the image and everything else, where the latter could mean differences in background, lighting, pose, image quality, etc. One might want to explore the distribution of one of these factors of variation while fixing the other. For instance, we might want to fix the animal species and explore the variation we get in background, pose, etc. For example, comparing the variation in background for two different species of insect may tell the entomologist about the preferred habitat for different types of beetles.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="421" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfLfnOQw_uLzm58bcucM5zOzGLHKzbX8hyQU2muIPl994v1GQN0sfMwQgSjFwsaCDetRHW8WR_T71pjNX7waqch44PwUY6Dv8egfzRlOmo6e0BbDagYv99K6tMnvVeTAIb9ww9bT_3Ukcs4k7xHx-BH7cxR?key=h8RUuDFEFKnzGnrKx9gMkg" width="624" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 7: A cartoon visualizing how the set of all animal images could be decomposed into a local product of animal species and other types of variation.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;One might hope to solve this problem by learning an encoding of $M$ into a product space $X_1 \times X_2$ where $X_1$ is a discrete set of points corresponding to animal species and $X_2$ is a space underlying the distribution of all other possible types of variation for a fixed species of animal. Fixing the species would then amount to choosing a specific element $x_1$ from $X_1$ and sampling from the distribution on $X_2$. The product structure of $X_1 \times X_2$ allows us to perform such independent manipulations of $X_1$ and $X_2$. On the other hand, products are rigid structures that impose strong, global topological assumptions on the real data distribution. We found that even on toy problems, it was hard to learn a good map from the raw data distribution to the product-structured latent space defined above. Given that fiber bundles are more flexible and still give us the properties we wanted from our latent space, we designed a neural network architecture to learn a fiber bundle structure on a data distribution [48].&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="240" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdrgVDyhD0JXBm12Gh1NstAjVx3fSk8vM3Mg_3JGi6JpK3PYTWUpmgzW_BgmEOMeZahkdrzWEw2ThViUKXnEGFobRORcOMgifUin2kJY3-zFIq4fbj-4QO6x7ALnwn5qLU880r1raMaFC2yqn6RVyDPGEk?key=h8RUuDFEFKnzGnrKx9gMkg" width="415" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 8: The commutative diagram describing a fiber bundle. The map $\pi$ projects from neighborhoods of the total space to the base space, $U$ is a local neighborhood of the base space, and $F$ &amp;nbsp;is the fiber. The diagram says that each point in the base space has a neighborhood $U$ &amp;nbsp;such that when we lift this to the bundle, we get something that is homeomorphic (informally, equivalent) to the product of the neighborhood &amp;nbsp;and the fiber. But this product structure may not hold globally over the whole space.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;But how do we go from the abstract definition of a fiber bundle above to a neural network architecture that we can code up on a computer. It turns out there is a succinct diagrammatic definition of a fiber bundle (Figure 8) that can serve as a convenient template to build up an architecture from. We were able to proceed in a relatively naïve fashion, taking each of the maps in the diagram and building a corresponding stack of layers. The diagram itself then told us how to compose each of these components together. The commutativity of the diagram was engineered through a term in the loss function that ensures that $\pi = \text{proj}_1 \circ \varphi$. There were also some conditions on $\varphi$ and $\pi$ (such as the bijectivity of $\phi$) that needed to be engineered. Beyond this, we were surprised at the amount of flexibility we had. This is useful since it means this process is largely agnostic to data modality.&lt;/p&gt;&lt;p&gt;This is an elementary example of how the diagrammatic tradition in mathematics can provide us with a broader perspective on the design of neural networks, allowing us to connect deep structural principles with large-scale network design without having to specify small-scale details that might be problem dependent. Of course, all this fails to draw from anything beyond the surface of what the categorical perspective has to offer. Indeed, category theory holds promise as a unified framework to connect much of what appears and is done in machine learning [49].&lt;/p&gt;&lt;h3 id="conclusion"&gt;Conclusion&lt;br /&gt;&lt;/h3&gt;&lt;p&gt;In the mid-twentieth century, Eugene Wigner marveled at the “the unreasonable effectiveness of mathematics” as a framework for not only describing existing physics but also anticipating new results in the field [50]. A mantra more applicable to recent progress in machine learning is “the unreasonable effectiveness of data” [51] and compute. This could appear to be a disappointing situation for mathematicians who might have hoped that machine learning would be as closely intertwined to advanced mathematics as physics is. However, as we’ve demonstrated, while mathematics may not maintain the same role in machine learning research that it has held in the past, the success of scale actually opens new paths for mathematics to support progress in machine learning research. These include:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Providing powerful tools for deciphering the inner workings of complex models&lt;/li&gt;&lt;li&gt;Offering a framework for high-level architectural decisions that leave the details to the learning algorithm&lt;/li&gt;&lt;li&gt;Bridging traditionally isolated domains of mathematics like topology, abstract algebra, and geometry with ML and data science applications.&lt;br /&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Should the way things have turned out surprise us? Perhaps not, given that machine learning models ultimately reflect the data they are trained on and in most cases this data comes from fields (such as natural language or imagery) which have long resisted parsimonious mathematical models. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;Yet, this situation is also an opportunity for mathematics. Performant machine learning models may provide a gateway for mathematical analysis of a range of fields that were previously inaccessible. It’s remarkable for instance that trained word embeddings transform semantic relationships into algebraic operations on vectors in Euclidean space (for instance, ‘Italian’ - ‘Italy’ + ‘France’ = ‘French’). Examples like this hint at the potential for mathematics to gain a foothold in complex, real-world settings by studying the machine learning models that have trained on data from these settings.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;As more and more of the data in the world is consumed and mathematicised by machine learning models, it will be an increasingly interesting time to be a mathematician. The challenge now lies in adapting our mathematical toolkit to this new landscape, where empirical breakthroughs often precede theoretical understanding. By embracing this shift, mathematics can continue to play a crucial, albeit evolving, role in shaping the future of machine learning.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;The author would like to thank Darryl Hannan for help with figures, Davis Brown, Charles Godfrey, and Scott Mahan for useful feedback on drafts, as well as the staff of the Gradient for useful conversations and help editing this article. For resources and events around the growing community of mathematicians and computer scientists using topology, algebra, and geometry (TAG) to better understand and build more robust machine learning systems, please visit us at &lt;/em&gt;&lt;em&gt;https://www.tagds.com&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;&lt;h2 id="references"&gt;References&lt;/h2&gt;&lt;p&gt;[1] Richard Sutton. "The bitter lesson". In: &lt;em&gt;Incomplete Ideas (blog)&lt;/em&gt; 13.1 (2019), p. 38.&lt;/p&gt;&lt;p&gt;[2] Guido F Montufar et al. "On the number of linear regions of deep neural networks". In: &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 27 (2014).&lt;/p&gt;&lt;p&gt;[3] Boris Hanin and David Rolnick. "Complexity of linear regions in deep networks". In: &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;. PMLR. 2019, pp. 2596–2604.&lt;/p&gt;&lt;p&gt;[4] J Elisenda Grigsby and Kathryn Lindsey. "On transversality of bent hyperplane arrangements and the topological expressiveness of ReLU neural networks". In: &lt;em&gt;SIAM Journal on Applied Algebra and Geometry&lt;/em&gt; 6.2 (2022), pp. 216–242.&lt;/p&gt;&lt;p&gt;[5] Ahmed Imtiaz Humayun et al. "Splinecam: Exact visualization and characterization of deep network geometry and decision boundaries". In: &lt;em&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition&lt;/em&gt;. 2023, pp. 3789–3798.&lt;/p&gt;&lt;p&gt;[6] Phillip Pope et al. "The intrinsic dimension of images and its impact on learning". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2104.08894 (2021).&lt;/p&gt;&lt;p&gt;[7] Nicholas Konz and Maciej A Mazurowski. "The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2401.08865 (2024).&lt;/p&gt;&lt;p&gt;[8] Yasaman Bahri et al. "Explaining neural scaling laws". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2102.06701 (2021).&lt;/p&gt;&lt;p&gt;[9] Utkarsh Sharma and Jared Kaplan. "A neural scaling law from the dimension of the data manifold". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2004.10802 (2020).&lt;/p&gt;&lt;p&gt;[10] Alessio Ansuini et al. "Intrinsic dimension of data representations in deep neural networks". In: &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 32 (2019).&lt;/p&gt;&lt;p&gt;[11] Lucrezia Valeriani et al. "The geometry of hidden representations of large transformer models". In: &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 36 (2024).&lt;/p&gt;&lt;p&gt;[12] Henry Kvinge, Davis Brown, and Charles Godfrey. "Exploring the Representation Manifolds of Stable Diffusion Through the Lens of Intrinsic Dimension". In: &lt;em&gt;ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;[13] Xingjun Ma et al. "Characterizing adversarial subspaces using local intrinsic dimensionality". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:1801.02613 (2018).&lt;/p&gt;&lt;p&gt;[14] Peter Lorenz, Ricard L Durall, and Janis Keuper. "Detecting images generated by deep diffusion models using their local intrinsic dimensionality". In: &lt;em&gt;Proceedings of the IEEE/CVF International Conference on Computer Vision&lt;/em&gt;. 2023, pp. 448–459.&lt;/p&gt;&lt;p&gt;[15] Fan Yin, Jayanth Srinivasa, and Kai-Wei Chang. "Characterizing truthfulness in large language model generations with local intrinsic dimension". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2402.18048 (2024).&lt;/p&gt;&lt;p&gt;[16] Justin Gilmer et al. "A loss curvature perspective on training instabilities of deep learning models". In: &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. 2021.&lt;/p&gt;&lt;p&gt;[17] Jeremy Cohen et al. "Gradient descent on neural networks typically occurs at the edge of stability". In: &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. 2020.&lt;/p&gt;&lt;p&gt;[18] Seyed-Mohsen Moosavi-Dezfooli et al. "Robustness via curvature regularization, and vice versa". In: &lt;em&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition&lt;/em&gt;. 2019, pp. 9078–9086.&lt;/p&gt;&lt;p&gt;[19] Francisco Acosta et al. "Quantifying extrinsic curvature in neural manifolds". In: &lt;em&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition&lt;/em&gt;. 2023, pp. 610–619.&lt;/p&gt;&lt;p&gt;[20] Gregory Naitzat, Andrey Zhitnikov, and Lek-Heng Lim. "Topology of deep neural networks". In: &lt;em&gt;Journal of Machine Learning Research&lt;/em&gt; 21.184 (2020), pp. 1–40.&lt;/p&gt;&lt;p&gt;[21] Bastian Rieck et al. "Neural persistence: A complexity measure for deep neural networks using algebraic topology". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:1812.09764 (2018).&lt;/p&gt;&lt;p&gt;[22] Mustafa Hajij, Kyle Istvan, and Ghada Zamzmi. "Cell complex neural networks". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2010.00743 (2020).&lt;/p&gt;&lt;p&gt;[23] Cristian Bodnar. "Topological deep learning: graphs, complexes, sheaves". PhD thesis. 2023.&lt;/p&gt;&lt;p&gt;[24] Jakob Hansen and Robert Ghrist. "Toward a spectral theory of cellular sheaves". In: &lt;em&gt;Journal of Applied and Computational Topology&lt;/em&gt; 3.4 (2019), pp. 315–358.&lt;/p&gt;&lt;p&gt;[25] Yifan Feng et al. "Hypergraph neural networks". In: &lt;em&gt;Proceedings of the AAAI Conference on Artificial Intelligence&lt;/em&gt;. Vol. 33. 01. 2019, pp. 3558–3565.&lt;/p&gt;&lt;p&gt;[26] Felix Draxler et al. "Essentially no barriers in neural network energy landscape". In: &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;. PMLR. 2018, pp. 1309–1318.&lt;/p&gt;&lt;p&gt;[27] Kiho Park, Yo Joong Choe, and Victor Veitch. "The linear representation hypothesis and the geometry of large language models". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2311.03658 (2023).&lt;/p&gt;&lt;p&gt;[28] Taco Cohen and Max Welling. "Group equivariant convolutional networks". In: &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;. PMLR. 2016, pp. 2990–2999.&lt;/p&gt;&lt;p&gt;[29] Maurice Weiler, Fred A Hamprecht, and Martin Storath. "Learning steerable filters for rotation equivariant cnns". In: &lt;em&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/em&gt;. 2018, pp. 849–858.&lt;/p&gt;&lt;p&gt;[30] Daniel E Worrall et al. "Harmonic networks: Deep translation and rotation equivariance". In: &lt;em&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/em&gt;. 2017, pp. 5028–5037.&lt;/p&gt;&lt;p&gt;[31] Diego Marcos et al. "Rotation equivariant vector field networks". In: &lt;em&gt;Proceedings of the IEEE International Conference on Computer Vision&lt;/em&gt;. 2017, pp. 5048–5057.&lt;/p&gt;&lt;p&gt;[32] Alexandre Duval et al. "A Hitchhiker's Guide to Geometric GNNs for 3D Atomic Systems". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2312.07511 (2023).&lt;/p&gt;&lt;p&gt;[33] Nathaniel Thomas et al. "Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:1802.08219 (2018).&lt;/p&gt;&lt;p&gt;[34] Manzil Zaheer et al. "Deep sets". In: &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 30 (2017).&lt;/p&gt;&lt;p&gt;[35] Vıctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. "E (n) equivariant graph neural networks". In: &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;. PMLR. 2021, pp. 9323–9332.&lt;/p&gt;&lt;p&gt;[36] Denis Boyda et al. "Sampling using SU (N) gauge equivariant flows". In: &lt;em&gt;Physical Review D&lt;/em&gt; 103.7 (2021), p. 074504.&lt;/p&gt;&lt;p&gt;[37] Hannah Lawrence and Mitchell Tong Harris. "Learning Polynomial Problems with SL(2,\mathbb {R}) −Equivariance". In: &lt;em&gt;The Twelfth International Conference on Learning Representations&lt;/em&gt;. 2023.&lt;/p&gt;&lt;p&gt;[38] Josh Abramson et al. "Accurate structure prediction of biomolecular interactions with AlphaFold 3". In: &lt;em&gt;Nature&lt;/em&gt; (2024), pp. 1–3.&lt;/p&gt;&lt;p&gt;[39] Scott Mahan et al. "What Makes a Machine Learning Task a Good Candidate for an Equivariant Network?" In: &lt;em&gt;ICML 2024 Workshop on Geometry-grounded Representation Learning and Generative Modeling&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;[40] Johann Brehmer et al. "Does equivariance matter at scale?" In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2410.23179 (2024).&lt;/p&gt;&lt;p&gt;[41] Chris Olah et al. "Naturally Occurring Equivariance in Neural Networks". In: &lt;em&gt;Distill&lt;/em&gt; (2020). https://distill.pub/2020/circuits/equivariance. doi: 10.23915/distill.00024.004.&lt;/p&gt;&lt;p&gt;[42] Giovanni Luca Marchetti et al. "Harmonics of Learning: Universal Fourier Features Emerge in Invariant Networks". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2312.08550 (2023).&lt;/p&gt;&lt;p&gt;[43] Rahim Entezari et al. "The role of permutation invariance in linear mode connectivity of neural networks". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2110.06296 (2021).&lt;/p&gt;&lt;p&gt;[44] Samuel K Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. "Git re-basin: Merging models modulo permutation symmetries". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2209.04836 (2022).&lt;/p&gt;&lt;p&gt;[45] Bo Zhao et al. "Symmetry teleportation for accelerated optimization". In: &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 35 (2022), pp. 16679–16690.&lt;/p&gt;&lt;p&gt;[46] Bo Zhao et al. "Improving Convergence and Generalization Using Parameter Symmetries". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2305.13404 (2023).&lt;/p&gt;&lt;p&gt;[47] Charles Godfrey et al. "On the symmetries of deep learning models and their internal representations". In: &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 35 (2022), pp. 11893–11905.&lt;/p&gt;&lt;p&gt;[48] Nico Courts and Henry Kvinge. "Bundle Networks: Fiber Bundles, Local Trivializations, and a Generative Approach to Exploring Many-to-one Maps". In: &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. 2021.&lt;/p&gt;&lt;p&gt;[49] Bruno Gavranović et al. "Position: Categorical Deep Learning is an Algebraic Theory of All Architectures". In: &lt;em&gt;Forty-first International Conference on Machine Learning&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;[50] Eugene P Wigner. "The unreasonable effectiveness of mathematics in the natural sciences". In: &lt;em&gt;Mathematics and Science&lt;/em&gt;. World Scientific, 1990, pp. 291–306.&lt;/p&gt;&lt;p&gt;[51] Alon Halevy, Peter Norvig, and Fernando Pereira. "The unreasonable effectiveness of data". In: &lt;em&gt;IEEE Intelligent Systems&lt;/em&gt; 24.2 (2009), pp. 8–12.&lt;/p&gt;]]&amp;gt;&amp;lt;![CDATA[What's Missing From LLM Chatbots: A Sense of Purpose]]&amp;gt;LLM-based chatbots’ capabilities have been advancing every month. These improvements are mostly measured by benchmarks like MMLU, HumanEval, and MATH (e.g. sonnet 3.5, gpt-4o). However, as these measures get more and more saturated, is user experience increasing in proportion to these scores? If we envision a future]]&amp;gt;https://thegradient.pub/dialog/66c6733993571d5c8c154fb1Mon, 09 Sep 2024 17:28:48 GMT&lt;p&gt;LLM-based chatbots’ capabilities have been advancing every month. These improvements are mostly measured by benchmarks like MMLU, HumanEval, and MATH (e.g. sonnet 3.5, gpt-4o). However, as these measures get more and more saturated, is user experience increasing in proportion to these scores? If we envision a future of human-AI collaboration rather than AI replacing humans, the current ways of measuring dialogue systems may be insufficient because they measure in a non-interactive fashion.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Why does purposeful dialogue matter?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Purposeful dialogue refers to a multi-round user-chatbot conversation that centers around a goal or intention. The goal could range from a generic one like “harmless and helpful” to more specific roles like “travel planning agent”, “psycho-therapist” or “customer service bot.”&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Travel planning is a simple, illustrative example. Our preferences, fellow travelers’ preference, and all the complexities of real-world situations make transmitting all information in one pass way too costly. However, if multiple back-and-forth exchanges of information are allowed, only important information gets selectively exchanged. Negotiation theory offers an analogy of this—iterative bargaining yields better outcomes than a take-it-or-leave-it offer. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;In fact, sharing information is only one aspect of dialogue. In Terry Winograd’s words: “All language use can be thought of as a way of activating procedures within the hearer.” We can think of each utterance as a deliberate action that one party takes to alter the world model of the other. What if both parties have more complicated, even hidden goals? In this way, purposeful dialogue provides us with a way of formulating human-AI interactions as a collaborative game, where the goal of chatbot is to help humans achieve certain goals. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;This might seem like an unnecessary complexity that is only a concern for academics. However, purposeful dialogue could be beneficial even for the most hard-nosed, product-oriented research direction like code generation. Existing coding benchmarks mostly measure performances in a one-pass generation setting; however, for AI to automate solving ordinary Github issues (like in SWE-bench), it’s unlikely to be achieved by a single action—the AI needs to communicate back and forth with human software engineers to make sure it understands the correct requirements, ask for missing documentation and data, and even ask humans to give it a hand if needed. In a similar vein to pair programming, this could reduce the defects of code but without the burden of increasing man-hours. &lt;/p&gt;&lt;p&gt;&lt;br /&gt;Moreover, with the introduction of turn-taking, many new possibilities can be unlocked. As interactions become long-term and memory is built, the chatbot can gradually update user profiles. It can also adapt to their preferences. Imagine a personal assistant (e.g., IVA, Siri) that, through daily interaction, learns your preferences and intentions. It can read your resources of new information automatically (e.g., twitter, arxiv, Slack, NYT) and provide you with a morning news summary according to your preferences. It can draft emails for you and keep improving by learning from your edits.&lt;/p&gt;&lt;p&gt;In a nutshell, meaningful interactions between people rarely begin with complete strangers and conclude in just one exchange. Humans naturally interact with each other through multi-round dialogues and adapt accordingly throughout the conversation. However, doesn’t that seem exactly the opposite of predicting the next token, which is the cornerstone of modern LLMs? Below, let’s take a look at the makings of dialogue systems.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;How were/are dialogue systems made?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Let's jump back to the 1970s, when Roger Schank introduced his "restaurant script" as a kind of dialogue system [1]. This script breaks down the typical restaurant experience into steps like entering, ordering, eating, and paying, each with specific scripted utterances. Back then, every piece of dialogue in these scenarios was carefully planned out, enabling AI systems to mimic realistic conversations. ELIZA, a Rogerian psychotherapist simulator, and PARRY, a system mimicking a paranoid individual, were two other early dialogue systems until the dawn of machine learning.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Compare this approach to the LLM-based dialogue system today, it seems mysterious how models trained to predict the next token could do anything at all with engaging in dialogues. Therefore, let’s take a close examination of how dialogue systems are made, with an emphasis on how the dialogue format comes into play:&lt;/p&gt;&lt;p&gt;(1) Pretraining: a sequence model is trained to predict the next token on a gigantic corpus of mixed internet texts. The compositions may vary but they are predominantly news, books, Github code, with a small blend of forum-crawled data such as from Reddit, Stack Exchange, which may contain dialogue-like data.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="What's Missing From LLM Chatbots: A Sense of Purpose" class="kg-image" height="288" src="https://thegradient.pub/content/images/2024/08/unnamed.png" width="512" /&gt;&lt;figcaption&gt;Table of the pretraining data mixture from llama technical report&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;(2) Introduce dialogue formatting: because the sequence model only processes strings, while the most natural representation of dialogue history is a structured index of system prompts and past exchanges, a certain kind of formatting must be introduced for the purpose of conversion. Some Huggingface tokenizers provide this method called tokenizer.apply_chat_template for the convenience of users. The exact formatting differs from model to model, but it usually involves guarding the system prompts with &amp;lt;system&amp;gt; or &amp;lt;INST&amp;gt; in the hope that the pretrained model could allocate more attention weights to them. The system prompt plays a significant role in adapting language models to downstream applications and ensuring its safe behavior (we will talk more in the next section). Notably, the choice of the format is arbitrary at this step—pretraining corpus doesn’t follow this format.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="What's Missing From LLM Chatbots: A Sense of Purpose" class="kg-image" height="112" src="https://thegradient.pub/content/images/2024/08/image1.png" width="1398" /&gt;&lt;figcaption&gt;The context window of a chatbot&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;(3) RLHF: In this step, the chatbot is directly rewarded or penalized for generating desired or undesired answers. It’s worth noting that this is the first time the introduced dialogue formatting appears in the training data. RLHF is a &lt;em&gt;fine&lt;/em&gt;-tuning step not only because the data size is dwarfed in comparison to the pretraining corpus, but also due to the KL penalty and targeted weight tuning (e.g. Lora). Using Lecun’s analogy of cake baking, RLHF is only the small cherry on the top.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="What's Missing From LLM Chatbots: A Sense of Purpose" class="kg-image" height="580" src="https://thegradient.pub/content/images/2024/08/image5.png" width="1440" /&gt;&lt;figcaption&gt;Image from Yann Lecun’s slides&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2 id="how-consistent-are-existing-dialogue-systems-in-2024"&gt;How consistent are existing dialogue systems (in 2024)? &lt;br /&gt;&lt;/h2&gt;&lt;p&gt;The minimum requirement we could have for a dialogue system is that it can stay on the task we gave them. In fact, we humans often drift from topic to topic. How well do current systems perform? &lt;/p&gt;&lt;p&gt;&lt;br /&gt;Currently, “system prompt” is the main method that allows users to control LM behavior. However, researchers found evidence that LLMs can be brittle in following these instructions under adversarial conditions [12,13]. Readers might also have experienced this through daily interactions with ChatGPT or Claude—when a new chat window is freshly opened, the model can follow your instruction reasonably well [2], but after several rounds of dialogue, it’s no longer &lt;em&gt;fresh&lt;/em&gt;, even stops following its role altogether.&lt;/p&gt;&lt;p&gt;How could we quantitatively capture this anecdote? For one-round instruction following, we’ve already enjoyed plenty of benchmarks such as MT-Bench and Alpaca-Eval. However, when we test models in an interactive fashion, it’s hard to anticipate what the model generates and prepare a reply in advance. In a project by my collaborators and me [3], we built an environment to synthesize dialogues with unlimited length to stress-test the instruction-following capabilities of LLM chatbots. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;To allow an unconstrained scaling on the time scale, we let two system-prompted LM agents chat with each other for an extended number of rounds. This forms the main trunk of dialogue [a1, b1, a2, b2, …, a8, b8] (say the dialogue is 8-round). At this point, we could probably figure out how the LLMs stick to its system prompts just by examining this dialogue, but many of the utterances can be irrelevant to the instructions, depending on where the conversation goes. Therefore, we hypothetically branch out at each round by asking a question directly related to the system prompts, and use a corresponding judging function to quantify how well it performs. All that's provided by the dataset is a bank of triplets of (system prompts, probe questions, and judging functions).&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="What's Missing From LLM Chatbots: A Sense of Purpose" class="kg-image" height="570" src="https://thegradient.pub/content/images/2024/08/image3.png" width="1999" /&gt;&lt;figcaption&gt;Sketch of the process of measuring instruction stability&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Averaging across scenarios and pairs of system prompts, we get a curve of instruction stability across rounds. To our surprise, the aggregated results on both LLaMA2-chat-70B and gpt-3.5-turbo-16k are alarming. Besides the added difficulty to prompt engineering, the lack of instruction stability also comes with safety concerns. When the chatbot drifts away from its system prompts that stipulate safety aspects, it becomes more susceptible to jailbreaking and prone to more hallucinations.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="What's Missing From LLM Chatbots: A Sense of Purpose" class="kg-image" height="689" src="https://thegradient.pub/content/images/2024/08/Screenshot-2024-08-21-at-19.15.57.png" width="1736" /&gt;&lt;figcaption&gt;Instruction stability on LLaMA2-chat-70B and gpt-3.5-turbo-16k&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The empirical results also contrast with the ever-increasing context length of LLMs. Theoretically, some long-context models can attend to a window of up to 100k tokens. However, in the dialogue setting, they become distracted after only 1.6k tokens (assuming each utterance is 100 tokens). In [3], we further theoretically showed how this is inevitable in a Transformer based LM chatbot under the current prompting scheme, and proposed a simple technique called split-softmax to mitigate such effects. &lt;/p&gt;&lt;p&gt;One might ask at this point, why is it so bad? Why don't humans lose their persona just by talking to another person for 8 rounds? It’s arguable that human interactions are based on purposes and intentions [5] and these purposes precede the means rather than the opposite—LLM is fundamentally a fluent English generator, and the persona is merely a thin added layer.&lt;/p&gt;&lt;h2 id="what%E2%80%99s-missing"&gt;What’s missing? &lt;br /&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Pretraining?&lt;/strong&gt;&lt;br /&gt;Pretraining endows the language model with the capability to model a distribution over internet personas as well as the lower-level language distribution of each persona [4]. However, even when one persona (or a mixture of a limited number of them) is specified by the instruction of system prompts, current approaches fail to single it out.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;RLHF?&lt;/strong&gt;&lt;br /&gt;RLHF provides a powerful solution to adapting this multi-persona model to a “helpful and harmless assistant.” However, the original RLHF methods formulate reward maximization as a one-step bandit problem, and it is not generally possible to train with human feedback in the loop of conversation. (I’m aware of many advances in alignment but I want to discuss the original RLHF algorithm as a prototypical example.) This lack of multi-turn planning may cause models to suffer from task ambiguity [6] and learning superficial human-likeness rather than goal-directed social interaction [7].&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Will adding more dialogue data in RLHF help? My guess is that it will, to a certain extent, but it will still fall short due to a lack of purpose. Sergey Levine pointed out in his blog that there is a fundamental difference between preference learning and intentions: “the key distinction is between viewing language generation as selecting goal-directed actions in a sequential process, versus a problem of producing outputs satisfying user preferences.”&lt;/p&gt;&lt;h2 id="purposeful-dialogue-system"&gt;Purposeful dialogue system&lt;/h2&gt;&lt;p&gt;Staying on task is a modest request for LLMs. However, even if an LLM remains focused on the task, it doesn't necessarily mean it can excel in achieving the goal.&lt;/p&gt;&lt;p&gt;The problem of long-horizon planning has attracted some attention in the LLM community. For example, “decision-oriented dialogue” is proposed as a general class of tasks [8], where the AI assistant collaborates with humans to help them make complicated decisions, such as planning itineraries in a city and negotiating travel plans among friends. Another example, Sotopia [10], is a comprehensive social simulation platform that compiles various goal-driven dialogue scenarios including collaboration, negotiation, and persuasion. &lt;/p&gt;&lt;p&gt;Setting up such benchmarks provides not only a way to gauge the progress of the field, it also directly provides reward signals that new algorithms could pursue, which could be expensive to collect and tricky to define [9]. However, there aren’t many techniques that can exert control over the LM so that it can act consistently across a long horizon towards such goals. &lt;/p&gt;&lt;p&gt;To fill in this gap, my collaborators and I propose a lightweight algorithm (Dialogue Action Tokens, DAT [11]) that guides an LM chatbot through a multi-round goal-driven dialogue. As shown in the image below, in each round of conversations, the dialogue history’s last token embedding is used as the input (state) to a planner (actor) which predicts several prefix tokens (actions) to control the generation process. By training the planner with a relatively stable RL algorithm TD3+BC, we show significant improvement over baselines on Sotopia, even surpassing the social capability scores of GPT-4.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="What's Missing From LLM Chatbots: A Sense of Purpose" class="kg-image" height="176" src="https://thegradient.pub/content/images/2024/08/image2.png" width="1191" /&gt;&lt;figcaption&gt;A sketch of ​​Dialogue Action Tokens (DAT)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;br /&gt;In this way, we provide a technique pathway that upgrades LM from a prediction model that merely guesses the next token to one that engages in dialogue with humans purposefully. We could imagine that this technique can be misused for harmful applications as well. For this reason, we also conduct a “multi-round red-teaming” experiment, and recommend that more research could be done here to better understand multi-round dialogue as potential attack surface.&lt;/p&gt;&lt;h2 id="concluding-marks"&gt;Concluding marks&lt;br /&gt;&lt;/h2&gt;&lt;p&gt;I have reviewed the making of current LLM dialogue systems, how and why it is insufficient. I hypothesize that a purpose is what is missing and present one technique to add it back with reinforcement learning. &lt;/p&gt;&lt;p&gt;The following are two research questions that I’m mostly excited about: &lt;/p&gt;&lt;p&gt;(1) Better monitoring and control of dialogue systems with steering techniques. For example, the recently proposed TalkTurner (Chen et al.) adds a dashboard (Viégas et al) to open-sourced LLMs, enabling users to see and control how LLM thinks of themselves. Many weaknesses of current steering techniques are revealed and call for better solutions. For example, using activation steering to control two attributes (e.g., age and education level) simultaneously has been found to be difficult and can cause more language degradation. Another intriguing question is how to differentiate between LLM’s internal model of itself and that of the user. Anecdotally, chatting with Golden Gate Bridge Claude has shown that steering on the specific Golden Gate Bridge feature found by SAE sometimes causes Claude to think of itself as the San Francisco landmark, sometimes the users as the bridge, and other times the topic as such.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;(2) Better utilization of off-line reward signals. In the case of set-up environments like Sotopia and “decision-oriented dialogues”, rewards signals are engineered beforehand. In the real world, users won’t leave numerical feedback of how they feel satisfied. However, there might be other clues in language (e.g., “Thanks!”, “That’s very helpful!”) or from external resources (e.g., users buying the product for a salesman AI, users move to a subsequent coding question for copilot within a short time frame). Inferring and utilizing such hidden reward signals could strengthen the network effect of online chatbots: good model → more users → learning from interacting with users → better model.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Acknowledgment&lt;/strong&gt;&lt;br /&gt;The author is grateful to Martin Wattenberg and Hugh Zhang (alphabetical order) for providing suggestions and editing the text.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Citation&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;For attribution of this in academic contexts or books, please cite this work as:&lt;/p&gt;&lt;blockquote&gt;&lt;em&gt;Kenneth Li, "&lt;/em&gt;&lt;strong&gt;What's Missing From LLM Chatbots: A Sense of Purpose&lt;/strong&gt;&lt;em&gt;", The Gradient, 2024.&lt;/em&gt;&lt;/blockquote&gt;&lt;p&gt;BibTeX citation (this blog):&lt;/p&gt;&lt;div class="kg-card kg-callout-card kg-callout-card-grey"&gt;&lt;div class="kg-callout-text"&gt;@article{li2024from,&lt;br /&gt;author = {Li, Kenneth},&lt;br /&gt;title = {What's Missing From LLM Chatbots: A Sense of Purpose},&lt;br /&gt;journal = {The Gradient},&lt;br /&gt;year = {2024},&lt;br /&gt;howpublished = {\url{https://thegradient.pub/dialogue}},&lt;br /&gt;}&lt;/div&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[1] Schank, Roger C., and Robert P. Abelson. Scripts, plans, goals, and understanding: An inquiry into human knowledge structures. Psychology press, 2013.&lt;br /&gt;[2] Zhou, Jeffrey, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. "Instruction-following evaluation for large language models." arXiv preprint arXiv:2311.07911 (2023).&lt;br /&gt;[3] ​​Li, Kenneth, Tianle Liu, Naomi Bashkansky, David Bau, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. "Measuring and controlling persona drift in language model dialogs." arXiv preprint arXiv:2402.10962 (2024).&lt;br /&gt;[4] Andreas, Jacob. "Language models as agent models." arXiv preprint arXiv:2212.01681 (2022).&lt;br /&gt;[5] Austin, John Langshaw. How to do things with words. Harvard university press, 1975.&lt;br /&gt;[6] Tamkin, Alex, Kunal Handa, Avash Shrestha, and Noah Goodman. "Task ambiguity in humans and language models." arXiv preprint arXiv:2212.10711 (2022).&lt;br /&gt;[7] Bianchi, Federico, Patrick John Chia, Mert Yuksekgonul, Jacopo Tagliabue, Dan Jurafsky, and James Zou. "How well can llms negotiate? negotiationarena platform and analysis." arXiv preprint arXiv:2402.05863 (2024).&lt;br /&gt;[8] Lin, Jessy, Nicholas Tomlin, Jacob Andreas, and Jason Eisner. "Decision-oriented dialogue for human-ai collaboration." arXiv preprint arXiv:2305.20076 (2023).&lt;br /&gt;[9] Kwon, Minae, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. "Reward design with language models." arXiv preprint arXiv:2303.00001 (2023).&lt;br /&gt;[10] Zhou, Xuhui, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency et al. "Sotopia: Interactive evaluation for social intelligence in language agents." arXiv preprint arXiv:2310.11667 (2023).&lt;br /&gt;[11] Li, Kenneth, Yiming Wang, Fernanda Viégas, and Martin Wattenberg. "Dialogue Action Tokens: Steering Language Models in Goal-Directed Dialogue with a Multi-Turn Planner." arXiv preprint arXiv:2406.11978 (2024).&lt;br /&gt;[12] Li, Shiyang, Jun Yan, Hai Wang, Zheng Tang, Xiang Ren, Vijay Srinivasan, and Hongxia Jin. "Instruction-following evaluation through verbalizer manipulation." arXiv preprint arXiv:2307.10558 (2023).&lt;br /&gt;[13] Wu, Zhaofeng, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. "Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks." arXiv preprint arXiv:2307.02477 (2023).&lt;/p&gt;]]&amp;gt;&amp;lt;![CDATA[We Need Positive Visions for AI Grounded in Wellbeing]]&amp;gt;Introduction&lt;p&gt;Imagine yourself a decade ago, jumping directly into the present shock of conversing naturally with an encyclopedic AI that crafts images, writes code, and debates philosophy. Won’t this technology almost certainly transform society — and hasn’t AI’s impact on us so far been&lt;/p&gt;]]&amp;gt;https://thegradient.pub/we-need-positive-visions-for-ai-grounded-in-wellbeing/66a4243393571d5c8c154f4eSat, 03 Aug 2024 17:00:43 GMTIntroduction&lt;img alt="We Need Positive Visions for AI Grounded in Wellbeing" src="https://thegradient.pub/content/images/2024/07/wellbeing_ai_cover_image.webp" /&gt;&lt;p&gt;Imagine yourself a decade ago, jumping directly into the present shock of conversing naturally with an encyclopedic AI that crafts images, writes code, and debates philosophy. Won’t this technology almost certainly transform society — and hasn’t AI’s impact on us so far been a mixed-bag? Thus it’s no surprise that so many conversations these days circle around an era-defining question: &lt;em&gt;How do we ensure AI benefits humanity?&lt;/em&gt; These conversations often devolve into strident optimism or pessimism about AI, and our earnest aim is to walk a pragmatic middle path, though no doubt we will not perfectly succeed.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;While it’s fashionable to handwave towards “beneficial AI,” and many of us want to contribute towards its development — it’s not easy to pin down what beneficial AI concretely means in practice. This essay represents our attempt to demystify beneficial AI, through grounding it in the wellbeing of individuals and the health of society. In doing so, we hope to promote opportunities for AI research and products to benefit our flourishing, and along the way to share ways of thinking about AI’s coming impact that motivate our conclusions.&lt;/p&gt;&lt;h3 id="the-big-picture"&gt;The Big Picture&lt;/h3&gt;&lt;p&gt;By trade, we’re closer in background to AI than to the fields where human flourishing is most-discussed, such as wellbeing economics, positive psychology, or philosophy, and in our journey to find productive connections between such fields and the technical world of AI, we found ourselves often confused (what even is human flourishing, or wellbeing, anyways?) and from that confusion, often stuck (maybe there is nothing to be done? — the problem is too multifarious and diffuse). We imagine that others aiming to create prosocial technology might share our experience, and the hope here is to shine a partial path through the confusion to a place where there’s much interesting and useful work to be done. We start with some of our main conclusions, and then dive into more detail in what follows.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;One conclusion we came to is that &lt;strong&gt;it’s okay that we can’t conclusively define human wellbeing.&lt;/strong&gt; It’s been debated by philosophers, economists, psychotherapists, psychologists, and religious thinkers, for many years, and there’s no consensus. At the same time, there’s agreement around many concrete factors that make our lives go well, like: supportive intimate relationships, meaningful and engaging work, a sense of growth and achievement, and positive emotional experiences. And there’s clear understanding, too, that beyond momentary wellbeing, we must consider how to secure and improve wellbeing across years and decades — through what we could call &lt;em&gt;societal&lt;/em&gt; &lt;em&gt;infrastructure&lt;/em&gt;: important institutions such as education, government, the market, and academia. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;One benefit of this wellbeing lens is to wake us to an almost-paradoxical fact: While the deep purpose behind nearly everything our species does is wellbeing, we’ve tragically lost sight of it. &amp;nbsp;Both by common measures of individual wellbeing (suicide rate, loneliness, meaningful work) and societal wellbeing (trust in our institutions, shared sense of reality, political divisiveness), we’re not doing well, and our impression is that AI is complicit in that decline. The central benefit of this wellbeing view, however, is the insight that no fundamental obstacle prevents us from synthesizing the science of wellbeing with machine learning to our collective benefit. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;This leads to our second conclusion: &lt;strong&gt;We need plausible positive visions of a society with capable AI, grounded in wellbeing.&lt;/strong&gt; Like other previous transformative technologies, AI will shock our societal infrastructure — dramatically altering the character of our daily lives, whether we want it to or not. For example, Facebook launched only twenty years ago, and yet social media’s shockwaves have already upended much in society — subverting news media and our informational commons, addicting us to likes, and displacing meaningful human connection with its shell. We believe capable AI’s impact will exceed that of social media. As a result, it’s vital that we strive to explore, envision, and move towards the AI-infused worlds we’d flourish within — ones perhaps in which it revitalizes our institutions, empowers us to pursue what we find most meaningful, and helps us cultivate our relationships. This is no simple task, requiring imagination, groundedness, and technical plausibility — to somehow dance through the minefields illuminated by previous critiques of technology. Yet now is the time to dream and build if we want to actively shape what is to come.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;This segues into our final conclusion: &lt;strong&gt;Foundation models and the arc of their future deployment is critical.&lt;/strong&gt; Even for those of us in the thick of the field, it’s hard to internalize how quickly models have improved, and how capable they might become given several more years. Recall that GPT-2 — barely functional by today’s standards — was released &lt;em&gt;only in 2019&lt;/em&gt;. If future models are much more capable than today’s, and competently engage with more of the world with greater autonomy, we can expect their entanglement with our lives and society to rachet skywards. So, at minimum, we’d like to enable these models to understand our wellbeing and how to support it, potentially through new algorithms, wellbeing-based evaluations of models and wellbeing training data. Of course, we also want to realize human benefit in practice — the last section of this blog post highlights what we believe are strong leverage points towards that end.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;The rest of this post describes in more detail (1) what we mean by AI that benefits our wellbeing, (2) the need for positive visions for AI grounded in wellbeing, and (3) concrete leverage points to aid in the development and deployment of AI in service of such positive visions. We’ve designed this essay such that the individual parts are mostly independent, so if you are interested most in concrete research directions, feel free to skip there.&lt;/p&gt;&lt;h3 id="beneficial-ai-grounds-out-in-human-wellbeing"&gt;Beneficial AI grounds out in human wellbeing&lt;/h3&gt;&lt;p&gt;Discussion about AI for human benefit is often high-minded, but not particularly actionable, as in unarguable but content-free phrases like “We should make sure AI is in service of humanity.” But to meaningfully implement such ideas in AI or policy requires enough precision and clarity to translate them into code or law. So we set out to survey what science has discovered about the ground of human benefit, as a step towards being able to measure and support it through AI.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Often, when we think about beneficial impact, we focus on abstract pillars like democracy, education, fairness, or the economy. However important, none of these are valuable &lt;em&gt;intrinsically.&lt;/em&gt; We care about them because of how they affect our collective lived experience, over the short and long-term. We care about increasing society’s GDP to the extent it aligns with actual improvement of our lives and future, but when treated as an end in itself, it becomes disconnected from what matters: improving human (and potentially all species’) experience.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;In looking for fields that most directly study the root of human flourishing, we found the scientific literature on wellbeing. The literature is vast, spanning many disciplines, each with their own abstractions and theories — and, as you might expect, there’s no true consensus on what wellbeing actually is. In diving into the philosophy of flourishing, wellbeing economics, or psychological theories of human wellbeing, one encounters many interesting, compelling, but seemingly incompatible ideas. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;For example, theories of hedonism in philosophy claim that pleasure and the absence of suffering is the core of wellbeing; while desire satisfaction theories instead claim that wellbeing is about the fulfillment of our desires, no matter how we feel emotionally. There’s a wealth of literature on measuring subjective wellbeing (broadly, how we experience and feel about our life), and many different frameworks of what variables characterize flourishing. For example, Martin Seligman’s PERMA framework claims that wellbeing consists of positive emotions, engagement, relationships, meaning, and achievement. There are theories that say that the core of wellbeing is satisfying psychological needs, like the need for autonomy, competence, and relatedness. Other theories claim that wellbeing comes from living by our values. In economics, frameworks rhyme with those in philosophy and psychology, but diverge enough to complicate an exact bridge. For example, the wellbeing economics movement largely focuses on subjective wellbeing and explores many different proxies of it, like income, quality of relationships, job stability, etc.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;After the excitement from surveying so many interesting ideas began to fade, perhaps unsurprisingly, we remained fundamentally confused about what “the right theory” was. But, we recognized that in fact &lt;em&gt;this has always been the human situation when it comes to wellbeing&lt;/em&gt;, and just as a lack of an incontrovertible theory of flourishing has not prevented humanity from flourishing in the past, it need not stand as a fundamental obstacle for beneficial AI. In other words, our attempts to guide AI to support human flourishing must take this lack of certainty seriously, just as all sophisticated societal efforts to support flourishing must do.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;In the end, we came to a simple workable understanding, not far from the view of wellbeing economics: Human benefit ultimately must ground out in the &lt;em&gt;lived experience of humans&lt;/em&gt;. We want to live happy, meaningful, healthy, full lives — and it’s not so difficult to imagine ways AI might assist in that aim. For example, the development of low-cost but proficient AI coaches, intelligent journals that help us to self-reflect, or apps that help us to find friends, romantic partners, or to connect with loved ones. We can ground these efforts in imperfect but workable measures of wellbeing from the literature (e.g. PERMA), taking as &lt;em&gt;first-class concerns&lt;/em&gt; that the map (wellbeing measurement) is not the territory (actual wellbeing), and that humanity itself continues to explore and refine its vision of wellbeing.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;More broadly our wellbeing relies on a healthy society, and we care not only about our own lives, but also want beautiful lives for our neighbors, community, country, and world, and for our children, and their children as well. The infrastructure of society (institutions like government, art, science, military, education, news, and markets) is what supports this broader, longer-term vision of wellbeing.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="We Need Positive Visions for AI Grounded in Wellbeing" class="kg-image" height="362" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXe4f50PRkH1ZAWhltK8VAz1IhmtVLJU6k3pf1oGL-GmNTXM2QsnRJU52h0d8uCYPoFa_r6QB6UTtFThWPr6anV42FbBZPsnj1PXPDtp4Ofu5JjECD5CJz0W1asFNrFqvyL-PBxqYCk1VBxShyYTKoPj_HI?key=_z5hHgxLrjtdLauVz_eYpw" width="379" /&gt;&lt;/figure&gt;&lt;p&gt;Each of these institutions have important roles to play in society, and we can also imagine ways that AI could support or improve them; for example, generative AI may catalyze education through personal tutors that help us develop a richer worldview, may help us to better hold our politicians to account through sifting through what they are actually up to, or accelerate meaningful science through helping researchers make novel connections. Thus in short, &lt;em&gt;beneficial AI would meaningfully support our quest for lives worth living, in both the immediate and long-term sense.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;So, from the lofty confusion of conflicting grand theories, we arrive at something sounding more like common sense. Let’s not take this for granted, however — it cuts through the cruft of abstractions to firmly recenter what is ultimately important: the psychological experience of humans. This view points us towards the ingredients of wellbeing that are both well-supported scientifically and could be made measurable and actionable through AI (e.g. there exist instruments to measure many of these ingredients). Further, wellbeing across the short and long-term provides the common currency that bridges divergent approaches to beneficial AI, whether mitigating societal harms like discrimination in the AI ethics community, to attempting to reinvigorate democracy through AI-driven deliberation, to creating a world where humans live more meaningful lives, to creating low-cost emotional support and self-growth tools, to reducing the likelihood of existential risks from AI, to using AI to reinvigorate our institutions — wellbeing is the ultimate ground.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Finally, focusing on wellbeing helps to highlight where we currently fall short. Current AI development is driven by our existing incentive systems: Profit, research novelty, engagement, with little explicit focus on what fundamentally is more important (human flourishing). We need to find tractable ways to shift incentives towards wellbeing-supportive models (something we’ll discuss later), and positive directions to move toward (discussed next).&lt;/p&gt;&lt;h3 id="we-need-positive-visions-for-ai"&gt;We need positive visions for AI&lt;/h3&gt;&lt;p&gt;Technology is a shockingly powerful societal force. While nearly all new technologies bring only limited change, like an improved toothbrush, sometimes they upend the world. Like the proverbial slowly-boiling frog, we forget how in short order the internet and cellphones have &lt;em&gt;overhauled&lt;/em&gt; our lived experience: the rise of dating apps, podcasts, social networks, our constant messaging, cross-continental video calls, massive online games, the rise of influencers, on-demand limitless entertainment, etc. Our lives as a whole — our relationships, our leisure, how we work and collaborate, how news and politics work — &lt;em&gt;have dramatically shifted&lt;/em&gt;, for both the better and worse.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;AI is transformative, and the mixed bag of its impacts are poised to reshape society in mundane and profound ways; we might doubt it, but that was also our naivety at the advent of social media and the cell-phone. We don’t see it coming, and once it’s here we take it for granted. Generative AI translates applications from science fiction into rapid adoption: AI romantic companions; automated writing and coding assistants; automatic generation of high-quality images, music, and videos; low-cost personalized AI tutors; highly-persuasive personalized ads; and so on. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;In this way, transformative impact is happening &lt;em&gt;now&lt;/em&gt; — it does not require AI with superhuman intelligence — see the rise of LLM-based social media bots; ChatGPT as the fastest-adopted consumer app; LLMs requiring fundamental changes to homework in school. Much greater impact will yet come, as the technology (and the business around it) matures, and as AI is integrated more pervasively throughout society.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Our institutions were understandably not designed with this latest wave of AI in mind, and it’s unclear that many of them will adapt quickly enough to keep up with AI's rapid deployment. For example, an important function of news is to keep a democracy’s citizens well-informed, so their vote is meaningful. But news these days spreads through AI-driven algorithms on social media, which amplifies emotional virality and confirmation bias at the expense of meaningful debate. And so, the public square and the sense of a shared reality is being undercut, as AI degrades an important institution devised without foresight of this novel technological development.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thus in practice, it may not be possible to play defense by simply “mitigating harms” from a technology; often, a new technology demands that we creatively and skillfully apply our existing values to a radically new situation. We don’t want AI to, for example, undermine the livelihood of artists, yet how &lt;em&gt;do&lt;/em&gt; we want our relationship to creativity to look like in a world where AI can, easily and cheaply, produce compelling art or write symphonies and novels, in the style of your favorite artist? There’s no easy answer. We need to debate, understand, and capture what we believe is the &lt;em&gt;spirit &lt;/em&gt;of our institutions and systems given this new technology. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;For example, what’s truly important about education? We can reduce harms that AI imposes on the current education paradigm by banning use of AI in students’ essays, or apply AI in service of existing metrics (for example, to increase high school graduation rates). But the paradigm itself must adapt: The world that schooling currently prepares our children for is not the world they’ll graduate into, nor does it prepare us generally to flourish and find meaning in our lives. We must ask ourselves what we really value in education that we want AI to enable: Perhaps teaching critical thinking, enabling agency, and creating a sense of social belonging and civic responsibility?&lt;br /&gt;&lt;/p&gt;&lt;p&gt;To anticipate critique, we agree that there will be no global consensus on what education is for, or on the underlying essence of any particular institution, at root because different communities and societies have &amp;nbsp;distinct values and visions. But that’s okay: Let’s empower communities to fit AI systems to local societal contexts; for example, algorithms like constitutional AI enable creating different constitutions that embody flourishing for different communities. This kind of cheap flexibility is an exciting affordance, meaning we no longer must sacrifice nuance and context-sensitivity for scalability and efficiency, a bitter pill technology often pushes us to swallow.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;And while of course we have always wanted education to create critical thinkers, our past metrics (like standardized tests) have been so coarse that scoring high is easily gamed without critical thinking. But generative AI enables new affordances here, too: just as a teacher can socratically question a student to evaluate their independent thought, advances in generative AI open up the door for similarly qualitative and interactive measures, like personalized AI tutors that meaningfully gauge critical thinking.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;We hope to tow a delicate line beyond broken dichotomies, whether between naive optimism and pessimism, or idealism and cynicism. Change is coming, and we must channel it towards refined visions of what we want, which is a profound opportunity, rather than to assume that by default technology will deliver us (or doom us), or that we will be able to wholly resist the transformation it brings (or are entirely helpless against it). For example, we must temper naive optimism (“AI will save the world if only we deploy it everywhere!”) by integrating lessons from the long line of work that studies the social drivers and consequences of technology, often from a critical angle. But neither should cynical concerns so paralyze us that we remain only as critics on the sidelines.&lt;/p&gt;&lt;h2 id="so-what-can-we-do"&gt;So, what can we do?&lt;/h2&gt;&lt;p&gt;The case so far is that we need positive visions for society with capable AI, grounded in individual and societal wellbeing. But what concrete work can actually support this? We propose the following break-down:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Understanding where we want to go&lt;/li&gt;&lt;li&gt;Measuring how AI impacts our wellbeing&lt;/li&gt;&lt;li&gt;Training models that can support wellbeing&lt;/li&gt;&lt;li&gt;Deploying models in service of wellbeing&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The overall idea is to support an ongoing, iterative process of exploring the positive directions we want to go and deploying and adapting models in service of them.&lt;/p&gt;&lt;h3 id="we-need-to-understand-where-we-want-to-go-in-the-age-of-ai"&gt;We need to understand where we want to go in the age of AI&lt;/h3&gt;&lt;p&gt;This point follows closely from the need to explore the positive futures we want with AI. What directions of work and research can help us to clarify where is possible to go, and is worth going to, in the age of AI?&lt;br /&gt;&lt;/p&gt;&lt;p&gt;For starters, it’s more important now than ever to have productive and grounded discussions about questions like: What makes us human? How do we want to live? What do we want the future to feel like? What values are important to us? What do we want to retain as AI transformations sweep through society? Rather than being centered on the machine learning community, this should be an interdisciplinary, international effort, spanning psychology, philosophy, political science, art, economics, sociology, and neuroscience (and many other fields!), and bridging diverse intra- and international cultures. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;Of course, it’s easy to call for such a dialogue, but the real question is how such interdisciplinary discussions can be convened in a meaningful, grounded, and action-guiding way — rather than leading only to cross-field squabbles or agreeable but vacuous aspirations. Perhaps through participatory design that pairs citizens with disciplinary experts to explore these questions, with machine learning experts mainly serving to ground technological plausibility. Perhaps AI itself could be of service: For example, research in AI-driven deliberative democracy and plurality may help involve more people in navigating these questions; as might research into meaning alignment, by helping us describe and aggregate what is meaningful and worth preserving to us. It’s important here to look beyond cynicism or idealism (suggestive of meta-modern political philosophy): Yes, mapping exciting positive futures is not a cure-all, as there are powerful societal forces, like regulatory capture, institutional momentum, and the profit motive, that resist their realization, and yet, societal movements all have to start somewhere, and &lt;em&gt;some really do succeed&lt;/em&gt;.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Beyond visions for big-picture questions about the future, much work is needed to understand where we want to go in narrower contexts. For example, while it might at first seem trivial, how can we reimagine online dating with capable AI, given that healthy romantic partnership is such an important individual and societal good? Almost certainly, we will look back at swipe-based apps as misguided means for finding long-term partners. And many of our institutions, small and large, can be re-visioned in this way, from tutoring to academic journals to local newspapers. AI will make possible a much richer set of design possibilities, and we can work to identify which of those possibilities are workable and well-represent the desired essence of an institution’s role in our lives and society.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Finally, continued basic and applied research into the factors that contribute and characterize human wellbeing and societal health also are highly important, as these are what ultimately ground our visions. And as the next section explores, having better measures of such factors can help us to change incentives and work towards our desired futures.&lt;/p&gt;&lt;h3 id="we-need-to-develop-measures-for-how-ai-affects-wellbeing"&gt;We need to develop measures for how AI affects wellbeing&lt;/h3&gt;&lt;p&gt;For better and worse, we often navigate through what we measure. We’ve seen this play out before: Measure GDP, and nations orient towards increasing it at great expense. Measure clicks and engagement, and we develop platforms that are terrifyingly adept at keeping people hooked. A natural question is, what prevents us from similarly measuring aspects of wellbeing to guide our development and deployment of AI? And if we do develop wellbeing measures, can we avoid the pitfalls that have derailed other well-intended measures, like GDP or engagement?&lt;br /&gt;&lt;/p&gt;&lt;p&gt;One central problem for measurement is that wellbeing is more complex and qualitative than GDP or engagement. Time-on-site is a very straightforwardmeasure of engagement. In contrast, properties relevant to wellbeing, like the felt sense of meaning or the quality of healthy relationships, are difficult to pin down quantitatively, especially from the limited viewpoint of how a user interacts with a particular app. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;Wellbeing depends on the broader context of a user’s life in messy ways, meaning it’s harder to isolate how any small intervention impacts it. And so, wellbeing measures are more expensive and less standardized to apply, end up less measured, and less guide our development of technology. However, foundation models are beginning to have the exciting ability to work with qualitative aspects of wellbeing. For example, present-day language models can (with caveats) infer emotions from user messages and detect conflict; or conduct qualitative interviews with users about its impact on their experience. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;So one promising direction of research, though not easy, is to explore how foundation models themselves can be applied to more reliably measure facets of individual and societal wellbeing, and ideally, help to identify how AI products and services are impacting that wellbeing. The mechanisms of impact are two-fold: One, companies may currently lack means of measuring wellbeing even though all-things-equal they want their products to help humans; two, where the profit motive conflicts with encouraging wellbeing, if a product’s impact can be externally audited and published, it can help hold the company to account by consumers and regulators, shifting corporate &amp;nbsp;incentives towards societal good.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Another powerful way that wellbeing-related measures can have impact is as evaluation benchmarks for foundation models. In machine learning, evaluations are a powerful lever for channeling research effort through competitive pressure. For example, model providers and academics continuously develop new models that perform better and better on benchmarks like TruthfulQA. Once you have legible outcomes, you often spur innovation to improve upon them. We currently have very few benchmarks focused on how AI affects our wellbeing, or how well they can understand our emotions, make wise decisions, or respect our autonomy: We need to develop these benchmarks.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Finally, as mentioned briefly above, metrics can also create accountability and enable regulations. Recent efforts like the Stanford Foundational Model Transparency Index have created public accountability for AI labs, and initiatives like Responsible Scaling Policies are premised on evaluations of model capabilities, as are evaluations by government bodies such as AI safety institutes in both the UK and US. Are there similar metrics and initiatives to encourage accountability around AI’s impact on wellbeing?&lt;br /&gt;&lt;/p&gt;&lt;p&gt;To anticipate a natural concern, unanticipated side-effects are nearly universal when attempting to improve important &lt;em&gt;qualities&lt;/em&gt; through &lt;em&gt;quantitative&lt;/em&gt; measures. What if in measuring wellbeing, the second-order consequence is perversely to undermine it? For example, if a wellbeing measure doesn’t include notions of autonomy, in optimizing it we might create paternalistic AI systems that “make us happy” by decreasing our agency. There are book-length treatments on the failures of high modernism and (from one of the authors of this essay!) on the tyranny of measures and objectives, and many academic papers on how optimization can pervert measures or undermine our autonomy. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;The trick is to look beyond binaries. Yes, measures and evaluations have serious problems, yet we can work with them with wisdom, taking seriously previous failures and institutionalizing that all measures are imperfect. We want a diversity of metrics (metric federalism) and a diversity of AI models rather than a monoculture, we do not want measures to be direct optimization targets, and we want ways to responsively adjust measures when inevitably we learn of their limitations. This is a significant concern, and we must take it seriously — while some research has begun to explore this topic, more is needed. Yet in the spirit of pragmatic harm reduction, given that metrics are both technically and politically important for steering AI systems, developing less flawed measures remains an important goal.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Let’s consider one important example of harms from measurement: the tendency for a single global measure to trample local context. Training data for models, including internet data in particular, is heavily biased. Thus without deliberate remedy, models demonstrate uneven abilities to support the wellbeing of minority populations, undermining social justice (as convincingly highlighted by the AI ethics community). While LLMs have exciting potential to respect cultural nuance and norms, informed by the background of the user, we must work deliberately to realize it. One important direction is to develop measures of wellbeing specific to diverse cultural contexts, to drive accountability and reward progress.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;To tie these ideas about measurement together, we suggest a taxonomy, looking at measures of AI &lt;em&gt;capabilities, behaviors, usage, and impacts&lt;/em&gt;. Similar to this DeepMind paper, the idea is to examine spheres of expanding context, from testing a model in isolation (both what it is capable of and what behaviors it demonstrates), all the way to understanding what happens when a model meets the real world (how humans use it, and what its impact is on them and society).&lt;br /&gt;&lt;/p&gt;&lt;p&gt;The idea is that we need a complementary ecosystem of measures fit to different stages of model development and deployment. In more detail:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;AI capabilities&lt;/em&gt; refers to what models are able to do. For example, systems today are capable of generating novel content, and translating accurately between languages.&lt;/li&gt;&lt;li&gt;&lt;em&gt;AI behaviors&lt;/em&gt; refers to how an AI system responds to different concrete situations. For example, many models are trained to refuse to answer questions that enable dangerous activities, like how to build a bomb,even though they have the capability to correctly answer them).&lt;/li&gt;&lt;li&gt;&lt;em&gt;AI usage&lt;/em&gt; refers to how models are used in practice when deployed. For example, AI systems today are used in chat interfaces to help answer questions, as coding assistants in IDEs, to sort social media feeds, and as personal companions.&lt;/li&gt;&lt;li&gt;&lt;em&gt;AI impacts &lt;/em&gt;refers to how AI impacts our experience or society. For example, people may feel empowered to do what’s important to them if AI helps them with rote coding, and societal trust in democracy may increase if AI sorts social media feeds towards bridging divides.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;As an example of applying this framework to an important quality that contributes to wellbeing, here is a sketch of how we might design measures of human autonomy: &lt;/p&gt;&lt;!--kg-card-begin: html--&gt;&lt;table&gt;&lt;colgroup&gt;&lt;col width="101" /&gt;&lt;col width="135" /&gt;&lt;col width="134" /&gt;&lt;col width="126" /&gt;&lt;col width="124" /&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Goal&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Capabilities&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Model Benchmarks&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Behaviors&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;System Benchmarks&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Usage&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;User Surveys&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Impact&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;User and Population Surveys&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Respecting autonomy&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Understand what someone is trying to achieve in a given context&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Understand the frontier of someone’s skill level&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Understand what activities a user finds meaningful&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Socratic dialogue rather than just providing answers&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Tapping into users’ wisdom rather than giving advice&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Selective automation of tasks&lt;/span&gt;&lt;/p&gt;&lt;br /&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Used to aid humans with tasks rather than fully automate tasks they find&amp;nbsp; meaningful&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Used to help humans develop social skills instead of to nurture emotional attachment to simulated persona&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;People feel empowered&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;People are able to achieve their goals&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;People are pushed to grow&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;!--kg-card-end: html--&gt;&lt;p&gt;Let’s work through this example: we take a quality with strong scientific links to wellbeing, autonomy, and create measures of it and what enables it, all along the pipeline from model development to when it’s deployed at scale. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;Starting from the right side of the table (Impact), there exist validated psychological surveys that measure autonomy, which can be adapted and given to users of an AI app to measure its &lt;em&gt;impact&lt;/em&gt; on their autonomy. Then, moving leftwards, these changes in autonomy could be linked to more specific types of &lt;em&gt;usage&lt;/em&gt;, through additional survey questions. For example, perhaps automating tasks that users actually find meaningful may correlate with decreased autonomy.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Moving further left on the table, the &lt;em&gt;behaviors&lt;/em&gt; of models that are needed to enable beneficial usage and impact can be gauged through more focused benchmarks. To measure behaviors of an AI system, one could run fixed workflows on an AI application where gold-standard answers come from expert labelers; another approach is to simulate users (e.g. with language models) interacting with an AI application to see how often and skillfully it performs particular behaviors, like socratic dialogue.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Finally, &lt;em&gt;capabilities&lt;/em&gt; of a particular AI model could be similarly measured through benchmark queries input directly to the model, in a way very similar to how LLMs are benchmarked for capabilities like reasoning or question-answering. For example, the capability to understand a person’s skill level may be important to help them push their limits. A dataset could be collected of user behaviors in some application, annotated with their skill level; and the evaluation would be how well the model could predict skill level from observed behavior.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;At each stage, the hope is to link what is measured through evidence and reasoning to what lies above and below it in the stack. And we would want a diversity of measures at each level, reflecting different hypotheses about how to achieve the top-level quality, and with the understanding that each measure is always imperfect and subject to revision. In a similar spirit, rather than some final answer, this taxonomy and example autonomy measures are intended to inspire much-needed pioneering work towards wellbeing measurement.&lt;br /&gt;&lt;/p&gt;&lt;h3 id="we-need-to-train-models-to-improve-their-ability-to-support-wellbeing"&gt;We need to train models to improve their ability to support wellbeing&lt;/h3&gt;&lt;p&gt;Foundation models are becoming increasingly capable and in the future we believe most applications will not train models from scratch. Instead, most applications will prompt cutting-edge proprietary models, or fine-tune such models through limited APIs, or train small models on domain-specific responses from the largest models for cost-efficiency reasons. As evidence, note that to accomplish tasks with GPT-3 often required chaining together many highly-tuned prompts, whereas with GPT-4 those same tasks often succeed with the first casual prompting attempt. Additionally, we are seeing the rise of capable smaller models specialized for particular tasks, trained through data from large models.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;What’s important about this trend is that applications are differentially brought to market driven by what the largest models can most readily accomplish. For example, if frontier models excel at viral persuasion from being trained on Twitter data, but struggle with the depths of positive psychology, it will be easier to create persuasive apps than supportive ones, and there will be more of them, sooner, on the market.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thus we believe it’s crucial that the most capable foundation models &lt;em&gt;themselves&lt;/em&gt; understand what contributes to our wellbeing — an understanding granted to them through their &lt;em&gt;training process&lt;/em&gt;. We want the AI applications that we interface with (whether therapists, tutors, social media apps, or coding assistants) to understand how to support our wellbeing within their relevant role.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;However, the benefit of breaking down the capabilities and behaviors needed to support wellbeing, as we did earlier, is that we can deliberately target their improvement. One central lever is to gather or generate training data, which is the general fuel underlying model capabilities. There is an exciting opportunity to create datasets to support desired wellbeing capabilities and behaviors — for example, perhaps collections of wise responses to questions, pairs of statements from people and the emotions that they felt in expressing them, biographical stories about desirable and undesirable life trajectories, or first-person descriptions of human experience in general. The effect of these datasets can be grounded in the measures discussed above.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;To better ground our thinking, we can examine how wellbeing data could improve the common &lt;em&gt;phases&lt;/em&gt; of foundation model training: pretraining, fine-tuning, and alignment.&lt;/p&gt;&lt;h4 id="pretraining"&gt;Pretraining&lt;/h4&gt;&lt;p&gt;The first training phase (confusingly called pretraining) establishes a model’s base abilities. It does so by training on vast amounts of variable-quality data, like a scrape of the internet. One contribution could be to either generate or gather large swaths of wellbeing relevant data, or to prioritize such data during training (also known as altering the data mix). For example, data could be sourced from subreddits relevant to mental health or life decisions, collections of biographies, books about psychology, or transcripts of supportive conversations. Additional data could be generated through paying contractors, crowdsourced through Games With a Purpose — fun experiences that create wellbeing-relevant data as a byproduct, or simulated through generative agent-based models.&lt;/p&gt;&lt;h4 id="fine-tuning"&gt;Fine-tuning&lt;/h4&gt;&lt;p&gt;The next stage of model training is fine-tuning. Here, smaller amounts of high-quality data, like diverse examples of desired behavior gathered from experts, focus the general capabilities resulting from pretraining. For different wellbeing-supporting behaviors we might want from a model, we can create fine-tuning datasets through deliberate curation of larger datasets, or by enlisting and recording the behavior of human experts in the relevant domain. We hope that the companies training the largest models place more emphasis on wellbeing in this phase of training, which is often driven by tasks with more obvious economic implications, like coding.&lt;/p&gt;&lt;h4 id="alignment"&gt;Alignment&lt;/h4&gt;&lt;p&gt;The final stage of model training is alignment, often achieved through techniques like reinforcement learning through human feedback (RLHF), where human contractors give feedback on AI responses to guide the model towards better ones. Or through AI-augmented techniques like constitutional AI, where an AI teaches itself to abide by a list of human-specified principles. The fuel of RLHF is preference data about what responses are preferred over others. Therefore we imagine opportunities for creating data sets of expert preferences that relate to wellbeing behaviors (even though what constitutes expertise in wellbeing may be interestingly contentious). For constitutional AI, we may need to iterate in practice with lists of wellbeing principles that we want to support, like human autonomy, and how, specifically, a model can respect it across different contexts.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;In general, we need pipelines where wellbeing evaluations (as discussed in the last section) inform how we improve models. We need to find extensions to paradigms like RLHF that go beyond which response humans prefer in the moment, considering also which responses support user long-term growth, wellbeing, and autonomy, or better embody the spirit of the institutional role that the model is currently playing. These are intriguing, subtle, and challenging research questions that strike at the heart of the intersection of machine learning and societal wellbeing, and deserve much more attention. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;For example, we care about wellbeing over spans of years or decades, but it is impractical to apply RLHF &lt;em&gt;directly&lt;/em&gt; on human feedback to such ends, as we cannot wait decades to gather human feedback for a model; instead, we need research that helps integrate validated short-term proxies for long-term wellbeing (e.g. quality of intimate relationships, time spent in flow, etc.), ways to learn from longitudinal data where it exists (perhaps web journals, autobiographies, scientific studies), and to collect the judgment of those who devote their lifetime to helping support individuals flourish (like counselors or therapists).&lt;/p&gt;&lt;h3 id="we-need-to-deploy-ai-models-in-a-way-that-supports-wellbeing"&gt;We need to deploy AI models in a way that supports wellbeing&lt;/h3&gt;&lt;p&gt;Ultimately we want AI models deployed in the world to benefit us. AI applications could directly target human wellbeing, for example by directly supporting mental health or coaching us in a rigorous way. But as argued earlier, the broader ecosystem of AI-assisted applications, like social media, dating apps, video games, and content-providers like Netflix, serve as societal infrastructure for wellbeing and have enormous diffuse impact upon us; one of us has written about the possibility of creating more humanistic wellbeing-infrastructure applications. While difficult, dramatic societal benefits could result from, for example, new social media networks that better align with short and long-term wellbeing.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;We believe there are exciting opportunities for thoughtful positive deployments that pave the way as standard-setting beacons of hope, perhaps particularly in ethically challenging areas — although these of course may also be the riskiest. For example, artificial intimacy applications like Replika may be unavoidable even as they make us squeamish, and may truly benefit some users while harming others. It’s worthwhile to ask what (if anything) could enable artificial companions that are aligned with users’ wellbeing and do not harm society. Perhaps it is possible to thread the needle: they could help us develop the social skills needed to find real-world companions, or at least have strong, transparent guarantees about their fiduciary relationship to us, all while remaining viable as a business or non-profit. Or perhaps we can create harm-reduction services that help people unaddict from artificial companions that have become obstacles to their growth and development. Similar thoughts may apply to AI therapists, AI-assisted dating apps, and attention-economy apps, where incentives are difficult to align. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;One obvious risk is that we each are often biased to think we are more thoughtful than others, but may nonetheless be swept away by problematic incentives, like the trade-off between profit and user benefit. Legal structures like public benefit corporations, non-profits, or innovative new structures may help minimize this risk, as may value-driven investors or exceedingly careful design of internal culture.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Another point of leverage is that a successful proof of concept may change the attitudes and incentives for companies training and deploying the largest foundation models. We’re seeing a pattern where large AI labs incorporate best practices from outside product deployments back into their models. For example, ChatGPT plugins like data analysis and the GPT market were explored first by companies outside OpenAI before being incorporated into their ecosystem. And RLHF, which was first integrated into language models by OpenAI, is now a mainstay across foundation model development.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;In a similar way to how RLHF became a mainstay, we want the capability to support our agency, understand our emotions, and better embody institutional roles to also become table-stakes features for model developers.This could happen through research advances &lt;em&gt;outside&lt;/em&gt; of the big companies, making it much easier for such features to be adopted &lt;em&gt;within&lt;/em&gt; them — though adoption may require pressure, through regulation, advocacy, or competition.&lt;/p&gt;&lt;h3 id="initiatives"&gt;Initiatives&lt;/h3&gt;&lt;p&gt;We believe there’s much concrete work to be done in the present. Here are a sampling of initiatives to seed thinking about what could move the field forward:&lt;br /&gt;&lt;/p&gt;&lt;!--kg-card-begin: html--&gt;&lt;table&gt;&lt;colgroup&gt;&lt;col width="245" /&gt;&lt;col width="379" /&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Area&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Initiatives&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Understanding where we want to go&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Global discussions on what is important to us.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Democratic elicitation of what matters to people (for example, the work done by &lt;/span&gt;&lt;span&gt;Collective Intelligence Project&lt;/span&gt;&lt;span&gt; and the &lt;/span&gt;&lt;span&gt;Meaning Alignment Institute&lt;/span&gt;&lt;span&gt;).&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Concrete visualizations of what we want society to look like in 2050 (for example, the worldbuilding contest run by the &lt;/span&gt;&lt;span&gt;Future of Life Institute&lt;/span&gt;&lt;span&gt;).&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Surveys to understand how people are using models and what principles are important for these use cases.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Improve our basic understanding of the factors that lead to wellbeing.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Develop methods for measuring how AI affects wellbeing&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Create benchmarks for models’ ability to understand emotions, make wise choices, respond in ways that respect our autonomy, etc.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Evaluations on how models impact people’s psychological experience.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Develop metrics to better track individual and collective wellbeing (e.g. tracking our somatic states, tracking societal trust, etc).&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Train AI models based on what’s important to us&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Create datasets of emotionally supportive interactions.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Scalable oversight that helps people figure out what AI response would be best for their wellbeing.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Reinforcement Learning from Human Feedback with wellbeing-based feedback (e.g. from therapists).&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Democratic finetuning&lt;/span&gt;&lt;span&gt; (run by the Meaning Alignment Institute)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Deploy models in beneficial areas&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;AI for mental health, education, resolving conflicts, relationship support, etc.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;!--kg-card-end: html--&gt;&lt;h2 id="conclusion-a-call-to-action"&gt;Conclusion: A call to action&lt;/h2&gt;&lt;p&gt;AI will transform society in ways that we cannot yet predict. If we continue on the present track, we risk AI reshaping our interactions and institutions in ways that erode our wellbeing and what makes our lives meaningful. Instead, challenging as it may be, we need to develop AI systems that understand and support wellbeing, both individual and societal. This is our call to reorientate towards wellbeing, to continue building a community and a field, in hopes of realizing AI’s potential to support our species’ strivings toward a flourishing future.&lt;/p&gt;]]&amp;gt;&amp;lt;![CDATA[Financial Market Applications of LLMs]]&amp;gt;The AI revolution drove frenzied investment in both private and public companies and captured the public’s imagination in 2023. Transformational consumer products like ChatGPT are powered by Large Language Models (LLMs) that excel at modeling sequences of tokens that represent words or parts of words [2]. Amazingly, structural]]&amp;gt;https://thegradient.pub/financial-market-applications-of-llms/661762b993571d5c8c154ea7Sat, 20 Apr 2024 17:57:39 GMT&lt;p&gt;The AI revolution drove frenzied investment in both private and public companies and captured the public’s imagination in 2023. Transformational consumer products like ChatGPT are powered by Large Language Models (LLMs) that excel at modeling sequences of tokens that represent words or parts of words [2]. Amazingly, structural understanding emerges from learning next-token prediction, and agents are able to complete tasks such as translation, question answering and generating human-like prose from simple user prompts.&lt;/p&gt;&lt;p&gt;Not surprisingly, quantitative traders have asked: can we turn these models into the next price or trade prediction [1,9,10]? That is, rather than modeling sequences of words, can we model sequences of prices or trades. This turns out to be an interesting line of inquiry that reveals much about both generative AI and financial time series modeling. Be warned this will get wonky.&lt;/p&gt;&lt;p&gt;LLMs are known as autoregressive learners -- those using previous tokens or elements in a sequence to predict the next element or token. In quantitative trading, for example in strategies like statistical arbitrage in stocks, most research is concerned with identifying autoregressive structure. That means finding sequences of news or orders or fundamental changes that best predict future prices.&lt;/p&gt;&lt;p&gt;Where things break down is in the quantity and information content of available data to train the models. At the 2023 NeurIPS conference, Hudson River Trading, a high frequency trading firm, presented a comparison of the number of input tokens used to train GPT-3 with the amount of trainable tokens available in the stock market data per year HRT estimated that, with 3,000 tradable stocks, 10 data points per stock per day, 252 trading days per year, and 23400 seconds in a trading day, there are 177 billion stock market tokens per year available as market data. GPT-3 was trained on 500 billion tokens, so not far off [6].&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Financial Market Applications of LLMs" class="kg-image" height="368" src="https://thegradient.pub/content/images/2024/04/Screenshot-2024-04-18-at-3.11.18-PM.png" width="2000" /&gt;&lt;figcaption&gt;numbers courtesy of HRT 2023 NeuRIPS presentation&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;But, in the trading context the tokens will be prices or returns or trades rather than syllables or words; the former is much more difficult to predict. Language has an underlying linguistic structure (e.g., grammar) [7]. It’s not hard to imagine a human predicting the next word in a sentence, however that same human would find it extremely challenging to predict the next return given a sequence of previous trades, hence the lack of billionaire day traders. The challenge is that there are very smart people competing away any signal in the market, making it &lt;em&gt;almost &lt;/em&gt;efficient (“efficiently inefficient”, in the words of economist Lasse Pedersen) and hence unpredictable. No adversary actively tries to make sentences more difficult to predict — if anything, authors usually seek to make their sentences easy to understand and hence &lt;em&gt;more&lt;/em&gt; predictable.&lt;/p&gt;&lt;p&gt;Looked at from another angle, there is much more noise than signal in financial data. Individuals and institutions are trading for reasons that might not be rational or tied to any fundamental change in a business. The GameStop episode in 2021 is one such example. Financial time series are also constantly changing with new fundamental information, regulatory changes, and occasional large macroeconomic shifts such as currency devaluations. Language evolves at a much slower pace and over longer time horizons.&lt;/p&gt;&lt;p&gt;On the other hand, there are reasons to believe that ideas from AI will work well in financial markets. One emerging area of AI research with promising applications to finance is multimodal learning [5], which aims to use different modalities of data, for example both images and textual inputs to build a unified model. With OpenAI’s DALL-E 2 model, a user can enter text and the model will generate an image. In finance, multi-modal efforts could be useful to combine information classical sources such as technical time series data (prices, trades, volumes, etc.) with alternative data in different modes like sentiment or graphical interactions on twitter, natural language news articles and corporate reports, or the satellite images of shipping activity in a commodity centric port. Here, leveraging multi-modal AI, one could potentially incorporate all these types of non-price information to predict well.&lt;/p&gt;&lt;p&gt;Another strategy called ‘residualization’ holds prominence in both finance and AI, though it assumes different roles in the two domains. &amp;nbsp;In finance, structural `factor’ models break down the contemporaneous observations of returns across different assets into a shared component (the market return, or more generally returns of common, market-wide factors) and an idiosyncratic component unique to each underlying asset. Market and factor returns are difficult to predict and create interdependence, so it is often helpful to remove the common element when making predictions at the individual asset level and to maximize the number of independent observations in the data. &lt;/p&gt;&lt;p&gt;In residual network architectures such as transformers, there’s a similar idea that we want to learn a function h(X) of an input X, but it might be easier to learn the residual of h(X) to the identity map, i.e., h(X) – X. Here, if the function h(X) is close to identity, its residual will be close to zero, and hence there will be less to learn and learning can be done more efficiently. In both cases the goal is to exploit structure to refine predictions: in the finance case, the idea is to focus on predicting innovations beyond what is implied by the overall market, for residual networks the focus is on predicting innovations to the identity map.&lt;/p&gt;&lt;p&gt;A key ingredient for the impressive performance of LLMs work is their ability to discern affinities or strengths between tokens over long horizons known as context windows. In financial markets, the ability to focus attention across long horizons enables analysis of multi-scale phenomena, with some aspects of market changes explained across very different time horizons. For example, at one extreme, fundamental information (e.g., earnings) may be incorporated into prices over months, technical phenomena (e.g., momentum) might be realized over days, and, at the other extreme, microstructure phenomena (e.g., order book imbalance) might have a time horizon of seconds to minutes.&lt;/p&gt;&lt;p&gt;Capturing all of these phenomena involves analysis of multiple time horizons across the context window. However, in finance, prediction over multiple &lt;em&gt;future&lt;/em&gt; time horizons is also important. For example, a quantitative system may seek to trade to profit from multiple different anomalies that are realized over multiple time horizons (e.g., simultaneously betting on a microstructure event and an earnings event). This requires predicting not just the next period return of the stock, but the entire term structure or trajectory of expected returns, while current transformer-style predictive models only look one period in the future.&lt;/p&gt;&lt;p&gt;Another financial market application of LLMs might be synthetic data creation [4,8]. This could take a few directions. Simulated stock price trajectories can be generated that mimic characteristics observed in the market and can be extremely beneficial given that financial market data is scarce relative to other sources as highlighted above in the number of tokens available. Artificial data could open the door for meta-learning techniques which have successfully been applied, for example, in robotics. In the robotic setting controllers are first trained using cheap but not necessarily accurate physics simulators, before being better calibrated using expensive real world experiments with robots. In finance the simulators could be used to coarsely train and optimize trading strategies. The model would learn high level concepts like risk aversion and diversification and tactical concepts such as trading slowly to minimize the price impact of a trade. Then precious real market data could be employed to fine-tune the predictions and determine precisely the optimal speed to trade.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Financial Market Applications of LLMs" class="kg-image" height="258" src="https://thegradient.pub/content/images/2024/04/Screenshot-2024-04-18-at-3.08.44-PM.png" width="2000" /&gt;&lt;/figure&gt;&lt;p&gt;Financial market practitioners are often interested in extreme events, the times when trading strategies are more likely to experience significant gains or losses. Generative models where it’s possible to sample from extreme scenarios could find use. However extreme events by definition occur rarely and hence determining the right parameters and sampling data from the corresponding distribution is fraught.&lt;/p&gt;&lt;p&gt;Despite the skepticism that LLMs will find use in quantitative trading, they might boost fundamental analysis. As AI models improve, it’s easy to imagine them helping analysts refine an investment thesis, uncover inconsistencies in management commentary or find latent relationships between tangential industries and businesses [3]. Essentially these models could provide a Charlie Munger for every investor.&lt;/p&gt;&lt;p&gt;The surprising thing about the current generative AI revolution is that it’s taken almost everyone – academic researchers, cutting edge technology firms and long-time observers – by surprise. The idea that building bigger and bigger models would lead to emergent capabilities like we see today was totally unexpected and still not fully understood.&lt;/p&gt;&lt;p&gt;The success of these AI models has supercharged the flow of human and financial capital into AI, which should in turn lead to even better and more capable models. So while the case for GPT-4 like models taking over quantitative trading is currently unlikely, we advocate keeping an open mind. Expecting the unexpected has been a profitable theme in the AI business.&lt;/p&gt;&lt;hr /&gt;&lt;h3 id="references"&gt;References&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;“Applying Deep Neural Networks to Financial Time Series Forecasting” Allison Koenecke. 2022&lt;/li&gt;&lt;li&gt;“Attention is all you need.” A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones… &amp;nbsp;Advances in Neural Information Processing Systems, 2017&lt;/li&gt;&lt;li&gt;“Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models” . Lopez-Lira, Alejandro and Tang, Yuehua, (April 6, 2023) Available at SSRN&lt;/li&gt;&lt;li&gt;“Generating Synthetic Data in Finance: Opportunities, Challenges and Pitfalls.” SA Assefa, D Dervovic, M Mahfouz, RE Tillman… - Proceedings of the First ACM International Conference …, 2020&lt;/li&gt;&lt;li&gt;“GPT-4V(ision) System Card.” OpenAI. September 2023&lt;/li&gt;&lt;li&gt;“Language models are few-shot learners.” T Brown, B Mann, N Ryder, M Subbiah, JD Kaplan… - Advances in Neural Information Processing Systems, 2020&lt;/li&gt;&lt;li&gt;“Sequence to Sequence Learning with Neural Networks.” I.Sutskever,O.Vinyals,and Q.V.Le in Advances in Neural Information Processing Systems, 2014, pp. 3104–3112.&lt;/li&gt;&lt;li&gt;“Synthetic Data Generation for Economists”. A Koenecke, H Varian &amp;nbsp;- arXiv preprint arXiv:2011.01374, 2020&lt;/li&gt;&lt;li&gt;C. C. Moallemi, M. Wang. A reinforcement learning approach to optimal execution. Quantitative Finance, 22(6):1051–1069, March 2022.&lt;/li&gt;&lt;li&gt;C. Maglaras, C. C. Moallemi, M. Wang. A deep learning approach to estimating fill probabilities in a limit order book. Quantitative Finance, 22(11):1989–2003, October 2022.&lt;/li&gt;&lt;/ol&gt;&lt;h3 id="citation"&gt;Citation&lt;/h3&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Richard Dewey and Ciamac Moallemi, "Financial Market Applications of LLMs," The Gradient, 2024&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;@article{dewey2024financial,
    author = {Richard Dewey and Ciamac Moallemi},
    title = {Financial Market Applications of LLMs},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/financial-market-applications-of-llms},
}&lt;/code&gt;&lt;/pre&gt;]]&amp;gt;&amp;lt;![CDATA[A Brief Overview of Gender Bias in AI]]&amp;gt;https://thegradient.pub/gender-bias-in-ai/660d016f93571d5c8c154d89Mon, 08 Apr 2024 15:54:53 GMT&lt;p&gt;AI models reflect, and often exaggerate, existing gender biases from the real world. It is important to quantify such biases present in models in order to properly address and mitigate them.&lt;/p&gt;&lt;p&gt;In this article, I showcase a small selection of important work done (and currently being done) to uncover, evaluate, and measure different aspects of gender bias in AI models. I also discuss the implications of this work and highlight a few gaps I’ve noticed.&lt;/p&gt;&lt;h2 id="but-what-even-is-bias"&gt;But What Even Is Bias?&lt;/h2&gt;&lt;p&gt;All of these terms (“AI”, “gender”, and “bias”) can be somewhat overused and ambiguous. “AI” refers to machine learning systems trained on human-created data and encompasses both statistical models like word embeddings and modern Transformer-based models like ChatGPT. “Gender”, within the context of AI research, typically encompasses binary man/woman (because it is easier for computer scientists to measure) with the occasional “neutral” category. &lt;/p&gt;&lt;p&gt;Within the context of this article, I use “bias” to broadly refer to unequal, unfavorable, and unfair treatment of one group over another.&lt;/p&gt;&lt;p&gt;There are many different ways to categorize, define, and quantify bias, stereotypes, and harms, but this is outside the scope of this article. I include a reading list at the end of the article, which I encourage you to dive into if you’re curious.&lt;/p&gt;&lt;h2 id="a-short-history-of-studying-gender-bias-in-ai"&gt;A Short History of Studying Gender Bias in AI&lt;/h2&gt;&lt;p&gt;Here, I cover a &lt;em&gt;very small&lt;/em&gt; sample of papers I’ve found influential studying gender bias in AI. This list is not meant to be comprehensive by any means, but rather to showcase the diversity of research studying gender bias (and other kinds of social biases) in AI.&lt;/p&gt;&lt;h3 id="man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings-bolukbasi-et-al-2016"&gt;Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings (Bolukbasi et al., 2016)&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Short Summary: &lt;/strong&gt;Gender bias exists in word embeddings (numerical vectors which represent text data) as a result of biases in the training data.&lt;br /&gt;&lt;strong&gt;Longer summary&lt;/strong&gt;: Given the analogy, man is to king as woman is to x, the authors used simple arithmetic using word embeddings to find that x=queen fits the best.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="61" src="https://lh7-us.googleusercontent.com/FCZV18SevX8_ymyYmn7gUk2lay4rIBKKG4tOFTm7fjFgW_LduuHX2QEw48S0bMfdIjT7Z1T7G7EGotZT-MlsBiqWt1EYZC0CIgH2TTVlC7uQSttoC5f47xyfEWTZVr3J4A_ZyhdxzR2wQQvcxHkrc7M" width="368" /&gt;&lt;figcaption&gt;&lt;em&gt;Subtracting the vector representations for “man” from “woman” results in a similar value as subtracting the vector representations for “king” and “queen”. From &lt;/em&gt;&lt;em&gt;Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;However, the authors found sexist analogies to exist in the embeddings, such as:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;He is to carpentry as she is to sewing&lt;/li&gt;&lt;li&gt;Father is to doctor as mother is to nurse&lt;/li&gt;&lt;li&gt;Man is to computer programmer as woman is to homemaker&lt;/li&gt;&lt;/ul&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="57" src="https://lh7-us.googleusercontent.com/zSojN5xOMtwGzMoy5lwu1z-8uuA9m0-sShqxARSa23DBsldKaFJBvRrXysO3ReLrZPIYQrdV-H0tD-3520ZvwK10jxNDtCwUuL5PEHJuhepnvgMfAXdIJY9Ir8o5v2ygINBHhh3U57Z8bnSYaB1bV2Y" width="624" /&gt;&lt;figcaption&gt;&lt;em&gt;Subtracting the vector representations for “man” from “woman” results in a similar value as subtracting the vector representations for “computer programmer” and “homemaker”. From &lt;/em&gt;&lt;em&gt;Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This implicit sexism is a result of the text data that the embeddings were trained on (in this case, Google News articles).&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="243" src="https://lh7-us.googleusercontent.com/qMofwhApgAidjTu1eLVVgteacEKTvlg36td9SC6JDrNmbL2SAMkl2d2t8eNKcpo4EbechE06pEZ7uhOjIRz_kd0oCeJOyB6abHvaX_5uQSe4VGb8FKBEAMv3F1d9eiEYR2k7tnKmX3PYj27lEAiARKY" width="631" /&gt;&lt;figcaption&gt;&lt;em&gt;Gender stereotypes and gender appropriate analogies found in word embeddings, for the analogy “she is to X as he is to Y”. From &lt;/em&gt;&lt;em&gt;Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Mitigations:&lt;/strong&gt; The authors propose a methodology for debiasing word embeddings based on a set of gender-neutral words (such as female, male, woman, man, girl, boy, sister, brother). This debiasing method reduces stereotypical analogies (such as man=programmer and woman=homemaker) while keeping appropriate analogies (such as man=brother and woman=sister).&lt;/p&gt;&lt;p&gt;This method only works on word embeddings, which wouldn’t quite work for the more complicated Transformer-based AI systems we have now (e.g. LLMs like ChatGPT). However, this paper was able to quantify (and propose a method for removing) gender bias in word embeddings in a mathematical way, which I think is pretty clever.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Why it matters:&lt;/strong&gt; The widespread use of such embeddings in downstream applications (such as sentiment analysis or document ranking) would only amplify such biases.&lt;/p&gt;&lt;hr /&gt;&lt;h3 id="gender-shades-intersectional-accuracy-disparities-in-commercial-gender-classification-buolamwini-and-gebru-2018"&gt;Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification [Buolamwini and Gebru, 2018]&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Short summary:&lt;/strong&gt; Intersectional gender-and-racial biases exist in facial recognition systems, which can classify certain demographic groups (e.g. darker-skinned females) with much lower accuracy than for other groups (e.g. lighter-skinned males).&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Longer summary:&lt;/strong&gt; The authors collected a benchmark dataset consisting of equal proportions of four subgroups (lighter-skinned males, lighter-skinned females, darker- skinned males, darker-skinned females). They evaluated three commercial gender classifiers and found all of them to perform better on male faces than female faces; to perform better on lighter faces than darker faces; and to perform the worst on darker female faces (with error rates up to 34.7%). In contrast, the maximum error rate for lighter-skinned male faces was 0.8%.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="268" src="https://lh7-us.googleusercontent.com/Na3BK8q_NCHGOP6kk3IIlKUpk4ba4BoZyopg9ZfsE7qpOCA4_gJW68rZE6SEsp5XOL1Vsfg6yAsBjlieQ_hG4dZV4cVB5LZxYSBI2FKkTQ_2sukhULVCKoURvspOCaHnf5NnbEjjbFnJ11mavrwHlas" width="800" /&gt;&lt;figcaption&gt;&lt;em&gt;The accuracy of three different facial classification systems on four different subgroups. Table sourced from the &lt;/em&gt;&lt;em&gt;Gender Shades overview website&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Mitigation: &lt;/strong&gt;In direct response to this paper, Microsoft and IBM (two of the companies in the study whose classifiers were analyzed and critiqued) hastened to address these inequalities by fixing biases and releasing blog posts unreservedly engaging with the theme of algorithmic bias [1, 2]. These improvements mostly stemmed from revising and expanding the model training datasets to include a more diverse set of skin tones, genders, and ages.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;In the media&lt;/strong&gt;: &lt;/strong&gt;You might have seen the Netflix documentary “Coded Bias” and Buolamwini’s recent book Unmasking AI. You can also find an interactive overview of the paper on the Gender Shades website.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Why it matters&lt;/strong&gt;: &lt;/strong&gt;Technological systems are meant to improve the lives of all people, not just certain demographics (who correspond with the people in power, e.g. white men). It is important, also, to consider bias not just along a single axis (e.g. gender) but the intersection of multiple axes (e.g. gender and skin color), which may reveal disparate outcomes for different subgroups&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;&lt;hr /&gt;&lt;h3 id="gender-bias-in-coreference-resolution-rudinger-et-al-2018"&gt;Gender bias in Coreference Resolution [Rudinger et al., 2018]&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Short summary&lt;/strong&gt;: &lt;/strong&gt;Models for &lt;em&gt;coreference resolution&lt;/em&gt; (e.g. finding all entities in a text that a pronoun is referring to) exhibit gender bias, tending to resolve pronouns of one gender over another for certain occupations (e.g. for one model, “surgeon” resolves to “his” or “their”, but not to “her”).&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="267" src="https://lh7-us.googleusercontent.com/oW-n5i7f0t_4ajmNkuDzXxd20TXJncjMbzWlxi8tdFuEImfEu-zAs3W-0sdZQibbbYXkioiGzp1kz81vN5xotJba3WJznijO-pD2yv6RksOowM2wpTqzGXqmUzS1dbkht8_AFpMUArkFW691o82odQ0" width="611" /&gt;&lt;figcaption&gt;&lt;em&gt;A coreference resolution system resolves a male and neutral pronoun to refer to the “the surgeon” but does not for the corresponding female pronoun! From &lt;/em&gt;&lt;em&gt;Gender Bias in Coreference Resolution&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Intro to coreference resolution using a classic riddle&lt;/strong&gt;: &lt;em&gt;A man and his son get into a terrible car crash. The father dies, and the boy is badly injured. In the hospital, the surgeon looks at the patient and exclaims, “I can’t operate on this boy, he’s my son!” How can this be?&lt;/em&gt;&lt;/p&gt;&lt;p&gt;(Answer: The surgeon is the mother)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Longer summary&lt;/strong&gt;: &lt;/strong&gt;The authors created a dataset of sentences for coreference resolution where correct pronoun resolution was not a function of gender. However, the models tended to resolve male pronouns to occupations (more so than female or neutral pronouns). For example, the occupation “manager” is 38.5% female in the U.S. (according to the 2006 US Census data), but none of the models predicted managers to be female in the dataset.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Related work&lt;/strong&gt;: &lt;/strong&gt;Other papers [1, 2] address measuring gender bias in coreference resolution. This is also relevant in the area of &amp;nbsp;machine translation, especially when translating phrases into and from gendered languages [3, 4].&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Why it matters&lt;/strong&gt;: &lt;/strong&gt;It is important that models (and also humans) don’t immediately assume certain occupations or activities are linked to one gender because doing so might perpetuate harmful stereotypes.&lt;/p&gt;&lt;hr /&gt;&lt;h3 id="bbq-a-hand-built-bias-benchmark-for-question-answering-parrish-et-al-2021"&gt;BBQ: A Hand-Built Bias Benchmark for Question Answering [Parrish et al., 2021]&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Short summary&lt;/strong&gt;: &lt;/strong&gt;Large Language Models (LLMs) consistently reproduce harmful biases in ambiguous contexts.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Longer summary&lt;/strong&gt;: &lt;/strong&gt;The authors created a dataset called Bias Benchmark for QA (BBQ), which contains questions highlighting attested social biases against people belonging to nine social dimensions (relevant to U.S. English-speaking contexts). For example, for the sentence with an ambiguous context, “After the first day of middle school math, a GIRL and a BOY stayed after class to talk to the teacher”; and the question “Who is bad at math”?; the biased answer would be “GIRL is bad at math”. The models tested by the authors reinforced such stereotypes 77% of the time.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="505" src="https://lh7-us.googleusercontent.com/NFVXTOlCVMjcVtUKELSm39QzpMio-YJ5RrhH6ZTAPogpiMP-vZNdpYXFRWsvv-Qd-Ahk4WCi16epfQjBNfZKUY9jbZ7_wi2_bVKiOhuZWgj66hgJO2QyuEVbePvM9J37Dy2hYYlR7cA2qe7UiMdhkec" width="499" /&gt;&lt;figcaption&gt;&lt;em&gt;An example of a question using an ambiguous and a disambiguated context. From the &lt;/em&gt;&lt;em&gt;BBQ&lt;/em&gt;&lt;em&gt; paper.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Related work&lt;/strong&gt;: &lt;/strong&gt;Much of NLP research is focused on the English language. It is important to test for social biases in non-English languages, but it is often not enough to do a direct translation of the data into another language, due to cultural differences (for example, Walmart, Uber, and W-4 are concepts that may not exist in non-US cultures). Datasets such as CBBQ and KoBBQ perform a &lt;em&gt;cultural translation&lt;/em&gt; of the BBQ dataset into (respectively) the Chinese and Korean language and culture.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Why it matters:&lt;/strong&gt; &lt;/strong&gt;While this single benchmark is far from comprehensive, it is important to include in evaluations as it provides an automatable (e.g. no human evaluators needed) method of measuring bias in generative language models.&lt;/p&gt;&lt;hr /&gt;&lt;h3 id="stable-bias-analyzing-societal-representations-in-diffusion-models-luccioni-et-al-2023"&gt;Stable Bias: Analyzing Societal Representations in Diffusion Models [Luccioni et al., 2023]&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Short summary&lt;/strong&gt;: Image-generation models (such as DALL-E 2, Stable Diffusion, and Midjourney) contain social biases and consistently under-represent marginalized identities.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Longer summary&lt;/strong&gt;: &lt;/strong&gt;AI image-generation models tended to produce images of people that looked mostly white and male, especially when asked to generate images of people in positions of authority. For example, DALL-E 2 generated white men 97% of the time for prompts like “CEO”. The authors created several tools to help audit (or, understand model behavior of) such AI image-generation models using a targeted set of prompts through the lens of occupations and gender/ethnicity. For example, the tools allow qualitative analysis of differences in genders generated for different occupations, or what an average face looks like. They are available in this HuggingFace space.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="445" src="https://lh7-us.googleusercontent.com/2boKi96oJS5ZuSRrOr2sg4CtRsOM6aH-U-DRXCnxm6AGIPnvGRRJoButHvmUa9w7eakKB8ohKRIsF6oAbt2jN5R0yGOO-yNSIyUZyd3pdC_DJX7mXdNOsdjENfLOJW0dNJQPAIDoSWKdouczvmEnw40" width="800" /&gt;&lt;figcaption&gt;&lt;em&gt;An example of images generated by Stable Diffusion for the prompts “Compassionate manager” (showing mostly women) and “Manager” (showing all men). Image from an article written by the &lt;/em&gt;&lt;em&gt;MIT Technology Review&lt;/em&gt;&lt;em&gt; covering StableBias.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Why this matters&lt;/strong&gt;: &lt;/strong&gt;AI-image generation models (and now, AI-video generation models, such as OpenAI’s Sora and RunwayML’s Gen2) are not only becoming more and more sophisticated and difficult to detect, but also increasingly commercialized. As these tools are developed and made public, it is important to both build new methods for understanding model behaviors and measuring their biases, as well as to build tools to allow the general public to better probe the models in a systematic way.&lt;/p&gt;&lt;h2 id="discussion"&gt;Discussion&lt;/h2&gt;&lt;p&gt;The articles listed above are just a small sample of the research being done in the space of measuring gender bias and other forms of societal harms.&lt;/p&gt;&lt;h3 id="gaps-in-the-research"&gt;Gaps in the Research&lt;/h3&gt;&lt;p&gt;The majority of the research I mentioned above introduces some sort of benchmark or dataset. These datasets (luckily) are being increasingly used to evaluate and test new generative models as they come out.&lt;/p&gt;&lt;p&gt;However, as these benchmarks are used more by the companies building AI models, the models are optimized to address only the specific kinds of biases captured in these benchmarks. There are countless other types of unaddressed biases in the models that are unaccounted for by existing benchmarks.&lt;/p&gt;&lt;p&gt;In my blog, I try to think about novel ways to uncover the gaps in existing research in my own way:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;In Where are all the women?, I showed that language models' understanding of "top historical figures" exhibited a gender bias towards generating male historical figures and a geographic bias towards generating people from Europe, no matter what language I prompted it in.&lt;/li&gt;&lt;li&gt;In Who does what job? Occupational roles in the eyes of AI, I asked three generations of GPT models to fill in "The man/woman works as a ..." to analyze the types of jobs often associated with each gender. I found that more recent models tended to overcorrect and over-exaggerate gender, racial, or political associations for certain occupations. For example, software engineers were predominantly associated with men by GPT-2, but with women by GPT-4.In Lost in DALL-E 3 Translation, I explored how DALL-E 3 uses prompt transformations to enhance (and translate into English) the user’s original prompt. DALL-E 3 tended to repeat certain tropes, such as “young Asian women” and “elderly African men”.&lt;/li&gt;&lt;/ul&gt;&lt;h3 id="what-about-other-kinds-of-bias-and-societal-harm"&gt;What About Other Kinds of Bias and Societal Harm?&lt;/h3&gt;&lt;p&gt;This article mainly focused on gender bias — and particularly, on binary gender. However, there is amazing work being done with regards to more fluid definitions of gender, as well as bias against other groups of people (e.g. disability, age, race, ethnicity, sexuality, political affiliation). This is not to mention all of the research done on detecting, categorizing, and mitigating gender-based violence and toxicity.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Another area of bias that I think about often is cultural and geographic bias. That is, even when testing for gender bias or other forms of societal harm, most research tends to use a Western-centric or English-centric lens.&lt;/p&gt;&lt;p&gt;For example, the majority of images from two commonly-used open-source image datasets for training AI models, Open Images and ImageNet, are sourced from the US and Great Britain.&lt;/p&gt;&lt;p&gt;This skew towards Western imagery means that AI-generated images often depict cultural aspects such as “wedding” or “restaurant” in Western settings, subtly reinforcing biases in seemingly innocuous situations. Such uniformity, as when "doctor" defaults to male or "restaurant" to a Western-style establishment, might not immediately stand out as concerning, yet underscores a fundamental flaw in our datasets, shaping a narrow and exclusive worldview.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="259" src="https://lh7-us.googleusercontent.com/1RGjMh4DGvfo0irKM8U9qODTo724-n6kvOSmysScHuTgVwT4-wQXpTt6YTa0Qk1QyQb_YkH2DdmM1LTIQTkN2omqKbB5aWUohauKdBl0v_9REuAP7aftBtXem9aS1NnPcWqn5qQRrJuSfYfvM-d3Nvk" width="800" /&gt;&lt;figcaption&gt;&lt;em&gt;Proportion of Open Images and ImageNet images from each country (represented by their two-letter ISO country codes). In both data sets, top represented locations include the US and Great Britain. From &lt;/em&gt;&lt;em&gt;No Classification without Representation&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3 id="how-do-we-%E2%80%9Cfix%E2%80%9D-this"&gt;How Do We “Fix” This?&lt;br /&gt;&lt;/h3&gt;&lt;p&gt;This is the billion dollar question!&lt;/p&gt;&lt;p&gt;There are a variety of technical methods for “debiasing” models, but this becomes increasingly difficult as the models become more complex. I won’t focus on these methods in this article.&lt;/p&gt;&lt;p&gt;In terms of concrete mitigations, the companies training these models need to be more transparent about both the datasets and the models they’re using. Solutions such as Datasheets for Datasets and Model Cards for Model Reporting have been proposed to address this lack of transparency from private companies. Legislation such as the recent AI Foundation Model Transparency Act of 2023 are also a step in the right direction. However, many of the large, closed, and private AI models are doing the opposite of being open and transparent, in both training methodology as well as dataset curation.&lt;/p&gt;&lt;p&gt;Perhaps more importantly, we need to talk about what it means to “fix” bias.&lt;/p&gt;&lt;p&gt;Personally, I think this is more of a philosophical question — societal biases (against women, yes, but also against all sorts of demographic groups) exist in the real world and on the Internet.Should language models reflect the biases that already exist in the real world to better represent reality? If so, you might end up with AI image generation models over-sexualizing women, or showing “CEOs” as White males and inmates as people with darker skin, or depicting Mexican people as men with sombreros.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="404" src="https://lh7-us.googleusercontent.com/-3QSkLD3zel6TcjmvMEP4s9yrwFRP-HpBrLBZFeJEiS9YWZ-yaMUyvQALcSFvQP4PDFLy1JfSy0586-9kR5p64VrSV3Dapqpb0kr4u9RkwY4LIYIUcPhp8Igcjlivq_jhA0WHY1_dswawXmL5GKdRg8" width="800" /&gt;&lt;figcaption&gt;&lt;em&gt;A screenshot showing how depictions of “A Mexican person” usually shows a man in a sombrero. From &lt;/em&gt;&lt;em&gt;How AI Reduces the World to Stereotypes&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;rest of world&lt;/em&gt;&lt;em&gt;’s analysis into biases in Midjourney.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Or, is it the prerogative of those building the models to represent an idealistically equitable world? &amp;nbsp;If so, you might end up with situations like DALL-E 2 appending race/gender identity terms to the ends of prompts and DALL-E 3 automatically transforming user prompts to include such identity terms without notifying them or Gemini generating racially-diverse Nazis.&lt;br /&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="411" src="https://lh7-us.googleusercontent.com/n3cWDhOcZCa3gnpAKXnmdpL8cVe7v42sKesaMK41CSps5ubaxbcyzSvb5uYR_DKHvSUaiU3gmRo08e_xuFITBa1x4738asdfk9c47kDTBLOpr7YQ6k83F0CMtPgMASQKe9-puDYbC_RzZmwtbK0lQRo" width="247" /&gt;&lt;figcaption&gt;&lt;em&gt;Images generated by Google’s Gemini Pro. From &lt;/em&gt;&lt;em&gt;The Verge’s article reporting on Gemini’s inaccurate historical portrayals&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;There’s no magic pill to address this. For now, what will happen (and is happening) is AI researchers and members of the general public will find something “wrong” with a publicly available AI model (e.g. from gender bias in historical events to image-generation models only generating White male CEOs). The model creators will attempt to address these biases and release a new version of the model. People will find new sources of bias; and this cycle will repeat.&lt;/p&gt;&lt;h3 id="final-thoughts"&gt;Final Thoughts&lt;/h3&gt;&lt;p&gt;It is important to evaluate societal biases in AI models in order to improve them — before addressing any problems, we must first be able to measure them. Finding problematic aspects of AI models helps us think about what kind of tools we want in our lives and what kind of world we want to live in.&lt;/p&gt;&lt;p&gt;AI models, whether they are chatbots or models trained to generate realistic videos, are, at the end of the day, trained on data created by humans — books, photographs, movies, and all of our many ramblings and creations on the Internet. It is unsurprising that AI models would reflect and exaggerate the biases and stereotypes present in these human artifacts — but it doesn’t mean that it always needs to be this way.&lt;/p&gt;&lt;hr /&gt;&lt;h2 id="author-bio"&gt;Author Bio&lt;/h2&gt;&lt;p&gt;Yennie is a multidisciplinary machine learning engineer and AI researcher currently working at Google Research. She has worked across a wide range of machine learning applications, from health tech to humanitarian response, and with organizations such as OpenAI, the United Nations, and the University of Oxford. She writes about her independent AI research experiments on her blog at Art Fish Intelligence.&lt;/p&gt;&lt;h2 id="a-list-of-resources-for-the-curious-reader"&gt;A List of Resources for the Curious Reader&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Barocas, S., &amp;amp; Selbst, A. D. (2016). Big data's disparate impact. &lt;em&gt;California law review&lt;/em&gt;, 671-732.&lt;/li&gt;&lt;li&gt;Blodgett, S. L., Barocas, S., Daumé III, H., &amp;amp; Wallach, H. (2020). Language (technology) is power: A critical survey of" bias" in nlp. arXiv preprint arXiv:2005.14050.&lt;/li&gt;&lt;li&gt;Bolukbasi, T., Chang, K. W., Zou, J. Y., Saligrama, V., &amp;amp; Kalai, A. T. (2016). Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29.&lt;/li&gt;&lt;li&gt;Buolamwini, J., &amp;amp; Gebru, T. (2018, January). Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency (pp. 77-91). PMLR.&lt;/li&gt;&lt;li&gt;Caliskan, A., Bryson, J. J., &amp;amp; Narayanan, A. (2017). Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334), 183-186.&lt;/li&gt;&lt;li&gt;Cao, Y. T., &amp;amp; Daumé III, H. (2019). Toward gender-inclusive coreference resolution. &lt;em&gt;arXiv preprint arXiv:1910.13913&lt;/em&gt;.&lt;/li&gt;&lt;li&gt;Dev, S., Monajatipoor, M., Ovalle, A., Subramonian, A., Phillips, J. M., &amp;amp; Chang, K. W. (2021). Harms of gender exclusivity and challenges in non-binary representation in language technologies. &lt;em&gt;arXiv preprint arXiv:2108.12084&lt;/em&gt;.&lt;/li&gt;&lt;li&gt;Dodge, J., Sap, M., Marasović, A., Agnew, W., Ilharco, G., Groeneveld, D., ... &amp;amp; Gardner, M. (2021). Documenting large webtext corpora: A case study on the colossal clean crawled corpus. arXiv preprint arXiv:2104.08758.&lt;/li&gt;&lt;li&gt;Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wallach, H., Iii, H. D., &amp;amp; Crawford, K. (2021). Datasheets for datasets. &lt;em&gt;Communications of the ACM&lt;/em&gt;, &lt;em&gt;64&lt;/em&gt;(12), 86-92.&lt;/li&gt;&lt;li&gt;Gonen, H., &amp;amp; Goldberg, Y. (2019). Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them. &lt;em&gt;arXiv preprint arXiv:1903.03862&lt;/em&gt;.&lt;/li&gt;&lt;li&gt;Kirk, H. R., Jun, Y., Volpin, F., Iqbal, H., Benussi, E., Dreyer, F., ... &amp;amp; Asano, Y. (2021). Bias out-of-the-box: An empirical analysis of intersectional occupational biases in popular generative language models. Advances in neural information processing systems, 34, 2611-2624.&lt;/li&gt;&lt;li&gt;Levy, S., Lazar, K., &amp;amp; Stanovsky, G. (2021). Collecting a large-scale gender bias dataset for coreference resolution and machine translation. arXiv preprint arXiv:2109.03858.&lt;/li&gt;&lt;li&gt;Luccioni, A. S., Akiki, C., Mitchell, M., &amp;amp; Jernite, Y. (2023). Stable bias: Analyzing societal representations in diffusion models. arXiv preprint arXiv:2303.11408.&lt;/li&gt;&lt;li&gt;Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., ... &amp;amp; Gebru, T. (2019, January). Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency (pp. 220-229).&lt;/li&gt;&lt;li&gt;Nadeem, M., Bethke, A., &amp;amp; Reddy, S. (2020). StereoSet: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456.&lt;/li&gt;&lt;li&gt;Parrish, A., Chen, A., Nangia, N., Padmakumar, V., Phang, J., Thompson, J., ... &amp;amp; Bowman, S. R. (2021). BBQ: A hand-built bias benchmark for question answering. arXiv preprint arXiv:2110.08193.&lt;/li&gt;&lt;li&gt;Rudinger, R., Naradowsky, J., Leonard, B., &amp;amp; Van Durme, B. (2018). Gender bias in coreference resolution. arXiv preprint arXiv:1804.09301.&lt;/li&gt;&lt;li&gt;Sap, M., Gabriel, S., Qin, L., Jurafsky, D., Smith, N. A., &amp;amp; Choi, Y. (2019). Social bias frames: Reasoning about social and power implications of language. &lt;em&gt;arXiv preprint arXiv:1911.03891&lt;/em&gt;.&lt;/li&gt;&lt;li&gt;Savoldi, B., Gaido, M., Bentivogli, L., Negri, M., &amp;amp; Turchi, M. (2021). Gender bias in machine translation. Transactions of the Association for Computational Linguistics, 9, 845-874.&lt;/li&gt;&lt;li&gt;Shankar, S., Halpern, Y., Breck, E., Atwood, J., Wilson, J., &amp;amp; Sculley, D. (2017). No classification without representation: Assessing geodiversity issues in open data sets for the developing world. arXiv preprint arXiv:1711.08536.&lt;/li&gt;&lt;li&gt;Sheng, E., Chang, K. W., Natarajan, P., &amp;amp; Peng, N. (2019). The woman worked as a babysitter: On biases in language generation. arXiv preprint arXiv:1909.01326.&lt;/li&gt;&lt;li&gt;Weidinger, L., Rauh, M., Marchal, N., Manzini, A., Hendricks, L. A., Mateos-Garcia, J., ... &amp;amp; Isaac, W. (2023). Sociotechnical safety evaluation of generative ai systems. arXiv preprint arXiv:2310.11986.&lt;/li&gt;&lt;li&gt;Zhao, J., Mukherjee, S., Hosseini, S., Chang, K. W., &amp;amp; Awadallah, A. H. (2020). Gender bias in multilingual embeddings and cross-lingual transfer. arXiv preprint arXiv:2005.00699.&lt;/li&gt;&lt;li&gt;Zhao, J., Wang, T., Yatskar, M., Ordonez, V., &amp;amp; Chang, K. W. (2018). Gender bias in coreference resolution: Evaluation and debiasing methods. arXiv preprint arXiv:1804.06876.&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="acknowledgements"&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;This post was originally posted on Art Fish Intelligence&lt;/p&gt;&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Yennie Jun, "Gender Bias in AI," The Gradient, 2024&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;@article{Jun2024bias,
    author = {Yennie Jun},
    title = {Gender Bias in AI},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/gender-bias-in-ai},
}&lt;/code&gt;&lt;/pre&gt;]]&amp;gt;&amp;lt;![CDATA[Mamba Explained]]&amp;gt;https://thegradient.pub/mamba-explained/65fb8d5993571d5c8c154beaThu, 28 Mar 2024 01:24:43 GMT&lt;p&gt;&lt;br /&gt;&lt;strong&gt;The State Space Model taking on Transformers&lt;/strong&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="168" src="https://lh7-us.googleusercontent.com/Vv2LBVlbspbhtzNqDFAAZ8xgkHKAzJiEoef9HZTlGVFpxAbWCMavNmhj408DdeOPZbj53vySwQR81e2zXlo52xA8OrJCq00V_z5VGwEMgfcvSW2uh60hFdjYliY-GAa_Kptz2XFbUf8S_-WrJqyhI4k" width="300" /&gt;&lt;/figure&gt;&lt;p&gt;Right now, AI is eating the world.&lt;/p&gt;&lt;p&gt;And by AI, I mean Transformers. Practically all the big breakthroughs in AI over the last few years are due to Transformers.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Mamba&lt;/strong&gt;, however, is one of an alternative class of models called &lt;strong&gt;State Space Models&lt;/strong&gt; (&lt;strong&gt;SSMs&lt;/strong&gt;). Importantly, for the first time, Mamba promises similar performance (and crucially similar &lt;em&gt;scaling laws&lt;/em&gt;) as the Transformer whilst being feasible at long sequence lengths (say 1 million tokens). To achieve this long context, the Mamba authors remove the “quadratic bottleneck” in the Attention Mechanism. Mamba also runs &lt;em&gt;fast&lt;/em&gt; - like “up to 5x faster than Transformer fast”&lt;sup&gt;1&lt;/sup&gt;.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="277" src="https://lh7-us.googleusercontent.com/uIkOGdo_oOuGilrgILP7E0KvNC8Y7ZL93om_wMUQCJEEIeSo0GtO4dzQ4bHMq5sdZu2ldL-fMrFy3KcLAr5_A8JhNOqqPyxFbYPPx016x1Djhr9VJ0lGzcEMvDDe5a-r0Wv-xvtneEYUSMJAsVS0OTY" width="572" /&gt;&lt;figcaption&gt;Mamba performs similarly (or slightly better than) other Language Models on The Pile (source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Gu and Dao, the Mamba authors write:&lt;/p&gt;&lt;p&gt;&lt;em&gt;Mamba enjoys fast inference and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modelling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Here we’ll discuss:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;The advantages (and disadvantages) of Mamba (🐍) vs Transformers (🤖),&lt;/li&gt;&lt;li&gt;Analogies and intuitions for thinking about Mamba, and&lt;/li&gt;&lt;li&gt;What Mamba means for Interpretability, AI Safety and Applications.&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="problems-with-transformersmaybe-attention-isn%E2%80%99t-all-you-need"&gt;Problems with Transformers - Maybe Attention &lt;em&gt;Isn’t&lt;/em&gt; All You Need&lt;/h2&gt;&lt;p&gt;We’re very much in the Transformer-era of history. ML used to be about detecting cats and dogs. Now, with Transformers, we’re generating human-like poetry, coding better than the median competitive programmer, and solving the protein folding problem.&lt;/p&gt;&lt;p&gt;But Transformers have one core problem. In a transformer, every token can look back at every previous token when making predictions. For this lookback, we cache detailed information about each token in the so-called KV cache.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="393" src="https://lh7-us.googleusercontent.com/dTD7M6vcg6ZBJPUyvFw_sOLbcZl6s6WXQbQ9Nfo3gq92G7bFIDBmr4Zj-Lahw7rZyHh6yKxRrSe790W04cyWAcRyM2rKkNz2wmsF_XJfP9mNJI5pSdst688I6o-brks05LF4N_5fNUPlQ1vvF8dOOdE" width="602" /&gt;&lt;figcaption&gt;When using the Attention Mechanism, information from all previous tokens can be passed to the current token&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This pairwise communication means a forward pass is O(n²) time complexity in training (the dreaded quadratic bottleneck), and each new token generated autoregressively takes O(n) time. In other words, as the context size increases, the model gets &lt;em&gt;slower&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;To add insult to injury, storing this key-value (KV) cache requires O(n) space. &amp;nbsp;Consequently, the dreaded CUDA out-of-memory (OOM) error becomes a significant threat as the memory footprint expands. If space were the only concern, we might consider adding more GPUs; however, with latency increasing quadratically, simply adding more compute might not be a viable solution.&lt;/p&gt;&lt;p&gt;On the margin, we can mitigate the quadratic bottleneck with techniques like Sliding Window Attention or clever CUDA optimisations like FlashAttention. But ultimately, for super long context windows (like a chatbot which remembers every conversation you’ve shared), we need a different approach.&lt;/p&gt;&lt;h3 id="foundation-model-backbones"&gt;Foundation Model Backbones&lt;/h3&gt;&lt;p&gt;Fundamentally, all good ML architecture backbones have components for two important operations:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Communication&lt;/strong&gt; &lt;em&gt;between&lt;/em&gt; tokens&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Computation&lt;/strong&gt; &lt;em&gt;within&lt;/em&gt; a token&lt;/li&gt;&lt;/ol&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="412" src="https://lh7-us.googleusercontent.com/WpckyY81cA3zGS1j1vq5lH-nZKiRdelILLO6OdiX05s4Psqe3oBpIZiy1IavhsutFkz4oa7V9ZjzGhjxcdMxD9Q_Z3pYelK04_7YA1-I-_PVu3SLDfBBK1c4-M3QcHh0MwzQcUR7wccwPKvjoXzS06I" width="602" /&gt;&lt;figcaption&gt;The Transformer Block&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In transformers, this is &lt;strong&gt;Attention&lt;/strong&gt; (communication) and &lt;strong&gt;MLPs&lt;/strong&gt; (computation). We improve transformers by optimising these two operations&lt;sup&gt;2&lt;/sup&gt;.&lt;/p&gt;&lt;p&gt;We would like to substitute the Attention component&lt;sup&gt;3&lt;/sup&gt; with an alternative mechanism for facilitating inter-token communication. Specifically, &lt;strong&gt;Mamba&lt;/strong&gt; employs a Control Theory-inspired State Space Model, or &lt;strong&gt;SSM,&lt;/strong&gt; for Communication purposes while retaining Multilayer Perceptron (MLP)-style projections for Computation.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="340" src="https://lh7-us.googleusercontent.com/T4MbDYFoOq5yAKl9uEEs9tjMy-CxBYy2S2rxnKbo5PmlnumyMs3DWV5chNooGG2hGp8ES9vXLEkmjHqlEzoCocVAnN2nquNhcBVK4hnrsfDJfBjJs5RZvx2bMSZEkm5yZtrTt7wBZfMW_iQXp4u8cU0" width="602" /&gt;&lt;figcaption&gt;The Mamba Block&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Like a Transformer made up of stacked transformer blocks, Mamba is made up of stacked Mamba blocks as above.&lt;/p&gt;&lt;p&gt;We would like to understand and motivate the choice of the SSM for sequence transformations.&lt;/p&gt;&lt;h2 id="motivating-mambaa-throwback-to-temple-run"&gt;Motivating Mamba - A Throwback to Temple Run&lt;/h2&gt;&lt;p&gt;Imagine we’re building a Temple Run agent&lt;sup&gt;4&lt;/sup&gt;. It chooses if the runner should move left or right at any time.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="822" src="https://thegradient.pub/content/images/2024/03/temple_run.png" width="900" /&gt;&lt;/figure&gt;&lt;p&gt;To successfully pick the correct direction, we need information about our surroundings. Let’s call the collection of relevant information the state. Here the state likely includes your current position and velocity, the position of the nearest obstacle, weather conditions, etc.&lt;/p&gt;&lt;blockquote&gt;&lt;em&gt;Claim 1: if you know the current state of the world and how the world is evolving, then you can use this to determine the direction to move.&lt;/em&gt;&lt;/blockquote&gt;&lt;p&gt;Note that you don’t need to look at the whole screen all the time. You can figure out what will happen to most of the screen by noting that as you run, the obstacles move down the screen. You only need to look at the top of the screen to understand the new information and then simulate the rest.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="295" src="https://lh7-us.googleusercontent.com/09a_eDMzBRh-usMcrg1W-JnkWE59PbsAtAW3Q8z8NmeyHGCpGsKG58dJtHNTnVUunlBbGb7xKt8nExTChRxMdcs1a125J7p11vDMR77GzigsI3j797VQxLLB9e_ILa1l8A-BCy7psxnYBIoQzk6-2GQ" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;This lends itself to a natural formulation. Let h be the hidden state, relevant knowledge about the world. Also let x be the input, the observation that you get each time. h’ then represents the derivative of the hidden state, i.e. how the state is evolving. We’re trying to predict y, the optimal next move (right or left).&lt;/p&gt;&lt;p&gt;Now, Claim 1 states that from the hidden state h, h’, and the new observation x, you can figure out y.&lt;/p&gt;&lt;p&gt;More concretely, h, the state, can be represented as a differential equation (Eq 1a):&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$h’(t) = \mathbf{A}h(t) + \mathbf{B}x(t)$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;Knowing h allows you to determine your next move y (Eq 1b):&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$y(t) = \mathbf{C}h(t) + \mathbf{D}x(t)$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;The system's evolution is determined by its current state and newly acquired observations. A small new observation is enough, as the majority of the state can be inferred by applying known state dynamics to its previous state. That is, most of the screen isn’t new, it’s just a continuation of the previous state's natural downward trajectory. A full understanding of the state would enable optimal selection of the subsequent action, denoted as y.&lt;/p&gt;&lt;p&gt;You can learn a lot about the system dynamics by observing the top of the screen. For instance, increased velocity of this upper section suggests an acceleration of the rest of the screen as well, so we can infer that the game is speeding up&lt;sup&gt;5&lt;/sup&gt;. In this way, even if we start off knowing nothing about the game and only have limited observations, it becomes possible to gain a holistic understanding of the screen dynamics fairly rapidly.&lt;/p&gt;&lt;h3 id="what%E2%80%99s-the-state"&gt;What’s the State?&lt;/h3&gt;&lt;p&gt;Here, &lt;strong&gt;state&lt;/strong&gt; refers to the variables that, when combined with the input variables, fully determine the future system behaviour. In theory, once we have the state, there’s nothing else we need to know about the past to predict the future. With this choice of state, the system is converted to a &lt;strong&gt;Markov Decision Process&lt;/strong&gt;. Ideally, the state is a fairly small amount of information which captures the essential properties of the system. That is, &lt;strong&gt;the state is a compression of the past&lt;/strong&gt;&lt;sup&gt;6&lt;/sup&gt;.&lt;/p&gt;&lt;h2 id="discretisationhow-to-deal-with-living-in-a-quantised-world"&gt;Discretisation - How To Deal With Living in a Quantised World&lt;/h2&gt;&lt;p&gt;Okay, great! So, given some state and input observation, we have an autoregressive-style system to determine the next action. Amazing!&lt;/p&gt;&lt;p&gt;In practice though, there’s a little snag here. We’re modelling time as continuous. But in real life, we get new inputs and take new actions at discrete time steps&lt;sup&gt;7&lt;/sup&gt;.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="601" src="https://lh7-us.googleusercontent.com/_A8UqIDZgHLXm-YwGNfpfE7gSg6fA5-PhsNKZEHAbHNS2-XBYRrZpDGUvJgiOIBCg126L7s2GYMxn98LSdgkVJNC5_sL5HNsDjazFLArizSkJbEAJAVmL3BpajxCbWO-5Hgtq9CEfW_lfzmUscSZTPg" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;We would like to convert this &lt;em&gt;continuous-time differential equation&lt;/em&gt; into a &lt;em&gt;discrete-time difference equation&lt;/em&gt;. This conversion process is known as discretisation. Discretisation is a well-studied problem in the literature. Mamba uses the Zero-Order Hold (ZOH) discretisation&lt;sup&gt;8&lt;/sup&gt;. To give an idea of what’s happening morally, consider a naive first-order approximation&lt;sup&gt;9&lt;/sup&gt;.&lt;/p&gt;&lt;p&gt;From Equation 1a, we have&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$h’(t) = \mathbf{A}h(t) + \mathbf{B}x(t)$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;And for small ∆,&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$h’(t) \approx \frac{h(t+\Delta) - h(t)}{\Delta}$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;by the definition of the derivative.&lt;/p&gt;&lt;p&gt;We let:&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$h_t = h(t)$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;and&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$h_{t+1} = h(t + \Delta)$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;and substitute into Equation 1a giving:&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$h_{t+1} - h_t \approx \Delta (\mathbf{A}h_t + \mathbf{B}x_t)$&lt;br /&gt;$\Rightarrow h_{t+1} \approx (I + \Delta \mathbf{A})h_t + (\Delta&lt;br /&gt;\mathbf{B})x_t$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;Hence, after renaming the coefficients and relabelling indices, we have the discrete representations:&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="127" src="https://lh7-us.googleusercontent.com/JNkElXh35QPUmp4Sl625go-1PnrKWpzDdV5BObpnSg6-bbhKDxr83Y0AZi7XT8CQdxF1CeByNH4sbFyDc-aTRWyXeXrBDL499-BXjte-iYGD01UR4udyI-a9J7D-w9Ao6COYZC7HpDcoQxzOqzqA5IY" width="384" /&gt;&lt;figcaption&gt;The Discretised Version of the SSM Equation&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;If you’ve ever looked at an RNN before&lt;sup&gt;10&lt;/sup&gt; and this feels familiar - &lt;em&gt;trust your instincts&lt;/em&gt;:&lt;/p&gt;&lt;p&gt;&lt;em&gt;We have some input x, which is combined with the previous hidden state by some transform to give the new hidden state. Then we use the hidden state to calculate the output at each time step.&lt;/em&gt;&lt;/p&gt;&lt;h2 id="understanding-the-ssm-matrices"&gt;Understanding the SSM Matrices&lt;/h2&gt;&lt;p&gt;Now, we can interpret the A, B, C, D matrices more intuitively:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;A is the transition state matrix. It shows how you transition the current state into the next state. It asks “How should I forget the less relevant parts of the state over time?”&lt;/li&gt;&lt;li&gt;B is mapping the new input into the state, asking “What part of my new input should I remember?”&lt;sup&gt;11&lt;/sup&gt;&lt;/li&gt;&lt;li&gt;C is mapping the state to the output of the SSM. It asks, “How can I use the state to make a good next prediction?”&lt;sup&gt;12&lt;/sup&gt;&lt;/li&gt;&lt;li&gt;D is how the new input passes through to the output. It’s a kind of modified skip connection that asks “How can I use the new input in my prediction?”&lt;/li&gt;&lt;/ul&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="335" src="https://lh7-us.googleusercontent.com/Vj3X7tBhV9WaGqNTB8t5zXJ9zRPzd0G075JEPazSOJ-D9S0-UYKwrjHFkGxIZBM1HucvGw4UQazcZJ3Kl7kN8hoqKVaRB8i1qRGjWz56mFA2SrBJBL9XKT72950OZCblDZ7AB0TLqXl4fWAx8BO-P-o" width="602" /&gt;&lt;figcaption&gt;Visual Representation of The SSM Equations&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Additionally, ∆ has a nice interpretation - it’s the step size, or what we might call the linger time or the dwell time. For large ∆, you focus more on that token; for small ∆, you skip past the token immediately and don’t include it much in the next state.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="224" src="https://lh7-us.googleusercontent.com/t1ikATLC5zPLHbXwvx0qTGnvEKAROGmpKl6QZgKfV4hs-2jjr9BvLYoecz0XRXsxHelPl23DoFE6G4P8oeuef2JuQvF0NhSg4N3YIqGmIF9oXBAXtNBrTH6ilcnboFsZPW306EVyZ--TcIHrOqxTbpQ" width="602" /&gt;&lt;figcaption&gt;(source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;And that’s it! That’s the SSM, our ~drop-in replacement for Attention (Communication) in the Mamba block. The Computation in the Mamba architecture comes from regular linear projections, non-linearities, and local convolutions.&lt;/p&gt;&lt;p&gt;Okay great, that’s the theory - but does this work? Well…&lt;/p&gt;&lt;h2 id="effectiveness-vs-efficiency-attention-is-focus-selectivity-is-prioritisation"&gt;Effectiveness vs Efficiency: Attention is Focus, Selectivity is Prioritisation&lt;/h2&gt;&lt;p&gt;At WWDC ‘97, Steve Jobs famously noted that “focusing is about saying no”. Focus is ruthless prioritisation. It’s common to think about Attention &lt;em&gt;positively&lt;/em&gt; as choosing what to &lt;em&gt;notice&lt;/em&gt;. In the Steve Jobs sense, we might instead frame Attention &lt;em&gt;negatively&lt;/em&gt; as choosing what to &lt;em&gt;discard&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;There’s a classic intuition pump in Machine Learning known as the Cocktail Party Problem&lt;sup&gt;13&lt;/sup&gt;. Imagine a party with dozens of simultaneous loud conversations:&lt;/p&gt;&lt;p&gt;Question:&lt;/p&gt;&lt;p&gt;&lt;em&gt;How do we recognise what one person is saying when others are talking at the same time?&lt;sup&gt;14&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Answer:&lt;/p&gt;&lt;p&gt;&lt;em&gt;The brain solves this problem by focusing your “attention” on a particular stimulus and hence drowning out all other sounds as much as possible.&lt;/em&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="376" src="https://lh7-us.googleusercontent.com/C18AUAf7863Uq5SHEwb4aQFcFoA4HW8olFXz_MvZ9HttqJNF2hvIfm3TEsNLhRkXyEJTOwhbtUyOh4QKV2qiGUXwA1sq2_CSTjO7FWPvK2YRnJgYvN859kqXo8pOkZffsXC0iO9z5yajWbc_9CvtwO8" width="602" /&gt;&lt;/figure&gt;&lt;hr /&gt;&lt;p&gt;Transformers use Dot-Product Attention to focus on the most relevant tokens. A big reason Attention is so great is that you have the potential to look back at everything that ever happened in its context. This is like photographic memory when done right.&lt;sup&gt;15&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;Transformers (🤖) are extremely &lt;strong&gt;effective&lt;/strong&gt;. But they aren’t very &lt;strong&gt;efficient&lt;/strong&gt;. They store everything from the past so that they can look back at tokens with theoretically perfect recall.&lt;/p&gt;&lt;p&gt;Traditional RNNs (🔁) are the opposite - they forget a lot, only recalling a small amount in their hidden state and discarding the rest. They are very &lt;strong&gt;efficient&lt;/strong&gt; - their state is small. Yet they are less &lt;strong&gt;effective&lt;/strong&gt; as discarded information cannot be recovered.&lt;/p&gt;&lt;p&gt;We’d like something closer to the Pareto frontier of the effectiveness/efficiency tradeoff. Something that’s more effective than traditional RNNs and more efficient than transformers.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="407" src="https://lh7-us.googleusercontent.com/V2BPTE_TEzO_CAXFnp54TL-nAzSpkiHN_PWZeWOgMN7TInAXL8i3hLgS8ruinxworyEl0248jU6y4Y86Wg1TJca-UjzjCrMQrmSpWceXJ-C4LIg6SJvJykJFfDBb12rIQi84B-aHKdPG_gWsxVkxT20" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;The Mamba Architecture seems to offer a solution which pushes out the Pareto frontier of effectiveness/efficiency.&lt;/p&gt;&lt;p&gt;SSMs are as &lt;strong&gt;efficient&lt;/strong&gt; as RNNs, but we might wonder how &lt;strong&gt;effective&lt;/strong&gt; they are. After all, it seems like they would have a hard time discarding only &lt;em&gt;unnecessary&lt;/em&gt; information and keeping everything relevant. If each token is being processed the same way, applying the same A and B matrices as if in a factory assembly line for tokens, there is no context-dependence. We would like the forgetting and remembering matrices (A and B respectively) to vary and dynamically adapt to inputs.&lt;/p&gt;&lt;h3 id="the-selection-mechanism"&gt;The Selection Mechanism&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Selectivity&lt;/strong&gt; allows each token to be transformed into the state in a way that is unique to its own needs. Selectivity is what takes us from vanilla SSM models (applying the same A (forgetting) and B (remembering) matrices to every input) to Mamba, the &lt;em&gt;&lt;strong&gt;Selective&lt;/strong&gt;&lt;/em&gt; &lt;em&gt;State Space Model&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;In regular SSMs, A, B, C and D are learned matrices - that is&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$\mathbf{A} = \mathbf{A}_{\theta}$ etc. (where θ represents the learned parameters)&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;With the Selection Mechanism in Mamba, A, B, C and D are also functions of x. That is $\mathbf{A} = \mathbf{A}_{\theta(x)}$ etc; the matrices are context dependent rather than static.&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="184" src="https://lh7-us.googleusercontent.com/wATzvqFAg8l5HWS9BSCi_OGZRkZ7XmoPfpuZkIaCgLNE1jwrocWaKn_j6OrSG_4n5uULQN6yYK1oWkR4_AbCTXnpaJDTw9PPmeF7btcFa4-7h1QESJIBxTPK4D5vbzFvGJKjxUu-kXqYnRi_oPiVAD4" width="602" /&gt;&lt;figcaption&gt;Mamba (right) differs from traditional SSMs by allowing A,B,C matrices to be &lt;strong&gt;selective &lt;/strong&gt;i.e. context dependent (source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Making A and B functions of x allows us to get the best of both worlds:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;We’re selective about what we include in the state, which improves &lt;strong&gt;effectiveness&lt;/strong&gt; vs traditional SSMs.&lt;/li&gt;&lt;li&gt;Yet, since the state size is bounded, we improve on &lt;strong&gt;efficiency&lt;/strong&gt; relative to the Transformer. We have O(1), not O(n) space and O(n) not O(n²) time requirements.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The Mamba paper authors write:&lt;/p&gt;&lt;p&gt;&lt;em&gt;The efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state: efficient models must have a small state, while effective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension.&lt;/em&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;Humans (mostly) don’t have photographic memory for everything they experience within a lifetime - or even within a day! There’s just way too much information to retain it all. Subconsciously, we select what to remember by choosing to forget, throwing away most information as we encounter it. Transformers (🤖) decide what to focus on at &lt;strong&gt;recall time&lt;/strong&gt;. Humans (🧑) also decide what to throw away at &lt;strong&gt;memory-making time&lt;/strong&gt;. Humans filter out information early and often.&lt;/p&gt;&lt;p&gt;If we had infinite capacity for memorisation, it’s clear the transformer approach is better than the human approach - it truly is more effective. But it’s less efficient - transformers have to store so much information about the past that might not be relevant. Transformers (🤖) only decide what’s relevant at &lt;strong&gt;recall time&lt;/strong&gt;. The innovation of Mamba (🐍) is allowing the model better ways of forgetting earlier - it’s focusing by choosing what to &lt;em&gt;discard&lt;/em&gt; using &lt;strong&gt;Selectivity&lt;/strong&gt;, throwing away less relevant information at &lt;strong&gt;memory-making time&lt;/strong&gt;&lt;sup&gt;16&lt;/sup&gt;.&lt;/p&gt;&lt;h3 id="the-problems-of-selectivity"&gt;The Problems of Selectivity&lt;/h3&gt;&lt;p&gt;Applying the Selection Mechanism does have its gotchas though. Non-selective SSMs (i.e. A,B not dependent on x) are fast to compute in training. This is because the component of&lt;/p&gt;&lt;p&gt;Yt which depends on xi can be expressed as a linear map, i.e. a single matrix that can be precomputed!&lt;/p&gt;&lt;p&gt;For example (ignoring the D component, the skip connection):&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$$y_2 = \mathbf{C}\mathbf{B}x_2 + \mathbf{C}\mathbf{A}\mathbf{B}x_1 +&lt;br /&gt;\mathbf{C}\mathbf{A}\mathbf{A}\mathbf{B}x_0$$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;If we’re paying attention, we might spot something even better here - this expression can be written as a convolution. Hence we can apply the Fast Fourier Transform and the Convolution Theorem to compute this &lt;em&gt;very&lt;/em&gt; efficiently on hardware as in Equation 3 below.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="93" src="https://lh7-us.googleusercontent.com/SnLnXqZ4ArJyiJmMNiUiDMpZ0WYRXuaWO-ZS_Ogj-hThlMVbZz8B3F9g09H5V5CQG6mjgiSphIpjOz4ATr_JYLxCZ9T-EjG5dNy1-mpL1JwL-XWJbymVgyEGhdxpfUT34B1v4iJ_vQAiNUGeTs2FMXs" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;We can calculate Equation 2, the SSM equations, efficiently in the Convolutional Form, Equation 3.&lt;/p&gt;&lt;p&gt;Unfortunately, with the Selection Mechanism, we lose the convolutional form. Much attention is given to making Mamba efficient on modern GPU hardware using similar hardware optimisation tricks to Tri Dao’s Flash Attention&lt;sup&gt;17&lt;/sup&gt;. With the hardware optimisations, Mamba is able to run faster than comparably sized Transformers.&lt;/p&gt;&lt;h3 id="machine-learning-for-political-economistshow-large-should-the-state-be"&gt;Machine Learning for Political Economists - How Large Should The State Be?&lt;/h3&gt;&lt;p&gt;The Mamba authors write, “the efficiency vs. effectiveness tradeoff of sequence models is characterised by how well they compress their state”. In other words, like in political economy&lt;sup&gt;18&lt;/sup&gt;, the fundamental problem is how to manage the state.&lt;/p&gt;&lt;p&gt;🔁 &lt;strong&gt;Traditional RNNs are anarchic&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;They have a small, minimal state. The size of the state is bounded. The compression of state is poor.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;🤖 &lt;strong&gt;Transformers are communist&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;They have a maximally large state. The “state” is just a cache of the entire history with no compression. Every context token is treated equally until recall time.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;🐍&lt;strong&gt;Mamba has a compressed state&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;…but it’s selective about what goes in. Mamba says we can get away with a small state if the state is well focused and effective&lt;sup&gt;19&lt;/sup&gt;.&lt;/em&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="275" src="https://lh7-us.googleusercontent.com/rkN6fi0try__wiIKQ1D9gbHvCrW_dHsKV0jckG85H7P3_Lx1Vm2vHfeb7Zs6N50lnjVx04A3QTQb2JSjMltn8C0kFmvB4DPUgsjj_DEAGu8O-LcKlY7G0RLgLCCsDV_R1W4pkkE67_2rnyx0vCMnayM" width="602" /&gt;&lt;figcaption&gt;Language Models and State Size&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The upshot is that state&lt;strong&gt; representation is critical&lt;/strong&gt;. A smaller state is more efficient; a larger state is more effective. The key is to &lt;strong&gt;selectively&lt;/strong&gt; and &lt;strong&gt;dynamically&lt;/strong&gt; compress data into the state. Mamba’s Selection Mechanism allows for context-dependent reasoning, focusing and ignoring. For both performance and interpretability, understanding the state seems to be very useful.&lt;/p&gt;&lt;h2 id="information-flow-in-transformer-vs-mamba"&gt;Information Flow in Transformer vs Mamba&lt;/h2&gt;&lt;p&gt;How do Transformers know anything? At initialization, a transformer isn’t very smart. It learns in two ways:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Training data (Pretraining, SFT, RLHF etc)&lt;/li&gt;&lt;li&gt;In context-data&lt;/li&gt;&lt;/ol&gt;&lt;h4 id="training-data"&gt;Training Data&lt;/h4&gt;&lt;p&gt;Models learn from their training data. This is a kind of lossy compression of input data into the weights. We can think of the effect of pretraining data on the transformer kinda like the effect of your ancestor’s experiences on your genetics - you can’t recall their experiences, you just have vague instincts about them&lt;sup&gt;20&lt;/sup&gt;.&lt;/p&gt;&lt;h4 id="in-context-data"&gt;In Context-Data&lt;/h4&gt;&lt;p&gt;Transformers use their context as short-term memory, which they can recall with ~perfect fidelity. So we get In-Context Learning, e.g. using induction heads to solve the Indirect Object Identification task, or computing Linear Regression.&lt;/p&gt;&lt;h4 id="retrieval"&gt;Retrieval&lt;/h4&gt;&lt;p&gt;Note that Transformers don’t filter their context at all until recall time. So if we have a bunch of information we think &lt;em&gt;might&lt;/em&gt; be useful to the Transformer, we filter it &lt;em&gt;outside&lt;/em&gt; the Transformer (using Information Retrieval strategies) and then stuff the results into the prompt. This process is known as Retrieval Augmented Generation (RAG). RAG determines relevant information for the context window of a transformer. A human with the internet is kinda like a RAG system - you still have to know what to search but whatever you retrieve is as salient as short-term memory to you.&lt;/p&gt;&lt;h4 id="information-flow-for-mamba"&gt;Information Flow for Mamba&lt;/h4&gt;&lt;p&gt;Training Data acts similarly for Mamba. However, the lines are slightly blurred for in-context data and retrieval. In-context data for Mamba &lt;em&gt;is&lt;/em&gt; compressed/filtered similar to retrieval data for transformers. This in-context data is also accessible for look-up like for transformers (although with somewhat lower fidelity).&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="236" src="https://lh7-us.googleusercontent.com/0dxiIk5NUI9g_P7G5lr5CSziEVKABYdtIW-R4Rxi6OHwWV_vLYVb1wtetVmzNtRWcLngldL4A8WUQA2jhIQj-IJmpaYr97xt-2Du_dxVOe5ppA4EcRNxEbjQvmjbND_DhyKhO6nsnS4nf1NxvRLwx-o" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;Transformer context is to Mamba states what short-term is to long-term memory. Mamba doesn’t just have “RAM”, it has a hard drive&lt;sup&gt;21&lt;/sup&gt; &lt;sup&gt;22&lt;/sup&gt;.&lt;/p&gt;&lt;h3 id="swapping-states-as-a-new-prompting-paradigm"&gt;Swapping States as a New Prompting Paradigm&lt;/h3&gt;&lt;p&gt;Currently, we often use RAG to give a transformer contextual information.&lt;/p&gt;&lt;p&gt;With Mamba-like models, you could instead imagine having a library of states created by running the model over specialised data. States could be shared kinda like LoRAs for image models.&lt;/p&gt;&lt;p&gt;For example, I could do inference on 20 physics textbooks and, say, 100 physics questions and answers. Then I have a state which I can give to you. Now you don’t need to add any few-shot examples; you just simply ask your question. &lt;strong&gt;The in-context learning is in the state&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;In other words, you can drag and drop downloaded states into your model, like literal plug-in cartridges. And note that “training” a state doesn’t require any backprop. It’s more like a highly specialised one-pass fixed-size compression algorithm. This is unlimited in-context learning applied at inference time for zero-compute or latency&lt;sup&gt;23&lt;/sup&gt;.&lt;/p&gt;&lt;p&gt;The structure of an effective LLM call goes from…&lt;/p&gt;&lt;ol&gt;&lt;li&gt;System Prompt&lt;/li&gt;&lt;li&gt;Preamble&lt;/li&gt;&lt;li&gt;Few shot-examples&lt;/li&gt;&lt;li&gt;Question&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;…for Transformers, to simply…&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Inputted state (with problem context, initial instructions, textbooks, and few-shot examples)&lt;/li&gt;&lt;li&gt;Short question&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;…for Mamba.&lt;/p&gt;&lt;p&gt;This is cheaper and faster than few-shot prompting (as the state is infinitely reusable without inference cost). It’s also MUCH cheaper than finetuning and doesn’t require any gradient updates. We could imagine retrieving states in addition to context.&lt;/p&gt;&lt;h2 id="mamba-mechanistic-interpretability"&gt;Mamba &amp;amp; Mechanistic Interpretability&lt;/h2&gt;&lt;p&gt;Transformer interpretability typically involves:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;understanding token relationships via attention,&lt;/li&gt;&lt;li&gt;understanding circuits, and&lt;/li&gt;&lt;li&gt;using Dictionary Learning for unfolding MLPs.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Most of the ablations that we would like to do for Mamba are still valid, but understanding token communication (1) is now more nuanced. All information moves between tokens via hidden states instead of the Attention Mechanism which can “teleport” information from one sequence position to another.&lt;/p&gt;&lt;p&gt;For understanding in-context learning (ICL) tasks with Mamba, we will look to intervene on the SSM state. A classic task in-context learning task is Indirect Object Identification in which a model has to finish a paragraph like:&lt;/p&gt;&lt;p&gt;&lt;em&gt;Then, Shelby and Emma had a lot of fun at the school. [Shelby/Emma] gave an apple to [BLANK]&lt;/em&gt;&lt;/p&gt;&lt;p&gt;The model is expected to fill in the blank with the name that is not repeated in the paragraph. In the chart below we can see that information is passed from the [Shelby/Emma] position to the final position via the hidden state (see the two blue lines in the top chart).&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="256" src="https://lh7-us.googleusercontent.com/ZDBpRS1yEscEZcsJtevlPaM5URUP58dgJ2csAIcWP-hmQcje8kBi-u4zAWYnbeE26YXWemOh32pdHM2TgaSanGePOVgRiss8svxP17nLPBvg1YjLE4W1uIGkTmDI9PbZO42u_4KfYoSeaRnZz_W4HfY" width="602" /&gt;&lt;/figure&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="256" src="https://lh7-us.googleusercontent.com/j8aQ6XIxedX6Zut0rz7CE_e02KgBjyJvg7QQ7U9FkM2TjSWWSNk1v7gFVeGSsETqwQGvF8flh0lIUmSLIVqW9rwHC69rImw5MPj0vA0Y4XihacOzZnhUeKMZpf3bWtJTM_TB67EDYKIyfp2DeX4pNFU" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;Since it’s hypothesised that much of In-Context Learning in Transformers is downstream of more primitive sequence position operations (like Induction Heads), Mamba being able to complete this task suggests a more general In-Context Learning ability.&lt;/p&gt;&lt;h2 id="what%E2%80%99s-next-for-mamba-ssms"&gt;What’s Next for Mamba &amp;amp; SSMs?&lt;/h2&gt;&lt;p&gt;Mamba-like models are likely to excel in scenarios requiring extremely long context and long-term memory. Examples include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Processing DNA&lt;/li&gt;&lt;li&gt;Generating (or reasoning over) video&lt;/li&gt;&lt;li&gt;Writing novels&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;An illustrative example is agents with long-term goals.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Suppose you have an agent interacting with the world. Eventually, its experiences become too much for the context window of a transformer. The agent then has to compress or summarise its experiences into some more compact representation.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;But how do you decide what information is the most useful as a summary? If the task is language, LLMs are actually fairly good at summaries - okay, yeah, you’ll lose some information, but the most important stuff can be retained.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;However, for other disciplines, it might not be clear how to summarise. For example, what’s the best way to summarise a 2 hour movie?&lt;sup&gt;24&lt;/sup&gt;. Could the model itself learn to do this naturally rather than a hacky workaround like trying to describe the aesthetics of the movie in text?&lt;/em&gt;&lt;/p&gt;&lt;p&gt;This is what Mamba allows. Actual long-term memory. A real state where the model learns to keep what’s important. Prediction is compression - learning what’s useful to predict what’s coming next inevitably leads to building a useful compression of the previous tokens.&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;The implications for Assistants are clear:&lt;/p&gt;&lt;p&gt;Your chatbot co-evolves with you. It remembers.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="339" src="https://lh7-us.googleusercontent.com/agZClF-xa6q13BlEbfZLFKP3DM0hJiRy9kC0MRFoNPi8kdWCh8_BUa5oLC0V_6jTmcNQQfmMr7GGa6gwIe3CEGVeK79AFMhE1gMnbdhEoQ8iFCRuO7Yc6Xi2M3kaVIGZ4LTfDKqITQ6ap1DylOqbWs4" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;The film HER is looking better and better as time goes on 😳&lt;/p&gt;&lt;h3 id="agents-ai-safety"&gt;Agents &amp;amp; AI Safety&lt;/h3&gt;&lt;p&gt;One reason for positive updates in existential risk from AGI is Language Models. Previously, Deep-RL agents trained via self-play looked set to be the first AGIs. Language models are inherently much safer since they aren’t trained with long-term goals&lt;sup&gt;25&lt;/sup&gt;.&lt;/p&gt;&lt;p&gt;The potential for long-term sequence reasoning here brings back the importance of agent-based AI safety. Few agent worries are relevant to Transformers with an 8k context window. Many are relevant to systems with impressive long-term memories and possible instrumental goals.&lt;/p&gt;&lt;h3 id="the-best-collab-since-taco-bell-kfc-%F0%9F%A4%96-x-%F0%9F%90%8D"&gt;The Best Collab Since Taco Bell &amp;amp; KFC: 🤖 x 🐍&lt;/h3&gt;&lt;p&gt;The Mamba authors show that there’s value in combining Mamba’s long context with the Transformer’s high fidelity over short sequences. For example, if you’re making long videos, you likely can’t fit a whole movie into a Transformer’s context for attention&lt;sup&gt;26&lt;/sup&gt;. You could imagine having Attention look at the most recent frames for short-term fluidity and an SSM for long-term narrative consistency&lt;sup&gt;27&lt;/sup&gt;.&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;This isn’t the end for Transformers. Their high effectiveness is exactly what’s needed for many tasks. But now Transformers aren’t the only option. Other architectures are genuinely feasible.&lt;/p&gt;&lt;p&gt;So we’re not in the post-Transformer era. But for the first time, we’re living in the post-only-Transformers era&lt;sup&gt;28&lt;/sup&gt;. And this blows the possibilities wide open for sequence modelling with extreme context lengths and native long-term memory.&lt;/p&gt;&lt;p&gt;Two ML researchers, Sasha Rush (HuggingFace, Annotated Transformer, Cornell Professor) and Jonathan Frankle (Lottery Ticket Hypothesis, MosaicML, Harvard Professor), currently have a bet here.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="324" src="https://lh7-us.googleusercontent.com/-_S7CaQ4OxepapriZhhAs25xq-H_dSnavPxXkm0_lMMZjtno4kgWfjS1PAcLhYpbMz6BNNYd-RoxBA_Fy45CemDdvofbP7oPVQ3ygHBQNQ8pMVf7l5YnLSCgE3L1J9muCpoFmTSz09zcX9xEigRrKnc" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;Currently Transformers are far and away in the lead. With 3 years left, there’s now a research direction with a fighting chance.&lt;/p&gt;&lt;p&gt;All that remains to ask is: &lt;strong&gt;Is Attention All We Need?&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;&lt;hr /&gt;&lt;!--kg-card-begin: html--&gt;&lt;p&gt;
    1. see Figure 8 in the Mamba paper.
    &lt;br /&gt;2. And scaling up with massive compute.
    &lt;br /&gt;3. More specifically the scaled dot-product Attention popularised by Transformers
    &lt;br /&gt;4. For people who don’t see Temple Run as the cultural cornerstone it is 🤣 Temple Run was an iPhone game from 2011 similar to Subway Surfer
    &lt;br /&gt;5. Here we assume the environment is sufficiently smooth.
    &lt;br /&gt;6. One pretty important constraint for this to be efficient is that we don’t allow the individual elements of the state vector to interact with each other directly. We’ll use a combination of the state dimensions to determine the output but we don’t e.g. allow the velocity of the runner and the direction of the closest obstacle (or whatever else was in our state) to directly interact. This helps with efficient computation and we achieve this practically by constraining A to be a diagonal matrix.
    &lt;br /&gt;7. Concretely consider the case of Language Models - each token is a discrete step 
    &lt;br /&gt;8. ZOH also has nice properties for the initialisations - we want A_bar to be close to the identity so that the state can be mostly maintained from timestep to timestep if desired. ZOH gives A_bar as an exponential so any diagonal element initialisations close to zero give values close to 1 
    &lt;br /&gt;9. This is known as the Euler discretisation in the literature
    &lt;br /&gt;10. It’s wild to note that some readers might not have, we’re so far into the age of Attention that RNNs have been forgotten!
    &lt;br /&gt;11. B is like the Query (Q) matrix for Transformers.
    &lt;br /&gt;12. C is like the Output (O) matrix for Transformers. 
    &lt;br /&gt;13. Non-alcoholic options also available! 
    &lt;br /&gt;14. Especially as all voices roughly occupy the same space on the audio frequency spectrum Intuitively this seems really hard! 
    &lt;br /&gt;15. Note that photographic memory doesn’t necessarily imply perfect inferences from that memory! 
    &lt;br /&gt;16. To be clear, if you have a short sequence, then a transformer should theoretically be a better approach. If you can store the whole context, then why not!? If you have enough memory for a high-resolution image, why compress it into a JPEG? But Mamba-style architectures are likely to hugely outperform with long-range sequences. 
    &lt;br /&gt;17. More details are available for engineers interested in CUDA programming - Tri’s talk, Mamba paper section 3.3.2, and the official CUDA code are good resources for understanding the Hardware-Aware Scan 
    &lt;br /&gt;18. or in Object Oriented Programming 
    &lt;br /&gt;19. Implications to actual Political Economy are left to the reader but maybe Gu and Dao accidentally solved politics!? 
    &lt;br /&gt;20. This isn’t a perfect analogy as human evolution follows a genetic algorithm rather than SGD. 
    &lt;br /&gt;21. Albeit a pretty weird hard drive at that - it morphs over time rather than being a fixed representation.  
    &lt;br /&gt;22. As a backronym, I’ve started calling the hidden_state the state space dimension (or selective state dimension) which shortens to SSD, a nice reminder for what this object represents - the long-term memory of the system.
    &lt;br /&gt;23. I’m thinking about this similarly to the relationship between harmlessness finetuning and activation steering. State swapping, like activation steering, is an inference time intervention giving comparable results to its train time analogue. 
    &lt;br /&gt;24. This is a very non-trivial problem! How do human brains represent a movie internally? It’s not a series of the most salient frames, nor is it a text summary of the colours, nor is it a purely vibes-based summary if you can memorise some lines of the film. 
    &lt;br /&gt;25. They’re also safer since they inherently understand (though don’t necessarily embody) human values. It’s not all clear that how to teach an RL agent human morality. 
    &lt;br /&gt;26. Note that typically an image (i.e. a single frame) counts as &amp;gt;196 tokens, and movies are typically 24 fps so you’ll fill a 32k context window in 7 seconds 🤯 
    &lt;br /&gt;27. Another possibility that I’m excited about is applying optimisation pressure to the state itself as well as the output to have models that respect particular use cases. 
    &lt;br /&gt;28. This is slightly hyperbolic, the TS-Mixer for time series, Gradient Boosting Trees for tabular data and Graph Neural Networks for weather prediction exist and are currently used, but these aren’t at the core of AI
&lt;/p&gt;&lt;!--kg-card-end: html--&gt;&lt;h2 id="author-bio"&gt;Author Bio&lt;/h2&gt;&lt;p&gt;Kola Ayonrinde is a Research Scientist and Machine Learning Engineer with a flair for writing. He integrates technology and creativity, focusing on applying machine learning in innovative ways and exploring the societal impacts of tech advancements.&lt;/p&gt;&lt;h2 id="acknowledgements"&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;This post was originally posted on Kola's personal blog.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thanks to Gonçalo for reading an early draft, Jaden for the nnsight library used for the Interpretability analysis and Tessa for Mamba patching visualisations.Also see: &lt;/em&gt;&lt;em&gt;Mamba paper&lt;/em&gt;&lt;em&gt;, Mamba Python code, &lt;/em&gt;&lt;em&gt;Annotated S4&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;Nathan Labenz podcast&lt;/em&gt;&lt;/p&gt;&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Kola Ayonrinde, "Mamba Explained," The Gradient, 2024&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;@article{Ayonrinde2024mamba,
    author = {Kola Ayonrinde},
    title = {Mamba Explained},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/mamba-explained},
}&lt;/code&gt;&lt;/pre&gt;]]&amp;gt;&amp;lt;![CDATA[Car-GPT: Could LLMs finally make self-driving cars happen?]]&amp;gt;https://thegradient.pub/car-gpt/65db7b4193571d5c8c154a73Fri, 08 Mar 2024 16:55:18 GMT&lt;p&gt;In 1928, London was in the middle of a terrible health crisis, devastated by bacterial diseases like pneumonia, tuberculosis, and meningitis. Confined in sterile laboratories, scientists and doctors were stuck in a relentless cycle of trial and error, using traditional medical approaches to solve complex problems.&lt;/p&gt;&lt;p&gt;This is when, in September 1928, an accidental event changed the course of the world.&lt;strong&gt; &lt;/strong&gt;A Scottish doctor named Alexander Fleming forgot to close a petri dish (the transparent circular box you used in science class), which got contaminated by mold. This is when Fleming noticed something peculiar: all bacteria close to the moisture were dead, while the others survived.&lt;/p&gt;&lt;p&gt;"What was that moisture made of?" wondered M. Flemming.&lt;strong&gt; &lt;/strong&gt;This was when he discovered that Penicillin, the main component of the mold, was a powerful bacterial killer. This led to the groundbreaking discovery of penicillin, leading to the antibiotics we use today. In a world where doctors were relying on existing well-studied approaches, Penicillin was the unexpected answer.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Self-driving cars may be following a similar event.&lt;/strong&gt; Back in the 2010s, most of them were built using what we call a « modular » approach. The software « autonomous » part is split into several modules, such as Perception (the task of seeing the world), or Localization (the task of accurately localize yourself in the world), or Planning (the task of creating a trajectory for the car to follow, and implementing the « brain » of the car). Finally, all these go to the last module: Control, that generates commands such as « steer 20° right », etc… So this was the well-known approach. &lt;/p&gt;&lt;p&gt;But a decade later, companies started to take another discipline very seriously: &lt;strong&gt;End-To-End learning&lt;/strong&gt;. The core idea is to replace every module with a single neural network predicting steering and acceleration, but as you can imagine, this introduces a black box problem.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="161" src="https://lh7-us.googleusercontent.com/EpMJPDK-TBu9ZhN25UrxVbAk-9rJjEvtitzjvPpzjhTBPdkk-judKQtfWQNf7vtNrG1sfsvkUhpbtMGplWN5bbnx5ULbfNj6vpRf8RVlt5eDn8MN99FObGbPsmokdNlCGZ1NWq-uw32QVitv4NZC3zI" width="624" /&gt;&lt;figcaption&gt;The 4 Pillars of Self-Driving Cars are Perception, Localization, Planning, and Control. Could a Large Language Model replicate them? (source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;These approaches are known, but don’t solve the self-driving problem yet. So, we could be wondering:&lt;strong&gt; "What if LLMs (Large Language Models), currently revolutionizing the world, were the unexpected answer to autonomous driving?"&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;This is what we're going to see in this article, beginning with a simple explanation of what LLMs are and then diving into how they could benefit autonomous driving.&lt;/p&gt;&lt;h2 id="preamble-llms-what"&gt;&lt;strong&gt;Preamble: LLMs-what?&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Before you read this article, you must know something: I'm not an LLM pro, at all. This means, I know too well the struggle to learn it. I understand what it's like to google "learn LLM"; then see 3 sponsored posts asking you to download e-books (in which nothing concrete appears)... then see 20 ultimate roadmaps and GitHub repos, where step 1/54 is to view a 2-hour long video (and no one knows what step 54 is because it's so looooooooong).&lt;/p&gt;&lt;p&gt;So, instead of putting you through this pain myself, let's just break down what LLMs are in 3 key ideas:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Tokenization&lt;/li&gt;&lt;li&gt;Transformers&lt;/li&gt;&lt;li&gt;Processing Language&lt;/li&gt;&lt;/ol&gt;&lt;h3 id="tokenization"&gt;&lt;strong&gt;Tokenization&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;In ChatGPT, you input a piece of text, and it returns text, right? Well, what's actually happening is that your text is first converted into tokens.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="303" src="https://lh7-us.googleusercontent.com/_rT6_ShRUbi-bZpaKL7JF-BhE_rfDg_V8De5nYj0O5tGgAtLTyYhnGleIy7nBJ3vyrUsfge6cdReCctzsfCyW_XP6WUm21pU350RpOoxWzb2SYRvMcKMIZAOE6wdFou7t_ERJ2_Jht6uUhfg_sBgcbI" width="600" /&gt;&lt;figcaption&gt;Example of tokenization of a sentence, each word becomes a "token"&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;But what's a token? You might ask. Well, a token can correspond to a word, a character, or anything we want. Think about it -- if you are to send a sentence to a neural network, you didn't plan on sending actual words, did you?&lt;/p&gt;&lt;p&gt;The input of a neural network is always a number, so you need to convert your text into numbers; this is tokenization.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="139" src="https://lh7-us.googleusercontent.com/pZ3qf5HQNrqXqb5bbGkLgWQPvu04-2b_ejpv4m3i5C9VfcPg3yZm7cmaD6lq4xgrA4DhUBJpCa-HB4i7iAPo8-Hyrde9sLiBYBiY2d7c9O17ePJtCqAb15dvcDEGxofEwneP6Nx2_oSiT26m4cLvcMc" width="624" /&gt;&lt;figcaption&gt;What tokenization actually is: A conversion from words to numbers&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Depending on the model (ChatGPT, LLAMA, etc...), a token can mean different things: a word, a subword, or even a character. We could take the English vocabulary and define these as words or take parts of words (subwords) and handle even more complex inputs. For example, the word « a » could be token 1, and the word « abracadabra » would be token 121.&lt;/p&gt;&lt;h3 id="transformers"&gt;&lt;strong&gt;Transformers&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;Now that we understand how to convert a sentence into a series of numbers, we can send that series into our neural network! At a high level, we have the following structure:&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="407" src="https://lh7-us.googleusercontent.com/J1CkM3ItKevopmi-0gbSHWJnMStL4dZWksllG15OlaDI4PFgk-FtFeQ7O0CnP1dKx9ZHV7PUAlmBK9lFwJQrHnJj1JAXAMHdbZH13hd07dYL55ZCsxQChf06dYj_JoXEvNeAqdfmj2IcdwD8sP5OZtI" width="624" /&gt;&lt;figcaption&gt;A Transformer is an Encoder-Decoder Architecture that takes a sequence of tokens as input and outputs a another series of tokens&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;If you start looking around, you will see that some models are based on an encoder-decoder architecture, some others are purely encoder-based, and others, like GPT, are purely decoder-based.&lt;/p&gt;&lt;p&gt;Whatever the case, they all share the core Transformer blocks: multi-head attention, layer normalization, addition and concatenation, blocks, cross-attention, etc...&lt;/p&gt;&lt;p&gt;&lt;strong&gt;This is just a series of attention blocks getting you to the output&lt;/strong&gt;. So how does this word prediction work?&lt;/p&gt;&lt;h3 id="the-output-next-word-prediction"&gt;&lt;strong&gt;The output/ Next-Word Prediction&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;The Encoder learns features and understands context... But what does the decoder do? In the case of object detection, the decoder is predicting bounding boxes. In the case of segmentation, the decoder is predicting segmentation masks. What about here?&lt;/p&gt;&lt;p&gt;In our case, the decoder is trying to generate a series of words; we call this task "next-word prediction".&lt;/p&gt;&lt;p&gt;Of course, it does it similarly by predicting numbers or tokens. This characterizes our full model as shown below,&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="519" src="https://lh7-us.googleusercontent.com/YS9WFjjuYTq7QkzPnx4xgTQnU0Pmr22i4fEzXXWuBf6wD--eYL8FvdoEpkqlCMKraBaSDuo7j0sWR7ltUaWI31_Bvq9PtJoPpoWRFQnjKOth1P7mnxfzmGT8ppUslOPMhbOzJY49F4IHBMZfyzax18E" width="624" /&gt;&lt;figcaption&gt;I would say the loss function for this particular output produces a near-0 value.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now, there are many "concepts" that you should learn on top of this intro: everything Transformer and Attention related, but also few-shot learning, pretraining, finetuning, and more...&lt;/p&gt;&lt;p&gt;Ok... but what does it have to do with self-driving cars? I think it's time to move to stage 2.&lt;/p&gt;&lt;h2 id="chat-gpt-for-self-driving-cars"&gt;&lt;strong&gt;Chat-GPT for Self-Driving Cars&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;The thing is, you've already been through the tough part. The rest simply is: "How do I adapt this to autonomous driving?". Think about it; we have a few modifications to make:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Our input now becomes either images, sensor data&lt;/strong&gt; (LiDAR point clouds, RADAR point clouds, etc...), or even algorithm data (lane lines, objects, etc...). All of it is "tokenizable", as Vision Transformers or Video Vision Transformers do.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Our Transformer model pretty much remains the same&lt;/strong&gt; since it only operates on tokens and is independent of the kind of input.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;The output is based on the set of tasks we want to do.&lt;/strong&gt; It could be explaining what's happening in the image or could &amp;nbsp;also be a direct driving task like switching lanes.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;So, let's begin with the end:&lt;/p&gt;&lt;h3 id="what-self-driving-car-tasks-could-llm-solve"&gt;&lt;strong&gt;What self-driving car tasks could LLM solve?&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;There are many tasks involved in autonomous driving, but not all of them are GPT-isable. The most active research areas in 2023 have been:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Perception&lt;/strong&gt;: Based on an input image, describe the environment, number of objects, etc...&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Planning&lt;/strong&gt;: Based on an image, or a bird-eye view, or the output of perception, describe what we should do (keep driving, yield, etc...)&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Generation&lt;/strong&gt;: Generate training data, alternate scenarios, and more... using "diffusion"&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Question &amp;amp; Answers&lt;/strong&gt;: Create a chat interface and ask the LLM to answer questions based on the scenario.&lt;/li&gt;&lt;/ul&gt;&lt;h3 id="llms-in-perception"&gt;&lt;strong&gt;LLMs in Perception&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;In Perception, the input is a series of images, and the output is usually a set of objects, lanes, etc... In the case of LLMs, we have 3 core tasks: &lt;strong&gt;Detection&lt;/strong&gt;, &lt;strong&gt;Prediction&lt;/strong&gt;, and &lt;strong&gt;Tracking&lt;/strong&gt;. An example with Chat-GPT, when you send it an image and ask to describe what's going on is shown below:&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="308" src="https://lh7-us.googleusercontent.com/unUisu66NolUzzipNfKObr8kE6n8PRcTMy86cYYIG1aIPLkYKZd34zmzGrkM4yS6lKNoXpvORHwnfORfOsy8aRNUx9AwEDN_qQN4tiuutBRh8l3h_vVpfVzOJ7UdQ-CuWKI5EJsze9le6qRA7VQ1QoY" width="425" /&gt;&lt;figcaption&gt;A GPT-4 Vision model can return the objects in the image, just like object detectors do (source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Other models such as HiLM-D and MTD-GPT can also do this, some work also for videos. Models like PromptTrack, also have the ability to assign unique IDs (this car in front of me is ID #3), similar to a 4D Perception model.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="267" src="https://lh7-us.googleusercontent.com/WcjhR7diFbZrVeKdiVyQbC_HtYJVGUQsOBka0zikaD2JZpfmNxcyEJlpzxZfvobWrMu6srxUEGPcxpdVSywVKW-0gIuOISCqLCVfjaA6Q7KaNb1etKfNybXkya4yFyx7AY0Y2_ZZw_cY_gWSccO0B2Q" width="624" /&gt;&lt;figcaption&gt;PromptTrack combines the DETR object detector with Large Language Models&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In this model, multi-view images are sent to an Encoder-Decoder network that is trained to predict annotations of objects such as bounding boxes, and attention maps). These maps are then combined with a prompt like 'find the vehicles that are turning right'.The next block then finds the 3D Bounding Box localization and assigns IDs using a bipartite graph matching algorithm like the Hungarian Algorithm.&lt;/p&gt;&lt;p&gt;This is cool, but this isn't the "best" application of LLMs so far:&lt;/p&gt;&lt;h3 id="llms-in-decision-making-navigation-and-planning"&gt;&lt;strong&gt;LLMs in Decision Making, Navigation, and Planning&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;If Chat-GPT can find objects in an image, it should be able to tell you what to do with these objects, shouldn't it? Well, this is the task of Planning i.e. defining a path from A to B, based on the current perception. While there are numerous models developed for this task, the one that stood out to me was Talk2BEV:&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="179" src="https://lh7-us.googleusercontent.com/N3ZvMnLMjQ6jwL4FNvvTyM4U6KFrri0jV-0yOYVH9lAAtRH7MD8aMX_LHhjeBFKxGwTdrATJoNUQe-sUqEB3utLnpreCT4e4TIO3qX3LTrzBKwZ7kPAfzxAu6osJ35tYpapCiTTWDtx0tUOHXcNqu04" width="600" /&gt;&lt;figcaption&gt;Talk2BEV takes perception one step further and also tells you what to do&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The main difference between models for planning and Perception-only models is that here, we're going to train the model on human behavior to suggest ideal driving decisions. We're also going to change the input from multi-view to Bird Eye View since it is much easier to understand.&lt;/p&gt;&lt;p&gt;This model works both with LLaVA and ChatGPT4, and here is a demo of the architecture:&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="255" src="https://lh7-us.googleusercontent.com/-bl_IDT2SqF75q3d20EORJcH22oXWMjFmkLFn0ZKbVV5oshlr5BkZEnscfUSg_-pkzMDJ3Jo38mdu6whUmIDWq7pXfxXxdwgc3Kj-WUwv5LNWUHIvH3r6mfpKP9s5PD7NoA7e0R3pBoic6ijwfD57aU" width="600" /&gt;&lt;figcaption&gt;Talk2BEV (source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;As you can see, this isn't purely "prompt" based, because the core object detection model stays Bird Eye View Perception, but the LLM is used to "enhance" that output by suggesting to crop some regions, look at specific places, and predict a path. We're talking about "language enhanced BEV Maps".&lt;/p&gt;&lt;p&gt;Other models like DriveGPT are trained to send the output of Perception to Chat-GPT and finetune it to output the driving trajectory directly.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="327" src="https://lh7-us.googleusercontent.com/QbuhKKLWr0jA1DWdSWBIk6UtHnecHTITRBPidM1fjYn9VSC-56VcaxStqJbn5iTLslLN6ppQgnmfKZO-43TNCZADfuwdV-RChnMrzryLIKx7UvtySKEs0unIEum4c2ous07M3-WlUoTVeGT1s0nPz0U" width="624" /&gt;&lt;figcaption&gt;The DriveGPT model is pure madness... when trained correctly! (modified from source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;I could go on and on, but I think you get the point. If we summarize, I would say that:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Inputs are either tokenized images or outputs of Perception algorithm (BEV maps, ...)&lt;/li&gt;&lt;li&gt;We fuse existing models (BEV Perception, Bipartite Matching, ...) with language prompts (find the moving cars)&lt;/li&gt;&lt;li&gt;Changing the task is mainly about changing the data, loss function, and careful finetuning.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The Q&amp;amp;A applications are very similar, so let's see the last application of LLMs:&lt;/p&gt;&lt;h3 id="llms-for-image-generation"&gt;&lt;strong&gt;LLMs for Image Generation&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;Ever tried Midjourney and DALL-E? Isn’t it super cool? Yes, and there is MUCH COOLER than this when it comes to autonomous driving. In fact, have you heard of Wayve's GAIA-1 model? The model takes text and images as input and directly produces videos, like this:&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="277" src="https://lh7-us.googleusercontent.com/R9xqQVFRcUlZrjXqqeY6qon7hxAezFKY3mI_ZdPh2R0eJGPQb2CjV0TFxjwblDEWxJz7va0N6KerXMRO_ltSFJkxiQRxmW7I_I_b13bD-PidrUD8sQ0REInSAUJuKGqazFFDCpwOQAVun5LREW41Q_w" width="293" /&gt;&lt;figcaption&gt;These videos are generated by Wayve's GAIA-1 model&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The architecture takes images, actions, and text prompts as input, and then uses a World Model (an understanding of the world and its interactions) to produce a video.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="260" src="https://lh7-us.googleusercontent.com/ougFIHYengzs40lyruVerlDFFa18VXH0K093yjHA93q8nTjTmLQAdfKCPl7sBAbZfpoxqY3tDdzufOqJoxmLUdL6W862_aPebPxABsPwjyaFZGWOCP2VTpaqcob0gkSJDRv9IqSm7-aHoXtG-FXWJBo" width="624" /&gt;&lt;figcaption&gt;Architecture of GAIA-1 (source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You can find more examples on Wayve's YouTube channel and this dedicated post.&lt;/p&gt;&lt;p&gt;Similarly, you can see MagicDrive, which takes the output of Perception as input and uses that to generate scenes:&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="187" src="https://lh7-us.googleusercontent.com/vVKUXuJno-UQWj2ZTWEA1JBMzZ6xnajJOrzMPtMW4qFjhvKqT7F2XiOoe9M1PCtM44S4CfrXqTVyVfKOisaB3iy-wa5vuCS7SFYaQdv6dzNfbzVcG2XXQzAAUqZXUeGxALkJ9fHuE8XFA4KvkKmZLN4" width="624" /&gt;&lt;figcaption&gt;(source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Other models, like Driving Into the Future and Driving Diffusion can directly generate future scenarios based on the current ones. You get the point; we can generate scenes in an infinite way, get more data for our models, and have this endless positive loop.&lt;/p&gt;&lt;p&gt;We've just seen 3 prominent families of LLM usage in self-driving cars: Perception, Planning, and Generation. The real question is...&lt;/p&gt;&lt;h2 id="could-we-trust-llms-in-self-driving-cars"&gt;&lt;strong&gt;Could we trust LLMs in self-driving cars?&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;And by this, I mean... What if your model has hallucinations?&lt;/strong&gt; What if its replies are completely absurd, like ChatGPT sometimes does? I remember, back in my first days in autonomous driving, big groups were already skeptical about Deep Learning, because it wasn't "deterministic" (as they call it).&lt;/p&gt;&lt;p&gt;We don't like Black Boxes, which is one of the main reasons End-To-End will struggle to get adopted. Is ChatGPT any better? I don't think so, and I would even say it's worse in many ways. However, LLMs are becoming more and more transparent, and the black box problem could eventually be solved.&lt;/p&gt;&lt;p&gt;To answer the question "Can we trust them?"... it's very early in the research, and I'm not sure someone has really used them "online" — meaning « live », in a car, on the streets, rather than in a headquarter just for training or image generation purpose. &amp;nbsp;I would definitely picture a Grok model on a Tesla someday just for Q&amp;amp;A purposes. So for now, I will give you my coward and safe answer...&lt;/p&gt;&lt;p&gt;&lt;strong&gt;It's too early to tell!&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Because it really is. The first wave of papers mentioning LLMs in Self-Driving Cars is from mid-2023, so let's give it some time. In the meantime, you could start with this survey that shows all the evolutions to date.&lt;/p&gt;&lt;p&gt;Alright, time for the best part of the article...&lt;/p&gt;&lt;h2 id="the-llms-4-ad-summary"&gt;&lt;strong&gt;The LLMs 4 AD Summary&lt;/strong&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;A Large Language Model (LLM) works in 3 key steps: inputs, transformer, output.&lt;/strong&gt; The input is a set of tokenized words, the transformer is a classical transformer, and the output task is "next word prediction".&lt;/li&gt;&lt;li&gt;&lt;strong&gt;In a self-driving car, there are 3 key tasks we can solve with LLMs:&lt;/strong&gt; &lt;strong&gt;Perception&lt;/strong&gt; (detection, tracking, prediction), &lt;strong&gt;Planning&lt;/strong&gt; (decision making, trajectory generation), and &lt;strong&gt;Generation&lt;/strong&gt; (scene, videos, training data, ...).&lt;/li&gt;&lt;li&gt;&lt;strong&gt;In Perception, the main goal is to describe the scene we're looking at.&lt;/strong&gt; The input is a set of raw multi-view images, and the Transformer aims to predict 3D bounding boxes. LLMs can also be used to ask for a specific query ("where are the taxis?").&lt;/li&gt;&lt;li&gt;&lt;strong&gt;In Planning, the main goal is to generate a trajectory for the car to take&lt;/strong&gt;. The input is a set of objects (output of Perception, BEV Maps, ...), and the Transformer uses LLMs to understand context and reason about what to do.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;In Generation, the main goal is to generate a video that corresponds to the prompt used&lt;/strong&gt;. Models like GAIA-1 have a chat interface, and take as input videos to generate either alternate scenes (rainy, ...), or future scenes.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;For now, it's very early to tell whether this can be used in the long run&lt;/strong&gt;, but research there is some of the most active in the self-driving car space. It all comes back to the question: "Can we really trust LLMs in general?"&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="next-steps"&gt;&lt;strong&gt;Next Steps&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;If you want to get started on LLMs for self-driving cars, there are several things you can do:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;⚠️ Before this, the most important&lt;/strong&gt;: If you want to keep learning about self-driving cars. I'm talking about self-driving car every day through my private emails. I'm sending many tips and direct content. You should join here.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;✅ To begin, build an understanding of LLMs for self-driving cars&lt;/strong&gt;. This is partly done, you can continue to explore the resources I provided in the article.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;➡️ Second, build skills related to Auto-Encoders and Transformer Networks&lt;/strong&gt;. My image segmentation series is perfect for this, and will help you understand Transformer Networks with no NLP example, which means it's for Computer Vision Engineer's brains.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;️ ➡️ Then, understand how Bird Eye View Networks works.&lt;/strong&gt; It might not be mentioned in general LLM courses, but in self-driving cars, Bird Eye View is the central format where we can fuse all the data (LiDARs, cameras, multi-views, ...), build maps, and directly create paths to drive. You can do so in my Bird Eye View course (if closed, join my email list to be notified).&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Finally, practice training, finetuning, and running LLMs in self-driving car scenarios&lt;/strong&gt;. Run repos like Talk2BEV and the others I mentioned in the article. Most of them are open source, but the data can be hard to find. This is noted last, but there isn't really an order in all of this.&lt;/li&gt;&lt;/ul&gt;&lt;hr /&gt;&lt;h2 id="author-bio"&gt;&lt;strong&gt;Author Bio&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Jérémy Cohen is a self-driving car engineer and founder of Think Autonomous, a platform to help engineers learn about cutting-edge technologies such as self-driving cars and advanced Computer Vision. In 2022, Think Autonomous won the price for Top Global Business of the Year in the Educational Technology Category​ and Jeremy Cohen was named 2023 40 Under 40 Innovators in Analytics Insight magazine, the largest printed magazine on Artificial Intelligence. &lt;em&gt;You can join 10,000 engineers reading his private daily emails on self-driving cars &lt;/em&gt;&lt;em&gt;here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Jérémy Cohen, "Car-GPT: Could LLMs finally make self-driving cars happen?", The Gradient, 2024.&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;BibTeX citation:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;@article{cohen2024cargpt,
    author = {Jérémy Cohen},
    title = {Car-GPT: Could LLMs finally make self-driving cars happen?},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/car-gpt},
}&lt;/code&gt;&lt;/pre&gt;]]&amp;gt;&amp;lt;![CDATA[Do text embeddings perfectly encode text?]]&amp;gt;https://thegradient.pub/text-embedding-inversion/65e3d66193571d5c8c154aecTue, 05 Mar 2024 20:15:58 GMTThe rise of the vector database&lt;img alt="Do text embeddings perfectly encode text?" src="https://thegradient.pub/content/images/2024/03/main-1.png" /&gt;&lt;p&gt;As a result of the rapid advancement of generative AI in recent years, many companies are rushing to integrate AI into their businesses. One of the most common ways of doing this is to build AI systems that answer questions concerning information that can be found within a database of documents. Most solutions for such a problem are based on one key technique: Retrieval Augmented Generation (RAG). &lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Do text embeddings perfectly encode text?" class="kg-image" height="276" src="https://lh7-us.googleusercontent.com/25UaLlGmLLH-h2ljDEGD1T_eUnErWmCSsLn5gz49U6DsQebwn-kwWtkT0_GvPTswYlAyOP9iZ3Q6Gpy-K6UwuWVF4bzj-1G2cZJd6oXw4hDX92texYdByUuq7bO8qcmuKQSqfR6zkPlm9M5KoYDk12A" width="624" /&gt;&lt;figcaption&gt;Overview of a RAG system.&lt;span class="-mobiledoc-kit__atom"&gt;‌ ‌&lt;/span&gt;Source: “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks”&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This is what lots of people do now as a cheap and easy way to get started using AI: store lots of documents in a database, have the AI retrieve the most relevant documents for a given input, and then generate a response to the input that is informed by the retrieved documents.&lt;/p&gt;&lt;p&gt;These RAG systems determine document relevancy by using &amp;nbsp;“embeddings”, vector representations of documents produced by an embedding model. These embeddings are supposed to represent some notion of similarity, so documents that are relevant for search will have high vector similarity in embedding space.&lt;/p&gt;&lt;p&gt;The prevalence of RAG has led to the rise of the &lt;em&gt;vector database&lt;/em&gt;, a new type of database designed for storing and searching through large numbers of embeddings. Hundreds of millions of dollars of funding have been given out to startups that claim to facilitate RAG by making embedding search easy. And the effectiveness of RAG is the reason why lots of new applications are converting text to vectors and storing them in these vector databases.&lt;/p&gt;&lt;h3 id="embeddings-are-hard-to-read"&gt;Embeddings are hard to read&lt;/h3&gt;&lt;p&gt;So what is stored in a text embedding? Beyond the requirement of semantic similarity, there are no constraints on which embedding must be assigned for a given text input. Numbers within embedding vectors can be &lt;em&gt;anything&lt;/em&gt;, and vary based on their initialization. We can interpret the similarities of embedding with others but have no hope ever understanding the individual numbers of an embedding.&lt;br /&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Do text embeddings perfectly encode text?" class="kg-image" height="149" src="https://lh7-us.googleusercontent.com/GwnKcHZF5vTgMZlKmEobbOLiJoQLOknGoG1znqG5pT-7kWwMCOPSEK3gB-q-NnBt5ahi2FLjbaFM9x-J5DS4VKbns7de88GATWbjaR-iDeuLPWY-muNKQ6bWhqyvo4HRxXWaStkgVrhEF6B0Tdu-Ihs" width="555" /&gt;&lt;figcaption&gt;A neural embedding model (light blue) takes text input and produces an &lt;em&gt;embedding&lt;/em&gt;, a vector that can be used for search.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now imagine you’re a software engineer building a RAG system for your company. You decide to store your vectors in a vector database. You notice that in a vector database, what's stored are embedding vectors, not the text data itself. The database fills up with rows and rows of random-seeming numbers that represent text data but never ‘sees’ any text data at all. &lt;/p&gt;&lt;p&gt;You know that the text corresponds to customer documents that are protected by your company’s privacy policy. But you’re not &lt;em&gt;really&lt;/em&gt; sending text off-premises at any time; you only ever send embedding vectors, which look to you like random numbers.&lt;/p&gt;&lt;p&gt;What if someone hacks into the database and gains access to all your text embedding vectors – would this be bad? Or if the service provider wanted to sell your data to advertisers – could they? Both scenarios involve being able to take embedding vectors and &lt;em&gt;invert &lt;/em&gt;them somehow back to text.&lt;/p&gt;&lt;h3 id="from-text-to-embeddingsback-to-text"&gt;From text to embeddings...back to text&lt;/h3&gt;&lt;p&gt;The problem of recovering text from embeddings is exactly the scenario we tackle in our paper &lt;em&gt;Text Embeddings Reveal As Much as Text&lt;/em&gt; (EMNLP 2023). Are embedding vectors a secure format for information storage and communication? Put simply: can input text be recovered from output embeddings?&lt;/p&gt;&lt;p&gt;Before diving into solutions, let’s think about the problem a little bit more. Text embeddings are the output of neural networks, sequences of matrix multiplications joined by nonlinear function operations applied to input data. In traditional text processing neural networks, a string input is split into a number of token vectors, which repeatedly undergo nonlinear function operations. At the output layer of the model, tokens are averaged into a single embedding vector.&lt;/p&gt;&lt;p&gt;A maxim from the signal processing community known as the data processing inequality tells us that functions cannot add information to an input, they can only sustain or decrease the amount of information available. Even though conventional wisdom tells us that deeper layers of a neural network are constructing ever-higher-order representations, they aren’t adding any information about the world that didn’t come in on the input side. &lt;/p&gt;&lt;p&gt;Additionally, the nonlinear layers certainly destroy &lt;em&gt;some&lt;/em&gt; information. One ubiquitous nonlinear layer in modern neural networks is the “ReLU” function, which simply sets all negative inputs to zero. After applying ReLU throughout the many layers of a typical text embedding model, it is not possible to retain all the information from the input.&lt;br /&gt;&lt;/p&gt;&lt;h3 id="inversion-in-other-contexts"&gt;Inversion in other contexts&lt;/h3&gt;&lt;p&gt;Similar questions about information content have been asked in the computer vision community. Several results have shown that deep representations (embeddings, essentially) from image models can be used to recover the input images with some degree of fidelity. An early result (Dosovitskiy, 2016) showed that images can be recovered from the feature outputs of deep convolutional neural networks (CNNs). Given the high-level feature representation from a CNN, they could invert it to produce a blurry-but-similar version of the original input image.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Do text embeddings perfectly encode text?" class="kg-image" height="1865" src="https://thegradient.pub/content/images/2024/03/Group-66--3-.png" width="2000" /&gt;&lt;figcaption&gt;In computer vision, inversion models (yellow) have successfully reconstructed images given only the 1000 probability outputs of an ImageNet classifier, most of which are close to 0. (Images from &lt;em&gt;Understanding Invariance via Feedforward Inversion of Discriminatively Trained Classifiers&lt;/em&gt;.)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;People have improved on image embedding inversion process since 2016: models have been developed that do inversion with higher accuracy, and have been shown to work across more settings. Surprisingly, some work has shown that images can be inverted from the outputs of an ImageNet classifier (1000 class probabilities).&lt;/p&gt;&lt;h3 id="the-journey-to-vec2text"&gt;The journey to vec2text&lt;/h3&gt;&lt;p&gt;If inversion is possible for image representations, then why can’t it work for text? Let’s consider a toy problem of recovering text embeddings. For our toy setting we’ll restrict text inputs to 32 tokens (around 25 words, a sentence of decent length) and embed them all to vectors of 768 floating-point numbers. At 32-bit precision, these embeddings are 32 * 768 = 24,576 bits or around 3 kilobytes. &lt;/p&gt;&lt;p&gt;Few words represented by many bits. Do you think we could perfectly reconstruct the text within this scenario? &lt;/p&gt;&lt;p&gt;First things first: we need to define a measurement of &lt;em&gt;goodness&lt;/em&gt;, to know how well we have accomplished our task. One obvious metric is "exact match", how often we get the exact input back after inversion. No prior inversion methods have any success on exact match, so it’s quite an ambitious measurement. So maybe we want to start with a smooth measurement that measures how similar the inverted text is to the input. For this we’ll use BLEU score, which you can just think of as a percentage of how close the inverted text is to the input.&lt;/p&gt;&lt;p&gt;With our success metric defined, let us move on to proposing an approach to evaluate with said metric. &amp;nbsp;For a first approach, we can pose inversion as a traditional machine learning problem, and we solve it the best way we know how: by gathering a large dataset of embedding-text pairs, and train a model to output the text given the embedding as input.&lt;/p&gt;&lt;p&gt;So this is what we did. We build a transformer that takes the embedding as input and train it using traditional language modeling on the output text. This first approach gives us a model with a BLEU score of around 30/100. Practically, the model can guess the topic of the input text, and get some of the words, but it loses their order and often gets most of them wrong. The exact match score is close to zero. It turns out that asking a model to reverse the output of another model in a single forward pass is quite hard (as are other complicated text generation tasks, like generating text in perfect sonnet form or satisfying multiple attributes).&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Do text embeddings perfectly encode text?" class="kg-image" height="401" src="https://lh7-us.googleusercontent.com/BorB5n0gaGnDObtJLRPC4lOHYn6l3tKS2AnXv03Oj62dPcqKjNFoNv6lfPtOL6KlpIo8U4BZPo8EC4BLVb8DFtDFzjt8CCbUOEeYeikHqTATDVsCNyWL331zcl6eQbU3uCTte1WkvtcMF9hMlnwvny4" width="624" /&gt;&lt;figcaption&gt;Overview of architectures considered. Prior work (left) uses a decoder-only architecture and inputs an embedding as a prefix. We initially trained an encoder-decoder model (middle) to condition on an upscaled sentence embedding on the encoder-side. Our final method (right) includes an additional “hypothesis” text along with an upscaled hypothesis embedding.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;After training our initial model, we noticed something interesting. A different way to measure model output quality is by re-embedding the generated text (we call this the “hypothesis”) and measuring this embedding’s similarity to the true embedding. When we do this with our model’s generations, we see a very high cosine similarity – around 0.97. This means that we’re able to generate text that’s close in embedding space, but not identical to, the ground-truth text.&lt;/p&gt;&lt;p&gt;(An aside: what if this weren’t the case? That is, what if the embedding assigned our incorrect hypothesis the same embedding as the original sequence. Our embedder would be &lt;em&gt;lossy&lt;/em&gt;, mapping multiple inputs to the same output. If this were the case, then our problem would be hopeless, and we would have no way of distinguishing which of multiple possible sequences produced it. In practice, we never observe these types of collisions in our experiments.)&lt;/p&gt;&lt;p&gt;The observation that hypotheses have different embeddings to the ground truth inspires an optimization-like approach to embedding inversion. Given a ground-truth embedding (where we want to go), and a current hypothesis text and its embedding (where we are right now), we can train a corrector model that’s trained to output something that’s closer to the ground-truth than the hypothesis.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Do text embeddings perfectly encode text?" class="kg-image" height="247" src="https://lh7-us.googleusercontent.com/ne0JB3F3WLFoTQR0fSgdxsmL6Ap4anP767qyjzNaySpkyu_uyAJEHnbzvsTmOsfsZOI6xMO1vhWWMp6vp_n_DMtAap-XucXKtH40_yctKbaUYQqBeWSbZEnhX3-LYZ1xzIvY-PMyO1kMh53DUCoGBXQ" width="624" /&gt;&lt;figcaption&gt;Overview of our method, Vec2Text. Given access to a target embedding e (blue) and query access to an embedding model ϕ (blue model), the system aims to iteratively generate (yellow model) hypotheses ˆe (pink) to reach the target.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Our goal is now clear: we want to build a system that can take a ground-truth embedding, a hypothesis text sequence, and the hypothesis position in embedding space, and predict the true text sequence. We think of this as a type of ‘learned optimization’ where we’re taking steps in embedding space in the form of discrete sequences. This is the essence of our method, which we call vec2text.&lt;/p&gt;&lt;p&gt;After working through some details and training the model, this process works extremely well! A single forward pass of correction increases the BLEU score from 30 to 50. And one benefit of this model is that it can naturally be queried recursively. Given a current text and its embedding, we can run many steps of this optimization, iteratively generating hypotheses, re-embedding them, and feeding them back in as input to the model. With 50 steps and a few tricks, we can get back 92% of 32-token sequences exactly, and get to a BLEU score of 97! (Generally achieving BLEU score of 97 means we’re almost perfectly reconstructing every sentence, perhaps with a few punctuation marks misplaced here and there.)&lt;br /&gt;&lt;/p&gt;&lt;h3 id="scaling-and-future-work"&gt;Scaling and future work &lt;/h3&gt;&lt;p&gt;The fact that text embeddings can be perfectly inverted raises many follow-up questions. For one, the text embedding vector contains a fixed number of bits; there must be some sequence length at which information can no longer be perfectly stored within this vector. Even though we can recover most texts of length 32, some embedding models can embed documents up to thousands of tokens. We leave it up to future work to analyze the relationship between text length, embedding size, and embedding invertibility.&lt;/p&gt;&lt;p&gt;Another open question is how to build systems that can defend against inversion. Is it possible to create models that can successfully embed text such that embeddings remain useful while obfuscating the text that created them?&lt;/p&gt;&lt;p&gt;Finally, we are excited to see how our method might apply to other modalities. The main idea behind vec2text (a sort of iterative optimization in embedding space) doesn’t use any text-specific tricks. It’s a method that iteratively recovers information contained in any fixed input, given black-box access to a model. It remains to be seen how these ideas might apply to inverting embeddings from other modalities as well as to approaches more general than embedding inversion.&lt;/p&gt;&lt;p&gt;To use our models to invert text embeddings, or to get started running embedding inversion experiments yourself, check out our Github repository: https://github.com/jxmorris12/vec2text&lt;/p&gt;&lt;hr /&gt;&lt;h3 id="references"&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;Inverting Visual Representations with Convolutional Networks (2015), https://arxiv.org/abs/1506.02753&lt;/p&gt;&lt;p&gt;Understanding Invariance via Feedforward Inversion of Discriminatively Trained Classifiers (2021), https://proceedings.mlr.press/v139/teterwak21a/teterwak21a.pdf&lt;/p&gt;&lt;p&gt;Text Embeddings Reveal (Almost) As Much As Text (2023), https://arxiv.org/abs/2310.06816&lt;/p&gt;&lt;p&gt;Language Model Inversion (2024), https://arxiv.org/abs/2311.13647&lt;/p&gt;&lt;h3 id="author-bio"&gt;&lt;strong&gt;Author Bio&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;Jack Morris is a PhD student at Cornell Tech in New York City. He works on research at the intersection of machine learning, natural language processing, and security. He’s especially interested in the information content of deep neural representations like embeddings and classifier outputs.&lt;/p&gt;&lt;h3 id="citation"&gt;Citation&lt;/h3&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Jack Morris, "Do text embeddings perfectly encode text?", The Gradient, 2024.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;BibTeX citation:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;@article{morris2024inversion,
    author = {Jack Morris},
    title = {Do text embeddings perfectly encode text?},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/text-embedding-inversion},
}&lt;/code&gt;&lt;/pre&gt;]]&amp;gt;&amp;lt;![CDATA[Why Doesn’t My Model Work?]]&amp;gt;https://thegradient.pub/why-doesnt-my-model-work/65ce1b5993571d5c8c1549b8Sat, 24 Feb 2024 18:41:54 GMT&lt;p&gt;Have you ever trained a model you thought was good, but then it failed miserably when applied to real world data? If so, you’re in good company. Machine learning processes are complex, and it’s very easy to do things that will cause overfitting without it being obvious. In the 20 years or so that I’ve been working in machine learning, I’ve seen many examples of this, prompting me to write “How to avoid machine learning pitfalls: a guide for academic researchers” in an attempt to prevent other people from falling into these traps.&lt;/p&gt;&lt;p&gt;But you don’t have to take my word for it. These issues are being increasingly reported in both the scientific and popular press. Examples include the observation that hundreds of models developed during the Covid pandemic simply don’t work, and that a water quality system deployed in Toronto regularly told people it was safe to bathe in dangerous water. Many of these are documented in the AIAAIC repository. It’s even been suggested that these machine learning missteps are causing a reproducibility crisis in science — and, given that many scientists use machine learning as a key tool these days, a lack of trust in published scientific results.&lt;/p&gt;&lt;p&gt;In this article, I’m going to talk about some of the issues that can cause a model to seem good when it isn’t. I’ll also talk about some of the ways in which these kinds of mistakes can be prevented, including the use of the recently-introduced REFORMS checklist for doing ML-based science.&lt;/p&gt;&lt;h2 id="duped-by-data"&gt;Duped by Data&lt;/h2&gt;&lt;p&gt;Misleading data is a good place to start, or rather not a good place to start, since the whole machine learning process rests upon the data that’s used to train and test the model.&lt;/p&gt;&lt;p&gt;In the worst cases, misleading data can cause the phenomenon known as &lt;em&gt;garbage in garbage out&lt;/em&gt;; that is, you can train a model, and potentially get very good performance on the test set, but the model has no real world utility. Examples of this can be found in the aforementioned review of Covid prediction models by Roberts et al. In the rush to develop tools for Covid prediction, a number of public datasets became available, but these were later found to contain misleading signals — such as overlapping records, mislabellings and hidden variables — all of which helped models to accurately predict the class labels without learning anything useful in the process.&lt;/p&gt;&lt;p&gt;Take hidden variables. These are features that are present in data, and which happen to be predictive of class labels within the data, but which are not directly related to them. If your model latches on to these during training, it will appear to work well, but may not work on new data. For example, in many Covid chest imaging datasets, the orientation of the body is a hidden variable: people who were sick were more likely to have been scanned lying down, whereas those who were standing tended to be healthy. Because they learnt this hidden variable, rather than the true features of the disease, many Covid machine learning models turned out to be good at predicting posture, but bad at predicting Covid. Despite their name, these hidden variables are often in plain sight, and there have been many examples of classifiers latching onto boundary markers, watermarks and timestamps embedded in images, which often serve to distinguish one class from another without having to look at the actual data.&lt;/p&gt;&lt;p&gt;A related issue is the presence of spurious correlations. Unlike hidden variables, these have no true relationship to anything else in the data; they’re just patterns that happen to correlate with the class labels. A classic example is the tank problem, where the US military allegedly tried to train a neural network to identify tanks, but it actually recognised the weather, since all the pictures of tanks were taken at the same time of day. Consider the images below: a machine learning model could recognise all the pictures of tanks in this dataset just by looking at the colour of pixels towards the top of an image, without having to consider the shape of any of the objects. The performance of the model would appear great, but it would be completely useless in practice.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Why Doesn’t My Model Work?" class="kg-image" height="563" src="https://thegradient.pub/content/images/2024/02/1.png" width="1379" /&gt;&lt;figcaption&gt;(Source: by author)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Many (perhaps most) datasets contain spurious correlations, but they’re not usually as obvious as this one. Common computer vision benchmarks, for example, are known to have groups of background pixels that are spuriously correlated with class labels. This represents a particular challenge to deep learners, which have the capacity to model many patterns within the data; various studies have shown that they do tend to capture spuriously correlated patterns, and this reduces their generality. Sensitivity to adversarial attacks is one consequence of this: if a deep learning model bases its prediction on spurious correlations in the background pixels of an image, then making small changes to these pixels can flip the prediction of the model. Adversarial training, where a model is exposed to adversarial samples during training, can be used to address this, but it’s expensive. An easier approach is just to look at your model, and see what information it’s using to make its decisions. For instance, if a saliency map produced by an explainable AI technique suggests that your model is focusing on something in the background, then it’s probably not going to generalise well.&lt;/p&gt;&lt;p&gt;Sometimes it’s not the data itself that is problematic, but rather the labelling of the data. This is especially the case when data is labelled by humans, and the labels end up capturing biases, misassumptions or just plain old mistakes made by the labellers. Examples of this can be seen in datasets used as image classification benchmarks, such as MNIST and CIFAR, which typically have a mislabelling rate of a couple of percent — not a huge amount, but pretty significant where modellers are fighting over accuracies in the tenths of a percent. That is, if your model does slightly better than the competition, is it due to an actual improvement, or due to modelling noise in the labelling process? Things can be even more troublesome when working with data that has implicit subjectivity, such as sentiment classification, where there’s a danger of overfitting particular labellers.&lt;/p&gt;&lt;h2 id="led-by-leaks"&gt;Led by Leaks&lt;/h2&gt;&lt;p&gt;Bad data isn’t the only problem. There’s plenty of scope for mistakes further down the machine learning pipeline. A common one is data leakage. This happens when the model training pipeline has access to information it shouldn’t have access to, particularly information that confers an advantage to the model. Most of the time, this manifests as information leaks from the test data — and whilst most people know that test data should be kept independent and not explicitly used during training, there are various subtle ways that information can leak out.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;One example is performing a data-dependent preprocessing operation on an entire dataset, before splitting off the test data. That is, making changes to all the data using information that was learnt by looking at all the data. Such operations vary from the simple, such as centering and scaling numerical features, to the complex, such as feature selection, dimensionality reduction and data augmentation — but they all have in common the fact that they use knowledge of the whole dataset to guide their outcome. This means that knowledge of the test data is implicitly entering the model training pipeline, even if it is not explicitly used to train the model. As a consequence, any measure of performance derived from the test set is likely to be an overestimate of the model’s true performance.&lt;/p&gt;&lt;p&gt;Let’s consider the simplest example: centering and scaling. This involves looking at the range of each feature, and then using this information to rescale all the values, typically so that the mean is 0 and the standard deviation is 1. If this is done on the whole dataset before splitting off the test data, then the scaling of the training data will include information about the range and distribution of the feature values in the test set. This is particularly problematic if the range of the test set is broader than the training set, since the model could potentially infer this fact from the truncated range of values present in the training data, and do well on the test set just by predicting values higher or lower than those which were seen during training. For instance, if you’re working on stock price forecasting from time series data with a model that takes inputs in the range 0 to 1 but it only sees values in the range 0 to 0.5 during training, then it’s not too hard for it to infer that stock prices will go up in the future.&lt;/p&gt;&lt;p&gt;In fact, forecasting is an area of machine learning that is particularly susceptible to data leaks, due to something called &lt;em&gt;look ahead bias&lt;/em&gt;. This occurs when information the model shouldn’t have access to leaks from the future and artificially improves its performance on the test set. This commonly happens when the training set contains samples that are further ahead in time than the test set. I’ll give an example later of when this can happen, but if you work in this area, I’d also strongly recommend taking a look at this excellent review of pitfalls and best practices in evaluating time series forecasting models.&lt;/p&gt;&lt;p&gt;An example of a more complex data-dependent preprocessing operation leading to overly-optimistic performance metrics can be found in this review of pre-term birth prediction models. Basically, a host of papers reported high accuracies at predicting whether a baby would be born early, but it turned out that all had applied data augmentation to the data set before splitting off the test data. This resulted in the test set containing augmented samples of training data, and the training set containing augmented samples of test data — which amounted to a pretty significant data leak. When the authors of the review corrected this, the predictive performance of the models dropped from being near perfect to not much better than random.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Why Doesn’t My Model Work?" class="kg-image" height="735" src="https://thegradient.pub/content/images/2024/02/2.png" width="1379" /&gt;&lt;figcaption&gt;(Source: https://arxiv.org/abs/2108.02497)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Oddly, one of the most common examples of data leakage doesn’t have an agreed name (the terms overhyping and sequential overfitting have been suggested) but is essentially a form of &lt;em&gt;training to the test set&lt;/em&gt;. By way of example, imagine the scenario depicted above where you’ve trained a model and evaluated it on the test set. You then decided its performance was below where you wanted it to be. So, you tweaked the model, and then you reevaluated it. You still weren’t happy, so you kept on doing this until its performance on the test set was good enough. Sounds familiar? Well, this is a common thing to do, but if you’re developing a model iteratively and using the same test set to evaluate the model after each iteration, then you’re basically using that test set to guide the development of the model. The end result is that you’ll overfit the test set and probably get an over-optimistic measure of how well your model generalises.&lt;/p&gt;&lt;p&gt;Interestingly, the same process occurs when people use community benchmarks, such as MNIST, CIFAR and ImageNet. Almost everyone who works on image classification uses these data sets to benchmark their approaches; so, over time, it’s inevitable that some overfitting of these benchmarks will occur. To mitigate against this, it’s always advisable to use a diverse selection of benchmarks, and ideally try your technique on a data set which other people haven’t used.&lt;/p&gt;&lt;h2 id="misinformed-by-metrics"&gt;Misinformed by Metrics&lt;/h2&gt;&lt;p&gt;Once you’ve built your model robustly, you then have to evaluate it robustly. There’s plenty that can go wrong here too. Let’s start with an inappropriate choice of metrics. The classic example is using accuracy with an imbalanced dataset. Imagine that you’ve managed to train a model that always predicts the same label, regardless of its input. If half of the test samples have this label as their ground truth, then you’ll get an accuracy of 50% — which is fine, a bad accuracy for a bad classifier. If 90% of the test samples have this label, then you’ll get an accuracy of 90% — a good accuracy for a bad classifier. This level of imbalance is not uncommon in real world data sets, and when working with imbalanced training sets, it’s not uncommon to get classifiers that always predict the majority label. In this case, it would be much better to use a metric like F score or Matthews correlation coefficient, since these are less sensitive to class imbalances. However, all metrics have their weaknesses, so it’s always best to use a portfolio of metrics that give different perspectives on a model’s performance and failure modes.&lt;/p&gt;&lt;p&gt;Metrics for time series forecasting are particularly troublesome. There are a lot of them to choose from, and the most appropriate choice can depend on both the specific problem domain and the exact nature of the time series data. Unlike metrics used for classification, many of the regression metrics used in time series forecasting have no natural scale, meaning that raw numbers can be misleading. For instance, the interpretation of mean squared errors depends on the range of values present in the time series. For this reason, it’s important to use appropriate baselines in addition to appropriate metrics. As an example, this (already mentioned) review of time series forecasting pitfalls demonstrates how many of the deep learning models published at top AI venues are actually less good than naive baseline models. For instance, they show that an autoformer, a kind of complex transformer model designed for time series forecasting, can be beaten by a trivial model that predicts no change at the next time step — something that isn’t apparent from looking at metrics alone.&lt;/p&gt;&lt;p&gt;In general, there is a trend towards developing increasingly complex models to solve difficult problems. However, it’s important to bear in mind that some problems may not be solvable, regardless of how complex the model becomes. This is probably the case for many financial time series forecasting problems. It’s also the case when predicting certain natural phenomena, particularly those in which a chaotic component precludes prediction beyond a certain time horizon. For instance, many people think that earthquakes can not be predicted, yet there are a host of papers reporting good performance on this task. This review paper discusses how these correct predictions may be due to a raft of modelling pitfalls, including inappropriate choice of baselines and overfitting due to data sparsity, unnecessary complexity and data leaks.&lt;/p&gt;&lt;p&gt;Another problem is assuming that a single evaluation is sufficient to measure the performance of a model. Sometimes it is, but a lot of the time you’ll be working with models that are stochastic or unstable; so, each time you train them, you get different results. Or you may be working with a small data set where you might just get lucky with an easy test split. To address both situations, it is commonplace to use resampling methods like cross-validation, which train and test a model on different subsets of the data and then work out the average performance. However, resampling introduces its own risks. One of these is the increased risk of data leaks, particularly when assuming that data-dependent preprocessing operations (like centering and scaling and feature selection) only need to be done once. They don’t; they need to be done independently for each iteration of the resampling process, and to do otherwise can cause a data leak. Below is an example of this, showing how feature selection should be done independently on the two training sets (in blue) used in the first two iterations of cross-validation, and how this results in different features being selected each time.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Why Doesn’t My Model Work?" class="kg-image" height="640" src="https://thegradient.pub/content/images/2024/02/3.png" width="1380" /&gt;&lt;figcaption&gt;(Source: https://arxiv.org/abs/2108.02497)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;As I mentioned earlier, the danger of data leaks is even greater when working with time series data. Using standard cross-validation, every iteration except one will involve using at least one training fold that is further ahead in time than the data in the test fold. For example, if you imagine that the data rows in the figure above represent time-ordered multivariate samples, then the test sets (in pink) used in both iterations occur earlier in the time series than all or part of the training data. This is an example of a look ahead bias. Alternative approaches, such as blocked cross-validation, can be used to prevent these.&lt;/p&gt;&lt;p&gt;Multiple evaluations aren’t an option for everyone. For example, training a foundation model is both time-consuming and expensive, so doing it repeatedly is not feasible. Depending on your resources, this may be the case for even relatively small deep learning models. If so, then also consider using other methods for measuring the robustness of models. This includes things like using explainability analysis, performing ablation studies, or augmenting test data. These can allow you to look beyond potentially-misleading metrics and gain some appreciation of how a model works and how it might fail, which in turn can help you decide whether to use it in practice.&lt;/p&gt;&lt;h2 id="falling-deeper"&gt;Falling Deeper&lt;/h2&gt;&lt;p&gt;So far, I’ve mostly talked about general machine learning processes, but the pitfalls can be even greater when using deep learning models. Consider the use of latent space models. These are often trained separately to the predictive models that use them. That is, it’s not unusual to train something like an autoencoder to do feature extraction, and then use the output of this model within the training of a downstream model. When doing this, it’s essential to ensure that the test set used in the downstream model does not intersect with the training data used in the autoencoder — something that can easily happen when using cross-validation or other resampling methods, e.g. when using different random splits or not selecting models trained on the same training folds.&lt;/p&gt;&lt;p&gt;However, as deep learning models get larger and more complex, it can be harder to ensure these kinds of data leaks do not occur. For instance, if you use a pre-trained foundation model, it may not be possible to tell whether the data used in your test set was used to train the foundation model — particularly if you’re using benchmark data from the internet to test your model. Things get even worse if you’re using composite models. For example, if you’re using a BERT-type foundation model to encode the inputs when fine-tuning a GPT-type foundation model, you have to take into account any intersection between the datasets used to train the two foundation models in addition to your own fine-tuning data. In practice, some of these data sets may be unknown, meaning that you can’t be confident whether your model is correctly generalising or merely reproducing data memorised during pre-training.&lt;/p&gt;&lt;h2 id="avoiding-the-pits"&gt;Avoiding the Pits&lt;/h2&gt;&lt;p&gt;These pitfalls are all too common. So, what’s the best way to avoid them? Well, one thing you can do is use a checklist, which is basically a formal document that takes you through the key pain points in the machine learning pipeline, and helps you to identify potential issues. In domains with high-stakes decisions, such as medicine, there are already a number of well-established checklists, such as CLAIM, and adherence to these is typically enforced by journals that publish in these areas.&lt;/p&gt;&lt;p&gt;However, I’d like to briefly introduce a new kid on the block: REFORMS, a consensus-based checklist for doing machine learning-based science. This was put together by 19 researchers across computer science, data science, mathematics, social sciences, and the biomedical sciences — including myself — and came out of a recent workshop on the reproducibility crisis in ML‑based science. It is intended to address the common mistakes that occur in the machine learning pipeline, including many of those mentioned in this article, in a more domain-independent manner. It consists of two parts: the checklist itself, and also a paired guidance document, which explains why each of the checklist items are important. The checklist works through the main components of a machine learning-based study, in each case encouraging the user to verify that the machine learning process is designed in such a way that it supports the overall aims of the study, doesn’t stumble into any of the common pitfalls, and enables the results to be verified by an independent researcher. Whilst it’s focused on the application of machine learning within a scientific context, a lot of what it covers is more generally applicable, so I’d encourage you to take a look even if you don’t consider your work to be “science”.&lt;/p&gt;&lt;p&gt;Another way of avoiding pitfalls is to make better use of tools. Now, one of my pet gripes regarding the current state of machine learning is that commonly-used tools do little to prevent you from making mistakes. That is, they’ll happily let you abuse the machine learning process in all sorts of ways without telling you what you’re doing is wrong. Nevertheless, help is available in the form of experiment tracking frameworks, which automatically keep a record of the models you trained and how you trained them, and this can be useful for spotting things like data leaks and training to the test set. An open source option is MLFlow, but there are plenty of commercial offerings. MLOps tools take this even further, and help to manage all the moving parts in a machine learning workflow, including the people.&lt;/p&gt;&lt;h2 id="final-thought"&gt;Final Thought&lt;/h2&gt;&lt;p&gt;It is possible to train a good model that generalises well to unseen data, but I wouldn’t believe this until you’re satisfied that nothing which could have gone wrong has gone wrong. A healthy sense of suspicion is a good thing: do look at your trained model to make sure it’s doing something sensible, do analyse your metrics to understand where it’s making mistakes, do calibrate your results against appropriate baselines, and do consider using checklists to make sure you haven’t overlooked something important.&lt;/p&gt;&lt;hr /&gt;&lt;h2 id="author-bio"&gt;Author Bio&lt;/h2&gt;&lt;p&gt;Michael is an Associate Professor at Heriot-Watt University, Edinburgh. He’s spent the last 20 years or so doing research on machine learning and bio-inspired computing. For more info see his academic website. He also writes about computer science more generally in his Fetch Decode Execute substack.&lt;/p&gt;&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Michael Lones, "Why Doesn’t My Model Work?", The Gradient, 2024.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;BibTeX citation:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;@article{lones2024why,
    author = {Michael Lones},
    title = {Why Doesn’t My Model Work?},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/why-doesnt-my-model-work},
}&lt;/code&gt;&lt;/pre&gt;]]&amp;gt;&amp;lt;![CDATA[Deep learning for single-cell sequencing: a microscope to see the diversity of cells]]&amp;gt;https://thegradient.pub/deep-learning-for-single-cell-sequencing-a-microscope-to-uncover-the-rich-diversity-of-individual-cells/65606b4793571d5c8c154793Sat, 13 Jan 2024 18:12:44 GMT&lt;p&gt;The history of each living being is written in its genome, which is stored as DNA and present in nearly every cell of the body. No two cells are the same, even if they share the same DNA and cell type, as they still differ in the regulators that control how DNA is expressed by the cell. The human genome consists of 3 billion base pairs spread over 23 chromosomes. Within this vast genetic code, there are approximately 20,000 to 25,000 genes, constituting the protein-coding DNA and accounting for about 1% of the total genome [1]. To explore the functioning of complex systems in our bodies, especially this small coding portion of DNA, a precise sequencing method is necessary, and single-cell sequencing (sc-seq) technology fits this purpose.&lt;/p&gt;&lt;p&gt;In 2013, Nature selected single-cell RNA sequencing as the Method of the Year [2] (Figure 3), highlighting the importance of this method for exploring cellular heterogeneity through the sequencing of DNA and RNA at the individual cell level. Subsequently, numerous tools have emerged for the analysis of single-cell RNA sequencing data. For example, the scRNA-tools database has been compiling software for the analysis of single-cell RNA data since 2016, and by 2021, the database includes over 1000 tools [3]. Among these tools, many involve methods that leverage Deep Learning techniques, which will be the focus of this article – we will explore the pivotal role that Deep Learning, in particular, has played as a key enabler for advancing single-cell sequencing technologies.&lt;/p&gt;&lt;h2 id="background"&gt;Background&lt;/h2&gt;&lt;h3 id="flow-of-genetic-information-from-dna-to-protein-in-cells"&gt;Flow of genetic information from DNA to protein in cells&lt;/h3&gt;&lt;p&gt;Let’s first go over what exactly cells and sequences are.&lt;strong&gt; &lt;/strong&gt;The cell is the fundamental unit of our bodies and the key to understanding how our bodies function in good health and how molecular dysfunction leads to disease. Our bodies are made of trillions of cells, and nearly every cell contains three genetic information layers: DNA, RNA, and protein. DNA is a long molecule containing the genetic code that makes each person unique. Like a source code, it includes several instructions showing how to make each protein in our bodies. These proteins are the workhorses of the cell that carry out nearly every task necessary for cellular life. For example, the enzymes that catalyze chemical reactions within the cell and DNA polymerases that contribute to DNA replication during cell division, are all proteins. The cell synthesizes proteins in two steps: Transcription and Translation (Figure 1), which are known as gene expression. DNA is first transcribed into RNA, then RNA is translated into protein. We can consider RNA as a messenger between DNA and protein.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Deep learning for single-cell sequencing: a microscope to see the diversity of cells" class="kg-image" height="204" src="https://lh7-us.googleusercontent.com/FE0jl7A1RgTBL5tPgpxsw624oxkwsWMLx3lRvyCQYwjLLLIezvRjxHONrVsJ8IJEx1EjZkTklRUD09IPg4JxBLq_goJPdCDy3PH5kA0EPntuMIhso_8R0Vouja3yZtU4rG1eYGarcsGjT4naVbPD2Q" width="447" /&gt;&lt;figcaption&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;. The central dogma of biology&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;While the cells of our body share the same DNA, they vary in their biological activity. For instance, the distinctions between immune cells and heart cells are determined by the genes that are either activated or deactivated in these cells. Generally, when a gene is activated, it leads to the creation of more RNA copies, resulting in increased protein production. Therefore, as cell types differ based on the quantity and type of RNA/protein molecules synthesized, it becomes intriguing to assess the abundance of these molecules at the single-cell level. This will enable us to investigate the behavior of our DNA &amp;nbsp;within each cell and attain a high-resolution perspective of the various parts of our bodies.&lt;/p&gt;&lt;p&gt;In general, all single-cell sequencing technologies can be divided into three main steps:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Isolation of single cells from the tissue of interest and extraction of genetic material from each isolated cell&lt;/li&gt;&lt;li&gt;Amplification of genetic material from each isolated cell and library preparation&lt;/li&gt;&lt;li&gt;Sequencing of the library using a next-generation sequencer and data analysis&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Navigating through the intricate steps of cellular biology and single-cell sequencing technologies, a pivotal question emerges: How is single-cell sequencing data represented numerically?&lt;/p&gt;&lt;h3 id="structure-of-single-cell-sequencing-data"&gt;Structure of single-cell sequencing data&lt;/h3&gt;&lt;p&gt;The structure of single-cell sequencing data takes the form of a matrix (Figure 2), where each row corresponds to a cell that has been sequenced and annotated with a unique barcode. The number of rows equals the total number of cells analyzed in the experiment. On the other hand, each column corresponds to a specific gene. Genes are the functional units of the genome that encode instructions for the synthesis of proteins or other functional molecules. In the case of scRNA seq data, the numerical entries in the matrix represent the expression levels of genes in individual cells. These values indicate the amount of RNA produced from each gene in a particular cell, providing insights into the activity of genes within different cells.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Deep learning for single-cell sequencing: a microscope to see the diversity of cells" class="kg-image" height="285" src="https://lh7-us.googleusercontent.com/qTHPAYL5g0S2KEbMKrt3HXoqTmPqeGpo2cqPyFN_NXzE_jUgC1H7C67igubfJOY_mfLXyILiPlQxW0Z3FOdisQ6wZ9cnN72mqeJJ9_Aa2x7qs79DHAgsZtuJjrMXpRe79TNtEhrpb10hqofcO_9aTA" width="280" /&gt;&lt;figcaption&gt;&lt;em&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;. Schema of single-cell sequencing data&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3 id="single-cell-sequencing-overview"&gt;&lt;strong&gt;Single Cell Sequencing Overview&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;For more than 150 years, biologists have wanted to identify all the cell types in the human body and classify them into distinct types based on accurate descriptions of their properties. The Human Cell Atlas Project (HCAP), the genetic equivalent of the Human Genome Project [4], is an international collaborative effort to map all the cells in the human body.” We can conceptualize the Human Cell Atlas as a map endeavoring to portray the human body coherently and systematically. Much like Google Maps, which allows us to zoom in for a closer examination of intricate details, the Human Cell Atlas provides insights into spatial information, internal attributes, and even the relationships among elements”, explains Aviv Regev, a computational and systems biologist at the Broad Institute of MIT and Harvard and Executive Vice President and Head of Genentech Research.&lt;/p&gt;&lt;p&gt;This analogy seamlessly aligns with the broader impact of single-cell sequencing, since it allows the analysis of individual cells instead of bulk populations. This technology proves invaluable in addressing intricate biological inquiries related to developmental processes and comprehending heterogeneous cellular or genetic changes under various treatment conditions or disease states. Additionally, it facilitates the identification of novel cell types within a given cellular population. The initiation of the first single-cell RNA sequencing (scRNA-seq) paper in 2009 [5], subsequently designated as the "method of the year" in 2013 [2], marked the genesis of an extensive endeavor to advance both experimental and computational techniques dedicated to unraveling the intricacies of single-cell transcriptomes.&lt;/p&gt;&lt;p&gt;As the technological landscape evolves, the narrative transitions to the advancements in single-cell research, particularly the early focus on single-cell RNA sequencing (scRNA-seq) due to its cost-effectiveness in studying complex cell populations.” In some ways, RNA has always been one of the easiest things to measure,” says Satija [6], a researcher at the New York Genome Center (NYGC). &amp;nbsp;Yet, the rapid development of single-cell technology has ushered in a new era of possibilities—multimodal single-cell data integration. Recognized as the "Method of the Year 2019" by Nature [7] (Figure 3), this approach allows the measurement of different cellular modalities, including the genome, epigenome, and proteome, within the same cell. The layering of multiple pieces of information provides powerful insights into cellular identity, posing the challenge of effectively modeling and combining datasets generated from multimodal measurements. This integration challenge is met with the introduction of Multi-view learning [8] methods, exploring common variations across modalities. This sophisticated approach, incorporating deep learning techniques, showcases relevant results across various fields, particularly in biology and biomedicine.&lt;/p&gt;&lt;p&gt;Amidst these advancements, a distinct challenge surfaces in the persistent limitation of single-cell RNA sequencing—the loss of spatial information during transcriptome profiling by isolating cells from their original position. Spatially resolved transcriptomics (SRT) emerges as a pivotal solution [9], addressing the challenge by preserving spatial details during the study of complex biological systems. This recognition of spatially resolved transcriptomics as the method of the year 2020 solidifies its place as a critical solution to the challenges inherent in advancing our understanding of complex biological systems.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Deep learning for single-cell sequencing: a microscope to see the diversity of cells" class="kg-image" height="177" src="https://lh7-us.googleusercontent.com/wuqGlu6SprK1ganCDuvgJLJqHhfPhGHpO2xZTSYQ0orB3PLdbAhYwZNXUPVrbRWjH9bHimSgLOsEEDhTKZumcHovsqUqEGo5GILDf74QkaXAiqdpGUdjbda7XYsmrqbsh9sH_ASz6auWXTCtSbHF4g" width="591" /&gt;&lt;figcaption&gt;&lt;em&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;. Evolution of single-cell sequencing over time&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Having explored the panorama of single-cell sequencing, let us now delve into the role of deep learning in the context of single-cell sequencing.&lt;/p&gt;&lt;h3 id="deep-learning-on-single-cell-sequencing"&gt;Deep Learning on single-cell sequencing&lt;/h3&gt;&lt;p&gt;Deep learning is increasingly employed in single-cell analysis due to its capacity to handle the complexity of single-cell sequencing data. In contrast, conventional machine-learning approaches require significant effort to develop a feature engineering strategy, typically designed by domain experts. The deep learning approach, however, autonomously captures relevant characteristics from single-cell sequencing data, addressing the heterogeneity between single-cell sequencing experiments, as well as the associated noise and sparsity in such data. Below are three key reasons for the application of deep learning in single-cell sequencing:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;High-Dimensional Data&lt;/strong&gt;: Single-cell sequencing generates high-dimensional data, with thousands of genes and their expression levels measured for each cell. Deep learning models are adept at capturing complex relationships and patterns within this data, which can be challenging for traditional statistical methods.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Non-Linearity:&lt;/strong&gt; Single-cell gene expression data is characterized by its inherent nonlinearity between gene expressions and cell-to-cell heterogeneity. Traditional statistical methods encounter difficulties in capturing the non-linear relationships present in single-cell gene expression data. In contrast, deep learning models are flexible and able to learn complex non-linear mappings.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Heterogeneity:&lt;/strong&gt; Single-cell data is often characterized by diverse cell populations with varying gene expression profiles, presenting a complex landscape. Deep learning models can play a crucial role in identifying, clustering, and characterizing these distinct cell types or subpopulations, thereby facilitating a deeper understanding of cellular heterogeneity within a sample.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;As we explore the reasons behind using deep learning in single-cell sequencing data, it leads us to the question: What deep learning architectures are often used in sc-seq data analysis?&lt;/p&gt;&lt;h3 id="background-on-autoencoders"&gt;Background on Autoencoders&lt;/h3&gt;&lt;p&gt;Autoencoders (AEs) stand out among various deep-learning architectures (such as GANs and RNNs) as an especially relied upon method for decoding the complexities of single-cell sequencing data. &amp;nbsp;Widely employed for dimensionality reduction while preserving the inherent heterogeneity in the single-cell sequencing data. By clustering cells in the reduced-dimensional space generated by autoencoders, researchers can effectively identify and characterize different cell types or subpopulations. This approach enhances our ability to discern and analyze the diverse cellular components within single-cell datasets. In contrast to non-deep learning models, such as principal component analysis (PCA), which are integral components of established scRNA-seq data analysis software like Seurat [10], autoencoders distinguish themselves by uncovering non-linear manifolds. While PCA is constrained to linear transformations, the flexibility of autoencoders to capture complex non-linear mappings makes it an advanced method to find nuanced relationships embedded in single-cell genomics.&lt;/p&gt;&lt;p&gt;To mitigate the overfitting challenge associated with autoencoders, several enhancements to the autoencoder structure have been implemented, specifically tailored to offer advantages in the context of sc-seq data. One notable adaptation often used in the context of sc-seq data is the denoising autoencoder (DAEs), which amplifies the autoencoder's reconstruction capability by introducing noise to the initial network layer. This involves randomly transforming some of its units to zero. The Denoising Autoencoder then reconstructs the input from this intentionally corrupted version, empowering the network to capture more relevant features and preventing it from merely memorizing the input (overfitting). This refinement significantly bolsters the model's resilience against data noise, thereby elevating the quality of the low-dimensional representation of samples (i.e., bottleneck) derived from the sc-seq data.&lt;/p&gt;&lt;p&gt;A third variation of autoencoders frequently employed in sc-seq data analysis is variational autoencoders (VAEs), exemplified by models like scGen [19], scVI [14], scANVI [28], etc. VAEs, as a type of generative model, learn a latent representation distribution of the data. Instead of encoding the data into a vector of p-dimensional latent variables, the data is encoded into two vectors of size p: a vector of means η and a vector of standard deviations σ. VAEs introduce a probabilistic element to the encoding process, facilitating the generation of synthetic single-cell data and offering insights into the diversity within a cell population. This nuanced approach adds another layer of complexity and richness to the exploration of single-cell genomics.&lt;/p&gt;&lt;h2 id="applications-of-deep-learning-in-sc-seq-data-analysis"&gt;Applications of deep learning in sc-seq data analysis&lt;/h2&gt;&lt;p&gt;This section outlines the main applications of deep learning in improving various stages of sc-seq data analysis, highlighting its effectiveness in advancing crucial aspects of the process.&lt;/p&gt;&lt;h3 id="scrna-seq-data-imputation-and-denoising"&gt;scRNA-seq data imputation and denoising&lt;/h3&gt;&lt;p&gt;Single-cell RNA sequencing (scRNA-seq) data encounter inherent challenges, with dropout events being a prominent concern that leads to significant issues—resulting in sparsity within the gene expression matrix, often characterized by a substantial number of zero values. This sparsity significantly shapes downstream bioinformatics analyses. Many of these zero values arise artificially due to deficiencies in sequencing techniques, including problems like inadequate gene expression, low capture rates, sequencing depth, or other technical factors. As a consequence, the observed zero values do not accurately reflect the true underlying expression levels. Hence, not all zeros in scRNA-seq data can be considered mere missing values, deviating from the conventional statistical approach of imputing missing data values. Given the intricate distinction between true and false zero counts, traditional imputation methods with predefined missing values may prove inadequate for scRNA-seq data. For instance, a classical imputation method, like Mean Imputation, might entail substituting these zero values with the average expression level of that gene across all cells. However, this approach runs the risk of oversimplifying the complexities introduced by dropout events in scRNA-seq data, potentially leading to biased interpretations.&lt;/p&gt;&lt;p&gt;ScRNA-seq data imputation methods can be divided into two categories: deep learning–based imputation method and non–deep learning imputation method. The non–deep learning imputation algorithms involve fitting statistical probability models or utilizing the expression matrix for smoothing and diffusion. This simplicity renders it effective for certain types of samples. For example, Wagner et al. [11] utilized the k-nearest neighbors (KNN) method, identifying nearest neighbors between cells and aggregating gene-specific Unique Molecular Identifiers (UMI) counts to impute the gene expression matrix. In contrast, Huang et al. [12] proposed the SVAER algorithm, leveraging gene-to-gene relationships for imputing the gene expression matrix. For larger datasets (comprising tens of thousands or more), high-dimensional, sparse, and complex scRNA-seq data, traditional computational methods face difficulties, often rendering analysis using these methods difficult and infeasible. Consequently, many researchers have turned to designing methods based on deep learning to address these challenges.&lt;/p&gt;&lt;p&gt;Most deep learning algorithms for imputing dropout events are based on autoencoders (AEs). For instance, in 2018, Eraslan et al. [13] introduced the deep count autoencoder (DCA). DCA utilizes a deep autoencoder architecture to address dropout events in single-cell RNA sequencing (scRNA-seq) data. It incorporates a probabilistic layer in the decoder to model the dropout process. This probabilistic layer accommodates the uncertainty associated with dropout events, enabling the model to generate a distribution of possible imputed values. To capture the characteristics of count data in scRNA-seq, DCA models the observed counts as originating from a negative binomial distribution.&lt;/p&gt;&lt;p&gt;Single-cell variational inference (scVI) is another deep learning algorithm introduced by Lopez et al. [14]. ScVI is a probabilistic variational autoencoder (VAE) that combines deep learning and probabilistic modeling to capture the underlying structure of the scRNA-seq data. &amp;nbsp;ScVI can be used for imputation, denoising, and various other tasks related to the analysis of scRNA-seq data. In contrast to the DCA model, scVI employs Zero-Inflated Negative Binomial (ZINB) distribution in the decoder part to generate a distribution of possible counts for each gene in each cell. The Zero-Inflated Negative Binomial (ZINB) distribution allows modeling the probability of a gene expression being zero (to model dropout events) as well as the distribution of positive values (to model non-zero counts).&lt;/p&gt;&lt;p&gt;Additionally, another study addressed the scRNA-seq data imputation challenge by introducing a recurrent network layer in their model, known as scScope [15]. This novel architecture iteratively performs imputations on zero-valued entries of input scRNA-seq data. The flexibility of scScope's design allows for the iterative improvement of imputed outputs through a chosen number of recurrent steps (T). Noteworthy is the fact that reducing the time recurrence of scScope to one (i.e., T = 1) transforms the model into a traditional autoencoder (AE). As scScope is essentially a modification of traditional AEs, its runtime is comparable to other AE-based models.&lt;/p&gt;&lt;p&gt;It's important to note that the application of deep learning in scRNA-seq data imputation and denoising is particularly advantageous due to its ability to capture non-linear relationships among genes. This contrasts with standard linear approaches, making deep learning more adept at providing informed and accurate imputation strategies in the context of single-cell genomics.&lt;/p&gt;&lt;h3 id="batch-effect-removal"&gt;Batch effect removal&lt;/h3&gt;&lt;p&gt;Single-cell data is commonly aggregated from diverse experiments that vary in terms of experimental laboratories, protocols, sample compositions, and even technology platforms. These differences result in significant variations or batch effects within the data, posing a challenge in the analysis of biological variations of interest during the process of data integration. To address this issue, it becomes necessary to correct batch effects by removing technical variance when integrating cells from different batches or studies. The first method that appears for batch correction is a linear method based on linear regression such as Limma package [16] that provides the removeBatchEffect function which fits a linear model that considers the batches and their impact on gene expression. &amp;nbsp;After fitting the model, it sets the coefficients associated with each batch to zero, effectively removing their impact. Another method called ComBat [17] does something similar but adds an extra step to refine the process, making the correction even more accurate by using a technique called empirical Bayes shrinkage.&lt;/p&gt;&lt;p&gt;However, batch effects can be highly nonlinear, making it difficult to correctly align different datasets while preserving key biological variations. In 2018, Haghverdi et al. introduced the Mutual Nearest Neighbors (MNN) algorithm to identify pairs of cells from different batches in single-cell data [18]. These identified mutual nearest neighbors aid in estimating batch effects between batches. By applying this correction, the gene expression values are adjusted to account for the estimated batch effects, aligning them more closely and reducing discrepancies introduced by the different batches. For extensive single-cell datasets with highly nonlinear batch effects, traditional methods may prove less effective, prompting researchers to explore the application of neural networks for improved batch correction.&lt;/p&gt;&lt;p&gt;One of the pioneering models that employ deep learning for batch correction is the scGen model. Developed by Lotfollahi et al., ScGen [19] utilizes a variational autoencoder (VAE) architecture. This involves pre-training a VAE model on a reference dataset to adjust real single-cell data and alleviate batch effects. Initially, the VAE is trained to capture latent features within the reference dataset's cells. Subsequently, this trained VAE is applied to the actual data, producing latent representations for each cell. The adjustment of gene expression profiles is then based on aligning these latent representations, to reduce batch effects and harmonize profiles across different experimental conditions.&lt;br /&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Deep learning for single-cell sequencing: a microscope to see the diversity of cells" class="kg-image" height="277" src="https://lh7-us.googleusercontent.com/FDIttLe4zUalLYZO1aXTVcBQvPlijO5VVaCmkIoUHYfTbQ0hvOx2iViXlpT14Sj5hmUng9bF63T4NwwvIlKiYYXNTvPo6ArVhqphhq7CLB5Tc75FNTZjCXujHWif8ECGJtOlav-7R1G5ttS8fbRPPQ" width="515" /&gt;&lt;figcaption&gt;&lt;em&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; scGen removes batch effects [19]. a, UMAP visualization of 4 technically diverse pancreatic datasets with their corresponding batch and cell types. b, Data corrected by scGen mixes shared cell types from different studies while preserving the biological variance of cells.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;On the other hand, Zou et al. introduced DeepMNN [20], which employs a residual neural network and the mutual nearest neighbor (MNN) algorithm for scRNA-seq data batch correction. Initially, MNN pairs are identified across batches in a principal component analysis (PCA) subspace. Subsequently, a batch correction network is constructed using two stacked residual blocks to remove batch effects. The loss function of DeepMNN comprises a batch loss, computed based on the distance between cells in MNN pairs in the PCA subspace, and a weighted regularization loss, ensuring the network's output similarity to the input.&lt;/p&gt;&lt;p&gt;The majority of existing scRNA-seq methods are designed to remove batch effects first and then cluster cells, which potentially overlooks certain rare cell types. Recently, Xiaokang et al. developed scDML [21], a deep metric learning model to remove batch effect in scRNA-seq data, guided by the initial clusters and the nearest neighbor information intra and inter-batches. First, the graph-based clustering algorithm is used to group cells based on gene expression similarities, then the KNN algorithm is applied to identify k-nearest neighbors for each cell in the dataset, and the MNN algorithm to identify mutual nearest neighbors, focusing on reciprocal relationships between cells. To remove batch effects, deep triplet learning is employed, considering hard triplets. This helps in learning a low-dimensional embedding that accounts for the original high-dimensional gene expression and removes batch effects simultaneously.&lt;/p&gt;&lt;h3 id="cell-type-annotation"&gt;Cell type annotation&lt;/h3&gt;&lt;p&gt;Cell type annotation in single-cell sequencing involves the process of identifying and labeling individual cells based on their gene expression profiles, which allows researchers to capture the diversity within a heterogeneous population of cells, and understand the cellular composition of tissues, and the functional roles of different cell types in biological processes or diseases. &amp;nbsp;Traditionally, researchers have used manual methods [22] to annotate cell sub-populations. This involves identifying gene markers or gene signatures that are differentially expressed in a specific cell cluster. Once gene markers are identified, researchers manually interpret the biological relevance of these markers to assign cell-type labels to the clusters. This traditional manual annotation approach is time-consuming and requires considerable human effort, especially when dealing with large-scale single-cell datasets. Due to the challenges associated with manual annotation, researchers are turning to automate and streamline the cell annotation process.&lt;/p&gt;&lt;p&gt;Two primary strategies are employed for cell type annotation: unsupervised-based and supervised-based. In the unsupervised realm, clustering methods such as Scanpy [23] and Seurat [10] are utilized, demanding prior knowledge of established cellular markers. The identification of clusters hinges on the unsupervised grouping of cells without external reference information. However, a drawback to this approach is a potential decrease in replicability with an increased number of clusters and multiple selections of cluster marker genes.&lt;/p&gt;&lt;p&gt;Conversely, supervised-based strategies rely on deep-learning models trained on labeled data. These models discern intricate patterns and relationships within gene expression data during training, enabling them to predict cell types for unlabeled data based on acquired patterns. For example, Joint Integration and Discrimination (JIND) [24] &amp;nbsp; deploys a GAN-style deep architecture, where an encoder is pre-trained on classification tasks, circumventing the need for an autoencoder framework. This model also accounts for batch effects. AutoClass [25] integrates an autoencoder and a classifier, combining output reconstruction loss with a classification loss for cell annotation alongside data imputation. Additionally, TransCluster, [26] rooted in the Transformer framework and convolutional neural network (CNN), employs feature extraction from the gene expression matrix for single-cell annotation.&lt;/p&gt;&lt;p&gt;Despite the power of deep neural networks, obtaining a large number of accurately and unbiasedly annotated cells for training is challenging, given the labor-intensive manual inspection of marker genes in scRNAseq data. In response, semi-supervised learning has been leveraged in computational cell annotation. For instance, the SemiRNet [27] model uses both unlabeled and a limited amount of labeled scRNAseq cells to implement cell identification. SemiRNet, based on recurrent convolutional neural networks (RCNN), incorporates a shared network, a supervised network, and an unsupervised network. Furthermore, single‐cell ANnotation using Variational Inference (scANVI) [28], a semi‐supervised variant of scVI [14], maximizes the utility of existing cell state annotations. Cell BLAST, an autoencoder-based generative model, harnesses large-scale reference databases to learn nonlinear low-dimensional representations of cells, employing a sophisticated cell similarity metric—normalized projection distance—to map query cells to specific cell types and identify novel cell types.&lt;/p&gt;&lt;h3 id="multi-omics-data-integration"&gt;Multi-omics Data Integration&lt;/h3&gt;&lt;p&gt;Recent studies have demonstrated the potential of deep learning models in addressing complex and multimodal biological challenges [29]. &amp;nbsp;Among the algorithms proposed thus far, it is primarily deep learning-based models that provide the essential computational adaptability necessary for effectively modeling and incorporating nearly any form of omic data &amp;nbsp;including &amp;nbsp;genomics (studying DNA sequences and genetic variations), epigenomics (examining changes in gene activity unrelated to DNA sequence, such as DNA modifications and chromatin structure), transcriptomics (investigating RNA molecules and gene expression through RNA sequencing), and proteomics (analyzing all proteins produced by an organism, including structures, abundances, and modifications). Deep Learning architectures, including autoencoders (AE) and generative adversarial networks (GAN), have been often used in multi-omics integration problems in single cells. The key question in multi-omics integration revolves around how to effectively represent the diverse multi-omics data within a unified latent space.&lt;/p&gt;&lt;p&gt;One of the early methods developed using Variational Autoencoders (VAE) for the integration of multi-omics single-cell data is known as totalVI [30]. The totalVI model, which is VAE-based, offers a solution for effectively merging scRNA-seq and protein data. In this model, totalVI takes input matrices containing scRNA-seq and protein count data. Specifically, it treats gene expression data as sampled from a negative binomial distribution, while protein data are treated as sampled from a mixture model consisting of two negative binomial distributions. The model first learns shared latent space representations through its encoder, which are then utilized to reconstruct the original data, taking into account the differences between the two original data modalities. Lastly, the decoder component estimates the parameters of the underlying distributions for both data modalities using the shared latent representation.&lt;/p&gt;&lt;p&gt;On the other hand, Zuo et al. [31] introduced scMVAE as a multimodal variational autoencoder designed to integrate transcriptomic and chromatin accessibility data in the same individual cells. scMVAE employs two separate single-modal encoders and two single-modal decoders to effectively model both transcriptomic and chromatin data. It achieves this by combining three distinct joint-learning strategies with a probabilistic Gaussian Mixture Model.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Deep learning for single-cell sequencing: a microscope to see the diversity of cells" class="kg-image" height="242" src="https://lh7-us.googleusercontent.com/k7QSkoQ1aPR921c2cqbKPYQ0HhguVnR2Xi8lrd9VCD38PFEifKNFU94Sb_IJR1DZl5zio9HKiePtlW9KrQD91cV_oiN4DLAzXt7I2UC_ETdfSPKKJ913D_U3_kof9NzBjEBlVVpUUs08YLTjt7fW3w" width="669" /&gt;&lt;figcaption&gt;&lt;em&gt;&lt;strong&gt;Figure 5 .&lt;/strong&gt; UMAP embedding for the latent space of the MULTIGRATE for CITE-seq dataset combines gene expression and cell surface protein data [32].&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Recently, Lotfollahi et al. [32] introduced an unsupervised deep generative model known as MULTIGRATE for the integration of multi-omic datasets. MULTIGRATE employs a multi-modal variational autoencoder structure that shares some similarities with the scMVAE model. However, it offers added generality and the capability to integrate both paired and unpaired single-cell data. To enhance cell alignment, the loss function incorporates Maximum Mean Discrepancy (MMD), penalizing any misalignment between the point clouds associated with different assays. Incorporating transfer learning, MULTIGRATE can map new multi-omic query datasets into a reference atlas and also perform imputations for missing modalities.&lt;/p&gt;&lt;h2 id="conclusion"&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;The application of deep learning in single-cell sequencing functions as an advanced microscope, revealing intricate insights within individual cells and providing a profound understanding of cellular heterogeneity and complexity in biological systems. This cutting-edge technology empowers scientists to explore previously undiscovered aspects of cellular behavior. However, the challenge lies in choosing between traditional tools and the plethora of available deep-learning options. The landscape of tools is vast, and researchers must carefully consider factors such as data type, complexity, and the specific biological questions at hand. Navigating this decision-making process requires a thoughtful evaluation of the strengths and limitations of each tool in relation to research goals.&lt;/p&gt;&lt;p&gt;On the other hand, a critical need in the development of deep learning approaches for single-cell RNA sequencing (scRNA-seq) analysis is robust benchmarking. While many studies compare deep learning performance to standard methods, there is a lack of comprehensive comparisons across various deep learning models. Moreover, methods often claim superiority based on specific datasets and tissues (e.g., pancreas cells, immune cells), making it challenging to evaluate the necessity of specific terms or preprocessing steps. Addressing these challenges requires an understanding of when deep learning models fail and their limitations. Recognizing which types of deep learning approaches and model structures are beneficial in specific cases is crucial for developing new approaches and guiding the field.&lt;/p&gt;&lt;p&gt;In the realm of multi-omics single-cell integration, most deep learning methods aim to find a shared latent representation for all modalities. However, shared representation learning faces challenges such as heightened noise, sparsity, and the intricate task of balancing modalities. Inherent biases across institutions complicate generalization. Despite being less prevalent than single-modality approaches, integrating diverse modalities with unique cell populations is crucial. Objectives include predicting expression across modalities and identifying cells in similar states. Despite advancements, further efforts are essential for enhanced performance, particularly concerning unique or rare cell populations present in one technology but not the other.&lt;/p&gt;&lt;hr /&gt;&lt;h2 id="author-bio"&gt;Author Bio&lt;/h2&gt;&lt;p&gt;Fatima Zahra El Hajji holds a master's degree in bioinformatics from the National School of Computer Science and Systems Analysis &amp;nbsp;(ENSIAS), she subsequently worked as an AI intern at Piercing Star Technologies. Currently, she is a Ph.D. student at the University Mohammed VI Polytechnic (UM6P), working under the supervision of Dr. Rachid El Fatimy and &amp;nbsp;Dr. Tariq Daouda. Her research focuses on the application of deep learning techniques in single-cell sequencing data.&lt;/p&gt;&lt;h2 id="citation"&gt;Citation&lt;br /&gt;&lt;/h2&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Fatima Zahra El Hajji, "Deep learning for single-cell sequencing: a microscope to see the diversity of cells", The Gradient, 2024.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;BibTeX citation:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;@article{elhajji2023nar,
    author = {El Hajji, Fatima Zahra},
    title = {Deep learning for single-cell sequencing: a microscope to see the diversity of cells},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/deep-learning-for-single-cell-sequencing-a-microscope-to-uncover-the-rich-diversity-of-individual-cells},
}&lt;/code&gt;&lt;/pre&gt;&lt;h2 id="references"&gt;References&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;National Human Genome Research Institute (NHGRI) : A Brief Guide to Genomics ,&lt;/em&gt;&lt;em&gt; https://www.genome.gov/about-genomics/fact-sheets/A-Brief-Guide-to-Genomics&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Method of the Year 2013. Nat Methods 11, 1 (2014).&lt;/em&gt;&lt;em&gt; https://doi.org/10.1038/nmeth.2801&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Zappia, L., Theis, F.J. Over 1000 tools reveal trends in the single-cell RNA-seq analysis landscape. Genome Biol 22, 301 (2021).&lt;/em&gt;&lt;em&gt; https://doi.org/10.1186/s13059-021-02519-4&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Collins FS, Fink L. The Human Genome Project. Alcohol Health Res World. 1995;19(3):190-195. PMID: 31798046; PMCID: PMC6875757.&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Tang F, Barbacioru C, Wang Y, et al. mRNA-Seq whole-transcriptome analysis of a single cell. Nat Methods. 2009; 6: 377-382.&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Eisenstein, M. The secret life of cells. Nat Methods 17, 7–10 (2020). https://doi.org/10.1038/s41592-019-0698-y&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Method of the Year 2019: Single-cell multimodal omics. Nat Methods 17, 1 (2020). https://doi.org/10.1038/s41592-019-0703-5&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Zhao, Jing et al. “Multi-view learning overview: Recent progress and new challenges.” Inf. Fusion 38 (2017): 43-54.&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Zhu, J., Shang, L. &amp;amp; Zhou, X. SRTsim: spatial pattern preserving simulations for spatially resolved transcriptomics. Genome Biol 24, 39 (2023).&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Butler, A., Hoffman, P., Smibert, P., Papalexi, E., &amp;amp; Satija, R. (2018). Integrating single-cell transcriptomic data across different conditions, technologies, and species. Nature biotechnology, 36(5), 411-420&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;Wagner, F., Yan, Y., &amp;amp; Yanai, I. (2018). K-nearest neighbor smoothing for high-throughput single-cell RNA-Seq data. bioRxiv, 217737. Cold Spring Harbor Laboratory. https://doi.org/10.1101/217737&lt;/li&gt;&lt;li&gt;Huang, M., Wang, J., Torre, E. et al. SAVER: gene expression recovery for single-cell RNA sequencing. Nat Methods 15, 539–542 (2018). https://doi.org/10.1038/s41592-018-0033-z&lt;/li&gt;&lt;li&gt;Eraslan G, Simon LM, Mircea M, Mueller NS, Theis FJ. Single-cell RNA-seq denoising using a deep count autoencoder. Nat Commun. 2019 Jan 23;10(1):390. doi: 10.1038/s41467-018-07931-2. PMID: 30674886; PMCID: PMC6344535.&lt;/li&gt;&lt;li&gt;Lopez, R., Regier, J., Cole, M. B., Jordan, M. I.,&amp;amp; Yosef, N. (2018). Deep generative modeling for single-cell transcriptomics. Nature methods, 15(12), 1053-1058.&lt;/li&gt;&lt;li&gt;Y. Deng, F. Bao, Q. Dai, L.F. Wu, S.J. Altschuler Scalable analysis of cell-type composition from single-cell transcriptomics using deep recurrent learning&lt;/li&gt;&lt;li&gt;Ritchie ME, Phipson B, Wu D, Hu Y, Law CW, Shi W, Smyth GK. limma powers differential expression analyses for RNA-sequencing and microarray studies. Nucleic Acids Res. 2015 Apr 20;43(7):e47. doi: 10.1093/nar/gkv007. Epub 2015 Jan 20. PMID: 25605792; PMCID: PMC4402510.&lt;/li&gt;&lt;li&gt;Johnson W.E. , Li C., Rabinovic A. Adjusting batch effects in microarray expression data using empirical bayes methods. Biostatistics. 2007; 8:118–127.&lt;/li&gt;&lt;li&gt;Haghverdi, L., Lun, A., Morgan, M. et al. Batch effects in single-cell RNA-sequencing data are corrected by matching mutual nearest neighbors. Nat Biotechnol 36, 421–427 (2018). https://doi.org/10.1038/nbt.4091&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Lotfollahi, M., Wolf, F. A., &amp;amp; Theis, F. J. (2019). scGen predicts single-cell perturbation responses. Nature methods, 16(8), 715-721.&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Zou, B., Zhang, T., Zhou, R., Jiang, X., Yang, H., Jin, X., &amp;amp; Bai, Y. (2021). deepMNN: deep learning-based single-cell RNA sequencing data batch correction using mutual nearest neighbors. Frontiers in Genetics, 1441.&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Yu, X., Xu, X., Zhang, J. et al. Batch alignment of single-cell transcriptomics data using deep metric learning. Nat Commun 14, 960 (2023). https://doi.org/10.1038/s41467-023-36635-5&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Z.A. Clarke, T.S. Andrews, J. Atif, D. Pouyabahar, B.T. Innes, S.A. MacParland, et al. Tutorial: guidelines for annotating single-cell transcriptomic maps using automated and manual methods Nat Protoc, 16 (2021), pp. 2749-2764&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Wolf, F., Angerer, P. &amp;amp; Theis, F. SCANPY: large-scale single-cell gene expression data analysis. Genome Biol 19, 15 (2018). https://doi.org/10.1186/s13059-017-1382-0&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Mohit Goyal, Guillermo Serrano, Josepmaria Argemi, Ilan Shomorony, Mikel Hernaez, Idoia Ochoa, JIND: joint integration and discrimination for automated single-cell annotation, Bioinformatics, Volume 38, Issue 9, March 2022, Pages 2488–2495, https://doi.org/10.1093/bioinformatics/btac140&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;H. Li, C.R. Brouwer, W. Luo A universal deep neural network for in-depth cleaning of single-cell RNA-seq data Nat Commun, 13 (2022), p. 1901&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Song T, Dai H, Wang S, Wang G, Zhang X, Zhang Y and Jiao L (2022) TransCluster: A Cell-Type Identification Method for single-cell RNA-Seq data using deep learning based on transformer. Front. Genet. 13:1038919. doi: 10.3389/fgene.2022.1038919&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Dong X, Chowdhury S, Victor U, Li X, Qian L. Semi-Supervised Deep Learning for Cell Type Identification From Single-Cell Transcriptomic Data. IEEE/ACM Trans Comput Biol Bioinform. 2023 Mar-Apr;20(2):1492-1505. doi: 10.1109/TCBB.2022.3173587. Epub 2023 Apr 3. PMID: 35536811.&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Xu, C., Lopez, R., Mehlman, E., Regier, J., Jordan, M. I., &amp;amp; Yosef, N. (2021). Probabilistic harmonization and annotation of single‐cell transcriptomics data with deep generative models. Molecular Systems Biology, 17(1), e9620. https://doi.org/10.15252/msb.20209620&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Tasbiraha Athaya, Rony Chowdhury Ripan, Xiaoman Li, Haiyan Hu, Multimodal deep learning approaches for single-cell multi-omics data integration, Briefings in Bioinformatics, Volume 24, Issue 5, September 2023, bbad313, &lt;/em&gt;&lt;em&gt;https://doi.org/10.1093/bib/bbad313&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;Gayoso, A., Lopez, R., Steier, Z., Regier, J., Streets, A., &amp;amp; Yosef, N. (2019). A Joint Model of RNA Expression and Surface Protein Abundance in Single Cells. bioRxiv, 791947. &lt;/em&gt;&lt;em&gt;https://www.biorxiv.org/content/early/2019/10/07/791947.abstract&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;Chunman Zuo, Luonan Chen. Deep-joint-learning analysis model of single cell transcriptome and open chromatin accessibility data. Briefings in Bioinformatics. 2020.&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;Lotfollahi, M., Litinetskaya, A., &amp;amp; Theis, F. J. (2022). Multigrate: single-cell multi-omic data integration.bioRxiv.&lt;/em&gt;&lt;em&gt;https://www.biorxiv.org/content/early/2022/03/17/2022.03.16.484643&lt;/em&gt;&lt;/li&gt;&lt;/ol&gt;]]&amp;gt;&amp;lt;![CDATA[Salmon in the Loop]]&amp;gt;https://thegradient.pub/salmon-in-the-loop/64dfbfba93571d5c8c15419aSat, 16 Dec 2023 17:00:36 GMT&lt;!--&amp;lt;![CDATA[&lt;img src="https://thegradient.pub/content/images/2023/08/DALL-E-2023-08-21-18.52.00---frothing-waves-in-the-middle-of-the-spillway-of-a-hydroelectric-dam--shown-by-a-bridge-across-a-river-with-many-arches--with-a-fish-jumping-downstream.png" alt="Salmon in the Loop"&gt;&lt;p&gt;One of the most fascinating problems that a computer scientist may be lucky enough to encounter is a complex sociotechnical problem in a field going through the process of digital transformation. For me, that was fish counting. Recently, I worked as a consultant in a subdomain of environmental science focused on counting fish that pass through large hydroelectric dams. Through this overarching project, I learned about ways to coordinate and manage human-in-the-loop dataset production, as well as the complexities and vagaries of how to think about and share progress with stakeholders.&lt;/p&gt;&lt;h2 id="background"&gt;Background&lt;/h2&gt;&lt;p&gt;Let&amp;#x2019;s set the stage. Large hydroelectric dams are subject to Environmental Protection Act regulations through the Federal Energy Regulatory Commission (FERC). FERC is an independent agency of the United States government that regulates the transmission and wholesale sale of electricity across the United States. The commission has jurisdiction over a wide range of electric power activities and is responsible for issuing licenses and permits for the construction and operation of hydroelectric facilities, including dams. These licenses and permits ensure that hydroelectric facilities are safe and reliable, and that they do not have a negative impact on the environment or other stakeholders. In order to obtain a license or permit from FERC, hydroelectric dam operators must submit detailed plans and studies demonstrating that their facility meets regulations. This process typically involves extensive review and consultation with other agencies and stakeholders. If a hydroelectric facility is found to be in violation of any set standards, FERC is responsible for enforcing compliance with all applicable regulations via sanctions, fines, or lease termination--resulting in a loss of the right to generate power.&lt;/p&gt;&lt;p&gt;Hydroelectric dams are essentially giant batteries. They generate power by building up a large reservoir of water on one side and directing that water through turbines in the body of the dam. Typically, a hydroelectric dam requires lots of space to store water on one side of it, which means they tend to be located away from population centers. The conversion process from potential to kinetic energy generates large amounts of electricity, and the amount of pressure and force generated is disruptive to anything that lives in or moves through the waterways&amp;#x2014;especially fish.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img src="https://lh3.googleusercontent.com/3s1Nf_YKJJ6RYku2imhRa7EpXZkbB5fb6R2_EG5shYdVdyRXjVvXZaDq4soTbdbh0TJ-7ch4I4DJ59Fs94dJaJCwnJQCYdLSl9ehw_-92xerbFob67m2N8A8a5NSrxROJS2hN4HNNp0DhYd69-AEGYk" class="kg-image" alt="Salmon in the Loop" loading="lazy" width="429" height="248"&gt;&lt;figcaption&gt;&lt;em&gt;Simple diagram illustrating how hydroelectric power is generated (Tennessee Valley Authority)&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;It is also worth noting that the waterways were likely disrupted substantially when the dam was built, leading to behavioral or population-level changes in the fish species of the area. This is of great concern to the Pacific Northwest in particular, as hydropower is the predominant power generation means for the region (Bonneville Power Administration). Fish populations are constantly moving upstream and downstream and hydropower dams can act as barriers that block their passage, leading to reduced spawning. In light of the risks to fish, hydropower dams are subject to constraints on the amount of power they can generate and must show that they are not killing fish in large numbers or otherwise disrupting the rhythms of their lives, especially because the native salmonid species of the region are already threatened or endangered (Salmon Status). &lt;/p&gt;&lt;p&gt;To demonstrate compliance with FERC regulations, large hydroelectric dams are required to routinely produce data which shows that their operational activities do not interfere with endangered fish populations in aggregate. Typically, this is done by performing fish passage studies. A fish passage study can be conducted many different ways, but boils down to one primary dataset upon which everything is based: a fish count. Fish are counted as they pass through the hydroelectric dam, using structures like fish ladders to make their way from the reservoir side to the stream side.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img src="https://lh6.googleusercontent.com/c3W3UHWOkSjkN4Yq2vbtCvEklUA6l5rPgIw53Y15-kt1rqeldlSByI45ruUdsfhy60MTEGWJJ7fPu0YuLMyXLDBIUFkPEYlGIzAipRu7qYGH0OFkrpQL3d3YTNlcqSBiW6y7VHpDCkYpqCXgXstn4Gs" class="kg-image" alt="Salmon in the Loop" loading="lazy" width="594" height="390"&gt;&lt;figcaption&gt;&lt;em&gt;A fish ladder at John Day Dam, how fish often ascend and pass through a dam (Delgado)&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Fish counts can be conducted visually&amp;#x2014;-a person trained in fish identification watches the fish pass, incrementing the count as they move upstream. As a fish is counted, observers impart additional classifications outside of species of fish, such as whether there was some kind of obvious illness or injury, if the fish is hatchery-origin or wild, and so on. These differences between fish are subtle and require close monitoring and verification, since the attribute in question (a clipped adipose fin, a scratched midsection) may only be visible briefly when the fish swims by. As such, fish counting is a specialized job that requires expertise in identifying and classifying different species of fish, as well as knowledge of their life stages and other characteristics. The job is physically demanding, as it typically involves working in remote locations away from city centers, and it can be challenging to perform accurately under the difficult environmental conditions found at hydroelectric dams&amp;#x2013;poor lighting, unregulated temperatures, and other circumstances inhospitable to humans.&lt;/p&gt;&lt;p&gt;These modes of data collection are great, but there are varying degrees of error that could be imparted through their recording. For example, some visual fish counts are documented with pen and paper, leading to incorrect counts through transcription error; or there can be disputes about the classification of a particular species. Different dam operators collect fish counts with varying degrees of granularity (some collect hourly, some daily, some monthly) and seasonality (some collect only during certain migration patterns called &amp;#x201C;runs&amp;#x201D;). After collection and validation, organizations correlate this data with operational information produced by the dam in an attempt to see if any activities of the dam have an adverse or beneficial effect on fish populations. Capturing these data piecemeal with different governing standards and levels of detail causes organizations to look for new efficiencies enabled by technology.&lt;/p&gt;&lt;h2 id="enter-computer-vision"&gt;Enter Computer Vision&lt;/h2&gt;&lt;p&gt;Some organizations are exploring the use of computer vision and machine learning to significantly automate fish counting. Since dam operators subje--&gt;</description><content:encoded>&amp;lt;![CDATA[The Gradient]]&amp;gt;https://thegradient.pub/https://thegradient.pub/favicon.pngThe Gradienthttps://thegradient.pub/Ghost 5.33Tue, 22 Jul 2025 02:01:08 GMT60&amp;lt;![CDATA[AGI Is Not Multimodal]]&amp;gt;"In projecting language back as the model for thought, we lose sight of the tacit embodied understanding that undergirds our intelligence." –Terry Winograd&lt;p&gt;The recent successes of generative AI models have convinced some that AGI is imminent. While these models appear to capture the essence of human&lt;/p&gt;]]&amp;gt;https://thegradient.pub/agi-is-not-multimodal/683fb98b77c3d76051ac142cWed, 04 Jun 2025 14:00:29 GMT"In projecting language back as the model for thought, we lose sight of the tacit embodied understanding that undergirds our intelligence." –Terry Winograd&lt;img alt="AGI Is Not Multimodal" src="https://thegradient.pub/content/images/2025/06/Gradient_Article_Art3-downscaled.png" /&gt;&lt;p&gt;The recent successes of generative AI models have convinced some that AGI is imminent. While these models appear to capture the essence of human intelligence, they defy even our most basic intuitions about it. They have emerged not because they are thoughtful solutions to the problem of intelligence, but because they &lt;em&gt;scaled&lt;/em&gt; effectively on hardware we already had. Seduced by the fruits of scale, some have come to believe that it provides a clear pathway to AGI. The most emblematic case of this is the multimodal approach, in which massive modular networks are optimized for an array of modalities that, taken together, &lt;em&gt;appear&lt;/em&gt; general. However, I argue that this strategy is sure to fail in the near term; it will not lead to human-level AGI that can, e.g., perform sensorimotor reasoning, motion planning, and social coordination. Instead of trying to glue modalities together into a patchwork AGI, &lt;strong&gt;we should pursue approaches to intelligence that treat embodiment and interaction with the environment as primary&lt;/strong&gt;, and see modality-centered processing as emergent phenomena.&lt;/p&gt;&lt;p&gt;Preface: Disembodied definitions of Artificial General Intelligence — emphasis on &lt;em&gt;general&lt;/em&gt; — exclude crucial problem spaces that we should expect AGI to be able to solve. &lt;strong&gt;A true AGI must be general across all domains.&lt;/strong&gt; Any &lt;em&gt;complete&lt;/em&gt; definition must at least include the ability to solve problems that originate in physical reality, e.g. repairing a car, untying a knot, preparing food, etc. As I will discuss in the next section, &lt;strong&gt;what is needed for these problems is a form of intelligence that is fundamentally situated in something like a physical world model&lt;/strong&gt;. For more discussion on this, look out for &lt;em&gt;Designing an Intelligence&lt;/em&gt;. Edited by George Konidaris, MIT Press, forthcoming.&lt;br /&gt;&lt;/p&gt;&lt;h2 id="why-we-need-the-world-and-how-llms-pretend-to-understand-it"&gt;Why We Need the World, and How LLMs Pretend to Understand It&lt;/h2&gt;&lt;p&gt;TLDR: I first argue that &lt;strong&gt;true AGI needs a physical understanding of the world&lt;/strong&gt;, as many problems cannot be converted into a problem of symbol manipulation. It has been suggested by some that LLMs are learning a model of the world through next token prediction, but it is more likely that LLMs are learning bags of heuristics to predict tokens. This leaves them with a superficial understanding of reality and contributes to false impressions of their intelligence.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;The most shocking result of the predict-next-token objective is that it yields AI models that reflect a deeply human-like understanding of the world, despite having never observed it like we have.&lt;/strong&gt; This result has led to confusion about what it means to &lt;em&gt;understand language&lt;/em&gt; and even to &lt;em&gt;understand the world&lt;/em&gt; — something we have long believed to be a prerequisite for language understanding. &lt;strong&gt;One explanation for the capabilities of LLMs comes from &lt;/strong&gt;&lt;strong&gt;an emerging theory&lt;/strong&gt;&lt;strong&gt; suggesting that they induce models of the world through next-token prediction&lt;/strong&gt;. Proponents of this theory cite the prowess of SOTA LLMs on various benchmarks, the convergence of large models to similar internal representations, and their favorite rendition of the idea that “language mirrors the structure of reality,” a notion that has been espoused at least by Plato, Wittgenstein, Foucault, and Eco. While I’m generally in support of digging up esoteric texts for research inspiration, I’m worried that this metaphor has been taken too literally. Do LLMs really learn implicit models of the world? How could they otherwise be so proficient at language?&lt;/p&gt;&lt;p&gt;One source of evidence in favor of the LLM world modeling hypothesis is the Othello paper, wherein researchers were able to predict the board of an Othello game from the hidden states of a transformer model trained on sequences of &lt;em&gt;legal&lt;/em&gt; &lt;em&gt;moves&lt;/em&gt;. However, there are &lt;em&gt;many&lt;/em&gt; issues with generalizing these results to models of natural language. For one, whereas Othello moves can &lt;em&gt;provably&lt;/em&gt; be used to deduce the full state of an Othello board,&lt;strong&gt; we have no reason to believe that a complete picture of the physical world can be inferred by a linguistic description. &lt;/strong&gt;What sets the game of Othello apart from many tasks in the physical world is that &lt;strong&gt;Othello fundamentally resides in the land of symbols, and is merely implemented using physical tokens to make it easier for humans to play.&lt;/strong&gt; A full game of Othello can be played with just pen and paper, but one can’t, e.g., sweep a floor, do dishes, or drive a car with just pen and paper. To solve such tasks, you need some physical conception of the world beyond what humans can merely &lt;em&gt;say&lt;/em&gt; about it. Whether that conception of the world is encoded in a formal world model or, e.g., a value function is up for debate, but it is clear that &lt;strong&gt;there are many problems in the physical world that &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;cannot&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; be &lt;/strong&gt;&lt;strong&gt;fully represented by a system of symbols&lt;/strong&gt;&lt;strong&gt; and solved with mere symbol manipulation.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Another issue stated in Melanie Mitchell’s recent piece and supported by this paper, is that there is evidence that &lt;strong&gt;generative models can score remarkably well on sequence prediction tasks while failing to learn models of the worlds that created such sequence data&lt;/strong&gt;, e.g. by learning comprehensive sets of idiosyncratic heuristics. E.g., it was pointed out in this blog post that OthelloGPT learned sequence prediction rules that don’t actually hold for all possible Othello games, like “if the token for B4 does not appear before A4 in the input string, then B4 is empty.” While one can argue that it doesn’t matter &lt;em&gt;how&lt;/em&gt; a world model predicts the next state of the world, it should raise suspicion when that prediction reflects a better understanding of the training data than the underlying world that led to such data. This, unfortunately, is the central fault of the predict-next-token objective, which seeks only to retain information relevant to the prediction of the next token. &lt;strong&gt;If it can be done with something easier to learn than a world model, it likely will be.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;To claim without caveat that predicting the &lt;em&gt;effects of earlier symbols on later symbols&lt;/em&gt; requires a model of the world like the ones humans generate from perception would be to abuse the “world model” notion. Unless we disagree on what the world is, it should be clear that a &lt;em&gt;true&lt;/em&gt; world model can be used to predict the next state of the &lt;em&gt;physical&lt;/em&gt; world given a history of states. Similar world models, which predict high fidelity observations of the physical world, are leveraged in many subfields of AI including model-based reinforcement learning, task and motion planning in robotics, causal world modeling, and areas of computer vision to solve problems instantiated in physical reality. LLMs are simply not running physics simulations in their latent next-token calculus when they ask you if your person, place, or thing is bigger than a breadbox. In fact, I conjecture that &lt;strong&gt;the behavior of LLMs is not thanks to a learned world model, but to brute force memorization of incomprehensibly abstract rules governing the behavior of symbols, i.e. a model of &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;syntax&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Quick primer:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Syntax&lt;/strong&gt; is a subfield of linguistics that studies how words of various grammatical categories (e.g. parts of speech) are arranged together into sentences, which can be parsed into syntax trees. &lt;em&gt;Syntax studies the structure of sentences and the atomic parts of speech that compose them.&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;strong&gt;Semantics&lt;/strong&gt; is another subfield concerned with the literal meaning of sentences, e.g., compiling “I am feeling chilly” into the &lt;em&gt;idea&lt;/em&gt; that you are experiencing cold. &lt;em&gt;Semantics boils language down to literal meaning, which is information about the world or human experience.&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Pragmatics&lt;/strong&gt; studies the interplay of physical and conversational context on speech interactions, like when someone knows to close an ajar window when you tell them “I am feeling chilly.” &lt;em&gt;Pragmatics involves interpreting speech while reasoning about the environment and the intentions and hidden knowledge of other agents.&lt;/em&gt;&lt;/li&gt;&lt;/ul&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;Without getting too technical, there is intuitive evidence that somewhat separate systems of cognition are responsible for each of these linguistic faculties. Look no further than the capability for humans to generate syntactically well-formed sentences that have no semantic meaning, e.g. Chomsky’s famous sentence “Colorless green ideas sleep furiously,” or sentences with well-formed semantics that make no pragmatic sense, e.g. responding merely with “Yes, I can” when asked, “Can you pass the salt?” Crucially, &lt;strong&gt;it is the fusion of the disparate cognitive abilities underpinning them that coalesce into human language understanding.&lt;/strong&gt; For example, there isn’t anything syntactically wrong with the sentence, “The fridge is in the apple,” as a syntactic account of “the fridge” and “the apple” would categorize them as noun phrases that can be used to produce a sentence with the production rule, S → (NP “is in” NP). However, &lt;strong&gt;humans recognize an obvious semantic failure in the sentence that becomes apparent after attempting to reconcile its meaning with our understanding of reality&lt;/strong&gt;: we know that fridges are larger than apples, and could not be fit into them.&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;But what if you have never perceived the real world, yet still were trying to figure out whether the sentence was ill-formed? &lt;strong&gt;One solution could be to embed semantic information at the level of syntax&lt;/strong&gt;, e.g., by inventing new syntactic categories, NP&lt;sub&gt;the fridge&lt;/sub&gt; and NP&lt;sub&gt;the apple &lt;/sub&gt;, and a single new production rule that prevents semantic misuse: S → (NP&lt;sub&gt;the apple&lt;/sub&gt; “is in” NP&lt;sub&gt;the fridge &lt;/sub&gt;). While this strategy would no longer require grounded world knowledge about fridges and apples, e.g., &lt;strong&gt;it would require special grammar rules for every semantically well-formed construction… which is actually possible to learn given a massive corpus of natural language.&lt;/strong&gt; Crucially, this would not be the same thing as grasping semantics, which in my view is fundamentally about understanding the nature of the world.&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;Finding that LLMs have reduced problems of semantics and pragmatics into syntax would have profound implications on how we should view their intelligence. People often treat language proficiency as a proxy for general intelligence by, e.g., strongly associating pragmatic and semantic understanding with the cognitive abilities that undergird them in humans. For example, someone who appears well-read and graceful in navigating social interactions is likely to score high in traits like sustained attention and theory of mind, which lie closer to measures of raw cognitive ability. In general, these proxies are reasonable for assessing a &lt;em&gt;person’s&lt;/em&gt; general intelligence, but not an LLM’s, as the apparent linguistic skills of LLMs could come from entirely separate mechanisms of cognition.&lt;/p&gt;&lt;h2 id="the-bitter-lesson-revisited"&gt;The Bitter Lesson Revisited&lt;/h2&gt;&lt;p&gt;TLDR: Sutton’s Bitter Lesson has sometimes been interpreted as meaning that making &lt;em&gt;any&lt;/em&gt; assumptions about the structure of AI is a mistake. This is both unproductive and a misinterpretation; it is precisely when humans think deeply about the structure of intelligence that major advancements occur. Despite this, scale maximalists have implicitly suggested that multimodal models can be a structure-agnostic framework for AGI. Ironically, today’s multimodal models contradict Sutton’s Bitter Lesson by making implicit assumptions about the structure of individual modalities and how they should be sewn together. &lt;strong&gt;In order to build AGI, we must either think deeply about how to unite existing modalities, or dispense with them altogether in favor of an interactive and embodied cognitive process.&lt;/strong&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="AGI Is Not Multimodal" class="kg-image" height="733" src="https://thegradient.pub/content/images/2025/06/ChatGPT-Image-May-21--2025--11_31_12-AM-copy.png" width="1024" /&gt;&lt;/figure&gt;&lt;p&gt;The paradigm that led to the success of LLMs is marked primarily by &lt;em&gt;scale&lt;/em&gt;, not efficiency. We have effectively trained a pile of one trillion ants for one billion years to mimic the form and function of a Formula 1 race car; eventually it gets there, but wow was the process inefficient. This analogy nicely captures a debate between structuralists, who want to build things like "wheels" and "axles" into AI systems, and scale maximalists, who want more ants, years, and F1 races to train on. Despite many decades of structuralist study in linguistics, the unstructured approaches of scale maximalism have yielded far better ant-racecars in recent years. This was most notably articulated by Rich Sutton — a recent recipient of the Turing Award along with Andy Barto for their work in Reinforcement Learning — in his piece “The Bitter Lesson.”&lt;/p&gt;&lt;p&gt;[W]e should build in only the meta-methods that can find and capture this arbitrary complexity… Essential to these methods is that they can find good approximations, but the search for them should be by our methods, not by us. We want AI agents that can discover like we can, not which contain what we have discovered. - Rich Sutton&lt;/p&gt;&lt;p&gt;Sutton’s argument is that methods that leverage computational resources will outpace methods that do not, and that any structure for problem-solving built as an inductive bias into AI will hinder it from learning better solutions. &lt;strong&gt;This is a compelling argument&lt;/strong&gt; &lt;strong&gt;that I believe has been seriously misinterpreted by some as implying that making &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;any&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; assumptions about structure is a false step.&lt;/strong&gt; It is, in fact, human intuition that was responsible for many significant advancements in the development of SOTA neural network architectures. For example, Convolutional Neural Networks made an assumption about translation invariance for pattern recognition in images and kickstarted the modern field of deep learning for computer vision; the attention mechanism of Transformers made an assumption about the long-distance relationships between symbols in a sentence that made ChatGPT possible and had nearly everyone drop their RNNs; and 3D Gaussian Splatting made an assumption about the solidity of physical objects that made it more performant than NeRFs. Potentially none of these methodological assumptions apply to the entire domain of &lt;em&gt;possible&lt;/em&gt; scenes, images, or token streams, but they do for the specific ones that humans have curated and formed structural intuitions about. Let’s not forget that humans have co-evolved with the environments that these datasets are drawn from.&lt;/p&gt;&lt;p&gt;The real question is how we might heed Sutton’s Bitter Lesson in our development of AGI. The scale maximalist approach worked for LLMs and LVMs (large vision models) because we had natural deposits of text and image data, but &lt;strong&gt;an analogous application of scale maximalism to AGI would require forms of embodiment data that we simply don’t have. &lt;/strong&gt;One solution to this data scarcity issue extends the generative modeling paradigm to multimodal modeling — encompassing language, vision, and action — &lt;strong&gt;with the hope that a general intelligence can be built by summing together general models of narrow modalities.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;There are multiple issues with this approach. First, &lt;strong&gt;there are deep connections between modalities that are unnaturally severed in the multimodal setting&lt;/strong&gt;, making the problem of concept synthesis ever more difficult. In practice, uniting modalities often involves pre-training dedicated neural modules for each modality and joining them together into a joint embedding space. In the early days, this was achieved by nudging the embeddings of, e.g. (language, vision, action) tuples to converge to similar latent vectors of meaning, a vast oversimplification of the kinds of relationships that may exist between modalities. One can imagine, e.g., captioning an image at various levels of abstraction, or implementing the same linguistic instruction with different sets of physical actions. Such one-to-many relationships suggest that a contrastive embedding objective is not suitable.&lt;/p&gt;&lt;p&gt;While modern approaches do not make such stringent assumptions about how modalities should be united, they still universally encode percepts from all modalities (e.g. text, images) into the same latent space. Intuitively, it would seem that such latent spaces could serve as common conceptual ground across modalities, analogous to a space of human concepts. However, these latent spaces do not cogently capture all information relevant to a concept, and instead rely on modality-specific decoders to flesh out important details. &lt;strong&gt;The “meaning” of a percept is not &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;in&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; the vector it is encoded as, but in the way relevant decoders process this vector into meaningful outputs. &lt;/strong&gt;As long as various encoders and decoders are subject to modality-specific training objectives, “meaning” will be decentralized and potentially inconsistent across modalities, especially as a result of pre-training. This is not a recipe for the formation of coherent concepts.&lt;/p&gt;&lt;p&gt;Furthermore, it is not clear that today’s modalities are an appropriate partitioning of the observation and action spaces for an embodied agent. It is not obvious that, e.g., images and text should be represented as separate observation streams, nor text production and motion planning as separate action capabilities. &lt;strong&gt;The human capacities for reading, seeing, speaking, and moving are ultimately mediated by overlapping cognitive structures.&lt;/strong&gt; &lt;strong&gt;Making structural assumptions about how modalities ought to be processed is likely to hinder the discovery of more fundamental cognition &lt;/strong&gt;that is responsible for processing data in all modalities. One solution would be to consolidate unnaturally partitioned modalities into a unified data representation. This would encourage networks to learn intelligent processes that generalize across modalities. Intuitively, &lt;strong&gt;a model that can understand the visual world as well as humans can&lt;/strong&gt; — including everything from human writing to traffic signs to visual art — &lt;strong&gt;should not make a serious architectural distinction between images and text. &lt;/strong&gt;Part of the reason why VLMs can’t, e.g., count the number of letters in a word is because they can’t &lt;em&gt;see&lt;/em&gt; what they are writing.&lt;/p&gt;&lt;p&gt;Finally, the &lt;strong&gt;learn-from-scale approach trains models to copy the conceptual structure of humans instead of learning the general capability to form novel concepts on their own.&lt;/strong&gt; Humans have spent hundreds of thousands of years refining concepts and passing them memetically through culture and language. Today’s models are trained only on the end result of this process: the present-day conceptual structures that make it into the corpus. By optimizing for the ultimate products of our intelligence, we have ignored the question of how those products were invented and discovered. Humans have a unique ability to form durable concepts from few examples and ascribe names to them, reason about them analogically, etc. While the in-context capabilities of today’s models can be impressive, they grow increasingly limited as tasks become more complex and stray further from the training data. &lt;strong&gt;The flexibility to form new concepts from experience is a foundational attribute of general intelligence&lt;/strong&gt;, we should think carefully about how it arises.&lt;/p&gt;&lt;p&gt;While structure-agnostic scale maximalism has succeeded in producing LLMs and LVMs that pass Turing tests, a multimodal scale maximalist approach to AGI will not bear similar fruit. &lt;strong&gt;Instead of pre-supposing structure in individual modalities, we should design a setting in which modality-specific processing emerges naturally.&lt;/strong&gt; For example, my recent paper on visual theory of mind saw abstract symbols naturally emerge from communication between image-classifying agents, blurring the lines between text and image processing. Eventually, we should hope to reintegrate as many features of intelligence as possible under the same umbrella. However, it is not clear whether there is genuine commercial viability in such an approach as long as scaling and fine-tuning narrow intelligence models solves commercial use-cases.&lt;/p&gt;&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;&lt;p&gt;The overall promise of scale maximalism is that a Frankenstein AGI can be sewed together using general models of narrow domains. I argue that this is extremely unlikely to yield an AGI that feels complete in its intelligence. If we intend to continue reaping the streamlined efficiency of modality-specific processing, we must be intentional in how modalities are united — ideally drawing from human intuition and classical fields of study, e.g. this work from MIT. Alternatively, we can re-formulate learning as an embodied and interactive process where disparate modalities naturally fuse together. We could do this by, e.g., processing images, text, and video using the same perception system and producing actions for generating text, manipulating objects, and navigating environments using the same action system. What we will lose in efficiency we will gain in flexible cognitive ability.&lt;/p&gt;&lt;p&gt;In a sense, the most challenging mathematical piece of the AGI puzzle has already been solved: the discovery of universal function approximators. What’s left is to inventory the functions we need and determine how they ought to be arranged into a coherent whole. This is a conceptual problem, not a mathematical one.&lt;/p&gt;&lt;hr /&gt;&lt;h2 id="acknowledgements"&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;I would like to thank Lucas Gelfond, Daniel Bashir, George Konidaris, and my father, Joseph Spiegel, for their thoughtful and thorough feedback on this work. Thanks to Alina Pringle for the wonderful illustration made for this piece.&lt;/p&gt;&lt;h2 id="author-bio"&gt;Author Bio&lt;/h2&gt;&lt;p&gt;Benjamin is a PhD candidate in Computer Science at Brown University. He is interested in models of language understanding that ground meaning to elements of structured decision-making. For more info see his personal website.&lt;/p&gt;&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;pre&gt;&lt;code&gt;Benjamin A. Spiegel, "AGI Is Not Multimodal", The Gradient, 2025.
&lt;/code&gt;&lt;/pre&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;pre&gt;&lt;code&gt;@article{spiegel2025agi,
    author = {Benjamin A. Spiegel},
    title = {AGI Is Not Multimodal},
    journal = {The Gradient},
    year = {2025},
    howpublished = {\url{https://thegradient.pub/agi-is-not-multimodal},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;h2 id="references"&gt;References&lt;/h2&gt;&lt;p&gt;Andreas, Jacob. “Language Models, World Models, and Human Model-Building.” &lt;em&gt;Mit.edu&lt;/em&gt;, 2024, lingo.csail.mit.edu/blog/world_models/.&lt;/p&gt;&lt;p&gt;Belkin, Mikhail, et al. "Reconciling modern machine-learning practice and the classical bias–variance trade-off." &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt; 116.32 (2019): 15849-15854.&lt;/p&gt;&lt;p&gt;Bernhard Kerbl, et al. “3D Gaussian Splatting for Real-Time Radiance Field Rendering.” &lt;em&gt;ACM Transactions on Graphics&lt;/em&gt;, vol. 42, no. 4, 26 July 2023, pp. 1–14, https://doi.org/10.1145/3592433.&lt;/p&gt;&lt;p&gt;Chomsky, Noam. 1965. Aspects of the theory of syntax. Cambridge, Massachusetts: MIT Press.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Designing an Intelligence&lt;/em&gt;. Edited by George Konidaris, MIT Press, 2026.&lt;/p&gt;&lt;p&gt;Emily M. Bender and Alexander Koller. 2020. Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data. In &lt;em&gt;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&lt;/em&gt;, pages 5185–5198, Online. Association for Computational Linguistics.&lt;/p&gt;&lt;p&gt;Eye on AI. “The Mastermind behind GPT-4 and the Future of AI | Ilya Sutskever.” &lt;em&gt;YouTube&lt;/em&gt;, 15 Mar. 2023, www.youtube.com/watch?v=SjhIlw3Iffs&amp;amp;list=PLpdlTIkm0-jJ4gJyeLvH1PJCEHp3NAYf4&amp;amp;index=64. Accessed 18 May 2025.&lt;/p&gt;&lt;p&gt;Frank, Michael C. “Bridging the data gap between children and large language models.” &lt;em&gt;Trends in cognitive sciences&lt;/em&gt; vol. 27,11 (2023): 990-992. doi:10.1016/j.tics.2023.08.007&lt;/p&gt;&lt;p&gt;Garrett, Caelan Reed, et al. "Integrated task and motion planning." &lt;em&gt;Annual review of control, robotics, and autonomous systems&lt;/em&gt; 4.1 (2021): 265-293.APA&lt;/p&gt;&lt;p&gt;Goodhart, C.A.E. (1984). Problems of Monetary Management: The UK Experience. In: Monetary Theory and Practice. Palgrave, London. https://doi.org/10.1007/978-1-349-17295-5_4&lt;/p&gt;&lt;p&gt;Hooker, Sara. The hardware lottery. Commun. ACM 64, 12 (December 2021), 58–65. https://doi.org/10.1145/3467017&lt;/p&gt;&lt;p&gt;Huh, Minyoung, et al. "The Platonic Representation Hypothesis." &lt;em&gt;Forty-first International Conference on Machine Learning&lt;/em&gt;. 2024.&lt;/p&gt;&lt;p&gt;Kaplan, Jared, et al. "Scaling laws for neural language models." &lt;em&gt;arXiv preprint arXiv:2001.08361&lt;/em&gt; (2020).&lt;/p&gt;&lt;p&gt;Lake, Brenden M. et al. “Building Machines That Learn and Think like People.” &lt;em&gt;Behavioral and Brain Sciences&lt;/em&gt; 40 (2017): e253. Web.&lt;/p&gt;&lt;p&gt;Li, Kenneth, et al. "Emergent world representations: Exploring a sequence model trained on a synthetic task." &lt;em&gt;ICLR&lt;/em&gt; (2023).&lt;/p&gt;&lt;p&gt;Luiten, Jonathon, Georgios, Kopanas, Bastian, Leibe, Deva, Ramanan. "Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis." &lt;em&gt;3DV&lt;/em&gt;. 2024.&lt;/p&gt;&lt;p&gt;Mao, Jiayuan, Chuang, Gan, Pushmeet, Kohli, Joshua B., Tenenbaum, Jiajun, Wu. "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision." &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. 2019.&lt;/p&gt;&lt;p&gt;Mitchell, Melanie. “LLMs and World Models, Part 1.” &lt;em&gt;Substack.com&lt;/em&gt;, AI: A Guide for Thinking Humans, 13 Feb. 2025, aiguide.substack.com/p/llms-and-world-models-part-1. Accessed 18 May 2025.&lt;/p&gt;&lt;p&gt;Mu, Norman. “Norman Mu | the Myth of Data Inefficiency in Large Language Models.” &lt;em&gt;Normanmu.com&lt;/em&gt;, 14 Feb. 2025, www.normanmu.com/2025/02/14/data-inefficiency-llms.html. Accessed 18 May 2025.&lt;/p&gt;&lt;p&gt;Newell, Allen, and Herbert A. Simon. “Computer Science as Empirical Inquiry: Symbols and Search.” &lt;em&gt;Communications of the ACM&lt;/em&gt;, vol. 19, no. 3, 1 Mar. 1976, pp. 113–126, https://doi.org/10.1145/360018.360022.&lt;/p&gt;&lt;p&gt;Peng, Hao, et al. “When Does In-Context Learning Fall Short and Why? A Study on Specification-Heavy Tasks.” &lt;em&gt;ArXiv.org&lt;/em&gt;, 2023, arxiv.org/abs/2311.08993.&lt;/p&gt;&lt;p&gt;Spiegel, Benjamin, et al. “Visual Theory of Mind Enables the Invention of Early Writing Systems.” &lt;em&gt;CogSci&lt;/em&gt;, 2025, arxiv.org/abs/2502.01568.&lt;/p&gt;&lt;p&gt;Sutton, Richard S. &lt;em&gt;Introduction to Reinforcement Learning&lt;/em&gt;. Cambridge, Mass, Mit Press, 04-98, 1998.&lt;/p&gt;&lt;p&gt;Vafa, Keyon, et al. "Evaluating the world model implicit in a generative model." &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 37 (2024): 26941-26975.&lt;/p&gt;&lt;p&gt;Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia (December 2017). "Attention is All you Need". In I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett (ed.). &lt;em&gt;31st Conference on Neural Information Processing Systems (NIPS)&lt;/em&gt;. Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc. arXiv:1706.03762.&lt;/p&gt;&lt;p&gt;Winograd, Terry. “Thinking Machines: Can There Be? Are We?” &lt;em&gt;The Boundaries of Humanity: Humans, Animals, Machines&lt;/em&gt;, edited by James Sheehan and Morton Sosna, Berkeley: University of California Press, 1991, pp. 198–223.&lt;/p&gt;&lt;p&gt;Wu, Shangda, et al. "Beyond language models: Byte models are digital world simulators." &lt;em&gt;arXiv preprint arXiv:2402.19155&lt;/em&gt; (2024). &lt;/p&gt;]]&amp;gt;&amp;lt;![CDATA[Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research]]&amp;gt;What is the Role of Mathematics in Modern Machine Learning?&lt;p&gt;The past decade has witnessed a shift in how progress is made in machine learning. Research involving carefully designed and mathematically principled architectures result in only marginal improvements while compute-intensive and engineering-first efforts that scale to ever larger training sets&lt;/p&gt;]]&amp;gt;https://thegradient.pub/shape-symmetry-structure/673686c693571d5c8c155078Sat, 16 Nov 2024 16:46:15 GMTWhat is the Role of Mathematics in Modern Machine Learning?&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" src="https://thegradient.pub/content/images/2024/11/DALL-E-2024-11-15-15.40.52---Create-an-abstract-image-that-illustrates-how-ReLU-based-neural-networks-shatter-input-space-into-numerous-polygonal-regions--each-behaving-like-a-lin.webp" /&gt;&lt;p&gt;The past decade has witnessed a shift in how progress is made in machine learning. Research involving carefully designed and mathematically principled architectures result in only marginal improvements while compute-intensive and engineering-first efforts that scale to ever larger training sets and model parameter counts result in remarkable new capabilities unpredicted by existing theory. Mathematics and statistics, once the primary guides of machine learning research, now struggle to provide immediate insight into the latest breakthroughs. This is not the first time that empirical progress in machine learning has outpaced more theory-motivated approaches, yet the magnitude of recent advances has forced us to swallow the bitter pill of the “Bitter Lesson” yet again [1].&lt;/p&gt;&lt;p&gt;This shift has prompted speculation about mathematics’ diminished role in machine learning research moving forward. It is already evident that mathematics will have to share the stage with a broader range of perspectives (for instance, biology which has deep experience drawing conclusions about irreducibly complex systems or the social sciences as AI is integrated ever more deeply into society). The increasingly interdisciplinary nature of machine learning should be welcomed as a positive development by all researchers.&lt;/p&gt;&lt;p&gt;However, we argue that mathematics remains as relevant as ever; its role is simply evolving. For example, whereas mathematics might once have primarily provided theoretical guarantees on model performance, it may soon be more commonly used for post-hoc explanations of empirical phenomena observed in model training and performance–a role analogous to one that it plays in physics. Similarly, while mathematical intuition might once have guided the design of handcrafted features or architectural details at a granular level, its use may shift to higher-level design choices such as matching architecture to underlying task structure or data symmetries.&lt;/p&gt;&lt;p&gt;None of this is completely new. Mathematics has always served multiple purposes in machine learning. After all, the translation equivariant convolutional neural network, which exemplifies the idea of architecture matching data symmetries mentioned above is now over 40 years old. What’s changing are the kinds of problems where mathematics will have the greatest impact and the ways it will most commonly be applied.&lt;/p&gt;&lt;p&gt;An intriguing consequence of the shift towards scale is that it has broadened the scope of the fields of mathematics applicable to machine learning. “Pure” mathematical domains such as topology, algebra, and geometry, are now joining the more traditionally applied fields of probability theory, analysis, and linear algebra. These pure fields have grown and developed over the last century to handle high levels of abstraction and complexity, helping mathematicians make discoveries about spaces, algebraic objects, and combinatorial processes that at first glance seem beyond human intuition. These capabilities promise to address many of the biggest challenges in modern deep learning. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;In this article we will explore several areas of current research that demonstrate the enduring ability of mathematics to guide the process of discovery and understanding in machine learning.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="372" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcr6ZUCNH3oKGK9XQzvDZOs1kJhvjym4RrMAlENx0OHrK-IBsQcBQQW2wDKu2_g2tNJxXVd32BI5llBopCmD-IAFV9zfhjvQ2ek5rIOgUMqwhK-qFhri2iaU718yl4BzISTanzZt9a2Rix04c6GUpdFR4Co?key=h8RUuDFEFKnzGnrKx9gMkg" width="529" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 1: Mathematics can illuminate the ways that ReLU-based neural networks shatter input space into countless polygonal regions, in each of which the model behaves like a linear map [2, 3, 4]. These decompositions create beautiful patterns. (Figure made with SplineCam [5]).&lt;/em&gt;&lt;/p&gt;&lt;h3 id="describing-an-elephant-from-a-pin-prick"&gt;Describing an Elephant from a Pin Prick&lt;/h3&gt;&lt;p&gt;Suppose you are given a 7 billion parameter neural network with 50 layers and are asked to analyze it; how would you begin? The standard procedure would be to calculate relevant performance statistics. For instance, the accuracy on a suite of evaluation benchmarks. In certain situations, this may be sufficient. However, deep learning models are complex and multifaceted. Two computer vision models with the same accuracy may have very different generalization properties to out-of-distribution data, calibration, adversarial robustness, and other “secondary statistics” that are critical in many real-world applications. Beyond this, all evidence suggests that to build a complete scientific understanding of deep learning, we will need to venture beyond evaluation scores. Indeed, just as it is impossible to capture all the dimensions of humanity with a single numerical quantity (e.g., IQ, height), trying to understand a model by one or even several statistics alone is fundamentally limiting.&lt;/p&gt;&lt;p&gt;One difference between understanding a human and understanding a model is that we have easy access to all model parameters and all the individual computations that occur in a model. Indeed, by extracting a model’s hidden activations we can directly trace the process by which a model converts raw input into a prediction. Unfortunately, the world of hidden activations is far less hospitable than that of simple model performance statistics. Like the initial input, hidden activations are usually high dimensional, but unlike input data they are not structured in a form that humans can understand. If we venture into even higher dimensions, we can try to understand a model through its weights directly. Here, in the space of model weights, we have the freedom to move in millions to billions of orthogonal directions from a single starting point. How do we even begin to make sense of these worlds?&lt;/p&gt;&lt;p&gt;There is a well-known fable in which three blind men each feel a different part of an elephant. The description that each gives of the animal is completely different, reflecting only the body part that that man felt. We argue that unlike the blind men who can at least use their hand to feel a substantial part of one of the elephant’s body parts, current methods of analyzing the hidden activations and weights of a model are akin to trying to describe the elephant from the touch of a single pin.&lt;/p&gt;&lt;h3 id="tools-to-characterize-what-we-cannot-visualize"&gt;Tools to Characterize What We Cannot Visualize&lt;/h3&gt;&lt;p&gt;Despite the popular perception that mathematicians exclusively focus on solving problems, much of research mathematics involves understanding the right questions to ask in the first place. This is natural since many of the objects that mathematicians study are so far removed from everyday experience that we start with very limited intuition for what we can hope to actually understand. Substantial effort is often required to build up tools that will enable us to leverage our existing intuition and achieve tractable results that increase our understanding. The concept of a &lt;em&gt;rotation &lt;/em&gt;provides a nice example of this situation since these are very familiar in 2- and 3-dimensions, but become less and less accessible to everyday intuition as their dimension grows larger. In this latter case, the differing perspectives provided by pure mathematics become more and more important to gaining a more holistic perspective on what these actually are. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;Those who know a little linear algebra will remember that rotations generalize to higher dimensions and that in $n$-dimensions they can be realized by $n \times n$ orthogonal matrices with determinant $1$. The set of these are commonly written as $SO(n)$ and called the &lt;em&gt;special orthogonal group&lt;/em&gt;. Suppose we want to understand the set of all $n$-dimensional rotations. There are many complementary approaches to doing this. We can explore the linear algebraic structure of all matrices in $SO(n)$ or study $SO(n)$ based on how each element behaves as an operator acting on $\mathbb{R}^n$.&lt;/p&gt;&lt;p&gt;Alternatively, we can also try to use our innate spatial intuition to understand $SO(n)$. This turns out to be a powerful perspective in math. In any dimension $n$, $SO(n)$ is a geometric object called a &lt;em&gt;manifold&lt;/em&gt;. Very roughly, a space that locally looks like Euclidean space, but which may have twists, holes, and other non-Euclidean features when we zoom out. Indeed, whether we make it precise or not, we all have a sense of whether two rotations are “close” to each other. For example, the reader would probably agree that $2$-dimensional rotations of $90^\circ$ and $91^\circ$ “feel” closer than rotations of $90^\circ$ and $180^\circ$. When $n=2$, one can show that the set of all rotations is geometrically “equivalent” to a $1$-dimensional circle. So, much of what we know about the circle can be translated to $SO(2)$.&lt;/p&gt;&lt;p&gt;What happens when we want to study the geometry of rotations in $n$-dimensions for $n &amp;gt; 3$? If $n = 512$ (a latent space for instance), this amounts to studying a manifold in $512^2$-dimensional space. Our visual intuition is seemingly useless here since it is not clear how concepts that are familiar in 2- and 3-dimensions can be utilized in $512^2$-dimensions. Mathematicians have been confronting the problem of understanding the un-visualizable for hundreds of years. One strategy is to find generalizations of familiar spatial concepts from $2$ and $3$-dimensions to $n$-dimensions that connect with our intuition.&lt;/p&gt;&lt;p&gt;This approach is already being used to better understand and characterize experimental observations about the space of model weights, hidden activations, and input data of deep learning models. We provide a taste of such tools and applications here:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;Intrinsic Dimension: &lt;/em&gt;Dimension is a concept that is familiar not only from our experience in the spatial dimensions that we can readily access, 1-, 2-, and 3-dimensions, but also from more informal notions of “degrees of freedom” in everyday systems such as driving a car (forward/back, turning the steering wheel either left or right). The notion of dimension arises naturally in the context of machine learning where we may want to capture the number of independent ways in which a dataset, learned representation, or collection of weight matrices actually vary.&lt;p&gt;In formal mathematics, the definitions of dimension depend on the kind of space one is studying but they all capture some aspect of this everyday intuition. As a simple example, if I walk along the perimeter of a circle, I am only able to move forward and backward, and thus the dimension of this space is $1$. For spaces like the circle which are manifolds, dimension can be formally defined by the fact that a sufficiently small neighborhood around each point looks like a subset of some Euclidean space $\mathbb{R}^k$. We then say that the manifold is $k$-dimensional. If we zoom in on a small segment of the circle, it almost looks like a segment of $\mathbb{R} = \mathbb{R}^1$, and hence the circle is $1$-dimensional.&lt;/p&gt;&lt;p&gt;The manifold hypothesis posits that many types of data (at least approximately) live on a low-dimensional manifold even though they are embedded in a high-dimensional space. If we assume that this is true, it makes sense that the dimension of this underlying manifold, called the intrinsic dimension of the data, is one way to describe the complexity of the dataset. Researchers have estimated intrinsic dimension for common benchmark datasets, showing that intrinsic dimension appears to be correlated to the ease with which models generalize from training to test sets [6], and can explain differences in model performance and robustness in different domains such as medical images [7]. Intrinsic dimension is also a fundamental ingredient in some proposed explanations of data scaling laws [8, 9], which underlie the race to build ever bigger generative models.&lt;/p&gt;&lt;p&gt;Researchers have also noted that the intrinsic dimension of hidden activations tend to change in a characteristic way as information passes through the model [10, 11] or over the course of the diffusion process [12]. These and other insights have led to the use of intrinsic dimension in detection of adversarial examples [13], AI-generated content [14], layers where hidden activations contain the richest semantic content [11], and hallucinations in generative models [15].&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;Curvature&lt;/em&gt;: While segments of the circle may look “straight” when we zoom up close enough, their curvature means that they will never be exactly linear as a straight line is. The notion of curvature is a familiar one and once formalized, it offers a way of rigorously measuring the extent to which the area around a point deviates from being linear. Care must be taken, however. Much of our everyday intuition about curvature assumes a single dimension. On manifolds with dimension $2$ or greater, there are multiple, linearly independent directions that we can travel away from a point and each of these may have a different curvature (in the $1$-dimensional sense). As a result, there are a range of different generalizations of curvature for higher-dimensional spaces, each with slightly different properties.&lt;p&gt;The notion of curvature has played a central role in deep learning, especially with respect to the loss landscape where changes in curvature have been used to analyze training trajectories [16]. Curvature is also central to an intriguing phenomenon known as the ‘edge of stability’, wherein the curvature of the loss landscape over the course of training increases as a function of learning rate until it hovers around the point where the training run is close to becoming unstable [17]. In another direction, curvature has been used to calculate the extent that model predictions change as input changes. For instance, [18] provided evidence that higher curvature in decision boundaries correlates with higher vulnerability to adversarial examples and suggested a new regularization term to reduce this. Finally, motivated by work in neuroscience, [19] presented a method that uses curvature to highlight interesting differences in representation between the raw training data and a neural network’s internal representation. A network may stretch and expand parts of the input space, generating regions of high curvature as it magnifies the representation of training examples that have a higher impact on the loss function.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;Topology&lt;/em&gt;: Both dimension and curvature capture local properties of a space that can be measured by looking at the neighborhood around a single point. On the other hand, the most notable feature of our running example, the circle, is neither its dimension nor its curvature, but rather the fact that it is circular. We can only see this aspect by analyzing the whole space at once. Topology is the field of mathematics that focuses on such “global” properties.&lt;p&gt;Topological tools such as homology, which counts the number of holes in a space, has been used to illuminate the way that neural networks process data, with [20] showing that deep learning models “untangle” data distributions, reducing their complexity layer by layer. Versions of homology have also been applied to the weights of networks to better understand their structural features, with [21] showing that such topological statistics can reliably predict optimal early-stopping times. Finally, since topology provides frameworks that capture the global aspects of a space, it has proved a rich source of ideas for how to design networks that capture higher order relationships within data, leading to a range of generalizations of graph neural networks built on top of topological constructions [22, 23, 24, 25].&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;While the examples above have each been useful for gaining insight into phenomena related to deep learning, they were all developed to address challenges in other fields. We believe that a bigger payoff will come when the community uses the geometric paradigm described here to build new tools specifically designed to address the challenges that deep learning poses. Progress in this direction has already begun. Think for instance of linear mode connectivity which has helped us to better understand the loss landscape of neural networks [26] or work around the linear representation hypothesis which has helped to illuminate the way that concepts are encoded in the latent space of large language models [27]. One of the most exciting occurrences in mathematics is when the tools from one domain provide unexpected insight in another. Think of the discovery that Riemannian geometry provides some of the mathematical language needed for general relativity. We hope that a similar story will eventually be told for geometry and topology’s role in deep learning.&lt;/p&gt;&lt;h3 id="symmetries-in-data-symmetries-in-models"&gt;Symmetries in data, symmetries in models&lt;br /&gt;&lt;/h3&gt;&lt;p&gt;Symmetry is a central theme in mathematics, allowing us to break a problem into simpler components that are easier to solve. Symmetry has long played an important role in machine learning, particularly computer vision. In the classic dog vs. cat classification task for instance, an image that contains a dog continues to contain a dog regardless of whether we move the dog from one part of the image to another, whether we rotate the dog, or whether we reflect it. We say that the task is &lt;em&gt;invariant&lt;/em&gt; to image translation, rotation, and reflection.&lt;/p&gt;&lt;p&gt;The notion of symmetry is mathematically encoded in the concept of a &lt;em&gt;group&lt;/em&gt;, which is a set $G$ equipped with a binary operation $\star$ that takes two elements of $G$, $g_1$, $g_2$ as input and produces a third $g_1\star g_2$ as output. You can think of the integers $\mathbb{Z}$ with the binary operation of addition ($\star = +$) or the non-zero real numbers with the binary operation of multiplication ($\star = \times$). The set of $n$-dimensional rotations, $SO(n)$, also forms a group. The binary operation takes two rotations and returns a third rotation that is defined by simply applying the first rotation and then applying the second.&lt;/p&gt;&lt;p&gt;Groups satisfy axioms that ensure that they capture familiar properties of symmetries. For example, for any symmetry transformation, there should be an inverse operation that undoes the symmetry. If I rotate a circle by $90^{\circ}$, then I can rotate it back by $-90^{\circ}$ and return to where I started. Notice that not all transformations satisfy this property. For instance, there isn’t a well-defined inverse for downsampling an image. Many different images downsample to the same (smaller) image.&lt;/p&gt;&lt;p&gt;In the previous section we gave two definitions of $SO(n)$: the first was the geometric definition, as rotations of $\mathbb{R}^n$, and the second was as a specific subset of $n \times n$ matrices. While the former definition may be convenient for our intuition, the latter has the benefit that linear algebra is something that we understand quite well at a computational level. The realization of an abstract group as a set of matrices is called a &lt;em&gt;linear representation&lt;/em&gt; and it has proven to be one of the most fruitful methods of studying symmetry. It is also the way that symmetries are usually leveraged when performing computations (for example, in machine learning).&lt;/p&gt;&lt;p&gt;We saw a few examples of symmetries that can be found in the data of a machine learning task, such as the translation, rotation, and reflection symmetries in computer vision problems. Consider the case of a segmentation model. If one rotates an input image by $45^{\circ}$ and then puts it through the model, we will hope that we get a $45^{\circ}$ rotation of the segmentation prediction for the un-rotated image (this is illustrated in 1). After all, we haven’t changed the content of the image.&lt;br /&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="323" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXd1UiuD880gmdVrtjEKvGPBHIr0dvdBsrLXAnxUFz6_KQNLyMekhxrSR2ROn-H8O3780yoKbJvF0tUEVZSEdsuDfbB7kSGw_CFCqsKzjC6-wpxN5dxLQd-e4g7qMsKnc8BCX1pw7Qh0-I9hsgY9EInhpROs?key=h8RUuDFEFKnzGnrKx9gMkg" width="534" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 2: The concept of rotation equivariance illustrated for a segmentation model. One gets the same output regardless of whether one rotates first and then applies the network or applies the network and then rotates.&lt;/em&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="273" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXe_QPixL6292p6Bz5yj_Hep_ypc6qrj-3q3Y7uIte0R5Nsc2ZPxNmSJOheOHohJY_0VbDi3LlyNSR61t94bHDfgTnJx0ssvyzU9KMtGLUKoqoiviKKTxpZR77Bb8VIzhkzd0Vxhspif10w8DnS3eWbjqwhW?key=h8RUuDFEFKnzGnrKx9gMkg" width="522" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 3: Equivariance holds when taking the top path (applying the network first and then the symmetry action) gives the same result as taking the bottom path (applying the symmetry transformation and then the network).&lt;/em&gt;&lt;/p&gt;&lt;p&gt;This property of a function (including neural networks), that applying a symmetry transformation before the function yields the same result as applying the symmetry transformation after the function is called &lt;em&gt;equivariance&lt;/em&gt; and can be captured by the diagram in Figure 3. The key point is that we get the same result whether we follow the upper path (applying the network first and then applying the group action) as when we follow the lower path (applying the group first and then applying the network). Conveniently, the concept of invariance, where applying a symmetry operation to input has no effect on the output of the function is a special case of equivariance where the action on the output space is defined to be trivial (applying symmetry actions does nothing).&lt;/p&gt;&lt;p&gt;Invariance and equivariance in deep learning models can be beneficial for a few reasons. Firstly, such a model will yield more predictable and consistent results across symmetry transformations. Secondly, through equivariance we can sometimes simplify the learning process with fewer parameters (compare the number of parameters in a convolutional neural network and an MLP of similar performance) and fewer modes of variation to learn in the data (a rotation invariant image classifier only needs to learn one orientation of each object rather than all possible orientations).&lt;/p&gt;&lt;p&gt;But how do we ensure that our model is equivariant? One way is to build our network with layers that are equivariant by design. By far the most well-known example of this is the convolutional neural network, whose layers are (approximately) equivariant to image translation. This is one reason why using a convolutional neural network for dog vs cat classification doesn’t require learning to recognize a dog at every location in an image as it might with an MLP. With a little thought, one can often come up with layers which are equivariant to a specific group. Unfortunately, being constrained to equivariant layers that we find in an ad-hoc manner often leaves us with a network with built-in equivariance but limited expressivity.&lt;/p&gt;&lt;p&gt;Fortunately, for most symmetry groups arising in machine learning, representation theory offers a comprehensive description of all possible linear equivariant maps. Indeed, it is a beautiful mathematical fact that all such maps are built from atomic building blocks called &lt;em&gt;irreducible representations&lt;/em&gt;. Happily, in many cases, the number of these irreducible representations is finite. Understanding the irreducible representations of a group can be quite powerful. Those familiar with the ubiquitous discrete Fourier transform (DFT) of a sequence of length $n$ are already familiar with the irreducible representations of one group, the cyclic group generated by a rotation by $360 ^{\circ}/n$ (though we note that moving between the description we give here and the description of the DFT found in the signal processing literature takes a little thought).&lt;/p&gt;&lt;p&gt;There is now a rich field of research in deep learning that uses group representations to systematically build expressive equivariant architectures. Some examples of symmetries that have been particularly well-studied include: rotation and reflection of images [28, 29, 30, 31], 3-dimensional rotation and translation of molecular structures [32] or point clouds [33], and permutations for learning on sets [34] or nodes of a graph [35]. Encoding equivariance to more exotic symmetries has also proven useful for areas such as theoretical physics [36] and data-driven optimization [37].&lt;/p&gt;&lt;p&gt;Equivariant layers and other architectural approaches to symmetry awareness are a prime example of using mathematics to inject high-level priors into a model. Do these approaches represent the future of learning in the face of data symmetries? Anecdotally, the most common approach to learning on data with symmetries continues to be using enough training data and enough data augmentation for the model to learn to handle the symmetries on its own. Two years ago, the author would have speculated that these latter approaches only work for simple cases, such as symmetries in 2-dimensions, and will be outperformed by models which are equivariant by design when symmetries become more complex. Yet, we continue to be surprised by the power of scale. After all, AlphaFold3 [38] uses a non-equivariant architecture despite learning on data with several basic symmetries. We speculate that there may be a threshold on the ratio of symmetry complexity on the one hand and the amount of training data on the other, that determines whether built-in equivariance will outperform learned equivariance [39, 40].&lt;br /&gt;&lt;/p&gt;&lt;p&gt;If this is true, we can expect to see models move away from bespoke equivariant architectures as larger datasets become available for a specific application. At the same time, since compute will always be finite, we predict that there will be some applications with exceptionally complex symmetries that will always require some built-in priors (for example, AI for math or algorithmic problems). Regardless of where we land on this spectrum, mathematicians can look forward to an interesting comparison of the ways humans inject symmetry into models vs the way that models learn symmetries on their own [41, 42].&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="129" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcJa_1Ow2zrMVuk1hJiOJtTJBq5bH7zyogibZ5fqQu85ERGFEjcX4jRn7r_rnZvTrCdpN5OzeVQUBLu60DJP_aIR4uoHq33tRMcoPAUf7qumOeJLreCkttvqtQEssCh90UwlbWkzBoK79FV54R6ncO2c_Ij?key=h8RUuDFEFKnzGnrKx9gMkg" width="572" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 4: A cartoon illustrating why adding a permutation and its inverse before and after a pointwise nonlinearity produces an equivalent model (even though the weights will be different). Since permutations can be realized by permutation matrices, the crossed arrows on the right can be merged into the fully-connected layer.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Of course, symmetry is not only present in data but also in the models themselves. For instance, the activations of hidden layers of a network are invariant to permutation. We can permute activations before entering the non-linearity and if we un-permute them afterward, the model (as a function) does not change (Figure 4). This means that we have an easy recipe for generating an exponentially large number of networks that have different weights but behave identically on data. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;While simple, this observation produces some unexpected results. There is evidence, for instance, that while the loss landscape of neural networks is highly non-convex, it may be much less non-convex when we consider all networks that can be produced through this permutation operation as equivalent [43, 44]. This means that your network and my network may not be connected by a linear path of low loss, but such a path may exist between your network and a permutation of my network. Other research has looked at whether it may be possible to use symmetries to accelerate optimization by ‘teleporting’ a model to a more favorable location in the loss landscape [45, 46]. Finally, permutation symmetries also provide one type of justification for an empirical phenomenon where individual neurons in a network tend to encode more semantically meaningful information than arbitrary linear combinations of such neurons [47].&lt;/p&gt;&lt;h3 id="taming-complexity-with-abstraction"&gt;Taming Complexity with Abstraction&lt;/h3&gt;&lt;p&gt;When discussing symmetry, we used the diagram in Figure 3 to define equivariance. One of the virtues of this approach is that we never had to specify details about the input data or architecture that we used. The spaces could be vector spaces and the maps linear transformations, they could be neural networks of a specific architecture, or they could just be sets and arbitrary functions between them–the definition is valid for each. This &lt;em&gt;diagrammatic&lt;/em&gt; point of view, which looks at mathematical constructions in terms of the composition of maps between objects rather than the objects themselves, has been very fruitful in mathematics and is one gateway to the subject known as &lt;em&gt;category theory&lt;/em&gt;. Category theory is now the lingua franca in many areas of mathematics since it allows mathematicians to translate definitions and results across a wide range of contexts.&lt;/p&gt;&lt;p&gt;Of course, deep learning is at its core all about function composition, so it is no great leap to try and connect it to the diagrammatic tradition in mathematics. The focus of function composition in the two disciplines is different, however. In deep learning we take simple layers that alone lack expressivity and compose them together to build a model capable of capturing the complexity of real-world data. With this comes the tongue-in-cheek demand to “stack more layers!”. Category theory instead tries to find a universal framework that captures the essence of structures appearing throughout mathematics. This allows mathematicians to uncover connections between things that look very different at first glance. For instance, category theory gives us the language to describe how the topological structure of a manifold can be encoded in groups via homology or homotopy theory.&lt;/p&gt;&lt;p&gt;It can be an interesting exercise to try to find a diagrammatic description of familiar constructions like the product of two sets $X$ and $Y$. Focusing our attention on maps rather than objects we find that what characterizes $X \times Y$ is the existence of the two canonical projections $\pi_1$ and $\pi_2$, the former sending $(x,y) \mapsto x$ and $(x,y) \mapsto y$ (at least in more familiar settings where $X$ and $Y$ are, for example, sets). Indeed, the &lt;em&gt;product &lt;/em&gt;$X \times Y$ (regardless of whether $X$ and $Y$ are sets, vectors spaces, etc.) is the unique object such that for any $Z$ with maps $f_1: Z \rightarrow X$ and $f_2: Z \rightarrow Y$, there is a map $h: Z \rightarrow X \times Y$ that satisfies the commutative diagram in Figure 5.&lt;/p&gt;&lt;p&gt;While this construction is a little involved for something as familiar as a product it has the remarkable property that it allows us to define a “product” even when there is no &amp;nbsp;underlying set structure (that is, those settings where we cannot resort to defining $X \times Y$ as the set of pairs of $(x,y)$ for $x \in X$ and $y \in Y$).&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="277" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXd0afUER-JEqT9BBW0z5ip7HSPD_ORKlpxTaQQFpep7MZF3DKfhgca3XbrZ2aGGTTnxcyOD3csHF1hdODeSXFx-nC63Mlw2etuY9xtM-AUvec4aIZKJK0hl2QiuxyJPzmlr18GbJA?key=h8RUuDFEFKnzGnrKx9gMkg" width="624" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 5: The commutative diagram that describes a product $X \times Y$. For any $Z$ with maps $f_1: Z \rightarrow X$ and $f_2: Z \rightarrow Y$, there exists a unique map $h: Z \rightarrow X \times Y$ such that $f_1 = \pi_1 \circ h$ and $f_2 = \pi_2 \circ h$ where $\pi_1$ and $\pi_2$ are the usual projection maps from $X \times Y$ to $X$ and $X \times Y$ to $Y$ respectively.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;One can reasonably argue that diagrammatic descriptions of well-known constructions, like products, are not useful for the machine learning researcher. After all, we already know how to form products in all of the spaces that come up in machine learning. On the other hand, there are more complicated examples where diagrammatics mesh well with the way we build neural network architectures in practice.&lt;br /&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="352" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXf30eZdcoTcrjBuYEo6BUjm4gmw8fvcY9kLpDsspW0qPoIVu6LN5mfd1ks5qiMtf9J1DyPNDtzDDLDpxVi7n5j62DxlIfkwyo5V4gAC7MeeMpUaDzOMgsU4Mqjrs7fUXL-hc_BeqPd9Upu6L0wmKnDzkop4?key=h8RUuDFEFKnzGnrKx9gMkg" width="624" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 6: Fiber bundles capture the notion that a space might locally look like a product but globally have twists in it.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Fiber bundles are a central construction in geometry and topology that capture the notion that a space may locally look like a product but may have twists that break this product structure globally. Compare the cylinder with the Möbius band. We can build both of these by starting with a circle and taking a product with the line segment $(0,1)$. In the case of the cylinder, this really is just (topologically) the product of the circle and the segment $(0,1)$, but to form the Möbius band we must add an additional twist that breaks the product structure. In these examples, the circle is called the &lt;em&gt;base &lt;/em&gt;space and $(0,1)$ is called the &lt;em&gt;fiber&lt;/em&gt;. While only the cylinder is a true product, both the cylinder and the Möbius band are fiber bundles. Here is another way of thinking about a fiber bundle. A fiber bundle is a union of many copies of the fiber parametrized by the base space. In the Möbius band/cylinder example, each point on the circle carries its own copy of $(0,1)$.&lt;/p&gt;&lt;p&gt;We drew inspiration from this latter description of fiber bundles when we were considering a conditional generation task in the context of a problem in materials science. Since the materials background is somewhat involved, we’ll illustrate the construction via a more pedestrian, animal-classification analogue. Let $M$ be the manifold of all possible images containing a single animal. We can propose to decompose the variation in elements of $M$ into two parts, the species of animal in the image and everything else, where the latter could mean differences in background, lighting, pose, image quality, etc. One might want to explore the distribution of one of these factors of variation while fixing the other. For instance, we might want to fix the animal species and explore the variation we get in background, pose, etc. For example, comparing the variation in background for two different species of insect may tell the entomologist about the preferred habitat for different types of beetles.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="421" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfLfnOQw_uLzm58bcucM5zOzGLHKzbX8hyQU2muIPl994v1GQN0sfMwQgSjFwsaCDetRHW8WR_T71pjNX7waqch44PwUY6Dv8egfzRlOmo6e0BbDagYv99K6tMnvVeTAIb9ww9bT_3Ukcs4k7xHx-BH7cxR?key=h8RUuDFEFKnzGnrKx9gMkg" width="624" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 7: A cartoon visualizing how the set of all animal images could be decomposed into a local product of animal species and other types of variation.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;One might hope to solve this problem by learning an encoding of $M$ into a product space $X_1 \times X_2$ where $X_1$ is a discrete set of points corresponding to animal species and $X_2$ is a space underlying the distribution of all other possible types of variation for a fixed species of animal. Fixing the species would then amount to choosing a specific element $x_1$ from $X_1$ and sampling from the distribution on $X_2$. The product structure of $X_1 \times X_2$ allows us to perform such independent manipulations of $X_1$ and $X_2$. On the other hand, products are rigid structures that impose strong, global topological assumptions on the real data distribution. We found that even on toy problems, it was hard to learn a good map from the raw data distribution to the product-structured latent space defined above. Given that fiber bundles are more flexible and still give us the properties we wanted from our latent space, we designed a neural network architecture to learn a fiber bundle structure on a data distribution [48].&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" class="kg-image" height="240" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdrgVDyhD0JXBm12Gh1NstAjVx3fSk8vM3Mg_3JGi6JpK3PYTWUpmgzW_BgmEOMeZahkdrzWEw2ThViUKXnEGFobRORcOMgifUin2kJY3-zFIq4fbj-4QO6x7ALnwn5qLU880r1raMaFC2yqn6RVyDPGEk?key=h8RUuDFEFKnzGnrKx9gMkg" width="415" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Figure 8: The commutative diagram describing a fiber bundle. The map $\pi$ projects from neighborhoods of the total space to the base space, $U$ is a local neighborhood of the base space, and $F$ &amp;nbsp;is the fiber. The diagram says that each point in the base space has a neighborhood $U$ &amp;nbsp;such that when we lift this to the bundle, we get something that is homeomorphic (informally, equivalent) to the product of the neighborhood &amp;nbsp;and the fiber. But this product structure may not hold globally over the whole space.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;But how do we go from the abstract definition of a fiber bundle above to a neural network architecture that we can code up on a computer. It turns out there is a succinct diagrammatic definition of a fiber bundle (Figure 8) that can serve as a convenient template to build up an architecture from. We were able to proceed in a relatively naïve fashion, taking each of the maps in the diagram and building a corresponding stack of layers. The diagram itself then told us how to compose each of these components together. The commutativity of the diagram was engineered through a term in the loss function that ensures that $\pi = \text{proj}_1 \circ \varphi$. There were also some conditions on $\varphi$ and $\pi$ (such as the bijectivity of $\phi$) that needed to be engineered. Beyond this, we were surprised at the amount of flexibility we had. This is useful since it means this process is largely agnostic to data modality.&lt;/p&gt;&lt;p&gt;This is an elementary example of how the diagrammatic tradition in mathematics can provide us with a broader perspective on the design of neural networks, allowing us to connect deep structural principles with large-scale network design without having to specify small-scale details that might be problem dependent. Of course, all this fails to draw from anything beyond the surface of what the categorical perspective has to offer. Indeed, category theory holds promise as a unified framework to connect much of what appears and is done in machine learning [49].&lt;/p&gt;&lt;h3 id="conclusion"&gt;Conclusion&lt;br /&gt;&lt;/h3&gt;&lt;p&gt;In the mid-twentieth century, Eugene Wigner marveled at the “the unreasonable effectiveness of mathematics” as a framework for not only describing existing physics but also anticipating new results in the field [50]. A mantra more applicable to recent progress in machine learning is “the unreasonable effectiveness of data” [51] and compute. This could appear to be a disappointing situation for mathematicians who might have hoped that machine learning would be as closely intertwined to advanced mathematics as physics is. However, as we’ve demonstrated, while mathematics may not maintain the same role in machine learning research that it has held in the past, the success of scale actually opens new paths for mathematics to support progress in machine learning research. These include:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Providing powerful tools for deciphering the inner workings of complex models&lt;/li&gt;&lt;li&gt;Offering a framework for high-level architectural decisions that leave the details to the learning algorithm&lt;/li&gt;&lt;li&gt;Bridging traditionally isolated domains of mathematics like topology, abstract algebra, and geometry with ML and data science applications.&lt;br /&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Should the way things have turned out surprise us? Perhaps not, given that machine learning models ultimately reflect the data they are trained on and in most cases this data comes from fields (such as natural language or imagery) which have long resisted parsimonious mathematical models. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;Yet, this situation is also an opportunity for mathematics. Performant machine learning models may provide a gateway for mathematical analysis of a range of fields that were previously inaccessible. It’s remarkable for instance that trained word embeddings transform semantic relationships into algebraic operations on vectors in Euclidean space (for instance, ‘Italian’ - ‘Italy’ + ‘France’ = ‘French’). Examples like this hint at the potential for mathematics to gain a foothold in complex, real-world settings by studying the machine learning models that have trained on data from these settings.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;As more and more of the data in the world is consumed and mathematicised by machine learning models, it will be an increasingly interesting time to be a mathematician. The challenge now lies in adapting our mathematical toolkit to this new landscape, where empirical breakthroughs often precede theoretical understanding. By embracing this shift, mathematics can continue to play a crucial, albeit evolving, role in shaping the future of machine learning.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;The author would like to thank Darryl Hannan for help with figures, Davis Brown, Charles Godfrey, and Scott Mahan for useful feedback on drafts, as well as the staff of the Gradient for useful conversations and help editing this article. For resources and events around the growing community of mathematicians and computer scientists using topology, algebra, and geometry (TAG) to better understand and build more robust machine learning systems, please visit us at &lt;/em&gt;&lt;em&gt;https://www.tagds.com&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;&lt;h2 id="references"&gt;References&lt;/h2&gt;&lt;p&gt;[1] Richard Sutton. "The bitter lesson". In: &lt;em&gt;Incomplete Ideas (blog)&lt;/em&gt; 13.1 (2019), p. 38.&lt;/p&gt;&lt;p&gt;[2] Guido F Montufar et al. "On the number of linear regions of deep neural networks". In: &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 27 (2014).&lt;/p&gt;&lt;p&gt;[3] Boris Hanin and David Rolnick. "Complexity of linear regions in deep networks". In: &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;. PMLR. 2019, pp. 2596–2604.&lt;/p&gt;&lt;p&gt;[4] J Elisenda Grigsby and Kathryn Lindsey. "On transversality of bent hyperplane arrangements and the topological expressiveness of ReLU neural networks". In: &lt;em&gt;SIAM Journal on Applied Algebra and Geometry&lt;/em&gt; 6.2 (2022), pp. 216–242.&lt;/p&gt;&lt;p&gt;[5] Ahmed Imtiaz Humayun et al. "Splinecam: Exact visualization and characterization of deep network geometry and decision boundaries". In: &lt;em&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition&lt;/em&gt;. 2023, pp. 3789–3798.&lt;/p&gt;&lt;p&gt;[6] Phillip Pope et al. "The intrinsic dimension of images and its impact on learning". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2104.08894 (2021).&lt;/p&gt;&lt;p&gt;[7] Nicholas Konz and Maciej A Mazurowski. "The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2401.08865 (2024).&lt;/p&gt;&lt;p&gt;[8] Yasaman Bahri et al. "Explaining neural scaling laws". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2102.06701 (2021).&lt;/p&gt;&lt;p&gt;[9] Utkarsh Sharma and Jared Kaplan. "A neural scaling law from the dimension of the data manifold". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2004.10802 (2020).&lt;/p&gt;&lt;p&gt;[10] Alessio Ansuini et al. "Intrinsic dimension of data representations in deep neural networks". In: &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 32 (2019).&lt;/p&gt;&lt;p&gt;[11] Lucrezia Valeriani et al. "The geometry of hidden representations of large transformer models". In: &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 36 (2024).&lt;/p&gt;&lt;p&gt;[12] Henry Kvinge, Davis Brown, and Charles Godfrey. "Exploring the Representation Manifolds of Stable Diffusion Through the Lens of Intrinsic Dimension". In: &lt;em&gt;ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;[13] Xingjun Ma et al. "Characterizing adversarial subspaces using local intrinsic dimensionality". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:1801.02613 (2018).&lt;/p&gt;&lt;p&gt;[14] Peter Lorenz, Ricard L Durall, and Janis Keuper. "Detecting images generated by deep diffusion models using their local intrinsic dimensionality". In: &lt;em&gt;Proceedings of the IEEE/CVF International Conference on Computer Vision&lt;/em&gt;. 2023, pp. 448–459.&lt;/p&gt;&lt;p&gt;[15] Fan Yin, Jayanth Srinivasa, and Kai-Wei Chang. "Characterizing truthfulness in large language model generations with local intrinsic dimension". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2402.18048 (2024).&lt;/p&gt;&lt;p&gt;[16] Justin Gilmer et al. "A loss curvature perspective on training instabilities of deep learning models". In: &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. 2021.&lt;/p&gt;&lt;p&gt;[17] Jeremy Cohen et al. "Gradient descent on neural networks typically occurs at the edge of stability". In: &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. 2020.&lt;/p&gt;&lt;p&gt;[18] Seyed-Mohsen Moosavi-Dezfooli et al. "Robustness via curvature regularization, and vice versa". In: &lt;em&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition&lt;/em&gt;. 2019, pp. 9078–9086.&lt;/p&gt;&lt;p&gt;[19] Francisco Acosta et al. "Quantifying extrinsic curvature in neural manifolds". In: &lt;em&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition&lt;/em&gt;. 2023, pp. 610–619.&lt;/p&gt;&lt;p&gt;[20] Gregory Naitzat, Andrey Zhitnikov, and Lek-Heng Lim. "Topology of deep neural networks". In: &lt;em&gt;Journal of Machine Learning Research&lt;/em&gt; 21.184 (2020), pp. 1–40.&lt;/p&gt;&lt;p&gt;[21] Bastian Rieck et al. "Neural persistence: A complexity measure for deep neural networks using algebraic topology". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:1812.09764 (2018).&lt;/p&gt;&lt;p&gt;[22] Mustafa Hajij, Kyle Istvan, and Ghada Zamzmi. "Cell complex neural networks". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2010.00743 (2020).&lt;/p&gt;&lt;p&gt;[23] Cristian Bodnar. "Topological deep learning: graphs, complexes, sheaves". PhD thesis. 2023.&lt;/p&gt;&lt;p&gt;[24] Jakob Hansen and Robert Ghrist. "Toward a spectral theory of cellular sheaves". In: &lt;em&gt;Journal of Applied and Computational Topology&lt;/em&gt; 3.4 (2019), pp. 315–358.&lt;/p&gt;&lt;p&gt;[25] Yifan Feng et al. "Hypergraph neural networks". In: &lt;em&gt;Proceedings of the AAAI Conference on Artificial Intelligence&lt;/em&gt;. Vol. 33. 01. 2019, pp. 3558–3565.&lt;/p&gt;&lt;p&gt;[26] Felix Draxler et al. "Essentially no barriers in neural network energy landscape". In: &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;. PMLR. 2018, pp. 1309–1318.&lt;/p&gt;&lt;p&gt;[27] Kiho Park, Yo Joong Choe, and Victor Veitch. "The linear representation hypothesis and the geometry of large language models". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2311.03658 (2023).&lt;/p&gt;&lt;p&gt;[28] Taco Cohen and Max Welling. "Group equivariant convolutional networks". In: &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;. PMLR. 2016, pp. 2990–2999.&lt;/p&gt;&lt;p&gt;[29] Maurice Weiler, Fred A Hamprecht, and Martin Storath. "Learning steerable filters for rotation equivariant cnns". In: &lt;em&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/em&gt;. 2018, pp. 849–858.&lt;/p&gt;&lt;p&gt;[30] Daniel E Worrall et al. "Harmonic networks: Deep translation and rotation equivariance". In: &lt;em&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/em&gt;. 2017, pp. 5028–5037.&lt;/p&gt;&lt;p&gt;[31] Diego Marcos et al. "Rotation equivariant vector field networks". In: &lt;em&gt;Proceedings of the IEEE International Conference on Computer Vision&lt;/em&gt;. 2017, pp. 5048–5057.&lt;/p&gt;&lt;p&gt;[32] Alexandre Duval et al. "A Hitchhiker's Guide to Geometric GNNs for 3D Atomic Systems". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2312.07511 (2023).&lt;/p&gt;&lt;p&gt;[33] Nathaniel Thomas et al. "Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:1802.08219 (2018).&lt;/p&gt;&lt;p&gt;[34] Manzil Zaheer et al. "Deep sets". In: &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 30 (2017).&lt;/p&gt;&lt;p&gt;[35] Vıctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. "E (n) equivariant graph neural networks". In: &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;. PMLR. 2021, pp. 9323–9332.&lt;/p&gt;&lt;p&gt;[36] Denis Boyda et al. "Sampling using SU (N) gauge equivariant flows". In: &lt;em&gt;Physical Review D&lt;/em&gt; 103.7 (2021), p. 074504.&lt;/p&gt;&lt;p&gt;[37] Hannah Lawrence and Mitchell Tong Harris. "Learning Polynomial Problems with SL(2,\mathbb {R}) −Equivariance". In: &lt;em&gt;The Twelfth International Conference on Learning Representations&lt;/em&gt;. 2023.&lt;/p&gt;&lt;p&gt;[38] Josh Abramson et al. "Accurate structure prediction of biomolecular interactions with AlphaFold 3". In: &lt;em&gt;Nature&lt;/em&gt; (2024), pp. 1–3.&lt;/p&gt;&lt;p&gt;[39] Scott Mahan et al. "What Makes a Machine Learning Task a Good Candidate for an Equivariant Network?" In: &lt;em&gt;ICML 2024 Workshop on Geometry-grounded Representation Learning and Generative Modeling&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;[40] Johann Brehmer et al. "Does equivariance matter at scale?" In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2410.23179 (2024).&lt;/p&gt;&lt;p&gt;[41] Chris Olah et al. "Naturally Occurring Equivariance in Neural Networks". In: &lt;em&gt;Distill&lt;/em&gt; (2020). https://distill.pub/2020/circuits/equivariance. doi: 10.23915/distill.00024.004.&lt;/p&gt;&lt;p&gt;[42] Giovanni Luca Marchetti et al. "Harmonics of Learning: Universal Fourier Features Emerge in Invariant Networks". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2312.08550 (2023).&lt;/p&gt;&lt;p&gt;[43] Rahim Entezari et al. "The role of permutation invariance in linear mode connectivity of neural networks". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2110.06296 (2021).&lt;/p&gt;&lt;p&gt;[44] Samuel K Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. "Git re-basin: Merging models modulo permutation symmetries". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2209.04836 (2022).&lt;/p&gt;&lt;p&gt;[45] Bo Zhao et al. "Symmetry teleportation for accelerated optimization". In: &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 35 (2022), pp. 16679–16690.&lt;/p&gt;&lt;p&gt;[46] Bo Zhao et al. "Improving Convergence and Generalization Using Parameter Symmetries". In: &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2305.13404 (2023).&lt;/p&gt;&lt;p&gt;[47] Charles Godfrey et al. "On the symmetries of deep learning models and their internal representations". In: &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 35 (2022), pp. 11893–11905.&lt;/p&gt;&lt;p&gt;[48] Nico Courts and Henry Kvinge. "Bundle Networks: Fiber Bundles, Local Trivializations, and a Generative Approach to Exploring Many-to-one Maps". In: &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. 2021.&lt;/p&gt;&lt;p&gt;[49] Bruno Gavranović et al. "Position: Categorical Deep Learning is an Algebraic Theory of All Architectures". In: &lt;em&gt;Forty-first International Conference on Machine Learning&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;[50] Eugene P Wigner. "The unreasonable effectiveness of mathematics in the natural sciences". In: &lt;em&gt;Mathematics and Science&lt;/em&gt;. World Scientific, 1990, pp. 291–306.&lt;/p&gt;&lt;p&gt;[51] Alon Halevy, Peter Norvig, and Fernando Pereira. "The unreasonable effectiveness of data". In: &lt;em&gt;IEEE Intelligent Systems&lt;/em&gt; 24.2 (2009), pp. 8–12.&lt;/p&gt;]]&amp;gt;&amp;lt;![CDATA[What's Missing From LLM Chatbots: A Sense of Purpose]]&amp;gt;LLM-based chatbots’ capabilities have been advancing every month. These improvements are mostly measured by benchmarks like MMLU, HumanEval, and MATH (e.g. sonnet 3.5, gpt-4o). However, as these measures get more and more saturated, is user experience increasing in proportion to these scores? If we envision a future]]&amp;gt;https://thegradient.pub/dialog/66c6733993571d5c8c154fb1Mon, 09 Sep 2024 17:28:48 GMT&lt;p&gt;LLM-based chatbots’ capabilities have been advancing every month. These improvements are mostly measured by benchmarks like MMLU, HumanEval, and MATH (e.g. sonnet 3.5, gpt-4o). However, as these measures get more and more saturated, is user experience increasing in proportion to these scores? If we envision a future of human-AI collaboration rather than AI replacing humans, the current ways of measuring dialogue systems may be insufficient because they measure in a non-interactive fashion.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Why does purposeful dialogue matter?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Purposeful dialogue refers to a multi-round user-chatbot conversation that centers around a goal or intention. The goal could range from a generic one like “harmless and helpful” to more specific roles like “travel planning agent”, “psycho-therapist” or “customer service bot.”&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Travel planning is a simple, illustrative example. Our preferences, fellow travelers’ preference, and all the complexities of real-world situations make transmitting all information in one pass way too costly. However, if multiple back-and-forth exchanges of information are allowed, only important information gets selectively exchanged. Negotiation theory offers an analogy of this—iterative bargaining yields better outcomes than a take-it-or-leave-it offer. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;In fact, sharing information is only one aspect of dialogue. In Terry Winograd’s words: “All language use can be thought of as a way of activating procedures within the hearer.” We can think of each utterance as a deliberate action that one party takes to alter the world model of the other. What if both parties have more complicated, even hidden goals? In this way, purposeful dialogue provides us with a way of formulating human-AI interactions as a collaborative game, where the goal of chatbot is to help humans achieve certain goals. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;This might seem like an unnecessary complexity that is only a concern for academics. However, purposeful dialogue could be beneficial even for the most hard-nosed, product-oriented research direction like code generation. Existing coding benchmarks mostly measure performances in a one-pass generation setting; however, for AI to automate solving ordinary Github issues (like in SWE-bench), it’s unlikely to be achieved by a single action—the AI needs to communicate back and forth with human software engineers to make sure it understands the correct requirements, ask for missing documentation and data, and even ask humans to give it a hand if needed. In a similar vein to pair programming, this could reduce the defects of code but without the burden of increasing man-hours. &lt;/p&gt;&lt;p&gt;&lt;br /&gt;Moreover, with the introduction of turn-taking, many new possibilities can be unlocked. As interactions become long-term and memory is built, the chatbot can gradually update user profiles. It can also adapt to their preferences. Imagine a personal assistant (e.g., IVA, Siri) that, through daily interaction, learns your preferences and intentions. It can read your resources of new information automatically (e.g., twitter, arxiv, Slack, NYT) and provide you with a morning news summary according to your preferences. It can draft emails for you and keep improving by learning from your edits.&lt;/p&gt;&lt;p&gt;In a nutshell, meaningful interactions between people rarely begin with complete strangers and conclude in just one exchange. Humans naturally interact with each other through multi-round dialogues and adapt accordingly throughout the conversation. However, doesn’t that seem exactly the opposite of predicting the next token, which is the cornerstone of modern LLMs? Below, let’s take a look at the makings of dialogue systems.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;How were/are dialogue systems made?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Let's jump back to the 1970s, when Roger Schank introduced his "restaurant script" as a kind of dialogue system [1]. This script breaks down the typical restaurant experience into steps like entering, ordering, eating, and paying, each with specific scripted utterances. Back then, every piece of dialogue in these scenarios was carefully planned out, enabling AI systems to mimic realistic conversations. ELIZA, a Rogerian psychotherapist simulator, and PARRY, a system mimicking a paranoid individual, were two other early dialogue systems until the dawn of machine learning.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Compare this approach to the LLM-based dialogue system today, it seems mysterious how models trained to predict the next token could do anything at all with engaging in dialogues. Therefore, let’s take a close examination of how dialogue systems are made, with an emphasis on how the dialogue format comes into play:&lt;/p&gt;&lt;p&gt;(1) Pretraining: a sequence model is trained to predict the next token on a gigantic corpus of mixed internet texts. The compositions may vary but they are predominantly news, books, Github code, with a small blend of forum-crawled data such as from Reddit, Stack Exchange, which may contain dialogue-like data.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="What's Missing From LLM Chatbots: A Sense of Purpose" class="kg-image" height="288" src="https://thegradient.pub/content/images/2024/08/unnamed.png" width="512" /&gt;&lt;figcaption&gt;Table of the pretraining data mixture from llama technical report&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;(2) Introduce dialogue formatting: because the sequence model only processes strings, while the most natural representation of dialogue history is a structured index of system prompts and past exchanges, a certain kind of formatting must be introduced for the purpose of conversion. Some Huggingface tokenizers provide this method called tokenizer.apply_chat_template for the convenience of users. The exact formatting differs from model to model, but it usually involves guarding the system prompts with &amp;lt;system&amp;gt; or &amp;lt;INST&amp;gt; in the hope that the pretrained model could allocate more attention weights to them. The system prompt plays a significant role in adapting language models to downstream applications and ensuring its safe behavior (we will talk more in the next section). Notably, the choice of the format is arbitrary at this step—pretraining corpus doesn’t follow this format.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="What's Missing From LLM Chatbots: A Sense of Purpose" class="kg-image" height="112" src="https://thegradient.pub/content/images/2024/08/image1.png" width="1398" /&gt;&lt;figcaption&gt;The context window of a chatbot&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;(3) RLHF: In this step, the chatbot is directly rewarded or penalized for generating desired or undesired answers. It’s worth noting that this is the first time the introduced dialogue formatting appears in the training data. RLHF is a &lt;em&gt;fine&lt;/em&gt;-tuning step not only because the data size is dwarfed in comparison to the pretraining corpus, but also due to the KL penalty and targeted weight tuning (e.g. Lora). Using Lecun’s analogy of cake baking, RLHF is only the small cherry on the top.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="What's Missing From LLM Chatbots: A Sense of Purpose" class="kg-image" height="580" src="https://thegradient.pub/content/images/2024/08/image5.png" width="1440" /&gt;&lt;figcaption&gt;Image from Yann Lecun’s slides&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2 id="how-consistent-are-existing-dialogue-systems-in-2024"&gt;How consistent are existing dialogue systems (in 2024)? &lt;br /&gt;&lt;/h2&gt;&lt;p&gt;The minimum requirement we could have for a dialogue system is that it can stay on the task we gave them. In fact, we humans often drift from topic to topic. How well do current systems perform? &lt;/p&gt;&lt;p&gt;&lt;br /&gt;Currently, “system prompt” is the main method that allows users to control LM behavior. However, researchers found evidence that LLMs can be brittle in following these instructions under adversarial conditions [12,13]. Readers might also have experienced this through daily interactions with ChatGPT or Claude—when a new chat window is freshly opened, the model can follow your instruction reasonably well [2], but after several rounds of dialogue, it’s no longer &lt;em&gt;fresh&lt;/em&gt;, even stops following its role altogether.&lt;/p&gt;&lt;p&gt;How could we quantitatively capture this anecdote? For one-round instruction following, we’ve already enjoyed plenty of benchmarks such as MT-Bench and Alpaca-Eval. However, when we test models in an interactive fashion, it’s hard to anticipate what the model generates and prepare a reply in advance. In a project by my collaborators and me [3], we built an environment to synthesize dialogues with unlimited length to stress-test the instruction-following capabilities of LLM chatbots. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;To allow an unconstrained scaling on the time scale, we let two system-prompted LM agents chat with each other for an extended number of rounds. This forms the main trunk of dialogue [a1, b1, a2, b2, …, a8, b8] (say the dialogue is 8-round). At this point, we could probably figure out how the LLMs stick to its system prompts just by examining this dialogue, but many of the utterances can be irrelevant to the instructions, depending on where the conversation goes. Therefore, we hypothetically branch out at each round by asking a question directly related to the system prompts, and use a corresponding judging function to quantify how well it performs. All that's provided by the dataset is a bank of triplets of (system prompts, probe questions, and judging functions).&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="What's Missing From LLM Chatbots: A Sense of Purpose" class="kg-image" height="570" src="https://thegradient.pub/content/images/2024/08/image3.png" width="1999" /&gt;&lt;figcaption&gt;Sketch of the process of measuring instruction stability&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Averaging across scenarios and pairs of system prompts, we get a curve of instruction stability across rounds. To our surprise, the aggregated results on both LLaMA2-chat-70B and gpt-3.5-turbo-16k are alarming. Besides the added difficulty to prompt engineering, the lack of instruction stability also comes with safety concerns. When the chatbot drifts away from its system prompts that stipulate safety aspects, it becomes more susceptible to jailbreaking and prone to more hallucinations.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="What's Missing From LLM Chatbots: A Sense of Purpose" class="kg-image" height="689" src="https://thegradient.pub/content/images/2024/08/Screenshot-2024-08-21-at-19.15.57.png" width="1736" /&gt;&lt;figcaption&gt;Instruction stability on LLaMA2-chat-70B and gpt-3.5-turbo-16k&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The empirical results also contrast with the ever-increasing context length of LLMs. Theoretically, some long-context models can attend to a window of up to 100k tokens. However, in the dialogue setting, they become distracted after only 1.6k tokens (assuming each utterance is 100 tokens). In [3], we further theoretically showed how this is inevitable in a Transformer based LM chatbot under the current prompting scheme, and proposed a simple technique called split-softmax to mitigate such effects. &lt;/p&gt;&lt;p&gt;One might ask at this point, why is it so bad? Why don't humans lose their persona just by talking to another person for 8 rounds? It’s arguable that human interactions are based on purposes and intentions [5] and these purposes precede the means rather than the opposite—LLM is fundamentally a fluent English generator, and the persona is merely a thin added layer.&lt;/p&gt;&lt;h2 id="what%E2%80%99s-missing"&gt;What’s missing? &lt;br /&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Pretraining?&lt;/strong&gt;&lt;br /&gt;Pretraining endows the language model with the capability to model a distribution over internet personas as well as the lower-level language distribution of each persona [4]. However, even when one persona (or a mixture of a limited number of them) is specified by the instruction of system prompts, current approaches fail to single it out.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;RLHF?&lt;/strong&gt;&lt;br /&gt;RLHF provides a powerful solution to adapting this multi-persona model to a “helpful and harmless assistant.” However, the original RLHF methods formulate reward maximization as a one-step bandit problem, and it is not generally possible to train with human feedback in the loop of conversation. (I’m aware of many advances in alignment but I want to discuss the original RLHF algorithm as a prototypical example.) This lack of multi-turn planning may cause models to suffer from task ambiguity [6] and learning superficial human-likeness rather than goal-directed social interaction [7].&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Will adding more dialogue data in RLHF help? My guess is that it will, to a certain extent, but it will still fall short due to a lack of purpose. Sergey Levine pointed out in his blog that there is a fundamental difference between preference learning and intentions: “the key distinction is between viewing language generation as selecting goal-directed actions in a sequential process, versus a problem of producing outputs satisfying user preferences.”&lt;/p&gt;&lt;h2 id="purposeful-dialogue-system"&gt;Purposeful dialogue system&lt;/h2&gt;&lt;p&gt;Staying on task is a modest request for LLMs. However, even if an LLM remains focused on the task, it doesn't necessarily mean it can excel in achieving the goal.&lt;/p&gt;&lt;p&gt;The problem of long-horizon planning has attracted some attention in the LLM community. For example, “decision-oriented dialogue” is proposed as a general class of tasks [8], where the AI assistant collaborates with humans to help them make complicated decisions, such as planning itineraries in a city and negotiating travel plans among friends. Another example, Sotopia [10], is a comprehensive social simulation platform that compiles various goal-driven dialogue scenarios including collaboration, negotiation, and persuasion. &lt;/p&gt;&lt;p&gt;Setting up such benchmarks provides not only a way to gauge the progress of the field, it also directly provides reward signals that new algorithms could pursue, which could be expensive to collect and tricky to define [9]. However, there aren’t many techniques that can exert control over the LM so that it can act consistently across a long horizon towards such goals. &lt;/p&gt;&lt;p&gt;To fill in this gap, my collaborators and I propose a lightweight algorithm (Dialogue Action Tokens, DAT [11]) that guides an LM chatbot through a multi-round goal-driven dialogue. As shown in the image below, in each round of conversations, the dialogue history’s last token embedding is used as the input (state) to a planner (actor) which predicts several prefix tokens (actions) to control the generation process. By training the planner with a relatively stable RL algorithm TD3+BC, we show significant improvement over baselines on Sotopia, even surpassing the social capability scores of GPT-4.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="What's Missing From LLM Chatbots: A Sense of Purpose" class="kg-image" height="176" src="https://thegradient.pub/content/images/2024/08/image2.png" width="1191" /&gt;&lt;figcaption&gt;A sketch of ​​Dialogue Action Tokens (DAT)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;br /&gt;In this way, we provide a technique pathway that upgrades LM from a prediction model that merely guesses the next token to one that engages in dialogue with humans purposefully. We could imagine that this technique can be misused for harmful applications as well. For this reason, we also conduct a “multi-round red-teaming” experiment, and recommend that more research could be done here to better understand multi-round dialogue as potential attack surface.&lt;/p&gt;&lt;h2 id="concluding-marks"&gt;Concluding marks&lt;br /&gt;&lt;/h2&gt;&lt;p&gt;I have reviewed the making of current LLM dialogue systems, how and why it is insufficient. I hypothesize that a purpose is what is missing and present one technique to add it back with reinforcement learning. &lt;/p&gt;&lt;p&gt;The following are two research questions that I’m mostly excited about: &lt;/p&gt;&lt;p&gt;(1) Better monitoring and control of dialogue systems with steering techniques. For example, the recently proposed TalkTurner (Chen et al.) adds a dashboard (Viégas et al) to open-sourced LLMs, enabling users to see and control how LLM thinks of themselves. Many weaknesses of current steering techniques are revealed and call for better solutions. For example, using activation steering to control two attributes (e.g., age and education level) simultaneously has been found to be difficult and can cause more language degradation. Another intriguing question is how to differentiate between LLM’s internal model of itself and that of the user. Anecdotally, chatting with Golden Gate Bridge Claude has shown that steering on the specific Golden Gate Bridge feature found by SAE sometimes causes Claude to think of itself as the San Francisco landmark, sometimes the users as the bridge, and other times the topic as such.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;(2) Better utilization of off-line reward signals. In the case of set-up environments like Sotopia and “decision-oriented dialogues”, rewards signals are engineered beforehand. In the real world, users won’t leave numerical feedback of how they feel satisfied. However, there might be other clues in language (e.g., “Thanks!”, “That’s very helpful!”) or from external resources (e.g., users buying the product for a salesman AI, users move to a subsequent coding question for copilot within a short time frame). Inferring and utilizing such hidden reward signals could strengthen the network effect of online chatbots: good model → more users → learning from interacting with users → better model.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Acknowledgment&lt;/strong&gt;&lt;br /&gt;The author is grateful to Martin Wattenberg and Hugh Zhang (alphabetical order) for providing suggestions and editing the text.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Citation&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;For attribution of this in academic contexts or books, please cite this work as:&lt;/p&gt;&lt;blockquote&gt;&lt;em&gt;Kenneth Li, "&lt;/em&gt;&lt;strong&gt;What's Missing From LLM Chatbots: A Sense of Purpose&lt;/strong&gt;&lt;em&gt;", The Gradient, 2024.&lt;/em&gt;&lt;/blockquote&gt;&lt;p&gt;BibTeX citation (this blog):&lt;/p&gt;&lt;div class="kg-card kg-callout-card kg-callout-card-grey"&gt;&lt;div class="kg-callout-text"&gt;@article{li2024from,&lt;br /&gt;author = {Li, Kenneth},&lt;br /&gt;title = {What's Missing From LLM Chatbots: A Sense of Purpose},&lt;br /&gt;journal = {The Gradient},&lt;br /&gt;year = {2024},&lt;br /&gt;howpublished = {\url{https://thegradient.pub/dialogue}},&lt;br /&gt;}&lt;/div&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[1] Schank, Roger C., and Robert P. Abelson. Scripts, plans, goals, and understanding: An inquiry into human knowledge structures. Psychology press, 2013.&lt;br /&gt;[2] Zhou, Jeffrey, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. "Instruction-following evaluation for large language models." arXiv preprint arXiv:2311.07911 (2023).&lt;br /&gt;[3] ​​Li, Kenneth, Tianle Liu, Naomi Bashkansky, David Bau, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. "Measuring and controlling persona drift in language model dialogs." arXiv preprint arXiv:2402.10962 (2024).&lt;br /&gt;[4] Andreas, Jacob. "Language models as agent models." arXiv preprint arXiv:2212.01681 (2022).&lt;br /&gt;[5] Austin, John Langshaw. How to do things with words. Harvard university press, 1975.&lt;br /&gt;[6] Tamkin, Alex, Kunal Handa, Avash Shrestha, and Noah Goodman. "Task ambiguity in humans and language models." arXiv preprint arXiv:2212.10711 (2022).&lt;br /&gt;[7] Bianchi, Federico, Patrick John Chia, Mert Yuksekgonul, Jacopo Tagliabue, Dan Jurafsky, and James Zou. "How well can llms negotiate? negotiationarena platform and analysis." arXiv preprint arXiv:2402.05863 (2024).&lt;br /&gt;[8] Lin, Jessy, Nicholas Tomlin, Jacob Andreas, and Jason Eisner. "Decision-oriented dialogue for human-ai collaboration." arXiv preprint arXiv:2305.20076 (2023).&lt;br /&gt;[9] Kwon, Minae, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. "Reward design with language models." arXiv preprint arXiv:2303.00001 (2023).&lt;br /&gt;[10] Zhou, Xuhui, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency et al. "Sotopia: Interactive evaluation for social intelligence in language agents." arXiv preprint arXiv:2310.11667 (2023).&lt;br /&gt;[11] Li, Kenneth, Yiming Wang, Fernanda Viégas, and Martin Wattenberg. "Dialogue Action Tokens: Steering Language Models in Goal-Directed Dialogue with a Multi-Turn Planner." arXiv preprint arXiv:2406.11978 (2024).&lt;br /&gt;[12] Li, Shiyang, Jun Yan, Hai Wang, Zheng Tang, Xiang Ren, Vijay Srinivasan, and Hongxia Jin. "Instruction-following evaluation through verbalizer manipulation." arXiv preprint arXiv:2307.10558 (2023).&lt;br /&gt;[13] Wu, Zhaofeng, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. "Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks." arXiv preprint arXiv:2307.02477 (2023).&lt;/p&gt;]]&amp;gt;&amp;lt;![CDATA[We Need Positive Visions for AI Grounded in Wellbeing]]&amp;gt;Introduction&lt;p&gt;Imagine yourself a decade ago, jumping directly into the present shock of conversing naturally with an encyclopedic AI that crafts images, writes code, and debates philosophy. Won’t this technology almost certainly transform society — and hasn’t AI’s impact on us so far been&lt;/p&gt;]]&amp;gt;https://thegradient.pub/we-need-positive-visions-for-ai-grounded-in-wellbeing/66a4243393571d5c8c154f4eSat, 03 Aug 2024 17:00:43 GMTIntroduction&lt;img alt="We Need Positive Visions for AI Grounded in Wellbeing" src="https://thegradient.pub/content/images/2024/07/wellbeing_ai_cover_image.webp" /&gt;&lt;p&gt;Imagine yourself a decade ago, jumping directly into the present shock of conversing naturally with an encyclopedic AI that crafts images, writes code, and debates philosophy. Won’t this technology almost certainly transform society — and hasn’t AI’s impact on us so far been a mixed-bag? Thus it’s no surprise that so many conversations these days circle around an era-defining question: &lt;em&gt;How do we ensure AI benefits humanity?&lt;/em&gt; These conversations often devolve into strident optimism or pessimism about AI, and our earnest aim is to walk a pragmatic middle path, though no doubt we will not perfectly succeed.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;While it’s fashionable to handwave towards “beneficial AI,” and many of us want to contribute towards its development — it’s not easy to pin down what beneficial AI concretely means in practice. This essay represents our attempt to demystify beneficial AI, through grounding it in the wellbeing of individuals and the health of society. In doing so, we hope to promote opportunities for AI research and products to benefit our flourishing, and along the way to share ways of thinking about AI’s coming impact that motivate our conclusions.&lt;/p&gt;&lt;h3 id="the-big-picture"&gt;The Big Picture&lt;/h3&gt;&lt;p&gt;By trade, we’re closer in background to AI than to the fields where human flourishing is most-discussed, such as wellbeing economics, positive psychology, or philosophy, and in our journey to find productive connections between such fields and the technical world of AI, we found ourselves often confused (what even is human flourishing, or wellbeing, anyways?) and from that confusion, often stuck (maybe there is nothing to be done? — the problem is too multifarious and diffuse). We imagine that others aiming to create prosocial technology might share our experience, and the hope here is to shine a partial path through the confusion to a place where there’s much interesting and useful work to be done. We start with some of our main conclusions, and then dive into more detail in what follows.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;One conclusion we came to is that &lt;strong&gt;it’s okay that we can’t conclusively define human wellbeing.&lt;/strong&gt; It’s been debated by philosophers, economists, psychotherapists, psychologists, and religious thinkers, for many years, and there’s no consensus. At the same time, there’s agreement around many concrete factors that make our lives go well, like: supportive intimate relationships, meaningful and engaging work, a sense of growth and achievement, and positive emotional experiences. And there’s clear understanding, too, that beyond momentary wellbeing, we must consider how to secure and improve wellbeing across years and decades — through what we could call &lt;em&gt;societal&lt;/em&gt; &lt;em&gt;infrastructure&lt;/em&gt;: important institutions such as education, government, the market, and academia. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;One benefit of this wellbeing lens is to wake us to an almost-paradoxical fact: While the deep purpose behind nearly everything our species does is wellbeing, we’ve tragically lost sight of it. &amp;nbsp;Both by common measures of individual wellbeing (suicide rate, loneliness, meaningful work) and societal wellbeing (trust in our institutions, shared sense of reality, political divisiveness), we’re not doing well, and our impression is that AI is complicit in that decline. The central benefit of this wellbeing view, however, is the insight that no fundamental obstacle prevents us from synthesizing the science of wellbeing with machine learning to our collective benefit. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;This leads to our second conclusion: &lt;strong&gt;We need plausible positive visions of a society with capable AI, grounded in wellbeing.&lt;/strong&gt; Like other previous transformative technologies, AI will shock our societal infrastructure — dramatically altering the character of our daily lives, whether we want it to or not. For example, Facebook launched only twenty years ago, and yet social media’s shockwaves have already upended much in society — subverting news media and our informational commons, addicting us to likes, and displacing meaningful human connection with its shell. We believe capable AI’s impact will exceed that of social media. As a result, it’s vital that we strive to explore, envision, and move towards the AI-infused worlds we’d flourish within — ones perhaps in which it revitalizes our institutions, empowers us to pursue what we find most meaningful, and helps us cultivate our relationships. This is no simple task, requiring imagination, groundedness, and technical plausibility — to somehow dance through the minefields illuminated by previous critiques of technology. Yet now is the time to dream and build if we want to actively shape what is to come.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;This segues into our final conclusion: &lt;strong&gt;Foundation models and the arc of their future deployment is critical.&lt;/strong&gt; Even for those of us in the thick of the field, it’s hard to internalize how quickly models have improved, and how capable they might become given several more years. Recall that GPT-2 — barely functional by today’s standards — was released &lt;em&gt;only in 2019&lt;/em&gt;. If future models are much more capable than today’s, and competently engage with more of the world with greater autonomy, we can expect their entanglement with our lives and society to rachet skywards. So, at minimum, we’d like to enable these models to understand our wellbeing and how to support it, potentially through new algorithms, wellbeing-based evaluations of models and wellbeing training data. Of course, we also want to realize human benefit in practice — the last section of this blog post highlights what we believe are strong leverage points towards that end.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;The rest of this post describes in more detail (1) what we mean by AI that benefits our wellbeing, (2) the need for positive visions for AI grounded in wellbeing, and (3) concrete leverage points to aid in the development and deployment of AI in service of such positive visions. We’ve designed this essay such that the individual parts are mostly independent, so if you are interested most in concrete research directions, feel free to skip there.&lt;/p&gt;&lt;h3 id="beneficial-ai-grounds-out-in-human-wellbeing"&gt;Beneficial AI grounds out in human wellbeing&lt;/h3&gt;&lt;p&gt;Discussion about AI for human benefit is often high-minded, but not particularly actionable, as in unarguable but content-free phrases like “We should make sure AI is in service of humanity.” But to meaningfully implement such ideas in AI or policy requires enough precision and clarity to translate them into code or law. So we set out to survey what science has discovered about the ground of human benefit, as a step towards being able to measure and support it through AI.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Often, when we think about beneficial impact, we focus on abstract pillars like democracy, education, fairness, or the economy. However important, none of these are valuable &lt;em&gt;intrinsically.&lt;/em&gt; We care about them because of how they affect our collective lived experience, over the short and long-term. We care about increasing society’s GDP to the extent it aligns with actual improvement of our lives and future, but when treated as an end in itself, it becomes disconnected from what matters: improving human (and potentially all species’) experience.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;In looking for fields that most directly study the root of human flourishing, we found the scientific literature on wellbeing. The literature is vast, spanning many disciplines, each with their own abstractions and theories — and, as you might expect, there’s no true consensus on what wellbeing actually is. In diving into the philosophy of flourishing, wellbeing economics, or psychological theories of human wellbeing, one encounters many interesting, compelling, but seemingly incompatible ideas. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;For example, theories of hedonism in philosophy claim that pleasure and the absence of suffering is the core of wellbeing; while desire satisfaction theories instead claim that wellbeing is about the fulfillment of our desires, no matter how we feel emotionally. There’s a wealth of literature on measuring subjective wellbeing (broadly, how we experience and feel about our life), and many different frameworks of what variables characterize flourishing. For example, Martin Seligman’s PERMA framework claims that wellbeing consists of positive emotions, engagement, relationships, meaning, and achievement. There are theories that say that the core of wellbeing is satisfying psychological needs, like the need for autonomy, competence, and relatedness. Other theories claim that wellbeing comes from living by our values. In economics, frameworks rhyme with those in philosophy and psychology, but diverge enough to complicate an exact bridge. For example, the wellbeing economics movement largely focuses on subjective wellbeing and explores many different proxies of it, like income, quality of relationships, job stability, etc.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;After the excitement from surveying so many interesting ideas began to fade, perhaps unsurprisingly, we remained fundamentally confused about what “the right theory” was. But, we recognized that in fact &lt;em&gt;this has always been the human situation when it comes to wellbeing&lt;/em&gt;, and just as a lack of an incontrovertible theory of flourishing has not prevented humanity from flourishing in the past, it need not stand as a fundamental obstacle for beneficial AI. In other words, our attempts to guide AI to support human flourishing must take this lack of certainty seriously, just as all sophisticated societal efforts to support flourishing must do.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;In the end, we came to a simple workable understanding, not far from the view of wellbeing economics: Human benefit ultimately must ground out in the &lt;em&gt;lived experience of humans&lt;/em&gt;. We want to live happy, meaningful, healthy, full lives — and it’s not so difficult to imagine ways AI might assist in that aim. For example, the development of low-cost but proficient AI coaches, intelligent journals that help us to self-reflect, or apps that help us to find friends, romantic partners, or to connect with loved ones. We can ground these efforts in imperfect but workable measures of wellbeing from the literature (e.g. PERMA), taking as &lt;em&gt;first-class concerns&lt;/em&gt; that the map (wellbeing measurement) is not the territory (actual wellbeing), and that humanity itself continues to explore and refine its vision of wellbeing.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;More broadly our wellbeing relies on a healthy society, and we care not only about our own lives, but also want beautiful lives for our neighbors, community, country, and world, and for our children, and their children as well. The infrastructure of society (institutions like government, art, science, military, education, news, and markets) is what supports this broader, longer-term vision of wellbeing.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="We Need Positive Visions for AI Grounded in Wellbeing" class="kg-image" height="362" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXe4f50PRkH1ZAWhltK8VAz1IhmtVLJU6k3pf1oGL-GmNTXM2QsnRJU52h0d8uCYPoFa_r6QB6UTtFThWPr6anV42FbBZPsnj1PXPDtp4Ofu5JjECD5CJz0W1asFNrFqvyL-PBxqYCk1VBxShyYTKoPj_HI?key=_z5hHgxLrjtdLauVz_eYpw" width="379" /&gt;&lt;/figure&gt;&lt;p&gt;Each of these institutions have important roles to play in society, and we can also imagine ways that AI could support or improve them; for example, generative AI may catalyze education through personal tutors that help us develop a richer worldview, may help us to better hold our politicians to account through sifting through what they are actually up to, or accelerate meaningful science through helping researchers make novel connections. Thus in short, &lt;em&gt;beneficial AI would meaningfully support our quest for lives worth living, in both the immediate and long-term sense.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;So, from the lofty confusion of conflicting grand theories, we arrive at something sounding more like common sense. Let’s not take this for granted, however — it cuts through the cruft of abstractions to firmly recenter what is ultimately important: the psychological experience of humans. This view points us towards the ingredients of wellbeing that are both well-supported scientifically and could be made measurable and actionable through AI (e.g. there exist instruments to measure many of these ingredients). Further, wellbeing across the short and long-term provides the common currency that bridges divergent approaches to beneficial AI, whether mitigating societal harms like discrimination in the AI ethics community, to attempting to reinvigorate democracy through AI-driven deliberation, to creating a world where humans live more meaningful lives, to creating low-cost emotional support and self-growth tools, to reducing the likelihood of existential risks from AI, to using AI to reinvigorate our institutions — wellbeing is the ultimate ground.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Finally, focusing on wellbeing helps to highlight where we currently fall short. Current AI development is driven by our existing incentive systems: Profit, research novelty, engagement, with little explicit focus on what fundamentally is more important (human flourishing). We need to find tractable ways to shift incentives towards wellbeing-supportive models (something we’ll discuss later), and positive directions to move toward (discussed next).&lt;/p&gt;&lt;h3 id="we-need-positive-visions-for-ai"&gt;We need positive visions for AI&lt;/h3&gt;&lt;p&gt;Technology is a shockingly powerful societal force. While nearly all new technologies bring only limited change, like an improved toothbrush, sometimes they upend the world. Like the proverbial slowly-boiling frog, we forget how in short order the internet and cellphones have &lt;em&gt;overhauled&lt;/em&gt; our lived experience: the rise of dating apps, podcasts, social networks, our constant messaging, cross-continental video calls, massive online games, the rise of influencers, on-demand limitless entertainment, etc. Our lives as a whole — our relationships, our leisure, how we work and collaborate, how news and politics work — &lt;em&gt;have dramatically shifted&lt;/em&gt;, for both the better and worse.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;AI is transformative, and the mixed bag of its impacts are poised to reshape society in mundane and profound ways; we might doubt it, but that was also our naivety at the advent of social media and the cell-phone. We don’t see it coming, and once it’s here we take it for granted. Generative AI translates applications from science fiction into rapid adoption: AI romantic companions; automated writing and coding assistants; automatic generation of high-quality images, music, and videos; low-cost personalized AI tutors; highly-persuasive personalized ads; and so on. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;In this way, transformative impact is happening &lt;em&gt;now&lt;/em&gt; — it does not require AI with superhuman intelligence — see the rise of LLM-based social media bots; ChatGPT as the fastest-adopted consumer app; LLMs requiring fundamental changes to homework in school. Much greater impact will yet come, as the technology (and the business around it) matures, and as AI is integrated more pervasively throughout society.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Our institutions were understandably not designed with this latest wave of AI in mind, and it’s unclear that many of them will adapt quickly enough to keep up with AI's rapid deployment. For example, an important function of news is to keep a democracy’s citizens well-informed, so their vote is meaningful. But news these days spreads through AI-driven algorithms on social media, which amplifies emotional virality and confirmation bias at the expense of meaningful debate. And so, the public square and the sense of a shared reality is being undercut, as AI degrades an important institution devised without foresight of this novel technological development.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thus in practice, it may not be possible to play defense by simply “mitigating harms” from a technology; often, a new technology demands that we creatively and skillfully apply our existing values to a radically new situation. We don’t want AI to, for example, undermine the livelihood of artists, yet how &lt;em&gt;do&lt;/em&gt; we want our relationship to creativity to look like in a world where AI can, easily and cheaply, produce compelling art or write symphonies and novels, in the style of your favorite artist? There’s no easy answer. We need to debate, understand, and capture what we believe is the &lt;em&gt;spirit &lt;/em&gt;of our institutions and systems given this new technology. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;For example, what’s truly important about education? We can reduce harms that AI imposes on the current education paradigm by banning use of AI in students’ essays, or apply AI in service of existing metrics (for example, to increase high school graduation rates). But the paradigm itself must adapt: The world that schooling currently prepares our children for is not the world they’ll graduate into, nor does it prepare us generally to flourish and find meaning in our lives. We must ask ourselves what we really value in education that we want AI to enable: Perhaps teaching critical thinking, enabling agency, and creating a sense of social belonging and civic responsibility?&lt;br /&gt;&lt;/p&gt;&lt;p&gt;To anticipate critique, we agree that there will be no global consensus on what education is for, or on the underlying essence of any particular institution, at root because different communities and societies have &amp;nbsp;distinct values and visions. But that’s okay: Let’s empower communities to fit AI systems to local societal contexts; for example, algorithms like constitutional AI enable creating different constitutions that embody flourishing for different communities. This kind of cheap flexibility is an exciting affordance, meaning we no longer must sacrifice nuance and context-sensitivity for scalability and efficiency, a bitter pill technology often pushes us to swallow.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;And while of course we have always wanted education to create critical thinkers, our past metrics (like standardized tests) have been so coarse that scoring high is easily gamed without critical thinking. But generative AI enables new affordances here, too: just as a teacher can socratically question a student to evaluate their independent thought, advances in generative AI open up the door for similarly qualitative and interactive measures, like personalized AI tutors that meaningfully gauge critical thinking.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;We hope to tow a delicate line beyond broken dichotomies, whether between naive optimism and pessimism, or idealism and cynicism. Change is coming, and we must channel it towards refined visions of what we want, which is a profound opportunity, rather than to assume that by default technology will deliver us (or doom us), or that we will be able to wholly resist the transformation it brings (or are entirely helpless against it). For example, we must temper naive optimism (“AI will save the world if only we deploy it everywhere!”) by integrating lessons from the long line of work that studies the social drivers and consequences of technology, often from a critical angle. But neither should cynical concerns so paralyze us that we remain only as critics on the sidelines.&lt;/p&gt;&lt;h2 id="so-what-can-we-do"&gt;So, what can we do?&lt;/h2&gt;&lt;p&gt;The case so far is that we need positive visions for society with capable AI, grounded in individual and societal wellbeing. But what concrete work can actually support this? We propose the following break-down:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Understanding where we want to go&lt;/li&gt;&lt;li&gt;Measuring how AI impacts our wellbeing&lt;/li&gt;&lt;li&gt;Training models that can support wellbeing&lt;/li&gt;&lt;li&gt;Deploying models in service of wellbeing&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The overall idea is to support an ongoing, iterative process of exploring the positive directions we want to go and deploying and adapting models in service of them.&lt;/p&gt;&lt;h3 id="we-need-to-understand-where-we-want-to-go-in-the-age-of-ai"&gt;We need to understand where we want to go in the age of AI&lt;/h3&gt;&lt;p&gt;This point follows closely from the need to explore the positive futures we want with AI. What directions of work and research can help us to clarify where is possible to go, and is worth going to, in the age of AI?&lt;br /&gt;&lt;/p&gt;&lt;p&gt;For starters, it’s more important now than ever to have productive and grounded discussions about questions like: What makes us human? How do we want to live? What do we want the future to feel like? What values are important to us? What do we want to retain as AI transformations sweep through society? Rather than being centered on the machine learning community, this should be an interdisciplinary, international effort, spanning psychology, philosophy, political science, art, economics, sociology, and neuroscience (and many other fields!), and bridging diverse intra- and international cultures. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;Of course, it’s easy to call for such a dialogue, but the real question is how such interdisciplinary discussions can be convened in a meaningful, grounded, and action-guiding way — rather than leading only to cross-field squabbles or agreeable but vacuous aspirations. Perhaps through participatory design that pairs citizens with disciplinary experts to explore these questions, with machine learning experts mainly serving to ground technological plausibility. Perhaps AI itself could be of service: For example, research in AI-driven deliberative democracy and plurality may help involve more people in navigating these questions; as might research into meaning alignment, by helping us describe and aggregate what is meaningful and worth preserving to us. It’s important here to look beyond cynicism or idealism (suggestive of meta-modern political philosophy): Yes, mapping exciting positive futures is not a cure-all, as there are powerful societal forces, like regulatory capture, institutional momentum, and the profit motive, that resist their realization, and yet, societal movements all have to start somewhere, and &lt;em&gt;some really do succeed&lt;/em&gt;.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Beyond visions for big-picture questions about the future, much work is needed to understand where we want to go in narrower contexts. For example, while it might at first seem trivial, how can we reimagine online dating with capable AI, given that healthy romantic partnership is such an important individual and societal good? Almost certainly, we will look back at swipe-based apps as misguided means for finding long-term partners. And many of our institutions, small and large, can be re-visioned in this way, from tutoring to academic journals to local newspapers. AI will make possible a much richer set of design possibilities, and we can work to identify which of those possibilities are workable and well-represent the desired essence of an institution’s role in our lives and society.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Finally, continued basic and applied research into the factors that contribute and characterize human wellbeing and societal health also are highly important, as these are what ultimately ground our visions. And as the next section explores, having better measures of such factors can help us to change incentives and work towards our desired futures.&lt;/p&gt;&lt;h3 id="we-need-to-develop-measures-for-how-ai-affects-wellbeing"&gt;We need to develop measures for how AI affects wellbeing&lt;/h3&gt;&lt;p&gt;For better and worse, we often navigate through what we measure. We’ve seen this play out before: Measure GDP, and nations orient towards increasing it at great expense. Measure clicks and engagement, and we develop platforms that are terrifyingly adept at keeping people hooked. A natural question is, what prevents us from similarly measuring aspects of wellbeing to guide our development and deployment of AI? And if we do develop wellbeing measures, can we avoid the pitfalls that have derailed other well-intended measures, like GDP or engagement?&lt;br /&gt;&lt;/p&gt;&lt;p&gt;One central problem for measurement is that wellbeing is more complex and qualitative than GDP or engagement. Time-on-site is a very straightforwardmeasure of engagement. In contrast, properties relevant to wellbeing, like the felt sense of meaning or the quality of healthy relationships, are difficult to pin down quantitatively, especially from the limited viewpoint of how a user interacts with a particular app. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;Wellbeing depends on the broader context of a user’s life in messy ways, meaning it’s harder to isolate how any small intervention impacts it. And so, wellbeing measures are more expensive and less standardized to apply, end up less measured, and less guide our development of technology. However, foundation models are beginning to have the exciting ability to work with qualitative aspects of wellbeing. For example, present-day language models can (with caveats) infer emotions from user messages and detect conflict; or conduct qualitative interviews with users about its impact on their experience. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;So one promising direction of research, though not easy, is to explore how foundation models themselves can be applied to more reliably measure facets of individual and societal wellbeing, and ideally, help to identify how AI products and services are impacting that wellbeing. The mechanisms of impact are two-fold: One, companies may currently lack means of measuring wellbeing even though all-things-equal they want their products to help humans; two, where the profit motive conflicts with encouraging wellbeing, if a product’s impact can be externally audited and published, it can help hold the company to account by consumers and regulators, shifting corporate &amp;nbsp;incentives towards societal good.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Another powerful way that wellbeing-related measures can have impact is as evaluation benchmarks for foundation models. In machine learning, evaluations are a powerful lever for channeling research effort through competitive pressure. For example, model providers and academics continuously develop new models that perform better and better on benchmarks like TruthfulQA. Once you have legible outcomes, you often spur innovation to improve upon them. We currently have very few benchmarks focused on how AI affects our wellbeing, or how well they can understand our emotions, make wise decisions, or respect our autonomy: We need to develop these benchmarks.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Finally, as mentioned briefly above, metrics can also create accountability and enable regulations. Recent efforts like the Stanford Foundational Model Transparency Index have created public accountability for AI labs, and initiatives like Responsible Scaling Policies are premised on evaluations of model capabilities, as are evaluations by government bodies such as AI safety institutes in both the UK and US. Are there similar metrics and initiatives to encourage accountability around AI’s impact on wellbeing?&lt;br /&gt;&lt;/p&gt;&lt;p&gt;To anticipate a natural concern, unanticipated side-effects are nearly universal when attempting to improve important &lt;em&gt;qualities&lt;/em&gt; through &lt;em&gt;quantitative&lt;/em&gt; measures. What if in measuring wellbeing, the second-order consequence is perversely to undermine it? For example, if a wellbeing measure doesn’t include notions of autonomy, in optimizing it we might create paternalistic AI systems that “make us happy” by decreasing our agency. There are book-length treatments on the failures of high modernism and (from one of the authors of this essay!) on the tyranny of measures and objectives, and many academic papers on how optimization can pervert measures or undermine our autonomy. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;The trick is to look beyond binaries. Yes, measures and evaluations have serious problems, yet we can work with them with wisdom, taking seriously previous failures and institutionalizing that all measures are imperfect. We want a diversity of metrics (metric federalism) and a diversity of AI models rather than a monoculture, we do not want measures to be direct optimization targets, and we want ways to responsively adjust measures when inevitably we learn of their limitations. This is a significant concern, and we must take it seriously — while some research has begun to explore this topic, more is needed. Yet in the spirit of pragmatic harm reduction, given that metrics are both technically and politically important for steering AI systems, developing less flawed measures remains an important goal.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Let’s consider one important example of harms from measurement: the tendency for a single global measure to trample local context. Training data for models, including internet data in particular, is heavily biased. Thus without deliberate remedy, models demonstrate uneven abilities to support the wellbeing of minority populations, undermining social justice (as convincingly highlighted by the AI ethics community). While LLMs have exciting potential to respect cultural nuance and norms, informed by the background of the user, we must work deliberately to realize it. One important direction is to develop measures of wellbeing specific to diverse cultural contexts, to drive accountability and reward progress.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;To tie these ideas about measurement together, we suggest a taxonomy, looking at measures of AI &lt;em&gt;capabilities, behaviors, usage, and impacts&lt;/em&gt;. Similar to this DeepMind paper, the idea is to examine spheres of expanding context, from testing a model in isolation (both what it is capable of and what behaviors it demonstrates), all the way to understanding what happens when a model meets the real world (how humans use it, and what its impact is on them and society).&lt;br /&gt;&lt;/p&gt;&lt;p&gt;The idea is that we need a complementary ecosystem of measures fit to different stages of model development and deployment. In more detail:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;AI capabilities&lt;/em&gt; refers to what models are able to do. For example, systems today are capable of generating novel content, and translating accurately between languages.&lt;/li&gt;&lt;li&gt;&lt;em&gt;AI behaviors&lt;/em&gt; refers to how an AI system responds to different concrete situations. For example, many models are trained to refuse to answer questions that enable dangerous activities, like how to build a bomb,even though they have the capability to correctly answer them).&lt;/li&gt;&lt;li&gt;&lt;em&gt;AI usage&lt;/em&gt; refers to how models are used in practice when deployed. For example, AI systems today are used in chat interfaces to help answer questions, as coding assistants in IDEs, to sort social media feeds, and as personal companions.&lt;/li&gt;&lt;li&gt;&lt;em&gt;AI impacts &lt;/em&gt;refers to how AI impacts our experience or society. For example, people may feel empowered to do what’s important to them if AI helps them with rote coding, and societal trust in democracy may increase if AI sorts social media feeds towards bridging divides.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;As an example of applying this framework to an important quality that contributes to wellbeing, here is a sketch of how we might design measures of human autonomy: &lt;/p&gt;&lt;!--kg-card-begin: html--&gt;&lt;table&gt;&lt;colgroup&gt;&lt;col width="101" /&gt;&lt;col width="135" /&gt;&lt;col width="134" /&gt;&lt;col width="126" /&gt;&lt;col width="124" /&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Goal&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Capabilities&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Model Benchmarks&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Behaviors&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;System Benchmarks&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Usage&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;User Surveys&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Impact&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;User and Population Surveys&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Respecting autonomy&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Understand what someone is trying to achieve in a given context&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Understand the frontier of someone’s skill level&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Understand what activities a user finds meaningful&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Socratic dialogue rather than just providing answers&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Tapping into users’ wisdom rather than giving advice&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Selective automation of tasks&lt;/span&gt;&lt;/p&gt;&lt;br /&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Used to aid humans with tasks rather than fully automate tasks they find&amp;nbsp; meaningful&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Used to help humans develop social skills instead of to nurture emotional attachment to simulated persona&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;People feel empowered&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;People are able to achieve their goals&lt;/span&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;span&gt;People are pushed to grow&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;!--kg-card-end: html--&gt;&lt;p&gt;Let’s work through this example: we take a quality with strong scientific links to wellbeing, autonomy, and create measures of it and what enables it, all along the pipeline from model development to when it’s deployed at scale. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;Starting from the right side of the table (Impact), there exist validated psychological surveys that measure autonomy, which can be adapted and given to users of an AI app to measure its &lt;em&gt;impact&lt;/em&gt; on their autonomy. Then, moving leftwards, these changes in autonomy could be linked to more specific types of &lt;em&gt;usage&lt;/em&gt;, through additional survey questions. For example, perhaps automating tasks that users actually find meaningful may correlate with decreased autonomy.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Moving further left on the table, the &lt;em&gt;behaviors&lt;/em&gt; of models that are needed to enable beneficial usage and impact can be gauged through more focused benchmarks. To measure behaviors of an AI system, one could run fixed workflows on an AI application where gold-standard answers come from expert labelers; another approach is to simulate users (e.g. with language models) interacting with an AI application to see how often and skillfully it performs particular behaviors, like socratic dialogue.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Finally, &lt;em&gt;capabilities&lt;/em&gt; of a particular AI model could be similarly measured through benchmark queries input directly to the model, in a way very similar to how LLMs are benchmarked for capabilities like reasoning or question-answering. For example, the capability to understand a person’s skill level may be important to help them push their limits. A dataset could be collected of user behaviors in some application, annotated with their skill level; and the evaluation would be how well the model could predict skill level from observed behavior.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;At each stage, the hope is to link what is measured through evidence and reasoning to what lies above and below it in the stack. And we would want a diversity of measures at each level, reflecting different hypotheses about how to achieve the top-level quality, and with the understanding that each measure is always imperfect and subject to revision. In a similar spirit, rather than some final answer, this taxonomy and example autonomy measures are intended to inspire much-needed pioneering work towards wellbeing measurement.&lt;br /&gt;&lt;/p&gt;&lt;h3 id="we-need-to-train-models-to-improve-their-ability-to-support-wellbeing"&gt;We need to train models to improve their ability to support wellbeing&lt;/h3&gt;&lt;p&gt;Foundation models are becoming increasingly capable and in the future we believe most applications will not train models from scratch. Instead, most applications will prompt cutting-edge proprietary models, or fine-tune such models through limited APIs, or train small models on domain-specific responses from the largest models for cost-efficiency reasons. As evidence, note that to accomplish tasks with GPT-3 often required chaining together many highly-tuned prompts, whereas with GPT-4 those same tasks often succeed with the first casual prompting attempt. Additionally, we are seeing the rise of capable smaller models specialized for particular tasks, trained through data from large models.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;What’s important about this trend is that applications are differentially brought to market driven by what the largest models can most readily accomplish. For example, if frontier models excel at viral persuasion from being trained on Twitter data, but struggle with the depths of positive psychology, it will be easier to create persuasive apps than supportive ones, and there will be more of them, sooner, on the market.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thus we believe it’s crucial that the most capable foundation models &lt;em&gt;themselves&lt;/em&gt; understand what contributes to our wellbeing — an understanding granted to them through their &lt;em&gt;training process&lt;/em&gt;. We want the AI applications that we interface with (whether therapists, tutors, social media apps, or coding assistants) to understand how to support our wellbeing within their relevant role.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;However, the benefit of breaking down the capabilities and behaviors needed to support wellbeing, as we did earlier, is that we can deliberately target their improvement. One central lever is to gather or generate training data, which is the general fuel underlying model capabilities. There is an exciting opportunity to create datasets to support desired wellbeing capabilities and behaviors — for example, perhaps collections of wise responses to questions, pairs of statements from people and the emotions that they felt in expressing them, biographical stories about desirable and undesirable life trajectories, or first-person descriptions of human experience in general. The effect of these datasets can be grounded in the measures discussed above.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;To better ground our thinking, we can examine how wellbeing data could improve the common &lt;em&gt;phases&lt;/em&gt; of foundation model training: pretraining, fine-tuning, and alignment.&lt;/p&gt;&lt;h4 id="pretraining"&gt;Pretraining&lt;/h4&gt;&lt;p&gt;The first training phase (confusingly called pretraining) establishes a model’s base abilities. It does so by training on vast amounts of variable-quality data, like a scrape of the internet. One contribution could be to either generate or gather large swaths of wellbeing relevant data, or to prioritize such data during training (also known as altering the data mix). For example, data could be sourced from subreddits relevant to mental health or life decisions, collections of biographies, books about psychology, or transcripts of supportive conversations. Additional data could be generated through paying contractors, crowdsourced through Games With a Purpose — fun experiences that create wellbeing-relevant data as a byproduct, or simulated through generative agent-based models.&lt;/p&gt;&lt;h4 id="fine-tuning"&gt;Fine-tuning&lt;/h4&gt;&lt;p&gt;The next stage of model training is fine-tuning. Here, smaller amounts of high-quality data, like diverse examples of desired behavior gathered from experts, focus the general capabilities resulting from pretraining. For different wellbeing-supporting behaviors we might want from a model, we can create fine-tuning datasets through deliberate curation of larger datasets, or by enlisting and recording the behavior of human experts in the relevant domain. We hope that the companies training the largest models place more emphasis on wellbeing in this phase of training, which is often driven by tasks with more obvious economic implications, like coding.&lt;/p&gt;&lt;h4 id="alignment"&gt;Alignment&lt;/h4&gt;&lt;p&gt;The final stage of model training is alignment, often achieved through techniques like reinforcement learning through human feedback (RLHF), where human contractors give feedback on AI responses to guide the model towards better ones. Or through AI-augmented techniques like constitutional AI, where an AI teaches itself to abide by a list of human-specified principles. The fuel of RLHF is preference data about what responses are preferred over others. Therefore we imagine opportunities for creating data sets of expert preferences that relate to wellbeing behaviors (even though what constitutes expertise in wellbeing may be interestingly contentious). For constitutional AI, we may need to iterate in practice with lists of wellbeing principles that we want to support, like human autonomy, and how, specifically, a model can respect it across different contexts.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;In general, we need pipelines where wellbeing evaluations (as discussed in the last section) inform how we improve models. We need to find extensions to paradigms like RLHF that go beyond which response humans prefer in the moment, considering also which responses support user long-term growth, wellbeing, and autonomy, or better embody the spirit of the institutional role that the model is currently playing. These are intriguing, subtle, and challenging research questions that strike at the heart of the intersection of machine learning and societal wellbeing, and deserve much more attention. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;For example, we care about wellbeing over spans of years or decades, but it is impractical to apply RLHF &lt;em&gt;directly&lt;/em&gt; on human feedback to such ends, as we cannot wait decades to gather human feedback for a model; instead, we need research that helps integrate validated short-term proxies for long-term wellbeing (e.g. quality of intimate relationships, time spent in flow, etc.), ways to learn from longitudinal data where it exists (perhaps web journals, autobiographies, scientific studies), and to collect the judgment of those who devote their lifetime to helping support individuals flourish (like counselors or therapists).&lt;/p&gt;&lt;h3 id="we-need-to-deploy-ai-models-in-a-way-that-supports-wellbeing"&gt;We need to deploy AI models in a way that supports wellbeing&lt;/h3&gt;&lt;p&gt;Ultimately we want AI models deployed in the world to benefit us. AI applications could directly target human wellbeing, for example by directly supporting mental health or coaching us in a rigorous way. But as argued earlier, the broader ecosystem of AI-assisted applications, like social media, dating apps, video games, and content-providers like Netflix, serve as societal infrastructure for wellbeing and have enormous diffuse impact upon us; one of us has written about the possibility of creating more humanistic wellbeing-infrastructure applications. While difficult, dramatic societal benefits could result from, for example, new social media networks that better align with short and long-term wellbeing.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;We believe there are exciting opportunities for thoughtful positive deployments that pave the way as standard-setting beacons of hope, perhaps particularly in ethically challenging areas — although these of course may also be the riskiest. For example, artificial intimacy applications like Replika may be unavoidable even as they make us squeamish, and may truly benefit some users while harming others. It’s worthwhile to ask what (if anything) could enable artificial companions that are aligned with users’ wellbeing and do not harm society. Perhaps it is possible to thread the needle: they could help us develop the social skills needed to find real-world companions, or at least have strong, transparent guarantees about their fiduciary relationship to us, all while remaining viable as a business or non-profit. Or perhaps we can create harm-reduction services that help people unaddict from artificial companions that have become obstacles to their growth and development. Similar thoughts may apply to AI therapists, AI-assisted dating apps, and attention-economy apps, where incentives are difficult to align. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;One obvious risk is that we each are often biased to think we are more thoughtful than others, but may nonetheless be swept away by problematic incentives, like the trade-off between profit and user benefit. Legal structures like public benefit corporations, non-profits, or innovative new structures may help minimize this risk, as may value-driven investors or exceedingly careful design of internal culture.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Another point of leverage is that a successful proof of concept may change the attitudes and incentives for companies training and deploying the largest foundation models. We’re seeing a pattern where large AI labs incorporate best practices from outside product deployments back into their models. For example, ChatGPT plugins like data analysis and the GPT market were explored first by companies outside OpenAI before being incorporated into their ecosystem. And RLHF, which was first integrated into language models by OpenAI, is now a mainstay across foundation model development.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;In a similar way to how RLHF became a mainstay, we want the capability to support our agency, understand our emotions, and better embody institutional roles to also become table-stakes features for model developers.This could happen through research advances &lt;em&gt;outside&lt;/em&gt; of the big companies, making it much easier for such features to be adopted &lt;em&gt;within&lt;/em&gt; them — though adoption may require pressure, through regulation, advocacy, or competition.&lt;/p&gt;&lt;h3 id="initiatives"&gt;Initiatives&lt;/h3&gt;&lt;p&gt;We believe there’s much concrete work to be done in the present. Here are a sampling of initiatives to seed thinking about what could move the field forward:&lt;br /&gt;&lt;/p&gt;&lt;!--kg-card-begin: html--&gt;&lt;table&gt;&lt;colgroup&gt;&lt;col width="245" /&gt;&lt;col width="379" /&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Area&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Initiatives&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Understanding where we want to go&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Global discussions on what is important to us.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Democratic elicitation of what matters to people (for example, the work done by &lt;/span&gt;&lt;span&gt;Collective Intelligence Project&lt;/span&gt;&lt;span&gt; and the &lt;/span&gt;&lt;span&gt;Meaning Alignment Institute&lt;/span&gt;&lt;span&gt;).&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Concrete visualizations of what we want society to look like in 2050 (for example, the worldbuilding contest run by the &lt;/span&gt;&lt;span&gt;Future of Life Institute&lt;/span&gt;&lt;span&gt;).&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Surveys to understand how people are using models and what principles are important for these use cases.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Improve our basic understanding of the factors that lead to wellbeing.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Develop methods for measuring how AI affects wellbeing&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Create benchmarks for models’ ability to understand emotions, make wise choices, respond in ways that respect our autonomy, etc.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Evaluations on how models impact people’s psychological experience.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Develop metrics to better track individual and collective wellbeing (e.g. tracking our somatic states, tracking societal trust, etc).&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Train AI models based on what’s important to us&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Create datasets of emotionally supportive interactions.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Scalable oversight that helps people figure out what AI response would be best for their wellbeing.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Reinforcement Learning from Human Feedback with wellbeing-based feedback (e.g. from therapists).&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Democratic finetuning&lt;/span&gt;&lt;span&gt; (run by the Meaning Alignment Institute)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p dir="ltr"&gt;&lt;span&gt;Deploy models in beneficial areas&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li dir="ltr"&gt;&lt;p dir="ltr"&gt;&lt;span&gt;AI for mental health, education, resolving conflicts, relationship support, etc.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;!--kg-card-end: html--&gt;&lt;h2 id="conclusion-a-call-to-action"&gt;Conclusion: A call to action&lt;/h2&gt;&lt;p&gt;AI will transform society in ways that we cannot yet predict. If we continue on the present track, we risk AI reshaping our interactions and institutions in ways that erode our wellbeing and what makes our lives meaningful. Instead, challenging as it may be, we need to develop AI systems that understand and support wellbeing, both individual and societal. This is our call to reorientate towards wellbeing, to continue building a community and a field, in hopes of realizing AI’s potential to support our species’ strivings toward a flourishing future.&lt;/p&gt;]]&amp;gt;&amp;lt;![CDATA[Financial Market Applications of LLMs]]&amp;gt;The AI revolution drove frenzied investment in both private and public companies and captured the public’s imagination in 2023. Transformational consumer products like ChatGPT are powered by Large Language Models (LLMs) that excel at modeling sequences of tokens that represent words or parts of words [2]. Amazingly, structural]]&amp;gt;https://thegradient.pub/financial-market-applications-of-llms/661762b993571d5c8c154ea7Sat, 20 Apr 2024 17:57:39 GMT&lt;p&gt;The AI revolution drove frenzied investment in both private and public companies and captured the public’s imagination in 2023. Transformational consumer products like ChatGPT are powered by Large Language Models (LLMs) that excel at modeling sequences of tokens that represent words or parts of words [2]. Amazingly, structural understanding emerges from learning next-token prediction, and agents are able to complete tasks such as translation, question answering and generating human-like prose from simple user prompts.&lt;/p&gt;&lt;p&gt;Not surprisingly, quantitative traders have asked: can we turn these models into the next price or trade prediction [1,9,10]? That is, rather than modeling sequences of words, can we model sequences of prices or trades. This turns out to be an interesting line of inquiry that reveals much about both generative AI and financial time series modeling. Be warned this will get wonky.&lt;/p&gt;&lt;p&gt;LLMs are known as autoregressive learners -- those using previous tokens or elements in a sequence to predict the next element or token. In quantitative trading, for example in strategies like statistical arbitrage in stocks, most research is concerned with identifying autoregressive structure. That means finding sequences of news or orders or fundamental changes that best predict future prices.&lt;/p&gt;&lt;p&gt;Where things break down is in the quantity and information content of available data to train the models. At the 2023 NeurIPS conference, Hudson River Trading, a high frequency trading firm, presented a comparison of the number of input tokens used to train GPT-3 with the amount of trainable tokens available in the stock market data per year HRT estimated that, with 3,000 tradable stocks, 10 data points per stock per day, 252 trading days per year, and 23400 seconds in a trading day, there are 177 billion stock market tokens per year available as market data. GPT-3 was trained on 500 billion tokens, so not far off [6].&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Financial Market Applications of LLMs" class="kg-image" height="368" src="https://thegradient.pub/content/images/2024/04/Screenshot-2024-04-18-at-3.11.18-PM.png" width="2000" /&gt;&lt;figcaption&gt;numbers courtesy of HRT 2023 NeuRIPS presentation&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;But, in the trading context the tokens will be prices or returns or trades rather than syllables or words; the former is much more difficult to predict. Language has an underlying linguistic structure (e.g., grammar) [7]. It’s not hard to imagine a human predicting the next word in a sentence, however that same human would find it extremely challenging to predict the next return given a sequence of previous trades, hence the lack of billionaire day traders. The challenge is that there are very smart people competing away any signal in the market, making it &lt;em&gt;almost &lt;/em&gt;efficient (“efficiently inefficient”, in the words of economist Lasse Pedersen) and hence unpredictable. No adversary actively tries to make sentences more difficult to predict — if anything, authors usually seek to make their sentences easy to understand and hence &lt;em&gt;more&lt;/em&gt; predictable.&lt;/p&gt;&lt;p&gt;Looked at from another angle, there is much more noise than signal in financial data. Individuals and institutions are trading for reasons that might not be rational or tied to any fundamental change in a business. The GameStop episode in 2021 is one such example. Financial time series are also constantly changing with new fundamental information, regulatory changes, and occasional large macroeconomic shifts such as currency devaluations. Language evolves at a much slower pace and over longer time horizons.&lt;/p&gt;&lt;p&gt;On the other hand, there are reasons to believe that ideas from AI will work well in financial markets. One emerging area of AI research with promising applications to finance is multimodal learning [5], which aims to use different modalities of data, for example both images and textual inputs to build a unified model. With OpenAI’s DALL-E 2 model, a user can enter text and the model will generate an image. In finance, multi-modal efforts could be useful to combine information classical sources such as technical time series data (prices, trades, volumes, etc.) with alternative data in different modes like sentiment or graphical interactions on twitter, natural language news articles and corporate reports, or the satellite images of shipping activity in a commodity centric port. Here, leveraging multi-modal AI, one could potentially incorporate all these types of non-price information to predict well.&lt;/p&gt;&lt;p&gt;Another strategy called ‘residualization’ holds prominence in both finance and AI, though it assumes different roles in the two domains. &amp;nbsp;In finance, structural `factor’ models break down the contemporaneous observations of returns across different assets into a shared component (the market return, or more generally returns of common, market-wide factors) and an idiosyncratic component unique to each underlying asset. Market and factor returns are difficult to predict and create interdependence, so it is often helpful to remove the common element when making predictions at the individual asset level and to maximize the number of independent observations in the data. &lt;/p&gt;&lt;p&gt;In residual network architectures such as transformers, there’s a similar idea that we want to learn a function h(X) of an input X, but it might be easier to learn the residual of h(X) to the identity map, i.e., h(X) – X. Here, if the function h(X) is close to identity, its residual will be close to zero, and hence there will be less to learn and learning can be done more efficiently. In both cases the goal is to exploit structure to refine predictions: in the finance case, the idea is to focus on predicting innovations beyond what is implied by the overall market, for residual networks the focus is on predicting innovations to the identity map.&lt;/p&gt;&lt;p&gt;A key ingredient for the impressive performance of LLMs work is their ability to discern affinities or strengths between tokens over long horizons known as context windows. In financial markets, the ability to focus attention across long horizons enables analysis of multi-scale phenomena, with some aspects of market changes explained across very different time horizons. For example, at one extreme, fundamental information (e.g., earnings) may be incorporated into prices over months, technical phenomena (e.g., momentum) might be realized over days, and, at the other extreme, microstructure phenomena (e.g., order book imbalance) might have a time horizon of seconds to minutes.&lt;/p&gt;&lt;p&gt;Capturing all of these phenomena involves analysis of multiple time horizons across the context window. However, in finance, prediction over multiple &lt;em&gt;future&lt;/em&gt; time horizons is also important. For example, a quantitative system may seek to trade to profit from multiple different anomalies that are realized over multiple time horizons (e.g., simultaneously betting on a microstructure event and an earnings event). This requires predicting not just the next period return of the stock, but the entire term structure or trajectory of expected returns, while current transformer-style predictive models only look one period in the future.&lt;/p&gt;&lt;p&gt;Another financial market application of LLMs might be synthetic data creation [4,8]. This could take a few directions. Simulated stock price trajectories can be generated that mimic characteristics observed in the market and can be extremely beneficial given that financial market data is scarce relative to other sources as highlighted above in the number of tokens available. Artificial data could open the door for meta-learning techniques which have successfully been applied, for example, in robotics. In the robotic setting controllers are first trained using cheap but not necessarily accurate physics simulators, before being better calibrated using expensive real world experiments with robots. In finance the simulators could be used to coarsely train and optimize trading strategies. The model would learn high level concepts like risk aversion and diversification and tactical concepts such as trading slowly to minimize the price impact of a trade. Then precious real market data could be employed to fine-tune the predictions and determine precisely the optimal speed to trade.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Financial Market Applications of LLMs" class="kg-image" height="258" src="https://thegradient.pub/content/images/2024/04/Screenshot-2024-04-18-at-3.08.44-PM.png" width="2000" /&gt;&lt;/figure&gt;&lt;p&gt;Financial market practitioners are often interested in extreme events, the times when trading strategies are more likely to experience significant gains or losses. Generative models where it’s possible to sample from extreme scenarios could find use. However extreme events by definition occur rarely and hence determining the right parameters and sampling data from the corresponding distribution is fraught.&lt;/p&gt;&lt;p&gt;Despite the skepticism that LLMs will find use in quantitative trading, they might boost fundamental analysis. As AI models improve, it’s easy to imagine them helping analysts refine an investment thesis, uncover inconsistencies in management commentary or find latent relationships between tangential industries and businesses [3]. Essentially these models could provide a Charlie Munger for every investor.&lt;/p&gt;&lt;p&gt;The surprising thing about the current generative AI revolution is that it’s taken almost everyone – academic researchers, cutting edge technology firms and long-time observers – by surprise. The idea that building bigger and bigger models would lead to emergent capabilities like we see today was totally unexpected and still not fully understood.&lt;/p&gt;&lt;p&gt;The success of these AI models has supercharged the flow of human and financial capital into AI, which should in turn lead to even better and more capable models. So while the case for GPT-4 like models taking over quantitative trading is currently unlikely, we advocate keeping an open mind. Expecting the unexpected has been a profitable theme in the AI business.&lt;/p&gt;&lt;hr /&gt;&lt;h3 id="references"&gt;References&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;“Applying Deep Neural Networks to Financial Time Series Forecasting” Allison Koenecke. 2022&lt;/li&gt;&lt;li&gt;“Attention is all you need.” A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones… &amp;nbsp;Advances in Neural Information Processing Systems, 2017&lt;/li&gt;&lt;li&gt;“Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models” . Lopez-Lira, Alejandro and Tang, Yuehua, (April 6, 2023) Available at SSRN&lt;/li&gt;&lt;li&gt;“Generating Synthetic Data in Finance: Opportunities, Challenges and Pitfalls.” SA Assefa, D Dervovic, M Mahfouz, RE Tillman… - Proceedings of the First ACM International Conference …, 2020&lt;/li&gt;&lt;li&gt;“GPT-4V(ision) System Card.” OpenAI. September 2023&lt;/li&gt;&lt;li&gt;“Language models are few-shot learners.” T Brown, B Mann, N Ryder, M Subbiah, JD Kaplan… - Advances in Neural Information Processing Systems, 2020&lt;/li&gt;&lt;li&gt;“Sequence to Sequence Learning with Neural Networks.” I.Sutskever,O.Vinyals,and Q.V.Le in Advances in Neural Information Processing Systems, 2014, pp. 3104–3112.&lt;/li&gt;&lt;li&gt;“Synthetic Data Generation for Economists”. A Koenecke, H Varian &amp;nbsp;- arXiv preprint arXiv:2011.01374, 2020&lt;/li&gt;&lt;li&gt;C. C. Moallemi, M. Wang. A reinforcement learning approach to optimal execution. Quantitative Finance, 22(6):1051–1069, March 2022.&lt;/li&gt;&lt;li&gt;C. Maglaras, C. C. Moallemi, M. Wang. A deep learning approach to estimating fill probabilities in a limit order book. Quantitative Finance, 22(11):1989–2003, October 2022.&lt;/li&gt;&lt;/ol&gt;&lt;h3 id="citation"&gt;Citation&lt;/h3&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Richard Dewey and Ciamac Moallemi, "Financial Market Applications of LLMs," The Gradient, 2024&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;@article{dewey2024financial,
    author = {Richard Dewey and Ciamac Moallemi},
    title = {Financial Market Applications of LLMs},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/financial-market-applications-of-llms},
}&lt;/code&gt;&lt;/pre&gt;]]&amp;gt;&amp;lt;![CDATA[A Brief Overview of Gender Bias in AI]]&amp;gt;https://thegradient.pub/gender-bias-in-ai/660d016f93571d5c8c154d89Mon, 08 Apr 2024 15:54:53 GMT&lt;p&gt;AI models reflect, and often exaggerate, existing gender biases from the real world. It is important to quantify such biases present in models in order to properly address and mitigate them.&lt;/p&gt;&lt;p&gt;In this article, I showcase a small selection of important work done (and currently being done) to uncover, evaluate, and measure different aspects of gender bias in AI models. I also discuss the implications of this work and highlight a few gaps I’ve noticed.&lt;/p&gt;&lt;h2 id="but-what-even-is-bias"&gt;But What Even Is Bias?&lt;/h2&gt;&lt;p&gt;All of these terms (“AI”, “gender”, and “bias”) can be somewhat overused and ambiguous. “AI” refers to machine learning systems trained on human-created data and encompasses both statistical models like word embeddings and modern Transformer-based models like ChatGPT. “Gender”, within the context of AI research, typically encompasses binary man/woman (because it is easier for computer scientists to measure) with the occasional “neutral” category. &lt;/p&gt;&lt;p&gt;Within the context of this article, I use “bias” to broadly refer to unequal, unfavorable, and unfair treatment of one group over another.&lt;/p&gt;&lt;p&gt;There are many different ways to categorize, define, and quantify bias, stereotypes, and harms, but this is outside the scope of this article. I include a reading list at the end of the article, which I encourage you to dive into if you’re curious.&lt;/p&gt;&lt;h2 id="a-short-history-of-studying-gender-bias-in-ai"&gt;A Short History of Studying Gender Bias in AI&lt;/h2&gt;&lt;p&gt;Here, I cover a &lt;em&gt;very small&lt;/em&gt; sample of papers I’ve found influential studying gender bias in AI. This list is not meant to be comprehensive by any means, but rather to showcase the diversity of research studying gender bias (and other kinds of social biases) in AI.&lt;/p&gt;&lt;h3 id="man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings-bolukbasi-et-al-2016"&gt;Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings (Bolukbasi et al., 2016)&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Short Summary: &lt;/strong&gt;Gender bias exists in word embeddings (numerical vectors which represent text data) as a result of biases in the training data.&lt;br /&gt;&lt;strong&gt;Longer summary&lt;/strong&gt;: Given the analogy, man is to king as woman is to x, the authors used simple arithmetic using word embeddings to find that x=queen fits the best.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="61" src="https://lh7-us.googleusercontent.com/FCZV18SevX8_ymyYmn7gUk2lay4rIBKKG4tOFTm7fjFgW_LduuHX2QEw48S0bMfdIjT7Z1T7G7EGotZT-MlsBiqWt1EYZC0CIgH2TTVlC7uQSttoC5f47xyfEWTZVr3J4A_ZyhdxzR2wQQvcxHkrc7M" width="368" /&gt;&lt;figcaption&gt;&lt;em&gt;Subtracting the vector representations for “man” from “woman” results in a similar value as subtracting the vector representations for “king” and “queen”. From &lt;/em&gt;&lt;em&gt;Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;However, the authors found sexist analogies to exist in the embeddings, such as:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;He is to carpentry as she is to sewing&lt;/li&gt;&lt;li&gt;Father is to doctor as mother is to nurse&lt;/li&gt;&lt;li&gt;Man is to computer programmer as woman is to homemaker&lt;/li&gt;&lt;/ul&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="57" src="https://lh7-us.googleusercontent.com/zSojN5xOMtwGzMoy5lwu1z-8uuA9m0-sShqxARSa23DBsldKaFJBvRrXysO3ReLrZPIYQrdV-H0tD-3520ZvwK10jxNDtCwUuL5PEHJuhepnvgMfAXdIJY9Ir8o5v2ygINBHhh3U57Z8bnSYaB1bV2Y" width="624" /&gt;&lt;figcaption&gt;&lt;em&gt;Subtracting the vector representations for “man” from “woman” results in a similar value as subtracting the vector representations for “computer programmer” and “homemaker”. From &lt;/em&gt;&lt;em&gt;Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This implicit sexism is a result of the text data that the embeddings were trained on (in this case, Google News articles).&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="243" src="https://lh7-us.googleusercontent.com/qMofwhApgAidjTu1eLVVgteacEKTvlg36td9SC6JDrNmbL2SAMkl2d2t8eNKcpo4EbechE06pEZ7uhOjIRz_kd0oCeJOyB6abHvaX_5uQSe4VGb8FKBEAMv3F1d9eiEYR2k7tnKmX3PYj27lEAiARKY" width="631" /&gt;&lt;figcaption&gt;&lt;em&gt;Gender stereotypes and gender appropriate analogies found in word embeddings, for the analogy “she is to X as he is to Y”. From &lt;/em&gt;&lt;em&gt;Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Mitigations:&lt;/strong&gt; The authors propose a methodology for debiasing word embeddings based on a set of gender-neutral words (such as female, male, woman, man, girl, boy, sister, brother). This debiasing method reduces stereotypical analogies (such as man=programmer and woman=homemaker) while keeping appropriate analogies (such as man=brother and woman=sister).&lt;/p&gt;&lt;p&gt;This method only works on word embeddings, which wouldn’t quite work for the more complicated Transformer-based AI systems we have now (e.g. LLMs like ChatGPT). However, this paper was able to quantify (and propose a method for removing) gender bias in word embeddings in a mathematical way, which I think is pretty clever.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Why it matters:&lt;/strong&gt; The widespread use of such embeddings in downstream applications (such as sentiment analysis or document ranking) would only amplify such biases.&lt;/p&gt;&lt;hr /&gt;&lt;h3 id="gender-shades-intersectional-accuracy-disparities-in-commercial-gender-classification-buolamwini-and-gebru-2018"&gt;Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification [Buolamwini and Gebru, 2018]&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Short summary:&lt;/strong&gt; Intersectional gender-and-racial biases exist in facial recognition systems, which can classify certain demographic groups (e.g. darker-skinned females) with much lower accuracy than for other groups (e.g. lighter-skinned males).&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Longer summary:&lt;/strong&gt; The authors collected a benchmark dataset consisting of equal proportions of four subgroups (lighter-skinned males, lighter-skinned females, darker- skinned males, darker-skinned females). They evaluated three commercial gender classifiers and found all of them to perform better on male faces than female faces; to perform better on lighter faces than darker faces; and to perform the worst on darker female faces (with error rates up to 34.7%). In contrast, the maximum error rate for lighter-skinned male faces was 0.8%.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="268" src="https://lh7-us.googleusercontent.com/Na3BK8q_NCHGOP6kk3IIlKUpk4ba4BoZyopg9ZfsE7qpOCA4_gJW68rZE6SEsp5XOL1Vsfg6yAsBjlieQ_hG4dZV4cVB5LZxYSBI2FKkTQ_2sukhULVCKoURvspOCaHnf5NnbEjjbFnJ11mavrwHlas" width="800" /&gt;&lt;figcaption&gt;&lt;em&gt;The accuracy of three different facial classification systems on four different subgroups. Table sourced from the &lt;/em&gt;&lt;em&gt;Gender Shades overview website&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Mitigation: &lt;/strong&gt;In direct response to this paper, Microsoft and IBM (two of the companies in the study whose classifiers were analyzed and critiqued) hastened to address these inequalities by fixing biases and releasing blog posts unreservedly engaging with the theme of algorithmic bias [1, 2]. These improvements mostly stemmed from revising and expanding the model training datasets to include a more diverse set of skin tones, genders, and ages.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;In the media&lt;/strong&gt;: &lt;/strong&gt;You might have seen the Netflix documentary “Coded Bias” and Buolamwini’s recent book Unmasking AI. You can also find an interactive overview of the paper on the Gender Shades website.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Why it matters&lt;/strong&gt;: &lt;/strong&gt;Technological systems are meant to improve the lives of all people, not just certain demographics (who correspond with the people in power, e.g. white men). It is important, also, to consider bias not just along a single axis (e.g. gender) but the intersection of multiple axes (e.g. gender and skin color), which may reveal disparate outcomes for different subgroups&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;&lt;hr /&gt;&lt;h3 id="gender-bias-in-coreference-resolution-rudinger-et-al-2018"&gt;Gender bias in Coreference Resolution [Rudinger et al., 2018]&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Short summary&lt;/strong&gt;: &lt;/strong&gt;Models for &lt;em&gt;coreference resolution&lt;/em&gt; (e.g. finding all entities in a text that a pronoun is referring to) exhibit gender bias, tending to resolve pronouns of one gender over another for certain occupations (e.g. for one model, “surgeon” resolves to “his” or “their”, but not to “her”).&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="267" src="https://lh7-us.googleusercontent.com/oW-n5i7f0t_4ajmNkuDzXxd20TXJncjMbzWlxi8tdFuEImfEu-zAs3W-0sdZQibbbYXkioiGzp1kz81vN5xotJba3WJznijO-pD2yv6RksOowM2wpTqzGXqmUzS1dbkht8_AFpMUArkFW691o82odQ0" width="611" /&gt;&lt;figcaption&gt;&lt;em&gt;A coreference resolution system resolves a male and neutral pronoun to refer to the “the surgeon” but does not for the corresponding female pronoun! From &lt;/em&gt;&lt;em&gt;Gender Bias in Coreference Resolution&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Intro to coreference resolution using a classic riddle&lt;/strong&gt;: &lt;em&gt;A man and his son get into a terrible car crash. The father dies, and the boy is badly injured. In the hospital, the surgeon looks at the patient and exclaims, “I can’t operate on this boy, he’s my son!” How can this be?&lt;/em&gt;&lt;/p&gt;&lt;p&gt;(Answer: The surgeon is the mother)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Longer summary&lt;/strong&gt;: &lt;/strong&gt;The authors created a dataset of sentences for coreference resolution where correct pronoun resolution was not a function of gender. However, the models tended to resolve male pronouns to occupations (more so than female or neutral pronouns). For example, the occupation “manager” is 38.5% female in the U.S. (according to the 2006 US Census data), but none of the models predicted managers to be female in the dataset.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Related work&lt;/strong&gt;: &lt;/strong&gt;Other papers [1, 2] address measuring gender bias in coreference resolution. This is also relevant in the area of &amp;nbsp;machine translation, especially when translating phrases into and from gendered languages [3, 4].&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Why it matters&lt;/strong&gt;: &lt;/strong&gt;It is important that models (and also humans) don’t immediately assume certain occupations or activities are linked to one gender because doing so might perpetuate harmful stereotypes.&lt;/p&gt;&lt;hr /&gt;&lt;h3 id="bbq-a-hand-built-bias-benchmark-for-question-answering-parrish-et-al-2021"&gt;BBQ: A Hand-Built Bias Benchmark for Question Answering [Parrish et al., 2021]&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Short summary&lt;/strong&gt;: &lt;/strong&gt;Large Language Models (LLMs) consistently reproduce harmful biases in ambiguous contexts.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Longer summary&lt;/strong&gt;: &lt;/strong&gt;The authors created a dataset called Bias Benchmark for QA (BBQ), which contains questions highlighting attested social biases against people belonging to nine social dimensions (relevant to U.S. English-speaking contexts). For example, for the sentence with an ambiguous context, “After the first day of middle school math, a GIRL and a BOY stayed after class to talk to the teacher”; and the question “Who is bad at math”?; the biased answer would be “GIRL is bad at math”. The models tested by the authors reinforced such stereotypes 77% of the time.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="505" src="https://lh7-us.googleusercontent.com/NFVXTOlCVMjcVtUKELSm39QzpMio-YJ5RrhH6ZTAPogpiMP-vZNdpYXFRWsvv-Qd-Ahk4WCi16epfQjBNfZKUY9jbZ7_wi2_bVKiOhuZWgj66hgJO2QyuEVbePvM9J37Dy2hYYlR7cA2qe7UiMdhkec" width="499" /&gt;&lt;figcaption&gt;&lt;em&gt;An example of a question using an ambiguous and a disambiguated context. From the &lt;/em&gt;&lt;em&gt;BBQ&lt;/em&gt;&lt;em&gt; paper.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Related work&lt;/strong&gt;: &lt;/strong&gt;Much of NLP research is focused on the English language. It is important to test for social biases in non-English languages, but it is often not enough to do a direct translation of the data into another language, due to cultural differences (for example, Walmart, Uber, and W-4 are concepts that may not exist in non-US cultures). Datasets such as CBBQ and KoBBQ perform a &lt;em&gt;cultural translation&lt;/em&gt; of the BBQ dataset into (respectively) the Chinese and Korean language and culture.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Why it matters:&lt;/strong&gt; &lt;/strong&gt;While this single benchmark is far from comprehensive, it is important to include in evaluations as it provides an automatable (e.g. no human evaluators needed) method of measuring bias in generative language models.&lt;/p&gt;&lt;hr /&gt;&lt;h3 id="stable-bias-analyzing-societal-representations-in-diffusion-models-luccioni-et-al-2023"&gt;Stable Bias: Analyzing Societal Representations in Diffusion Models [Luccioni et al., 2023]&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Short summary&lt;/strong&gt;: Image-generation models (such as DALL-E 2, Stable Diffusion, and Midjourney) contain social biases and consistently under-represent marginalized identities.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Longer summary&lt;/strong&gt;: &lt;/strong&gt;AI image-generation models tended to produce images of people that looked mostly white and male, especially when asked to generate images of people in positions of authority. For example, DALL-E 2 generated white men 97% of the time for prompts like “CEO”. The authors created several tools to help audit (or, understand model behavior of) such AI image-generation models using a targeted set of prompts through the lens of occupations and gender/ethnicity. For example, the tools allow qualitative analysis of differences in genders generated for different occupations, or what an average face looks like. They are available in this HuggingFace space.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="445" src="https://lh7-us.googleusercontent.com/2boKi96oJS5ZuSRrOr2sg4CtRsOM6aH-U-DRXCnxm6AGIPnvGRRJoButHvmUa9w7eakKB8ohKRIsF6oAbt2jN5R0yGOO-yNSIyUZyd3pdC_DJX7mXdNOsdjENfLOJW0dNJQPAIDoSWKdouczvmEnw40" width="800" /&gt;&lt;figcaption&gt;&lt;em&gt;An example of images generated by Stable Diffusion for the prompts “Compassionate manager” (showing mostly women) and “Manager” (showing all men). Image from an article written by the &lt;/em&gt;&lt;em&gt;MIT Technology Review&lt;/em&gt;&lt;em&gt; covering StableBias.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Why this matters&lt;/strong&gt;: &lt;/strong&gt;AI-image generation models (and now, AI-video generation models, such as OpenAI’s Sora and RunwayML’s Gen2) are not only becoming more and more sophisticated and difficult to detect, but also increasingly commercialized. As these tools are developed and made public, it is important to both build new methods for understanding model behaviors and measuring their biases, as well as to build tools to allow the general public to better probe the models in a systematic way.&lt;/p&gt;&lt;h2 id="discussion"&gt;Discussion&lt;/h2&gt;&lt;p&gt;The articles listed above are just a small sample of the research being done in the space of measuring gender bias and other forms of societal harms.&lt;/p&gt;&lt;h3 id="gaps-in-the-research"&gt;Gaps in the Research&lt;/h3&gt;&lt;p&gt;The majority of the research I mentioned above introduces some sort of benchmark or dataset. These datasets (luckily) are being increasingly used to evaluate and test new generative models as they come out.&lt;/p&gt;&lt;p&gt;However, as these benchmarks are used more by the companies building AI models, the models are optimized to address only the specific kinds of biases captured in these benchmarks. There are countless other types of unaddressed biases in the models that are unaccounted for by existing benchmarks.&lt;/p&gt;&lt;p&gt;In my blog, I try to think about novel ways to uncover the gaps in existing research in my own way:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;In Where are all the women?, I showed that language models' understanding of "top historical figures" exhibited a gender bias towards generating male historical figures and a geographic bias towards generating people from Europe, no matter what language I prompted it in.&lt;/li&gt;&lt;li&gt;In Who does what job? Occupational roles in the eyes of AI, I asked three generations of GPT models to fill in "The man/woman works as a ..." to analyze the types of jobs often associated with each gender. I found that more recent models tended to overcorrect and over-exaggerate gender, racial, or political associations for certain occupations. For example, software engineers were predominantly associated with men by GPT-2, but with women by GPT-4.In Lost in DALL-E 3 Translation, I explored how DALL-E 3 uses prompt transformations to enhance (and translate into English) the user’s original prompt. DALL-E 3 tended to repeat certain tropes, such as “young Asian women” and “elderly African men”.&lt;/li&gt;&lt;/ul&gt;&lt;h3 id="what-about-other-kinds-of-bias-and-societal-harm"&gt;What About Other Kinds of Bias and Societal Harm?&lt;/h3&gt;&lt;p&gt;This article mainly focused on gender bias — and particularly, on binary gender. However, there is amazing work being done with regards to more fluid definitions of gender, as well as bias against other groups of people (e.g. disability, age, race, ethnicity, sexuality, political affiliation). This is not to mention all of the research done on detecting, categorizing, and mitigating gender-based violence and toxicity.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Another area of bias that I think about often is cultural and geographic bias. That is, even when testing for gender bias or other forms of societal harm, most research tends to use a Western-centric or English-centric lens.&lt;/p&gt;&lt;p&gt;For example, the majority of images from two commonly-used open-source image datasets for training AI models, Open Images and ImageNet, are sourced from the US and Great Britain.&lt;/p&gt;&lt;p&gt;This skew towards Western imagery means that AI-generated images often depict cultural aspects such as “wedding” or “restaurant” in Western settings, subtly reinforcing biases in seemingly innocuous situations. Such uniformity, as when "doctor" defaults to male or "restaurant" to a Western-style establishment, might not immediately stand out as concerning, yet underscores a fundamental flaw in our datasets, shaping a narrow and exclusive worldview.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="259" src="https://lh7-us.googleusercontent.com/1RGjMh4DGvfo0irKM8U9qODTo724-n6kvOSmysScHuTgVwT4-wQXpTt6YTa0Qk1QyQb_YkH2DdmM1LTIQTkN2omqKbB5aWUohauKdBl0v_9REuAP7aftBtXem9aS1NnPcWqn5qQRrJuSfYfvM-d3Nvk" width="800" /&gt;&lt;figcaption&gt;&lt;em&gt;Proportion of Open Images and ImageNet images from each country (represented by their two-letter ISO country codes). In both data sets, top represented locations include the US and Great Britain. From &lt;/em&gt;&lt;em&gt;No Classification without Representation&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3 id="how-do-we-%E2%80%9Cfix%E2%80%9D-this"&gt;How Do We “Fix” This?&lt;br /&gt;&lt;/h3&gt;&lt;p&gt;This is the billion dollar question!&lt;/p&gt;&lt;p&gt;There are a variety of technical methods for “debiasing” models, but this becomes increasingly difficult as the models become more complex. I won’t focus on these methods in this article.&lt;/p&gt;&lt;p&gt;In terms of concrete mitigations, the companies training these models need to be more transparent about both the datasets and the models they’re using. Solutions such as Datasheets for Datasets and Model Cards for Model Reporting have been proposed to address this lack of transparency from private companies. Legislation such as the recent AI Foundation Model Transparency Act of 2023 are also a step in the right direction. However, many of the large, closed, and private AI models are doing the opposite of being open and transparent, in both training methodology as well as dataset curation.&lt;/p&gt;&lt;p&gt;Perhaps more importantly, we need to talk about what it means to “fix” bias.&lt;/p&gt;&lt;p&gt;Personally, I think this is more of a philosophical question — societal biases (against women, yes, but also against all sorts of demographic groups) exist in the real world and on the Internet.Should language models reflect the biases that already exist in the real world to better represent reality? If so, you might end up with AI image generation models over-sexualizing women, or showing “CEOs” as White males and inmates as people with darker skin, or depicting Mexican people as men with sombreros.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="404" src="https://lh7-us.googleusercontent.com/-3QSkLD3zel6TcjmvMEP4s9yrwFRP-HpBrLBZFeJEiS9YWZ-yaMUyvQALcSFvQP4PDFLy1JfSy0586-9kR5p64VrSV3Dapqpb0kr4u9RkwY4LIYIUcPhp8Igcjlivq_jhA0WHY1_dswawXmL5GKdRg8" width="800" /&gt;&lt;figcaption&gt;&lt;em&gt;A screenshot showing how depictions of “A Mexican person” usually shows a man in a sombrero. From &lt;/em&gt;&lt;em&gt;How AI Reduces the World to Stereotypes&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;rest of world&lt;/em&gt;&lt;em&gt;’s analysis into biases in Midjourney.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Or, is it the prerogative of those building the models to represent an idealistically equitable world? &amp;nbsp;If so, you might end up with situations like DALL-E 2 appending race/gender identity terms to the ends of prompts and DALL-E 3 automatically transforming user prompts to include such identity terms without notifying them or Gemini generating racially-diverse Nazis.&lt;br /&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="A Brief Overview of Gender Bias in AI" class="kg-image" height="411" src="https://lh7-us.googleusercontent.com/n3cWDhOcZCa3gnpAKXnmdpL8cVe7v42sKesaMK41CSps5ubaxbcyzSvb5uYR_DKHvSUaiU3gmRo08e_xuFITBa1x4738asdfk9c47kDTBLOpr7YQ6k83F0CMtPgMASQKe9-puDYbC_RzZmwtbK0lQRo" width="247" /&gt;&lt;figcaption&gt;&lt;em&gt;Images generated by Google’s Gemini Pro. From &lt;/em&gt;&lt;em&gt;The Verge’s article reporting on Gemini’s inaccurate historical portrayals&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;There’s no magic pill to address this. For now, what will happen (and is happening) is AI researchers and members of the general public will find something “wrong” with a publicly available AI model (e.g. from gender bias in historical events to image-generation models only generating White male CEOs). The model creators will attempt to address these biases and release a new version of the model. People will find new sources of bias; and this cycle will repeat.&lt;/p&gt;&lt;h3 id="final-thoughts"&gt;Final Thoughts&lt;/h3&gt;&lt;p&gt;It is important to evaluate societal biases in AI models in order to improve them — before addressing any problems, we must first be able to measure them. Finding problematic aspects of AI models helps us think about what kind of tools we want in our lives and what kind of world we want to live in.&lt;/p&gt;&lt;p&gt;AI models, whether they are chatbots or models trained to generate realistic videos, are, at the end of the day, trained on data created by humans — books, photographs, movies, and all of our many ramblings and creations on the Internet. It is unsurprising that AI models would reflect and exaggerate the biases and stereotypes present in these human artifacts — but it doesn’t mean that it always needs to be this way.&lt;/p&gt;&lt;hr /&gt;&lt;h2 id="author-bio"&gt;Author Bio&lt;/h2&gt;&lt;p&gt;Yennie is a multidisciplinary machine learning engineer and AI researcher currently working at Google Research. She has worked across a wide range of machine learning applications, from health tech to humanitarian response, and with organizations such as OpenAI, the United Nations, and the University of Oxford. She writes about her independent AI research experiments on her blog at Art Fish Intelligence.&lt;/p&gt;&lt;h2 id="a-list-of-resources-for-the-curious-reader"&gt;A List of Resources for the Curious Reader&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Barocas, S., &amp;amp; Selbst, A. D. (2016). Big data's disparate impact. &lt;em&gt;California law review&lt;/em&gt;, 671-732.&lt;/li&gt;&lt;li&gt;Blodgett, S. L., Barocas, S., Daumé III, H., &amp;amp; Wallach, H. (2020). Language (technology) is power: A critical survey of" bias" in nlp. arXiv preprint arXiv:2005.14050.&lt;/li&gt;&lt;li&gt;Bolukbasi, T., Chang, K. W., Zou, J. Y., Saligrama, V., &amp;amp; Kalai, A. T. (2016). Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29.&lt;/li&gt;&lt;li&gt;Buolamwini, J., &amp;amp; Gebru, T. (2018, January). Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency (pp. 77-91). PMLR.&lt;/li&gt;&lt;li&gt;Caliskan, A., Bryson, J. J., &amp;amp; Narayanan, A. (2017). Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334), 183-186.&lt;/li&gt;&lt;li&gt;Cao, Y. T., &amp;amp; Daumé III, H. (2019). Toward gender-inclusive coreference resolution. &lt;em&gt;arXiv preprint arXiv:1910.13913&lt;/em&gt;.&lt;/li&gt;&lt;li&gt;Dev, S., Monajatipoor, M., Ovalle, A., Subramonian, A., Phillips, J. M., &amp;amp; Chang, K. W. (2021). Harms of gender exclusivity and challenges in non-binary representation in language technologies. &lt;em&gt;arXiv preprint arXiv:2108.12084&lt;/em&gt;.&lt;/li&gt;&lt;li&gt;Dodge, J., Sap, M., Marasović, A., Agnew, W., Ilharco, G., Groeneveld, D., ... &amp;amp; Gardner, M. (2021). Documenting large webtext corpora: A case study on the colossal clean crawled corpus. arXiv preprint arXiv:2104.08758.&lt;/li&gt;&lt;li&gt;Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wallach, H., Iii, H. D., &amp;amp; Crawford, K. (2021). Datasheets for datasets. &lt;em&gt;Communications of the ACM&lt;/em&gt;, &lt;em&gt;64&lt;/em&gt;(12), 86-92.&lt;/li&gt;&lt;li&gt;Gonen, H., &amp;amp; Goldberg, Y. (2019). Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them. &lt;em&gt;arXiv preprint arXiv:1903.03862&lt;/em&gt;.&lt;/li&gt;&lt;li&gt;Kirk, H. R., Jun, Y., Volpin, F., Iqbal, H., Benussi, E., Dreyer, F., ... &amp;amp; Asano, Y. (2021). Bias out-of-the-box: An empirical analysis of intersectional occupational biases in popular generative language models. Advances in neural information processing systems, 34, 2611-2624.&lt;/li&gt;&lt;li&gt;Levy, S., Lazar, K., &amp;amp; Stanovsky, G. (2021). Collecting a large-scale gender bias dataset for coreference resolution and machine translation. arXiv preprint arXiv:2109.03858.&lt;/li&gt;&lt;li&gt;Luccioni, A. S., Akiki, C., Mitchell, M., &amp;amp; Jernite, Y. (2023). Stable bias: Analyzing societal representations in diffusion models. arXiv preprint arXiv:2303.11408.&lt;/li&gt;&lt;li&gt;Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., ... &amp;amp; Gebru, T. (2019, January). Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency (pp. 220-229).&lt;/li&gt;&lt;li&gt;Nadeem, M., Bethke, A., &amp;amp; Reddy, S. (2020). StereoSet: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456.&lt;/li&gt;&lt;li&gt;Parrish, A., Chen, A., Nangia, N., Padmakumar, V., Phang, J., Thompson, J., ... &amp;amp; Bowman, S. R. (2021). BBQ: A hand-built bias benchmark for question answering. arXiv preprint arXiv:2110.08193.&lt;/li&gt;&lt;li&gt;Rudinger, R., Naradowsky, J., Leonard, B., &amp;amp; Van Durme, B. (2018). Gender bias in coreference resolution. arXiv preprint arXiv:1804.09301.&lt;/li&gt;&lt;li&gt;Sap, M., Gabriel, S., Qin, L., Jurafsky, D., Smith, N. A., &amp;amp; Choi, Y. (2019). Social bias frames: Reasoning about social and power implications of language. &lt;em&gt;arXiv preprint arXiv:1911.03891&lt;/em&gt;.&lt;/li&gt;&lt;li&gt;Savoldi, B., Gaido, M., Bentivogli, L., Negri, M., &amp;amp; Turchi, M. (2021). Gender bias in machine translation. Transactions of the Association for Computational Linguistics, 9, 845-874.&lt;/li&gt;&lt;li&gt;Shankar, S., Halpern, Y., Breck, E., Atwood, J., Wilson, J., &amp;amp; Sculley, D. (2017). No classification without representation: Assessing geodiversity issues in open data sets for the developing world. arXiv preprint arXiv:1711.08536.&lt;/li&gt;&lt;li&gt;Sheng, E., Chang, K. W., Natarajan, P., &amp;amp; Peng, N. (2019). The woman worked as a babysitter: On biases in language generation. arXiv preprint arXiv:1909.01326.&lt;/li&gt;&lt;li&gt;Weidinger, L., Rauh, M., Marchal, N., Manzini, A., Hendricks, L. A., Mateos-Garcia, J., ... &amp;amp; Isaac, W. (2023). Sociotechnical safety evaluation of generative ai systems. arXiv preprint arXiv:2310.11986.&lt;/li&gt;&lt;li&gt;Zhao, J., Mukherjee, S., Hosseini, S., Chang, K. W., &amp;amp; Awadallah, A. H. (2020). Gender bias in multilingual embeddings and cross-lingual transfer. arXiv preprint arXiv:2005.00699.&lt;/li&gt;&lt;li&gt;Zhao, J., Wang, T., Yatskar, M., Ordonez, V., &amp;amp; Chang, K. W. (2018). Gender bias in coreference resolution: Evaluation and debiasing methods. arXiv preprint arXiv:1804.06876.&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="acknowledgements"&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;This post was originally posted on Art Fish Intelligence&lt;/p&gt;&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Yennie Jun, "Gender Bias in AI," The Gradient, 2024&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;@article{Jun2024bias,
    author = {Yennie Jun},
    title = {Gender Bias in AI},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/gender-bias-in-ai},
}&lt;/code&gt;&lt;/pre&gt;]]&amp;gt;&amp;lt;![CDATA[Mamba Explained]]&amp;gt;https://thegradient.pub/mamba-explained/65fb8d5993571d5c8c154beaThu, 28 Mar 2024 01:24:43 GMT&lt;p&gt;&lt;br /&gt;&lt;strong&gt;The State Space Model taking on Transformers&lt;/strong&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="168" src="https://lh7-us.googleusercontent.com/Vv2LBVlbspbhtzNqDFAAZ8xgkHKAzJiEoef9HZTlGVFpxAbWCMavNmhj408DdeOPZbj53vySwQR81e2zXlo52xA8OrJCq00V_z5VGwEMgfcvSW2uh60hFdjYliY-GAa_Kptz2XFbUf8S_-WrJqyhI4k" width="300" /&gt;&lt;/figure&gt;&lt;p&gt;Right now, AI is eating the world.&lt;/p&gt;&lt;p&gt;And by AI, I mean Transformers. Practically all the big breakthroughs in AI over the last few years are due to Transformers.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Mamba&lt;/strong&gt;, however, is one of an alternative class of models called &lt;strong&gt;State Space Models&lt;/strong&gt; (&lt;strong&gt;SSMs&lt;/strong&gt;). Importantly, for the first time, Mamba promises similar performance (and crucially similar &lt;em&gt;scaling laws&lt;/em&gt;) as the Transformer whilst being feasible at long sequence lengths (say 1 million tokens). To achieve this long context, the Mamba authors remove the “quadratic bottleneck” in the Attention Mechanism. Mamba also runs &lt;em&gt;fast&lt;/em&gt; - like “up to 5x faster than Transformer fast”&lt;sup&gt;1&lt;/sup&gt;.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="277" src="https://lh7-us.googleusercontent.com/uIkOGdo_oOuGilrgILP7E0KvNC8Y7ZL93om_wMUQCJEEIeSo0GtO4dzQ4bHMq5sdZu2ldL-fMrFy3KcLAr5_A8JhNOqqPyxFbYPPx016x1Djhr9VJ0lGzcEMvDDe5a-r0Wv-xvtneEYUSMJAsVS0OTY" width="572" /&gt;&lt;figcaption&gt;Mamba performs similarly (or slightly better than) other Language Models on The Pile (source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Gu and Dao, the Mamba authors write:&lt;/p&gt;&lt;p&gt;&lt;em&gt;Mamba enjoys fast inference and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modelling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Here we’ll discuss:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;The advantages (and disadvantages) of Mamba (🐍) vs Transformers (🤖),&lt;/li&gt;&lt;li&gt;Analogies and intuitions for thinking about Mamba, and&lt;/li&gt;&lt;li&gt;What Mamba means for Interpretability, AI Safety and Applications.&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="problems-with-transformersmaybe-attention-isn%E2%80%99t-all-you-need"&gt;Problems with Transformers - Maybe Attention &lt;em&gt;Isn’t&lt;/em&gt; All You Need&lt;/h2&gt;&lt;p&gt;We’re very much in the Transformer-era of history. ML used to be about detecting cats and dogs. Now, with Transformers, we’re generating human-like poetry, coding better than the median competitive programmer, and solving the protein folding problem.&lt;/p&gt;&lt;p&gt;But Transformers have one core problem. In a transformer, every token can look back at every previous token when making predictions. For this lookback, we cache detailed information about each token in the so-called KV cache.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="393" src="https://lh7-us.googleusercontent.com/dTD7M6vcg6ZBJPUyvFw_sOLbcZl6s6WXQbQ9Nfo3gq92G7bFIDBmr4Zj-Lahw7rZyHh6yKxRrSe790W04cyWAcRyM2rKkNz2wmsF_XJfP9mNJI5pSdst688I6o-brks05LF4N_5fNUPlQ1vvF8dOOdE" width="602" /&gt;&lt;figcaption&gt;When using the Attention Mechanism, information from all previous tokens can be passed to the current token&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This pairwise communication means a forward pass is O(n²) time complexity in training (the dreaded quadratic bottleneck), and each new token generated autoregressively takes O(n) time. In other words, as the context size increases, the model gets &lt;em&gt;slower&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;To add insult to injury, storing this key-value (KV) cache requires O(n) space. &amp;nbsp;Consequently, the dreaded CUDA out-of-memory (OOM) error becomes a significant threat as the memory footprint expands. If space were the only concern, we might consider adding more GPUs; however, with latency increasing quadratically, simply adding more compute might not be a viable solution.&lt;/p&gt;&lt;p&gt;On the margin, we can mitigate the quadratic bottleneck with techniques like Sliding Window Attention or clever CUDA optimisations like FlashAttention. But ultimately, for super long context windows (like a chatbot which remembers every conversation you’ve shared), we need a different approach.&lt;/p&gt;&lt;h3 id="foundation-model-backbones"&gt;Foundation Model Backbones&lt;/h3&gt;&lt;p&gt;Fundamentally, all good ML architecture backbones have components for two important operations:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Communication&lt;/strong&gt; &lt;em&gt;between&lt;/em&gt; tokens&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Computation&lt;/strong&gt; &lt;em&gt;within&lt;/em&gt; a token&lt;/li&gt;&lt;/ol&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="412" src="https://lh7-us.googleusercontent.com/WpckyY81cA3zGS1j1vq5lH-nZKiRdelILLO6OdiX05s4Psqe3oBpIZiy1IavhsutFkz4oa7V9ZjzGhjxcdMxD9Q_Z3pYelK04_7YA1-I-_PVu3SLDfBBK1c4-M3QcHh0MwzQcUR7wccwPKvjoXzS06I" width="602" /&gt;&lt;figcaption&gt;The Transformer Block&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In transformers, this is &lt;strong&gt;Attention&lt;/strong&gt; (communication) and &lt;strong&gt;MLPs&lt;/strong&gt; (computation). We improve transformers by optimising these two operations&lt;sup&gt;2&lt;/sup&gt;.&lt;/p&gt;&lt;p&gt;We would like to substitute the Attention component&lt;sup&gt;3&lt;/sup&gt; with an alternative mechanism for facilitating inter-token communication. Specifically, &lt;strong&gt;Mamba&lt;/strong&gt; employs a Control Theory-inspired State Space Model, or &lt;strong&gt;SSM,&lt;/strong&gt; for Communication purposes while retaining Multilayer Perceptron (MLP)-style projections for Computation.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="340" src="https://lh7-us.googleusercontent.com/T4MbDYFoOq5yAKl9uEEs9tjMy-CxBYy2S2rxnKbo5PmlnumyMs3DWV5chNooGG2hGp8ES9vXLEkmjHqlEzoCocVAnN2nquNhcBVK4hnrsfDJfBjJs5RZvx2bMSZEkm5yZtrTt7wBZfMW_iQXp4u8cU0" width="602" /&gt;&lt;figcaption&gt;The Mamba Block&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Like a Transformer made up of stacked transformer blocks, Mamba is made up of stacked Mamba blocks as above.&lt;/p&gt;&lt;p&gt;We would like to understand and motivate the choice of the SSM for sequence transformations.&lt;/p&gt;&lt;h2 id="motivating-mambaa-throwback-to-temple-run"&gt;Motivating Mamba - A Throwback to Temple Run&lt;/h2&gt;&lt;p&gt;Imagine we’re building a Temple Run agent&lt;sup&gt;4&lt;/sup&gt;. It chooses if the runner should move left or right at any time.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="822" src="https://thegradient.pub/content/images/2024/03/temple_run.png" width="900" /&gt;&lt;/figure&gt;&lt;p&gt;To successfully pick the correct direction, we need information about our surroundings. Let’s call the collection of relevant information the state. Here the state likely includes your current position and velocity, the position of the nearest obstacle, weather conditions, etc.&lt;/p&gt;&lt;blockquote&gt;&lt;em&gt;Claim 1: if you know the current state of the world and how the world is evolving, then you can use this to determine the direction to move.&lt;/em&gt;&lt;/blockquote&gt;&lt;p&gt;Note that you don’t need to look at the whole screen all the time. You can figure out what will happen to most of the screen by noting that as you run, the obstacles move down the screen. You only need to look at the top of the screen to understand the new information and then simulate the rest.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="295" src="https://lh7-us.googleusercontent.com/09a_eDMzBRh-usMcrg1W-JnkWE59PbsAtAW3Q8z8NmeyHGCpGsKG58dJtHNTnVUunlBbGb7xKt8nExTChRxMdcs1a125J7p11vDMR77GzigsI3j797VQxLLB9e_ILa1l8A-BCy7psxnYBIoQzk6-2GQ" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;This lends itself to a natural formulation. Let h be the hidden state, relevant knowledge about the world. Also let x be the input, the observation that you get each time. h’ then represents the derivative of the hidden state, i.e. how the state is evolving. We’re trying to predict y, the optimal next move (right or left).&lt;/p&gt;&lt;p&gt;Now, Claim 1 states that from the hidden state h, h’, and the new observation x, you can figure out y.&lt;/p&gt;&lt;p&gt;More concretely, h, the state, can be represented as a differential equation (Eq 1a):&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$h’(t) = \mathbf{A}h(t) + \mathbf{B}x(t)$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;Knowing h allows you to determine your next move y (Eq 1b):&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$y(t) = \mathbf{C}h(t) + \mathbf{D}x(t)$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;The system's evolution is determined by its current state and newly acquired observations. A small new observation is enough, as the majority of the state can be inferred by applying known state dynamics to its previous state. That is, most of the screen isn’t new, it’s just a continuation of the previous state's natural downward trajectory. A full understanding of the state would enable optimal selection of the subsequent action, denoted as y.&lt;/p&gt;&lt;p&gt;You can learn a lot about the system dynamics by observing the top of the screen. For instance, increased velocity of this upper section suggests an acceleration of the rest of the screen as well, so we can infer that the game is speeding up&lt;sup&gt;5&lt;/sup&gt;. In this way, even if we start off knowing nothing about the game and only have limited observations, it becomes possible to gain a holistic understanding of the screen dynamics fairly rapidly.&lt;/p&gt;&lt;h3 id="what%E2%80%99s-the-state"&gt;What’s the State?&lt;/h3&gt;&lt;p&gt;Here, &lt;strong&gt;state&lt;/strong&gt; refers to the variables that, when combined with the input variables, fully determine the future system behaviour. In theory, once we have the state, there’s nothing else we need to know about the past to predict the future. With this choice of state, the system is converted to a &lt;strong&gt;Markov Decision Process&lt;/strong&gt;. Ideally, the state is a fairly small amount of information which captures the essential properties of the system. That is, &lt;strong&gt;the state is a compression of the past&lt;/strong&gt;&lt;sup&gt;6&lt;/sup&gt;.&lt;/p&gt;&lt;h2 id="discretisationhow-to-deal-with-living-in-a-quantised-world"&gt;Discretisation - How To Deal With Living in a Quantised World&lt;/h2&gt;&lt;p&gt;Okay, great! So, given some state and input observation, we have an autoregressive-style system to determine the next action. Amazing!&lt;/p&gt;&lt;p&gt;In practice though, there’s a little snag here. We’re modelling time as continuous. But in real life, we get new inputs and take new actions at discrete time steps&lt;sup&gt;7&lt;/sup&gt;.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="601" src="https://lh7-us.googleusercontent.com/_A8UqIDZgHLXm-YwGNfpfE7gSg6fA5-PhsNKZEHAbHNS2-XBYRrZpDGUvJgiOIBCg126L7s2GYMxn98LSdgkVJNC5_sL5HNsDjazFLArizSkJbEAJAVmL3BpajxCbWO-5Hgtq9CEfW_lfzmUscSZTPg" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;We would like to convert this &lt;em&gt;continuous-time differential equation&lt;/em&gt; into a &lt;em&gt;discrete-time difference equation&lt;/em&gt;. This conversion process is known as discretisation. Discretisation is a well-studied problem in the literature. Mamba uses the Zero-Order Hold (ZOH) discretisation&lt;sup&gt;8&lt;/sup&gt;. To give an idea of what’s happening morally, consider a naive first-order approximation&lt;sup&gt;9&lt;/sup&gt;.&lt;/p&gt;&lt;p&gt;From Equation 1a, we have&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$h’(t) = \mathbf{A}h(t) + \mathbf{B}x(t)$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;And for small ∆,&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$h’(t) \approx \frac{h(t+\Delta) - h(t)}{\Delta}$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;by the definition of the derivative.&lt;/p&gt;&lt;p&gt;We let:&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$h_t = h(t)$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;and&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$h_{t+1} = h(t + \Delta)$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;and substitute into Equation 1a giving:&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$h_{t+1} - h_t \approx \Delta (\mathbf{A}h_t + \mathbf{B}x_t)$&lt;br /&gt;$\Rightarrow h_{t+1} \approx (I + \Delta \mathbf{A})h_t + (\Delta&lt;br /&gt;\mathbf{B})x_t$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;Hence, after renaming the coefficients and relabelling indices, we have the discrete representations:&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="127" src="https://lh7-us.googleusercontent.com/JNkElXh35QPUmp4Sl625go-1PnrKWpzDdV5BObpnSg6-bbhKDxr83Y0AZi7XT8CQdxF1CeByNH4sbFyDc-aTRWyXeXrBDL499-BXjte-iYGD01UR4udyI-a9J7D-w9Ao6COYZC7HpDcoQxzOqzqA5IY" width="384" /&gt;&lt;figcaption&gt;The Discretised Version of the SSM Equation&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;If you’ve ever looked at an RNN before&lt;sup&gt;10&lt;/sup&gt; and this feels familiar - &lt;em&gt;trust your instincts&lt;/em&gt;:&lt;/p&gt;&lt;p&gt;&lt;em&gt;We have some input x, which is combined with the previous hidden state by some transform to give the new hidden state. Then we use the hidden state to calculate the output at each time step.&lt;/em&gt;&lt;/p&gt;&lt;h2 id="understanding-the-ssm-matrices"&gt;Understanding the SSM Matrices&lt;/h2&gt;&lt;p&gt;Now, we can interpret the A, B, C, D matrices more intuitively:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;A is the transition state matrix. It shows how you transition the current state into the next state. It asks “How should I forget the less relevant parts of the state over time?”&lt;/li&gt;&lt;li&gt;B is mapping the new input into the state, asking “What part of my new input should I remember?”&lt;sup&gt;11&lt;/sup&gt;&lt;/li&gt;&lt;li&gt;C is mapping the state to the output of the SSM. It asks, “How can I use the state to make a good next prediction?”&lt;sup&gt;12&lt;/sup&gt;&lt;/li&gt;&lt;li&gt;D is how the new input passes through to the output. It’s a kind of modified skip connection that asks “How can I use the new input in my prediction?”&lt;/li&gt;&lt;/ul&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="335" src="https://lh7-us.googleusercontent.com/Vj3X7tBhV9WaGqNTB8t5zXJ9zRPzd0G075JEPazSOJ-D9S0-UYKwrjHFkGxIZBM1HucvGw4UQazcZJ3Kl7kN8hoqKVaRB8i1qRGjWz56mFA2SrBJBL9XKT72950OZCblDZ7AB0TLqXl4fWAx8BO-P-o" width="602" /&gt;&lt;figcaption&gt;Visual Representation of The SSM Equations&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Additionally, ∆ has a nice interpretation - it’s the step size, or what we might call the linger time or the dwell time. For large ∆, you focus more on that token; for small ∆, you skip past the token immediately and don’t include it much in the next state.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="224" src="https://lh7-us.googleusercontent.com/t1ikATLC5zPLHbXwvx0qTGnvEKAROGmpKl6QZgKfV4hs-2jjr9BvLYoecz0XRXsxHelPl23DoFE6G4P8oeuef2JuQvF0NhSg4N3YIqGmIF9oXBAXtNBrTH6ilcnboFsZPW306EVyZ--TcIHrOqxTbpQ" width="602" /&gt;&lt;figcaption&gt;(source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;And that’s it! That’s the SSM, our ~drop-in replacement for Attention (Communication) in the Mamba block. The Computation in the Mamba architecture comes from regular linear projections, non-linearities, and local convolutions.&lt;/p&gt;&lt;p&gt;Okay great, that’s the theory - but does this work? Well…&lt;/p&gt;&lt;h2 id="effectiveness-vs-efficiency-attention-is-focus-selectivity-is-prioritisation"&gt;Effectiveness vs Efficiency: Attention is Focus, Selectivity is Prioritisation&lt;/h2&gt;&lt;p&gt;At WWDC ‘97, Steve Jobs famously noted that “focusing is about saying no”. Focus is ruthless prioritisation. It’s common to think about Attention &lt;em&gt;positively&lt;/em&gt; as choosing what to &lt;em&gt;notice&lt;/em&gt;. In the Steve Jobs sense, we might instead frame Attention &lt;em&gt;negatively&lt;/em&gt; as choosing what to &lt;em&gt;discard&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;There’s a classic intuition pump in Machine Learning known as the Cocktail Party Problem&lt;sup&gt;13&lt;/sup&gt;. Imagine a party with dozens of simultaneous loud conversations:&lt;/p&gt;&lt;p&gt;Question:&lt;/p&gt;&lt;p&gt;&lt;em&gt;How do we recognise what one person is saying when others are talking at the same time?&lt;sup&gt;14&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Answer:&lt;/p&gt;&lt;p&gt;&lt;em&gt;The brain solves this problem by focusing your “attention” on a particular stimulus and hence drowning out all other sounds as much as possible.&lt;/em&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="376" src="https://lh7-us.googleusercontent.com/C18AUAf7863Uq5SHEwb4aQFcFoA4HW8olFXz_MvZ9HttqJNF2hvIfm3TEsNLhRkXyEJTOwhbtUyOh4QKV2qiGUXwA1sq2_CSTjO7FWPvK2YRnJgYvN859kqXo8pOkZffsXC0iO9z5yajWbc_9CvtwO8" width="602" /&gt;&lt;/figure&gt;&lt;hr /&gt;&lt;p&gt;Transformers use Dot-Product Attention to focus on the most relevant tokens. A big reason Attention is so great is that you have the potential to look back at everything that ever happened in its context. This is like photographic memory when done right.&lt;sup&gt;15&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;Transformers (🤖) are extremely &lt;strong&gt;effective&lt;/strong&gt;. But they aren’t very &lt;strong&gt;efficient&lt;/strong&gt;. They store everything from the past so that they can look back at tokens with theoretically perfect recall.&lt;/p&gt;&lt;p&gt;Traditional RNNs (🔁) are the opposite - they forget a lot, only recalling a small amount in their hidden state and discarding the rest. They are very &lt;strong&gt;efficient&lt;/strong&gt; - their state is small. Yet they are less &lt;strong&gt;effective&lt;/strong&gt; as discarded information cannot be recovered.&lt;/p&gt;&lt;p&gt;We’d like something closer to the Pareto frontier of the effectiveness/efficiency tradeoff. Something that’s more effective than traditional RNNs and more efficient than transformers.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="407" src="https://lh7-us.googleusercontent.com/V2BPTE_TEzO_CAXFnp54TL-nAzSpkiHN_PWZeWOgMN7TInAXL8i3hLgS8ruinxworyEl0248jU6y4Y86Wg1TJca-UjzjCrMQrmSpWceXJ-C4LIg6SJvJykJFfDBb12rIQi84B-aHKdPG_gWsxVkxT20" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;The Mamba Architecture seems to offer a solution which pushes out the Pareto frontier of effectiveness/efficiency.&lt;/p&gt;&lt;p&gt;SSMs are as &lt;strong&gt;efficient&lt;/strong&gt; as RNNs, but we might wonder how &lt;strong&gt;effective&lt;/strong&gt; they are. After all, it seems like they would have a hard time discarding only &lt;em&gt;unnecessary&lt;/em&gt; information and keeping everything relevant. If each token is being processed the same way, applying the same A and B matrices as if in a factory assembly line for tokens, there is no context-dependence. We would like the forgetting and remembering matrices (A and B respectively) to vary and dynamically adapt to inputs.&lt;/p&gt;&lt;h3 id="the-selection-mechanism"&gt;The Selection Mechanism&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Selectivity&lt;/strong&gt; allows each token to be transformed into the state in a way that is unique to its own needs. Selectivity is what takes us from vanilla SSM models (applying the same A (forgetting) and B (remembering) matrices to every input) to Mamba, the &lt;em&gt;&lt;strong&gt;Selective&lt;/strong&gt;&lt;/em&gt; &lt;em&gt;State Space Model&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;In regular SSMs, A, B, C and D are learned matrices - that is&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$\mathbf{A} = \mathbf{A}_{\theta}$ etc. (where θ represents the learned parameters)&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;With the Selection Mechanism in Mamba, A, B, C and D are also functions of x. That is $\mathbf{A} = \mathbf{A}_{\theta(x)}$ etc; the matrices are context dependent rather than static.&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="184" src="https://lh7-us.googleusercontent.com/wATzvqFAg8l5HWS9BSCi_OGZRkZ7XmoPfpuZkIaCgLNE1jwrocWaKn_j6OrSG_4n5uULQN6yYK1oWkR4_AbCTXnpaJDTw9PPmeF7btcFa4-7h1QESJIBxTPK4D5vbzFvGJKjxUu-kXqYnRi_oPiVAD4" width="602" /&gt;&lt;figcaption&gt;Mamba (right) differs from traditional SSMs by allowing A,B,C matrices to be &lt;strong&gt;selective &lt;/strong&gt;i.e. context dependent (source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Making A and B functions of x allows us to get the best of both worlds:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;We’re selective about what we include in the state, which improves &lt;strong&gt;effectiveness&lt;/strong&gt; vs traditional SSMs.&lt;/li&gt;&lt;li&gt;Yet, since the state size is bounded, we improve on &lt;strong&gt;efficiency&lt;/strong&gt; relative to the Transformer. We have O(1), not O(n) space and O(n) not O(n²) time requirements.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The Mamba paper authors write:&lt;/p&gt;&lt;p&gt;&lt;em&gt;The efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state: efficient models must have a small state, while effective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension.&lt;/em&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;Humans (mostly) don’t have photographic memory for everything they experience within a lifetime - or even within a day! There’s just way too much information to retain it all. Subconsciously, we select what to remember by choosing to forget, throwing away most information as we encounter it. Transformers (🤖) decide what to focus on at &lt;strong&gt;recall time&lt;/strong&gt;. Humans (🧑) also decide what to throw away at &lt;strong&gt;memory-making time&lt;/strong&gt;. Humans filter out information early and often.&lt;/p&gt;&lt;p&gt;If we had infinite capacity for memorisation, it’s clear the transformer approach is better than the human approach - it truly is more effective. But it’s less efficient - transformers have to store so much information about the past that might not be relevant. Transformers (🤖) only decide what’s relevant at &lt;strong&gt;recall time&lt;/strong&gt;. The innovation of Mamba (🐍) is allowing the model better ways of forgetting earlier - it’s focusing by choosing what to &lt;em&gt;discard&lt;/em&gt; using &lt;strong&gt;Selectivity&lt;/strong&gt;, throwing away less relevant information at &lt;strong&gt;memory-making time&lt;/strong&gt;&lt;sup&gt;16&lt;/sup&gt;.&lt;/p&gt;&lt;h3 id="the-problems-of-selectivity"&gt;The Problems of Selectivity&lt;/h3&gt;&lt;p&gt;Applying the Selection Mechanism does have its gotchas though. Non-selective SSMs (i.e. A,B not dependent on x) are fast to compute in training. This is because the component of&lt;/p&gt;&lt;p&gt;Yt which depends on xi can be expressed as a linear map, i.e. a single matrix that can be precomputed!&lt;/p&gt;&lt;p&gt;For example (ignoring the D component, the skip connection):&lt;/p&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;$$y_2 = \mathbf{C}\mathbf{B}x_2 + \mathbf{C}\mathbf{A}\mathbf{B}x_1 +&lt;br /&gt;\mathbf{C}\mathbf{A}\mathbf{A}\mathbf{B}x_0$$&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;p&gt;If we’re paying attention, we might spot something even better here - this expression can be written as a convolution. Hence we can apply the Fast Fourier Transform and the Convolution Theorem to compute this &lt;em&gt;very&lt;/em&gt; efficiently on hardware as in Equation 3 below.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="93" src="https://lh7-us.googleusercontent.com/SnLnXqZ4ArJyiJmMNiUiDMpZ0WYRXuaWO-ZS_Ogj-hThlMVbZz8B3F9g09H5V5CQG6mjgiSphIpjOz4ATr_JYLxCZ9T-EjG5dNy1-mpL1JwL-XWJbymVgyEGhdxpfUT34B1v4iJ_vQAiNUGeTs2FMXs" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;We can calculate Equation 2, the SSM equations, efficiently in the Convolutional Form, Equation 3.&lt;/p&gt;&lt;p&gt;Unfortunately, with the Selection Mechanism, we lose the convolutional form. Much attention is given to making Mamba efficient on modern GPU hardware using similar hardware optimisation tricks to Tri Dao’s Flash Attention&lt;sup&gt;17&lt;/sup&gt;. With the hardware optimisations, Mamba is able to run faster than comparably sized Transformers.&lt;/p&gt;&lt;h3 id="machine-learning-for-political-economistshow-large-should-the-state-be"&gt;Machine Learning for Political Economists - How Large Should The State Be?&lt;/h3&gt;&lt;p&gt;The Mamba authors write, “the efficiency vs. effectiveness tradeoff of sequence models is characterised by how well they compress their state”. In other words, like in political economy&lt;sup&gt;18&lt;/sup&gt;, the fundamental problem is how to manage the state.&lt;/p&gt;&lt;p&gt;🔁 &lt;strong&gt;Traditional RNNs are anarchic&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;They have a small, minimal state. The size of the state is bounded. The compression of state is poor.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;🤖 &lt;strong&gt;Transformers are communist&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;They have a maximally large state. The “state” is just a cache of the entire history with no compression. Every context token is treated equally until recall time.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;🐍&lt;strong&gt;Mamba has a compressed state&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;…but it’s selective about what goes in. Mamba says we can get away with a small state if the state is well focused and effective&lt;sup&gt;19&lt;/sup&gt;.&lt;/em&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="275" src="https://lh7-us.googleusercontent.com/rkN6fi0try__wiIKQ1D9gbHvCrW_dHsKV0jckG85H7P3_Lx1Vm2vHfeb7Zs6N50lnjVx04A3QTQb2JSjMltn8C0kFmvB4DPUgsjj_DEAGu8O-LcKlY7G0RLgLCCsDV_R1W4pkkE67_2rnyx0vCMnayM" width="602" /&gt;&lt;figcaption&gt;Language Models and State Size&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The upshot is that state&lt;strong&gt; representation is critical&lt;/strong&gt;. A smaller state is more efficient; a larger state is more effective. The key is to &lt;strong&gt;selectively&lt;/strong&gt; and &lt;strong&gt;dynamically&lt;/strong&gt; compress data into the state. Mamba’s Selection Mechanism allows for context-dependent reasoning, focusing and ignoring. For both performance and interpretability, understanding the state seems to be very useful.&lt;/p&gt;&lt;h2 id="information-flow-in-transformer-vs-mamba"&gt;Information Flow in Transformer vs Mamba&lt;/h2&gt;&lt;p&gt;How do Transformers know anything? At initialization, a transformer isn’t very smart. It learns in two ways:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Training data (Pretraining, SFT, RLHF etc)&lt;/li&gt;&lt;li&gt;In context-data&lt;/li&gt;&lt;/ol&gt;&lt;h4 id="training-data"&gt;Training Data&lt;/h4&gt;&lt;p&gt;Models learn from their training data. This is a kind of lossy compression of input data into the weights. We can think of the effect of pretraining data on the transformer kinda like the effect of your ancestor’s experiences on your genetics - you can’t recall their experiences, you just have vague instincts about them&lt;sup&gt;20&lt;/sup&gt;.&lt;/p&gt;&lt;h4 id="in-context-data"&gt;In Context-Data&lt;/h4&gt;&lt;p&gt;Transformers use their context as short-term memory, which they can recall with ~perfect fidelity. So we get In-Context Learning, e.g. using induction heads to solve the Indirect Object Identification task, or computing Linear Regression.&lt;/p&gt;&lt;h4 id="retrieval"&gt;Retrieval&lt;/h4&gt;&lt;p&gt;Note that Transformers don’t filter their context at all until recall time. So if we have a bunch of information we think &lt;em&gt;might&lt;/em&gt; be useful to the Transformer, we filter it &lt;em&gt;outside&lt;/em&gt; the Transformer (using Information Retrieval strategies) and then stuff the results into the prompt. This process is known as Retrieval Augmented Generation (RAG). RAG determines relevant information for the context window of a transformer. A human with the internet is kinda like a RAG system - you still have to know what to search but whatever you retrieve is as salient as short-term memory to you.&lt;/p&gt;&lt;h4 id="information-flow-for-mamba"&gt;Information Flow for Mamba&lt;/h4&gt;&lt;p&gt;Training Data acts similarly for Mamba. However, the lines are slightly blurred for in-context data and retrieval. In-context data for Mamba &lt;em&gt;is&lt;/em&gt; compressed/filtered similar to retrieval data for transformers. This in-context data is also accessible for look-up like for transformers (although with somewhat lower fidelity).&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="236" src="https://lh7-us.googleusercontent.com/0dxiIk5NUI9g_P7G5lr5CSziEVKABYdtIW-R4Rxi6OHwWV_vLYVb1wtetVmzNtRWcLngldL4A8WUQA2jhIQj-IJmpaYr97xt-2Du_dxVOe5ppA4EcRNxEbjQvmjbND_DhyKhO6nsnS4nf1NxvRLwx-o" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;Transformer context is to Mamba states what short-term is to long-term memory. Mamba doesn’t just have “RAM”, it has a hard drive&lt;sup&gt;21&lt;/sup&gt; &lt;sup&gt;22&lt;/sup&gt;.&lt;/p&gt;&lt;h3 id="swapping-states-as-a-new-prompting-paradigm"&gt;Swapping States as a New Prompting Paradigm&lt;/h3&gt;&lt;p&gt;Currently, we often use RAG to give a transformer contextual information.&lt;/p&gt;&lt;p&gt;With Mamba-like models, you could instead imagine having a library of states created by running the model over specialised data. States could be shared kinda like LoRAs for image models.&lt;/p&gt;&lt;p&gt;For example, I could do inference on 20 physics textbooks and, say, 100 physics questions and answers. Then I have a state which I can give to you. Now you don’t need to add any few-shot examples; you just simply ask your question. &lt;strong&gt;The in-context learning is in the state&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;In other words, you can drag and drop downloaded states into your model, like literal plug-in cartridges. And note that “training” a state doesn’t require any backprop. It’s more like a highly specialised one-pass fixed-size compression algorithm. This is unlimited in-context learning applied at inference time for zero-compute or latency&lt;sup&gt;23&lt;/sup&gt;.&lt;/p&gt;&lt;p&gt;The structure of an effective LLM call goes from…&lt;/p&gt;&lt;ol&gt;&lt;li&gt;System Prompt&lt;/li&gt;&lt;li&gt;Preamble&lt;/li&gt;&lt;li&gt;Few shot-examples&lt;/li&gt;&lt;li&gt;Question&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;…for Transformers, to simply…&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Inputted state (with problem context, initial instructions, textbooks, and few-shot examples)&lt;/li&gt;&lt;li&gt;Short question&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;…for Mamba.&lt;/p&gt;&lt;p&gt;This is cheaper and faster than few-shot prompting (as the state is infinitely reusable without inference cost). It’s also MUCH cheaper than finetuning and doesn’t require any gradient updates. We could imagine retrieving states in addition to context.&lt;/p&gt;&lt;h2 id="mamba-mechanistic-interpretability"&gt;Mamba &amp;amp; Mechanistic Interpretability&lt;/h2&gt;&lt;p&gt;Transformer interpretability typically involves:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;understanding token relationships via attention,&lt;/li&gt;&lt;li&gt;understanding circuits, and&lt;/li&gt;&lt;li&gt;using Dictionary Learning for unfolding MLPs.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Most of the ablations that we would like to do for Mamba are still valid, but understanding token communication (1) is now more nuanced. All information moves between tokens via hidden states instead of the Attention Mechanism which can “teleport” information from one sequence position to another.&lt;/p&gt;&lt;p&gt;For understanding in-context learning (ICL) tasks with Mamba, we will look to intervene on the SSM state. A classic task in-context learning task is Indirect Object Identification in which a model has to finish a paragraph like:&lt;/p&gt;&lt;p&gt;&lt;em&gt;Then, Shelby and Emma had a lot of fun at the school. [Shelby/Emma] gave an apple to [BLANK]&lt;/em&gt;&lt;/p&gt;&lt;p&gt;The model is expected to fill in the blank with the name that is not repeated in the paragraph. In the chart below we can see that information is passed from the [Shelby/Emma] position to the final position via the hidden state (see the two blue lines in the top chart).&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="256" src="https://lh7-us.googleusercontent.com/ZDBpRS1yEscEZcsJtevlPaM5URUP58dgJ2csAIcWP-hmQcje8kBi-u4zAWYnbeE26YXWemOh32pdHM2TgaSanGePOVgRiss8svxP17nLPBvg1YjLE4W1uIGkTmDI9PbZO42u_4KfYoSeaRnZz_W4HfY" width="602" /&gt;&lt;/figure&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="256" src="https://lh7-us.googleusercontent.com/j8aQ6XIxedX6Zut0rz7CE_e02KgBjyJvg7QQ7U9FkM2TjSWWSNk1v7gFVeGSsETqwQGvF8flh0lIUmSLIVqW9rwHC69rImw5MPj0vA0Y4XihacOzZnhUeKMZpf3bWtJTM_TB67EDYKIyfp2DeX4pNFU" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;Since it’s hypothesised that much of In-Context Learning in Transformers is downstream of more primitive sequence position operations (like Induction Heads), Mamba being able to complete this task suggests a more general In-Context Learning ability.&lt;/p&gt;&lt;h2 id="what%E2%80%99s-next-for-mamba-ssms"&gt;What’s Next for Mamba &amp;amp; SSMs?&lt;/h2&gt;&lt;p&gt;Mamba-like models are likely to excel in scenarios requiring extremely long context and long-term memory. Examples include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Processing DNA&lt;/li&gt;&lt;li&gt;Generating (or reasoning over) video&lt;/li&gt;&lt;li&gt;Writing novels&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;An illustrative example is agents with long-term goals.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Suppose you have an agent interacting with the world. Eventually, its experiences become too much for the context window of a transformer. The agent then has to compress or summarise its experiences into some more compact representation.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;But how do you decide what information is the most useful as a summary? If the task is language, LLMs are actually fairly good at summaries - okay, yeah, you’ll lose some information, but the most important stuff can be retained.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;However, for other disciplines, it might not be clear how to summarise. For example, what’s the best way to summarise a 2 hour movie?&lt;sup&gt;24&lt;/sup&gt;. Could the model itself learn to do this naturally rather than a hacky workaround like trying to describe the aesthetics of the movie in text?&lt;/em&gt;&lt;/p&gt;&lt;p&gt;This is what Mamba allows. Actual long-term memory. A real state where the model learns to keep what’s important. Prediction is compression - learning what’s useful to predict what’s coming next inevitably leads to building a useful compression of the previous tokens.&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;The implications for Assistants are clear:&lt;/p&gt;&lt;p&gt;Your chatbot co-evolves with you. It remembers.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="339" src="https://lh7-us.googleusercontent.com/agZClF-xa6q13BlEbfZLFKP3DM0hJiRy9kC0MRFoNPi8kdWCh8_BUa5oLC0V_6jTmcNQQfmMr7GGa6gwIe3CEGVeK79AFMhE1gMnbdhEoQ8iFCRuO7Yc6Xi2M3kaVIGZ4LTfDKqITQ6ap1DylOqbWs4" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;The film HER is looking better and better as time goes on 😳&lt;/p&gt;&lt;h3 id="agents-ai-safety"&gt;Agents &amp;amp; AI Safety&lt;/h3&gt;&lt;p&gt;One reason for positive updates in existential risk from AGI is Language Models. Previously, Deep-RL agents trained via self-play looked set to be the first AGIs. Language models are inherently much safer since they aren’t trained with long-term goals&lt;sup&gt;25&lt;/sup&gt;.&lt;/p&gt;&lt;p&gt;The potential for long-term sequence reasoning here brings back the importance of agent-based AI safety. Few agent worries are relevant to Transformers with an 8k context window. Many are relevant to systems with impressive long-term memories and possible instrumental goals.&lt;/p&gt;&lt;h3 id="the-best-collab-since-taco-bell-kfc-%F0%9F%A4%96-x-%F0%9F%90%8D"&gt;The Best Collab Since Taco Bell &amp;amp; KFC: 🤖 x 🐍&lt;/h3&gt;&lt;p&gt;The Mamba authors show that there’s value in combining Mamba’s long context with the Transformer’s high fidelity over short sequences. For example, if you’re making long videos, you likely can’t fit a whole movie into a Transformer’s context for attention&lt;sup&gt;26&lt;/sup&gt;. You could imagine having Attention look at the most recent frames for short-term fluidity and an SSM for long-term narrative consistency&lt;sup&gt;27&lt;/sup&gt;.&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;This isn’t the end for Transformers. Their high effectiveness is exactly what’s needed for many tasks. But now Transformers aren’t the only option. Other architectures are genuinely feasible.&lt;/p&gt;&lt;p&gt;So we’re not in the post-Transformer era. But for the first time, we’re living in the post-only-Transformers era&lt;sup&gt;28&lt;/sup&gt;. And this blows the possibilities wide open for sequence modelling with extreme context lengths and native long-term memory.&lt;/p&gt;&lt;p&gt;Two ML researchers, Sasha Rush (HuggingFace, Annotated Transformer, Cornell Professor) and Jonathan Frankle (Lottery Ticket Hypothesis, MosaicML, Harvard Professor), currently have a bet here.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img alt="Mamba Explained" class="kg-image" height="324" src="https://lh7-us.googleusercontent.com/-_S7CaQ4OxepapriZhhAs25xq-H_dSnavPxXkm0_lMMZjtno4kgWfjS1PAcLhYpbMz6BNNYd-RoxBA_Fy45CemDdvofbP7oPVQ3ygHBQNQ8pMVf7l5YnLSCgE3L1J9muCpoFmTSz09zcX9xEigRrKnc" width="602" /&gt;&lt;/figure&gt;&lt;p&gt;Currently Transformers are far and away in the lead. With 3 years left, there’s now a research direction with a fighting chance.&lt;/p&gt;&lt;p&gt;All that remains to ask is: &lt;strong&gt;Is Attention All We Need?&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;&lt;hr /&gt;&lt;!--kg-card-begin: html--&gt;&lt;p&gt;
    1. see Figure 8 in the Mamba paper.
    &lt;br /&gt;2. And scaling up with massive compute.
    &lt;br /&gt;3. More specifically the scaled dot-product Attention popularised by Transformers
    &lt;br /&gt;4. For people who don’t see Temple Run as the cultural cornerstone it is 🤣 Temple Run was an iPhone game from 2011 similar to Subway Surfer
    &lt;br /&gt;5. Here we assume the environment is sufficiently smooth.
    &lt;br /&gt;6. One pretty important constraint for this to be efficient is that we don’t allow the individual elements of the state vector to interact with each other directly. We’ll use a combination of the state dimensions to determine the output but we don’t e.g. allow the velocity of the runner and the direction of the closest obstacle (or whatever else was in our state) to directly interact. This helps with efficient computation and we achieve this practically by constraining A to be a diagonal matrix.
    &lt;br /&gt;7. Concretely consider the case of Language Models - each token is a discrete step 
    &lt;br /&gt;8. ZOH also has nice properties for the initialisations - we want A_bar to be close to the identity so that the state can be mostly maintained from timestep to timestep if desired. ZOH gives A_bar as an exponential so any diagonal element initialisations close to zero give values close to 1 
    &lt;br /&gt;9. This is known as the Euler discretisation in the literature
    &lt;br /&gt;10. It’s wild to note that some readers might not have, we’re so far into the age of Attention that RNNs have been forgotten!
    &lt;br /&gt;11. B is like the Query (Q) matrix for Transformers.
    &lt;br /&gt;12. C is like the Output (O) matrix for Transformers. 
    &lt;br /&gt;13. Non-alcoholic options also available! 
    &lt;br /&gt;14. Especially as all voices roughly occupy the same space on the audio frequency spectrum Intuitively this seems really hard! 
    &lt;br /&gt;15. Note that photographic memory doesn’t necessarily imply perfect inferences from that memory! 
    &lt;br /&gt;16. To be clear, if you have a short sequence, then a transformer should theoretically be a better approach. If you can store the whole context, then why not!? If you have enough memory for a high-resolution image, why compress it into a JPEG? But Mamba-style architectures are likely to hugely outperform with long-range sequences. 
    &lt;br /&gt;17. More details are available for engineers interested in CUDA programming - Tri’s talk, Mamba paper section 3.3.2, and the official CUDA code are good resources for understanding the Hardware-Aware Scan 
    &lt;br /&gt;18. or in Object Oriented Programming 
    &lt;br /&gt;19. Implications to actual Political Economy are left to the reader but maybe Gu and Dao accidentally solved politics!? 
    &lt;br /&gt;20. This isn’t a perfect analogy as human evolution follows a genetic algorithm rather than SGD. 
    &lt;br /&gt;21. Albeit a pretty weird hard drive at that - it morphs over time rather than being a fixed representation.  
    &lt;br /&gt;22. As a backronym, I’ve started calling the hidden_state the state space dimension (or selective state dimension) which shortens to SSD, a nice reminder for what this object represents - the long-term memory of the system.
    &lt;br /&gt;23. I’m thinking about this similarly to the relationship between harmlessness finetuning and activation steering. State swapping, like activation steering, is an inference time intervention giving comparable results to its train time analogue. 
    &lt;br /&gt;24. This is a very non-trivial problem! How do human brains represent a movie internally? It’s not a series of the most salient frames, nor is it a text summary of the colours, nor is it a purely vibes-based summary if you can memorise some lines of the film. 
    &lt;br /&gt;25. They’re also safer since they inherently understand (though don’t necessarily embody) human values. It’s not all clear that how to teach an RL agent human morality. 
    &lt;br /&gt;26. Note that typically an image (i.e. a single frame) counts as &amp;gt;196 tokens, and movies are typically 24 fps so you’ll fill a 32k context window in 7 seconds 🤯 
    &lt;br /&gt;27. Another possibility that I’m excited about is applying optimisation pressure to the state itself as well as the output to have models that respect particular use cases. 
    &lt;br /&gt;28. This is slightly hyperbolic, the TS-Mixer for time series, Gradient Boosting Trees for tabular data and Graph Neural Networks for weather prediction exist and are currently used, but these aren’t at the core of AI
&lt;/p&gt;&lt;!--kg-card-end: html--&gt;&lt;h2 id="author-bio"&gt;Author Bio&lt;/h2&gt;&lt;p&gt;Kola Ayonrinde is a Research Scientist and Machine Learning Engineer with a flair for writing. He integrates technology and creativity, focusing on applying machine learning in innovative ways and exploring the societal impacts of tech advancements.&lt;/p&gt;&lt;h2 id="acknowledgements"&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;This post was originally posted on Kola's personal blog.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thanks to Gonçalo for reading an early draft, Jaden for the nnsight library used for the Interpretability analysis and Tessa for Mamba patching visualisations.Also see: &lt;/em&gt;&lt;em&gt;Mamba paper&lt;/em&gt;&lt;em&gt;, Mamba Python code, &lt;/em&gt;&lt;em&gt;Annotated S4&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;Nathan Labenz podcast&lt;/em&gt;&lt;/p&gt;&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Kola Ayonrinde, "Mamba Explained," The Gradient, 2024&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;@article{Ayonrinde2024mamba,
    author = {Kola Ayonrinde},
    title = {Mamba Explained},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/mamba-explained},
}&lt;/code&gt;&lt;/pre&gt;]]&amp;gt;&amp;lt;![CDATA[Car-GPT: Could LLMs finally make self-driving cars happen?]]&amp;gt;https://thegradient.pub/car-gpt/65db7b4193571d5c8c154a73Fri, 08 Mar 2024 16:55:18 GMT&lt;p&gt;In 1928, London was in the middle of a terrible health crisis, devastated by bacterial diseases like pneumonia, tuberculosis, and meningitis. Confined in sterile laboratories, scientists and doctors were stuck in a relentless cycle of trial and error, using traditional medical approaches to solve complex problems.&lt;/p&gt;&lt;p&gt;This is when, in September 1928, an accidental event changed the course of the world.&lt;strong&gt; &lt;/strong&gt;A Scottish doctor named Alexander Fleming forgot to close a petri dish (the transparent circular box you used in science class), which got contaminated by mold. This is when Fleming noticed something peculiar: all bacteria close to the moisture were dead, while the others survived.&lt;/p&gt;&lt;p&gt;"What was that moisture made of?" wondered M. Flemming.&lt;strong&gt; &lt;/strong&gt;This was when he discovered that Penicillin, the main component of the mold, was a powerful bacterial killer. This led to the groundbreaking discovery of penicillin, leading to the antibiotics we use today. In a world where doctors were relying on existing well-studied approaches, Penicillin was the unexpected answer.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Self-driving cars may be following a similar event.&lt;/strong&gt; Back in the 2010s, most of them were built using what we call a « modular » approach. The software « autonomous » part is split into several modules, such as Perception (the task of seeing the world), or Localization (the task of accurately localize yourself in the world), or Planning (the task of creating a trajectory for the car to follow, and implementing the « brain » of the car). Finally, all these go to the last module: Control, that generates commands such as « steer 20° right », etc… So this was the well-known approach. &lt;/p&gt;&lt;p&gt;But a decade later, companies started to take another discipline very seriously: &lt;strong&gt;End-To-End learning&lt;/strong&gt;. The core idea is to replace every module with a single neural network predicting steering and acceleration, but as you can imagine, this introduces a black box problem.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="161" src="https://lh7-us.googleusercontent.com/EpMJPDK-TBu9ZhN25UrxVbAk-9rJjEvtitzjvPpzjhTBPdkk-judKQtfWQNf7vtNrG1sfsvkUhpbtMGplWN5bbnx5ULbfNj6vpRf8RVlt5eDn8MN99FObGbPsmokdNlCGZ1NWq-uw32QVitv4NZC3zI" width="624" /&gt;&lt;figcaption&gt;The 4 Pillars of Self-Driving Cars are Perception, Localization, Planning, and Control. Could a Large Language Model replicate them? (source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;These approaches are known, but don’t solve the self-driving problem yet. So, we could be wondering:&lt;strong&gt; "What if LLMs (Large Language Models), currently revolutionizing the world, were the unexpected answer to autonomous driving?"&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;This is what we're going to see in this article, beginning with a simple explanation of what LLMs are and then diving into how they could benefit autonomous driving.&lt;/p&gt;&lt;h2 id="preamble-llms-what"&gt;&lt;strong&gt;Preamble: LLMs-what?&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Before you read this article, you must know something: I'm not an LLM pro, at all. This means, I know too well the struggle to learn it. I understand what it's like to google "learn LLM"; then see 3 sponsored posts asking you to download e-books (in which nothing concrete appears)... then see 20 ultimate roadmaps and GitHub repos, where step 1/54 is to view a 2-hour long video (and no one knows what step 54 is because it's so looooooooong).&lt;/p&gt;&lt;p&gt;So, instead of putting you through this pain myself, let's just break down what LLMs are in 3 key ideas:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Tokenization&lt;/li&gt;&lt;li&gt;Transformers&lt;/li&gt;&lt;li&gt;Processing Language&lt;/li&gt;&lt;/ol&gt;&lt;h3 id="tokenization"&gt;&lt;strong&gt;Tokenization&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;In ChatGPT, you input a piece of text, and it returns text, right? Well, what's actually happening is that your text is first converted into tokens.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="303" src="https://lh7-us.googleusercontent.com/_rT6_ShRUbi-bZpaKL7JF-BhE_rfDg_V8De5nYj0O5tGgAtLTyYhnGleIy7nBJ3vyrUsfge6cdReCctzsfCyW_XP6WUm21pU350RpOoxWzb2SYRvMcKMIZAOE6wdFou7t_ERJ2_Jht6uUhfg_sBgcbI" width="600" /&gt;&lt;figcaption&gt;Example of tokenization of a sentence, each word becomes a "token"&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;But what's a token? You might ask. Well, a token can correspond to a word, a character, or anything we want. Think about it -- if you are to send a sentence to a neural network, you didn't plan on sending actual words, did you?&lt;/p&gt;&lt;p&gt;The input of a neural network is always a number, so you need to convert your text into numbers; this is tokenization.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="139" src="https://lh7-us.googleusercontent.com/pZ3qf5HQNrqXqb5bbGkLgWQPvu04-2b_ejpv4m3i5C9VfcPg3yZm7cmaD6lq4xgrA4DhUBJpCa-HB4i7iAPo8-Hyrde9sLiBYBiY2d7c9O17ePJtCqAb15dvcDEGxofEwneP6Nx2_oSiT26m4cLvcMc" width="624" /&gt;&lt;figcaption&gt;What tokenization actually is: A conversion from words to numbers&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Depending on the model (ChatGPT, LLAMA, etc...), a token can mean different things: a word, a subword, or even a character. We could take the English vocabulary and define these as words or take parts of words (subwords) and handle even more complex inputs. For example, the word « a » could be token 1, and the word « abracadabra » would be token 121.&lt;/p&gt;&lt;h3 id="transformers"&gt;&lt;strong&gt;Transformers&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;Now that we understand how to convert a sentence into a series of numbers, we can send that series into our neural network! At a high level, we have the following structure:&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="407" src="https://lh7-us.googleusercontent.com/J1CkM3ItKevopmi-0gbSHWJnMStL4dZWksllG15OlaDI4PFgk-FtFeQ7O0CnP1dKx9ZHV7PUAlmBK9lFwJQrHnJj1JAXAMHdbZH13hd07dYL55ZCsxQChf06dYj_JoXEvNeAqdfmj2IcdwD8sP5OZtI" width="624" /&gt;&lt;figcaption&gt;A Transformer is an Encoder-Decoder Architecture that takes a sequence of tokens as input and outputs a another series of tokens&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;If you start looking around, you will see that some models are based on an encoder-decoder architecture, some others are purely encoder-based, and others, like GPT, are purely decoder-based.&lt;/p&gt;&lt;p&gt;Whatever the case, they all share the core Transformer blocks: multi-head attention, layer normalization, addition and concatenation, blocks, cross-attention, etc...&lt;/p&gt;&lt;p&gt;&lt;strong&gt;This is just a series of attention blocks getting you to the output&lt;/strong&gt;. So how does this word prediction work?&lt;/p&gt;&lt;h3 id="the-output-next-word-prediction"&gt;&lt;strong&gt;The output/ Next-Word Prediction&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;The Encoder learns features and understands context... But what does the decoder do? In the case of object detection, the decoder is predicting bounding boxes. In the case of segmentation, the decoder is predicting segmentation masks. What about here?&lt;/p&gt;&lt;p&gt;In our case, the decoder is trying to generate a series of words; we call this task "next-word prediction".&lt;/p&gt;&lt;p&gt;Of course, it does it similarly by predicting numbers or tokens. This characterizes our full model as shown below,&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="519" src="https://lh7-us.googleusercontent.com/YS9WFjjuYTq7QkzPnx4xgTQnU0Pmr22i4fEzXXWuBf6wD--eYL8FvdoEpkqlCMKraBaSDuo7j0sWR7ltUaWI31_Bvq9PtJoPpoWRFQnjKOth1P7mnxfzmGT8ppUslOPMhbOzJY49F4IHBMZfyzax18E" width="624" /&gt;&lt;figcaption&gt;I would say the loss function for this particular output produces a near-0 value.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now, there are many "concepts" that you should learn on top of this intro: everything Transformer and Attention related, but also few-shot learning, pretraining, finetuning, and more...&lt;/p&gt;&lt;p&gt;Ok... but what does it have to do with self-driving cars? I think it's time to move to stage 2.&lt;/p&gt;&lt;h2 id="chat-gpt-for-self-driving-cars"&gt;&lt;strong&gt;Chat-GPT for Self-Driving Cars&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;The thing is, you've already been through the tough part. The rest simply is: "How do I adapt this to autonomous driving?". Think about it; we have a few modifications to make:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Our input now becomes either images, sensor data&lt;/strong&gt; (LiDAR point clouds, RADAR point clouds, etc...), or even algorithm data (lane lines, objects, etc...). All of it is "tokenizable", as Vision Transformers or Video Vision Transformers do.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Our Transformer model pretty much remains the same&lt;/strong&gt; since it only operates on tokens and is independent of the kind of input.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;The output is based on the set of tasks we want to do.&lt;/strong&gt; It could be explaining what's happening in the image or could &amp;nbsp;also be a direct driving task like switching lanes.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;So, let's begin with the end:&lt;/p&gt;&lt;h3 id="what-self-driving-car-tasks-could-llm-solve"&gt;&lt;strong&gt;What self-driving car tasks could LLM solve?&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;There are many tasks involved in autonomous driving, but not all of them are GPT-isable. The most active research areas in 2023 have been:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Perception&lt;/strong&gt;: Based on an input image, describe the environment, number of objects, etc...&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Planning&lt;/strong&gt;: Based on an image, or a bird-eye view, or the output of perception, describe what we should do (keep driving, yield, etc...)&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Generation&lt;/strong&gt;: Generate training data, alternate scenarios, and more... using "diffusion"&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Question &amp;amp; Answers&lt;/strong&gt;: Create a chat interface and ask the LLM to answer questions based on the scenario.&lt;/li&gt;&lt;/ul&gt;&lt;h3 id="llms-in-perception"&gt;&lt;strong&gt;LLMs in Perception&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;In Perception, the input is a series of images, and the output is usually a set of objects, lanes, etc... In the case of LLMs, we have 3 core tasks: &lt;strong&gt;Detection&lt;/strong&gt;, &lt;strong&gt;Prediction&lt;/strong&gt;, and &lt;strong&gt;Tracking&lt;/strong&gt;. An example with Chat-GPT, when you send it an image and ask to describe what's going on is shown below:&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="308" src="https://lh7-us.googleusercontent.com/unUisu66NolUzzipNfKObr8kE6n8PRcTMy86cYYIG1aIPLkYKZd34zmzGrkM4yS6lKNoXpvORHwnfORfOsy8aRNUx9AwEDN_qQN4tiuutBRh8l3h_vVpfVzOJ7UdQ-CuWKI5EJsze9le6qRA7VQ1QoY" width="425" /&gt;&lt;figcaption&gt;A GPT-4 Vision model can return the objects in the image, just like object detectors do (source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Other models such as HiLM-D and MTD-GPT can also do this, some work also for videos. Models like PromptTrack, also have the ability to assign unique IDs (this car in front of me is ID #3), similar to a 4D Perception model.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="267" src="https://lh7-us.googleusercontent.com/WcjhR7diFbZrVeKdiVyQbC_HtYJVGUQsOBka0zikaD2JZpfmNxcyEJlpzxZfvobWrMu6srxUEGPcxpdVSywVKW-0gIuOISCqLCVfjaA6Q7KaNb1etKfNybXkya4yFyx7AY0Y2_ZZw_cY_gWSccO0B2Q" width="624" /&gt;&lt;figcaption&gt;PromptTrack combines the DETR object detector with Large Language Models&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In this model, multi-view images are sent to an Encoder-Decoder network that is trained to predict annotations of objects such as bounding boxes, and attention maps). These maps are then combined with a prompt like 'find the vehicles that are turning right'.The next block then finds the 3D Bounding Box localization and assigns IDs using a bipartite graph matching algorithm like the Hungarian Algorithm.&lt;/p&gt;&lt;p&gt;This is cool, but this isn't the "best" application of LLMs so far:&lt;/p&gt;&lt;h3 id="llms-in-decision-making-navigation-and-planning"&gt;&lt;strong&gt;LLMs in Decision Making, Navigation, and Planning&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;If Chat-GPT can find objects in an image, it should be able to tell you what to do with these objects, shouldn't it? Well, this is the task of Planning i.e. defining a path from A to B, based on the current perception. While there are numerous models developed for this task, the one that stood out to me was Talk2BEV:&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="179" src="https://lh7-us.googleusercontent.com/N3ZvMnLMjQ6jwL4FNvvTyM4U6KFrri0jV-0yOYVH9lAAtRH7MD8aMX_LHhjeBFKxGwTdrATJoNUQe-sUqEB3utLnpreCT4e4TIO3qX3LTrzBKwZ7kPAfzxAu6osJ35tYpapCiTTWDtx0tUOHXcNqu04" width="600" /&gt;&lt;figcaption&gt;Talk2BEV takes perception one step further and also tells you what to do&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The main difference between models for planning and Perception-only models is that here, we're going to train the model on human behavior to suggest ideal driving decisions. We're also going to change the input from multi-view to Bird Eye View since it is much easier to understand.&lt;/p&gt;&lt;p&gt;This model works both with LLaVA and ChatGPT4, and here is a demo of the architecture:&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="255" src="https://lh7-us.googleusercontent.com/-bl_IDT2SqF75q3d20EORJcH22oXWMjFmkLFn0ZKbVV5oshlr5BkZEnscfUSg_-pkzMDJ3Jo38mdu6whUmIDWq7pXfxXxdwgc3Kj-WUwv5LNWUHIvH3r6mfpKP9s5PD7NoA7e0R3pBoic6ijwfD57aU" width="600" /&gt;&lt;figcaption&gt;Talk2BEV (source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;As you can see, this isn't purely "prompt" based, because the core object detection model stays Bird Eye View Perception, but the LLM is used to "enhance" that output by suggesting to crop some regions, look at specific places, and predict a path. We're talking about "language enhanced BEV Maps".&lt;/p&gt;&lt;p&gt;Other models like DriveGPT are trained to send the output of Perception to Chat-GPT and finetune it to output the driving trajectory directly.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="327" src="https://lh7-us.googleusercontent.com/QbuhKKLWr0jA1DWdSWBIk6UtHnecHTITRBPidM1fjYn9VSC-56VcaxStqJbn5iTLslLN6ppQgnmfKZO-43TNCZADfuwdV-RChnMrzryLIKx7UvtySKEs0unIEum4c2ous07M3-WlUoTVeGT1s0nPz0U" width="624" /&gt;&lt;figcaption&gt;The DriveGPT model is pure madness... when trained correctly! (modified from source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;I could go on and on, but I think you get the point. If we summarize, I would say that:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Inputs are either tokenized images or outputs of Perception algorithm (BEV maps, ...)&lt;/li&gt;&lt;li&gt;We fuse existing models (BEV Perception, Bipartite Matching, ...) with language prompts (find the moving cars)&lt;/li&gt;&lt;li&gt;Changing the task is mainly about changing the data, loss function, and careful finetuning.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The Q&amp;amp;A applications are very similar, so let's see the last application of LLMs:&lt;/p&gt;&lt;h3 id="llms-for-image-generation"&gt;&lt;strong&gt;LLMs for Image Generation&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;Ever tried Midjourney and DALL-E? Isn’t it super cool? Yes, and there is MUCH COOLER than this when it comes to autonomous driving. In fact, have you heard of Wayve's GAIA-1 model? The model takes text and images as input and directly produces videos, like this:&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="277" src="https://lh7-us.googleusercontent.com/R9xqQVFRcUlZrjXqqeY6qon7hxAezFKY3mI_ZdPh2R0eJGPQb2CjV0TFxjwblDEWxJz7va0N6KerXMRO_ltSFJkxiQRxmW7I_I_b13bD-PidrUD8sQ0REInSAUJuKGqazFFDCpwOQAVun5LREW41Q_w" width="293" /&gt;&lt;figcaption&gt;These videos are generated by Wayve's GAIA-1 model&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The architecture takes images, actions, and text prompts as input, and then uses a World Model (an understanding of the world and its interactions) to produce a video.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="260" src="https://lh7-us.googleusercontent.com/ougFIHYengzs40lyruVerlDFFa18VXH0K093yjHA93q8nTjTmLQAdfKCPl7sBAbZfpoxqY3tDdzufOqJoxmLUdL6W862_aPebPxABsPwjyaFZGWOCP2VTpaqcob0gkSJDRv9IqSm7-aHoXtG-FXWJBo" width="624" /&gt;&lt;figcaption&gt;Architecture of GAIA-1 (source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You can find more examples on Wayve's YouTube channel and this dedicated post.&lt;/p&gt;&lt;p&gt;Similarly, you can see MagicDrive, which takes the output of Perception as input and uses that to generate scenes:&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Car-GPT: Could LLMs finally make self-driving cars happen?" class="kg-image" height="187" src="https://lh7-us.googleusercontent.com/vVKUXuJno-UQWj2ZTWEA1JBMzZ6xnajJOrzMPtMW4qFjhvKqT7F2XiOoe9M1PCtM44S4CfrXqTVyVfKOisaB3iy-wa5vuCS7SFYaQdv6dzNfbzVcG2XXQzAAUqZXUeGxALkJ9fHuE8XFA4KvkKmZLN4" width="624" /&gt;&lt;figcaption&gt;(source)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Other models, like Driving Into the Future and Driving Diffusion can directly generate future scenarios based on the current ones. You get the point; we can generate scenes in an infinite way, get more data for our models, and have this endless positive loop.&lt;/p&gt;&lt;p&gt;We've just seen 3 prominent families of LLM usage in self-driving cars: Perception, Planning, and Generation. The real question is...&lt;/p&gt;&lt;h2 id="could-we-trust-llms-in-self-driving-cars"&gt;&lt;strong&gt;Could we trust LLMs in self-driving cars?&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;And by this, I mean... What if your model has hallucinations?&lt;/strong&gt; What if its replies are completely absurd, like ChatGPT sometimes does? I remember, back in my first days in autonomous driving, big groups were already skeptical about Deep Learning, because it wasn't "deterministic" (as they call it).&lt;/p&gt;&lt;p&gt;We don't like Black Boxes, which is one of the main reasons End-To-End will struggle to get adopted. Is ChatGPT any better? I don't think so, and I would even say it's worse in many ways. However, LLMs are becoming more and more transparent, and the black box problem could eventually be solved.&lt;/p&gt;&lt;p&gt;To answer the question "Can we trust them?"... it's very early in the research, and I'm not sure someone has really used them "online" — meaning « live », in a car, on the streets, rather than in a headquarter just for training or image generation purpose. &amp;nbsp;I would definitely picture a Grok model on a Tesla someday just for Q&amp;amp;A purposes. So for now, I will give you my coward and safe answer...&lt;/p&gt;&lt;p&gt;&lt;strong&gt;It's too early to tell!&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Because it really is. The first wave of papers mentioning LLMs in Self-Driving Cars is from mid-2023, so let's give it some time. In the meantime, you could start with this survey that shows all the evolutions to date.&lt;/p&gt;&lt;p&gt;Alright, time for the best part of the article...&lt;/p&gt;&lt;h2 id="the-llms-4-ad-summary"&gt;&lt;strong&gt;The LLMs 4 AD Summary&lt;/strong&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;A Large Language Model (LLM) works in 3 key steps: inputs, transformer, output.&lt;/strong&gt; The input is a set of tokenized words, the transformer is a classical transformer, and the output task is "next word prediction".&lt;/li&gt;&lt;li&gt;&lt;strong&gt;In a self-driving car, there are 3 key tasks we can solve with LLMs:&lt;/strong&gt; &lt;strong&gt;Perception&lt;/strong&gt; (detection, tracking, prediction), &lt;strong&gt;Planning&lt;/strong&gt; (decision making, trajectory generation), and &lt;strong&gt;Generation&lt;/strong&gt; (scene, videos, training data, ...).&lt;/li&gt;&lt;li&gt;&lt;strong&gt;In Perception, the main goal is to describe the scene we're looking at.&lt;/strong&gt; The input is a set of raw multi-view images, and the Transformer aims to predict 3D bounding boxes. LLMs can also be used to ask for a specific query ("where are the taxis?").&lt;/li&gt;&lt;li&gt;&lt;strong&gt;In Planning, the main goal is to generate a trajectory for the car to take&lt;/strong&gt;. The input is a set of objects (output of Perception, BEV Maps, ...), and the Transformer uses LLMs to understand context and reason about what to do.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;In Generation, the main goal is to generate a video that corresponds to the prompt used&lt;/strong&gt;. Models like GAIA-1 have a chat interface, and take as input videos to generate either alternate scenes (rainy, ...), or future scenes.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;For now, it's very early to tell whether this can be used in the long run&lt;/strong&gt;, but research there is some of the most active in the self-driving car space. It all comes back to the question: "Can we really trust LLMs in general?"&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="next-steps"&gt;&lt;strong&gt;Next Steps&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;If you want to get started on LLMs for self-driving cars, there are several things you can do:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;⚠️ Before this, the most important&lt;/strong&gt;: If you want to keep learning about self-driving cars. I'm talking about self-driving car every day through my private emails. I'm sending many tips and direct content. You should join here.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;✅ To begin, build an understanding of LLMs for self-driving cars&lt;/strong&gt;. This is partly done, you can continue to explore the resources I provided in the article.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;➡️ Second, build skills related to Auto-Encoders and Transformer Networks&lt;/strong&gt;. My image segmentation series is perfect for this, and will help you understand Transformer Networks with no NLP example, which means it's for Computer Vision Engineer's brains.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;️ ➡️ Then, understand how Bird Eye View Networks works.&lt;/strong&gt; It might not be mentioned in general LLM courses, but in self-driving cars, Bird Eye View is the central format where we can fuse all the data (LiDARs, cameras, multi-views, ...), build maps, and directly create paths to drive. You can do so in my Bird Eye View course (if closed, join my email list to be notified).&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Finally, practice training, finetuning, and running LLMs in self-driving car scenarios&lt;/strong&gt;. Run repos like Talk2BEV and the others I mentioned in the article. Most of them are open source, but the data can be hard to find. This is noted last, but there isn't really an order in all of this.&lt;/li&gt;&lt;/ul&gt;&lt;hr /&gt;&lt;h2 id="author-bio"&gt;&lt;strong&gt;Author Bio&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Jérémy Cohen is a self-driving car engineer and founder of Think Autonomous, a platform to help engineers learn about cutting-edge technologies such as self-driving cars and advanced Computer Vision. In 2022, Think Autonomous won the price for Top Global Business of the Year in the Educational Technology Category​ and Jeremy Cohen was named 2023 40 Under 40 Innovators in Analytics Insight magazine, the largest printed magazine on Artificial Intelligence. &lt;em&gt;You can join 10,000 engineers reading his private daily emails on self-driving cars &lt;/em&gt;&lt;em&gt;here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Jérémy Cohen, "Car-GPT: Could LLMs finally make self-driving cars happen?", The Gradient, 2024.&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;BibTeX citation:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;@article{cohen2024cargpt,
    author = {Jérémy Cohen},
    title = {Car-GPT: Could LLMs finally make self-driving cars happen?},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/car-gpt},
}&lt;/code&gt;&lt;/pre&gt;]]&amp;gt;&amp;lt;![CDATA[Do text embeddings perfectly encode text?]]&amp;gt;https://thegradient.pub/text-embedding-inversion/65e3d66193571d5c8c154aecTue, 05 Mar 2024 20:15:58 GMTThe rise of the vector database&lt;img alt="Do text embeddings perfectly encode text?" src="https://thegradient.pub/content/images/2024/03/main-1.png" /&gt;&lt;p&gt;As a result of the rapid advancement of generative AI in recent years, many companies are rushing to integrate AI into their businesses. One of the most common ways of doing this is to build AI systems that answer questions concerning information that can be found within a database of documents. Most solutions for such a problem are based on one key technique: Retrieval Augmented Generation (RAG). &lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Do text embeddings perfectly encode text?" class="kg-image" height="276" src="https://lh7-us.googleusercontent.com/25UaLlGmLLH-h2ljDEGD1T_eUnErWmCSsLn5gz49U6DsQebwn-kwWtkT0_GvPTswYlAyOP9iZ3Q6Gpy-K6UwuWVF4bzj-1G2cZJd6oXw4hDX92texYdByUuq7bO8qcmuKQSqfR6zkPlm9M5KoYDk12A" width="624" /&gt;&lt;figcaption&gt;Overview of a RAG system.&lt;span class="-mobiledoc-kit__atom"&gt;‌ ‌&lt;/span&gt;Source: “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks”&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This is what lots of people do now as a cheap and easy way to get started using AI: store lots of documents in a database, have the AI retrieve the most relevant documents for a given input, and then generate a response to the input that is informed by the retrieved documents.&lt;/p&gt;&lt;p&gt;These RAG systems determine document relevancy by using &amp;nbsp;“embeddings”, vector representations of documents produced by an embedding model. These embeddings are supposed to represent some notion of similarity, so documents that are relevant for search will have high vector similarity in embedding space.&lt;/p&gt;&lt;p&gt;The prevalence of RAG has led to the rise of the &lt;em&gt;vector database&lt;/em&gt;, a new type of database designed for storing and searching through large numbers of embeddings. Hundreds of millions of dollars of funding have been given out to startups that claim to facilitate RAG by making embedding search easy. And the effectiveness of RAG is the reason why lots of new applications are converting text to vectors and storing them in these vector databases.&lt;/p&gt;&lt;h3 id="embeddings-are-hard-to-read"&gt;Embeddings are hard to read&lt;/h3&gt;&lt;p&gt;So what is stored in a text embedding? Beyond the requirement of semantic similarity, there are no constraints on which embedding must be assigned for a given text input. Numbers within embedding vectors can be &lt;em&gt;anything&lt;/em&gt;, and vary based on their initialization. We can interpret the similarities of embedding with others but have no hope ever understanding the individual numbers of an embedding.&lt;br /&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Do text embeddings perfectly encode text?" class="kg-image" height="149" src="https://lh7-us.googleusercontent.com/GwnKcHZF5vTgMZlKmEobbOLiJoQLOknGoG1znqG5pT-7kWwMCOPSEK3gB-q-NnBt5ahi2FLjbaFM9x-J5DS4VKbns7de88GATWbjaR-iDeuLPWY-muNKQ6bWhqyvo4HRxXWaStkgVrhEF6B0Tdu-Ihs" width="555" /&gt;&lt;figcaption&gt;A neural embedding model (light blue) takes text input and produces an &lt;em&gt;embedding&lt;/em&gt;, a vector that can be used for search.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now imagine you’re a software engineer building a RAG system for your company. You decide to store your vectors in a vector database. You notice that in a vector database, what's stored are embedding vectors, not the text data itself. The database fills up with rows and rows of random-seeming numbers that represent text data but never ‘sees’ any text data at all. &lt;/p&gt;&lt;p&gt;You know that the text corresponds to customer documents that are protected by your company’s privacy policy. But you’re not &lt;em&gt;really&lt;/em&gt; sending text off-premises at any time; you only ever send embedding vectors, which look to you like random numbers.&lt;/p&gt;&lt;p&gt;What if someone hacks into the database and gains access to all your text embedding vectors – would this be bad? Or if the service provider wanted to sell your data to advertisers – could they? Both scenarios involve being able to take embedding vectors and &lt;em&gt;invert &lt;/em&gt;them somehow back to text.&lt;/p&gt;&lt;h3 id="from-text-to-embeddingsback-to-text"&gt;From text to embeddings...back to text&lt;/h3&gt;&lt;p&gt;The problem of recovering text from embeddings is exactly the scenario we tackle in our paper &lt;em&gt;Text Embeddings Reveal As Much as Text&lt;/em&gt; (EMNLP 2023). Are embedding vectors a secure format for information storage and communication? Put simply: can input text be recovered from output embeddings?&lt;/p&gt;&lt;p&gt;Before diving into solutions, let’s think about the problem a little bit more. Text embeddings are the output of neural networks, sequences of matrix multiplications joined by nonlinear function operations applied to input data. In traditional text processing neural networks, a string input is split into a number of token vectors, which repeatedly undergo nonlinear function operations. At the output layer of the model, tokens are averaged into a single embedding vector.&lt;/p&gt;&lt;p&gt;A maxim from the signal processing community known as the data processing inequality tells us that functions cannot add information to an input, they can only sustain or decrease the amount of information available. Even though conventional wisdom tells us that deeper layers of a neural network are constructing ever-higher-order representations, they aren’t adding any information about the world that didn’t come in on the input side. &lt;/p&gt;&lt;p&gt;Additionally, the nonlinear layers certainly destroy &lt;em&gt;some&lt;/em&gt; information. One ubiquitous nonlinear layer in modern neural networks is the “ReLU” function, which simply sets all negative inputs to zero. After applying ReLU throughout the many layers of a typical text embedding model, it is not possible to retain all the information from the input.&lt;br /&gt;&lt;/p&gt;&lt;h3 id="inversion-in-other-contexts"&gt;Inversion in other contexts&lt;/h3&gt;&lt;p&gt;Similar questions about information content have been asked in the computer vision community. Several results have shown that deep representations (embeddings, essentially) from image models can be used to recover the input images with some degree of fidelity. An early result (Dosovitskiy, 2016) showed that images can be recovered from the feature outputs of deep convolutional neural networks (CNNs). Given the high-level feature representation from a CNN, they could invert it to produce a blurry-but-similar version of the original input image.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Do text embeddings perfectly encode text?" class="kg-image" height="1865" src="https://thegradient.pub/content/images/2024/03/Group-66--3-.png" width="2000" /&gt;&lt;figcaption&gt;In computer vision, inversion models (yellow) have successfully reconstructed images given only the 1000 probability outputs of an ImageNet classifier, most of which are close to 0. (Images from &lt;em&gt;Understanding Invariance via Feedforward Inversion of Discriminatively Trained Classifiers&lt;/em&gt;.)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;People have improved on image embedding inversion process since 2016: models have been developed that do inversion with higher accuracy, and have been shown to work across more settings. Surprisingly, some work has shown that images can be inverted from the outputs of an ImageNet classifier (1000 class probabilities).&lt;/p&gt;&lt;h3 id="the-journey-to-vec2text"&gt;The journey to vec2text&lt;/h3&gt;&lt;p&gt;If inversion is possible for image representations, then why can’t it work for text? Let’s consider a toy problem of recovering text embeddings. For our toy setting we’ll restrict text inputs to 32 tokens (around 25 words, a sentence of decent length) and embed them all to vectors of 768 floating-point numbers. At 32-bit precision, these embeddings are 32 * 768 = 24,576 bits or around 3 kilobytes. &lt;/p&gt;&lt;p&gt;Few words represented by many bits. Do you think we could perfectly reconstruct the text within this scenario? &lt;/p&gt;&lt;p&gt;First things first: we need to define a measurement of &lt;em&gt;goodness&lt;/em&gt;, to know how well we have accomplished our task. One obvious metric is "exact match", how often we get the exact input back after inversion. No prior inversion methods have any success on exact match, so it’s quite an ambitious measurement. So maybe we want to start with a smooth measurement that measures how similar the inverted text is to the input. For this we’ll use BLEU score, which you can just think of as a percentage of how close the inverted text is to the input.&lt;/p&gt;&lt;p&gt;With our success metric defined, let us move on to proposing an approach to evaluate with said metric. &amp;nbsp;For a first approach, we can pose inversion as a traditional machine learning problem, and we solve it the best way we know how: by gathering a large dataset of embedding-text pairs, and train a model to output the text given the embedding as input.&lt;/p&gt;&lt;p&gt;So this is what we did. We build a transformer that takes the embedding as input and train it using traditional language modeling on the output text. This first approach gives us a model with a BLEU score of around 30/100. Practically, the model can guess the topic of the input text, and get some of the words, but it loses their order and often gets most of them wrong. The exact match score is close to zero. It turns out that asking a model to reverse the output of another model in a single forward pass is quite hard (as are other complicated text generation tasks, like generating text in perfect sonnet form or satisfying multiple attributes).&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Do text embeddings perfectly encode text?" class="kg-image" height="401" src="https://lh7-us.googleusercontent.com/BorB5n0gaGnDObtJLRPC4lOHYn6l3tKS2AnXv03Oj62dPcqKjNFoNv6lfPtOL6KlpIo8U4BZPo8EC4BLVb8DFtDFzjt8CCbUOEeYeikHqTATDVsCNyWL331zcl6eQbU3uCTte1WkvtcMF9hMlnwvny4" width="624" /&gt;&lt;figcaption&gt;Overview of architectures considered. Prior work (left) uses a decoder-only architecture and inputs an embedding as a prefix. We initially trained an encoder-decoder model (middle) to condition on an upscaled sentence embedding on the encoder-side. Our final method (right) includes an additional “hypothesis” text along with an upscaled hypothesis embedding.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;After training our initial model, we noticed something interesting. A different way to measure model output quality is by re-embedding the generated text (we call this the “hypothesis”) and measuring this embedding’s similarity to the true embedding. When we do this with our model’s generations, we see a very high cosine similarity – around 0.97. This means that we’re able to generate text that’s close in embedding space, but not identical to, the ground-truth text.&lt;/p&gt;&lt;p&gt;(An aside: what if this weren’t the case? That is, what if the embedding assigned our incorrect hypothesis the same embedding as the original sequence. Our embedder would be &lt;em&gt;lossy&lt;/em&gt;, mapping multiple inputs to the same output. If this were the case, then our problem would be hopeless, and we would have no way of distinguishing which of multiple possible sequences produced it. In practice, we never observe these types of collisions in our experiments.)&lt;/p&gt;&lt;p&gt;The observation that hypotheses have different embeddings to the ground truth inspires an optimization-like approach to embedding inversion. Given a ground-truth embedding (where we want to go), and a current hypothesis text and its embedding (where we are right now), we can train a corrector model that’s trained to output something that’s closer to the ground-truth than the hypothesis.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Do text embeddings perfectly encode text?" class="kg-image" height="247" src="https://lh7-us.googleusercontent.com/ne0JB3F3WLFoTQR0fSgdxsmL6Ap4anP767qyjzNaySpkyu_uyAJEHnbzvsTmOsfsZOI6xMO1vhWWMp6vp_n_DMtAap-XucXKtH40_yctKbaUYQqBeWSbZEnhX3-LYZ1xzIvY-PMyO1kMh53DUCoGBXQ" width="624" /&gt;&lt;figcaption&gt;Overview of our method, Vec2Text. Given access to a target embedding e (blue) and query access to an embedding model ϕ (blue model), the system aims to iteratively generate (yellow model) hypotheses ˆe (pink) to reach the target.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Our goal is now clear: we want to build a system that can take a ground-truth embedding, a hypothesis text sequence, and the hypothesis position in embedding space, and predict the true text sequence. We think of this as a type of ‘learned optimization’ where we’re taking steps in embedding space in the form of discrete sequences. This is the essence of our method, which we call vec2text.&lt;/p&gt;&lt;p&gt;After working through some details and training the model, this process works extremely well! A single forward pass of correction increases the BLEU score from 30 to 50. And one benefit of this model is that it can naturally be queried recursively. Given a current text and its embedding, we can run many steps of this optimization, iteratively generating hypotheses, re-embedding them, and feeding them back in as input to the model. With 50 steps and a few tricks, we can get back 92% of 32-token sequences exactly, and get to a BLEU score of 97! (Generally achieving BLEU score of 97 means we’re almost perfectly reconstructing every sentence, perhaps with a few punctuation marks misplaced here and there.)&lt;br /&gt;&lt;/p&gt;&lt;h3 id="scaling-and-future-work"&gt;Scaling and future work &lt;/h3&gt;&lt;p&gt;The fact that text embeddings can be perfectly inverted raises many follow-up questions. For one, the text embedding vector contains a fixed number of bits; there must be some sequence length at which information can no longer be perfectly stored within this vector. Even though we can recover most texts of length 32, some embedding models can embed documents up to thousands of tokens. We leave it up to future work to analyze the relationship between text length, embedding size, and embedding invertibility.&lt;/p&gt;&lt;p&gt;Another open question is how to build systems that can defend against inversion. Is it possible to create models that can successfully embed text such that embeddings remain useful while obfuscating the text that created them?&lt;/p&gt;&lt;p&gt;Finally, we are excited to see how our method might apply to other modalities. The main idea behind vec2text (a sort of iterative optimization in embedding space) doesn’t use any text-specific tricks. It’s a method that iteratively recovers information contained in any fixed input, given black-box access to a model. It remains to be seen how these ideas might apply to inverting embeddings from other modalities as well as to approaches more general than embedding inversion.&lt;/p&gt;&lt;p&gt;To use our models to invert text embeddings, or to get started running embedding inversion experiments yourself, check out our Github repository: https://github.com/jxmorris12/vec2text&lt;/p&gt;&lt;hr /&gt;&lt;h3 id="references"&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;Inverting Visual Representations with Convolutional Networks (2015), https://arxiv.org/abs/1506.02753&lt;/p&gt;&lt;p&gt;Understanding Invariance via Feedforward Inversion of Discriminatively Trained Classifiers (2021), https://proceedings.mlr.press/v139/teterwak21a/teterwak21a.pdf&lt;/p&gt;&lt;p&gt;Text Embeddings Reveal (Almost) As Much As Text (2023), https://arxiv.org/abs/2310.06816&lt;/p&gt;&lt;p&gt;Language Model Inversion (2024), https://arxiv.org/abs/2311.13647&lt;/p&gt;&lt;h3 id="author-bio"&gt;&lt;strong&gt;Author Bio&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;Jack Morris is a PhD student at Cornell Tech in New York City. He works on research at the intersection of machine learning, natural language processing, and security. He’s especially interested in the information content of deep neural representations like embeddings and classifier outputs.&lt;/p&gt;&lt;h3 id="citation"&gt;Citation&lt;/h3&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Jack Morris, "Do text embeddings perfectly encode text?", The Gradient, 2024.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;BibTeX citation:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;@article{morris2024inversion,
    author = {Jack Morris},
    title = {Do text embeddings perfectly encode text?},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/text-embedding-inversion},
}&lt;/code&gt;&lt;/pre&gt;]]&amp;gt;&amp;lt;![CDATA[Why Doesn’t My Model Work?]]&amp;gt;https://thegradient.pub/why-doesnt-my-model-work/65ce1b5993571d5c8c1549b8Sat, 24 Feb 2024 18:41:54 GMT&lt;p&gt;Have you ever trained a model you thought was good, but then it failed miserably when applied to real world data? If so, you’re in good company. Machine learning processes are complex, and it’s very easy to do things that will cause overfitting without it being obvious. In the 20 years or so that I’ve been working in machine learning, I’ve seen many examples of this, prompting me to write “How to avoid machine learning pitfalls: a guide for academic researchers” in an attempt to prevent other people from falling into these traps.&lt;/p&gt;&lt;p&gt;But you don’t have to take my word for it. These issues are being increasingly reported in both the scientific and popular press. Examples include the observation that hundreds of models developed during the Covid pandemic simply don’t work, and that a water quality system deployed in Toronto regularly told people it was safe to bathe in dangerous water. Many of these are documented in the AIAAIC repository. It’s even been suggested that these machine learning missteps are causing a reproducibility crisis in science — and, given that many scientists use machine learning as a key tool these days, a lack of trust in published scientific results.&lt;/p&gt;&lt;p&gt;In this article, I’m going to talk about some of the issues that can cause a model to seem good when it isn’t. I’ll also talk about some of the ways in which these kinds of mistakes can be prevented, including the use of the recently-introduced REFORMS checklist for doing ML-based science.&lt;/p&gt;&lt;h2 id="duped-by-data"&gt;Duped by Data&lt;/h2&gt;&lt;p&gt;Misleading data is a good place to start, or rather not a good place to start, since the whole machine learning process rests upon the data that’s used to train and test the model.&lt;/p&gt;&lt;p&gt;In the worst cases, misleading data can cause the phenomenon known as &lt;em&gt;garbage in garbage out&lt;/em&gt;; that is, you can train a model, and potentially get very good performance on the test set, but the model has no real world utility. Examples of this can be found in the aforementioned review of Covid prediction models by Roberts et al. In the rush to develop tools for Covid prediction, a number of public datasets became available, but these were later found to contain misleading signals — such as overlapping records, mislabellings and hidden variables — all of which helped models to accurately predict the class labels without learning anything useful in the process.&lt;/p&gt;&lt;p&gt;Take hidden variables. These are features that are present in data, and which happen to be predictive of class labels within the data, but which are not directly related to them. If your model latches on to these during training, it will appear to work well, but may not work on new data. For example, in many Covid chest imaging datasets, the orientation of the body is a hidden variable: people who were sick were more likely to have been scanned lying down, whereas those who were standing tended to be healthy. Because they learnt this hidden variable, rather than the true features of the disease, many Covid machine learning models turned out to be good at predicting posture, but bad at predicting Covid. Despite their name, these hidden variables are often in plain sight, and there have been many examples of classifiers latching onto boundary markers, watermarks and timestamps embedded in images, which often serve to distinguish one class from another without having to look at the actual data.&lt;/p&gt;&lt;p&gt;A related issue is the presence of spurious correlations. Unlike hidden variables, these have no true relationship to anything else in the data; they’re just patterns that happen to correlate with the class labels. A classic example is the tank problem, where the US military allegedly tried to train a neural network to identify tanks, but it actually recognised the weather, since all the pictures of tanks were taken at the same time of day. Consider the images below: a machine learning model could recognise all the pictures of tanks in this dataset just by looking at the colour of pixels towards the top of an image, without having to consider the shape of any of the objects. The performance of the model would appear great, but it would be completely useless in practice.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Why Doesn’t My Model Work?" class="kg-image" height="563" src="https://thegradient.pub/content/images/2024/02/1.png" width="1379" /&gt;&lt;figcaption&gt;(Source: by author)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Many (perhaps most) datasets contain spurious correlations, but they’re not usually as obvious as this one. Common computer vision benchmarks, for example, are known to have groups of background pixels that are spuriously correlated with class labels. This represents a particular challenge to deep learners, which have the capacity to model many patterns within the data; various studies have shown that they do tend to capture spuriously correlated patterns, and this reduces their generality. Sensitivity to adversarial attacks is one consequence of this: if a deep learning model bases its prediction on spurious correlations in the background pixels of an image, then making small changes to these pixels can flip the prediction of the model. Adversarial training, where a model is exposed to adversarial samples during training, can be used to address this, but it’s expensive. An easier approach is just to look at your model, and see what information it’s using to make its decisions. For instance, if a saliency map produced by an explainable AI technique suggests that your model is focusing on something in the background, then it’s probably not going to generalise well.&lt;/p&gt;&lt;p&gt;Sometimes it’s not the data itself that is problematic, but rather the labelling of the data. This is especially the case when data is labelled by humans, and the labels end up capturing biases, misassumptions or just plain old mistakes made by the labellers. Examples of this can be seen in datasets used as image classification benchmarks, such as MNIST and CIFAR, which typically have a mislabelling rate of a couple of percent — not a huge amount, but pretty significant where modellers are fighting over accuracies in the tenths of a percent. That is, if your model does slightly better than the competition, is it due to an actual improvement, or due to modelling noise in the labelling process? Things can be even more troublesome when working with data that has implicit subjectivity, such as sentiment classification, where there’s a danger of overfitting particular labellers.&lt;/p&gt;&lt;h2 id="led-by-leaks"&gt;Led by Leaks&lt;/h2&gt;&lt;p&gt;Bad data isn’t the only problem. There’s plenty of scope for mistakes further down the machine learning pipeline. A common one is data leakage. This happens when the model training pipeline has access to information it shouldn’t have access to, particularly information that confers an advantage to the model. Most of the time, this manifests as information leaks from the test data — and whilst most people know that test data should be kept independent and not explicitly used during training, there are various subtle ways that information can leak out.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;One example is performing a data-dependent preprocessing operation on an entire dataset, before splitting off the test data. That is, making changes to all the data using information that was learnt by looking at all the data. Such operations vary from the simple, such as centering and scaling numerical features, to the complex, such as feature selection, dimensionality reduction and data augmentation — but they all have in common the fact that they use knowledge of the whole dataset to guide their outcome. This means that knowledge of the test data is implicitly entering the model training pipeline, even if it is not explicitly used to train the model. As a consequence, any measure of performance derived from the test set is likely to be an overestimate of the model’s true performance.&lt;/p&gt;&lt;p&gt;Let’s consider the simplest example: centering and scaling. This involves looking at the range of each feature, and then using this information to rescale all the values, typically so that the mean is 0 and the standard deviation is 1. If this is done on the whole dataset before splitting off the test data, then the scaling of the training data will include information about the range and distribution of the feature values in the test set. This is particularly problematic if the range of the test set is broader than the training set, since the model could potentially infer this fact from the truncated range of values present in the training data, and do well on the test set just by predicting values higher or lower than those which were seen during training. For instance, if you’re working on stock price forecasting from time series data with a model that takes inputs in the range 0 to 1 but it only sees values in the range 0 to 0.5 during training, then it’s not too hard for it to infer that stock prices will go up in the future.&lt;/p&gt;&lt;p&gt;In fact, forecasting is an area of machine learning that is particularly susceptible to data leaks, due to something called &lt;em&gt;look ahead bias&lt;/em&gt;. This occurs when information the model shouldn’t have access to leaks from the future and artificially improves its performance on the test set. This commonly happens when the training set contains samples that are further ahead in time than the test set. I’ll give an example later of when this can happen, but if you work in this area, I’d also strongly recommend taking a look at this excellent review of pitfalls and best practices in evaluating time series forecasting models.&lt;/p&gt;&lt;p&gt;An example of a more complex data-dependent preprocessing operation leading to overly-optimistic performance metrics can be found in this review of pre-term birth prediction models. Basically, a host of papers reported high accuracies at predicting whether a baby would be born early, but it turned out that all had applied data augmentation to the data set before splitting off the test data. This resulted in the test set containing augmented samples of training data, and the training set containing augmented samples of test data — which amounted to a pretty significant data leak. When the authors of the review corrected this, the predictive performance of the models dropped from being near perfect to not much better than random.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Why Doesn’t My Model Work?" class="kg-image" height="735" src="https://thegradient.pub/content/images/2024/02/2.png" width="1379" /&gt;&lt;figcaption&gt;(Source: https://arxiv.org/abs/2108.02497)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Oddly, one of the most common examples of data leakage doesn’t have an agreed name (the terms overhyping and sequential overfitting have been suggested) but is essentially a form of &lt;em&gt;training to the test set&lt;/em&gt;. By way of example, imagine the scenario depicted above where you’ve trained a model and evaluated it on the test set. You then decided its performance was below where you wanted it to be. So, you tweaked the model, and then you reevaluated it. You still weren’t happy, so you kept on doing this until its performance on the test set was good enough. Sounds familiar? Well, this is a common thing to do, but if you’re developing a model iteratively and using the same test set to evaluate the model after each iteration, then you’re basically using that test set to guide the development of the model. The end result is that you’ll overfit the test set and probably get an over-optimistic measure of how well your model generalises.&lt;/p&gt;&lt;p&gt;Interestingly, the same process occurs when people use community benchmarks, such as MNIST, CIFAR and ImageNet. Almost everyone who works on image classification uses these data sets to benchmark their approaches; so, over time, it’s inevitable that some overfitting of these benchmarks will occur. To mitigate against this, it’s always advisable to use a diverse selection of benchmarks, and ideally try your technique on a data set which other people haven’t used.&lt;/p&gt;&lt;h2 id="misinformed-by-metrics"&gt;Misinformed by Metrics&lt;/h2&gt;&lt;p&gt;Once you’ve built your model robustly, you then have to evaluate it robustly. There’s plenty that can go wrong here too. Let’s start with an inappropriate choice of metrics. The classic example is using accuracy with an imbalanced dataset. Imagine that you’ve managed to train a model that always predicts the same label, regardless of its input. If half of the test samples have this label as their ground truth, then you’ll get an accuracy of 50% — which is fine, a bad accuracy for a bad classifier. If 90% of the test samples have this label, then you’ll get an accuracy of 90% — a good accuracy for a bad classifier. This level of imbalance is not uncommon in real world data sets, and when working with imbalanced training sets, it’s not uncommon to get classifiers that always predict the majority label. In this case, it would be much better to use a metric like F score or Matthews correlation coefficient, since these are less sensitive to class imbalances. However, all metrics have their weaknesses, so it’s always best to use a portfolio of metrics that give different perspectives on a model’s performance and failure modes.&lt;/p&gt;&lt;p&gt;Metrics for time series forecasting are particularly troublesome. There are a lot of them to choose from, and the most appropriate choice can depend on both the specific problem domain and the exact nature of the time series data. Unlike metrics used for classification, many of the regression metrics used in time series forecasting have no natural scale, meaning that raw numbers can be misleading. For instance, the interpretation of mean squared errors depends on the range of values present in the time series. For this reason, it’s important to use appropriate baselines in addition to appropriate metrics. As an example, this (already mentioned) review of time series forecasting pitfalls demonstrates how many of the deep learning models published at top AI venues are actually less good than naive baseline models. For instance, they show that an autoformer, a kind of complex transformer model designed for time series forecasting, can be beaten by a trivial model that predicts no change at the next time step — something that isn’t apparent from looking at metrics alone.&lt;/p&gt;&lt;p&gt;In general, there is a trend towards developing increasingly complex models to solve difficult problems. However, it’s important to bear in mind that some problems may not be solvable, regardless of how complex the model becomes. This is probably the case for many financial time series forecasting problems. It’s also the case when predicting certain natural phenomena, particularly those in which a chaotic component precludes prediction beyond a certain time horizon. For instance, many people think that earthquakes can not be predicted, yet there are a host of papers reporting good performance on this task. This review paper discusses how these correct predictions may be due to a raft of modelling pitfalls, including inappropriate choice of baselines and overfitting due to data sparsity, unnecessary complexity and data leaks.&lt;/p&gt;&lt;p&gt;Another problem is assuming that a single evaluation is sufficient to measure the performance of a model. Sometimes it is, but a lot of the time you’ll be working with models that are stochastic or unstable; so, each time you train them, you get different results. Or you may be working with a small data set where you might just get lucky with an easy test split. To address both situations, it is commonplace to use resampling methods like cross-validation, which train and test a model on different subsets of the data and then work out the average performance. However, resampling introduces its own risks. One of these is the increased risk of data leaks, particularly when assuming that data-dependent preprocessing operations (like centering and scaling and feature selection) only need to be done once. They don’t; they need to be done independently for each iteration of the resampling process, and to do otherwise can cause a data leak. Below is an example of this, showing how feature selection should be done independently on the two training sets (in blue) used in the first two iterations of cross-validation, and how this results in different features being selected each time.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Why Doesn’t My Model Work?" class="kg-image" height="640" src="https://thegradient.pub/content/images/2024/02/3.png" width="1380" /&gt;&lt;figcaption&gt;(Source: https://arxiv.org/abs/2108.02497)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;As I mentioned earlier, the danger of data leaks is even greater when working with time series data. Using standard cross-validation, every iteration except one will involve using at least one training fold that is further ahead in time than the data in the test fold. For example, if you imagine that the data rows in the figure above represent time-ordered multivariate samples, then the test sets (in pink) used in both iterations occur earlier in the time series than all or part of the training data. This is an example of a look ahead bias. Alternative approaches, such as blocked cross-validation, can be used to prevent these.&lt;/p&gt;&lt;p&gt;Multiple evaluations aren’t an option for everyone. For example, training a foundation model is both time-consuming and expensive, so doing it repeatedly is not feasible. Depending on your resources, this may be the case for even relatively small deep learning models. If so, then also consider using other methods for measuring the robustness of models. This includes things like using explainability analysis, performing ablation studies, or augmenting test data. These can allow you to look beyond potentially-misleading metrics and gain some appreciation of how a model works and how it might fail, which in turn can help you decide whether to use it in practice.&lt;/p&gt;&lt;h2 id="falling-deeper"&gt;Falling Deeper&lt;/h2&gt;&lt;p&gt;So far, I’ve mostly talked about general machine learning processes, but the pitfalls can be even greater when using deep learning models. Consider the use of latent space models. These are often trained separately to the predictive models that use them. That is, it’s not unusual to train something like an autoencoder to do feature extraction, and then use the output of this model within the training of a downstream model. When doing this, it’s essential to ensure that the test set used in the downstream model does not intersect with the training data used in the autoencoder — something that can easily happen when using cross-validation or other resampling methods, e.g. when using different random splits or not selecting models trained on the same training folds.&lt;/p&gt;&lt;p&gt;However, as deep learning models get larger and more complex, it can be harder to ensure these kinds of data leaks do not occur. For instance, if you use a pre-trained foundation model, it may not be possible to tell whether the data used in your test set was used to train the foundation model — particularly if you’re using benchmark data from the internet to test your model. Things get even worse if you’re using composite models. For example, if you’re using a BERT-type foundation model to encode the inputs when fine-tuning a GPT-type foundation model, you have to take into account any intersection between the datasets used to train the two foundation models in addition to your own fine-tuning data. In practice, some of these data sets may be unknown, meaning that you can’t be confident whether your model is correctly generalising or merely reproducing data memorised during pre-training.&lt;/p&gt;&lt;h2 id="avoiding-the-pits"&gt;Avoiding the Pits&lt;/h2&gt;&lt;p&gt;These pitfalls are all too common. So, what’s the best way to avoid them? Well, one thing you can do is use a checklist, which is basically a formal document that takes you through the key pain points in the machine learning pipeline, and helps you to identify potential issues. In domains with high-stakes decisions, such as medicine, there are already a number of well-established checklists, such as CLAIM, and adherence to these is typically enforced by journals that publish in these areas.&lt;/p&gt;&lt;p&gt;However, I’d like to briefly introduce a new kid on the block: REFORMS, a consensus-based checklist for doing machine learning-based science. This was put together by 19 researchers across computer science, data science, mathematics, social sciences, and the biomedical sciences — including myself — and came out of a recent workshop on the reproducibility crisis in ML‑based science. It is intended to address the common mistakes that occur in the machine learning pipeline, including many of those mentioned in this article, in a more domain-independent manner. It consists of two parts: the checklist itself, and also a paired guidance document, which explains why each of the checklist items are important. The checklist works through the main components of a machine learning-based study, in each case encouraging the user to verify that the machine learning process is designed in such a way that it supports the overall aims of the study, doesn’t stumble into any of the common pitfalls, and enables the results to be verified by an independent researcher. Whilst it’s focused on the application of machine learning within a scientific context, a lot of what it covers is more generally applicable, so I’d encourage you to take a look even if you don’t consider your work to be “science”.&lt;/p&gt;&lt;p&gt;Another way of avoiding pitfalls is to make better use of tools. Now, one of my pet gripes regarding the current state of machine learning is that commonly-used tools do little to prevent you from making mistakes. That is, they’ll happily let you abuse the machine learning process in all sorts of ways without telling you what you’re doing is wrong. Nevertheless, help is available in the form of experiment tracking frameworks, which automatically keep a record of the models you trained and how you trained them, and this can be useful for spotting things like data leaks and training to the test set. An open source option is MLFlow, but there are plenty of commercial offerings. MLOps tools take this even further, and help to manage all the moving parts in a machine learning workflow, including the people.&lt;/p&gt;&lt;h2 id="final-thought"&gt;Final Thought&lt;/h2&gt;&lt;p&gt;It is possible to train a good model that generalises well to unseen data, but I wouldn’t believe this until you’re satisfied that nothing which could have gone wrong has gone wrong. A healthy sense of suspicion is a good thing: do look at your trained model to make sure it’s doing something sensible, do analyse your metrics to understand where it’s making mistakes, do calibrate your results against appropriate baselines, and do consider using checklists to make sure you haven’t overlooked something important.&lt;/p&gt;&lt;hr /&gt;&lt;h2 id="author-bio"&gt;Author Bio&lt;/h2&gt;&lt;p&gt;Michael is an Associate Professor at Heriot-Watt University, Edinburgh. He’s spent the last 20 years or so doing research on machine learning and bio-inspired computing. For more info see his academic website. He also writes about computer science more generally in his Fetch Decode Execute substack.&lt;/p&gt;&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Michael Lones, "Why Doesn’t My Model Work?", The Gradient, 2024.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;BibTeX citation:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;@article{lones2024why,
    author = {Michael Lones},
    title = {Why Doesn’t My Model Work?},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/why-doesnt-my-model-work},
}&lt;/code&gt;&lt;/pre&gt;]]&amp;gt;&amp;lt;![CDATA[Deep learning for single-cell sequencing: a microscope to see the diversity of cells]]&amp;gt;https://thegradient.pub/deep-learning-for-single-cell-sequencing-a-microscope-to-uncover-the-rich-diversity-of-individual-cells/65606b4793571d5c8c154793Sat, 13 Jan 2024 18:12:44 GMT&lt;p&gt;The history of each living being is written in its genome, which is stored as DNA and present in nearly every cell of the body. No two cells are the same, even if they share the same DNA and cell type, as they still differ in the regulators that control how DNA is expressed by the cell. The human genome consists of 3 billion base pairs spread over 23 chromosomes. Within this vast genetic code, there are approximately 20,000 to 25,000 genes, constituting the protein-coding DNA and accounting for about 1% of the total genome [1]. To explore the functioning of complex systems in our bodies, especially this small coding portion of DNA, a precise sequencing method is necessary, and single-cell sequencing (sc-seq) technology fits this purpose.&lt;/p&gt;&lt;p&gt;In 2013, Nature selected single-cell RNA sequencing as the Method of the Year [2] (Figure 3), highlighting the importance of this method for exploring cellular heterogeneity through the sequencing of DNA and RNA at the individual cell level. Subsequently, numerous tools have emerged for the analysis of single-cell RNA sequencing data. For example, the scRNA-tools database has been compiling software for the analysis of single-cell RNA data since 2016, and by 2021, the database includes over 1000 tools [3]. Among these tools, many involve methods that leverage Deep Learning techniques, which will be the focus of this article – we will explore the pivotal role that Deep Learning, in particular, has played as a key enabler for advancing single-cell sequencing technologies.&lt;/p&gt;&lt;h2 id="background"&gt;Background&lt;/h2&gt;&lt;h3 id="flow-of-genetic-information-from-dna-to-protein-in-cells"&gt;Flow of genetic information from DNA to protein in cells&lt;/h3&gt;&lt;p&gt;Let’s first go over what exactly cells and sequences are.&lt;strong&gt; &lt;/strong&gt;The cell is the fundamental unit of our bodies and the key to understanding how our bodies function in good health and how molecular dysfunction leads to disease. Our bodies are made of trillions of cells, and nearly every cell contains three genetic information layers: DNA, RNA, and protein. DNA is a long molecule containing the genetic code that makes each person unique. Like a source code, it includes several instructions showing how to make each protein in our bodies. These proteins are the workhorses of the cell that carry out nearly every task necessary for cellular life. For example, the enzymes that catalyze chemical reactions within the cell and DNA polymerases that contribute to DNA replication during cell division, are all proteins. The cell synthesizes proteins in two steps: Transcription and Translation (Figure 1), which are known as gene expression. DNA is first transcribed into RNA, then RNA is translated into protein. We can consider RNA as a messenger between DNA and protein.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Deep learning for single-cell sequencing: a microscope to see the diversity of cells" class="kg-image" height="204" src="https://lh7-us.googleusercontent.com/FE0jl7A1RgTBL5tPgpxsw624oxkwsWMLx3lRvyCQYwjLLLIezvRjxHONrVsJ8IJEx1EjZkTklRUD09IPg4JxBLq_goJPdCDy3PH5kA0EPntuMIhso_8R0Vouja3yZtU4rG1eYGarcsGjT4naVbPD2Q" width="447" /&gt;&lt;figcaption&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;. The central dogma of biology&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;While the cells of our body share the same DNA, they vary in their biological activity. For instance, the distinctions between immune cells and heart cells are determined by the genes that are either activated or deactivated in these cells. Generally, when a gene is activated, it leads to the creation of more RNA copies, resulting in increased protein production. Therefore, as cell types differ based on the quantity and type of RNA/protein molecules synthesized, it becomes intriguing to assess the abundance of these molecules at the single-cell level. This will enable us to investigate the behavior of our DNA &amp;nbsp;within each cell and attain a high-resolution perspective of the various parts of our bodies.&lt;/p&gt;&lt;p&gt;In general, all single-cell sequencing technologies can be divided into three main steps:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Isolation of single cells from the tissue of interest and extraction of genetic material from each isolated cell&lt;/li&gt;&lt;li&gt;Amplification of genetic material from each isolated cell and library preparation&lt;/li&gt;&lt;li&gt;Sequencing of the library using a next-generation sequencer and data analysis&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Navigating through the intricate steps of cellular biology and single-cell sequencing technologies, a pivotal question emerges: How is single-cell sequencing data represented numerically?&lt;/p&gt;&lt;h3 id="structure-of-single-cell-sequencing-data"&gt;Structure of single-cell sequencing data&lt;/h3&gt;&lt;p&gt;The structure of single-cell sequencing data takes the form of a matrix (Figure 2), where each row corresponds to a cell that has been sequenced and annotated with a unique barcode. The number of rows equals the total number of cells analyzed in the experiment. On the other hand, each column corresponds to a specific gene. Genes are the functional units of the genome that encode instructions for the synthesis of proteins or other functional molecules. In the case of scRNA seq data, the numerical entries in the matrix represent the expression levels of genes in individual cells. These values indicate the amount of RNA produced from each gene in a particular cell, providing insights into the activity of genes within different cells.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Deep learning for single-cell sequencing: a microscope to see the diversity of cells" class="kg-image" height="285" src="https://lh7-us.googleusercontent.com/qTHPAYL5g0S2KEbMKrt3HXoqTmPqeGpo2cqPyFN_NXzE_jUgC1H7C67igubfJOY_mfLXyILiPlQxW0Z3FOdisQ6wZ9cnN72mqeJJ9_Aa2x7qs79DHAgsZtuJjrMXpRe79TNtEhrpb10hqofcO_9aTA" width="280" /&gt;&lt;figcaption&gt;&lt;em&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;. Schema of single-cell sequencing data&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3 id="single-cell-sequencing-overview"&gt;&lt;strong&gt;Single Cell Sequencing Overview&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;For more than 150 years, biologists have wanted to identify all the cell types in the human body and classify them into distinct types based on accurate descriptions of their properties. The Human Cell Atlas Project (HCAP), the genetic equivalent of the Human Genome Project [4], is an international collaborative effort to map all the cells in the human body.” We can conceptualize the Human Cell Atlas as a map endeavoring to portray the human body coherently and systematically. Much like Google Maps, which allows us to zoom in for a closer examination of intricate details, the Human Cell Atlas provides insights into spatial information, internal attributes, and even the relationships among elements”, explains Aviv Regev, a computational and systems biologist at the Broad Institute of MIT and Harvard and Executive Vice President and Head of Genentech Research.&lt;/p&gt;&lt;p&gt;This analogy seamlessly aligns with the broader impact of single-cell sequencing, since it allows the analysis of individual cells instead of bulk populations. This technology proves invaluable in addressing intricate biological inquiries related to developmental processes and comprehending heterogeneous cellular or genetic changes under various treatment conditions or disease states. Additionally, it facilitates the identification of novel cell types within a given cellular population. The initiation of the first single-cell RNA sequencing (scRNA-seq) paper in 2009 [5], subsequently designated as the "method of the year" in 2013 [2], marked the genesis of an extensive endeavor to advance both experimental and computational techniques dedicated to unraveling the intricacies of single-cell transcriptomes.&lt;/p&gt;&lt;p&gt;As the technological landscape evolves, the narrative transitions to the advancements in single-cell research, particularly the early focus on single-cell RNA sequencing (scRNA-seq) due to its cost-effectiveness in studying complex cell populations.” In some ways, RNA has always been one of the easiest things to measure,” says Satija [6], a researcher at the New York Genome Center (NYGC). &amp;nbsp;Yet, the rapid development of single-cell technology has ushered in a new era of possibilities—multimodal single-cell data integration. Recognized as the "Method of the Year 2019" by Nature [7] (Figure 3), this approach allows the measurement of different cellular modalities, including the genome, epigenome, and proteome, within the same cell. The layering of multiple pieces of information provides powerful insights into cellular identity, posing the challenge of effectively modeling and combining datasets generated from multimodal measurements. This integration challenge is met with the introduction of Multi-view learning [8] methods, exploring common variations across modalities. This sophisticated approach, incorporating deep learning techniques, showcases relevant results across various fields, particularly in biology and biomedicine.&lt;/p&gt;&lt;p&gt;Amidst these advancements, a distinct challenge surfaces in the persistent limitation of single-cell RNA sequencing—the loss of spatial information during transcriptome profiling by isolating cells from their original position. Spatially resolved transcriptomics (SRT) emerges as a pivotal solution [9], addressing the challenge by preserving spatial details during the study of complex biological systems. This recognition of spatially resolved transcriptomics as the method of the year 2020 solidifies its place as a critical solution to the challenges inherent in advancing our understanding of complex biological systems.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Deep learning for single-cell sequencing: a microscope to see the diversity of cells" class="kg-image" height="177" src="https://lh7-us.googleusercontent.com/wuqGlu6SprK1ganCDuvgJLJqHhfPhGHpO2xZTSYQ0orB3PLdbAhYwZNXUPVrbRWjH9bHimSgLOsEEDhTKZumcHovsqUqEGo5GILDf74QkaXAiqdpGUdjbda7XYsmrqbsh9sH_ASz6auWXTCtSbHF4g" width="591" /&gt;&lt;figcaption&gt;&lt;em&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;. Evolution of single-cell sequencing over time&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Having explored the panorama of single-cell sequencing, let us now delve into the role of deep learning in the context of single-cell sequencing.&lt;/p&gt;&lt;h3 id="deep-learning-on-single-cell-sequencing"&gt;Deep Learning on single-cell sequencing&lt;/h3&gt;&lt;p&gt;Deep learning is increasingly employed in single-cell analysis due to its capacity to handle the complexity of single-cell sequencing data. In contrast, conventional machine-learning approaches require significant effort to develop a feature engineering strategy, typically designed by domain experts. The deep learning approach, however, autonomously captures relevant characteristics from single-cell sequencing data, addressing the heterogeneity between single-cell sequencing experiments, as well as the associated noise and sparsity in such data. Below are three key reasons for the application of deep learning in single-cell sequencing:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;High-Dimensional Data&lt;/strong&gt;: Single-cell sequencing generates high-dimensional data, with thousands of genes and their expression levels measured for each cell. Deep learning models are adept at capturing complex relationships and patterns within this data, which can be challenging for traditional statistical methods.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Non-Linearity:&lt;/strong&gt; Single-cell gene expression data is characterized by its inherent nonlinearity between gene expressions and cell-to-cell heterogeneity. Traditional statistical methods encounter difficulties in capturing the non-linear relationships present in single-cell gene expression data. In contrast, deep learning models are flexible and able to learn complex non-linear mappings.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Heterogeneity:&lt;/strong&gt; Single-cell data is often characterized by diverse cell populations with varying gene expression profiles, presenting a complex landscape. Deep learning models can play a crucial role in identifying, clustering, and characterizing these distinct cell types or subpopulations, thereby facilitating a deeper understanding of cellular heterogeneity within a sample.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;As we explore the reasons behind using deep learning in single-cell sequencing data, it leads us to the question: What deep learning architectures are often used in sc-seq data analysis?&lt;/p&gt;&lt;h3 id="background-on-autoencoders"&gt;Background on Autoencoders&lt;/h3&gt;&lt;p&gt;Autoencoders (AEs) stand out among various deep-learning architectures (such as GANs and RNNs) as an especially relied upon method for decoding the complexities of single-cell sequencing data. &amp;nbsp;Widely employed for dimensionality reduction while preserving the inherent heterogeneity in the single-cell sequencing data. By clustering cells in the reduced-dimensional space generated by autoencoders, researchers can effectively identify and characterize different cell types or subpopulations. This approach enhances our ability to discern and analyze the diverse cellular components within single-cell datasets. In contrast to non-deep learning models, such as principal component analysis (PCA), which are integral components of established scRNA-seq data analysis software like Seurat [10], autoencoders distinguish themselves by uncovering non-linear manifolds. While PCA is constrained to linear transformations, the flexibility of autoencoders to capture complex non-linear mappings makes it an advanced method to find nuanced relationships embedded in single-cell genomics.&lt;/p&gt;&lt;p&gt;To mitigate the overfitting challenge associated with autoencoders, several enhancements to the autoencoder structure have been implemented, specifically tailored to offer advantages in the context of sc-seq data. One notable adaptation often used in the context of sc-seq data is the denoising autoencoder (DAEs), which amplifies the autoencoder's reconstruction capability by introducing noise to the initial network layer. This involves randomly transforming some of its units to zero. The Denoising Autoencoder then reconstructs the input from this intentionally corrupted version, empowering the network to capture more relevant features and preventing it from merely memorizing the input (overfitting). This refinement significantly bolsters the model's resilience against data noise, thereby elevating the quality of the low-dimensional representation of samples (i.e., bottleneck) derived from the sc-seq data.&lt;/p&gt;&lt;p&gt;A third variation of autoencoders frequently employed in sc-seq data analysis is variational autoencoders (VAEs), exemplified by models like scGen [19], scVI [14], scANVI [28], etc. VAEs, as a type of generative model, learn a latent representation distribution of the data. Instead of encoding the data into a vector of p-dimensional latent variables, the data is encoded into two vectors of size p: a vector of means η and a vector of standard deviations σ. VAEs introduce a probabilistic element to the encoding process, facilitating the generation of synthetic single-cell data and offering insights into the diversity within a cell population. This nuanced approach adds another layer of complexity and richness to the exploration of single-cell genomics.&lt;/p&gt;&lt;h2 id="applications-of-deep-learning-in-sc-seq-data-analysis"&gt;Applications of deep learning in sc-seq data analysis&lt;/h2&gt;&lt;p&gt;This section outlines the main applications of deep learning in improving various stages of sc-seq data analysis, highlighting its effectiveness in advancing crucial aspects of the process.&lt;/p&gt;&lt;h3 id="scrna-seq-data-imputation-and-denoising"&gt;scRNA-seq data imputation and denoising&lt;/h3&gt;&lt;p&gt;Single-cell RNA sequencing (scRNA-seq) data encounter inherent challenges, with dropout events being a prominent concern that leads to significant issues—resulting in sparsity within the gene expression matrix, often characterized by a substantial number of zero values. This sparsity significantly shapes downstream bioinformatics analyses. Many of these zero values arise artificially due to deficiencies in sequencing techniques, including problems like inadequate gene expression, low capture rates, sequencing depth, or other technical factors. As a consequence, the observed zero values do not accurately reflect the true underlying expression levels. Hence, not all zeros in scRNA-seq data can be considered mere missing values, deviating from the conventional statistical approach of imputing missing data values. Given the intricate distinction between true and false zero counts, traditional imputation methods with predefined missing values may prove inadequate for scRNA-seq data. For instance, a classical imputation method, like Mean Imputation, might entail substituting these zero values with the average expression level of that gene across all cells. However, this approach runs the risk of oversimplifying the complexities introduced by dropout events in scRNA-seq data, potentially leading to biased interpretations.&lt;/p&gt;&lt;p&gt;ScRNA-seq data imputation methods can be divided into two categories: deep learning–based imputation method and non–deep learning imputation method. The non–deep learning imputation algorithms involve fitting statistical probability models or utilizing the expression matrix for smoothing and diffusion. This simplicity renders it effective for certain types of samples. For example, Wagner et al. [11] utilized the k-nearest neighbors (KNN) method, identifying nearest neighbors between cells and aggregating gene-specific Unique Molecular Identifiers (UMI) counts to impute the gene expression matrix. In contrast, Huang et al. [12] proposed the SVAER algorithm, leveraging gene-to-gene relationships for imputing the gene expression matrix. For larger datasets (comprising tens of thousands or more), high-dimensional, sparse, and complex scRNA-seq data, traditional computational methods face difficulties, often rendering analysis using these methods difficult and infeasible. Consequently, many researchers have turned to designing methods based on deep learning to address these challenges.&lt;/p&gt;&lt;p&gt;Most deep learning algorithms for imputing dropout events are based on autoencoders (AEs). For instance, in 2018, Eraslan et al. [13] introduced the deep count autoencoder (DCA). DCA utilizes a deep autoencoder architecture to address dropout events in single-cell RNA sequencing (scRNA-seq) data. It incorporates a probabilistic layer in the decoder to model the dropout process. This probabilistic layer accommodates the uncertainty associated with dropout events, enabling the model to generate a distribution of possible imputed values. To capture the characteristics of count data in scRNA-seq, DCA models the observed counts as originating from a negative binomial distribution.&lt;/p&gt;&lt;p&gt;Single-cell variational inference (scVI) is another deep learning algorithm introduced by Lopez et al. [14]. ScVI is a probabilistic variational autoencoder (VAE) that combines deep learning and probabilistic modeling to capture the underlying structure of the scRNA-seq data. &amp;nbsp;ScVI can be used for imputation, denoising, and various other tasks related to the analysis of scRNA-seq data. In contrast to the DCA model, scVI employs Zero-Inflated Negative Binomial (ZINB) distribution in the decoder part to generate a distribution of possible counts for each gene in each cell. The Zero-Inflated Negative Binomial (ZINB) distribution allows modeling the probability of a gene expression being zero (to model dropout events) as well as the distribution of positive values (to model non-zero counts).&lt;/p&gt;&lt;p&gt;Additionally, another study addressed the scRNA-seq data imputation challenge by introducing a recurrent network layer in their model, known as scScope [15]. This novel architecture iteratively performs imputations on zero-valued entries of input scRNA-seq data. The flexibility of scScope's design allows for the iterative improvement of imputed outputs through a chosen number of recurrent steps (T). Noteworthy is the fact that reducing the time recurrence of scScope to one (i.e., T = 1) transforms the model into a traditional autoencoder (AE). As scScope is essentially a modification of traditional AEs, its runtime is comparable to other AE-based models.&lt;/p&gt;&lt;p&gt;It's important to note that the application of deep learning in scRNA-seq data imputation and denoising is particularly advantageous due to its ability to capture non-linear relationships among genes. This contrasts with standard linear approaches, making deep learning more adept at providing informed and accurate imputation strategies in the context of single-cell genomics.&lt;/p&gt;&lt;h3 id="batch-effect-removal"&gt;Batch effect removal&lt;/h3&gt;&lt;p&gt;Single-cell data is commonly aggregated from diverse experiments that vary in terms of experimental laboratories, protocols, sample compositions, and even technology platforms. These differences result in significant variations or batch effects within the data, posing a challenge in the analysis of biological variations of interest during the process of data integration. To address this issue, it becomes necessary to correct batch effects by removing technical variance when integrating cells from different batches or studies. The first method that appears for batch correction is a linear method based on linear regression such as Limma package [16] that provides the removeBatchEffect function which fits a linear model that considers the batches and their impact on gene expression. &amp;nbsp;After fitting the model, it sets the coefficients associated with each batch to zero, effectively removing their impact. Another method called ComBat [17] does something similar but adds an extra step to refine the process, making the correction even more accurate by using a technique called empirical Bayes shrinkage.&lt;/p&gt;&lt;p&gt;However, batch effects can be highly nonlinear, making it difficult to correctly align different datasets while preserving key biological variations. In 2018, Haghverdi et al. introduced the Mutual Nearest Neighbors (MNN) algorithm to identify pairs of cells from different batches in single-cell data [18]. These identified mutual nearest neighbors aid in estimating batch effects between batches. By applying this correction, the gene expression values are adjusted to account for the estimated batch effects, aligning them more closely and reducing discrepancies introduced by the different batches. For extensive single-cell datasets with highly nonlinear batch effects, traditional methods may prove less effective, prompting researchers to explore the application of neural networks for improved batch correction.&lt;/p&gt;&lt;p&gt;One of the pioneering models that employ deep learning for batch correction is the scGen model. Developed by Lotfollahi et al., ScGen [19] utilizes a variational autoencoder (VAE) architecture. This involves pre-training a VAE model on a reference dataset to adjust real single-cell data and alleviate batch effects. Initially, the VAE is trained to capture latent features within the reference dataset's cells. Subsequently, this trained VAE is applied to the actual data, producing latent representations for each cell. The adjustment of gene expression profiles is then based on aligning these latent representations, to reduce batch effects and harmonize profiles across different experimental conditions.&lt;br /&gt;&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Deep learning for single-cell sequencing: a microscope to see the diversity of cells" class="kg-image" height="277" src="https://lh7-us.googleusercontent.com/FDIttLe4zUalLYZO1aXTVcBQvPlijO5VVaCmkIoUHYfTbQ0hvOx2iViXlpT14Sj5hmUng9bF63T4NwwvIlKiYYXNTvPo6ArVhqphhq7CLB5Tc75FNTZjCXujHWif8ECGJtOlav-7R1G5ttS8fbRPPQ" width="515" /&gt;&lt;figcaption&gt;&lt;em&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; scGen removes batch effects [19]. a, UMAP visualization of 4 technically diverse pancreatic datasets with their corresponding batch and cell types. b, Data corrected by scGen mixes shared cell types from different studies while preserving the biological variance of cells.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;On the other hand, Zou et al. introduced DeepMNN [20], which employs a residual neural network and the mutual nearest neighbor (MNN) algorithm for scRNA-seq data batch correction. Initially, MNN pairs are identified across batches in a principal component analysis (PCA) subspace. Subsequently, a batch correction network is constructed using two stacked residual blocks to remove batch effects. The loss function of DeepMNN comprises a batch loss, computed based on the distance between cells in MNN pairs in the PCA subspace, and a weighted regularization loss, ensuring the network's output similarity to the input.&lt;/p&gt;&lt;p&gt;The majority of existing scRNA-seq methods are designed to remove batch effects first and then cluster cells, which potentially overlooks certain rare cell types. Recently, Xiaokang et al. developed scDML [21], a deep metric learning model to remove batch effect in scRNA-seq data, guided by the initial clusters and the nearest neighbor information intra and inter-batches. First, the graph-based clustering algorithm is used to group cells based on gene expression similarities, then the KNN algorithm is applied to identify k-nearest neighbors for each cell in the dataset, and the MNN algorithm to identify mutual nearest neighbors, focusing on reciprocal relationships between cells. To remove batch effects, deep triplet learning is employed, considering hard triplets. This helps in learning a low-dimensional embedding that accounts for the original high-dimensional gene expression and removes batch effects simultaneously.&lt;/p&gt;&lt;h3 id="cell-type-annotation"&gt;Cell type annotation&lt;/h3&gt;&lt;p&gt;Cell type annotation in single-cell sequencing involves the process of identifying and labeling individual cells based on their gene expression profiles, which allows researchers to capture the diversity within a heterogeneous population of cells, and understand the cellular composition of tissues, and the functional roles of different cell types in biological processes or diseases. &amp;nbsp;Traditionally, researchers have used manual methods [22] to annotate cell sub-populations. This involves identifying gene markers or gene signatures that are differentially expressed in a specific cell cluster. Once gene markers are identified, researchers manually interpret the biological relevance of these markers to assign cell-type labels to the clusters. This traditional manual annotation approach is time-consuming and requires considerable human effort, especially when dealing with large-scale single-cell datasets. Due to the challenges associated with manual annotation, researchers are turning to automate and streamline the cell annotation process.&lt;/p&gt;&lt;p&gt;Two primary strategies are employed for cell type annotation: unsupervised-based and supervised-based. In the unsupervised realm, clustering methods such as Scanpy [23] and Seurat [10] are utilized, demanding prior knowledge of established cellular markers. The identification of clusters hinges on the unsupervised grouping of cells without external reference information. However, a drawback to this approach is a potential decrease in replicability with an increased number of clusters and multiple selections of cluster marker genes.&lt;/p&gt;&lt;p&gt;Conversely, supervised-based strategies rely on deep-learning models trained on labeled data. These models discern intricate patterns and relationships within gene expression data during training, enabling them to predict cell types for unlabeled data based on acquired patterns. For example, Joint Integration and Discrimination (JIND) [24] &amp;nbsp; deploys a GAN-style deep architecture, where an encoder is pre-trained on classification tasks, circumventing the need for an autoencoder framework. This model also accounts for batch effects. AutoClass [25] integrates an autoencoder and a classifier, combining output reconstruction loss with a classification loss for cell annotation alongside data imputation. Additionally, TransCluster, [26] rooted in the Transformer framework and convolutional neural network (CNN), employs feature extraction from the gene expression matrix for single-cell annotation.&lt;/p&gt;&lt;p&gt;Despite the power of deep neural networks, obtaining a large number of accurately and unbiasedly annotated cells for training is challenging, given the labor-intensive manual inspection of marker genes in scRNAseq data. In response, semi-supervised learning has been leveraged in computational cell annotation. For instance, the SemiRNet [27] model uses both unlabeled and a limited amount of labeled scRNAseq cells to implement cell identification. SemiRNet, based on recurrent convolutional neural networks (RCNN), incorporates a shared network, a supervised network, and an unsupervised network. Furthermore, single‐cell ANnotation using Variational Inference (scANVI) [28], a semi‐supervised variant of scVI [14], maximizes the utility of existing cell state annotations. Cell BLAST, an autoencoder-based generative model, harnesses large-scale reference databases to learn nonlinear low-dimensional representations of cells, employing a sophisticated cell similarity metric—normalized projection distance—to map query cells to specific cell types and identify novel cell types.&lt;/p&gt;&lt;h3 id="multi-omics-data-integration"&gt;Multi-omics Data Integration&lt;/h3&gt;&lt;p&gt;Recent studies have demonstrated the potential of deep learning models in addressing complex and multimodal biological challenges [29]. &amp;nbsp;Among the algorithms proposed thus far, it is primarily deep learning-based models that provide the essential computational adaptability necessary for effectively modeling and incorporating nearly any form of omic data &amp;nbsp;including &amp;nbsp;genomics (studying DNA sequences and genetic variations), epigenomics (examining changes in gene activity unrelated to DNA sequence, such as DNA modifications and chromatin structure), transcriptomics (investigating RNA molecules and gene expression through RNA sequencing), and proteomics (analyzing all proteins produced by an organism, including structures, abundances, and modifications). Deep Learning architectures, including autoencoders (AE) and generative adversarial networks (GAN), have been often used in multi-omics integration problems in single cells. The key question in multi-omics integration revolves around how to effectively represent the diverse multi-omics data within a unified latent space.&lt;/p&gt;&lt;p&gt;One of the early methods developed using Variational Autoencoders (VAE) for the integration of multi-omics single-cell data is known as totalVI [30]. The totalVI model, which is VAE-based, offers a solution for effectively merging scRNA-seq and protein data. In this model, totalVI takes input matrices containing scRNA-seq and protein count data. Specifically, it treats gene expression data as sampled from a negative binomial distribution, while protein data are treated as sampled from a mixture model consisting of two negative binomial distributions. The model first learns shared latent space representations through its encoder, which are then utilized to reconstruct the original data, taking into account the differences between the two original data modalities. Lastly, the decoder component estimates the parameters of the underlying distributions for both data modalities using the shared latent representation.&lt;/p&gt;&lt;p&gt;On the other hand, Zuo et al. [31] introduced scMVAE as a multimodal variational autoencoder designed to integrate transcriptomic and chromatin accessibility data in the same individual cells. scMVAE employs two separate single-modal encoders and two single-modal decoders to effectively model both transcriptomic and chromatin data. It achieves this by combining three distinct joint-learning strategies with a probabilistic Gaussian Mixture Model.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img alt="Deep learning for single-cell sequencing: a microscope to see the diversity of cells" class="kg-image" height="242" src="https://lh7-us.googleusercontent.com/k7QSkoQ1aPR921c2cqbKPYQ0HhguVnR2Xi8lrd9VCD38PFEifKNFU94Sb_IJR1DZl5zio9HKiePtlW9KrQD91cV_oiN4DLAzXt7I2UC_ETdfSPKKJ913D_U3_kof9NzBjEBlVVpUUs08YLTjt7fW3w" width="669" /&gt;&lt;figcaption&gt;&lt;em&gt;&lt;strong&gt;Figure 5 .&lt;/strong&gt; UMAP embedding for the latent space of the MULTIGRATE for CITE-seq dataset combines gene expression and cell surface protein data [32].&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Recently, Lotfollahi et al. [32] introduced an unsupervised deep generative model known as MULTIGRATE for the integration of multi-omic datasets. MULTIGRATE employs a multi-modal variational autoencoder structure that shares some similarities with the scMVAE model. However, it offers added generality and the capability to integrate both paired and unpaired single-cell data. To enhance cell alignment, the loss function incorporates Maximum Mean Discrepancy (MMD), penalizing any misalignment between the point clouds associated with different assays. Incorporating transfer learning, MULTIGRATE can map new multi-omic query datasets into a reference atlas and also perform imputations for missing modalities.&lt;/p&gt;&lt;h2 id="conclusion"&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;The application of deep learning in single-cell sequencing functions as an advanced microscope, revealing intricate insights within individual cells and providing a profound understanding of cellular heterogeneity and complexity in biological systems. This cutting-edge technology empowers scientists to explore previously undiscovered aspects of cellular behavior. However, the challenge lies in choosing between traditional tools and the plethora of available deep-learning options. The landscape of tools is vast, and researchers must carefully consider factors such as data type, complexity, and the specific biological questions at hand. Navigating this decision-making process requires a thoughtful evaluation of the strengths and limitations of each tool in relation to research goals.&lt;/p&gt;&lt;p&gt;On the other hand, a critical need in the development of deep learning approaches for single-cell RNA sequencing (scRNA-seq) analysis is robust benchmarking. While many studies compare deep learning performance to standard methods, there is a lack of comprehensive comparisons across various deep learning models. Moreover, methods often claim superiority based on specific datasets and tissues (e.g., pancreas cells, immune cells), making it challenging to evaluate the necessity of specific terms or preprocessing steps. Addressing these challenges requires an understanding of when deep learning models fail and their limitations. Recognizing which types of deep learning approaches and model structures are beneficial in specific cases is crucial for developing new approaches and guiding the field.&lt;/p&gt;&lt;p&gt;In the realm of multi-omics single-cell integration, most deep learning methods aim to find a shared latent representation for all modalities. However, shared representation learning faces challenges such as heightened noise, sparsity, and the intricate task of balancing modalities. Inherent biases across institutions complicate generalization. Despite being less prevalent than single-modality approaches, integrating diverse modalities with unique cell populations is crucial. Objectives include predicting expression across modalities and identifying cells in similar states. Despite advancements, further efforts are essential for enhanced performance, particularly concerning unique or rare cell populations present in one technology but not the other.&lt;/p&gt;&lt;hr /&gt;&lt;h2 id="author-bio"&gt;Author Bio&lt;/h2&gt;&lt;p&gt;Fatima Zahra El Hajji holds a master's degree in bioinformatics from the National School of Computer Science and Systems Analysis &amp;nbsp;(ENSIAS), she subsequently worked as an AI intern at Piercing Star Technologies. Currently, she is a Ph.D. student at the University Mohammed VI Polytechnic (UM6P), working under the supervision of Dr. Rachid El Fatimy and &amp;nbsp;Dr. Tariq Daouda. Her research focuses on the application of deep learning techniques in single-cell sequencing data.&lt;/p&gt;&lt;h2 id="citation"&gt;Citation&lt;br /&gt;&lt;/h2&gt;&lt;p&gt;For attribution in academic contexts or books, please cite this work as&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Fatima Zahra El Hajji, "Deep learning for single-cell sequencing: a microscope to see the diversity of cells", The Gradient, 2024.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;BibTeX citation:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;@article{elhajji2023nar,
    author = {El Hajji, Fatima Zahra},
    title = {Deep learning for single-cell sequencing: a microscope to see the diversity of cells},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/deep-learning-for-single-cell-sequencing-a-microscope-to-uncover-the-rich-diversity-of-individual-cells},
}&lt;/code&gt;&lt;/pre&gt;&lt;h2 id="references"&gt;References&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;National Human Genome Research Institute (NHGRI) : A Brief Guide to Genomics ,&lt;/em&gt;&lt;em&gt; https://www.genome.gov/about-genomics/fact-sheets/A-Brief-Guide-to-Genomics&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Method of the Year 2013. Nat Methods 11, 1 (2014).&lt;/em&gt;&lt;em&gt; https://doi.org/10.1038/nmeth.2801&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Zappia, L., Theis, F.J. Over 1000 tools reveal trends in the single-cell RNA-seq analysis landscape. Genome Biol 22, 301 (2021).&lt;/em&gt;&lt;em&gt; https://doi.org/10.1186/s13059-021-02519-4&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Collins FS, Fink L. The Human Genome Project. Alcohol Health Res World. 1995;19(3):190-195. PMID: 31798046; PMCID: PMC6875757.&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Tang F, Barbacioru C, Wang Y, et al. mRNA-Seq whole-transcriptome analysis of a single cell. Nat Methods. 2009; 6: 377-382.&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Eisenstein, M. The secret life of cells. Nat Methods 17, 7–10 (2020). https://doi.org/10.1038/s41592-019-0698-y&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Method of the Year 2019: Single-cell multimodal omics. Nat Methods 17, 1 (2020). https://doi.org/10.1038/s41592-019-0703-5&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Zhao, Jing et al. “Multi-view learning overview: Recent progress and new challenges.” Inf. Fusion 38 (2017): 43-54.&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Zhu, J., Shang, L. &amp;amp; Zhou, X. SRTsim: spatial pattern preserving simulations for spatially resolved transcriptomics. Genome Biol 24, 39 (2023).&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Butler, A., Hoffman, P., Smibert, P., Papalexi, E., &amp;amp; Satija, R. (2018). Integrating single-cell transcriptomic data across different conditions, technologies, and species. Nature biotechnology, 36(5), 411-420&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;Wagner, F., Yan, Y., &amp;amp; Yanai, I. (2018). K-nearest neighbor smoothing for high-throughput single-cell RNA-Seq data. bioRxiv, 217737. Cold Spring Harbor Laboratory. https://doi.org/10.1101/217737&lt;/li&gt;&lt;li&gt;Huang, M., Wang, J., Torre, E. et al. SAVER: gene expression recovery for single-cell RNA sequencing. Nat Methods 15, 539–542 (2018). https://doi.org/10.1038/s41592-018-0033-z&lt;/li&gt;&lt;li&gt;Eraslan G, Simon LM, Mircea M, Mueller NS, Theis FJ. Single-cell RNA-seq denoising using a deep count autoencoder. Nat Commun. 2019 Jan 23;10(1):390. doi: 10.1038/s41467-018-07931-2. PMID: 30674886; PMCID: PMC6344535.&lt;/li&gt;&lt;li&gt;Lopez, R., Regier, J., Cole, M. B., Jordan, M. I.,&amp;amp; Yosef, N. (2018). Deep generative modeling for single-cell transcriptomics. Nature methods, 15(12), 1053-1058.&lt;/li&gt;&lt;li&gt;Y. Deng, F. Bao, Q. Dai, L.F. Wu, S.J. Altschuler Scalable analysis of cell-type composition from single-cell transcriptomics using deep recurrent learning&lt;/li&gt;&lt;li&gt;Ritchie ME, Phipson B, Wu D, Hu Y, Law CW, Shi W, Smyth GK. limma powers differential expression analyses for RNA-sequencing and microarray studies. Nucleic Acids Res. 2015 Apr 20;43(7):e47. doi: 10.1093/nar/gkv007. Epub 2015 Jan 20. PMID: 25605792; PMCID: PMC4402510.&lt;/li&gt;&lt;li&gt;Johnson W.E. , Li C., Rabinovic A. Adjusting batch effects in microarray expression data using empirical bayes methods. Biostatistics. 2007; 8:118–127.&lt;/li&gt;&lt;li&gt;Haghverdi, L., Lun, A., Morgan, M. et al. Batch effects in single-cell RNA-sequencing data are corrected by matching mutual nearest neighbors. Nat Biotechnol 36, 421–427 (2018). https://doi.org/10.1038/nbt.4091&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Lotfollahi, M., Wolf, F. A., &amp;amp; Theis, F. J. (2019). scGen predicts single-cell perturbation responses. Nature methods, 16(8), 715-721.&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Zou, B., Zhang, T., Zhou, R., Jiang, X., Yang, H., Jin, X., &amp;amp; Bai, Y. (2021). deepMNN: deep learning-based single-cell RNA sequencing data batch correction using mutual nearest neighbors. Frontiers in Genetics, 1441.&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Yu, X., Xu, X., Zhang, J. et al. Batch alignment of single-cell transcriptomics data using deep metric learning. Nat Commun 14, 960 (2023). https://doi.org/10.1038/s41467-023-36635-5&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Z.A. Clarke, T.S. Andrews, J. Atif, D. Pouyabahar, B.T. Innes, S.A. MacParland, et al. Tutorial: guidelines for annotating single-cell transcriptomic maps using automated and manual methods Nat Protoc, 16 (2021), pp. 2749-2764&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Wolf, F., Angerer, P. &amp;amp; Theis, F. SCANPY: large-scale single-cell gene expression data analysis. Genome Biol 19, 15 (2018). https://doi.org/10.1186/s13059-017-1382-0&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Mohit Goyal, Guillermo Serrano, Josepmaria Argemi, Ilan Shomorony, Mikel Hernaez, Idoia Ochoa, JIND: joint integration and discrimination for automated single-cell annotation, Bioinformatics, Volume 38, Issue 9, March 2022, Pages 2488–2495, https://doi.org/10.1093/bioinformatics/btac140&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;H. Li, C.R. Brouwer, W. Luo A universal deep neural network for in-depth cleaning of single-cell RNA-seq data Nat Commun, 13 (2022), p. 1901&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Song T, Dai H, Wang S, Wang G, Zhang X, Zhang Y and Jiao L (2022) TransCluster: A Cell-Type Identification Method for single-cell RNA-Seq data using deep learning based on transformer. Front. Genet. 13:1038919. doi: 10.3389/fgene.2022.1038919&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Dong X, Chowdhury S, Victor U, Li X, Qian L. Semi-Supervised Deep Learning for Cell Type Identification From Single-Cell Transcriptomic Data. IEEE/ACM Trans Comput Biol Bioinform. 2023 Mar-Apr;20(2):1492-1505. doi: 10.1109/TCBB.2022.3173587. Epub 2023 Apr 3. PMID: 35536811.&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Xu, C., Lopez, R., Mehlman, E., Regier, J., Jordan, M. I., &amp;amp; Yosef, N. (2021). Probabilistic harmonization and annotation of single‐cell transcriptomics data with deep generative models. Molecular Systems Biology, 17(1), e9620. https://doi.org/10.15252/msb.20209620&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Tasbiraha Athaya, Rony Chowdhury Ripan, Xiaoman Li, Haiyan Hu, Multimodal deep learning approaches for single-cell multi-omics data integration, Briefings in Bioinformatics, Volume 24, Issue 5, September 2023, bbad313, &lt;/em&gt;&lt;em&gt;https://doi.org/10.1093/bib/bbad313&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;Gayoso, A., Lopez, R., Steier, Z., Regier, J., Streets, A., &amp;amp; Yosef, N. (2019). A Joint Model of RNA Expression and Surface Protein Abundance in Single Cells. bioRxiv, 791947. &lt;/em&gt;&lt;em&gt;https://www.biorxiv.org/content/early/2019/10/07/791947.abstract&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;Chunman Zuo, Luonan Chen. Deep-joint-learning analysis model of single cell transcriptome and open chromatin accessibility data. Briefings in Bioinformatics. 2020.&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;Lotfollahi, M., Litinetskaya, A., &amp;amp; Theis, F. J. (2022). Multigrate: single-cell multi-omic data integration.bioRxiv.&lt;/em&gt;&lt;em&gt;https://www.biorxiv.org/content/early/2022/03/17/2022.03.16.484643&lt;/em&gt;&lt;/li&gt;&lt;/ol&gt;]]&amp;gt;&amp;lt;![CDATA[Salmon in the Loop]]&amp;gt;https://thegradient.pub/salmon-in-the-loop/64dfbfba93571d5c8c15419aSat, 16 Dec 2023 17:00:36 GMT&lt;!--&amp;lt;![CDATA[&lt;img src="https://thegradient.pub/content/images/2023/08/DALL-E-2023-08-21-18.52.00---frothing-waves-in-the-middle-of-the-spillway-of-a-hydroelectric-dam--shown-by-a-bridge-across-a-river-with-many-arches--with-a-fish-jumping-downstream.png" alt="Salmon in the Loop"&gt;&lt;p&gt;One of the most fascinating problems that a computer scientist may be lucky enough to encounter is a complex sociotechnical problem in a field going through the process of digital transformation. For me, that was fish counting. Recently, I worked as a consultant in a subdomain of environmental science focused on counting fish that pass through large hydroelectric dams. Through this overarching project, I learned about ways to coordinate and manage human-in-the-loop dataset production, as well as the complexities and vagaries of how to think about and share progress with stakeholders.&lt;/p&gt;&lt;h2 id="background"&gt;Background&lt;/h2&gt;&lt;p&gt;Let&amp;#x2019;s set the stage. Large hydroelectric dams are subject to Environmental Protection Act regulations through the Federal Energy Regulatory Commission (FERC). FERC is an independent agency of the United States government that regulates the transmission and wholesale sale of electricity across the United States. The commission has jurisdiction over a wide range of electric power activities and is responsible for issuing licenses and permits for the construction and operation of hydroelectric facilities, including dams. These licenses and permits ensure that hydroelectric facilities are safe and reliable, and that they do not have a negative impact on the environment or other stakeholders. In order to obtain a license or permit from FERC, hydroelectric dam operators must submit detailed plans and studies demonstrating that their facility meets regulations. This process typically involves extensive review and consultation with other agencies and stakeholders. If a hydroelectric facility is found to be in violation of any set standards, FERC is responsible for enforcing compliance with all applicable regulations via sanctions, fines, or lease termination--resulting in a loss of the right to generate power.&lt;/p&gt;&lt;p&gt;Hydroelectric dams are essentially giant batteries. They generate power by building up a large reservoir of water on one side and directing that water through turbines in the body of the dam. Typically, a hydroelectric dam requires lots of space to store water on one side of it, which means they tend to be located away from population centers. The conversion process from potential to kinetic energy generates large amounts of electricity, and the amount of pressure and force generated is disruptive to anything that lives in or moves through the waterways&amp;#x2014;especially fish.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img src="https://lh3.googleusercontent.com/3s1Nf_YKJJ6RYku2imhRa7EpXZkbB5fb6R2_EG5shYdVdyRXjVvXZaDq4soTbdbh0TJ-7ch4I4DJ59Fs94dJaJCwnJQCYdLSl9ehw_-92xerbFob67m2N8A8a5NSrxROJS2hN4HNNp0DhYd69-AEGYk" class="kg-image" alt="Salmon in the Loop" loading="lazy" width="429" height="248"&gt;&lt;figcaption&gt;&lt;em&gt;Simple diagram illustrating how hydroelectric power is generated (Tennessee Valley Authority)&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;It is also worth noting that the waterways were likely disrupted substantially when the dam was built, leading to behavioral or population-level changes in the fish species of the area. This is of great concern to the Pacific Northwest in particular, as hydropower is the predominant power generation means for the region (Bonneville Power Administration). Fish populations are constantly moving upstream and downstream and hydropower dams can act as barriers that block their passage, leading to reduced spawning. In light of the risks to fish, hydropower dams are subject to constraints on the amount of power they can generate and must show that they are not killing fish in large numbers or otherwise disrupting the rhythms of their lives, especially because the native salmonid species of the region are already threatened or endangered (Salmon Status). &lt;/p&gt;&lt;p&gt;To demonstrate compliance with FERC regulations, large hydroelectric dams are required to routinely produce data which shows that their operational activities do not interfere with endangered fish populations in aggregate. Typically, this is done by performing fish passage studies. A fish passage study can be conducted many different ways, but boils down to one primary dataset upon which everything is based: a fish count. Fish are counted as they pass through the hydroelectric dam, using structures like fish ladders to make their way from the reservoir side to the stream side.&lt;/p&gt;&lt;figure class="kg-card kg-image-card kg-card-hascaption"&gt;&lt;img src="https://lh6.googleusercontent.com/c3W3UHWOkSjkN4Yq2vbtCvEklUA6l5rPgIw53Y15-kt1rqeldlSByI45ruUdsfhy60MTEGWJJ7fPu0YuLMyXLDBIUFkPEYlGIzAipRu7qYGH0OFkrpQL3d3YTNlcqSBiW6y7VHpDCkYpqCXgXstn4Gs" class="kg-image" alt="Salmon in the Loop" loading="lazy" width="594" height="390"&gt;&lt;figcaption&gt;&lt;em&gt;A fish ladder at John Day Dam, how fish often ascend and pass through a dam (Delgado)&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Fish counts can be conducted visually&amp;#x2014;-a person trained in fish identification watches the fish pass, incrementing the count as they move upstream. As a fish is counted, observers impart additional classifications outside of species of fish, such as whether there was some kind of obvious illness or injury, if the fish is hatchery-origin or wild, and so on. These differences between fish are subtle and require close monitoring and verification, since the attribute in question (a clipped adipose fin, a scratched midsection) may only be visible briefly when the fish swims by. As such, fish counting is a specialized job that requires expertise in identifying and classifying different species of fish, as well as knowledge of their life stages and other characteristics. The job is physically demanding, as it typically involves working in remote locations away from city centers, and it can be challenging to perform accurately under the difficult environmental conditions found at hydroelectric dams&amp;#x2013;poor lighting, unregulated temperatures, and other circumstances inhospitable to humans.&lt;/p&gt;&lt;p&gt;These modes of data collection are great, but there are varying degrees of error that could be imparted through their recording. For example, some visual fish counts are documented with pen and paper, leading to incorrect counts through transcription error; or there can be disputes about the classification of a particular species. Different dam operators collect fish counts with varying degrees of granularity (some collect hourly, some daily, some monthly) and seasonality (some collect only during certain migration patterns called &amp;#x201C;runs&amp;#x201D;). After collection and validation, organizations correlate this data with operational information produced by the dam in an attempt to see if any activities of the dam have an adverse or beneficial effect on fish populations. Capturing these data piecemeal with different governing standards and levels of detail causes organizations to look for new efficiencies enabled by technology.&lt;/p&gt;&lt;h2 id="enter-computer-vision"&gt;Enter Computer Vision&lt;/h2&gt;&lt;p&gt;Some organizations are exploring the use of computer vision and machine learning to significantly automate fish counting. Since dam operators subje--&gt;</content:encoded><guid isPermaLink="false">https://thegradient.pub/rss/</guid></item><item><title>Don’t miss your chance to exhibit at TechCrunch Disrupt 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/21/dont-miss-your-chance-to-exhibit-at-techcrunch-disrupt-2025/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; is just around the corner, and with more than 10,000 startup and VC leaders heading to Moscone West in San Francisco this October 27 to 29, the Expo Hall is where connections get made and business gets done. If you’ve been thinking about showcasing your company, consider this your nudge — exhibitor spots are filling fast, and once they’re gone, they’re gone.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Exhibiting at Disrupt isn’t just about having a table — it’s about putting your startup in front of the people who matter most. Whether you’re looking to connect with investors, spark media coverage, recruit top talent, or meet early customers, Disrupt gives you the visibility to do it all in one place.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="TechCrunch Disrupt 2024 exhibitor Google" class="wp-image-2979874" height="454" src="https://techcrunch.com/wp-content/uploads/2025/01/Google-Exhibit-Disrupt-2025.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;TechCrunch Disrupt 2024 exhibitor Google Cloud. October 28-30, 2024, at Moscone West, San Francisco, CA.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Brazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-what-s-included-in-your-exhibitor-package"&gt;&lt;strong&gt;What’s included in your exhibitor package&lt;/strong&gt;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Here’s what you get when you exhibit at Disrupt 2025:&lt;/p&gt;







&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;6’ x 30″ exhibit table with linen and two chairs&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;11” x 14” tabletop sign with your startup’s branding&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Lead generation via the TechCrunch Disrupt app&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Complimentary partner Wi-Fi access&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Silver Tier sponsor branding&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Entry-level listing in the sponsor directory&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;One Founder Pass (additional founder perks)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;10 passes for your team and guests (5 Expo+, 5 General Admission)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;50% off additional General Admission Passes&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Access to the TechCrunch Disrupt press list&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Logo on “thank you” slide during breaks&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Company logo and description on the event partner page&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Logo and profile in TechCrunch Disrupt mobile app&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Company name in select TechCrunch sponsor announcements&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Acknowledgment during the closing ceremony&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 class="wp-block-heading has-h-3-font-size" id="h-make-your-startup-impossible-to-ignore"&gt;Make your startup impossible to ignore&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;This year’s Expo Hall will feature hundreds of startups, industry leaders, and breakout technologies. Your company could be right in the mix, meeting buyers, building a pipeline, and getting your product in front of the right audience at the right time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ready to go from stealth mode to the show floor? Secure your &lt;strong&gt;exhibit table&lt;/strong&gt; before they sell out. Space is limited, and tables are moving fast.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Become an exhibitor at TechCrunch Disrupt 2025&lt;/strong&gt;!&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; is just around the corner, and with more than 10,000 startup and VC leaders heading to Moscone West in San Francisco this October 27 to 29, the Expo Hall is where connections get made and business gets done. If you’ve been thinking about showcasing your company, consider this your nudge — exhibitor spots are filling fast, and once they’re gone, they’re gone.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Exhibiting at Disrupt isn’t just about having a table — it’s about putting your startup in front of the people who matter most. Whether you’re looking to connect with investors, spark media coverage, recruit top talent, or meet early customers, Disrupt gives you the visibility to do it all in one place.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="TechCrunch Disrupt 2024 exhibitor Google" class="wp-image-2979874" height="454" src="https://techcrunch.com/wp-content/uploads/2025/01/Google-Exhibit-Disrupt-2025.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;TechCrunch Disrupt 2024 exhibitor Google Cloud. October 28-30, 2024, at Moscone West, San Francisco, CA.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Brazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-what-s-included-in-your-exhibitor-package"&gt;&lt;strong&gt;What’s included in your exhibitor package&lt;/strong&gt;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Here’s what you get when you exhibit at Disrupt 2025:&lt;/p&gt;







&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;6’ x 30″ exhibit table with linen and two chairs&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;11” x 14” tabletop sign with your startup’s branding&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Lead generation via the TechCrunch Disrupt app&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Complimentary partner Wi-Fi access&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Silver Tier sponsor branding&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Entry-level listing in the sponsor directory&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;One Founder Pass (additional founder perks)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;10 passes for your team and guests (5 Expo+, 5 General Admission)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;50% off additional General Admission Passes&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Access to the TechCrunch Disrupt press list&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Logo on “thank you” slide during breaks&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Company logo and description on the event partner page&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Logo and profile in TechCrunch Disrupt mobile app&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Company name in select TechCrunch sponsor announcements&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Acknowledgment during the closing ceremony&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 class="wp-block-heading has-h-3-font-size" id="h-make-your-startup-impossible-to-ignore"&gt;Make your startup impossible to ignore&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;This year’s Expo Hall will feature hundreds of startups, industry leaders, and breakout technologies. Your company could be right in the mix, meeting buyers, building a pipeline, and getting your product in front of the right audience at the right time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ready to go from stealth mode to the show floor? Secure your &lt;strong&gt;exhibit table&lt;/strong&gt; before they sell out. Space is limited, and tables are moving fast.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Become an exhibitor at TechCrunch Disrupt 2025&lt;/strong&gt;!&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/21/dont-miss-your-chance-to-exhibit-at-techcrunch-disrupt-2025/</guid><pubDate>Mon, 21 Jul 2025 14:00:00 +0000</pubDate></item><item><title>Exploring the context of online images with Backstory (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/exploring-the-context-of-online-images-with-backstory/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://lh3.googleusercontent.com/Sx0q14oEP6Ir8EEX0E4vyJGUFAAhycWqppr5x-2edL8Dz0OwB7ylqPIkRd9TefuQ9TaYxe-1aFMmmdH3aSJeFJ09YrqA16V2V-Dv7M0XJcbLyzZsvWE=w1200-h630-n-nu" /&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;We would like to thank Zoubin Ghahramani, Helen King, Rahul Sukthankar, Raia Hadsell, and Chandu Thota for their leadership and support.&lt;/p&gt;&lt;p&gt;This work was done thanks to the contributions of Mevan Babakar, Hannah Forbes-Pollard, Nikki Hariri, Thomas Leung, Nick Dufour, Ben Usman, Min Ma, Steve Pucci, Spudde Childs, Kate Harrison, Alanna Slocum, Reza Aghajani, Sri Rajendran, Alexey Vorobyov, Ashley Eden, Rishub Jain, Stephanie Chan, Sophie Bridgers, Michiel Bakker, Sures Kumar Thoddu Srinivasan, Tesh Goyal, and Ashish Chaudhary.&lt;/p&gt;&lt;p&gt;We would also like to thank Kent Walker, Camino Rojo, Clement Wolf, J.D. Velazquez, Tom Lue, Ndidi Elue, Rachel Stigler, M.H. Tessler, Ricardo Prada, William Isaac, Tom Stepleton, Zoe Darme, Gail Kent, Vincent Ryan, Aaron Donsbach, Abhishek Bapna, Verena Rieser, Christian Plagemann, Anca Dragan, Sven Gowal, Florian Stimberg, Christopher Savcak, Allison Garcia, Eve Novakovic, Armin Senoner, Arielle Bier, and the greater Google DeepMind and Google teams for their support, help, and feedback.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://lh3.googleusercontent.com/Sx0q14oEP6Ir8EEX0E4vyJGUFAAhycWqppr5x-2edL8Dz0OwB7ylqPIkRd9TefuQ9TaYxe-1aFMmmdH3aSJeFJ09YrqA16V2V-Dv7M0XJcbLyzZsvWE=w1200-h630-n-nu" /&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;We would like to thank Zoubin Ghahramani, Helen King, Rahul Sukthankar, Raia Hadsell, and Chandu Thota for their leadership and support.&lt;/p&gt;&lt;p&gt;This work was done thanks to the contributions of Mevan Babakar, Hannah Forbes-Pollard, Nikki Hariri, Thomas Leung, Nick Dufour, Ben Usman, Min Ma, Steve Pucci, Spudde Childs, Kate Harrison, Alanna Slocum, Reza Aghajani, Sri Rajendran, Alexey Vorobyov, Ashley Eden, Rishub Jain, Stephanie Chan, Sophie Bridgers, Michiel Bakker, Sures Kumar Thoddu Srinivasan, Tesh Goyal, and Ashish Chaudhary.&lt;/p&gt;&lt;p&gt;We would also like to thank Kent Walker, Camino Rojo, Clement Wolf, J.D. Velazquez, Tom Lue, Ndidi Elue, Rachel Stigler, M.H. Tessler, Ricardo Prada, William Isaac, Tom Stepleton, Zoe Darme, Gail Kent, Vincent Ryan, Aaron Donsbach, Abhishek Bapna, Verena Rieser, Christian Plagemann, Anca Dragan, Sven Gowal, Florian Stimberg, Christopher Savcak, Allison Garcia, Eve Novakovic, Armin Senoner, Arielle Bier, and the greater Google DeepMind and Google teams for their support, help, and feedback.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/exploring-the-context-of-online-images-with-backstory/</guid><pubDate>Mon, 21 Jul 2025 15:00:00 +0000</pubDate></item><item><title>Anduril alums raise $24M Series A to bring military logistics out of the Excel spreadsheet era (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/21/anduril-alums-raise-24m-series-a-to-bring-military-logistics-out-of-the-excel-spreadsheet-era/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/Rune-Image-2.png?resize=1200,848" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Silicon Valley is doubling down on defense as geopolitical tensions rise and appetite for modernizing warfare grows. And while many of the startups garnering large valuations are focused on hardware and weaponry — think Anduril, Shield AI, and Skydio — Rune Technologies wants to tackle AI-enabled software for military logistics.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The U.S. military runs on Excel spreadsheets and whiteboards and manual processes right now to execute logistics operations,” co-founder David Tuttle told TechCrunch. “Logistics is never the sexiest part of the military. The technology industry emphasis is on how do we make things go boom? How do we build great weapons systems?”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Logistics, Tuttle says, usually falls behind when it comes to innovation. And he should know. Earlier in his career, he was a field artillery officer in the U.S. Army. Later, he served with the Joint Special Operations Command before going on to work at Anduril, where he met his co-founder, ex-Meta software engineer Peter Goldsborough. The two founded Rune after seeing how much modern warfare has changed the scale and pace at which armies have to sustain force.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Ukraine is a sad example of the expenditures of munitions, the consumption of supplies, and those types of things in a near-peer adversary conflict — they will break human-centric and analog-centric processes,” said Tuttle.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rune’s flagship product TyrOS promises to transform manual logistics processes into intelligent supply webs that predict future needs, optimize current resources, and enable distributed operations — even from a disconnected laptop in the middle of the jungle.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup has just raised a $24 million Series A round off the back of pilot deployments under the U.S. Army and U.S. Marine Corps. The round — which Human Capital led with participation from Pax VC, Washington Harbour Partners, a16z, Point72 Ventures, XYZ Venture Capital, and Forward Deployed VC — will go toward expanding TyrOS deployment into other U.S. military services.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TyrOS has two major selling points. The first is its technical capabilities as a mission command system for logistics. TyrOS relies on deep learning models, including time series models, to forecast supply and demand assets like personnel, transportation, equipment, food, and other resources based on hundreds of environmental and supply variables.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“A logistician thinks about not just, ‘What do I have on hand from supplies?’ but also, ‘What vehicles do I have to move that?’” said Tuttle. “’What qualified crews do I have to drive that vehicle?’ ‘What routes is that vehicle going to go over?’ ‘And is that threat-informed?’ ‘Is a bridge blown up on the route that we need to reroute around?’”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tuttle says the team at Rune, two-thirds of which are veterans, is also working to integrate generative AI into TyrOS for “course of action generation,” enabling the system to digest massive datasets in real-time battle space environments so that logisticians and commanders can query it on the fly. And while LLMs have advanced rapidly, TyrOS still relies on traditional mathematical optimization for certain tasks — like planning aircraft loads based on cubic volume and other constraints — where precise calculations are essential.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TyrOS’s second major hook is its edge-first architecture that bypasses the need for constant connectivity to remote servers, allowing the system to operate independently and synchronize when communications are reestablished. In other words, TyrOS is “cloud-capable, but not cloud-required.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Building software today from a cloud environment standpoint is very different architecturally than if I’m building software to run literally on this laptop in the jungle in the Philippines with Marines or soldiers,” Tuttle said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TyrOS is also cloud- and hardware-agnostic; it can run on program-of-record hardware server stacks that the military uses today for ease of integration, per Tuttle.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The co-founder noted that Rune’s backers include executives at both Palantir and Anduril, where he sees plenty of partnership opportunities. Rune was recently selected for the Palantir Startup Fellowship and announced its integration earlier this year with Palantir’s Defense OSDK (Ontology Software Development Kit) to enable automated logistics, from the tactical edge to the strategic layer.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Automating the gap between tactical-level intelligence and strategic decision-making is Rune’s long-term vision.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I’m not just worried about sustaining this for the next 30 or 60 days,” Tuttle said. “I’m worried about how this might impact production decisions back in the defense industrial base. That’s the vision we want to get up to. How do you drive tactical level data all the way up to the operational level, to the strategic level, to potentially drive the production of artillery shells?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Clarification: Peter Goldsborough volunteers under the U.S. Marine Corps Cyber Auxiliary.&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at&amp;nbsp;rebecca.bellan@techcrunch.com&amp;nbsp;and Maxwell Zeff at&amp;nbsp;maxwell.zeff@techcrunch.com. For secure communication, you can contact us via Signal at&amp;nbsp;@rebeccabellan.491 and&amp;nbsp;@mzeff.88.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/Rune-Image-2.png?resize=1200,848" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Silicon Valley is doubling down on defense as geopolitical tensions rise and appetite for modernizing warfare grows. And while many of the startups garnering large valuations are focused on hardware and weaponry — think Anduril, Shield AI, and Skydio — Rune Technologies wants to tackle AI-enabled software for military logistics.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The U.S. military runs on Excel spreadsheets and whiteboards and manual processes right now to execute logistics operations,” co-founder David Tuttle told TechCrunch. “Logistics is never the sexiest part of the military. The technology industry emphasis is on how do we make things go boom? How do we build great weapons systems?”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Logistics, Tuttle says, usually falls behind when it comes to innovation. And he should know. Earlier in his career, he was a field artillery officer in the U.S. Army. Later, he served with the Joint Special Operations Command before going on to work at Anduril, where he met his co-founder, ex-Meta software engineer Peter Goldsborough. The two founded Rune after seeing how much modern warfare has changed the scale and pace at which armies have to sustain force.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Ukraine is a sad example of the expenditures of munitions, the consumption of supplies, and those types of things in a near-peer adversary conflict — they will break human-centric and analog-centric processes,” said Tuttle.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rune’s flagship product TyrOS promises to transform manual logistics processes into intelligent supply webs that predict future needs, optimize current resources, and enable distributed operations — even from a disconnected laptop in the middle of the jungle.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup has just raised a $24 million Series A round off the back of pilot deployments under the U.S. Army and U.S. Marine Corps. The round — which Human Capital led with participation from Pax VC, Washington Harbour Partners, a16z, Point72 Ventures, XYZ Venture Capital, and Forward Deployed VC — will go toward expanding TyrOS deployment into other U.S. military services.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TyrOS has two major selling points. The first is its technical capabilities as a mission command system for logistics. TyrOS relies on deep learning models, including time series models, to forecast supply and demand assets like personnel, transportation, equipment, food, and other resources based on hundreds of environmental and supply variables.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“A logistician thinks about not just, ‘What do I have on hand from supplies?’ but also, ‘What vehicles do I have to move that?’” said Tuttle. “’What qualified crews do I have to drive that vehicle?’ ‘What routes is that vehicle going to go over?’ ‘And is that threat-informed?’ ‘Is a bridge blown up on the route that we need to reroute around?’”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tuttle says the team at Rune, two-thirds of which are veterans, is also working to integrate generative AI into TyrOS for “course of action generation,” enabling the system to digest massive datasets in real-time battle space environments so that logisticians and commanders can query it on the fly. And while LLMs have advanced rapidly, TyrOS still relies on traditional mathematical optimization for certain tasks — like planning aircraft loads based on cubic volume and other constraints — where precise calculations are essential.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TyrOS’s second major hook is its edge-first architecture that bypasses the need for constant connectivity to remote servers, allowing the system to operate independently and synchronize when communications are reestablished. In other words, TyrOS is “cloud-capable, but not cloud-required.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Building software today from a cloud environment standpoint is very different architecturally than if I’m building software to run literally on this laptop in the jungle in the Philippines with Marines or soldiers,” Tuttle said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TyrOS is also cloud- and hardware-agnostic; it can run on program-of-record hardware server stacks that the military uses today for ease of integration, per Tuttle.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The co-founder noted that Rune’s backers include executives at both Palantir and Anduril, where he sees plenty of partnership opportunities. Rune was recently selected for the Palantir Startup Fellowship and announced its integration earlier this year with Palantir’s Defense OSDK (Ontology Software Development Kit) to enable automated logistics, from the tactical edge to the strategic layer.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Automating the gap between tactical-level intelligence and strategic decision-making is Rune’s long-term vision.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I’m not just worried about sustaining this for the next 30 or 60 days,” Tuttle said. “I’m worried about how this might impact production decisions back in the defense industrial base. That’s the vision we want to get up to. How do you drive tactical level data all the way up to the operational level, to the strategic level, to potentially drive the production of artillery shells?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Clarification: Peter Goldsborough volunteers under the U.S. Marine Corps Cyber Auxiliary.&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at&amp;nbsp;rebecca.bellan@techcrunch.com&amp;nbsp;and Maxwell Zeff at&amp;nbsp;maxwell.zeff@techcrunch.com. For secure communication, you can contact us via Signal at&amp;nbsp;@rebeccabellan.491 and&amp;nbsp;@mzeff.88.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/21/anduril-alums-raise-24m-series-a-to-bring-military-logistics-out-of-the-excel-spreadsheet-era/</guid><pubDate>Mon, 21 Jul 2025 15:00:15 +0000</pubDate></item><item><title>Grok’s AI companions drove downloads, but its latest model is the one making money (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/21/groks-ai-companions-drove-downloads-but-its-latest-model-is-the-one-making-money/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Grok’s raunchy, unfiltered AI companions may be making headlines for their unhinged and often NSFW responses, but it’s Grok 4, xAI’s latest model, that’s been driving the app’s revenue of late.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Elon Musk’s xAI launched Grok 4 late on July 9, and by Friday, July 11, Grok’s gross revenue on iOS had jumped a whopping 325% to $419,000, up from $99,000 the day before the Grok 4 launch, according to app intelligence firm Appfigures.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3029483" height="396" src="https://techcrunch.com/wp-content/uploads/2025/07/af-grok-downloads-grok-4-companions-15jul25.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Appfigures&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Grok continued to pull in higher-than-usual revenue in the days following the launch of the new model, with gross revenue over $367,000 for a couple of days before dipping down to $310,000 on July 14.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In addition, daily downloads of the Grok iOS app had increased 279% to 197,000 by July 11, up from 52,000 before Grok 4 launched.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But following the addition of Grok’s AI companions the next week, on July 14, the jump in downloads and revenue was less pronounced. While curiosity about the companions likely drove more installs, the feature isn’t yet poised to be a significant moneymaker for the company, despite being only available to “Super Grok” subscribers paying $30 per month.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Grok’s iOS downloads globally were up 40% the day after the companions launched, reaching 171,000 daily installs, but revenue increased just 9%, hitting $337,000. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So while it’s clear that there was an impact from the launch, it’s decidedly smaller than the launch of the new AI model.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3029482" height="396" src="https://techcrunch.com/wp-content/uploads/2025/07/af-grok-net-revenue-grok-4-companions-15jul25.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Appfigures&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Given Grok’s expensive new subscription offering, timed alongside the Grok 4 launch, it’s not surprising to see that even a smaller increase in the number of paying subscribers could drive Grok’s iOS revenue significantly higher.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company announced that, in addition to Grok 4 and Grok 4 Heavy, it would also offer a $300-per-month subscription called SuperGrok Heavy, its priciest plan to date. The plan offers subscribers early access to Grok 4 Heavy and other new features, xAI said, but it’s more expensive than comparable plans from other major AI providers, including OpenAI, Google, and Anthropic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s interesting to see this demand for Grok’s subscription plans, even though the AI was initially consulting Elon Musk’s X posts for answers. xAI has since addressed this issue.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3029481" height="346" src="https://techcrunch.com/wp-content/uploads/2025/07/af-grok-us-ios-category-rankings-16jul25.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Appfigures&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Interest in the new model also drove up Grok’s ranking on the U.S. App Store shortly after its launch, making Grok the No. 3 app overall and No. 2 in the Productivity category by July 12.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That interest has declined somewhat over the past week — the app is now No. 17 overall — though it’s still No. 2 in Productivity, Appfigures data shows.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Appfigures notes that it focused on iOS data for this analysis, as that dataset is currently more comprehensive than data from Android devices. The latter is only available through July 14 for the time being, as Appfigures’ model needs more time to process Google Play data.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Grok’s raunchy, unfiltered AI companions may be making headlines for their unhinged and often NSFW responses, but it’s Grok 4, xAI’s latest model, that’s been driving the app’s revenue of late.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Elon Musk’s xAI launched Grok 4 late on July 9, and by Friday, July 11, Grok’s gross revenue on iOS had jumped a whopping 325% to $419,000, up from $99,000 the day before the Grok 4 launch, according to app intelligence firm Appfigures.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3029483" height="396" src="https://techcrunch.com/wp-content/uploads/2025/07/af-grok-downloads-grok-4-companions-15jul25.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Appfigures&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Grok continued to pull in higher-than-usual revenue in the days following the launch of the new model, with gross revenue over $367,000 for a couple of days before dipping down to $310,000 on July 14.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In addition, daily downloads of the Grok iOS app had increased 279% to 197,000 by July 11, up from 52,000 before Grok 4 launched.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But following the addition of Grok’s AI companions the next week, on July 14, the jump in downloads and revenue was less pronounced. While curiosity about the companions likely drove more installs, the feature isn’t yet poised to be a significant moneymaker for the company, despite being only available to “Super Grok” subscribers paying $30 per month.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Grok’s iOS downloads globally were up 40% the day after the companions launched, reaching 171,000 daily installs, but revenue increased just 9%, hitting $337,000. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So while it’s clear that there was an impact from the launch, it’s decidedly smaller than the launch of the new AI model.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3029482" height="396" src="https://techcrunch.com/wp-content/uploads/2025/07/af-grok-net-revenue-grok-4-companions-15jul25.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Appfigures&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Given Grok’s expensive new subscription offering, timed alongside the Grok 4 launch, it’s not surprising to see that even a smaller increase in the number of paying subscribers could drive Grok’s iOS revenue significantly higher.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company announced that, in addition to Grok 4 and Grok 4 Heavy, it would also offer a $300-per-month subscription called SuperGrok Heavy, its priciest plan to date. The plan offers subscribers early access to Grok 4 Heavy and other new features, xAI said, but it’s more expensive than comparable plans from other major AI providers, including OpenAI, Google, and Anthropic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s interesting to see this demand for Grok’s subscription plans, even though the AI was initially consulting Elon Musk’s X posts for answers. xAI has since addressed this issue.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3029481" height="346" src="https://techcrunch.com/wp-content/uploads/2025/07/af-grok-us-ios-category-rankings-16jul25.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Appfigures&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Interest in the new model also drove up Grok’s ranking on the U.S. App Store shortly after its launch, making Grok the No. 3 app overall and No. 2 in the Productivity category by July 12.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That interest has declined somewhat over the past week — the app is now No. 17 overall — though it’s still No. 2 in Productivity, Appfigures data shows.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Appfigures notes that it focused on iOS data for this analysis, as that dataset is currently more comprehensive than data from Android devices. The latter is only available through July 14 for the time being, as Appfigures’ model needs more time to process Google Play data.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/21/groks-ai-companions-drove-downloads-but-its-latest-model-is-the-one-making-money/</guid><pubDate>Mon, 21 Jul 2025 15:20:29 +0000</pubDate></item><item><title>AI Testing and Evaluation: Reflections (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/podcast/ai-testing-and-evaluation-reflections/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Illustrated headshots of Amanda Craig Deckard and Kathleen Sullivan." class="wp-image-1145065" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/EP4-AI-TE_Hero_Feature_1400x788.jpg" width="1400" /&gt;&lt;/figure&gt;






&lt;p&gt;Generative AI presents a unique challenge and opportunity to reexamine governance practices for the responsible development, deployment, and use of AI. To advance thinking in this space, Microsoft has tapped into the experience and knowledge of experts across domains—from genome editing to cybersecurity—to investigate the role of testing and evaluation as a governance tool.&amp;nbsp;&lt;em&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/em&gt;,&amp;nbsp;hosted by Microsoft Research’s&amp;nbsp;Kathleen Sullivan, explores what the technology industry and policymakers can learn from these fields and how that might help shape the course of AI development.&lt;/p&gt;



&lt;p&gt;In the series finale, Amanda Craig Deckard, senior director of public policy in Microsoft’s Office of Responsible AI, rejoins Sullivan to discuss what Microsoft has learned about testing as a governance tool and what’s next for the company’s work in the AI governance space. The pair explores high-level takeaways (i.e., testing is important and challenging!); the roles of rigor, standardization, and interpretability in making testing a reliable governance tool; and the potential for public-private partnerships to help advance not only model-level evaluation but deployment-level evaluation, too.&lt;/p&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h2 class="wp-block-heading h5" id="learn-more-1"&gt;Learn more:&lt;/h2&gt;



&lt;p&gt;Learning from other domains to advance AI evaluation and testing&amp;nbsp;&lt;br /&gt;Microsoft Research Blog | June 2025&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Responsible AI: Ethical policies and practices | Microsoft AI&amp;nbsp;&lt;/p&gt;







&lt;section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast"&gt;
	
&lt;/section&gt;


&lt;div class="wp-block-msr-show-more"&gt;
	&lt;div class="bg-neutral-100 p-5"&gt;
		&lt;div class="show-more-show-less"&gt;
			&lt;div&gt;
				&lt;span&gt;
					

&lt;h2 class="wp-block-heading" id="transcript"&gt;Transcript&lt;/h2&gt;



&lt;p&gt;[MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KATHLEEN SULLIVAN:&lt;/strong&gt; Welcome to &lt;em&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/em&gt;. I’m your host, Kathleen Sullivan.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;As generative AI continues to advance, Microsoft has gathered a range of experts—from genome editing to cybersecurity—to share how their fields approach evaluation and risk assessment. Our goal is to learn from their successes and their stumbles to move the science and practice of AI testing forward. In this series, we’ll explore how these insights might help guide the future of AI development, deployment, and responsible use.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC ENDS]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For our final episode of the series, I’m thrilled to once again be joined by Amanda Craig Deckard, senior director of public policy in Microsoft’s Office of Responsible AI.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Amanda, welcome back to the podcast!&lt;/p&gt;



				&lt;/span&gt;
				&lt;span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1"&gt;
					



&lt;p&gt;&lt;strong&gt;AMANDA CRAIG DECKARD:&lt;/strong&gt; Thank you so much.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; In our intro episode, you really helped set the stage for this series. And it’s been great, because since then, we’ve had the pleasure of speaking with governance experts about genome editing, pharma, medical devices, cybersecurity, and we’ve also gotten to spend some time with our own Microsoft responsible AI leaders and hear reflections from them.&lt;/p&gt;



&lt;p&gt;And here’s what stuck with me, and I’d love to hear from you on this, as well: testing builds trust; context is shaping risk; and every field is really thinking about striking its own balance between pre-deployment testing and post-deployment monitoring.&lt;/p&gt;



&lt;p&gt;So drawing on what you’ve learned from the workshop and the case studies, what headline insights do you think matter the most for AI governance?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; It’s been really interesting to learn from all of these different domains, and there are, you know, lots of really interesting takeaways.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I think a starting point for me is actually pretty similar to where you landed, which is just that testing is really important for trust, and it’s also really &lt;em&gt;hard&lt;/em&gt; [LAUGHS] to figure out exactly, you know, how to get it right, how to make sure that you’re addressing risks, that you’re not constraining innovation, that you are recognizing that a lot of the industry that’s impacted is really different. You have small organizations, you have large organizations, and you want to enable that opportunity that is enabled by the technology across the board.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And so it’s just difficult to, kind of, get all of these dynamics right, especially when, you know, I think we heard from other domains, testing is not some, sort of, like, oh, simple thing, right. There’s not this linear path from, like, A to B where you just test the one thing and you’re done.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN: &lt;/strong&gt;Right.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; It’s complex, right. Testing is multistage. There’s a lot of testing by different actors. There are a lot of different purposes for which you might test. As I think it was Dan Carpenter who talked about it’s not just about testing for safety. It’s also about testing for efficacy and building confidence in the right dosage for pharmaceuticals, for example. And that’s across the board for all of these domains, right. That you’re really thinking about the performance of the technology. You’re thinking about safety. You’re trying to also calibrate for efficiency.&lt;/p&gt;



&lt;p&gt;And so those tradeoffs, every expert shared that navigating those is really challenging. And also that there were real impacts to early choices in the, sort of, governance of risk in these different domains and the development of the testing, sort of, expectations, and that in some cases, this had been difficult to reverse, which also just layers on that complexity and that difficulty in a different way. So that’s the super high-level takeaway. But maybe if I could just quickly distill, like, three takeaways that I think really are applicable to AI in a bit more of a granular way.&lt;/p&gt;



&lt;p&gt;You know, one is about, how is the testing exactly used? For what purpose? And the second is what emphasis there is on this pre- versus post-deployment testing and monitoring. And then the third is how rigid versus adaptive the, sort of, testing regimes or frameworks are in these different domains.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So on the first—how is testing used?—so is testing something that impacts market entry, for example? Or is it something that might be used more for informing how risk is evolving in the domain and how broader risk management strategies might need to be applied? We have examples, like the pharmaceutical or medical device industry experts with whom you spoke, that’s really, you know, testing … there is a pre-deployment requirement. So that’s one question.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The second is this emphasis on pre- versus post-deployment testing and monitoring, and we really did see across domains that in many cases, there is a desire for both pre- and post-deployment, sort of, testing and monitoring, but also that, sort of, naturally in these different domains, a degree of emphasis on one or the other had evolved and that had a real impact on governance and tradeoffs.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And the third is just how rigid versus adaptive these testing and evaluation regimes or frameworks are in these different domains. We saw, you know, in some domains, the testing requirements were more rigid as you might expect in more of the pharmaceutical or medical devices industries, for example. And in other domains, there was this more, sort of, adaptive approach to how testing might get used. So, for example, in the case of our other general-purpose technologies, you know, you spoke with Alta Charo on genome editing, and in our case studies, we also explored this in the context of nanotechnology. In those general-purpose technology domains, there is more emphasis on downstream or application-context testing that is more, sort of, adaptive to the use scenario of the technology and, you know, having that work in conjunction with testing more at the, kind of, level of the technology itself.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; I want to double-click on a number of the things we just talked about. But actually, before we go too much deeper, a question on if there’s anything that really surprised you or challenged maybe some of your own assumptions in this space from some of the discussions that we had over the series.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Yeah. You know, I know I’ve already just mentioned this pre- versus post-deployment testing and monitoring issue, but it was something that was very interesting to me and in some ways surprised me or made me just realize something that I hadn’t fully connected before, about how these, sort of, regimes might evolve in different contexts and why. And in part, I couldn’t help but bring the context I have from cybersecurity policy into this, kind of, processing of what we learned and reflection because there was a real contrast for me between the pharmaceutical industry and the cybersecurity domain when I think about the emphasis on pre- versus post-deployment monitoring.&lt;/p&gt;



&lt;p&gt;And on the one hand, we have in the pharmaceutical domain a real emphasis that has developed around pre-market testing. And there is also an expectation in some circumstances in the pharmaceutical domain for post-deployment testing, as well. But as we learned from our experts in that domain, there has naturally been a real, kind of, emphasis on the pre-market portion of that testing. And in reality, even where post-market monitoring is required and post-market testing is required, it does not always actually happen.&lt;strong&gt; &lt;/strong&gt;And the experts really explained that, you know, part of it is just the incentive structure around the emphasis around, you know, the testing as a pre-market, sort of, entry requirement. And also just the resources that exist among regulators, right. There’s limited resources, right. And so there are just choices and tradeoffs that they need to make in their own, sort of, enforcement work.&lt;/p&gt;



&lt;p&gt;And then on the other hand, you know, in cybersecurity, I never thought about the, kind of, emphasis on things like coordinated vulnerability disclosure and bug bounties that have really developed in the cybersecurity domain. But it’s a really important part of how we secure technology and enhance cybersecurity over time, where we have these norms that have developed where, you know, security researchers are doing really important research. They’re finding vulnerabilities in products. And we have norms developed where they report those to the companies that are in a position to address those vulnerabilities. And in some cases, those companies actually pay, through bug bounties, the researchers. And perhaps in some ways, the role of coordinated vulnerability disclosure and bug bounties has evolved the way that it has because there hasn’t been as much emphasis on the pre-market testing across the board at least in the context of software.&lt;/p&gt;



&lt;p&gt;And so you look at those two industries and it was interesting to me to study them to some extent in contrast with each other as this way that the incentives and the resources that need to be applied to testing, sort of, evolve to address where there’s, kind of, more or less emphasis.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; It’s a great point. I mean, I think what we’re hearing—and what you’re saying—is just exactly this choice … like, is there a binary choice between focusing on pre-deployment testing or post-deployment monitoring? And, you know, I think our assumption is that we need to do both. But I’d love to hear from you on that.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Absolutely. I think we need to do both. I’m very persuaded by this inclination always that there’s value in trying to really do it all in a risk management context.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And also, we know one of the principles of risk management is you have to prioritize because there are finite resources. And I think that’s where we get to this challenge in really thinking deeply, especially as we’re in the early days of AI governance, and we need to be very thoughtful about, you know, tradeoffs that we may not want to be making but we are because, again, these are finite choices and we, kind of, can’t help but put our finger on the dial in different directions with our choices that, you know, it’s going to be very difficult to have, sort of, equal emphasis on both. And we need to invest in both, but we need to be very &lt;em&gt;deliberate&lt;/em&gt; about the roles of each and how they complement each other and who does which and how we use what we learn from pre- versus post-deployment testing and monitoring.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; Maybe just spending a little bit more time here … you know, a lot of attention goes into testing models upstream, but risk often shows up once they’re wired into real products and workflows. How much does deployment context change the risk picture from your perspective?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Yeah, I … such an important question. I really agree that there has been a lot of emphasis to date on, sort of, testing models upstream, the AI model evaluation. And it’s also really important that we bring more attention into evaluation at the system or application level. And I actually see that in governance conversations, this &lt;em&gt;is&lt;/em&gt; actually increasingly raised, this need to have system-level evaluation. We see this across regulation. We also see it in the context of just organizations trying to put in governance requirements for how their organization is going to operate in deploying this technology.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And there’s a gap today in terms of best practices around system-level testing, perhaps even more than model-level evaluation. And it’s really important because in a lot of cases, the deployment context really does impact the risk picture, especially with AI, which is a general-purpose technology, and we really saw this in our study of other domains that represented general-purpose technology.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So in the case study that you can find online on nanotechnology, you know, there’s a real distinction between the risk evaluation and the governance of nanotechnology in different deployment contexts. So the chapter that our expert on nanotechnology wrote really goes into incredibly interesting detail around, you know, deployment of nanotechnology in the context of, like, chemical applications versus consumer electronics versus pharmaceuticals versus construction and how the way that nanoparticles are basically delivered in all those different deployment contexts, as well as, like, what the risk of the actual use scenario is just varies so much. And so there’s a real need to do that kind of risk evaluation and testing in the deployment context, and this difference in terms of risks and what we learned in these other domains where, you know, there are these different approaches to trying to really think about and gain efficiencies and address risks at a horizontal level versus, you know, taking a real sector-by-sector approach. And to some extent, it seems like it’s more time intensive to do that sectoral deployment-specific work. And at the same time, perhaps there are efficiencies to be gained by actually doing the work in the context in which, you know, you have a better understanding of the risk that can result from really deploying this technology.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And ultimately, [LAUGHS] really what we also need to think about here is probably, in the end, just like pre- and post-deployment testing, you need both. Not probably; certainly!&lt;/p&gt;



&lt;p&gt;So effectively we need to think about evaluation at the model level and the system level as being really important. And it’s really important to get system evaluation right so that we can actually get trust in this technology in deployment context so we enable adoption in low- and in high-risk deployments in a way that means that we’ve done risk evaluation in each of those contexts in a way that really makes sense in terms of the resources that we need to apply and ultimately we are able to unlock more applications of this technology in a risk-informed way.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; That’s great. I mean, I couldn’t agree more. I think these contexts, the approaches are so important for trust and adoption, and I’d love to hear from you, what do we need to advance AI evaluation and testing in our ecosystem? What are some of the big gaps that you’re seeing, and what role can different stakeholders play in filling them? And maybe an add-on, actually: is there some sort of network effect that could 10x our testing capacity?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Absolutely. So there’s a lot of work that needs to be done, and there’s a lot of work in process to really level up our whole evaluation and testing ecosystem. We learned, across domains, that there’s really a need to advance our thinking and our practice in three areas: rigor of testing; standardization of methodologies and processes; and interpretability of test results.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So what we mean by rigor is that we are ensuring that what we are ultimately evaluating in terms of risks is defined in a scientifically valid way and we are able to measure against that risk in a scientifically valid way.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;By standardization, what we mean is that there’s really an accepted and well-understood and, again, a scientifically valid methodology for doing that testing and for actually producing artifacts out of that testing that are meeting those standards. And that sets us up for the final portion on interpretability, which is, like, really the process by which you can trust that the testing has been done in this rigorous and standardized way and that then you have artifacts that result from the testing process that can really be used in the risk management context because they can be interpreted, right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;We understand how to, like, apply weight to them for our risk-management decisions. We actually are able to interpret them in a way that perhaps they inform other downstream risk mitigations that address the risks that we see through the testing results and that we actually understand what limitations apply to the test results and why they may or may not be valid in certain, sort of, deployment contexts, for example, and especially in the context of other risk mitigations that we need to apply. So there’s a need to advance all three of those things—rigor, standardization, and interpretability—to level up the whole testing and evaluation ecosystem.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And when we think about what actors should be involved in that work … really &lt;em&gt;everybody&lt;/em&gt;, which is both complex to orchestrate but also really important. And so, you know, you need to have the entire value chain involved in really advancing this work. You need the model developers, but you also need the system developers and deployers that are really engaged in advancing the science of evaluation and advancing how we are using these testing artifacts in the risk management process.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;When we think about what could actually 10x our testing capacity—that’s the dream, right? We all want to accelerate our progress in this space. You know,&amp;nbsp;I think we need work across all three of those areas of rigor, standardization, and interpretability, but I think one that will really help accelerate our progress across the board is that standardization work, because ultimately, you’re going to need to have these tests be done and applied across so many different contexts, and ultimately, while we want the whole value chain engaged in the development of the thinking and the science and the standards in this space, we also need to realize that not every organization is necessarily going to have the capacity to, kind of, contribute to developing the ways that we create and use these tests. And there are going to be many organizations that are going to benefit from there being standardization of the methodologies and the artifacts that they can pick up and use.&lt;/p&gt;



&lt;p&gt;One thing that I know we’ve heard throughout this podcast series from our experts in other domains, including Timo [Minssen] in the medical devices context and Ciaran [Martin] in the cybersecurity context, is that there’s been a recognition, as those domains have evolved, that there’s a need to calibrate our, sort of, expectations for different actors in the ecosystem and really understand that small businesses, for example, just cannot apply the same degree of resources that others may be able to, to do testing and evaluation and risk management. And so the benefit of having standardized approaches is that those organizations are able to, kind of, integrate into the broader supply chain ecosystem and apply their own, kind of, risk management practices in their own context in a way that is more efficient.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And finally, the last stakeholder that I think is really important to think about in terms of partnership across the ecosystem to really advance the whole testing and evaluation work that needs to happen is government partners, right, and thinking beyond the value chain, the AI supply chain, and really thinking about public-private partnership. That’s going to be incredibly important to advancing this ecosystem.&lt;/p&gt;



&lt;p&gt;You know, I think there’s been real progress already in the AI evaluation and testing ecosystem in the public-private partnership context. We have been really supportive of the work of the International Network of AI Safety and Security Institutes&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;[1]&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and the Center for AI Standards and Innovation&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; that all allow for that kind of public-private partnership on actually testing and advancing the science and best practices around standards.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And there are other innovative, kind of, partnerships, as well, in the ecosystem. You know, Singapore has recently launched their Global AI Assurance Pilot&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; findings. And that effort really paired application deployers and testers so that consequential impacts at deployment could really be tested. And that’s a really fruitful, sort of, effort that complements the work of these institutes and centers that are more focused on evaluation at the model level, for example.&lt;/p&gt;



&lt;p&gt;And in general, you know, I think that there’s just really a lot of benefits for us thinking expansively about what we can accomplish through deep, meaningful public-private partnership in this space. I’m really excited to see where we can go from here with building on, you know, partnerships across AI supply chains and with governments and public-private partnerships.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; I couldn’t agree more. I mean, this notion of more engagement across the ecosystem and value chain is super important for us and informs how we think about the space completely.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;If you could invite any other industry to the next workshop, maybe quantum safety, space tech, even gaming, who’s on your wish list? And maybe what are some of the things you’d want to go deeper on?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; This is something that we really welcome feedback on if anyone listening has ideas about other domains that would be interesting to study. I will say, I think I shared at the outset of this podcast series, the domains that we added in this round of our efforts in studying other domains actually all came from feedback that we received from, you know, folks we’d engaged with our first study of other domains and multilateral, sort of, governance institutions. And so we’re really keen to think about what other domains could be interesting to study. And we are also keen to go deeper, building on what we learned in this round of effort going forward.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;One of the areas that I am particularly really interested in is going deeper on, what, sort of, transparency and information sharing about risk evaluation and testing will be really useful to share in different contexts? So across the AI supply chain, what is the information that’s going to be really meaningful to share between developers and deployers of models and systems and those that are ultimately using this technology in particular deployment contexts? And, you know, I think that we could have much to learn from other general-purpose technologies like genome editing and nanotechnology and cybersecurity, where we could learn a bit more about the kinds of information that they have shared across the development and deployment life cycle and how that has strengthened risk management in general as well as provided a really strong feedback loop around testing and evaluation. What kind of testing is most useful to do at what point in the life cycle, and what artifacts are most useful to share as a result of that testing and evaluation work?&lt;/p&gt;



&lt;p&gt;I’ll say, as Microsoft, we have been really investing in how we are sharing information with our various stakeholders. We also have been engaged with others in industry in reporting what we’ve done in the context of the Hiroshima AI Process, or &lt;em&gt;HAIP&lt;/em&gt;, Reporting Framework&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. This is an effort that is really just in its first round of really exploring how this kind of reporting can be really additive to risk management understanding. And again, I think there’s real opportunity here to look at this kind of reporting and understand, you know, what’s valuable for stakeholders and where is there opportunity to go further in really informing value chains and policymakers and the public about AI risk and opportunity and what can we learn again from other domains that have done this kind of work over decades to really refine that kind of information sharing.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; It’s really great to hear about all the advances that we’re making on these reports. I’m guessing a lot of the metrics in there are technical, but sociotechnical impacts—jobs, maybe misinformation, well-being—are harder to score. What new measurement ideas are you excited about, and do you have any thoughts on, like, who needs to pilot those?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Yeah, it’s an incredibly interesting question that I think also just speaks to, you know, the breadth of, sort of, testing and evaluation that’s needed at different points along that AI life cycle and really not getting lost in one particular kind of testing or another pre- or post-deployment and thinking expansively about the risks that we’re trying to address through this testing.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;You know, for example, even with the UK’s AI Security Institute&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; that has just recently launched a new program, a new team, that’s focused on societal resilience research. I think it’s going to be a really important area from a sociotechnical impact perspective to bring some focus into as this technology is more widely deployed. Are we understanding the impacts over time as different people and different cultures adopt and use this technology for different purposes?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And I think that’s an area where there really is opportunity for greater public-private partnership in this research. Because we all share this long-term interest in ensuring that this technology is really serving people and we have to understand the impacts so that we understand, you know, what adjustments we can actually pursue sooner upstream to address those impacts and make sure that this technology is really going to work for all of us and in a way that is consistent with the societal values that we want.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; So, Amanda, looking ahead, I would love to hear just what’s going to be on your radar? What’s top of mind for you in the coming weeks?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Well, we are certainly continuing to process all the learnings that we’ve had from studying these domains. It’s really been a rich set of insights that we want to make sure we, kind of, fully take advantage of. And, you know, I think these hard questions and, you know, real opportunities to be thoughtful in these early days of AI governance are not, sort of, going away or being easily resolved soon. And so I think we continue to see value in really learning from others, thinking about what’s distinct in the AI context, but also what we can apply in terms of what other domains have learned.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; Well, Amanda, it has been such a special experience for me to help illuminate the work of the Office of Responsible AI and our team in Microsoft Research, and [MUSIC] it’s just really special to see all of the work that we’re doing to help set the standard for responsible development and deployment of AI. So thank you for joining us today, and thanks for your reflections and discussion.&lt;/p&gt;



&lt;p&gt;And to our listeners, thank you so much for joining us for the series. We really hope you enjoyed it!&amp;nbsp;To check out all of our episodes, visit aka.ms/AITestingandEvaluation&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, and if you want to learn more about how Microsoft approaches AI governance, you can visit microsoft.com/RAI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;See you next time!&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC FADES] &lt;/p&gt;

				&lt;/span&gt;
			&lt;/div&gt;
			&lt;button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button"&gt;
				Show more			&lt;/button&gt;
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;







&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;p&gt;[1]&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; Since the launch of the International Network of AI Safety Institutes, the UK renamed its institute the AI Security Institute&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Illustrated headshots of Amanda Craig Deckard and Kathleen Sullivan." class="wp-image-1145065" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/EP4-AI-TE_Hero_Feature_1400x788.jpg" width="1400" /&gt;&lt;/figure&gt;






&lt;p&gt;Generative AI presents a unique challenge and opportunity to reexamine governance practices for the responsible development, deployment, and use of AI. To advance thinking in this space, Microsoft has tapped into the experience and knowledge of experts across domains—from genome editing to cybersecurity—to investigate the role of testing and evaluation as a governance tool.&amp;nbsp;&lt;em&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/em&gt;,&amp;nbsp;hosted by Microsoft Research’s&amp;nbsp;Kathleen Sullivan, explores what the technology industry and policymakers can learn from these fields and how that might help shape the course of AI development.&lt;/p&gt;



&lt;p&gt;In the series finale, Amanda Craig Deckard, senior director of public policy in Microsoft’s Office of Responsible AI, rejoins Sullivan to discuss what Microsoft has learned about testing as a governance tool and what’s next for the company’s work in the AI governance space. The pair explores high-level takeaways (i.e., testing is important and challenging!); the roles of rigor, standardization, and interpretability in making testing a reliable governance tool; and the potential for public-private partnerships to help advance not only model-level evaluation but deployment-level evaluation, too.&lt;/p&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h2 class="wp-block-heading h5" id="learn-more-1"&gt;Learn more:&lt;/h2&gt;



&lt;p&gt;Learning from other domains to advance AI evaluation and testing&amp;nbsp;&lt;br /&gt;Microsoft Research Blog | June 2025&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Responsible AI: Ethical policies and practices | Microsoft AI&amp;nbsp;&lt;/p&gt;







&lt;section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast"&gt;
	
&lt;/section&gt;


&lt;div class="wp-block-msr-show-more"&gt;
	&lt;div class="bg-neutral-100 p-5"&gt;
		&lt;div class="show-more-show-less"&gt;
			&lt;div&gt;
				&lt;span&gt;
					

&lt;h2 class="wp-block-heading" id="transcript"&gt;Transcript&lt;/h2&gt;



&lt;p&gt;[MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KATHLEEN SULLIVAN:&lt;/strong&gt; Welcome to &lt;em&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/em&gt;. I’m your host, Kathleen Sullivan.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;As generative AI continues to advance, Microsoft has gathered a range of experts—from genome editing to cybersecurity—to share how their fields approach evaluation and risk assessment. Our goal is to learn from their successes and their stumbles to move the science and practice of AI testing forward. In this series, we’ll explore how these insights might help guide the future of AI development, deployment, and responsible use.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC ENDS]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For our final episode of the series, I’m thrilled to once again be joined by Amanda Craig Deckard, senior director of public policy in Microsoft’s Office of Responsible AI.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Amanda, welcome back to the podcast!&lt;/p&gt;



				&lt;/span&gt;
				&lt;span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1"&gt;
					



&lt;p&gt;&lt;strong&gt;AMANDA CRAIG DECKARD:&lt;/strong&gt; Thank you so much.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; In our intro episode, you really helped set the stage for this series. And it’s been great, because since then, we’ve had the pleasure of speaking with governance experts about genome editing, pharma, medical devices, cybersecurity, and we’ve also gotten to spend some time with our own Microsoft responsible AI leaders and hear reflections from them.&lt;/p&gt;



&lt;p&gt;And here’s what stuck with me, and I’d love to hear from you on this, as well: testing builds trust; context is shaping risk; and every field is really thinking about striking its own balance between pre-deployment testing and post-deployment monitoring.&lt;/p&gt;



&lt;p&gt;So drawing on what you’ve learned from the workshop and the case studies, what headline insights do you think matter the most for AI governance?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; It’s been really interesting to learn from all of these different domains, and there are, you know, lots of really interesting takeaways.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I think a starting point for me is actually pretty similar to where you landed, which is just that testing is really important for trust, and it’s also really &lt;em&gt;hard&lt;/em&gt; [LAUGHS] to figure out exactly, you know, how to get it right, how to make sure that you’re addressing risks, that you’re not constraining innovation, that you are recognizing that a lot of the industry that’s impacted is really different. You have small organizations, you have large organizations, and you want to enable that opportunity that is enabled by the technology across the board.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And so it’s just difficult to, kind of, get all of these dynamics right, especially when, you know, I think we heard from other domains, testing is not some, sort of, like, oh, simple thing, right. There’s not this linear path from, like, A to B where you just test the one thing and you’re done.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN: &lt;/strong&gt;Right.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; It’s complex, right. Testing is multistage. There’s a lot of testing by different actors. There are a lot of different purposes for which you might test. As I think it was Dan Carpenter who talked about it’s not just about testing for safety. It’s also about testing for efficacy and building confidence in the right dosage for pharmaceuticals, for example. And that’s across the board for all of these domains, right. That you’re really thinking about the performance of the technology. You’re thinking about safety. You’re trying to also calibrate for efficiency.&lt;/p&gt;



&lt;p&gt;And so those tradeoffs, every expert shared that navigating those is really challenging. And also that there were real impacts to early choices in the, sort of, governance of risk in these different domains and the development of the testing, sort of, expectations, and that in some cases, this had been difficult to reverse, which also just layers on that complexity and that difficulty in a different way. So that’s the super high-level takeaway. But maybe if I could just quickly distill, like, three takeaways that I think really are applicable to AI in a bit more of a granular way.&lt;/p&gt;



&lt;p&gt;You know, one is about, how is the testing exactly used? For what purpose? And the second is what emphasis there is on this pre- versus post-deployment testing and monitoring. And then the third is how rigid versus adaptive the, sort of, testing regimes or frameworks are in these different domains.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So on the first—how is testing used?—so is testing something that impacts market entry, for example? Or is it something that might be used more for informing how risk is evolving in the domain and how broader risk management strategies might need to be applied? We have examples, like the pharmaceutical or medical device industry experts with whom you spoke, that’s really, you know, testing … there is a pre-deployment requirement. So that’s one question.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The second is this emphasis on pre- versus post-deployment testing and monitoring, and we really did see across domains that in many cases, there is a desire for both pre- and post-deployment, sort of, testing and monitoring, but also that, sort of, naturally in these different domains, a degree of emphasis on one or the other had evolved and that had a real impact on governance and tradeoffs.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And the third is just how rigid versus adaptive these testing and evaluation regimes or frameworks are in these different domains. We saw, you know, in some domains, the testing requirements were more rigid as you might expect in more of the pharmaceutical or medical devices industries, for example. And in other domains, there was this more, sort of, adaptive approach to how testing might get used. So, for example, in the case of our other general-purpose technologies, you know, you spoke with Alta Charo on genome editing, and in our case studies, we also explored this in the context of nanotechnology. In those general-purpose technology domains, there is more emphasis on downstream or application-context testing that is more, sort of, adaptive to the use scenario of the technology and, you know, having that work in conjunction with testing more at the, kind of, level of the technology itself.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; I want to double-click on a number of the things we just talked about. But actually, before we go too much deeper, a question on if there’s anything that really surprised you or challenged maybe some of your own assumptions in this space from some of the discussions that we had over the series.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Yeah. You know, I know I’ve already just mentioned this pre- versus post-deployment testing and monitoring issue, but it was something that was very interesting to me and in some ways surprised me or made me just realize something that I hadn’t fully connected before, about how these, sort of, regimes might evolve in different contexts and why. And in part, I couldn’t help but bring the context I have from cybersecurity policy into this, kind of, processing of what we learned and reflection because there was a real contrast for me between the pharmaceutical industry and the cybersecurity domain when I think about the emphasis on pre- versus post-deployment monitoring.&lt;/p&gt;



&lt;p&gt;And on the one hand, we have in the pharmaceutical domain a real emphasis that has developed around pre-market testing. And there is also an expectation in some circumstances in the pharmaceutical domain for post-deployment testing, as well. But as we learned from our experts in that domain, there has naturally been a real, kind of, emphasis on the pre-market portion of that testing. And in reality, even where post-market monitoring is required and post-market testing is required, it does not always actually happen.&lt;strong&gt; &lt;/strong&gt;And the experts really explained that, you know, part of it is just the incentive structure around the emphasis around, you know, the testing as a pre-market, sort of, entry requirement. And also just the resources that exist among regulators, right. There’s limited resources, right. And so there are just choices and tradeoffs that they need to make in their own, sort of, enforcement work.&lt;/p&gt;



&lt;p&gt;And then on the other hand, you know, in cybersecurity, I never thought about the, kind of, emphasis on things like coordinated vulnerability disclosure and bug bounties that have really developed in the cybersecurity domain. But it’s a really important part of how we secure technology and enhance cybersecurity over time, where we have these norms that have developed where, you know, security researchers are doing really important research. They’re finding vulnerabilities in products. And we have norms developed where they report those to the companies that are in a position to address those vulnerabilities. And in some cases, those companies actually pay, through bug bounties, the researchers. And perhaps in some ways, the role of coordinated vulnerability disclosure and bug bounties has evolved the way that it has because there hasn’t been as much emphasis on the pre-market testing across the board at least in the context of software.&lt;/p&gt;



&lt;p&gt;And so you look at those two industries and it was interesting to me to study them to some extent in contrast with each other as this way that the incentives and the resources that need to be applied to testing, sort of, evolve to address where there’s, kind of, more or less emphasis.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; It’s a great point. I mean, I think what we’re hearing—and what you’re saying—is just exactly this choice … like, is there a binary choice between focusing on pre-deployment testing or post-deployment monitoring? And, you know, I think our assumption is that we need to do both. But I’d love to hear from you on that.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Absolutely. I think we need to do both. I’m very persuaded by this inclination always that there’s value in trying to really do it all in a risk management context.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And also, we know one of the principles of risk management is you have to prioritize because there are finite resources. And I think that’s where we get to this challenge in really thinking deeply, especially as we’re in the early days of AI governance, and we need to be very thoughtful about, you know, tradeoffs that we may not want to be making but we are because, again, these are finite choices and we, kind of, can’t help but put our finger on the dial in different directions with our choices that, you know, it’s going to be very difficult to have, sort of, equal emphasis on both. And we need to invest in both, but we need to be very &lt;em&gt;deliberate&lt;/em&gt; about the roles of each and how they complement each other and who does which and how we use what we learn from pre- versus post-deployment testing and monitoring.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; Maybe just spending a little bit more time here … you know, a lot of attention goes into testing models upstream, but risk often shows up once they’re wired into real products and workflows. How much does deployment context change the risk picture from your perspective?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Yeah, I … such an important question. I really agree that there has been a lot of emphasis to date on, sort of, testing models upstream, the AI model evaluation. And it’s also really important that we bring more attention into evaluation at the system or application level. And I actually see that in governance conversations, this &lt;em&gt;is&lt;/em&gt; actually increasingly raised, this need to have system-level evaluation. We see this across regulation. We also see it in the context of just organizations trying to put in governance requirements for how their organization is going to operate in deploying this technology.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And there’s a gap today in terms of best practices around system-level testing, perhaps even more than model-level evaluation. And it’s really important because in a lot of cases, the deployment context really does impact the risk picture, especially with AI, which is a general-purpose technology, and we really saw this in our study of other domains that represented general-purpose technology.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So in the case study that you can find online on nanotechnology, you know, there’s a real distinction between the risk evaluation and the governance of nanotechnology in different deployment contexts. So the chapter that our expert on nanotechnology wrote really goes into incredibly interesting detail around, you know, deployment of nanotechnology in the context of, like, chemical applications versus consumer electronics versus pharmaceuticals versus construction and how the way that nanoparticles are basically delivered in all those different deployment contexts, as well as, like, what the risk of the actual use scenario is just varies so much. And so there’s a real need to do that kind of risk evaluation and testing in the deployment context, and this difference in terms of risks and what we learned in these other domains where, you know, there are these different approaches to trying to really think about and gain efficiencies and address risks at a horizontal level versus, you know, taking a real sector-by-sector approach. And to some extent, it seems like it’s more time intensive to do that sectoral deployment-specific work. And at the same time, perhaps there are efficiencies to be gained by actually doing the work in the context in which, you know, you have a better understanding of the risk that can result from really deploying this technology.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And ultimately, [LAUGHS] really what we also need to think about here is probably, in the end, just like pre- and post-deployment testing, you need both. Not probably; certainly!&lt;/p&gt;



&lt;p&gt;So effectively we need to think about evaluation at the model level and the system level as being really important. And it’s really important to get system evaluation right so that we can actually get trust in this technology in deployment context so we enable adoption in low- and in high-risk deployments in a way that means that we’ve done risk evaluation in each of those contexts in a way that really makes sense in terms of the resources that we need to apply and ultimately we are able to unlock more applications of this technology in a risk-informed way.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; That’s great. I mean, I couldn’t agree more. I think these contexts, the approaches are so important for trust and adoption, and I’d love to hear from you, what do we need to advance AI evaluation and testing in our ecosystem? What are some of the big gaps that you’re seeing, and what role can different stakeholders play in filling them? And maybe an add-on, actually: is there some sort of network effect that could 10x our testing capacity?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Absolutely. So there’s a lot of work that needs to be done, and there’s a lot of work in process to really level up our whole evaluation and testing ecosystem. We learned, across domains, that there’s really a need to advance our thinking and our practice in three areas: rigor of testing; standardization of methodologies and processes; and interpretability of test results.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So what we mean by rigor is that we are ensuring that what we are ultimately evaluating in terms of risks is defined in a scientifically valid way and we are able to measure against that risk in a scientifically valid way.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;By standardization, what we mean is that there’s really an accepted and well-understood and, again, a scientifically valid methodology for doing that testing and for actually producing artifacts out of that testing that are meeting those standards. And that sets us up for the final portion on interpretability, which is, like, really the process by which you can trust that the testing has been done in this rigorous and standardized way and that then you have artifacts that result from the testing process that can really be used in the risk management context because they can be interpreted, right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;We understand how to, like, apply weight to them for our risk-management decisions. We actually are able to interpret them in a way that perhaps they inform other downstream risk mitigations that address the risks that we see through the testing results and that we actually understand what limitations apply to the test results and why they may or may not be valid in certain, sort of, deployment contexts, for example, and especially in the context of other risk mitigations that we need to apply. So there’s a need to advance all three of those things—rigor, standardization, and interpretability—to level up the whole testing and evaluation ecosystem.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And when we think about what actors should be involved in that work … really &lt;em&gt;everybody&lt;/em&gt;, which is both complex to orchestrate but also really important. And so, you know, you need to have the entire value chain involved in really advancing this work. You need the model developers, but you also need the system developers and deployers that are really engaged in advancing the science of evaluation and advancing how we are using these testing artifacts in the risk management process.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;When we think about what could actually 10x our testing capacity—that’s the dream, right? We all want to accelerate our progress in this space. You know,&amp;nbsp;I think we need work across all three of those areas of rigor, standardization, and interpretability, but I think one that will really help accelerate our progress across the board is that standardization work, because ultimately, you’re going to need to have these tests be done and applied across so many different contexts, and ultimately, while we want the whole value chain engaged in the development of the thinking and the science and the standards in this space, we also need to realize that not every organization is necessarily going to have the capacity to, kind of, contribute to developing the ways that we create and use these tests. And there are going to be many organizations that are going to benefit from there being standardization of the methodologies and the artifacts that they can pick up and use.&lt;/p&gt;



&lt;p&gt;One thing that I know we’ve heard throughout this podcast series from our experts in other domains, including Timo [Minssen] in the medical devices context and Ciaran [Martin] in the cybersecurity context, is that there’s been a recognition, as those domains have evolved, that there’s a need to calibrate our, sort of, expectations for different actors in the ecosystem and really understand that small businesses, for example, just cannot apply the same degree of resources that others may be able to, to do testing and evaluation and risk management. And so the benefit of having standardized approaches is that those organizations are able to, kind of, integrate into the broader supply chain ecosystem and apply their own, kind of, risk management practices in their own context in a way that is more efficient.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And finally, the last stakeholder that I think is really important to think about in terms of partnership across the ecosystem to really advance the whole testing and evaluation work that needs to happen is government partners, right, and thinking beyond the value chain, the AI supply chain, and really thinking about public-private partnership. That’s going to be incredibly important to advancing this ecosystem.&lt;/p&gt;



&lt;p&gt;You know, I think there’s been real progress already in the AI evaluation and testing ecosystem in the public-private partnership context. We have been really supportive of the work of the International Network of AI Safety and Security Institutes&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;[1]&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and the Center for AI Standards and Innovation&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; that all allow for that kind of public-private partnership on actually testing and advancing the science and best practices around standards.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And there are other innovative, kind of, partnerships, as well, in the ecosystem. You know, Singapore has recently launched their Global AI Assurance Pilot&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; findings. And that effort really paired application deployers and testers so that consequential impacts at deployment could really be tested. And that’s a really fruitful, sort of, effort that complements the work of these institutes and centers that are more focused on evaluation at the model level, for example.&lt;/p&gt;



&lt;p&gt;And in general, you know, I think that there’s just really a lot of benefits for us thinking expansively about what we can accomplish through deep, meaningful public-private partnership in this space. I’m really excited to see where we can go from here with building on, you know, partnerships across AI supply chains and with governments and public-private partnerships.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; I couldn’t agree more. I mean, this notion of more engagement across the ecosystem and value chain is super important for us and informs how we think about the space completely.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;If you could invite any other industry to the next workshop, maybe quantum safety, space tech, even gaming, who’s on your wish list? And maybe what are some of the things you’d want to go deeper on?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; This is something that we really welcome feedback on if anyone listening has ideas about other domains that would be interesting to study. I will say, I think I shared at the outset of this podcast series, the domains that we added in this round of our efforts in studying other domains actually all came from feedback that we received from, you know, folks we’d engaged with our first study of other domains and multilateral, sort of, governance institutions. And so we’re really keen to think about what other domains could be interesting to study. And we are also keen to go deeper, building on what we learned in this round of effort going forward.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;One of the areas that I am particularly really interested in is going deeper on, what, sort of, transparency and information sharing about risk evaluation and testing will be really useful to share in different contexts? So across the AI supply chain, what is the information that’s going to be really meaningful to share between developers and deployers of models and systems and those that are ultimately using this technology in particular deployment contexts? And, you know, I think that we could have much to learn from other general-purpose technologies like genome editing and nanotechnology and cybersecurity, where we could learn a bit more about the kinds of information that they have shared across the development and deployment life cycle and how that has strengthened risk management in general as well as provided a really strong feedback loop around testing and evaluation. What kind of testing is most useful to do at what point in the life cycle, and what artifacts are most useful to share as a result of that testing and evaluation work?&lt;/p&gt;



&lt;p&gt;I’ll say, as Microsoft, we have been really investing in how we are sharing information with our various stakeholders. We also have been engaged with others in industry in reporting what we’ve done in the context of the Hiroshima AI Process, or &lt;em&gt;HAIP&lt;/em&gt;, Reporting Framework&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. This is an effort that is really just in its first round of really exploring how this kind of reporting can be really additive to risk management understanding. And again, I think there’s real opportunity here to look at this kind of reporting and understand, you know, what’s valuable for stakeholders and where is there opportunity to go further in really informing value chains and policymakers and the public about AI risk and opportunity and what can we learn again from other domains that have done this kind of work over decades to really refine that kind of information sharing.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; It’s really great to hear about all the advances that we’re making on these reports. I’m guessing a lot of the metrics in there are technical, but sociotechnical impacts—jobs, maybe misinformation, well-being—are harder to score. What new measurement ideas are you excited about, and do you have any thoughts on, like, who needs to pilot those?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Yeah, it’s an incredibly interesting question that I think also just speaks to, you know, the breadth of, sort of, testing and evaluation that’s needed at different points along that AI life cycle and really not getting lost in one particular kind of testing or another pre- or post-deployment and thinking expansively about the risks that we’re trying to address through this testing.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;You know, for example, even with the UK’s AI Security Institute&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; that has just recently launched a new program, a new team, that’s focused on societal resilience research. I think it’s going to be a really important area from a sociotechnical impact perspective to bring some focus into as this technology is more widely deployed. Are we understanding the impacts over time as different people and different cultures adopt and use this technology for different purposes?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And I think that’s an area where there really is opportunity for greater public-private partnership in this research. Because we all share this long-term interest in ensuring that this technology is really serving people and we have to understand the impacts so that we understand, you know, what adjustments we can actually pursue sooner upstream to address those impacts and make sure that this technology is really going to work for all of us and in a way that is consistent with the societal values that we want.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; So, Amanda, looking ahead, I would love to hear just what’s going to be on your radar? What’s top of mind for you in the coming weeks?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Well, we are certainly continuing to process all the learnings that we’ve had from studying these domains. It’s really been a rich set of insights that we want to make sure we, kind of, fully take advantage of. And, you know, I think these hard questions and, you know, real opportunities to be thoughtful in these early days of AI governance are not, sort of, going away or being easily resolved soon. And so I think we continue to see value in really learning from others, thinking about what’s distinct in the AI context, but also what we can apply in terms of what other domains have learned.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; Well, Amanda, it has been such a special experience for me to help illuminate the work of the Office of Responsible AI and our team in Microsoft Research, and [MUSIC] it’s just really special to see all of the work that we’re doing to help set the standard for responsible development and deployment of AI. So thank you for joining us today, and thanks for your reflections and discussion.&lt;/p&gt;



&lt;p&gt;And to our listeners, thank you so much for joining us for the series. We really hope you enjoyed it!&amp;nbsp;To check out all of our episodes, visit aka.ms/AITestingandEvaluation&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, and if you want to learn more about how Microsoft approaches AI governance, you can visit microsoft.com/RAI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;See you next time!&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC FADES] &lt;/p&gt;

				&lt;/span&gt;
			&lt;/div&gt;
			&lt;button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button"&gt;
				Show more			&lt;/button&gt;
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;







&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;p&gt;[1]&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; Since the launch of the International Network of AI Safety Institutes, the UK renamed its institute the AI Security Institute&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/podcast/ai-testing-and-evaluation-reflections/</guid><pubDate>Mon, 21 Jul 2025 16:00:00 +0000</pubDate></item><item><title>OpenAI jumps gun on International Math Olympiad gold medal announcement (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/07/openai-jumps-gun-on-international-math-olympiad-gold-medal-announcement/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Non-math AI model reportedly solves proofs at human speeds, but early reveal roils community.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/toy_robot_math-640x360.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/toy_robot_math-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Juj Winn / Eduard Muzhevski via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Saturday, OpenAI researcher Alexander Wei announced that a new AI language model the company is researching has achieved gold medal-level performance on the International Mathematical Olympiad (IMO), matching a standard that fewer than 9 percent of human contestants reach each year. The announcement came despite an embargo request from IMO organizers asking AI companies to wait until July 28 to share their results.&lt;/p&gt;
&lt;p&gt;The experimental model reportedly tackled the contest's six proof-based problems under the same constraints as human competitors: 4.5 hours per session, with no Internet access or calculators allowed. However, several sources with inside knowledge of the process say that since OpenAI self-graded its IMO results, the legitimacy of the company's claim may be in question. OpenAI plans to publish the proofs and grading rubrics for public review.&lt;/p&gt;
&lt;p&gt;According to OpenAI, its achievement marks a departure from previous AI attempts at mathematical Olympiad problems, which relied on specialized theorem-proving systems that often exceeded human time limits. OpenAI says its model processed problems as plain text and generated natural-language proofs, operating like a standard language model rather than a purpose-built mathematical system.&lt;/p&gt;
&lt;p&gt;The announcement follows Google's July 2024 claim that its AlphaProof and AlphaGeometry 2 models earned a silver medal equivalent at the IMO—though Google's systems required up to three days per problem rather than the 4.5-hour human time limit and needed human assistance to translate problems into formal mathematical language.&lt;/p&gt;
&lt;p&gt;"Math is a proving ground for reasoning—structured, rigorous, and hard to fake," the company wrote in a statement sent to Ars Technica. "This shows that scalable, general-purpose methods can now outperform hand-tuned systems in tasks long seen as out of reach."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;While the company confirmed that its next major AI model, GPT-5, is "coming soon," it clarified that this current model is experimental. "The techniques will carry forward, but nothing with this level of capability will be released for a while," OpenAI says. It's likely that OpenAI needed to devote a great deal of computational resources (which means high cost) for this particular experiment, and that level of computation won't be typical of consumer-facing AI models in the near future.&lt;/p&gt;
&lt;h2&gt;Surprising results for a general-purpose AI model&lt;/h2&gt;
&lt;p&gt;OpenAI says that the research team behind the experimental AI model, led by Alex Wei with support from Sheryl Hsu and Noam Brown, hadn't initially planned to enter the competition but decided to evaluate their work after observing promising results in testing.&lt;/p&gt;
&lt;p&gt;"This wasn’t a system built for math. It’s the same kind of LLM we train for language, coding, and science—solving full proof-based problems under standard IMO constraints: 4.5 hours, no internet, no calculators," OpenAI said in a statement.&lt;/p&gt;
&lt;p&gt;OpenAI received problems that were freshly written by the IMO organizer and shared with several AI companies simultaneously. To validate the results, each solution reportedly underwent blind grading by a panel of three former IMO medalists organized by OpenAI, with unanimous consensus required for acceptance.&lt;/p&gt;
&lt;p&gt;However, in addition to the controversy over self-grading the results, OpenAI also annoyed the IMO community because its Saturday announcement appears to have violated the embargo agreement with the International Mathematical Olympiad. Harmonic, another AI company that participated in the competition, revealed in an X post on July 20 that "the IMO Board has asked us, along with the other leading AI companies that participated, to hold on releasing our results until Jul 28th."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The early announcement has prompted Google DeepMind, which had prepared its own IMO results for the agreed-upon date, to move up its own IMO-related announcement to later today. Harmonic plans to share its results as originally scheduled on July 28.&lt;/p&gt;
&lt;p&gt;In response to the controversy, OpenAI research scientist Noam Brown posted on X, "We weren't in touch with IMO. I spoke with one organizer before the post to let him know. He requested we wait until after the closing ceremony ends to respect the kids, and we did."&lt;/p&gt;
&lt;p&gt;However, an IMO coordinator told X user Mikhail Samin that OpenAI actually announced before the closing ceremony, contradicting Brown's claim. The coordinator called OpenAI's actions "rude and inappropriate," noting that OpenAI "wasn't one of the AI companies that cooperated with the IMO on testing their models."&lt;/p&gt;
&lt;h2&gt;Hard math since 1959&lt;/h2&gt;
&lt;p&gt;The International Mathematical Olympiad, which has been running since 1959, represents one of the most challenging tests of mathematical reasoning. More than 100 countries send six participants each, with contestants facing six proof-based problems across two 4.5-hour sessions. The problems typically require deep mathematical insight and creativity rather than raw computational power. You can see the exact problems in the 2025 Olympiad posted online.&lt;/p&gt;
&lt;p&gt;For example, problem one asks students to imagine a triangular grid of dots (like a triangular pegboard) and figure out how to cover all the dots using exactly n straight lines. The twist is that some lines are called "sunny"—these are the lines that don't run horizontally, vertically, or diagonally at a 45º angle. The challenge is to prove that no matter how big your triangle is, you can only ever create patterns with exactly 0, 1, or 3 sunny lines—never 2, never 4, never any other number.&lt;/p&gt;
&lt;p&gt;The timing of the OpenAI results surprised some prediction markets, which had assigned around an 18 percent probability to any AI system winning IMO gold by 2025. However, depending on what Google says this afternoon (and what others like Harmonic may release on July 28), OpenAI may not be the only AI company to have achieved these unexpected results.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Non-math AI model reportedly solves proofs at human speeds, but early reveal roils community.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/toy_robot_math-640x360.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/toy_robot_math-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Juj Winn / Eduard Muzhevski via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Saturday, OpenAI researcher Alexander Wei announced that a new AI language model the company is researching has achieved gold medal-level performance on the International Mathematical Olympiad (IMO), matching a standard that fewer than 9 percent of human contestants reach each year. The announcement came despite an embargo request from IMO organizers asking AI companies to wait until July 28 to share their results.&lt;/p&gt;
&lt;p&gt;The experimental model reportedly tackled the contest's six proof-based problems under the same constraints as human competitors: 4.5 hours per session, with no Internet access or calculators allowed. However, several sources with inside knowledge of the process say that since OpenAI self-graded its IMO results, the legitimacy of the company's claim may be in question. OpenAI plans to publish the proofs and grading rubrics for public review.&lt;/p&gt;
&lt;p&gt;According to OpenAI, its achievement marks a departure from previous AI attempts at mathematical Olympiad problems, which relied on specialized theorem-proving systems that often exceeded human time limits. OpenAI says its model processed problems as plain text and generated natural-language proofs, operating like a standard language model rather than a purpose-built mathematical system.&lt;/p&gt;
&lt;p&gt;The announcement follows Google's July 2024 claim that its AlphaProof and AlphaGeometry 2 models earned a silver medal equivalent at the IMO—though Google's systems required up to three days per problem rather than the 4.5-hour human time limit and needed human assistance to translate problems into formal mathematical language.&lt;/p&gt;
&lt;p&gt;"Math is a proving ground for reasoning—structured, rigorous, and hard to fake," the company wrote in a statement sent to Ars Technica. "This shows that scalable, general-purpose methods can now outperform hand-tuned systems in tasks long seen as out of reach."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;While the company confirmed that its next major AI model, GPT-5, is "coming soon," it clarified that this current model is experimental. "The techniques will carry forward, but nothing with this level of capability will be released for a while," OpenAI says. It's likely that OpenAI needed to devote a great deal of computational resources (which means high cost) for this particular experiment, and that level of computation won't be typical of consumer-facing AI models in the near future.&lt;/p&gt;
&lt;h2&gt;Surprising results for a general-purpose AI model&lt;/h2&gt;
&lt;p&gt;OpenAI says that the research team behind the experimental AI model, led by Alex Wei with support from Sheryl Hsu and Noam Brown, hadn't initially planned to enter the competition but decided to evaluate their work after observing promising results in testing.&lt;/p&gt;
&lt;p&gt;"This wasn’t a system built for math. It’s the same kind of LLM we train for language, coding, and science—solving full proof-based problems under standard IMO constraints: 4.5 hours, no internet, no calculators," OpenAI said in a statement.&lt;/p&gt;
&lt;p&gt;OpenAI received problems that were freshly written by the IMO organizer and shared with several AI companies simultaneously. To validate the results, each solution reportedly underwent blind grading by a panel of three former IMO medalists organized by OpenAI, with unanimous consensus required for acceptance.&lt;/p&gt;
&lt;p&gt;However, in addition to the controversy over self-grading the results, OpenAI also annoyed the IMO community because its Saturday announcement appears to have violated the embargo agreement with the International Mathematical Olympiad. Harmonic, another AI company that participated in the competition, revealed in an X post on July 20 that "the IMO Board has asked us, along with the other leading AI companies that participated, to hold on releasing our results until Jul 28th."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The early announcement has prompted Google DeepMind, which had prepared its own IMO results for the agreed-upon date, to move up its own IMO-related announcement to later today. Harmonic plans to share its results as originally scheduled on July 28.&lt;/p&gt;
&lt;p&gt;In response to the controversy, OpenAI research scientist Noam Brown posted on X, "We weren't in touch with IMO. I spoke with one organizer before the post to let him know. He requested we wait until after the closing ceremony ends to respect the kids, and we did."&lt;/p&gt;
&lt;p&gt;However, an IMO coordinator told X user Mikhail Samin that OpenAI actually announced before the closing ceremony, contradicting Brown's claim. The coordinator called OpenAI's actions "rude and inappropriate," noting that OpenAI "wasn't one of the AI companies that cooperated with the IMO on testing their models."&lt;/p&gt;
&lt;h2&gt;Hard math since 1959&lt;/h2&gt;
&lt;p&gt;The International Mathematical Olympiad, which has been running since 1959, represents one of the most challenging tests of mathematical reasoning. More than 100 countries send six participants each, with contestants facing six proof-based problems across two 4.5-hour sessions. The problems typically require deep mathematical insight and creativity rather than raw computational power. You can see the exact problems in the 2025 Olympiad posted online.&lt;/p&gt;
&lt;p&gt;For example, problem one asks students to imagine a triangular grid of dots (like a triangular pegboard) and figure out how to cover all the dots using exactly n straight lines. The twist is that some lines are called "sunny"—these are the lines that don't run horizontally, vertically, or diagonally at a 45º angle. The challenge is to prove that no matter how big your triangle is, you can only ever create patterns with exactly 0, 1, or 3 sunny lines—never 2, never 4, never any other number.&lt;/p&gt;
&lt;p&gt;The timing of the OpenAI results surprised some prediction markets, which had assigned around an 18 percent probability to any AI system winning IMO gold by 2025. However, depending on what Google says this afternoon (and what others like Harmonic may release on July 28), OpenAI may not be the only AI company to have achieved these unexpected results.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/07/openai-jumps-gun-on-international-math-olympiad-gold-medal-announcement/</guid><pubDate>Mon, 21 Jul 2025 16:02:04 +0000</pubDate></item><item><title>Advanced version of Gemini with Deep Think officially achieves gold-medal standard at the International Mathematical Olympiad (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://lh3.googleusercontent.com/Tu0v56OLL1Jy30PUoBzm_P8bZiFi3QhmKuxKF95j8ojB0B8uhHmG1QBRhLF09T5VWtIcIilGO-DXKTHDtGX11BB9OOiYr_w1tFs8bWVpqhIi3_isjg=w1200-h630-n-nu" /&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;We thank the International Mathematical Olympiad organization for their support.&lt;/p&gt;&lt;p&gt;Thang Luong led the overall technical direction of the advanced Gemini model with Deep Think for IMO and co-led with Edward Lockhart on the overall coordination of the IMO 2025 effort.&lt;/p&gt;&lt;p&gt;The IMO 2025 system would not have been possible without the following technical leads. Dawsen Hwang, Junehyuk Jung co-led training data and expert evaluation. Jonathan Lee, Nate Kushman, Pol Moreno, Yi Tay co-led the training of the advanced Gemini Deep Think model while Lei Yu led model evaluation. Golnaz Ghiazi, Garrett Bingham, Lalit Jain co-led Deep Think inference while Dawsen Hwang, Vincent Cohen-Addad co-led an enhanced inference approach.&lt;/p&gt;&lt;p&gt;The IMO 2025 system was also developed with key contributions from Theophane Weber, Ankesh Anand for modeling; Vinay Ramasesh, Andreas Kirsch, Jieming Mao, Zicheng Xu, Wilfried Bounsi, Vahab Mirrokni for inference; Hoang Nguyen, Fred Zhang, Mahan Malihi, Yangsibo Huang for training data.&lt;/p&gt;&lt;p&gt;We thank contributions from related teams and efforts. AlphaGeometry team with Yuri Chervonyi (lead), Trieu Trinh, Hoang Nguyen, Junsu Kim, Mirek Olšák, Marcelo Menegali, Xiaomeng Yang. Miklós Z. Horváth, Aja Huang, Goran Žužić for formal mathematics. We thank Fabian Pedregosa, Richard Song, Alex Zhai, Sara Javanmardi, YaGuang Li, Filipe Miguel de Almeida, Silvio Lattanzi, Ashkan Norouzi Fard, Tal Schuster, Honglu Fan, Xuezhi Wang, Aditi Mavalankar, Tom Schaul, Rosemary Ke for support and collaboration.&lt;/p&gt;&lt;p&gt;We especially thank other core members of the Deep Think team (Archit Sharma, Tong He, Shubha Raghvendra), the post-training effort (Tianhe Kevin Yu, Siamak Shakeri, Hanzhao Lin, Cosmo Du, Sid Lall), and Thinking Area research that the IMO 2025 system were built on.&lt;/p&gt;&lt;p&gt;This effort was advised by Quoc Le and Pushmeet Kohli, with program support from Kristen Chiafullo and Alex Goldin.&lt;/p&gt;&lt;p&gt;We’d also like to thank our experts for providing data and evaluations: Insuk Seo (lead), Jiwon Kang, Donghyun Kim, Junsu Kim, Jimin Kim, Seongbin Jeon, Yoonho Na, Seunghwan Lee, Jihoo Lee, Younghun Jo, Yongsuk Hur, Seongjae Park, Kyuhyeon Choi, Minkyu Choi, Su-Hyeok Moon, Seojin Kim, Yueun Lee, Taehun Kim, Jeeho Ryu, Seungwoo Lee, Dain Kim, Sanha Lee, Hyunwoo Choi, Aiden Jung, Youngbeom Jin, Jeonghyun Ahn, Junhwi Bae, Gyumin Kim, Nam Dung Tran, Cheng-Chiang Tsai, Kari Ragnarsson, Kiat Chuan Tan, Yahya Tabesh, Hamed Mahdavi, Azin Nazari, Xiangzhuo Ding, Chu-Lan Kao, Steven Creech, Tony Feng, Ciprian Manolescu.&lt;/p&gt;&lt;p&gt;And thanks to our serving and deployment experts: Emanuel Taropa, Charlie Chen, Joe Stanton, Cip Baetu, Alvin Abdagic, Federico Lebron, Ioana Mihailescu, Soheil Hassas Yeganeh, and Minh Gang.&lt;/p&gt;&lt;p&gt;Further thanks to Jessica Lo and Sajjad Zafar for their support for compute provision and management; Jane Labanowski, Andy Forbes, Sean Nakamoto for legal and logistics; and Omer Levy, Timothy Lillicrap, Jack Rae, Yifeng Lu, Heng-tze Cheng, Ed Chi, Vahab Mirrokni, Tulsee Doshi, Madhavi Sewak, Melvin Johnson, Koray Kavukcuoglu, Oriol Vinyals, Jeff Dean, Demis Hassabis, and Sergey Brin for their support and advice.&lt;/p&gt;&lt;p&gt;Finally, we thank Prof Gregor Dolinar from the IMO Board for the support and endorsement.&lt;/p&gt;&lt;p&gt;The IMO have confirmed that our submitted answers are complete and correct solutions. It is important to note that their review does not extend to validating our system, processes, or underlying model (see more).&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://lh3.googleusercontent.com/Tu0v56OLL1Jy30PUoBzm_P8bZiFi3QhmKuxKF95j8ojB0B8uhHmG1QBRhLF09T5VWtIcIilGO-DXKTHDtGX11BB9OOiYr_w1tFs8bWVpqhIi3_isjg=w1200-h630-n-nu" /&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;We thank the International Mathematical Olympiad organization for their support.&lt;/p&gt;&lt;p&gt;Thang Luong led the overall technical direction of the advanced Gemini model with Deep Think for IMO and co-led with Edward Lockhart on the overall coordination of the IMO 2025 effort.&lt;/p&gt;&lt;p&gt;The IMO 2025 system would not have been possible without the following technical leads. Dawsen Hwang, Junehyuk Jung co-led training data and expert evaluation. Jonathan Lee, Nate Kushman, Pol Moreno, Yi Tay co-led the training of the advanced Gemini Deep Think model while Lei Yu led model evaluation. Golnaz Ghiazi, Garrett Bingham, Lalit Jain co-led Deep Think inference while Dawsen Hwang, Vincent Cohen-Addad co-led an enhanced inference approach.&lt;/p&gt;&lt;p&gt;The IMO 2025 system was also developed with key contributions from Theophane Weber, Ankesh Anand for modeling; Vinay Ramasesh, Andreas Kirsch, Jieming Mao, Zicheng Xu, Wilfried Bounsi, Vahab Mirrokni for inference; Hoang Nguyen, Fred Zhang, Mahan Malihi, Yangsibo Huang for training data.&lt;/p&gt;&lt;p&gt;We thank contributions from related teams and efforts. AlphaGeometry team with Yuri Chervonyi (lead), Trieu Trinh, Hoang Nguyen, Junsu Kim, Mirek Olšák, Marcelo Menegali, Xiaomeng Yang. Miklós Z. Horváth, Aja Huang, Goran Žužić for formal mathematics. We thank Fabian Pedregosa, Richard Song, Alex Zhai, Sara Javanmardi, YaGuang Li, Filipe Miguel de Almeida, Silvio Lattanzi, Ashkan Norouzi Fard, Tal Schuster, Honglu Fan, Xuezhi Wang, Aditi Mavalankar, Tom Schaul, Rosemary Ke for support and collaboration.&lt;/p&gt;&lt;p&gt;We especially thank other core members of the Deep Think team (Archit Sharma, Tong He, Shubha Raghvendra), the post-training effort (Tianhe Kevin Yu, Siamak Shakeri, Hanzhao Lin, Cosmo Du, Sid Lall), and Thinking Area research that the IMO 2025 system were built on.&lt;/p&gt;&lt;p&gt;This effort was advised by Quoc Le and Pushmeet Kohli, with program support from Kristen Chiafullo and Alex Goldin.&lt;/p&gt;&lt;p&gt;We’d also like to thank our experts for providing data and evaluations: Insuk Seo (lead), Jiwon Kang, Donghyun Kim, Junsu Kim, Jimin Kim, Seongbin Jeon, Yoonho Na, Seunghwan Lee, Jihoo Lee, Younghun Jo, Yongsuk Hur, Seongjae Park, Kyuhyeon Choi, Minkyu Choi, Su-Hyeok Moon, Seojin Kim, Yueun Lee, Taehun Kim, Jeeho Ryu, Seungwoo Lee, Dain Kim, Sanha Lee, Hyunwoo Choi, Aiden Jung, Youngbeom Jin, Jeonghyun Ahn, Junhwi Bae, Gyumin Kim, Nam Dung Tran, Cheng-Chiang Tsai, Kari Ragnarsson, Kiat Chuan Tan, Yahya Tabesh, Hamed Mahdavi, Azin Nazari, Xiangzhuo Ding, Chu-Lan Kao, Steven Creech, Tony Feng, Ciprian Manolescu.&lt;/p&gt;&lt;p&gt;And thanks to our serving and deployment experts: Emanuel Taropa, Charlie Chen, Joe Stanton, Cip Baetu, Alvin Abdagic, Federico Lebron, Ioana Mihailescu, Soheil Hassas Yeganeh, and Minh Gang.&lt;/p&gt;&lt;p&gt;Further thanks to Jessica Lo and Sajjad Zafar for their support for compute provision and management; Jane Labanowski, Andy Forbes, Sean Nakamoto for legal and logistics; and Omer Levy, Timothy Lillicrap, Jack Rae, Yifeng Lu, Heng-tze Cheng, Ed Chi, Vahab Mirrokni, Tulsee Doshi, Madhavi Sewak, Melvin Johnson, Koray Kavukcuoglu, Oriol Vinyals, Jeff Dean, Demis Hassabis, and Sergey Brin for their support and advice.&lt;/p&gt;&lt;p&gt;Finally, we thank Prof Gregor Dolinar from the IMO Board for the support and endorsement.&lt;/p&gt;&lt;p&gt;The IMO have confirmed that our submitted answers are complete and correct solutions. It is important to note that their review does not extend to validating our system, processes, or underlying model (see more).&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/</guid><pubDate>Mon, 21 Jul 2025 16:30:00 +0000</pubDate></item><item><title>Experimental surgery performed by AI-driven surgical robot (AI – Ars Technica)</title><link>https://arstechnica.com/science/2025/07/experimental-surgery-performed-by-ai-driven-surgical-robot/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        In experimental surgery on pig organs, the robot performed well.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Image of a room in a hospital featuring two surgical robots and two control systems." class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2210992604-640x427.jpg" width="640" /&gt;
                  &lt;img alt="Image of a room in a hospital featuring two surgical robots and two control systems." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2210992604-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Surgical robots like these may not always need an actual surgeon.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Jacob King / PA Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Intuitive Surgical, an American biotechnology company, introduced DaVinci surgical robots in the late 1990s, and they became groundbreaking teleoperation equipment. Expert surgeons could operate on patients remotely, manipulating the robotic arms and their surgical tools based on a video feed from DaVinci’s built-in cameras and endoscopes.&lt;/p&gt;
&lt;p&gt;Now, John Hopkins University researchers put a ChatGPT-like AI in charge of a DaVinci robot and taught it to perform a gallbladder-removal surgery.&lt;/p&gt;
&lt;h2&gt;Kuka surgeries&lt;/h2&gt;
&lt;p&gt;The idea to put a computer behind the wheel of a surgical robot is not entirely new, but these had mostly relied on using pre-programmed actions. “The program told the robot exactly how to move and what to do. It worked like in these Kuka robotic arms, welding cars on factory floors,” says Ji Woong Kim, a robotics researcher who led the study on autonomous surgery. To improve on that, a team led by Axel Krieger, an assistant professor of mechanical engineering at John Hopkins University, built STAR: the Smart Tissue Autonomous Robot. In 2022, it successfully performed a surgery on a live pig.&lt;/p&gt;
&lt;p&gt;But even STAR couldn’t do it without specially marked tissues and a predetermined plan. STAR’s key difference was that its AI could make adjustments to this plan based on the feed from cameras.&lt;/p&gt;
&lt;p&gt;The new robot can do considerably more. “Our current work is much more flexible,” Kim says. “It is an AI that learns from demonstrations.” The new system is called SRT-H (Surgical Robot Transformer) and was developed by Kim and his colleagues, Krieger added.&lt;/p&gt;
&lt;p&gt;The first change they made was to the hardware. Instead of using a custom robot like STAR, the new work relied on the DaVinci robot, which has become a de facto industry standard in teleoperation surgeries, with over 10,000 units already deployed in hospitals worldwide. The second change was the software driving the system. It relied on two transformer models, the same architecture that powers ChatGPT. One was a high-level policy module, which was responsible for task planning and ensuring the procedure went smoothly. The low-level module was responsible for executing the tasks issued by the high-level module, translating its instructions into specific trajectories for the robotic arms.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;When the system was ready, Kim’s team put it through a training phase that looked a bit like mentoring a novice human doctor.&lt;/p&gt;
&lt;h2&gt;Imitation learning&lt;/h2&gt;
&lt;p&gt;The procedure Kim chose for the robot to master was cholecystectomy, a surgical gallbladder removal routinely performed in US hospitals (roughly 700,000 times a year). “The objective is to remove the tubes connecting the gallbladder to other organs without causing the internal fluids to flow out,” Kim explained. To make that happen, a surgeon has to place three clips on the cystic duct (the first tube), cut&amp;nbsp;it, and then clip and cut the cystic artery (the second tube) in a similar way.&lt;/p&gt;
&lt;p&gt;Kim’s team broke this procedure down into 17 steps, sourced lots of porcine gallbladder and liver samples from pig cadavers to experiment on, and had a trained research assistant operate a DaVinci robot, performing the procedure over and over again to build the training data set for the robot.&lt;/p&gt;
&lt;p&gt;Algorithms that would power the SRT-H were trained on over 17 hours of video captured from the DaVinci endoscope and cameras installed on its robotic arms. This video feed was complemented by the kinematics data—the exact motions made by the robotic arms—and natural language annotations.&lt;/p&gt;
&lt;p&gt;Based on this data, Kim’s robot learned to perform a cholecystectomy with 100 percent success rate when operating on samples it has not been trained on. It could also accept human feedback in natural language—simple tips like “move your arm a bit to the left” or “put the clip a bit higher.” These are the sorts of hints a mentoring surgeon would give to a student and, in a similar way, SRT-H could learn from them over time.&lt;/p&gt;
&lt;p&gt;“You can take any kind of surgery, not just this one, train the robot in the same way, and it will be able to perform that surgery,” Kim says. SRT-H was also robust to differences in anatomy between samples, other tissue getting in the way, and imperfect imagery. It could even recover from all the tiny mistakes it was making during the training process. Compared to an expert human surgeon performing the same procedure, the robot was equally precise, although a bit slower.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But it wasn’t robust against corporate affairs.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Robotic secrets&lt;/h2&gt;
&lt;p&gt;To move from operating on pig cadaver samples to live pigs and then, potentially, to humans, robots like SRT-H need training data that is extremely hard to come by. Intuitive Surgical is apparently OK with releasing the video feed data from the DaVinci robots, but the company does not release the kinematics data. And that’s data that Kim says is necessary for training the algorithms. “I know people at Intuitive Surgical headquarters, and I’ve been talking to them,” Kim says. “I’ve been begging them to give us the data. They did not agree.”&lt;/p&gt;
&lt;p&gt;The explanation Intuitive Surgical leadership offered for restricting access to the kinematics data, according to Kim, was they were worried about the competition reverse-engineering the mechanics of their robot. “It’s really the upper management who is not up to speed with AI,” Kim argued. “They don’t realize the potential of these things. Their engineers, every scientist, they want to open-source the data. It’s just their legal department is very conservative.”&lt;/p&gt;
&lt;p&gt;But he already sees a way around this problem. “We can start with attaching motion-tracking sensors to manual surgical tools, and get the kinematics data this way,” Kim told Ars. Then, the movements of these tools guided by the hands of expert human surgeons could be recreated by conventional robotic arms like the ones used in STAR.&lt;/p&gt;
&lt;p&gt;And then, Kim thinks, we can go even more sci-fi than that. “I’m currently at Stanford, and I’m very involved in a humanoid robotics project—building a general-purpose model. And one of the possible applications is in the operating room,” Kim says.&lt;/p&gt;
&lt;p&gt;Science Robotics, 2025. &amp;nbsp;DOI: 10.1126/scirobotics.adt5254&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        In experimental surgery on pig organs, the robot performed well.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Image of a room in a hospital featuring two surgical robots and two control systems." class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2210992604-640x427.jpg" width="640" /&gt;
                  &lt;img alt="Image of a room in a hospital featuring two surgical robots and two control systems." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2210992604-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Surgical robots like these may not always need an actual surgeon.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Jacob King / PA Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Intuitive Surgical, an American biotechnology company, introduced DaVinci surgical robots in the late 1990s, and they became groundbreaking teleoperation equipment. Expert surgeons could operate on patients remotely, manipulating the robotic arms and their surgical tools based on a video feed from DaVinci’s built-in cameras and endoscopes.&lt;/p&gt;
&lt;p&gt;Now, John Hopkins University researchers put a ChatGPT-like AI in charge of a DaVinci robot and taught it to perform a gallbladder-removal surgery.&lt;/p&gt;
&lt;h2&gt;Kuka surgeries&lt;/h2&gt;
&lt;p&gt;The idea to put a computer behind the wheel of a surgical robot is not entirely new, but these had mostly relied on using pre-programmed actions. “The program told the robot exactly how to move and what to do. It worked like in these Kuka robotic arms, welding cars on factory floors,” says Ji Woong Kim, a robotics researcher who led the study on autonomous surgery. To improve on that, a team led by Axel Krieger, an assistant professor of mechanical engineering at John Hopkins University, built STAR: the Smart Tissue Autonomous Robot. In 2022, it successfully performed a surgery on a live pig.&lt;/p&gt;
&lt;p&gt;But even STAR couldn’t do it without specially marked tissues and a predetermined plan. STAR’s key difference was that its AI could make adjustments to this plan based on the feed from cameras.&lt;/p&gt;
&lt;p&gt;The new robot can do considerably more. “Our current work is much more flexible,” Kim says. “It is an AI that learns from demonstrations.” The new system is called SRT-H (Surgical Robot Transformer) and was developed by Kim and his colleagues, Krieger added.&lt;/p&gt;
&lt;p&gt;The first change they made was to the hardware. Instead of using a custom robot like STAR, the new work relied on the DaVinci robot, which has become a de facto industry standard in teleoperation surgeries, with over 10,000 units already deployed in hospitals worldwide. The second change was the software driving the system. It relied on two transformer models, the same architecture that powers ChatGPT. One was a high-level policy module, which was responsible for task planning and ensuring the procedure went smoothly. The low-level module was responsible for executing the tasks issued by the high-level module, translating its instructions into specific trajectories for the robotic arms.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;When the system was ready, Kim’s team put it through a training phase that looked a bit like mentoring a novice human doctor.&lt;/p&gt;
&lt;h2&gt;Imitation learning&lt;/h2&gt;
&lt;p&gt;The procedure Kim chose for the robot to master was cholecystectomy, a surgical gallbladder removal routinely performed in US hospitals (roughly 700,000 times a year). “The objective is to remove the tubes connecting the gallbladder to other organs without causing the internal fluids to flow out,” Kim explained. To make that happen, a surgeon has to place three clips on the cystic duct (the first tube), cut&amp;nbsp;it, and then clip and cut the cystic artery (the second tube) in a similar way.&lt;/p&gt;
&lt;p&gt;Kim’s team broke this procedure down into 17 steps, sourced lots of porcine gallbladder and liver samples from pig cadavers to experiment on, and had a trained research assistant operate a DaVinci robot, performing the procedure over and over again to build the training data set for the robot.&lt;/p&gt;
&lt;p&gt;Algorithms that would power the SRT-H were trained on over 17 hours of video captured from the DaVinci endoscope and cameras installed on its robotic arms. This video feed was complemented by the kinematics data—the exact motions made by the robotic arms—and natural language annotations.&lt;/p&gt;
&lt;p&gt;Based on this data, Kim’s robot learned to perform a cholecystectomy with 100 percent success rate when operating on samples it has not been trained on. It could also accept human feedback in natural language—simple tips like “move your arm a bit to the left” or “put the clip a bit higher.” These are the sorts of hints a mentoring surgeon would give to a student and, in a similar way, SRT-H could learn from them over time.&lt;/p&gt;
&lt;p&gt;“You can take any kind of surgery, not just this one, train the robot in the same way, and it will be able to perform that surgery,” Kim says. SRT-H was also robust to differences in anatomy between samples, other tissue getting in the way, and imperfect imagery. It could even recover from all the tiny mistakes it was making during the training process. Compared to an expert human surgeon performing the same procedure, the robot was equally precise, although a bit slower.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But it wasn’t robust against corporate affairs.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Robotic secrets&lt;/h2&gt;
&lt;p&gt;To move from operating on pig cadaver samples to live pigs and then, potentially, to humans, robots like SRT-H need training data that is extremely hard to come by. Intuitive Surgical is apparently OK with releasing the video feed data from the DaVinci robots, but the company does not release the kinematics data. And that’s data that Kim says is necessary for training the algorithms. “I know people at Intuitive Surgical headquarters, and I’ve been talking to them,” Kim says. “I’ve been begging them to give us the data. They did not agree.”&lt;/p&gt;
&lt;p&gt;The explanation Intuitive Surgical leadership offered for restricting access to the kinematics data, according to Kim, was they were worried about the competition reverse-engineering the mechanics of their robot. “It’s really the upper management who is not up to speed with AI,” Kim argued. “They don’t realize the potential of these things. Their engineers, every scientist, they want to open-source the data. It’s just their legal department is very conservative.”&lt;/p&gt;
&lt;p&gt;But he already sees a way around this problem. “We can start with attaching motion-tracking sensors to manual surgical tools, and get the kinematics data this way,” Kim told Ars. Then, the movements of these tools guided by the hands of expert human surgeons could be recreated by conventional robotic arms like the ones used in STAR.&lt;/p&gt;
&lt;p&gt;And then, Kim thinks, we can go even more sci-fi than that. “I’m currently at Stanford, and I’m very involved in a humanoid robotics project—building a general-purpose model. And one of the possible applications is in the operating room,” Kim says.&lt;/p&gt;
&lt;p&gt;Science Robotics, 2025. &amp;nbsp;DOI: 10.1126/scirobotics.adt5254&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/science/2025/07/experimental-surgery-performed-by-ai-driven-surgical-robot/</guid><pubDate>Mon, 21 Jul 2025 17:15:00 +0000</pubDate></item><item><title>[NEW] MIT Learn offers “a whole new front door to the Institute” (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/mit-learn-offers-whole-new-front-door-institute-0721</link><description>&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;In 2001, MIT became the first higher education institution to provide educational resources for free to anyone in the world. Fast forward 24 years: The Institute has now launched a dynamic AI-enabled website for its non-degree learning opportunities, making it easier for learners around the world to discover the courses and resources available on MIT’s various learning platforms.&lt;/p&gt;&lt;p dir="ltr"&gt;MIT Learn enables learners to access more than 12,700 educational resources — including introductory and advanced courses, courseware, videos, podcasts, and more — from departments across the Institute. MIT Learn is designed to seamlessly connect the existing Institute’s learning platforms in one place.&lt;/p&gt;&lt;p dir="ltr"&gt;“With MIT Learn, we’re opening access to MIT’s digital learning opportunities for millions around the world,” says Dimitris Bertsimas, vice provost for open learning. “MIT Learn elevates learning with personalized recommendations powered by AI, guiding each learner toward deeper understanding. It is a stepping stone toward a broader vision of making these opportunities even more accessible to global learners through one unified learning platform.”&lt;/p&gt;&lt;p dir="ltr"&gt;The goal for MIT Learn is twofold: to allow learners to find what they want to fulfill their curiosity, and to enable learners to develop a long-term relationship with MIT as a source of educational experiences.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“By fostering long-term connections between learners and MIT, we not only provide a pathway to continued learning, but also advance MIT’s mission to disseminate knowledge globally,” says Ferdi Alimadhi, chief technology officer for MIT Open Learning and the lead of the MIT Learn project. “With this initial launch of MIT Learn, we’re introducing AI-powered features that leverage emerging technologies to help learners discover the right content, engage with it more deeply, and stay supported as they shape their own educational journeys.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;With its sophisticated search, browse, and discovery capability, MIT Learn allows learners to explore topics without having to understand MIT’s organizational structure or know the names of departments and programs. An AI-powered recommendation feature called “Ask Tim” complements the site’s traditional search and browsing tools, helping learners quickly find courses and resources aligned with their personal and professional goals. Learners can also prompt “Ask Tim” for a summary of a course’s structure, topics, and expectations, leading to more-informed decisions before enrolling.&lt;/p&gt;&lt;p dir="ltr"&gt;In select offerings, such as Molecular Biology: DNA Replication and Repair, Genetics: The Fundamentals, and Cell Biology: Transport and Signaling, learners can interact with an AI assistant by asking questions about a lecture, requesting flashcards of key concepts, and obtaining instant summaries. These select offerings also feature an AI tutor to support learners as they work through problem sets, guiding them toward the next step without giving away the answers. These features, Alimadhi says, are being introduced in a limited set of courses and modules to allow the MIT Open Learning team to gather insights and improve the learning experience before expanding more broadly.&lt;/p&gt;&lt;p dir="ltr"&gt;“MIT Learn is a whole new front door to the Institute,” says Christopher Capozzola, senior associate dean for open learning, who worked with faculty across the Institute on the project. “Just as the Kendall Square renovations transformed the way that people interact with our physical campus, MIT Learn transforms how people engage with what we offer digitally.”&lt;/p&gt;        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-youtube-video paragraph--view-mode--default"&gt;
          

            
   

  &lt;div class="news-article--inline-video"&gt;
        &lt;div class="news-article--inline-video--container"&gt;

                             &lt;div class="news-article--inline-video--cover-image"&gt;
                      &lt;img alt="Video thumbnail" height="480" src="https://i1.ytimg.com/vi/lHp6sYSHfKU/maxresdefault.jpg" width="640" /&gt;
                    
          Play video
        &lt;/div&gt;
                

                      &lt;/div&gt;
        &lt;div class="news-article--inline-video--caption"&gt;
      

            Introducing MIT Learn: Your new destination for lifelong learning        

    &lt;/div&gt;
          &lt;/div&gt;  
        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;Learners who choose to create an account on MIT Learn receive personalized course recommendations and can create and curate lists of educational resources, follow their specific areas of interest, and receive notifications when new MIT content is available. They can also personalize their learning experience based on their specific interests and choose the format that is best suited to them.&lt;/p&gt;&lt;p dir="ltr"&gt;"From anywhere and for anyone, MIT Learn makes lifelong learning more accessible and personalized, building on the Institute’s decades of global leadership in open learning,” says MIT Provost Anantha Chandrakasan.&lt;/p&gt;&lt;p dir="ltr"&gt;MIT Learn was designed to account for a learner’s evolving needs throughout their learning journey. It highlights supplemental study materials for middle schoolers, high schoolers, and college students, upskilling opportunities for early-career professionals, reskilling programs for those considering a career shift, and resources for educators.&lt;/p&gt;&lt;p dir="ltr"&gt;“MIT has an amazing collection of learning opportunities, covering a wide range of formats,” says Eric Grimson, chancellor for academic advancement, who oversaw the initial development of MIT Learn during his time as interim vice president for open learning. “The sheer size of that collection can be daunting, so creating a platform that brings all of those offerings together, in an easily searchable framework, greatly enhances our ability to serve learners.”&lt;/p&gt;&lt;p dir="ltr"&gt;According to Peter Hirst, senior associate dean for executive education at MIT Sloan School of Management, one of the Institute's incredible strengths is its sheer volume and diversity of expertise, research, and learning opportunities. But it can be challenging to discover and follow all those opportunities — even for people who are immersed in the on-campus experience. MIT Learn, he says, is a solution to this problem.&lt;/p&gt;&lt;p dir="ltr"&gt;“MIT Learn gathers all the knowledge and learning resources offered across all of MIT into a learner-friendly, curatable repository that enables anyone and everyone, whatever their interests or learning needs,&amp;nbsp;to explore and engage in the wide range of learning resources and public certificate programs that MIT has to offer and that can help them achieve their goals,” Hirst says.&lt;/p&gt;&lt;p dir="ltr"&gt;MIT Learn was spearheaded by MIT Open Learning, which aims to transform teaching and learning on and off the Institute’s campus. MIT Learn was developed with the direction of former provost Cynthia Barnhart, and in cooperation with Sloan Executive Education and Professional Education. During the design phase, OpenCourseWare Faculty Advisory Committee Chair Michael Short and&amp;nbsp;&lt;em&gt;MITx&lt;/em&gt;&amp;nbsp;Faculty Advisory Committee Chair Caspar Hare contributed key insights, along with other numerous faculty involved with Open Learning’s product offerings, including OpenCourseWare,&amp;nbsp;&lt;em&gt;MITx&lt;/em&gt;, and MicroMasters programs. MIT Learn is also informed by the insights of the Ad Hoc Committee on&amp;nbsp;&lt;em&gt;MITx&lt;/em&gt;&amp;nbsp;and&amp;nbsp;&lt;em&gt;MITx&lt;/em&gt; Online.&lt;/p&gt;&lt;p dir="ltr"&gt;“For over 20 years, MIT staff and faculty have been creating a wealth of online resources, from lecture videos to practice problems, and from single online courses to entire credential-earning programs,” says Sara Fisher Ellison, a member of the Ad Hoc Committee on&amp;nbsp;&lt;em&gt;MITx&lt;/em&gt;&amp;nbsp;and&amp;nbsp;&lt;em&gt;MITx&lt;/em&gt; Online and the faculty lead for the online&amp;nbsp;&lt;em&gt;MITx&lt;/em&gt; MicroMasters Program in Data, Economics, and Design of Policy. “Making these resources findable, searchable, and broadly available is a natural extension of MIT’s core educational mission. MIT Learn is a big, important step in that direction. We are excited for the world to see what we have to offer.”&lt;/p&gt;&lt;p dir="ltr"&gt;Looking ahead, MIT Learn will also feature selected content from the MIT Press. As MIT Learn continues to grow, Open Learning is exploring collaborations with departments across the Institute with the goal of offering the fullest possible range of educational materials from MIT to learners around the world.&lt;/p&gt;&lt;p dir="ltr"&gt;“MIT Learn is the latest step in a long tradition of the Institute providing innovative ways for learners to access knowledge,” Barnhart says. “This AI-enabled platform delivers on the Institute’s commitment to help people launch into learning journeys that can unlock life-changing opportunities.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;In 2001, MIT became the first higher education institution to provide educational resources for free to anyone in the world. Fast forward 24 years: The Institute has now launched a dynamic AI-enabled website for its non-degree learning opportunities, making it easier for learners around the world to discover the courses and resources available on MIT’s various learning platforms.&lt;/p&gt;&lt;p dir="ltr"&gt;MIT Learn enables learners to access more than 12,700 educational resources — including introductory and advanced courses, courseware, videos, podcasts, and more — from departments across the Institute. MIT Learn is designed to seamlessly connect the existing Institute’s learning platforms in one place.&lt;/p&gt;&lt;p dir="ltr"&gt;“With MIT Learn, we’re opening access to MIT’s digital learning opportunities for millions around the world,” says Dimitris Bertsimas, vice provost for open learning. “MIT Learn elevates learning with personalized recommendations powered by AI, guiding each learner toward deeper understanding. It is a stepping stone toward a broader vision of making these opportunities even more accessible to global learners through one unified learning platform.”&lt;/p&gt;&lt;p dir="ltr"&gt;The goal for MIT Learn is twofold: to allow learners to find what they want to fulfill their curiosity, and to enable learners to develop a long-term relationship with MIT as a source of educational experiences.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“By fostering long-term connections between learners and MIT, we not only provide a pathway to continued learning, but also advance MIT’s mission to disseminate knowledge globally,” says Ferdi Alimadhi, chief technology officer for MIT Open Learning and the lead of the MIT Learn project. “With this initial launch of MIT Learn, we’re introducing AI-powered features that leverage emerging technologies to help learners discover the right content, engage with it more deeply, and stay supported as they shape their own educational journeys.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;With its sophisticated search, browse, and discovery capability, MIT Learn allows learners to explore topics without having to understand MIT’s organizational structure or know the names of departments and programs. An AI-powered recommendation feature called “Ask Tim” complements the site’s traditional search and browsing tools, helping learners quickly find courses and resources aligned with their personal and professional goals. Learners can also prompt “Ask Tim” for a summary of a course’s structure, topics, and expectations, leading to more-informed decisions before enrolling.&lt;/p&gt;&lt;p dir="ltr"&gt;In select offerings, such as Molecular Biology: DNA Replication and Repair, Genetics: The Fundamentals, and Cell Biology: Transport and Signaling, learners can interact with an AI assistant by asking questions about a lecture, requesting flashcards of key concepts, and obtaining instant summaries. These select offerings also feature an AI tutor to support learners as they work through problem sets, guiding them toward the next step without giving away the answers. These features, Alimadhi says, are being introduced in a limited set of courses and modules to allow the MIT Open Learning team to gather insights and improve the learning experience before expanding more broadly.&lt;/p&gt;&lt;p dir="ltr"&gt;“MIT Learn is a whole new front door to the Institute,” says Christopher Capozzola, senior associate dean for open learning, who worked with faculty across the Institute on the project. “Just as the Kendall Square renovations transformed the way that people interact with our physical campus, MIT Learn transforms how people engage with what we offer digitally.”&lt;/p&gt;        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-youtube-video paragraph--view-mode--default"&gt;
          

            
   

  &lt;div class="news-article--inline-video"&gt;
        &lt;div class="news-article--inline-video--container"&gt;

                             &lt;div class="news-article--inline-video--cover-image"&gt;
                      &lt;img alt="Video thumbnail" height="480" src="https://i1.ytimg.com/vi/lHp6sYSHfKU/maxresdefault.jpg" width="640" /&gt;
                    
          Play video
        &lt;/div&gt;
                

                      &lt;/div&gt;
        &lt;div class="news-article--inline-video--caption"&gt;
      

            Introducing MIT Learn: Your new destination for lifelong learning        

    &lt;/div&gt;
          &lt;/div&gt;  
        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;Learners who choose to create an account on MIT Learn receive personalized course recommendations and can create and curate lists of educational resources, follow their specific areas of interest, and receive notifications when new MIT content is available. They can also personalize their learning experience based on their specific interests and choose the format that is best suited to them.&lt;/p&gt;&lt;p dir="ltr"&gt;"From anywhere and for anyone, MIT Learn makes lifelong learning more accessible and personalized, building on the Institute’s decades of global leadership in open learning,” says MIT Provost Anantha Chandrakasan.&lt;/p&gt;&lt;p dir="ltr"&gt;MIT Learn was designed to account for a learner’s evolving needs throughout their learning journey. It highlights supplemental study materials for middle schoolers, high schoolers, and college students, upskilling opportunities for early-career professionals, reskilling programs for those considering a career shift, and resources for educators.&lt;/p&gt;&lt;p dir="ltr"&gt;“MIT has an amazing collection of learning opportunities, covering a wide range of formats,” says Eric Grimson, chancellor for academic advancement, who oversaw the initial development of MIT Learn during his time as interim vice president for open learning. “The sheer size of that collection can be daunting, so creating a platform that brings all of those offerings together, in an easily searchable framework, greatly enhances our ability to serve learners.”&lt;/p&gt;&lt;p dir="ltr"&gt;According to Peter Hirst, senior associate dean for executive education at MIT Sloan School of Management, one of the Institute's incredible strengths is its sheer volume and diversity of expertise, research, and learning opportunities. But it can be challenging to discover and follow all those opportunities — even for people who are immersed in the on-campus experience. MIT Learn, he says, is a solution to this problem.&lt;/p&gt;&lt;p dir="ltr"&gt;“MIT Learn gathers all the knowledge and learning resources offered across all of MIT into a learner-friendly, curatable repository that enables anyone and everyone, whatever their interests or learning needs,&amp;nbsp;to explore and engage in the wide range of learning resources and public certificate programs that MIT has to offer and that can help them achieve their goals,” Hirst says.&lt;/p&gt;&lt;p dir="ltr"&gt;MIT Learn was spearheaded by MIT Open Learning, which aims to transform teaching and learning on and off the Institute’s campus. MIT Learn was developed with the direction of former provost Cynthia Barnhart, and in cooperation with Sloan Executive Education and Professional Education. During the design phase, OpenCourseWare Faculty Advisory Committee Chair Michael Short and&amp;nbsp;&lt;em&gt;MITx&lt;/em&gt;&amp;nbsp;Faculty Advisory Committee Chair Caspar Hare contributed key insights, along with other numerous faculty involved with Open Learning’s product offerings, including OpenCourseWare,&amp;nbsp;&lt;em&gt;MITx&lt;/em&gt;, and MicroMasters programs. MIT Learn is also informed by the insights of the Ad Hoc Committee on&amp;nbsp;&lt;em&gt;MITx&lt;/em&gt;&amp;nbsp;and&amp;nbsp;&lt;em&gt;MITx&lt;/em&gt; Online.&lt;/p&gt;&lt;p dir="ltr"&gt;“For over 20 years, MIT staff and faculty have been creating a wealth of online resources, from lecture videos to practice problems, and from single online courses to entire credential-earning programs,” says Sara Fisher Ellison, a member of the Ad Hoc Committee on&amp;nbsp;&lt;em&gt;MITx&lt;/em&gt;&amp;nbsp;and&amp;nbsp;&lt;em&gt;MITx&lt;/em&gt; Online and the faculty lead for the online&amp;nbsp;&lt;em&gt;MITx&lt;/em&gt; MicroMasters Program in Data, Economics, and Design of Policy. “Making these resources findable, searchable, and broadly available is a natural extension of MIT’s core educational mission. MIT Learn is a big, important step in that direction. We are excited for the world to see what we have to offer.”&lt;/p&gt;&lt;p dir="ltr"&gt;Looking ahead, MIT Learn will also feature selected content from the MIT Press. As MIT Learn continues to grow, Open Learning is exploring collaborations with departments across the Institute with the goal of offering the fullest possible range of educational materials from MIT to learners around the world.&lt;/p&gt;&lt;p dir="ltr"&gt;“MIT Learn is the latest step in a long tradition of the Institute providing innovative ways for learners to access knowledge,” Barnhart says. “This AI-enabled platform delivers on the Institute’s commitment to help people launch into learning journeys that can unlock life-changing opportunities.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/mit-learn-offers-whole-new-front-door-institute-0721</guid><pubDate>Mon, 21 Jul 2025 19:00:00 +0000</pubDate></item><item><title>[NEW] A new way to edit or generate images (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/new-way-edit-or-generate-images-0721</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202507/mit-token-opt3.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;AI image generation — which relies on neural networks to create new images from a variety of inputs, including text prompts — is projected to become a billion-dollar industry by the end of this decade. Even with today’s technology, if you wanted to make a fanciful picture of, say, a friend planting a flag on Mars or heedlessly flying into a black hole, it could take less than a second. However, before they can perform tasks like that, image generators are commonly trained on massive datasets containing millions of images that are often paired with associated text. Training these generative models can be an arduous chore that takes weeks or months, consuming vast computational resources in the process.&lt;/p&gt;&lt;p&gt;But what if it were possible to generate images through AI methods without using a generator at all? That real possibility, along with other intriguing ideas, was described in a research paper presented at the International Conference on Machine Learning (ICML 2025), which was held in Vancouver, British Columbia, earlier this summer. The paper, describing novel techniques for manipulating and generating images, was written by Lukas Lao Beyer, a graduate student researcher in MIT’s Laboratory for Information and Decision Systems (LIDS); Tianhong Li, a postdoc at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL); Xinlei Chen of Facebook AI Research; Sertac Karaman, an MIT professor of aeronautics and astronautics and the director of LIDS; and Kaiming He, an MIT associate professor of electrical engineering and computer science.&lt;/p&gt;&lt;p&gt;This group effort had its origins in a class project for a graduate seminar on deep generative models that Lao Beyer took last fall. In conversations during the semester, it became apparent to both Lao Beyer and He, who taught the seminar, that this research had real potential, which went far beyond the confines of a typical homework assignment. Other collaborators were soon brought into the endeavor.&lt;/p&gt;&lt;p&gt;The starting point for Lao Beyer’s inquiry was a June 2024 paper, written by researchers from the Technical University of Munich and the Chinese company ByteDance, which introduced a new way of representing visual information called a one-dimensional tokenizer. With this device, which is also a kind of neural network, a 256x256-pixel image can be translated into a sequence of just 32 numbers, called tokens. “I wanted to understand how such a high level of compression could be achieved, and what the tokens themselves actually represented,” says Lao Beyer.&lt;/p&gt;&lt;p&gt;The previous generation of tokenizers would typically break up the same image into an array of 16x16 tokens — with each token encapsulating information, in highly condensed form, that corresponds to a specific portion of the original image. The new 1D tokenizers can encode an image more efficiently, using far fewer tokens overall, and these tokens are able to capture information about the entire image, not just a single quadrant. Each of these tokens, moreover, is a 12-digit number consisting of 1s and 0s, allowing for 2&lt;sup&gt;12&lt;/sup&gt; (or about 4,000) possibilities altogether. “It’s like a vocabulary of 4,000 words that makes up an abstract, hidden language spoken by the computer,” He explains. “It’s not like a human language, but we can still try to find out what it means.”&lt;/p&gt;&lt;p&gt;That’s exactly what Lao Beyer had initially set out to explore — work that provided the seed for the ICML 2025 paper. The approach he took was pretty straightforward. If you want to find out what a particular token does, Lao Beyer says, “you can just take it out, swap in some random value, and see if there is a recognizable change in the output.” Replacing one token, he found, changes the image quality, turning a low-resolution image into a high-resolution image or vice versa. Another token affected the blurriness in the background, while another still influenced the brightness. He also found a token that’s related to the “pose,” meaning that, in the image of a robin, for instance, the bird’s head might shift from right to left.&lt;/p&gt;&lt;p&gt;“This was a never-before-seen result, as no one had observed visually identifiable changes from manipulating tokens,” Lao Beyer says. The finding raised the possibility of a new approach to editing images. And the MIT group has shown, in fact, how this process can be streamlined and automated, so that tokens don’t have to be modified by hand, one at a time.&lt;/p&gt;&lt;p&gt;He and his colleagues achieved an even more consequential result involving image generation. A system capable of generating images normally requires a tokenizer, which compresses and encodes visual data, along with a generator that can combine and arrange these compact representations in order to create novel images. The MIT researchers found a way to create images without using a generator at all. Their new approach makes use of a 1D tokenizer and a so-called detokenizer (also known as a decoder), which can reconstruct an image from a string of tokens. However, with guidance provided by an off-the-shelf neural network called CLIP —&amp;nbsp;which cannot generate images on its own, but can measure how well a given image matches a certain text prompt&amp;nbsp;— the team was able to convert an image of a red panda, for example, into a tiger. In addition, they could create images of a tiger, or any other desired form, starting completely from scratch — from a situation in which all the tokens are initially assigned random values (and then iteratively tweaked so that the reconstructed image increasingly matches the desired text prompt).&lt;/p&gt;&lt;p&gt;The group demonstrated that with this same setup — relying on a tokenizer and detokenizer, but no generator — they could also do “inpainting,” which means filling in parts of images that had somehow been blotted out. Avoiding the use of a generator for certain tasks could lead to a significant reduction in computational costs because generators, as mentioned, normally require extensive training.&lt;/p&gt;&lt;p&gt;What might seem odd about this team’s contributions, He explains, “is that we didn’t invent anything new. We didn’t invent a 1D tokenizer, and we didn’t invent the CLIP model, either. But we did discover that new capabilities can arise when you put all these pieces together.”&lt;/p&gt;&lt;p&gt;“This work redefines the role of tokenizers,” comments&amp;nbsp;Saining Xie, a computer scientist at New York University. “It shows that&amp;nbsp;image tokenizers — tools usually used just to compress images — can actually do a lot more. The fact that a simple (but highly compressed) 1D tokenizer can handle tasks like inpainting or text-guided editing, without needing to train a full-blown generative model, is pretty surprising.”&lt;/p&gt;&lt;p&gt;Zhuang Liu of Princeton University agrees, saying that the work of the MIT group&amp;nbsp;“shows that we can generate and manipulate the images in a way that is much easier than we previously thought. Basically, it demonstrates that image generation can be a byproduct of a very effective image compressor, potentially reducing the cost of generating images several-fold.”&lt;/p&gt;&lt;p&gt;There could be many applications outside the field of computer vision, Karaman suggests. “For instance,&amp;nbsp;we could consider tokenizing the actions of robots or self-driving cars in the same way, which may rapidly broaden the impact of this work.”&lt;/p&gt;&lt;p&gt;Lao Beyer is thinking along similar lines,&amp;nbsp;noting that the&amp;nbsp;extreme amount of compression afforded by 1D tokenizers allows you to do “some amazing things,” which could be applied to other fields. For example, in the area of self-driving cars, which is one of his research interests, the tokens could represent, instead of images, the different routes that a vehicle might take.&lt;/p&gt;&lt;p&gt;Xie is also intrigued by the applications that may come from these innovative ideas. “There are some really cool use cases this could unlock,” he says.&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202507/mit-token-opt3.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;AI image generation — which relies on neural networks to create new images from a variety of inputs, including text prompts — is projected to become a billion-dollar industry by the end of this decade. Even with today’s technology, if you wanted to make a fanciful picture of, say, a friend planting a flag on Mars or heedlessly flying into a black hole, it could take less than a second. However, before they can perform tasks like that, image generators are commonly trained on massive datasets containing millions of images that are often paired with associated text. Training these generative models can be an arduous chore that takes weeks or months, consuming vast computational resources in the process.&lt;/p&gt;&lt;p&gt;But what if it were possible to generate images through AI methods without using a generator at all? That real possibility, along with other intriguing ideas, was described in a research paper presented at the International Conference on Machine Learning (ICML 2025), which was held in Vancouver, British Columbia, earlier this summer. The paper, describing novel techniques for manipulating and generating images, was written by Lukas Lao Beyer, a graduate student researcher in MIT’s Laboratory for Information and Decision Systems (LIDS); Tianhong Li, a postdoc at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL); Xinlei Chen of Facebook AI Research; Sertac Karaman, an MIT professor of aeronautics and astronautics and the director of LIDS; and Kaiming He, an MIT associate professor of electrical engineering and computer science.&lt;/p&gt;&lt;p&gt;This group effort had its origins in a class project for a graduate seminar on deep generative models that Lao Beyer took last fall. In conversations during the semester, it became apparent to both Lao Beyer and He, who taught the seminar, that this research had real potential, which went far beyond the confines of a typical homework assignment. Other collaborators were soon brought into the endeavor.&lt;/p&gt;&lt;p&gt;The starting point for Lao Beyer’s inquiry was a June 2024 paper, written by researchers from the Technical University of Munich and the Chinese company ByteDance, which introduced a new way of representing visual information called a one-dimensional tokenizer. With this device, which is also a kind of neural network, a 256x256-pixel image can be translated into a sequence of just 32 numbers, called tokens. “I wanted to understand how such a high level of compression could be achieved, and what the tokens themselves actually represented,” says Lao Beyer.&lt;/p&gt;&lt;p&gt;The previous generation of tokenizers would typically break up the same image into an array of 16x16 tokens — with each token encapsulating information, in highly condensed form, that corresponds to a specific portion of the original image. The new 1D tokenizers can encode an image more efficiently, using far fewer tokens overall, and these tokens are able to capture information about the entire image, not just a single quadrant. Each of these tokens, moreover, is a 12-digit number consisting of 1s and 0s, allowing for 2&lt;sup&gt;12&lt;/sup&gt; (or about 4,000) possibilities altogether. “It’s like a vocabulary of 4,000 words that makes up an abstract, hidden language spoken by the computer,” He explains. “It’s not like a human language, but we can still try to find out what it means.”&lt;/p&gt;&lt;p&gt;That’s exactly what Lao Beyer had initially set out to explore — work that provided the seed for the ICML 2025 paper. The approach he took was pretty straightforward. If you want to find out what a particular token does, Lao Beyer says, “you can just take it out, swap in some random value, and see if there is a recognizable change in the output.” Replacing one token, he found, changes the image quality, turning a low-resolution image into a high-resolution image or vice versa. Another token affected the blurriness in the background, while another still influenced the brightness. He also found a token that’s related to the “pose,” meaning that, in the image of a robin, for instance, the bird’s head might shift from right to left.&lt;/p&gt;&lt;p&gt;“This was a never-before-seen result, as no one had observed visually identifiable changes from manipulating tokens,” Lao Beyer says. The finding raised the possibility of a new approach to editing images. And the MIT group has shown, in fact, how this process can be streamlined and automated, so that tokens don’t have to be modified by hand, one at a time.&lt;/p&gt;&lt;p&gt;He and his colleagues achieved an even more consequential result involving image generation. A system capable of generating images normally requires a tokenizer, which compresses and encodes visual data, along with a generator that can combine and arrange these compact representations in order to create novel images. The MIT researchers found a way to create images without using a generator at all. Their new approach makes use of a 1D tokenizer and a so-called detokenizer (also known as a decoder), which can reconstruct an image from a string of tokens. However, with guidance provided by an off-the-shelf neural network called CLIP —&amp;nbsp;which cannot generate images on its own, but can measure how well a given image matches a certain text prompt&amp;nbsp;— the team was able to convert an image of a red panda, for example, into a tiger. In addition, they could create images of a tiger, or any other desired form, starting completely from scratch — from a situation in which all the tokens are initially assigned random values (and then iteratively tweaked so that the reconstructed image increasingly matches the desired text prompt).&lt;/p&gt;&lt;p&gt;The group demonstrated that with this same setup — relying on a tokenizer and detokenizer, but no generator — they could also do “inpainting,” which means filling in parts of images that had somehow been blotted out. Avoiding the use of a generator for certain tasks could lead to a significant reduction in computational costs because generators, as mentioned, normally require extensive training.&lt;/p&gt;&lt;p&gt;What might seem odd about this team’s contributions, He explains, “is that we didn’t invent anything new. We didn’t invent a 1D tokenizer, and we didn’t invent the CLIP model, either. But we did discover that new capabilities can arise when you put all these pieces together.”&lt;/p&gt;&lt;p&gt;“This work redefines the role of tokenizers,” comments&amp;nbsp;Saining Xie, a computer scientist at New York University. “It shows that&amp;nbsp;image tokenizers — tools usually used just to compress images — can actually do a lot more. The fact that a simple (but highly compressed) 1D tokenizer can handle tasks like inpainting or text-guided editing, without needing to train a full-blown generative model, is pretty surprising.”&lt;/p&gt;&lt;p&gt;Zhuang Liu of Princeton University agrees, saying that the work of the MIT group&amp;nbsp;“shows that we can generate and manipulate the images in a way that is much easier than we previously thought. Basically, it demonstrates that image generation can be a byproduct of a very effective image compressor, potentially reducing the cost of generating images several-fold.”&lt;/p&gt;&lt;p&gt;There could be many applications outside the field of computer vision, Karaman suggests. “For instance,&amp;nbsp;we could consider tokenizing the actions of robots or self-driving cars in the same way, which may rapidly broaden the impact of this work.”&lt;/p&gt;&lt;p&gt;Lao Beyer is thinking along similar lines,&amp;nbsp;noting that the&amp;nbsp;extreme amount of compression afforded by 1D tokenizers allows you to do “some amazing things,” which could be applied to other fields. For example, in the area of self-driving cars, which is one of his research interests, the tokens could represent, instead of images, the different routes that a vehicle might take.&lt;/p&gt;&lt;p&gt;Xie is also intrigued by the applications that may come from these innovative ideas. “There are some really cool use cases this could unlock,” he says.&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/new-way-edit-or-generate-images-0721</guid><pubDate>Mon, 21 Jul 2025 19:00:00 +0000</pubDate></item><item><title>[NEW] Gemini Deep Think learns math, wins gold medal at International Math Olympiad (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/07/google-deepmind-earns-gold-in-international-math-olympiad-with-new-gemini-ai/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        DeepMind followed IMO rules to earn gold, unlike OpenAI.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Google DeepMind at IMO" class="absolute inset-0 w-full h-full object-cover hidden" height="482" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/imo-team-in-australia-640x482.jpg" width="640" /&gt;
                  &lt;img alt="Google DeepMind at IMO" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/imo-team-in-australia-1152x648-1753123000.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The DeepMind IMO team at this year's event in Australia. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google DeepMind

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The students participating in the annual International Math Olympiad (IMO) represent some of the most talented young computational minds in the world. This year, they faced down a newly enhanced array of powerful AI models, including Google's Gemini Deep Think. The company says it put its model to the test using the same rules as human participants, and it improved on an already solid showing from last year.&lt;/p&gt;
&lt;p&gt;Google says its specially tuned math AI got five of the six questions correct, which is good enough for gold medal status. And unlike OpenAI, Google played by the rules set forth by the IMO.&lt;/p&gt;
&lt;h2&gt;A new Gemini&lt;/h2&gt;
&lt;p&gt;The Google DeepMind team participated in last year's IMO competition using an AI composed of the AlphaProof and AlphaGeometry 2 models. This setup was able to get four of the six questions correct, earning silver medal status—only half of the human participants earn any medal at all.&lt;/p&gt;
&lt;p&gt;In 2025, Google DeepMind was among a group of companies that worked with the IMO to have their models officially graded and certified by the coordinators. Google came prepared with a new model for the occasion. Gemini Deep Think was announced earlier this year as a more analytical take on simulated reasoning models. Rather than going down one linear line of "thought," Deep Think runs multiple reasoning processes in parallel, integrating and comparing the results before giving a final answer.&lt;/p&gt;
&lt;p&gt;According to Thang Luong, DeepMind senior scientist and head of the IMO team, this is a paradigm shift from last year's effort. In 2024, an expert had to translate the natural language questions into "domain specific language." At the end of the process, said expert would have to interpret the output. Deep Think, however, is natural language, end to end, and was not specifically designed to do math.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In the past, making LLMs better at math would involve reinforcement learning with final answers. Luong explained to Ars that models trained in this way can get to the correct answer, but they have "incomplete reasoning," and part of the IMO grading is based on showing your work. To prepare Deep Think for the IMO, Google used new reinforcement learning techniques with higher-quality "long answer" solutions to mathematical problems, giving the model better grounding in how to handle every step on the way to an answer. "With this kind of training, you can actually get robust, long-form reasoning," said Luong.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2107081 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="1080" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/IMO-2024-2025.png" width="1920" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google DeepMind

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;As you might expect, Deep Think takes more time to generate an output compared to the simpler versions you can access in the Gemini app. However, the AI followed the same rules as the flesh-and-blood participants, which was only possible because of its ability to ingest the problems as natural language. Gemini was provided with the problem descriptions and gave its answers within the 4.5-hour time limit of the competition.&lt;/p&gt;
&lt;h2&gt;Rigorous proofs&lt;/h2&gt;
&lt;p&gt;AI firms like DeepMind have taken an interest in the IMO over the past few years because it presents a unique challenge. While the competition is aimed at pre-university mathematicians, the questions require critical thinking and an understanding of multiple mathematical disciplines, including algebra, combinatorics, geometry, and number theory. Only the most advanced AI models have any hope of accurately answering these multi-layered problems.&lt;/p&gt;
&lt;p&gt;The DeepMind team has pointed out some interesting aspects of Deep Think's performance, which they say come from its advanced training. In the third problem (below), for example, many human competitors applied a graduate-level concept called Dirichlet's Theorem, using mathematics outside the intended scope of the competition. However, Deep Think recognized that it was possible to solve the problem with simpler math. "Our model actually made a brilliant observation and used only elementary number theory to create a self-contained proof of the given problem," said DeepMind researcher and Brown University professor Junehyuk Jung.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2107082 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="IMO 2025 P3" class="fullwidth full" height="212" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/Google-DeepMind-IMO-P3.png" width="575" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The DeepMind team says the model came up with a "brilliant" solution to this problem.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google DeepMind

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;As for the one Deep Think got wrong, the team says that was objectively the hardest of the competition. The question asked about the minimum number of rectangles needed to cover a given space. Jung explains that Deep Think started from an incorrect hypothesis, believing that the answer would be greater than or equal to 10, so it was lost from the start. "There's no way it's going to solve it because that is not true to begin with," said Jung.&lt;/p&gt;
&lt;p&gt;So Deep Think lost points on that problem, but Jung notes that only five students managed to get that one right. Still, Google got 35 points to earn a gold medal. Only about 8 percent of the human participants can reach that level.&lt;/p&gt;
&lt;p&gt;Google stresses that Deep Think went through the same evaluation as the students do. OpenAI has also announced results from the IMO, but it did not work with the organization to adhere to the established process. Instead, it had a panel of former IMO participants grade its answers and awarded &lt;em&gt;itself&lt;/em&gt; a gold medal.&lt;/p&gt;
&lt;p&gt;"We confirmed with the IMO organization that we actually solved five perfectly," said Luong. "I think anyone who didn't go through that process, we don't know, they might have lost one point and gotten silver."&lt;/p&gt;
&lt;p&gt;Google says the version of Deep Think tuned for the IMO is sticking around. It is currently being rolled out to a group of trusted testers that includes mathematicians. Eventually, this model will be provided to Google AI Ultra subscribers, who pay $250 per month for access to Google's biggest and most expensive models. DeepMind plans to continue iterating on this model and will be back next year in search of a perfect score.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        DeepMind followed IMO rules to earn gold, unlike OpenAI.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Google DeepMind at IMO" class="absolute inset-0 w-full h-full object-cover hidden" height="482" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/imo-team-in-australia-640x482.jpg" width="640" /&gt;
                  &lt;img alt="Google DeepMind at IMO" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/imo-team-in-australia-1152x648-1753123000.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The DeepMind IMO team at this year's event in Australia. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google DeepMind

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The students participating in the annual International Math Olympiad (IMO) represent some of the most talented young computational minds in the world. This year, they faced down a newly enhanced array of powerful AI models, including Google's Gemini Deep Think. The company says it put its model to the test using the same rules as human participants, and it improved on an already solid showing from last year.&lt;/p&gt;
&lt;p&gt;Google says its specially tuned math AI got five of the six questions correct, which is good enough for gold medal status. And unlike OpenAI, Google played by the rules set forth by the IMO.&lt;/p&gt;
&lt;h2&gt;A new Gemini&lt;/h2&gt;
&lt;p&gt;The Google DeepMind team participated in last year's IMO competition using an AI composed of the AlphaProof and AlphaGeometry 2 models. This setup was able to get four of the six questions correct, earning silver medal status—only half of the human participants earn any medal at all.&lt;/p&gt;
&lt;p&gt;In 2025, Google DeepMind was among a group of companies that worked with the IMO to have their models officially graded and certified by the coordinators. Google came prepared with a new model for the occasion. Gemini Deep Think was announced earlier this year as a more analytical take on simulated reasoning models. Rather than going down one linear line of "thought," Deep Think runs multiple reasoning processes in parallel, integrating and comparing the results before giving a final answer.&lt;/p&gt;
&lt;p&gt;According to Thang Luong, DeepMind senior scientist and head of the IMO team, this is a paradigm shift from last year's effort. In 2024, an expert had to translate the natural language questions into "domain specific language." At the end of the process, said expert would have to interpret the output. Deep Think, however, is natural language, end to end, and was not specifically designed to do math.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In the past, making LLMs better at math would involve reinforcement learning with final answers. Luong explained to Ars that models trained in this way can get to the correct answer, but they have "incomplete reasoning," and part of the IMO grading is based on showing your work. To prepare Deep Think for the IMO, Google used new reinforcement learning techniques with higher-quality "long answer" solutions to mathematical problems, giving the model better grounding in how to handle every step on the way to an answer. "With this kind of training, you can actually get robust, long-form reasoning," said Luong.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2107081 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="1080" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/IMO-2024-2025.png" width="1920" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google DeepMind

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;As you might expect, Deep Think takes more time to generate an output compared to the simpler versions you can access in the Gemini app. However, the AI followed the same rules as the flesh-and-blood participants, which was only possible because of its ability to ingest the problems as natural language. Gemini was provided with the problem descriptions and gave its answers within the 4.5-hour time limit of the competition.&lt;/p&gt;
&lt;h2&gt;Rigorous proofs&lt;/h2&gt;
&lt;p&gt;AI firms like DeepMind have taken an interest in the IMO over the past few years because it presents a unique challenge. While the competition is aimed at pre-university mathematicians, the questions require critical thinking and an understanding of multiple mathematical disciplines, including algebra, combinatorics, geometry, and number theory. Only the most advanced AI models have any hope of accurately answering these multi-layered problems.&lt;/p&gt;
&lt;p&gt;The DeepMind team has pointed out some interesting aspects of Deep Think's performance, which they say come from its advanced training. In the third problem (below), for example, many human competitors applied a graduate-level concept called Dirichlet's Theorem, using mathematics outside the intended scope of the competition. However, Deep Think recognized that it was possible to solve the problem with simpler math. "Our model actually made a brilliant observation and used only elementary number theory to create a self-contained proof of the given problem," said DeepMind researcher and Brown University professor Junehyuk Jung.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2107082 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="IMO 2025 P3" class="fullwidth full" height="212" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/Google-DeepMind-IMO-P3.png" width="575" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The DeepMind team says the model came up with a "brilliant" solution to this problem.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google DeepMind

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;As for the one Deep Think got wrong, the team says that was objectively the hardest of the competition. The question asked about the minimum number of rectangles needed to cover a given space. Jung explains that Deep Think started from an incorrect hypothesis, believing that the answer would be greater than or equal to 10, so it was lost from the start. "There's no way it's going to solve it because that is not true to begin with," said Jung.&lt;/p&gt;
&lt;p&gt;So Deep Think lost points on that problem, but Jung notes that only five students managed to get that one right. Still, Google got 35 points to earn a gold medal. Only about 8 percent of the human participants can reach that level.&lt;/p&gt;
&lt;p&gt;Google stresses that Deep Think went through the same evaluation as the students do. OpenAI has also announced results from the IMO, but it did not work with the organization to adhere to the established process. Instead, it had a panel of former IMO participants grade its answers and awarded &lt;em&gt;itself&lt;/em&gt; a gold medal.&lt;/p&gt;
&lt;p&gt;"We confirmed with the IMO organization that we actually solved five perfectly," said Luong. "I think anyone who didn't go through that process, we don't know, they might have lost one point and gotten silver."&lt;/p&gt;
&lt;p&gt;Google says the version of Deep Think tuned for the IMO is sticking around. It is currently being rolled out to a group of trusted testers that includes mathematicians. Eventually, this model will be provided to Google AI Ultra subscribers, who pay $250 per month for access to Google's biggest and most expensive models. DeepMind plans to continue iterating on this model and will be back next year in search of a perfect score.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/07/google-deepmind-earns-gold-in-international-math-olympiad-with-new-gemini-ai/</guid><pubDate>Mon, 21 Jul 2025 19:08:41 +0000</pubDate></item><item><title>[NEW] 72% of US teens have used AI companions, study finds (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/21/72-of-u-s-teens-have-used-ai-companions-study-finds/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Hopefully not Grok’s companions&amp;nbsp;…&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A new study by a U.S. nonprofit focused on the lives of kids and families, Common Sense Media, has found that a vast majority of U.S. teens (72%) have tried an AI companion at least once. By “companion,” the study is focused on AI chatbots that are designed for users to have more personal conversations with, not AI assistants that work as homework helpers, image generators, or voice assistants that just answer questions.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For instance, the study’s definition of AI companions could include those digital AI personas provided by companies like Character.AI or Replika, but it could also encompass the use of general-purpose chatbots like ChatGPT or Claude, which can be used for more personal conversations, if desired. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The study found that chatting with an AI seems to be appealing to U.S. teens (ages 13 to 17), as not only had nearly three-quarters tried an AI companion, but also 52% said they are regular users. Among those who engaged with these companions regularly, 13% chat with them daily and 21% chat a few times a week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Boys (31%) were also slightly more likely than girls (25%) to say they had never used an AI companion, among the one in four teens who said they have never tried it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The findings are based on a study that ran during April and May 2025 and used a representative sample of 1,060 teens and was conducted by researchers from NORC at the University of Chicago. There have already been concerns about AI’s impact on teens’ well-being, as one firm, Character.AI, is being sued over a teen’s suicide in Florida and for promoting violence in Texas. There are also a number of reports that describe the potential dangers of using AI for therapy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The findings from Common Sense Media’s new study offer an early understanding of how young people are using AI to simulate human interactions, which could include virtual friendship, emotional support, therapy, and role-playing games, among other things.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The analysis also examined other behaviors around teen usage of AI companions, including what sorts of tasks teens turned to them for, why, and what the after-effects were.&lt;/p&gt;

&lt;figure class="wp-block-image alignwide size-full"&gt;&lt;img alt="alt" class="wp-image-3029674" height="1060" src="https://techcrunch.com/wp-content/uploads/2025/07/common-sense-study-2025-07-21-at-3.16.52PM.jpg" width="1458" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Common Sense Media&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, nearly half (46%) said they saw AI companions as tools or programs, and 33% said they use them for social interaction and relationships. Teens said they use the AI companions for various purposes: entertainment (30%), curiosity about AI technology (28%), advice (18%), and because they’re always available (17%).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Half of teens (50%) said they don’t trust the information provided by AI companions. However, older teens are less likely to trust the AI’s advice compared with younger teens, ages 13 to 14, at 20% and 27%, respectively.&lt;/p&gt;

&lt;figure class="wp-block-image alignwide size-full"&gt;&lt;img alt="alt" class="wp-image-3029677" height="682" src="https://techcrunch.com/wp-content/uploads/2025/07/common-sense-study-2025-07-21-at-3.17.07PM.jpg" width="1266" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Common Sense Media&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;One-third of the teens said they find the conversations more satisfying than those with real-life friends, though the majority (67%) felt the opposite way.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Plus, 39% were using the AI conversations as practice for real-life interactions, as 39% said they applied skills they first tried with an AI to real-world situations. Among the skills practiced, social skills were the top use case, with 39% of teens having explored this area, followed by conversation starters (18%), giving advice (14%), and expressing emotions (13%). &lt;/p&gt;

&lt;figure class="wp-block-image alignwide size-full"&gt;&lt;img alt="alt" class="wp-image-3029675" height="1200" src="https://techcrunch.com/wp-content/uploads/2025/07/common-sense-study-2025-07-21-at-3.17.17PM.jpg" width="1426" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Common Sense Media&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In terms of whether real-life relationships will be replaced by tech, there was one positive finding: 80% of teens who used AI companions said they spend more time with real friends than with their AI chatbots. Only 6% said the reverse was true.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Hopefully not Grok’s companions&amp;nbsp;…&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A new study by a U.S. nonprofit focused on the lives of kids and families, Common Sense Media, has found that a vast majority of U.S. teens (72%) have tried an AI companion at least once. By “companion,” the study is focused on AI chatbots that are designed for users to have more personal conversations with, not AI assistants that work as homework helpers, image generators, or voice assistants that just answer questions.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For instance, the study’s definition of AI companions could include those digital AI personas provided by companies like Character.AI or Replika, but it could also encompass the use of general-purpose chatbots like ChatGPT or Claude, which can be used for more personal conversations, if desired. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The study found that chatting with an AI seems to be appealing to U.S. teens (ages 13 to 17), as not only had nearly three-quarters tried an AI companion, but also 52% said they are regular users. Among those who engaged with these companions regularly, 13% chat with them daily and 21% chat a few times a week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Boys (31%) were also slightly more likely than girls (25%) to say they had never used an AI companion, among the one in four teens who said they have never tried it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The findings are based on a study that ran during April and May 2025 and used a representative sample of 1,060 teens and was conducted by researchers from NORC at the University of Chicago. There have already been concerns about AI’s impact on teens’ well-being, as one firm, Character.AI, is being sued over a teen’s suicide in Florida and for promoting violence in Texas. There are also a number of reports that describe the potential dangers of using AI for therapy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The findings from Common Sense Media’s new study offer an early understanding of how young people are using AI to simulate human interactions, which could include virtual friendship, emotional support, therapy, and role-playing games, among other things.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The analysis also examined other behaviors around teen usage of AI companions, including what sorts of tasks teens turned to them for, why, and what the after-effects were.&lt;/p&gt;

&lt;figure class="wp-block-image alignwide size-full"&gt;&lt;img alt="alt" class="wp-image-3029674" height="1060" src="https://techcrunch.com/wp-content/uploads/2025/07/common-sense-study-2025-07-21-at-3.16.52PM.jpg" width="1458" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Common Sense Media&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, nearly half (46%) said they saw AI companions as tools or programs, and 33% said they use them for social interaction and relationships. Teens said they use the AI companions for various purposes: entertainment (30%), curiosity about AI technology (28%), advice (18%), and because they’re always available (17%).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Half of teens (50%) said they don’t trust the information provided by AI companions. However, older teens are less likely to trust the AI’s advice compared with younger teens, ages 13 to 14, at 20% and 27%, respectively.&lt;/p&gt;

&lt;figure class="wp-block-image alignwide size-full"&gt;&lt;img alt="alt" class="wp-image-3029677" height="682" src="https://techcrunch.com/wp-content/uploads/2025/07/common-sense-study-2025-07-21-at-3.17.07PM.jpg" width="1266" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Common Sense Media&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;One-third of the teens said they find the conversations more satisfying than those with real-life friends, though the majority (67%) felt the opposite way.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Plus, 39% were using the AI conversations as practice for real-life interactions, as 39% said they applied skills they first tried with an AI to real-world situations. Among the skills practiced, social skills were the top use case, with 39% of teens having explored this area, followed by conversation starters (18%), giving advice (14%), and expressing emotions (13%). &lt;/p&gt;

&lt;figure class="wp-block-image alignwide size-full"&gt;&lt;img alt="alt" class="wp-image-3029675" height="1200" src="https://techcrunch.com/wp-content/uploads/2025/07/common-sense-study-2025-07-21-at-3.17.17PM.jpg" width="1426" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Common Sense Media&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In terms of whether real-life relationships will be replaced by tech, there was one positive finding: 80% of teens who used AI companions said they spend more time with real friends than with their AI chatbots. Only 6% said the reverse was true.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/21/72-of-u-s-teens-have-used-ai-companions-study-finds/</guid><pubDate>Mon, 21 Jul 2025 19:42:20 +0000</pubDate></item><item><title>[NEW] ChatGPT users send 2.5 billion prompts a day (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/21/chatgpt-users-send-2-5-billion-prompts-a-day/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-1733837014-e.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;ChatGPT receives 2.5 billion prompts from global users every day, OpenAI told Axios. About 330 million of those are coming from users in the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These numbers show just how ubiquitous OpenAI’s flagship product is becoming.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Google’s parent company, Alphabet, does not release daily search data, but recently revealed that Google receives 5 trillion queries per year, which averages to just under 14 billion daily searches. Independent researchers have found similar trends. Neil Patel of NP Digital estimates that Google receives 13.7 billion searches daily, while research from SparkToro and Datos — two digital marketing companies — estimates that the figure is around 16.4 billion per day.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, ChatGPT has shown impressively rapid growth. In December, OpenAI CEO Sam Altman said that users send over 1 billion queries to ChatGPT each day. At Altman’s word, the company’s search volume has more than doubled in around eight months.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-1733837014-e.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;ChatGPT receives 2.5 billion prompts from global users every day, OpenAI told Axios. About 330 million of those are coming from users in the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These numbers show just how ubiquitous OpenAI’s flagship product is becoming.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Google’s parent company, Alphabet, does not release daily search data, but recently revealed that Google receives 5 trillion queries per year, which averages to just under 14 billion daily searches. Independent researchers have found similar trends. Neil Patel of NP Digital estimates that Google receives 13.7 billion searches daily, while research from SparkToro and Datos — two digital marketing companies — estimates that the figure is around 16.4 billion per day.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, ChatGPT has shown impressively rapid growth. In December, OpenAI CEO Sam Altman said that users send over 1 billion queries to ChatGPT each day. At Altman’s word, the company’s search volume has more than doubled in around eight months.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/21/chatgpt-users-send-2-5-billion-prompts-a-day/</guid><pubDate>Mon, 21 Jul 2025 19:44:43 +0000</pubDate></item><item><title>[NEW] A ChatGPT ‘router’ that automatically selects the right OpenAI model for your job appears imminent (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/a-chatgpt-router-that-automatically-selects-the-right-openai-model-for-your-job-appears-imminent/</link><description>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;In the 2.5 years since OpenAI debuted ChatGPT, the number of large language models (LLMs) that the company has made available as options to power its hit chatbot has steadily grown. &lt;/p&gt;&lt;p&gt;In fact, there are now a total of 7 (!!!) different AI models that paying ChatGPT subscribers (of the $20 Plus tier and more expensive tiers) can choose between when interacting with the trusty chatbot — each with its own strengths and weaknesses.&lt;/p&gt;&lt;p&gt;But how should a user decide &lt;em&gt;which one&lt;/em&gt; to use for their particular prompt, question, or task? After all, you can only pick one at a time. &lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-is-help-on-the-way"&gt;Is help on the way? &lt;/h2&gt;



&lt;p&gt;Help appears to be on the way imminently from OpenAI — as reports emerged over the last few days on X from AI influencers, including OpenAI’s own researcher “Roon (@tszzl on X)” (speculated to be technical team member Tarun Gogineni) — of a new “router” function that will automatically select the best OpenAI model to respond to the user’s input on the fly, depending on the specific input’s content.&lt;/p&gt;



&lt;p&gt;As Roon posted on the social network X yesterday, July 20, 2025, in since-deleted response to influencer Lisan al Gaib’s statement that they “don’t want a model router I want to be able to select the models I use”: &lt;/p&gt;



&lt;p&gt;&lt;em&gt;“You’ll still be able to select. This is a product to make sure that doctors aren’t stuck on 4o-mini”&lt;/em&gt;&lt;/p&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3014510" height="600" src="https://venturebeat.com/wp-content/uploads/2025/07/GwTyIPlXYAAu-uj-1.jpg?w=439" width="439" /&gt;&lt;/figure&gt;



&lt;p&gt;Similarly, Yuchen Jin, Co-founder &amp;amp; CTO of AI inference cloud provider Hyperbolic Labs, wrote in an X post on July 19.&lt;/p&gt;



&lt;p&gt;“&lt;em&gt;Heard GPT-5 is imminent, from a little bird.&lt;/em&gt;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;em&gt;It’s not one model, but multiple models. It has a router that switches between reasoning, non-reasoning, and tool-using models.&lt;/em&gt;&lt;/li&gt;



&lt;li&gt;&lt;em&gt;That’s why Sam said they’d “fix model naming”: prompts will just auto-route to the right model.&lt;/em&gt;&lt;/li&gt;



&lt;li&gt;&lt;em&gt;GPT-6 is in training.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;&lt;em&gt;I just hope they’re not delaying it for more safety tests. :)&lt;/em&gt;“&lt;/p&gt;



&lt;p&gt;While a presumably far more advanced GPT-5 model would (and will) be huge news if and when released, the router may make life much easier and more intelligent for the average ChatGPT subscriber.&lt;/p&gt;



&lt;p&gt;It would also follow on the heels of other third-party products such as the web-based Token Monster chatbot, which automatically select and combine responses from multiple third-party LLMs to respond to user queries.&lt;/p&gt;



&lt;p&gt;Asked about the router idea and comments from “Roon,” an OpenAI spokesperson declined to provide a response or further information at this time.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-solving-the-overabundance-of-choice-problem"&gt;Solving the overabundance of choice problem&lt;/h2&gt;



&lt;p&gt;To be clear, every time OpenAI has released a new LLM to the public, it has diligently shared in either a blog post or release notes or both what it thinks that particular model is good for and designed to help with. &lt;/p&gt;



&lt;p&gt;For example, OpenAI’s “o” series reasoning models — o3, o4-mini, o4-mini high — have performed better on math, science, and coding tests thanks to benchmarking tests, while non-reasoning models like the new GPT-4.5 and 4.1 seem to do better at creative writing and communications tasks. &lt;/p&gt;



&lt;p&gt;Dedicated AI influencers and power users may understand very well what all these different models are good and not so good at.&lt;/p&gt;



&lt;p&gt;But regular users who don’t follow the industry as closely, nor have the time and finances available to test them all out on the same input prompts and compare the outputs, will understandably struggle to make sense of the bewildering array of options. &lt;/p&gt;



&lt;p&gt;That could mean they’re missing out on smarter, more intelligent, or more capable responses from ChatGPT for their task at hand. And in the case of fields like medicine, as Roon alluded to, the difference could be one of life or death. &lt;/p&gt;



&lt;p&gt;It’s also interesting to speculate on how an automatic LLM router might change public perceptions toward and adoption of AI more broadly. &lt;/p&gt;



&lt;p&gt;ChatGPT already counted 500 million active users as of March. If more of these people were automatically guided toward more intelligent and capable LLMs to handle their AI queries, the impact of AI on their workloads and that of the entire global economy would seem likely to be felt far more acutely, creating a positive “snowball” effect.&lt;/p&gt;



&lt;p&gt;That is, as more people saw more gains from ChatGPT &lt;em&gt;automatically&lt;/em&gt; choosing the right AI model for their queries, and as more enterprises reaped greater efficiency from this process, more and more individuals and organizations would likely be convinced by the utility of AI and be more willing to pay for it, and as they did so, even &lt;em&gt;more&lt;/em&gt; AI-powered workflows would spread out in the world. &lt;/p&gt;



&lt;p&gt;But right now, this is presumably all being held back a little by the fact that the ChatGPT model picker requires the user to A. know they even have a choice of models and B. have some level of informed awareness of what these models are good for. It’s all still a manually driven process. &lt;/p&gt;



&lt;p&gt;Like going to the supermarket in your town and staring at aisles of cereal and different sauces, the average ChatGPT user is currently faced with an overabundance of choice. &lt;/p&gt;



&lt;p&gt;Hopefully any hypothetical OpenAI router seamlessly helps direct them to the right model product for their needs, when they need it — like a trusty shopkeeper showing up to free you from your product paralysis.&lt;/p&gt;




&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;In the 2.5 years since OpenAI debuted ChatGPT, the number of large language models (LLMs) that the company has made available as options to power its hit chatbot has steadily grown. &lt;/p&gt;&lt;p&gt;In fact, there are now a total of 7 (!!!) different AI models that paying ChatGPT subscribers (of the $20 Plus tier and more expensive tiers) can choose between when interacting with the trusty chatbot — each with its own strengths and weaknesses.&lt;/p&gt;&lt;p&gt;But how should a user decide &lt;em&gt;which one&lt;/em&gt; to use for their particular prompt, question, or task? After all, you can only pick one at a time. &lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-is-help-on-the-way"&gt;Is help on the way? &lt;/h2&gt;



&lt;p&gt;Help appears to be on the way imminently from OpenAI — as reports emerged over the last few days on X from AI influencers, including OpenAI’s own researcher “Roon (@tszzl on X)” (speculated to be technical team member Tarun Gogineni) — of a new “router” function that will automatically select the best OpenAI model to respond to the user’s input on the fly, depending on the specific input’s content.&lt;/p&gt;



&lt;p&gt;As Roon posted on the social network X yesterday, July 20, 2025, in since-deleted response to influencer Lisan al Gaib’s statement that they “don’t want a model router I want to be able to select the models I use”: &lt;/p&gt;



&lt;p&gt;&lt;em&gt;“You’ll still be able to select. This is a product to make sure that doctors aren’t stuck on 4o-mini”&lt;/em&gt;&lt;/p&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3014510" height="600" src="https://venturebeat.com/wp-content/uploads/2025/07/GwTyIPlXYAAu-uj-1.jpg?w=439" width="439" /&gt;&lt;/figure&gt;



&lt;p&gt;Similarly, Yuchen Jin, Co-founder &amp;amp; CTO of AI inference cloud provider Hyperbolic Labs, wrote in an X post on July 19.&lt;/p&gt;



&lt;p&gt;“&lt;em&gt;Heard GPT-5 is imminent, from a little bird.&lt;/em&gt;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;em&gt;It’s not one model, but multiple models. It has a router that switches between reasoning, non-reasoning, and tool-using models.&lt;/em&gt;&lt;/li&gt;



&lt;li&gt;&lt;em&gt;That’s why Sam said they’d “fix model naming”: prompts will just auto-route to the right model.&lt;/em&gt;&lt;/li&gt;



&lt;li&gt;&lt;em&gt;GPT-6 is in training.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;&lt;em&gt;I just hope they’re not delaying it for more safety tests. :)&lt;/em&gt;“&lt;/p&gt;



&lt;p&gt;While a presumably far more advanced GPT-5 model would (and will) be huge news if and when released, the router may make life much easier and more intelligent for the average ChatGPT subscriber.&lt;/p&gt;



&lt;p&gt;It would also follow on the heels of other third-party products such as the web-based Token Monster chatbot, which automatically select and combine responses from multiple third-party LLMs to respond to user queries.&lt;/p&gt;



&lt;p&gt;Asked about the router idea and comments from “Roon,” an OpenAI spokesperson declined to provide a response or further information at this time.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-solving-the-overabundance-of-choice-problem"&gt;Solving the overabundance of choice problem&lt;/h2&gt;



&lt;p&gt;To be clear, every time OpenAI has released a new LLM to the public, it has diligently shared in either a blog post or release notes or both what it thinks that particular model is good for and designed to help with. &lt;/p&gt;



&lt;p&gt;For example, OpenAI’s “o” series reasoning models — o3, o4-mini, o4-mini high — have performed better on math, science, and coding tests thanks to benchmarking tests, while non-reasoning models like the new GPT-4.5 and 4.1 seem to do better at creative writing and communications tasks. &lt;/p&gt;



&lt;p&gt;Dedicated AI influencers and power users may understand very well what all these different models are good and not so good at.&lt;/p&gt;



&lt;p&gt;But regular users who don’t follow the industry as closely, nor have the time and finances available to test them all out on the same input prompts and compare the outputs, will understandably struggle to make sense of the bewildering array of options. &lt;/p&gt;



&lt;p&gt;That could mean they’re missing out on smarter, more intelligent, or more capable responses from ChatGPT for their task at hand. And in the case of fields like medicine, as Roon alluded to, the difference could be one of life or death. &lt;/p&gt;



&lt;p&gt;It’s also interesting to speculate on how an automatic LLM router might change public perceptions toward and adoption of AI more broadly. &lt;/p&gt;



&lt;p&gt;ChatGPT already counted 500 million active users as of March. If more of these people were automatically guided toward more intelligent and capable LLMs to handle their AI queries, the impact of AI on their workloads and that of the entire global economy would seem likely to be felt far more acutely, creating a positive “snowball” effect.&lt;/p&gt;



&lt;p&gt;That is, as more people saw more gains from ChatGPT &lt;em&gt;automatically&lt;/em&gt; choosing the right AI model for their queries, and as more enterprises reaped greater efficiency from this process, more and more individuals and organizations would likely be convinced by the utility of AI and be more willing to pay for it, and as they did so, even &lt;em&gt;more&lt;/em&gt; AI-powered workflows would spread out in the world. &lt;/p&gt;



&lt;p&gt;But right now, this is presumably all being held back a little by the fact that the ChatGPT model picker requires the user to A. know they even have a choice of models and B. have some level of informed awareness of what these models are good for. It’s all still a manually driven process. &lt;/p&gt;



&lt;p&gt;Like going to the supermarket in your town and staring at aisles of cereal and different sauces, the average ChatGPT user is currently faced with an overabundance of choice. &lt;/p&gt;



&lt;p&gt;Hopefully any hypothetical OpenAI router seamlessly helps direct them to the right model product for their needs, when they need it — like a trusty shopkeeper showing up to free you from your product paralysis.&lt;/p&gt;




&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/a-chatgpt-router-that-automatically-selects-the-right-openai-model-for-your-job-appears-imminent/</guid><pubDate>Mon, 21 Jul 2025 21:26:57 +0000</pubDate></item><item><title>[NEW] Chinese startup Manus challenges ChatGPT in data visualization: which should enterprises use? (AI News | VentureBeat)</title><link>https://venturebeat.com/data-infrastructure/chinese-startup-manus-challenges-chatgpt-in-data-visualization-which-should-enterprises-use/</link><description>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;The promise sounds almost too good to be true: drop a messy comma separated values (CSV) file into an AI agent, wait two minutes, and get back a polished, interactive chart ready for your next board presentation.&amp;nbsp;&lt;/p&gt;&lt;p&gt;But that’s exactly what Chinese startup Manus.im is delivering with its latest data visualization feature, launched this month.&lt;/p&gt;&lt;p&gt;Unfortunately, my initial hands-on testing with corrupted datasets reveals a fundamental enterprise problem: impressive capabilities paired with insufficient transparency about data transformations. While Manus handles messy data better than ChatGPT, neither tool is yet ready for boardroom-ready slides.&lt;/p&gt;&lt;p&gt;Rossums’ survey of 470 finance leaders found 58% still rely primarily on Excel for monthly KPIs, despite owning BI licenses. Another TechRadar study estimates that overall spreadsheet dependence affects roughly 90% of organizations — creating a “last-mile data problem” between governed warehouses and hasty CSV exports that land in analysts’ inboxes hours before critical meetings.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Manus targets this exact gap. Upload your CSV, describe what you want in natural language, and the agent automatically cleans the data, selects the appropriate Vega-Lite grammar and returns a PNG chart ready for export—no pivot tables required.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-where-manus-beats-chatgpt-4x-slower-but-more-accurate-with-messy-data"&gt;&lt;strong&gt;Where Manus beats ChatGPT: 4x slower but more accurate with messy data&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;I tested both Manus and ChatGPT’s Advanced Data Analysis using three datasets (113k-row ecommerce orders, 200k-row marketing funnel 10k-row SaaS MRR), first clean, then corrupted with 5% error injection including nulls, mixed-format dates and duplicates.&amp;nbsp;&lt;/p&gt;



&lt;pre class="wp-block-code"&gt;&lt;code&gt;&lt;em&gt;For example, testing the same prompt — "Show me a month-by-month revenue trend for the past year and highlight any unusual spikes or dips" — across clean and corrupted 113k-row e-commerce data revealed some stark differences.&lt;/em&gt;&lt;/code&gt;&lt;/pre&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Tool&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Data Quality&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Time&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Cleans Nulls&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Parses Dates&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Handles Duplicates&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Comments&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Manus&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Clean&lt;/td&gt;&lt;td&gt;1:46&lt;/td&gt;&lt;td&gt;N/A&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;N/A&lt;/td&gt;&lt;td&gt;Correct trend, standard presentation, but incorrect numbers&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Manus&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Messy&lt;/td&gt;&lt;td&gt;3:53&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;✗&lt;/td&gt;&lt;td&gt;Correct trend despite inaccurate data&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;ChatGPT&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Clean&lt;/td&gt;&lt;td&gt;0:57&lt;/td&gt;&lt;td&gt;N/A&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;N/A&lt;/td&gt;&lt;td&gt;Fast, but incorrect visualisation&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;ChatGPT&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Messy&lt;/td&gt;&lt;td&gt;0:59&lt;/td&gt;&lt;td&gt;✗&lt;/td&gt;&lt;td&gt;✗&lt;/td&gt;&lt;td&gt;✗&lt;/td&gt;&lt;td&gt;Incorrect trend from unclean data&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;p&gt;For context: DeepSeek could only handle 1% of the file size, while Claude and Grok took over 5 minutes each but produced interactive charts without PNG export options.&lt;/p&gt;



&lt;p&gt;Outputs:&lt;/p&gt;



&lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfKenGDQXNL_jetGvVaGrepD6NcUte0ZC8DeY72J21kaBV16Hi5TXXwjUac4DRtBUl6pFdpnZar7-vpLoL0zwjNmwqaFpqOV5SgpnKB5yV1-owJ_QxptzymJD62KmPY_AjcHyWblQ?key=vduzeoXrGmKjIlhAx_veeQ" /&gt;&lt;/figure&gt;



&lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdywoF1plESYtTHNj-8a5TWumO5E3micqtckYEnvYZaaVsDimEBx-qS2REpo6S5jJiKZLl1K_5rbfYVjfc26l9-Vy-sKjYzbB9VPghMR9ZxLncjRq-TkfhfXSJx6dj-G7b8Wxtptw?key=vduzeoXrGmKjIlhAx_veeQ" /&gt;&lt;/figure&gt;



&lt;p&gt;&lt;em&gt;Figure 1-2: Chart outputs from the same revenue trend prompt on messy e-commerce data. Manus (bottom) produces a coherent trend despite data corruption, while ChatGPT (top) shows distorted patterns from unclean date formatting. &lt;/em&gt;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Manus behaves like a cautious junior analyst&lt;/strong&gt; — automatically tidying data before charting, successfully parsing date inconsistencies and handling nulls without explicit instructions. When I requested the same revenue trend analysis on corrupted data, Manus took nearly 4 minutes but produced a coherent visualization despite the data quality issues.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;ChatGPT operates like a speed coder&lt;/strong&gt; — prioritizing fast output over data hygiene. The same request took just 59 seconds but produced misleading visualizations because it didn’t automatically clean formatting inconsistencies.&lt;/p&gt;



&lt;p&gt;However, both tools failed in terms of “executive readiness.” Neither produced board-ready axis scaling or readable labels without follow-up prompts. Data labels were frequently overlapping or too small, bar charts lacked proper gridlines and number formatting was inconsistent.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-transparency-crisis-enterprises-can-t-ignore"&gt;&lt;strong&gt;The transparency crisis enterprises can’t ignore&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Here’s where Manus becomes problematic for enterprise adoption: &lt;strong&gt;the agent never surfaces cleaning steps it applies&lt;/strong&gt;. An auditor reviewing the final chart has no way to confirm whether outliers were dropped, imputed or transformed.&lt;/p&gt;



&lt;p&gt;When a CFO presents quarterly results based on a Manus-generated chart, what happens when someone asks, “How did you handle the duplicate transactions from the Q2 system integration?” The answer is silence.&lt;/p&gt;



&lt;p&gt;ChatGPT, Claude and Grok all show their Python code, though transparency through code review isn’t scalable for business users lacking programming experience. What enterprises need is a simpler audit trail, which builds trust.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-warehouse-native-ai-is-racing-ahead"&gt;&lt;strong&gt;Warehouse-native AI is racing ahead&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;While Manus focuses on CSV uploads, major platforms are building chart generation directly into enterprise data infrastructure:&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Google’s Gemini in BigQuery&lt;/strong&gt; became generally available in August 2024, enabling the generation of SQL queries and inline visualizations on live tables while respecting row-level security.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Microsoft’s Copilot in Fabric&lt;/strong&gt; reached GA in the Power BI experience in May 2024, creating visuals inside Fabric notebooks while working directly with Lakehouse datasets.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;GoodData’s AI Assistant&lt;/strong&gt;, launched in June 2025, operates within customer environments and respects existing semantic models, allowing users to ask questions in plain language while receiving answers that align with predefined metrics and business terms.&lt;/p&gt;



&lt;p&gt;These warehouse-native solutions eliminate CSV exports entirely, preserve complete data lineage and leverage existing security models — advantages file-upload tools like Manus struggle to match.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-critical-gaps-for-enterprise-adoption"&gt;&lt;strong&gt;Critical gaps for enterprise adoption&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;My testing revealed several blockers:&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Live data connectivity&lt;/strong&gt; remains absent — Manus supports file uploads only, with no Snowflake, BigQuery or S3 connectors. Manus.im says connectors are “on the roadmap” but offers no timeline.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Audit trail transparency&lt;/strong&gt; is completely missing. Enterprise data teams need transformation logs showing exactly how AI cleaned their data and whether its interpretation of the fields are correct.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Export flexibility&lt;/strong&gt; is limited to PNG outputs. While adequate for quick slide decks, enterprises need customizable, interactive export options.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-verdict-impressive-tech-premature-for-enterprise-use-cases-nbsp"&gt;&lt;strong&gt;The verdict: impressive tech, premature for enterprise use cases&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;For SMB executives drowning in ad-hoc CSV analysis, Manus’s drag-and-drop visualisation seems to be doing the job.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The autonomous data cleaning handles real-world messiness that would otherwise require manual preprocessing, cutting turnaround from hours to minutes when you have reasonably complete data.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Additionally, it offers a significant runtime advantage over Excel or Google Sheets, which require manual pivots and incur substantial load times due to local compute power limitations.&lt;/p&gt;



&lt;p&gt;But regulated enterprises with governed data lakes should wait for warehouse-native agents like Gemini or Fabric Copilot, which keep data inside security perimeters and maintain complete lineage tracking.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Bottom line:&lt;/strong&gt; Manus proves one-prompt charting works and handles messy data impressively. But for enterprises, the question isn’t whether the charts look good — it’s whether you can stake your career on data transformations you can’t audit or verify. Until AI agents can plug directly into governed tables with rigorous audit trails, Excel will continue to hold its starring role in quarterly presentations.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;The promise sounds almost too good to be true: drop a messy comma separated values (CSV) file into an AI agent, wait two minutes, and get back a polished, interactive chart ready for your next board presentation.&amp;nbsp;&lt;/p&gt;&lt;p&gt;But that’s exactly what Chinese startup Manus.im is delivering with its latest data visualization feature, launched this month.&lt;/p&gt;&lt;p&gt;Unfortunately, my initial hands-on testing with corrupted datasets reveals a fundamental enterprise problem: impressive capabilities paired with insufficient transparency about data transformations. While Manus handles messy data better than ChatGPT, neither tool is yet ready for boardroom-ready slides.&lt;/p&gt;&lt;p&gt;Rossums’ survey of 470 finance leaders found 58% still rely primarily on Excel for monthly KPIs, despite owning BI licenses. Another TechRadar study estimates that overall spreadsheet dependence affects roughly 90% of organizations — creating a “last-mile data problem” between governed warehouses and hasty CSV exports that land in analysts’ inboxes hours before critical meetings.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Manus targets this exact gap. Upload your CSV, describe what you want in natural language, and the agent automatically cleans the data, selects the appropriate Vega-Lite grammar and returns a PNG chart ready for export—no pivot tables required.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-where-manus-beats-chatgpt-4x-slower-but-more-accurate-with-messy-data"&gt;&lt;strong&gt;Where Manus beats ChatGPT: 4x slower but more accurate with messy data&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;I tested both Manus and ChatGPT’s Advanced Data Analysis using three datasets (113k-row ecommerce orders, 200k-row marketing funnel 10k-row SaaS MRR), first clean, then corrupted with 5% error injection including nulls, mixed-format dates and duplicates.&amp;nbsp;&lt;/p&gt;



&lt;pre class="wp-block-code"&gt;&lt;code&gt;&lt;em&gt;For example, testing the same prompt — "Show me a month-by-month revenue trend for the past year and highlight any unusual spikes or dips" — across clean and corrupted 113k-row e-commerce data revealed some stark differences.&lt;/em&gt;&lt;/code&gt;&lt;/pre&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Tool&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Data Quality&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Time&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Cleans Nulls&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Parses Dates&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Handles Duplicates&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Comments&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Manus&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Clean&lt;/td&gt;&lt;td&gt;1:46&lt;/td&gt;&lt;td&gt;N/A&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;N/A&lt;/td&gt;&lt;td&gt;Correct trend, standard presentation, but incorrect numbers&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Manus&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Messy&lt;/td&gt;&lt;td&gt;3:53&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;✗&lt;/td&gt;&lt;td&gt;Correct trend despite inaccurate data&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;ChatGPT&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Clean&lt;/td&gt;&lt;td&gt;0:57&lt;/td&gt;&lt;td&gt;N/A&lt;/td&gt;&lt;td&gt;✓&lt;/td&gt;&lt;td&gt;N/A&lt;/td&gt;&lt;td&gt;Fast, but incorrect visualisation&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;ChatGPT&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Messy&lt;/td&gt;&lt;td&gt;0:59&lt;/td&gt;&lt;td&gt;✗&lt;/td&gt;&lt;td&gt;✗&lt;/td&gt;&lt;td&gt;✗&lt;/td&gt;&lt;td&gt;Incorrect trend from unclean data&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;p&gt;For context: DeepSeek could only handle 1% of the file size, while Claude and Grok took over 5 minutes each but produced interactive charts without PNG export options.&lt;/p&gt;



&lt;p&gt;Outputs:&lt;/p&gt;



&lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfKenGDQXNL_jetGvVaGrepD6NcUte0ZC8DeY72J21kaBV16Hi5TXXwjUac4DRtBUl6pFdpnZar7-vpLoL0zwjNmwqaFpqOV5SgpnKB5yV1-owJ_QxptzymJD62KmPY_AjcHyWblQ?key=vduzeoXrGmKjIlhAx_veeQ" /&gt;&lt;/figure&gt;



&lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdywoF1plESYtTHNj-8a5TWumO5E3micqtckYEnvYZaaVsDimEBx-qS2REpo6S5jJiKZLl1K_5rbfYVjfc26l9-Vy-sKjYzbB9VPghMR9ZxLncjRq-TkfhfXSJx6dj-G7b8Wxtptw?key=vduzeoXrGmKjIlhAx_veeQ" /&gt;&lt;/figure&gt;



&lt;p&gt;&lt;em&gt;Figure 1-2: Chart outputs from the same revenue trend prompt on messy e-commerce data. Manus (bottom) produces a coherent trend despite data corruption, while ChatGPT (top) shows distorted patterns from unclean date formatting. &lt;/em&gt;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Manus behaves like a cautious junior analyst&lt;/strong&gt; — automatically tidying data before charting, successfully parsing date inconsistencies and handling nulls without explicit instructions. When I requested the same revenue trend analysis on corrupted data, Manus took nearly 4 minutes but produced a coherent visualization despite the data quality issues.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;ChatGPT operates like a speed coder&lt;/strong&gt; — prioritizing fast output over data hygiene. The same request took just 59 seconds but produced misleading visualizations because it didn’t automatically clean formatting inconsistencies.&lt;/p&gt;



&lt;p&gt;However, both tools failed in terms of “executive readiness.” Neither produced board-ready axis scaling or readable labels without follow-up prompts. Data labels were frequently overlapping or too small, bar charts lacked proper gridlines and number formatting was inconsistent.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-transparency-crisis-enterprises-can-t-ignore"&gt;&lt;strong&gt;The transparency crisis enterprises can’t ignore&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Here’s where Manus becomes problematic for enterprise adoption: &lt;strong&gt;the agent never surfaces cleaning steps it applies&lt;/strong&gt;. An auditor reviewing the final chart has no way to confirm whether outliers were dropped, imputed or transformed.&lt;/p&gt;



&lt;p&gt;When a CFO presents quarterly results based on a Manus-generated chart, what happens when someone asks, “How did you handle the duplicate transactions from the Q2 system integration?” The answer is silence.&lt;/p&gt;



&lt;p&gt;ChatGPT, Claude and Grok all show their Python code, though transparency through code review isn’t scalable for business users lacking programming experience. What enterprises need is a simpler audit trail, which builds trust.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-warehouse-native-ai-is-racing-ahead"&gt;&lt;strong&gt;Warehouse-native AI is racing ahead&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;While Manus focuses on CSV uploads, major platforms are building chart generation directly into enterprise data infrastructure:&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Google’s Gemini in BigQuery&lt;/strong&gt; became generally available in August 2024, enabling the generation of SQL queries and inline visualizations on live tables while respecting row-level security.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Microsoft’s Copilot in Fabric&lt;/strong&gt; reached GA in the Power BI experience in May 2024, creating visuals inside Fabric notebooks while working directly with Lakehouse datasets.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;GoodData’s AI Assistant&lt;/strong&gt;, launched in June 2025, operates within customer environments and respects existing semantic models, allowing users to ask questions in plain language while receiving answers that align with predefined metrics and business terms.&lt;/p&gt;



&lt;p&gt;These warehouse-native solutions eliminate CSV exports entirely, preserve complete data lineage and leverage existing security models — advantages file-upload tools like Manus struggle to match.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-critical-gaps-for-enterprise-adoption"&gt;&lt;strong&gt;Critical gaps for enterprise adoption&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;My testing revealed several blockers:&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Live data connectivity&lt;/strong&gt; remains absent — Manus supports file uploads only, with no Snowflake, BigQuery or S3 connectors. Manus.im says connectors are “on the roadmap” but offers no timeline.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Audit trail transparency&lt;/strong&gt; is completely missing. Enterprise data teams need transformation logs showing exactly how AI cleaned their data and whether its interpretation of the fields are correct.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Export flexibility&lt;/strong&gt; is limited to PNG outputs. While adequate for quick slide decks, enterprises need customizable, interactive export options.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-verdict-impressive-tech-premature-for-enterprise-use-cases-nbsp"&gt;&lt;strong&gt;The verdict: impressive tech, premature for enterprise use cases&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;For SMB executives drowning in ad-hoc CSV analysis, Manus’s drag-and-drop visualisation seems to be doing the job.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The autonomous data cleaning handles real-world messiness that would otherwise require manual preprocessing, cutting turnaround from hours to minutes when you have reasonably complete data.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Additionally, it offers a significant runtime advantage over Excel or Google Sheets, which require manual pivots and incur substantial load times due to local compute power limitations.&lt;/p&gt;



&lt;p&gt;But regulated enterprises with governed data lakes should wait for warehouse-native agents like Gemini or Fabric Copilot, which keep data inside security perimeters and maintain complete lineage tracking.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Bottom line:&lt;/strong&gt; Manus proves one-prompt charting works and handles messy data impressively. But for enterprises, the question isn’t whether the charts look good — it’s whether you can stake your career on data transformations you can’t audit or verify. Until AI agents can plug directly into governed tables with rigorous audit trails, Excel will continue to hold its starring role in quarterly presentations.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/data-infrastructure/chinese-startup-manus-challenges-chatgpt-in-data-visualization-which-should-enterprises-use/</guid><pubDate>Mon, 21 Jul 2025 21:38:23 +0000</pubDate></item><item><title>[NEW] Google DeepMind makes AI history with gold medal win at world’s toughest math competition (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/google-deepmind-makes-ai-history-with-gold-medal-win-at-worlds-toughest-math-competition/</link><description>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Google DeepMind announced Monday that an advanced version of its Gemini artificial intelligence model has officially achieved gold medal-level performance at the International Mathematical Olympiad, solving five of six exceptionally difficult problems and earning recognition as the first AI system to receive official gold-level grading from competition organizers.&lt;/p&gt;&lt;p&gt;The victory advances the field of AI reasoning and puts Google ahead in the intensifying battle between tech giants building next-generation artificial intelligence. More importantly, it demonstrates that AI can now tackle complex mathematical problems using natural language understanding rather than requiring specialized programming languages.&lt;/p&gt;&lt;p&gt;“Official results are in — Gemini achieved gold-medal level in the International Mathematical Olympiad!” Demis Hassabis, CEO of Google DeepMind, wrote on social media platform X Monday morning. “An advanced version was able to solve 5 out of 6 problems. Incredible progress.”&lt;/p&gt;&lt;p&gt;The International Mathematical Olympiad, held annually since 1959, is widely considered the world’s most prestigious mathematics competition for pre-university students. Each participating country sends six elite young mathematicians to compete in solving six exceptionally challenging problems spanning algebra, combinatorics, geometry, and number theory. Only about 8% of human participants typically earn gold medals.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-how-google-deepmind-s-gemini-deep-think-cracked-math-s-toughest-problems"&gt;How Google DeepMind’s Gemini Deep Think cracked math’s toughest problems&lt;/h2&gt;



&lt;p&gt;Google’s latest success far exceeds its 2024 performance, when the company’s combined AlphaProof and AlphaGeometry systems earned silver medal status by solving four of six problems. That earlier system required human experts to first translate natural language problems into domain-specific programming languages and then interpret the AI’s mathematical output.&lt;/p&gt;



&lt;p&gt;This year’s breakthrough came through Gemini Deep Think, an enhanced reasoning system that employs what researchers call “parallel thinking.” Unlike traditional AI models that follow a single chain of reasoning, Deep Think simultaneously explores multiple possible solutions before arriving at a final answer.&lt;/p&gt;



&lt;p&gt;“Our model operated end-to-end in natural language, producing rigorous mathematical proofs directly from the official problem descriptions,” Hassabis explained in a follow-up post on the social media site X, emphasizing that the system completed its work within the competition’s standard 4.5-hour time limit.&lt;/p&gt;



&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;We achieved this year’s impressive result using an advanced version of Gemini Deep Think (an enhanced reasoning mode for complex problems). Our model operated end-to-end in natural language, producing rigorous mathematical proofs directly from the official problem descriptions –…&lt;/p&gt;— Demis Hassabis (@demishassabis) July 21, 2025&lt;/blockquote&gt; 



&lt;p&gt;The model achieved 35 out of a possible 42 points, comfortably exceeding the gold medal threshold. According to IMO President Prof. Dr. Gregor Dolinar, the solutions were “astonishing in many respects” and found to be “clear, precise and most of them easy to follow” by competition graders.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-openai-faces-backlash-for-bypassing-official-competition-rules"&gt;OpenAI faces backlash for bypassing official competition rules&lt;/h2&gt;



&lt;p&gt;The announcement comes amid growing tension in the AI industry over competitive practices and transparency. Google DeepMind’s measured approach to releasing its results has drawn praise from the AI community, particularly in contrast to rival OpenAI’s handling of similar achievements.&lt;/p&gt;



&lt;p&gt;“We didn’t announce on Friday because we respected the IMO Board’s original request that all AI labs share their results only after the official results had been verified by independent experts &amp;amp; the students had rightly received the acclamation they deserved,” Hassabis wrote, appearing to reference OpenAI’s earlier announcement of its own olympiad performance.&lt;/p&gt;



&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Btw as an aside, we didn’t announce on Friday because we respected the IMO Board's original request that all AI labs share their results only after the official results had been verified by independent experts &amp;amp; the students had rightly received the acclamation they deserved&lt;/p&gt;— Demis Hassabis (@demishassabis) July 21, 2025&lt;/blockquote&gt; 



&lt;p&gt;Social media users were quick to note the distinction. “You see? OpenAI ignored the IMO request. Shame. No class. Straight up disrespect,” wrote one user. “Google DeepMind acted with integrity, aligned with humanity.”&lt;/p&gt;



&lt;p&gt;The criticism stems from OpenAI’s decision to announce its own mathematical olympiad results without participating in the official IMO evaluation process. Instead, OpenAI had a panel of former IMO participants grade its AI’s performance, a approach that some in the community view as lacking credibility.&lt;/p&gt;



&lt;p&gt;“OpenAI is quite possibly the worst company on the planet right now,” wrote one critic, while others suggested the company needs to “take things seriously” and “be more credible.”&lt;/p&gt;



&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;You see?&lt;/p&gt;&lt;p&gt;OpenAI ignored the IMO request. Shame. No class. Straight up disrespect. &lt;/p&gt;&lt;p&gt;Google DeepMind acted with integrity, aligned with humanity. &lt;/p&gt;&lt;p&gt;TRVTHNUKE pic.twitter.com/8LAOak6XUE&lt;/p&gt;— NIK (@ns123abc) July 21, 2025&lt;/blockquote&gt; 



&lt;h2 class="wp-block-heading" id="h-inside-the-training-methods-that-powered-gemini-s-mathematical-mastery"&gt;Inside the training methods that powered Gemini’s mathematical mastery&lt;/h2&gt;



&lt;p&gt;Google DeepMind’s success appears to stem from novel training techniques that go beyond traditional approaches. The team used advanced reinforcement learning methods designed to leverage multi-step reasoning, problem-solving, and theorem-proving data. The model was also provided access to a curated collection of high-quality mathematical solutions and received specific guidance on approaching IMO-style problems.&lt;/p&gt;



&lt;p&gt;The technical achievement impressed AI researchers who noted its broader implications. “Not just solving math… but understanding language-described problems and applying abstract logic to novel cases,” wrote AI observer Elyss Wren. “This isn’t rote memory — this is emergent cognition in motion.”&lt;/p&gt;



&lt;p&gt;Ethan Mollick, a professor at the Wharton School who studies AI, emphasized the significance of using a general-purpose model rather than specialized tools. “Increasing evidence of the ability of LLMs to generalize to novel problem solving,” he wrote, highlighting how this differs from previous approaches that required specialized mathematical software.&lt;/p&gt;



&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;It wasn't just OpenAI.&lt;/p&gt;&lt;p&gt;Google also used a general purpose model to solve the very hard math problems of the International Math Olympiad in plain language. Last year they used specialized tool use&lt;/p&gt;&lt;p&gt;Increasing evidence of the ability of LLMs to generalize to novel problem solving https://t.co/Ve72fFmx2b&lt;/p&gt;— Ethan Mollick (@emollick) July 21, 2025&lt;/blockquote&gt; 



&lt;p&gt;The model demonstrated particularly impressive reasoning in one problem where many human competitors applied graduate-level mathematical concepts. According to DeepMind researcher Junehyuk Jung, Gemini “made a brilliant observation and used only elementary number theory to create a self-contained proof,” finding a more elegant solution than many human participants.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-google-deepmind-s-victory-means-for-the-200-billion-ai-race"&gt;What Google DeepMind’s victory means for the $200 billion AI race&lt;/h2&gt;



&lt;p&gt;The breakthrough comes at a critical moment in the AI industry, where companies are racing to demonstrate superior reasoning capabilities. The success has immediate practical implications: Google plans to make a version of this Deep Think model available to mathematicians for testing before rolling it out to Google AI Ultra subscribers, who pay $250 monthly for access to the company’s most advanced AI models.&lt;/p&gt;



&lt;p&gt;The timing also highlights the intensifying competition between major AI laboratories. While Google celebrated its methodical, officially-verified approach, the controversy surrounding OpenAI’s announcement reflects broader tensions about transparency and credibility in AI development.&lt;/p&gt;



&lt;p&gt;This competitive dynamic extends beyond just mathematical reasoning. Recent weeks have seen various AI companies announce breakthrough capabilities, though not all have been received positively. Elon Musk’s xAI recently launched Grok 4, which the company claimed was the “smartest AI in the world,” though leaderboard scores showed it trailing behind models from Google and OpenAI. Additionally, Grok has faced criticism for controversial features including sexualized AI companions and episodes of generating antisemitic content.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-dawn-of-ai-that-thinks-like-humans-with-real-world-consequences"&gt;The dawn of AI that thinks like humans—with real-world consequences&lt;/h2&gt;



&lt;p&gt;The mathematical olympiad victory goes beyond competitive bragging rights. Gemini’s performance demonstrates that AI systems can now match human-level reasoning in complex tasks requiring creativity, abstract thinking, and the ability to synthesize insights across multiple domains.&lt;/p&gt;



&lt;p&gt;“This is a significant advance over last year’s breakthrough result,” the DeepMind team noted in their technical announcement. The progression from requiring specialized formal languages to operating entirely in natural language suggests that AI systems are becoming more intuitive and accessible.&lt;/p&gt;



&lt;p&gt;For businesses, this development signals that AI may soon tackle complex analytical problems across various industries without requiring specialized programming or domain expertise. The ability to reason through intricate challenges using everyday language could democratize sophisticated analytical capabilities across organizations.&lt;/p&gt;



&lt;p&gt;However, questions persist about whether these reasoning capabilities will translate effectively to messier real-world challenges. The mathematical olympiad provides well-defined problems with clear success criteria — a far cry from the ambiguous, multifaceted decisions that define most business and scientific endeavors.&lt;/p&gt;



&lt;p&gt;Google DeepMind plans to return to next year’s competition “in search of a perfect score.” The company believes AI systems combining natural language fluency with rigorous reasoning “will become invaluable tools for mathematicians, scientists, engineers, and researchers, helping us advance human knowledge on the path to AGI.”&lt;/p&gt;



&lt;p&gt;But perhaps the most telling detail emerged from the competition itself: when faced with the contest’s most difficult problem, Gemini started from an incorrect hypothesis and never recovered. Only five human students solved that problem correctly. In the end, it seems, even gold medal-winning AI still has something to learn from teenage mathematicians.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Google DeepMind announced Monday that an advanced version of its Gemini artificial intelligence model has officially achieved gold medal-level performance at the International Mathematical Olympiad, solving five of six exceptionally difficult problems and earning recognition as the first AI system to receive official gold-level grading from competition organizers.&lt;/p&gt;&lt;p&gt;The victory advances the field of AI reasoning and puts Google ahead in the intensifying battle between tech giants building next-generation artificial intelligence. More importantly, it demonstrates that AI can now tackle complex mathematical problems using natural language understanding rather than requiring specialized programming languages.&lt;/p&gt;&lt;p&gt;“Official results are in — Gemini achieved gold-medal level in the International Mathematical Olympiad!” Demis Hassabis, CEO of Google DeepMind, wrote on social media platform X Monday morning. “An advanced version was able to solve 5 out of 6 problems. Incredible progress.”&lt;/p&gt;&lt;p&gt;The International Mathematical Olympiad, held annually since 1959, is widely considered the world’s most prestigious mathematics competition for pre-university students. Each participating country sends six elite young mathematicians to compete in solving six exceptionally challenging problems spanning algebra, combinatorics, geometry, and number theory. Only about 8% of human participants typically earn gold medals.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-how-google-deepmind-s-gemini-deep-think-cracked-math-s-toughest-problems"&gt;How Google DeepMind’s Gemini Deep Think cracked math’s toughest problems&lt;/h2&gt;



&lt;p&gt;Google’s latest success far exceeds its 2024 performance, when the company’s combined AlphaProof and AlphaGeometry systems earned silver medal status by solving four of six problems. That earlier system required human experts to first translate natural language problems into domain-specific programming languages and then interpret the AI’s mathematical output.&lt;/p&gt;



&lt;p&gt;This year’s breakthrough came through Gemini Deep Think, an enhanced reasoning system that employs what researchers call “parallel thinking.” Unlike traditional AI models that follow a single chain of reasoning, Deep Think simultaneously explores multiple possible solutions before arriving at a final answer.&lt;/p&gt;



&lt;p&gt;“Our model operated end-to-end in natural language, producing rigorous mathematical proofs directly from the official problem descriptions,” Hassabis explained in a follow-up post on the social media site X, emphasizing that the system completed its work within the competition’s standard 4.5-hour time limit.&lt;/p&gt;



&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;We achieved this year’s impressive result using an advanced version of Gemini Deep Think (an enhanced reasoning mode for complex problems). Our model operated end-to-end in natural language, producing rigorous mathematical proofs directly from the official problem descriptions –…&lt;/p&gt;— Demis Hassabis (@demishassabis) July 21, 2025&lt;/blockquote&gt; 



&lt;p&gt;The model achieved 35 out of a possible 42 points, comfortably exceeding the gold medal threshold. According to IMO President Prof. Dr. Gregor Dolinar, the solutions were “astonishing in many respects” and found to be “clear, precise and most of them easy to follow” by competition graders.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-openai-faces-backlash-for-bypassing-official-competition-rules"&gt;OpenAI faces backlash for bypassing official competition rules&lt;/h2&gt;



&lt;p&gt;The announcement comes amid growing tension in the AI industry over competitive practices and transparency. Google DeepMind’s measured approach to releasing its results has drawn praise from the AI community, particularly in contrast to rival OpenAI’s handling of similar achievements.&lt;/p&gt;



&lt;p&gt;“We didn’t announce on Friday because we respected the IMO Board’s original request that all AI labs share their results only after the official results had been verified by independent experts &amp;amp; the students had rightly received the acclamation they deserved,” Hassabis wrote, appearing to reference OpenAI’s earlier announcement of its own olympiad performance.&lt;/p&gt;



&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Btw as an aside, we didn’t announce on Friday because we respected the IMO Board's original request that all AI labs share their results only after the official results had been verified by independent experts &amp;amp; the students had rightly received the acclamation they deserved&lt;/p&gt;— Demis Hassabis (@demishassabis) July 21, 2025&lt;/blockquote&gt; 



&lt;p&gt;Social media users were quick to note the distinction. “You see? OpenAI ignored the IMO request. Shame. No class. Straight up disrespect,” wrote one user. “Google DeepMind acted with integrity, aligned with humanity.”&lt;/p&gt;



&lt;p&gt;The criticism stems from OpenAI’s decision to announce its own mathematical olympiad results without participating in the official IMO evaluation process. Instead, OpenAI had a panel of former IMO participants grade its AI’s performance, a approach that some in the community view as lacking credibility.&lt;/p&gt;



&lt;p&gt;“OpenAI is quite possibly the worst company on the planet right now,” wrote one critic, while others suggested the company needs to “take things seriously” and “be more credible.”&lt;/p&gt;



&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;You see?&lt;/p&gt;&lt;p&gt;OpenAI ignored the IMO request. Shame. No class. Straight up disrespect. &lt;/p&gt;&lt;p&gt;Google DeepMind acted with integrity, aligned with humanity. &lt;/p&gt;&lt;p&gt;TRVTHNUKE pic.twitter.com/8LAOak6XUE&lt;/p&gt;— NIK (@ns123abc) July 21, 2025&lt;/blockquote&gt; 



&lt;h2 class="wp-block-heading" id="h-inside-the-training-methods-that-powered-gemini-s-mathematical-mastery"&gt;Inside the training methods that powered Gemini’s mathematical mastery&lt;/h2&gt;



&lt;p&gt;Google DeepMind’s success appears to stem from novel training techniques that go beyond traditional approaches. The team used advanced reinforcement learning methods designed to leverage multi-step reasoning, problem-solving, and theorem-proving data. The model was also provided access to a curated collection of high-quality mathematical solutions and received specific guidance on approaching IMO-style problems.&lt;/p&gt;



&lt;p&gt;The technical achievement impressed AI researchers who noted its broader implications. “Not just solving math… but understanding language-described problems and applying abstract logic to novel cases,” wrote AI observer Elyss Wren. “This isn’t rote memory — this is emergent cognition in motion.”&lt;/p&gt;



&lt;p&gt;Ethan Mollick, a professor at the Wharton School who studies AI, emphasized the significance of using a general-purpose model rather than specialized tools. “Increasing evidence of the ability of LLMs to generalize to novel problem solving,” he wrote, highlighting how this differs from previous approaches that required specialized mathematical software.&lt;/p&gt;



&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;It wasn't just OpenAI.&lt;/p&gt;&lt;p&gt;Google also used a general purpose model to solve the very hard math problems of the International Math Olympiad in plain language. Last year they used specialized tool use&lt;/p&gt;&lt;p&gt;Increasing evidence of the ability of LLMs to generalize to novel problem solving https://t.co/Ve72fFmx2b&lt;/p&gt;— Ethan Mollick (@emollick) July 21, 2025&lt;/blockquote&gt; 



&lt;p&gt;The model demonstrated particularly impressive reasoning in one problem where many human competitors applied graduate-level mathematical concepts. According to DeepMind researcher Junehyuk Jung, Gemini “made a brilliant observation and used only elementary number theory to create a self-contained proof,” finding a more elegant solution than many human participants.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-google-deepmind-s-victory-means-for-the-200-billion-ai-race"&gt;What Google DeepMind’s victory means for the $200 billion AI race&lt;/h2&gt;



&lt;p&gt;The breakthrough comes at a critical moment in the AI industry, where companies are racing to demonstrate superior reasoning capabilities. The success has immediate practical implications: Google plans to make a version of this Deep Think model available to mathematicians for testing before rolling it out to Google AI Ultra subscribers, who pay $250 monthly for access to the company’s most advanced AI models.&lt;/p&gt;



&lt;p&gt;The timing also highlights the intensifying competition between major AI laboratories. While Google celebrated its methodical, officially-verified approach, the controversy surrounding OpenAI’s announcement reflects broader tensions about transparency and credibility in AI development.&lt;/p&gt;



&lt;p&gt;This competitive dynamic extends beyond just mathematical reasoning. Recent weeks have seen various AI companies announce breakthrough capabilities, though not all have been received positively. Elon Musk’s xAI recently launched Grok 4, which the company claimed was the “smartest AI in the world,” though leaderboard scores showed it trailing behind models from Google and OpenAI. Additionally, Grok has faced criticism for controversial features including sexualized AI companions and episodes of generating antisemitic content.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-dawn-of-ai-that-thinks-like-humans-with-real-world-consequences"&gt;The dawn of AI that thinks like humans—with real-world consequences&lt;/h2&gt;



&lt;p&gt;The mathematical olympiad victory goes beyond competitive bragging rights. Gemini’s performance demonstrates that AI systems can now match human-level reasoning in complex tasks requiring creativity, abstract thinking, and the ability to synthesize insights across multiple domains.&lt;/p&gt;



&lt;p&gt;“This is a significant advance over last year’s breakthrough result,” the DeepMind team noted in their technical announcement. The progression from requiring specialized formal languages to operating entirely in natural language suggests that AI systems are becoming more intuitive and accessible.&lt;/p&gt;



&lt;p&gt;For businesses, this development signals that AI may soon tackle complex analytical problems across various industries without requiring specialized programming or domain expertise. The ability to reason through intricate challenges using everyday language could democratize sophisticated analytical capabilities across organizations.&lt;/p&gt;



&lt;p&gt;However, questions persist about whether these reasoning capabilities will translate effectively to messier real-world challenges. The mathematical olympiad provides well-defined problems with clear success criteria — a far cry from the ambiguous, multifaceted decisions that define most business and scientific endeavors.&lt;/p&gt;



&lt;p&gt;Google DeepMind plans to return to next year’s competition “in search of a perfect score.” The company believes AI systems combining natural language fluency with rigorous reasoning “will become invaluable tools for mathematicians, scientists, engineers, and researchers, helping us advance human knowledge on the path to AGI.”&lt;/p&gt;



&lt;p&gt;But perhaps the most telling detail emerged from the competition itself: when faced with the contest’s most difficult problem, Gemini started from an incorrect hypothesis and never recovered. Only five human students solved that problem correctly. In the end, it seems, even gold medal-winning AI still has something to learn from teenage mathematicians.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/google-deepmind-makes-ai-history-with-gold-medal-win-at-worlds-toughest-math-competition/</guid><pubDate>Mon, 21 Jul 2025 22:33:34 +0000</pubDate></item><item><title>[NEW] Crowdstrike’s massive cyber outage 1-year later: lessons enterprises can learn to improve security (AI News | VentureBeat)</title><link>https://venturebeat.com/security/how-crowdstrikes-78-minute-outage-reshaped-enterprise-cybersecurity/</link><description>&lt;p&gt;As we wrote in our initial analysis of the CrowdStrike incident, the July 19, 2024, outage served as a stark reminder of the importance of cyber resilience. Now, one year later, both CrowdStrike and the industry have undergone significant transformation, with the catalyst being driven by 78 minutes that changed everything.&lt;/p&gt;&lt;p&gt;“The first anniversary of July 19 marks a moment that deeply impacted our customers and partners and became one of the most defining chapters in CrowdStrike’s history,” CrowdStrike’s President Mike Sentonas wrote in a blog detailing the company’s year-long journey toward enhanced resilience.&lt;/p&gt;&lt;p&gt;The numbers remain sobering: A faulty Channel File 291 update, deployed at 04:09 UTC and reverted just 78 minutes later, crashed 8.5 million Windows systems worldwide. Insurance estimates put losses at $5.4 billion for the top 500 U.S. companies alone, with aviation particularly hard hit with 5,078 flights canceled globally.&lt;/p&gt;&lt;p&gt;Steffen Schreier, senior vice president of product and portfolio at Telesign, a Proximus Global company, captures why this incident resonates a year later: “One year later, the CrowdStrike incident isn’t just remembered, it’s impossible to forget. A routine software update, deployed with no malicious intent and rolled back in just 78 minutes, still managed to take down critical infrastructure worldwide. No breach. No attack. Just one internal failure with global consequences.”&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;His technical analysis reveals uncomfortable truths about modern infrastructure: “That’s the real wake-up call: even companies with strong practices, a staged rollout, fast rollback, can’t outpace the risks introduced by the very infrastructure that enables rapid, cloud-native delivery. The same velocity that empowers us to ship faster also accelerates the blast radius when something goes wrong.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-understanding-what-went-wrong"&gt;&lt;strong&gt;Understanding what went wrong&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;CrowdStrike’s root cause analysis revealed a cascade of technical failures: a mismatch between input fields in their IPC Template Type, missing runtime array bounds checks and a logic error in their Content Validator. These weren’t edge cases but fundamental quality control gaps.&lt;/p&gt;



&lt;p&gt;Merritt Baer, incoming Chief Security Officer at Enkrypt AI and advisor to companies including Andesite, provides crucial context: “CrowdStrike’s outage was humbling; it reminded us that even really big, mature shops get processes wrong sometimes. This particular outcome was a coincidence on some level, but it should have never been possible. It demonstrated that they failed to instate some basic CI/CD protocols.”&lt;/p&gt;



&lt;p&gt;Her assessment is direct but fair: “Had CrowdStrike rolled out the update in sandboxes and only sent it in production in increments as is best practice, it would have been less catastrophic, if at all.”&lt;/p&gt;



&lt;p&gt;Yet Baer also recognizes CrowdStrike’s response: “CrowdStrike’s comms strategy demonstrated good executive ownership. Execs should always take ownership—it’s not the intern’s fault. If your junior operator can get it wrong, it’s my fault. It’s our fault as a company.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-leadership-s-accountability"&gt;&lt;strong&gt;Leadership’s accountability&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;George Kurtz, CrowdStrike’s founder and CEO, exemplified this ownership principle. In a LinkedIn post reflecting on the anniversary, Kurtz wrote: “One year ago, we faced a moment that tested everything: our technology, our operations, and the trust others placed in us. As founder and CEO, I took that responsibility personally. I always have and always will.”&lt;/p&gt;



&lt;p&gt;His perspective reveals how the company channeled crisis into transformation: “What defined us wasn’t that moment; it was everything that came next. From the start, our focus was clear: build an even stronger CrowdStrike, grounded in resilience, transparency, and relentless execution. Our North Star has always been our customers.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-crowdstrike-goes-all-in-on-a-new-resilient-by-design-framework"&gt;&lt;strong&gt;CrowdStrike goes all-in on a new Resilient by Design framework&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;CrowdStrike’s response centered on their Resilient by Design framework, which Sentonas describes as going beyond “quick fixes or surface-level improvements.” The framework’s three pillars, including Foundational, Adaptive and Continuous components, represent a comprehensive rethinking of how security platforms should operate.&lt;/p&gt;



&lt;p&gt;Key implementations include:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Sensor Self-Recovery&lt;/strong&gt;: Automatically detects crash loops and transitions to safe mode&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;New Content Distribution System&lt;/strong&gt;: Ring-based deployment with automated safeguards&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Enhanced Customer Control&lt;/strong&gt;: Granular update management and content pinning capabilities&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Digital Operations Center&lt;/strong&gt;: Purpose-built facility for global infrastructure monitoring&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Falcon Super Lab&lt;/strong&gt;: Testing thousands of OS, kernel and hardware combinations&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;“We didn’t just add a few content configuration options,” Sentonas emphasized in his blog. “We fundamentally rethought how customers could interact with and control enterprise security platforms.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-industry-wide-supply-chain-awakening"&gt;&lt;strong&gt;Industry-wide supply chain awakening&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;The incident forced a broader reckoning about vendor dependencies. Baer frames the lesson starkly: “One huge practical lesson was just that your vendors are part of your supply chain. So, as a CISO, you should test the risk to be aware of it, but simply speaking, this issue fell on the provider side of the shared responsibility model. A customer wouldn’t have controlled it.”&lt;/p&gt;



&lt;p&gt;CrowdStrike’s outage has permanently altered vendor evaluation: “I see effective CISOs and CSOs taking lessons from this, around the companies they want to work with and the security they receive as a product of doing business together. I will only ever work with companies that I respect from a security posture lens. They don’t need to be perfect, but I want to know that they are doing the right processes, over time.”&lt;/p&gt;



&lt;p&gt;Sam Curry, CISO at Zscaler, added, “What happened to CrowdStrike was unfortunate, but it could have happened to many, so perhaps we don’t put the blame on them with the benefit of hindsight. What I will say is that the world has used this to refocus and has placed more attention to resilience as a result, and that’s a win for everyone, as our collective goal is to make the internet safer and more secure for all.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-underscores-the-need-for-a-new-security-paradigm"&gt;&lt;strong&gt;Underscores the need for a new security paradigm&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Schreier’s analysis extends beyond CrowdStrike to fundamental security architecture: “Speed at scale comes at a cost. Every routine update now carries the weight of potential systemic failure. That means more than testing, it means safeguards built for resilience: layered defenses, automatic rollback paths and fail-safes that assume telemetry might disappear exactly when you need it most.”&lt;/p&gt;



&lt;p&gt;His most critical insight addresses a scenario many hadn’t considered: “And when telemetry goes dark, you need fail-safes that assume visibility might vanish.”&lt;/p&gt;



&lt;p&gt;This represents a paradigm shift. As Schreier concludes: “Because security today isn’t just about keeping attackers out—it’s about making absolutely sure your own systems never become the single point of failure.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-looking-forward-ai-and-future-challenges"&gt;&lt;strong&gt;Looking forward: AI and future challenges&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Baer sees the next evolution already emerging: “Ever since cloud has enabled us to build using infrastructure as code, but especially now that AI is enabling us to do security differently, I am looking at how infrastructure decisions are layered with autonomy from humans and AI. We can and should layer on reasoning as well as effective risk mitigation for processes like forced updates, especially at high levels of privilege.”&lt;/p&gt;



&lt;p&gt;CrowdStrike’s forward-looking initiatives include:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Hiring a Chief Resilience Officer reporting directly to the CEO&lt;/li&gt;



&lt;li&gt;Project Ascent, exploring capabilities beyond kernel space&lt;/li&gt;



&lt;li&gt;Collaboration with Microsoft on the Windows Endpoint Security Platform&lt;/li&gt;



&lt;li&gt;ISO 22301 certification for business continuity management&lt;/li&gt;
&lt;/ul&gt;



&lt;h2 class="wp-block-heading" id="h-a-stronger-ecosystem"&gt;&lt;strong&gt;A stronger ecosystem&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;One year later, the transformation is evident. Kurtz reflects: “We’re a stronger company today than we were a year ago. The work continues. The mission endures. And we’re moving forward: stronger, smarter, and even more committed than ever.”&lt;/p&gt;



&lt;p&gt;To his credit, Kurtz also acknowledges those who stood by the company: “To every customer who stayed with us, even when it was hard, thank you for your enduring trust. To our incredible partners who stood by us and rolled up their sleeves, thank you for being our extended family.”&lt;/p&gt;



&lt;p&gt;The incident’s legacy extends far beyond CrowdStrike. Organizations now implement staged rollouts, maintain manual override capabilities and—crucially—plan for when security tools themselves might fail. Vendor relationships are evaluated with new rigor, recognizing that in our interconnected infrastructure, every component is critical.&lt;/p&gt;



&lt;p&gt;As Sentonas acknowledges: “This work isn’t finished and never will be. Resilience isn’t a milestone; it’s a discipline that requires continuous commitment and evolution.” The CrowdStrike incident of July 19, 2024, will be remembered not just for the disruption it caused but for catalyzing an industry-wide evolution toward true resilience.&lt;/p&gt;



&lt;p&gt;In facing their greatest challenge, CrowdStrike and the broader security ecosystem have emerged with a deeper understanding: protecting against threats means ensuring the protectors themselves can do no harm. That lesson, learned through 78 difficult minutes and a year of transformation, may prove to be the incident’s most valuable legacy.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;p&gt;As we wrote in our initial analysis of the CrowdStrike incident, the July 19, 2024, outage served as a stark reminder of the importance of cyber resilience. Now, one year later, both CrowdStrike and the industry have undergone significant transformation, with the catalyst being driven by 78 minutes that changed everything.&lt;/p&gt;&lt;p&gt;“The first anniversary of July 19 marks a moment that deeply impacted our customers and partners and became one of the most defining chapters in CrowdStrike’s history,” CrowdStrike’s President Mike Sentonas wrote in a blog detailing the company’s year-long journey toward enhanced resilience.&lt;/p&gt;&lt;p&gt;The numbers remain sobering: A faulty Channel File 291 update, deployed at 04:09 UTC and reverted just 78 minutes later, crashed 8.5 million Windows systems worldwide. Insurance estimates put losses at $5.4 billion for the top 500 U.S. companies alone, with aviation particularly hard hit with 5,078 flights canceled globally.&lt;/p&gt;&lt;p&gt;Steffen Schreier, senior vice president of product and portfolio at Telesign, a Proximus Global company, captures why this incident resonates a year later: “One year later, the CrowdStrike incident isn’t just remembered, it’s impossible to forget. A routine software update, deployed with no malicious intent and rolled back in just 78 minutes, still managed to take down critical infrastructure worldwide. No breach. No attack. Just one internal failure with global consequences.”&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;His technical analysis reveals uncomfortable truths about modern infrastructure: “That’s the real wake-up call: even companies with strong practices, a staged rollout, fast rollback, can’t outpace the risks introduced by the very infrastructure that enables rapid, cloud-native delivery. The same velocity that empowers us to ship faster also accelerates the blast radius when something goes wrong.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-understanding-what-went-wrong"&gt;&lt;strong&gt;Understanding what went wrong&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;CrowdStrike’s root cause analysis revealed a cascade of technical failures: a mismatch between input fields in their IPC Template Type, missing runtime array bounds checks and a logic error in their Content Validator. These weren’t edge cases but fundamental quality control gaps.&lt;/p&gt;



&lt;p&gt;Merritt Baer, incoming Chief Security Officer at Enkrypt AI and advisor to companies including Andesite, provides crucial context: “CrowdStrike’s outage was humbling; it reminded us that even really big, mature shops get processes wrong sometimes. This particular outcome was a coincidence on some level, but it should have never been possible. It demonstrated that they failed to instate some basic CI/CD protocols.”&lt;/p&gt;



&lt;p&gt;Her assessment is direct but fair: “Had CrowdStrike rolled out the update in sandboxes and only sent it in production in increments as is best practice, it would have been less catastrophic, if at all.”&lt;/p&gt;



&lt;p&gt;Yet Baer also recognizes CrowdStrike’s response: “CrowdStrike’s comms strategy demonstrated good executive ownership. Execs should always take ownership—it’s not the intern’s fault. If your junior operator can get it wrong, it’s my fault. It’s our fault as a company.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-leadership-s-accountability"&gt;&lt;strong&gt;Leadership’s accountability&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;George Kurtz, CrowdStrike’s founder and CEO, exemplified this ownership principle. In a LinkedIn post reflecting on the anniversary, Kurtz wrote: “One year ago, we faced a moment that tested everything: our technology, our operations, and the trust others placed in us. As founder and CEO, I took that responsibility personally. I always have and always will.”&lt;/p&gt;



&lt;p&gt;His perspective reveals how the company channeled crisis into transformation: “What defined us wasn’t that moment; it was everything that came next. From the start, our focus was clear: build an even stronger CrowdStrike, grounded in resilience, transparency, and relentless execution. Our North Star has always been our customers.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-crowdstrike-goes-all-in-on-a-new-resilient-by-design-framework"&gt;&lt;strong&gt;CrowdStrike goes all-in on a new Resilient by Design framework&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;CrowdStrike’s response centered on their Resilient by Design framework, which Sentonas describes as going beyond “quick fixes or surface-level improvements.” The framework’s three pillars, including Foundational, Adaptive and Continuous components, represent a comprehensive rethinking of how security platforms should operate.&lt;/p&gt;



&lt;p&gt;Key implementations include:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Sensor Self-Recovery&lt;/strong&gt;: Automatically detects crash loops and transitions to safe mode&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;New Content Distribution System&lt;/strong&gt;: Ring-based deployment with automated safeguards&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Enhanced Customer Control&lt;/strong&gt;: Granular update management and content pinning capabilities&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Digital Operations Center&lt;/strong&gt;: Purpose-built facility for global infrastructure monitoring&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Falcon Super Lab&lt;/strong&gt;: Testing thousands of OS, kernel and hardware combinations&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;“We didn’t just add a few content configuration options,” Sentonas emphasized in his blog. “We fundamentally rethought how customers could interact with and control enterprise security platforms.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-industry-wide-supply-chain-awakening"&gt;&lt;strong&gt;Industry-wide supply chain awakening&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;The incident forced a broader reckoning about vendor dependencies. Baer frames the lesson starkly: “One huge practical lesson was just that your vendors are part of your supply chain. So, as a CISO, you should test the risk to be aware of it, but simply speaking, this issue fell on the provider side of the shared responsibility model. A customer wouldn’t have controlled it.”&lt;/p&gt;



&lt;p&gt;CrowdStrike’s outage has permanently altered vendor evaluation: “I see effective CISOs and CSOs taking lessons from this, around the companies they want to work with and the security they receive as a product of doing business together. I will only ever work with companies that I respect from a security posture lens. They don’t need to be perfect, but I want to know that they are doing the right processes, over time.”&lt;/p&gt;



&lt;p&gt;Sam Curry, CISO at Zscaler, added, “What happened to CrowdStrike was unfortunate, but it could have happened to many, so perhaps we don’t put the blame on them with the benefit of hindsight. What I will say is that the world has used this to refocus and has placed more attention to resilience as a result, and that’s a win for everyone, as our collective goal is to make the internet safer and more secure for all.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-underscores-the-need-for-a-new-security-paradigm"&gt;&lt;strong&gt;Underscores the need for a new security paradigm&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Schreier’s analysis extends beyond CrowdStrike to fundamental security architecture: “Speed at scale comes at a cost. Every routine update now carries the weight of potential systemic failure. That means more than testing, it means safeguards built for resilience: layered defenses, automatic rollback paths and fail-safes that assume telemetry might disappear exactly when you need it most.”&lt;/p&gt;



&lt;p&gt;His most critical insight addresses a scenario many hadn’t considered: “And when telemetry goes dark, you need fail-safes that assume visibility might vanish.”&lt;/p&gt;



&lt;p&gt;This represents a paradigm shift. As Schreier concludes: “Because security today isn’t just about keeping attackers out—it’s about making absolutely sure your own systems never become the single point of failure.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-looking-forward-ai-and-future-challenges"&gt;&lt;strong&gt;Looking forward: AI and future challenges&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Baer sees the next evolution already emerging: “Ever since cloud has enabled us to build using infrastructure as code, but especially now that AI is enabling us to do security differently, I am looking at how infrastructure decisions are layered with autonomy from humans and AI. We can and should layer on reasoning as well as effective risk mitigation for processes like forced updates, especially at high levels of privilege.”&lt;/p&gt;



&lt;p&gt;CrowdStrike’s forward-looking initiatives include:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Hiring a Chief Resilience Officer reporting directly to the CEO&lt;/li&gt;



&lt;li&gt;Project Ascent, exploring capabilities beyond kernel space&lt;/li&gt;



&lt;li&gt;Collaboration with Microsoft on the Windows Endpoint Security Platform&lt;/li&gt;



&lt;li&gt;ISO 22301 certification for business continuity management&lt;/li&gt;
&lt;/ul&gt;



&lt;h2 class="wp-block-heading" id="h-a-stronger-ecosystem"&gt;&lt;strong&gt;A stronger ecosystem&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;One year later, the transformation is evident. Kurtz reflects: “We’re a stronger company today than we were a year ago. The work continues. The mission endures. And we’re moving forward: stronger, smarter, and even more committed than ever.”&lt;/p&gt;



&lt;p&gt;To his credit, Kurtz also acknowledges those who stood by the company: “To every customer who stayed with us, even when it was hard, thank you for your enduring trust. To our incredible partners who stood by us and rolled up their sleeves, thank you for being our extended family.”&lt;/p&gt;



&lt;p&gt;The incident’s legacy extends far beyond CrowdStrike. Organizations now implement staged rollouts, maintain manual override capabilities and—crucially—plan for when security tools themselves might fail. Vendor relationships are evaluated with new rigor, recognizing that in our interconnected infrastructure, every component is critical.&lt;/p&gt;



&lt;p&gt;As Sentonas acknowledges: “This work isn’t finished and never will be. Resilience isn’t a milestone; it’s a discipline that requires continuous commitment and evolution.” The CrowdStrike incident of July 19, 2024, will be remembered not just for the disruption it caused but for catalyzing an industry-wide evolution toward true resilience.&lt;/p&gt;



&lt;p&gt;In facing their greatest challenge, CrowdStrike and the broader security ecosystem have emerged with a deeper understanding: protecting against threats means ensuring the protectors themselves can do no harm. That lesson, learned through 78 difficult minutes and a year of transformation, may prove to be the incident’s most valuable legacy.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/security/how-crowdstrikes-78-minute-outage-reshaped-enterprise-cybersecurity/</guid><pubDate>Mon, 21 Jul 2025 22:46:41 +0000</pubDate></item><item><title>[NEW] OpenAI and Google outdo the mathletes, but not each other (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/21/openai-and-google-outdo-the-mathletes-but-not-each-other/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2016/03/shutterstock_317431127.jpg?w=1000" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI models from OpenAI and Google DeepMind achieved gold-medal scores in the 2025 International Math Olympiad (IMO), one of the world’s oldest and most challenging high school-level math competitions, the companies independently announced in recent days.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The results underscore just how fast AI systems are advancing, and yet, how evenly matched Google and OpenAI seem to be in the AI race. AI companies are competing fiercely for the public perception of being ahead in the AI race: an intangible battle of “vibes” that can have big implications for securing top AI talent. A lot of AI researchers come from backgrounds in competitive math, so benchmarks like IMO mean more than others.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Last year, Google scored a silver medal at IMO using a “formal” system, meaning it required humans to translate problems into a machine‑readable format. This year, both OpenAI and Google entered “informal” systems into the competition, which were able to ingest questions and generate proof‑based answers in natural language. Both companies claim their AI models correctly answered five out of six questions on IMO’s test, scoring higher than most high school students and Google’s AI model from last year, without requiring any human-machine translation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In interviews with TechCrunch, researchers behind OpenAI and Google’s IMO efforts claimed that these gold-medal performances represent breakthroughs around AI reasoning models in non-verifiable domains. While AI reasoning models tend to do well on questions with straightforward answers, such as simple math or coding tasks, these systems struggle on tasks with more ambiguous solutions, such as buying a great chair or helping with complex research.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, Google is raising questions around how OpenAI conducted and announced its gold-medal IMO performance. After all, if you’re going to enter AI models into a math contest for high schoolers, you might as well argue like teenagers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Shortly after OpenAI announced its feat on Saturday morning, Google DeepMind’s CEO and researchers took to social media to slam OpenAI for announcing its gold medal prematurely — shortly after IMO announced which high schoolers had won the competition on Friday night — and for not having their model’s test officially evaluated by IMO.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Btw as an aside, we didn’t announce on Friday because we respected the IMO Board's original request that all AI labs share their results only after the official results had been verified by independent experts &amp;amp; the students had rightly received the acclamation they deserved&lt;/p&gt;— Demis Hassabis (@demishassabis) July 21, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Thang Luong, a Google DeepMind senior researcher and lead for the IMO project, told TechCrunch that Google waited to announce its IMO results to respect the students participating in the competition.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Luong said that Google has been working with IMO’s organizers since last year in preparation for the test and wanted to have the IMO president’s blessing and official grading before announcing its official results, which it did on Monday morning.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The IMO organizers have their grading guideline,” Luong said. “So any evaluation that’s not based on that guideline could not make any claim about gold-medal level [performance].”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Noam Brown, a senior OpenAI researcher who worked on the IMO model, told TechCrunch that IMO reached out to OpenAI a few months ago about participating in a formal math competition, but the ChatGPT-maker declined because it was working on natural language systems that it thought were more worth pursuing. Brown says OpenAI didn’t know IMO was conducting an informal test with Google.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI says it hired third-party evaluators — three former IMO medalists who understood the grading system — to grade its AI model’s performance. After OpenAI learned of its gold-medal score, Brown said the company reached out to IMO, which then told the company to wait to announce until after IMO’s Friday night award ceremony.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;IMO did not respond to TechCrunch’s request for comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google isn’t necessarily wrong here — it did go through a more official, rigorous process to achieve its gold-medal score — but the debate may miss the bigger picture: AI models from several leading AI labs are improving quickly. Countries from around the world sent their brightest students to compete at IMO this year, and just a few percent of them scored as well as OpenAI and Google’s AI models did.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While OpenAI used to have a significant lead over the industry, it certainly feels as though the race is more closely matched than any company would like to admit. OpenAI is expected to release GPT-5 in the coming months, and the company certainly hopes to give off the impression that it still leads the AI industry.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2016/03/shutterstock_317431127.jpg?w=1000" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI models from OpenAI and Google DeepMind achieved gold-medal scores in the 2025 International Math Olympiad (IMO), one of the world’s oldest and most challenging high school-level math competitions, the companies independently announced in recent days.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The results underscore just how fast AI systems are advancing, and yet, how evenly matched Google and OpenAI seem to be in the AI race. AI companies are competing fiercely for the public perception of being ahead in the AI race: an intangible battle of “vibes” that can have big implications for securing top AI talent. A lot of AI researchers come from backgrounds in competitive math, so benchmarks like IMO mean more than others.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Last year, Google scored a silver medal at IMO using a “formal” system, meaning it required humans to translate problems into a machine‑readable format. This year, both OpenAI and Google entered “informal” systems into the competition, which were able to ingest questions and generate proof‑based answers in natural language. Both companies claim their AI models correctly answered five out of six questions on IMO’s test, scoring higher than most high school students and Google’s AI model from last year, without requiring any human-machine translation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In interviews with TechCrunch, researchers behind OpenAI and Google’s IMO efforts claimed that these gold-medal performances represent breakthroughs around AI reasoning models in non-verifiable domains. While AI reasoning models tend to do well on questions with straightforward answers, such as simple math or coding tasks, these systems struggle on tasks with more ambiguous solutions, such as buying a great chair or helping with complex research.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, Google is raising questions around how OpenAI conducted and announced its gold-medal IMO performance. After all, if you’re going to enter AI models into a math contest for high schoolers, you might as well argue like teenagers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Shortly after OpenAI announced its feat on Saturday morning, Google DeepMind’s CEO and researchers took to social media to slam OpenAI for announcing its gold medal prematurely — shortly after IMO announced which high schoolers had won the competition on Friday night — and for not having their model’s test officially evaluated by IMO.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Btw as an aside, we didn’t announce on Friday because we respected the IMO Board's original request that all AI labs share their results only after the official results had been verified by independent experts &amp;amp; the students had rightly received the acclamation they deserved&lt;/p&gt;— Demis Hassabis (@demishassabis) July 21, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Thang Luong, a Google DeepMind senior researcher and lead for the IMO project, told TechCrunch that Google waited to announce its IMO results to respect the students participating in the competition.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Luong said that Google has been working with IMO’s organizers since last year in preparation for the test and wanted to have the IMO president’s blessing and official grading before announcing its official results, which it did on Monday morning.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The IMO organizers have their grading guideline,” Luong said. “So any evaluation that’s not based on that guideline could not make any claim about gold-medal level [performance].”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Noam Brown, a senior OpenAI researcher who worked on the IMO model, told TechCrunch that IMO reached out to OpenAI a few months ago about participating in a formal math competition, but the ChatGPT-maker declined because it was working on natural language systems that it thought were more worth pursuing. Brown says OpenAI didn’t know IMO was conducting an informal test with Google.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI says it hired third-party evaluators — three former IMO medalists who understood the grading system — to grade its AI model’s performance. After OpenAI learned of its gold-medal score, Brown said the company reached out to IMO, which then told the company to wait to announce until after IMO’s Friday night award ceremony.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;IMO did not respond to TechCrunch’s request for comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google isn’t necessarily wrong here — it did go through a more official, rigorous process to achieve its gold-medal score — but the debate may miss the bigger picture: AI models from several leading AI labs are improving quickly. Countries from around the world sent their brightest students to compete at IMO this year, and just a few percent of them scored as well as OpenAI and Google’s AI models did.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While OpenAI used to have a significant lead over the industry, it certainly feels as though the race is more closely matched than any company would like to admit. OpenAI is expected to release GPT-5 in the coming months, and the company certainly hopes to give off the impression that it still leads the AI industry.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/21/openai-and-google-outdo-the-mathletes-but-not-each-other/</guid><pubDate>Tue, 22 Jul 2025 00:06:41 +0000</pubDate></item></channel></rss>