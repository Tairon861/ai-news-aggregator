<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 27 Nov 2025 12:47:57 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>[NEW] Edge AI inside the human body: Cochlear’s machine learning implant breakthrough (AI News)</title><link>https://www.artificialintelligence-news.com/news/edge-ai-medical-devices-cochlear-implants/</link><description>&lt;p&gt;The next frontier for edge AI medical devices isn’t wearables or bedside monitors—it’s inside the human body itself. Cochlear’s newly launched&amp;nbsp;Nucleus Nexa System&amp;nbsp;represents the first cochlear implant capable of running machine learning algorithms while managing extreme power constraints, storing&amp;nbsp;personalised data on-device, and receiving over-the-air firmware updates to improve its AI models over time.&lt;/p&gt;&lt;p&gt;For AI practitioners, the technical challenge is staggering: build a decision-tree model that classifies five distinct auditory environments in real time, optimise it to run on a device with&amp;nbsp;a&amp;nbsp;minimal power budget that must last decades, and do it all while directly interfacing with human neural tissue.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-110944" height="683" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/20251113-Cochlear-30-Years-Anniversary_Press-2-1024x683.jpg" width="1024" /&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-decision-trees-meet-ultra-low-power-computing"&gt;Decision trees meet ultra-low power computing&lt;/h3&gt;&lt;p&gt;At the core of the system’s intelligence lies SCAN 2, an environmental classifier that analyses incoming audio and categorises it as Speech, Speech in Noise, Noise, Music, or Quiet.&lt;/p&gt;&lt;p&gt;“These classifications are then input to a decision tree, which is a type of machine learning model,” explains Jan Janssen, Cochlear’s Global CTO, in an exclusive interview with&amp;nbsp;&lt;em&gt;AI News&lt;/em&gt;. “This decision is used to adjust sound processing settings for that situation, which adapts the electrical signals sent to the implant.”&lt;/p&gt;&lt;p&gt;The model runs on the external sound processor, but here’s where it gets interesting: the implant itself participates in the intelligence through Dynamic Power Management. Data and power are interleaved between the processor and implant via an enhanced RF link, allowing the chipset to optimise power efficiency based on the ML model’s environmental classifications.&lt;/p&gt;&lt;p&gt;This isn’t just smart power management—it’s edge AI medical devices solving one of the hardest problems in implantable computing: how do you keep a device operational for 40+ years when you can’t replace its battery?&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-spatial-intelligence-layer"&gt;The spatial intelligence layer&lt;/h3&gt;&lt;p&gt;Beyond environmental classification, the system employs ForwardFocus, a spatial noise algorithm that uses inputs from two omnidirectional microphones to create target and noise spatial patterns. The algorithm assumes target signals originate from the front while noise comes from the sides or behind, then applies spatial filtering to attenuate background interference.&lt;/p&gt;&lt;p&gt;What makes this noteworthy from an AI perspective is the automation layer. ForwardFocus can operate autonomously, removing cognitive load from users navigating complex auditory scenes. The decision to activate spatial filtering happens algorithmically based on environmental analysis—no user intervention required.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-upgradeability-the-medical-device-ai-paradigm-shift"&gt;Upgradeability: The medical device AI paradigm shift&lt;/h3&gt;&lt;p&gt;Here’s the breakthrough that separates this from previous-generation implants: upgradeable firmware in the implanted device itself. Historically, once a cochlear implant was surgically placed, its capabilities were frozen. New signal processing algorithms, improved ML models, better noise reduction—none of it could benefit existing patients.&lt;/p&gt;&lt;figure class="wp-block-image alignleft size-large"&gt;&lt;img alt="alt" class="wp-image-110945" height="683" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/20251113-Cochlear-30-Years-Anniversary_Press-7-1024x683.jpg" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;&lt;strong&gt;Jan Janssen, Chief Technology Officer, Cochlear Limited&lt;/strong&gt;&lt;br /&gt;&lt;/em&gt;&lt;br /&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The Nucleus Nexa Implant changes that equation. Using Cochlear’s proprietary short-range RF link, audiologists can deliver firmware updates through the external processor to the implant. Security relies on physical constraints—the limited transmission range and low power output require proximity during updates—combined with protocol-level safeguards.&lt;/p&gt;&lt;p&gt;“With the smart implants, we actually keep a copy [of the user’s personalised hearing map] on the implant,” Janssen explained. “So you lose this [external processor], we can send you a blank processor and put it on—it retrieves the map from the implant.”&lt;/p&gt;&lt;p&gt;The implant stores up to four unique maps in its internal memory. From an AI deployment perspective, this solves a critical challenge: how do you maintain personalised model parameters when hardware components fail or get replaced?&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-from-decision-trees-to-deep-neural-networks"&gt;From decision trees to deep neural networks&lt;/h3&gt;&lt;p&gt;Cochlear’s current implementation uses decision tree models for environmental classification—a pragmatic choice given power constraints and interpretability requirements for medical devices. But Janssen outlined where the technology is headed: “Artificial intelligence through deep neural networks—a complex form of machine learning—in the future may provide further improvement in hearing in noisy situations.”&lt;/p&gt;&lt;p&gt;The company is also exploring AI applications beyond signal processing. “Cochlear is investigating the use of artificial intelligence and connectivity to automate routine check-ups and reduce lifetime care costs,” Janssen noted.&lt;/p&gt;&lt;p&gt;This points to a broader trajectory for edge AI medical devices: from reactive signal processing to predictive health monitoring, from manual clinical adjustments to autonomous optimisation.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-edge-ai-constraint-problem"&gt;The Edge AI constraint problem&lt;/h3&gt;&lt;p&gt;What makes this deployment fascinating from an ML engineering standpoint is the constraint stack:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Power&lt;/strong&gt;: The device must run for decades on minimal energy, with battery life measured in full days despite continuous audio processing and wireless transmission.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Latency:&amp;nbsp;&lt;/strong&gt;Audio processing happens in real-time with imperceptible delay—users can’t tolerate lag between speech and neural stimulation.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Safety&lt;/strong&gt;: This is a life-critical medical device directly stimulating neural tissue. Model failures aren’t just inconvenient—they impact quality of life.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Upgradeability:&lt;/strong&gt;&amp;nbsp;The implant must support model improvements over 40+ years without hardware replacement.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Privacy:&amp;nbsp;&lt;/strong&gt;Health data processing happens on-device, with Cochlear applying rigorous de-identification before any data enters their Real-World Evidence program for model training across their 500,000+ patient dataset.&lt;/p&gt;&lt;p&gt;These constraints force architectural decisions you don’t face when deploying ML models in the cloud or even on smartphones. Every milliwatt matters. Every algorithm must be validated for medical safety. Every firmware update must be bulletproof.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-beyond-bluetooth-the-connected-implant-future"&gt;Beyond Bluetooth: The connected implant future&lt;/h3&gt;&lt;p&gt;Looking ahead, Cochlear is implementing Bluetooth LE Audio and Auracast broadcast audio capabilities—both requiring future firmware updates to the implant. These protocols offer better audio quality than traditional Bluetooth while reducing power consumption, but more importantly, they position the implant as a node in broader assistive listening networks.&lt;/p&gt;&lt;p&gt;Auracast broadcast audio allows direct connection to audio streams in public venues, airports, and gyms—transforming the implant from an isolated medical device into a connected edge AI medical device participating in ambient computing environments.&lt;/p&gt;&lt;p&gt;The longer-term vision includes totally implantable devices with integrated microphones and batteries, eliminating external components entirely.&amp;nbsp;At that point, you’re talking about fully autonomous AI systems operating inside the human body—adjusting to environments, optimising power,&amp;nbsp;streaming&amp;nbsp;connectivity, all without user interaction.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-medical-device-ai-blueprint"&gt;The medical device AI blueprint&lt;/h3&gt;&lt;p&gt;Cochlear’s deployment offers a blueprint for edge AI medical devices facing similar constraints: start with interpretable models like decision trees, optimise aggressively for power, build in upgradeability from day one, and architect for the 40-year horizon rather than the typical 2-3 year consumer device cycle.&lt;/p&gt;&lt;p&gt;As Janssen noted, the smart implant launching today “is actually the first step to an even smarter implant.” For an industry built on rapid iteration and continuous deployment, adapting to decade-long product lifecycles while maintaining AI advancement represents a fascinating engineering challenge.&lt;/p&gt;&lt;p&gt;The question isn’t whether AI will transform medical devices—Cochlear’s deployment proves it already has. The question is how quickly other manufacturers can solve the constraint problem and bring similarly intelligent systems to market.&lt;/p&gt;&lt;p&gt;For 546 million people with hearing loss in the Western Pacific Region alone, the pace of that innovation will determine whether AI in medicine remains a prototype story or becomes standard of care.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Cochlear)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: FDA AI deployment: Innovation vs oversight in drug regulation&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image"&gt;&lt;img alt="alt" class="wp-image-110938" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-17.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;The next frontier for edge AI medical devices isn’t wearables or bedside monitors—it’s inside the human body itself. Cochlear’s newly launched&amp;nbsp;Nucleus Nexa System&amp;nbsp;represents the first cochlear implant capable of running machine learning algorithms while managing extreme power constraints, storing&amp;nbsp;personalised data on-device, and receiving over-the-air firmware updates to improve its AI models over time.&lt;/p&gt;&lt;p&gt;For AI practitioners, the technical challenge is staggering: build a decision-tree model that classifies five distinct auditory environments in real time, optimise it to run on a device with&amp;nbsp;a&amp;nbsp;minimal power budget that must last decades, and do it all while directly interfacing with human neural tissue.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-110944" height="683" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/20251113-Cochlear-30-Years-Anniversary_Press-2-1024x683.jpg" width="1024" /&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-decision-trees-meet-ultra-low-power-computing"&gt;Decision trees meet ultra-low power computing&lt;/h3&gt;&lt;p&gt;At the core of the system’s intelligence lies SCAN 2, an environmental classifier that analyses incoming audio and categorises it as Speech, Speech in Noise, Noise, Music, or Quiet.&lt;/p&gt;&lt;p&gt;“These classifications are then input to a decision tree, which is a type of machine learning model,” explains Jan Janssen, Cochlear’s Global CTO, in an exclusive interview with&amp;nbsp;&lt;em&gt;AI News&lt;/em&gt;. “This decision is used to adjust sound processing settings for that situation, which adapts the electrical signals sent to the implant.”&lt;/p&gt;&lt;p&gt;The model runs on the external sound processor, but here’s where it gets interesting: the implant itself participates in the intelligence through Dynamic Power Management. Data and power are interleaved between the processor and implant via an enhanced RF link, allowing the chipset to optimise power efficiency based on the ML model’s environmental classifications.&lt;/p&gt;&lt;p&gt;This isn’t just smart power management—it’s edge AI medical devices solving one of the hardest problems in implantable computing: how do you keep a device operational for 40+ years when you can’t replace its battery?&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-spatial-intelligence-layer"&gt;The spatial intelligence layer&lt;/h3&gt;&lt;p&gt;Beyond environmental classification, the system employs ForwardFocus, a spatial noise algorithm that uses inputs from two omnidirectional microphones to create target and noise spatial patterns. The algorithm assumes target signals originate from the front while noise comes from the sides or behind, then applies spatial filtering to attenuate background interference.&lt;/p&gt;&lt;p&gt;What makes this noteworthy from an AI perspective is the automation layer. ForwardFocus can operate autonomously, removing cognitive load from users navigating complex auditory scenes. The decision to activate spatial filtering happens algorithmically based on environmental analysis—no user intervention required.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-upgradeability-the-medical-device-ai-paradigm-shift"&gt;Upgradeability: The medical device AI paradigm shift&lt;/h3&gt;&lt;p&gt;Here’s the breakthrough that separates this from previous-generation implants: upgradeable firmware in the implanted device itself. Historically, once a cochlear implant was surgically placed, its capabilities were frozen. New signal processing algorithms, improved ML models, better noise reduction—none of it could benefit existing patients.&lt;/p&gt;&lt;figure class="wp-block-image alignleft size-large"&gt;&lt;img alt="alt" class="wp-image-110945" height="683" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/20251113-Cochlear-30-Years-Anniversary_Press-7-1024x683.jpg" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;&lt;strong&gt;Jan Janssen, Chief Technology Officer, Cochlear Limited&lt;/strong&gt;&lt;br /&gt;&lt;/em&gt;&lt;br /&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The Nucleus Nexa Implant changes that equation. Using Cochlear’s proprietary short-range RF link, audiologists can deliver firmware updates through the external processor to the implant. Security relies on physical constraints—the limited transmission range and low power output require proximity during updates—combined with protocol-level safeguards.&lt;/p&gt;&lt;p&gt;“With the smart implants, we actually keep a copy [of the user’s personalised hearing map] on the implant,” Janssen explained. “So you lose this [external processor], we can send you a blank processor and put it on—it retrieves the map from the implant.”&lt;/p&gt;&lt;p&gt;The implant stores up to four unique maps in its internal memory. From an AI deployment perspective, this solves a critical challenge: how do you maintain personalised model parameters when hardware components fail or get replaced?&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-from-decision-trees-to-deep-neural-networks"&gt;From decision trees to deep neural networks&lt;/h3&gt;&lt;p&gt;Cochlear’s current implementation uses decision tree models for environmental classification—a pragmatic choice given power constraints and interpretability requirements for medical devices. But Janssen outlined where the technology is headed: “Artificial intelligence through deep neural networks—a complex form of machine learning—in the future may provide further improvement in hearing in noisy situations.”&lt;/p&gt;&lt;p&gt;The company is also exploring AI applications beyond signal processing. “Cochlear is investigating the use of artificial intelligence and connectivity to automate routine check-ups and reduce lifetime care costs,” Janssen noted.&lt;/p&gt;&lt;p&gt;This points to a broader trajectory for edge AI medical devices: from reactive signal processing to predictive health monitoring, from manual clinical adjustments to autonomous optimisation.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-edge-ai-constraint-problem"&gt;The Edge AI constraint problem&lt;/h3&gt;&lt;p&gt;What makes this deployment fascinating from an ML engineering standpoint is the constraint stack:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Power&lt;/strong&gt;: The device must run for decades on minimal energy, with battery life measured in full days despite continuous audio processing and wireless transmission.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Latency:&amp;nbsp;&lt;/strong&gt;Audio processing happens in real-time with imperceptible delay—users can’t tolerate lag between speech and neural stimulation.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Safety&lt;/strong&gt;: This is a life-critical medical device directly stimulating neural tissue. Model failures aren’t just inconvenient—they impact quality of life.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Upgradeability:&lt;/strong&gt;&amp;nbsp;The implant must support model improvements over 40+ years without hardware replacement.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Privacy:&amp;nbsp;&lt;/strong&gt;Health data processing happens on-device, with Cochlear applying rigorous de-identification before any data enters their Real-World Evidence program for model training across their 500,000+ patient dataset.&lt;/p&gt;&lt;p&gt;These constraints force architectural decisions you don’t face when deploying ML models in the cloud or even on smartphones. Every milliwatt matters. Every algorithm must be validated for medical safety. Every firmware update must be bulletproof.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-beyond-bluetooth-the-connected-implant-future"&gt;Beyond Bluetooth: The connected implant future&lt;/h3&gt;&lt;p&gt;Looking ahead, Cochlear is implementing Bluetooth LE Audio and Auracast broadcast audio capabilities—both requiring future firmware updates to the implant. These protocols offer better audio quality than traditional Bluetooth while reducing power consumption, but more importantly, they position the implant as a node in broader assistive listening networks.&lt;/p&gt;&lt;p&gt;Auracast broadcast audio allows direct connection to audio streams in public venues, airports, and gyms—transforming the implant from an isolated medical device into a connected edge AI medical device participating in ambient computing environments.&lt;/p&gt;&lt;p&gt;The longer-term vision includes totally implantable devices with integrated microphones and batteries, eliminating external components entirely.&amp;nbsp;At that point, you’re talking about fully autonomous AI systems operating inside the human body—adjusting to environments, optimising power,&amp;nbsp;streaming&amp;nbsp;connectivity, all without user interaction.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-medical-device-ai-blueprint"&gt;The medical device AI blueprint&lt;/h3&gt;&lt;p&gt;Cochlear’s deployment offers a blueprint for edge AI medical devices facing similar constraints: start with interpretable models like decision trees, optimise aggressively for power, build in upgradeability from day one, and architect for the 40-year horizon rather than the typical 2-3 year consumer device cycle.&lt;/p&gt;&lt;p&gt;As Janssen noted, the smart implant launching today “is actually the first step to an even smarter implant.” For an industry built on rapid iteration and continuous deployment, adapting to decade-long product lifecycles while maintaining AI advancement represents a fascinating engineering challenge.&lt;/p&gt;&lt;p&gt;The question isn’t whether AI will transform medical devices—Cochlear’s deployment proves it already has. The question is how quickly other manufacturers can solve the constraint problem and bring similarly intelligent systems to market.&lt;/p&gt;&lt;p&gt;For 546 million people with hearing loss in the Western Pacific Region alone, the pace of that innovation will determine whether AI in medicine remains a prototype story or becomes standard of care.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Cochlear)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: FDA AI deployment: Innovation vs oversight in drug regulation&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image"&gt;&lt;img alt="alt" class="wp-image-110938" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-17.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/edge-ai-medical-devices-cochlear-implants/</guid><pubDate>Thu, 27 Nov 2025 09:00:00 +0000</pubDate></item><item><title>[NEW] Moving toward LessOps with VMware-to-cloud migrations (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/27/1128465/moving-toward-lessops-with-vmware-to-cloud-migrations/</link><description>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;Cognizant&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Today’s IT leaders face competing mandates to do more (“make us an ‘AI-first’ enterprise—yesterday”) with less (“no new hires for at least the next six months”).&lt;/p&gt;  &lt;p&gt;VMware has become a focal point of these dueling directives. It remains central to enterprise IT, with 80% of organizations using VMware infrastructure products. But shifting licensing models are prompting teams to reconsider how they manage and scale these workloads, often on tighter budgets.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1128467" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/Cognizant-Landing-Page-Card-1.png" /&gt;&lt;/figure&gt;    &lt;p&gt;For many organizations, the path forward involves adopting a LessOps model, an operational strategy that makes hybrid environments manageable without increasing headcount. This operational philosophy minimizes human intervention through extensive automation and selfservice capabilities while maintaining governance and compliance.&lt;/p&gt;  &lt;p&gt;In practice, VMware-to-cloud migrations create a “two birds, one stone” opportunity. They present a practical moment to codify the automation and governance practices LessOps depends on—laying the groundwork for a leaner, more resilient IT operating model.&lt;/p&gt; 
 &lt;p&gt;Download the full article.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.&lt;/em&gt; &lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was researched, designed, and written by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;Cognizant&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Today’s IT leaders face competing mandates to do more (“make us an ‘AI-first’ enterprise—yesterday”) with less (“no new hires for at least the next six months”).&lt;/p&gt;  &lt;p&gt;VMware has become a focal point of these dueling directives. It remains central to enterprise IT, with 80% of organizations using VMware infrastructure products. But shifting licensing models are prompting teams to reconsider how they manage and scale these workloads, often on tighter budgets.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1128467" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/Cognizant-Landing-Page-Card-1.png" /&gt;&lt;/figure&gt;    &lt;p&gt;For many organizations, the path forward involves adopting a LessOps model, an operational strategy that makes hybrid environments manageable without increasing headcount. This operational philosophy minimizes human intervention through extensive automation and selfservice capabilities while maintaining governance and compliance.&lt;/p&gt;  &lt;p&gt;In practice, VMware-to-cloud migrations create a “two birds, one stone” opportunity. They present a practical moment to codify the automation and governance practices LessOps depends on—laying the groundwork for a leaner, more resilient IT operating model.&lt;/p&gt; 
 &lt;p&gt;Download the full article.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.&lt;/em&gt; &lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was researched, designed, and written by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/11/27/1128465/moving-toward-lessops-with-vmware-to-cloud-migrations/</guid><pubDate>Thu, 27 Nov 2025 10:47:24 +0000</pubDate></item><item><title>[NEW] This year’s UN climate talks avoided fossil fuels, again (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/27/1128443/climate-talks-fossil-fuels/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/GettyImages-1303511343.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;If we didn’t have pictures and videos, I almost wouldn’t believe the imagery that came out of this year’s UN climate talks.&lt;/p&gt;  &lt;p&gt;Over the past few weeks in Belem, Brazil, attendees dealt with oppressive heat and flooding, and at one point a literal fire broke out, delaying negotiations. The symbolism was almost too much to bear.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;While many, including the president of Brazil, framed this year’s conference as one of action, the talks ended with a watered-down agreement. The final draft doesn’t even include the phrase “fossil fuels.”&lt;/p&gt;  &lt;p&gt;As emissions and global temperatures reach record highs again this year, I’m left wondering: Why is it so hard to formally acknowledge what’s causing the problem?&lt;/p&gt; 
 &lt;p&gt;This is the 30th time that leaders have gathered for the Conference of the Parties, or COP, an annual UN conference focused on climate change. COP30 also marks 10 years since the gathering that produced the Paris Agreement, in which world powers committed to limiting global warming to “well below” 2.0 °C above preindustrial levels, with a goal of staying below the 1.5 °C mark. (That’s 3.6 °F and 2.7 °F, respectively, for my fellow Americans.)&lt;/p&gt;  &lt;p&gt;Before the conference kicked off this year, host country Brazil’s president, Luiz Inácio Lula da Silva, cast this as the “implementation COP” and called for negotiators to focus on action, and specifically to deliver a road map for a global transition away from fossil fuels.&lt;/p&gt; 
 &lt;p&gt;The science is clear—burning fossil fuels emits greenhouse gases and drives climate change. Reports have shown that meeting the goal of limiting warming to 1.5 °C would require stopping new fossil-fuel exploration and development.&lt;/p&gt;  &lt;p&gt;The problem is, “fossil fuels” might as well be a curse word at global climate negotiations. Two years ago, fights over how to address fossil fuels brought talks at COP28 to a standstill. (It’s worth noting that the conference was hosted in Dubai in the UAE, and the leader was literally the head of the country’s national oil company.)&lt;/p&gt;  &lt;p&gt;The agreement in Dubai ended up including a line that called on countries to transition away from fossil fuels in energy systems. It was short of what many advocates wanted, which was a more explicit call to phase out fossil fuels entirely. But it was still hailed as a win. As I wrote at the time: “The bar is truly on the floor.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;And yet this year, it seems we’ve dug into the basement.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;At one point about 80 countries, a little under half of those present, demanded a concrete plan to move away from fossil fuels.&lt;/p&gt;  &lt;p&gt;But oil producers like Saudi Arabia were insistent that fossil fuels not be singled out. Other countries, including some in Africa and Asia, also made a very fair point: Western nations like the US have burned the most fossil fuels and benefited from it economically. This contingent maintains that legacy polluters have a unique responsibility to finance the transition for less wealthy and developing nations rather than simply barring them from taking the same development route.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The US, by the way, didn’t send a formal delegation to the talks, for the first time in 30 years. But the absence spoke volumes. In a statement to the &lt;em&gt;New York Times&lt;/em&gt; that sidestepped the COP talks, White House spokesperson Taylor Rogers said that president Trump had “set a strong example for the rest of the world” by pursuing new fossil-fuel development.&lt;/p&gt;  &lt;p&gt;To sum up: Some countries are economically dependent on fossil fuels, some don’t want to stop depending on fossil fuels without incentives from other countries, and the current US administration would rather keep using fossil fuels than switch to other energy sources.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;All those factors combined help explain why, in its final form, COP30’s agreement doesn’t name fossil fuels at all. Instead, there’s a vague line that leaders should take into account the decisions made in Dubai, and an acknowledgement that the “global transition towards low greenhouse-gas emissions and climate-resilient development is irreversible and the trend of the future.”&lt;/p&gt;  &lt;p&gt;Hopefully, that’s true. But it’s concerning that even on the world’s biggest stage, naming what we’re supposed to be transitioning away from and putting together any sort of plan to actually do it seems to be impossible.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark,&amp;nbsp;&lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday,&amp;nbsp;&lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/GettyImages-1303511343.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;If we didn’t have pictures and videos, I almost wouldn’t believe the imagery that came out of this year’s UN climate talks.&lt;/p&gt;  &lt;p&gt;Over the past few weeks in Belem, Brazil, attendees dealt with oppressive heat and flooding, and at one point a literal fire broke out, delaying negotiations. The symbolism was almost too much to bear.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;While many, including the president of Brazil, framed this year’s conference as one of action, the talks ended with a watered-down agreement. The final draft doesn’t even include the phrase “fossil fuels.”&lt;/p&gt;  &lt;p&gt;As emissions and global temperatures reach record highs again this year, I’m left wondering: Why is it so hard to formally acknowledge what’s causing the problem?&lt;/p&gt; 
 &lt;p&gt;This is the 30th time that leaders have gathered for the Conference of the Parties, or COP, an annual UN conference focused on climate change. COP30 also marks 10 years since the gathering that produced the Paris Agreement, in which world powers committed to limiting global warming to “well below” 2.0 °C above preindustrial levels, with a goal of staying below the 1.5 °C mark. (That’s 3.6 °F and 2.7 °F, respectively, for my fellow Americans.)&lt;/p&gt;  &lt;p&gt;Before the conference kicked off this year, host country Brazil’s president, Luiz Inácio Lula da Silva, cast this as the “implementation COP” and called for negotiators to focus on action, and specifically to deliver a road map for a global transition away from fossil fuels.&lt;/p&gt; 
 &lt;p&gt;The science is clear—burning fossil fuels emits greenhouse gases and drives climate change. Reports have shown that meeting the goal of limiting warming to 1.5 °C would require stopping new fossil-fuel exploration and development.&lt;/p&gt;  &lt;p&gt;The problem is, “fossil fuels” might as well be a curse word at global climate negotiations. Two years ago, fights over how to address fossil fuels brought talks at COP28 to a standstill. (It’s worth noting that the conference was hosted in Dubai in the UAE, and the leader was literally the head of the country’s national oil company.)&lt;/p&gt;  &lt;p&gt;The agreement in Dubai ended up including a line that called on countries to transition away from fossil fuels in energy systems. It was short of what many advocates wanted, which was a more explicit call to phase out fossil fuels entirely. But it was still hailed as a win. As I wrote at the time: “The bar is truly on the floor.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;And yet this year, it seems we’ve dug into the basement.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;At one point about 80 countries, a little under half of those present, demanded a concrete plan to move away from fossil fuels.&lt;/p&gt;  &lt;p&gt;But oil producers like Saudi Arabia were insistent that fossil fuels not be singled out. Other countries, including some in Africa and Asia, also made a very fair point: Western nations like the US have burned the most fossil fuels and benefited from it economically. This contingent maintains that legacy polluters have a unique responsibility to finance the transition for less wealthy and developing nations rather than simply barring them from taking the same development route.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The US, by the way, didn’t send a formal delegation to the talks, for the first time in 30 years. But the absence spoke volumes. In a statement to the &lt;em&gt;New York Times&lt;/em&gt; that sidestepped the COP talks, White House spokesperson Taylor Rogers said that president Trump had “set a strong example for the rest of the world” by pursuing new fossil-fuel development.&lt;/p&gt;  &lt;p&gt;To sum up: Some countries are economically dependent on fossil fuels, some don’t want to stop depending on fossil fuels without incentives from other countries, and the current US administration would rather keep using fossil fuels than switch to other energy sources.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;All those factors combined help explain why, in its final form, COP30’s agreement doesn’t name fossil fuels at all. Instead, there’s a vague line that leaders should take into account the decisions made in Dubai, and an acknowledgement that the “global transition towards low greenhouse-gas emissions and climate-resilient development is irreversible and the trend of the future.”&lt;/p&gt;  &lt;p&gt;Hopefully, that’s true. But it’s concerning that even on the world’s biggest stage, naming what we’re supposed to be transitioning away from and putting together any sort of plan to actually do it seems to be impossible.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark,&amp;nbsp;&lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday,&amp;nbsp;&lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/11/27/1128443/climate-talks-fossil-fuels/</guid><pubDate>Thu, 27 Nov 2025 11:00:00 +0000</pubDate></item></channel></rss>