<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 28 Aug 2025 12:42:27 +0000</lastBuildDate><item><title>Maisa AI gets $25M to fix enterprise AI’s 95% failure rate (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/27/maisa-ai-gets-25m-to-fix-enterprise-ais-95-failure-rate/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A staggering 95% of generative AI pilots at companies are failing, according to a recent report published by MIT’s NANDA initiative. But rather than giving up on the technology altogether, the most advanced organizations are experimenting with agentic AI systems that can learn and be supervised.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s where Maisa AI comes in. The year-old startup has built its entire approach around the premise that enterprise automation requires accountable AI agents, not opaque black boxes. With a new, $25 million seed round led by European VC firm Creandum, it has now launched Maisa Studio, a model-agnostic self-serve platform that helps users deploy digital workers that can be trained with natural language.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While that might sound familiar — reminiscent of so-called vibe coding platforms like Cursor and the Creandum-backed Lovable — Maisa argues that its approach is fundamentally different. “Instead of using AI to build the responses, we use AI to build the process that needs to be executed to get to the response — what we call ‘chain-of-work,” Maisa CEO David Villalón told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The principal architect behind this process is Maisa’s co-founder and Chief Scientific Officer, Manuel Romero, who had previously worked with Villalón at Spanish AI startup Clibrain. In 2024, the duo teamed up to build a solution to hallucinations after seeing firsthand that “you could not rely on AI,” Villalón said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The pair isn’t skeptical about AI, but they think it won’t be feasible for humans to review “three months of work done in five minutes.” To address this, Maisa employs a system called HALP, standing for Human-Augmented LLM Processing. This custom method works like students at the blackboard — it asks users about their needs while the digital workers outline each step they will follow.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Maisa AI - Worker builder" class="wp-image-3040098" height="460" src="https://techcrunch.com/wp-content/uploads/2025/08/Maisa-AI-Worker-builder.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maisa AI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The startup also developed the Knowledge Processing Unit (KPU), a deterministic system designed to limit hallucinations. While Maisa started out from this technical challenge rather than a use case, it soon found that its bet on trustworthiness and accountability resonated with companies hoping to apply AI to critical tasks. For instance, clients that currently use Maisa in production include a large bank, as well as companies in the car manufacturing and energy sectors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By serving these enterprise clients, Maisa hopes to position itself as a more advanced form of robotic process automation (RPA) that unlocks productivity gains without requiring companies to rely on rigid predefined rules or extensive manual programming. To meet their needs, the startup also offers them either deployment in its secure cloud or through on-premise deployment.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;This enterprise-first approach means Maisa’s customer base is still very small compared to the millions flocking to freemium vibe-coding platforms. But as these platforms are now exploring how to win enterprise customers, Maisa is moving in the opposite direction with Maisa Studio, which is designed to grow its customer funnel and ease adoption.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup also plans to expand with existing customers that have operations in multiple countries. With dual headquarters in Valencia and San Francisco, Maisa itself already has a foothold in the U.S., as reflected in its cap table; its $5 million pre-seed round last December was led by the San Francisco-based venture firms NFX and Village Global.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, TechCrunch learned exclusively that U.S. firm Forgepoint Capital International participated in this new round via its European joint venture with Spanish bank Banco Santander, highlighting its appeal for regulated sectors.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Focusing on complex use cases demanding accountability from non-technical users could be a differentiator for Maisa, whose competitors include CrewAI and many other AI-powered, business-focused workflow automation products. In a LinkedIn post, Villalón highlighted this “AI framework gold rush,” warning that the “quick start” becomes a long nightmare when you need reliability, auditability, or the ability to fix what went wrong.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Doubling down on its goal to help AI scale, Maisa plans to use its funding to grow from 35 to as many as 65 people by the first quarter of 2026 in order to meet demand. Starting in the last quarter of this year, the startup anticipates rapid growth as it begins serving its waiting list. “We are going to show the market that there is a company that is delivering what has been promised, and that it’s working,” Villalón said.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A staggering 95% of generative AI pilots at companies are failing, according to a recent report published by MIT’s NANDA initiative. But rather than giving up on the technology altogether, the most advanced organizations are experimenting with agentic AI systems that can learn and be supervised.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s where Maisa AI comes in. The year-old startup has built its entire approach around the premise that enterprise automation requires accountable AI agents, not opaque black boxes. With a new, $25 million seed round led by European VC firm Creandum, it has now launched Maisa Studio, a model-agnostic self-serve platform that helps users deploy digital workers that can be trained with natural language.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While that might sound familiar — reminiscent of so-called vibe coding platforms like Cursor and the Creandum-backed Lovable — Maisa argues that its approach is fundamentally different. “Instead of using AI to build the responses, we use AI to build the process that needs to be executed to get to the response — what we call ‘chain-of-work,” Maisa CEO David Villalón told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The principal architect behind this process is Maisa’s co-founder and Chief Scientific Officer, Manuel Romero, who had previously worked with Villalón at Spanish AI startup Clibrain. In 2024, the duo teamed up to build a solution to hallucinations after seeing firsthand that “you could not rely on AI,” Villalón said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The pair isn’t skeptical about AI, but they think it won’t be feasible for humans to review “three months of work done in five minutes.” To address this, Maisa employs a system called HALP, standing for Human-Augmented LLM Processing. This custom method works like students at the blackboard — it asks users about their needs while the digital workers outline each step they will follow.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Maisa AI - Worker builder" class="wp-image-3040098" height="460" src="https://techcrunch.com/wp-content/uploads/2025/08/Maisa-AI-Worker-builder.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maisa AI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The startup also developed the Knowledge Processing Unit (KPU), a deterministic system designed to limit hallucinations. While Maisa started out from this technical challenge rather than a use case, it soon found that its bet on trustworthiness and accountability resonated with companies hoping to apply AI to critical tasks. For instance, clients that currently use Maisa in production include a large bank, as well as companies in the car manufacturing and energy sectors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By serving these enterprise clients, Maisa hopes to position itself as a more advanced form of robotic process automation (RPA) that unlocks productivity gains without requiring companies to rely on rigid predefined rules or extensive manual programming. To meet their needs, the startup also offers them either deployment in its secure cloud or through on-premise deployment.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;This enterprise-first approach means Maisa’s customer base is still very small compared to the millions flocking to freemium vibe-coding platforms. But as these platforms are now exploring how to win enterprise customers, Maisa is moving in the opposite direction with Maisa Studio, which is designed to grow its customer funnel and ease adoption.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup also plans to expand with existing customers that have operations in multiple countries. With dual headquarters in Valencia and San Francisco, Maisa itself already has a foothold in the U.S., as reflected in its cap table; its $5 million pre-seed round last December was led by the San Francisco-based venture firms NFX and Village Global.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, TechCrunch learned exclusively that U.S. firm Forgepoint Capital International participated in this new round via its European joint venture with Spanish bank Banco Santander, highlighting its appeal for regulated sectors.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Focusing on complex use cases demanding accountability from non-technical users could be a differentiator for Maisa, whose competitors include CrewAI and many other AI-powered, business-focused workflow automation products. In a LinkedIn post, Villalón highlighted this “AI framework gold rush,” warning that the “quick start” becomes a long nightmare when you need reliability, auditability, or the ability to fix what went wrong.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Doubling down on its goal to help AI scale, Maisa plans to use its funding to grow from 35 to as many as 65 people by the first quarter of 2026 in order to meet demand. Starting in the last quarter of this year, the startup anticipates rapid growth as it begins serving its waiting list. “We are going to show the market that there is a company that is delivering what has been promised, and that it’s working,” Villalón said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/27/maisa-ai-gets-25m-to-fix-enterprise-ais-95-failure-rate/</guid><pubDate>Thu, 28 Aug 2025 05:00:00 +0000</pubDate></item><item><title>[NEW] What Rollup News says about battling disinformation (AI News)</title><link>https://www.artificialintelligence-news.com/news/what-rollup-news-says-about-battling-disinformation/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/glenn-carstens-peters-npxXWgQ33ZQ-unsplash-4-scaled.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Swarm Network, a platform developing decentralised protocols for AI agents, recently announced the successful results of its first Swarm, a tool (perhaps “organism” is the better term) built to tackle disinformation. Called Rollup News, the swarm is not an app, a software platform, nor a centralised algorithm. It is a decentralised collection of AI agents that collaborate to solve a bigger problem. The problem is that platforms like X allow any type of viral claims, some by incredibly influential people. How can we know what is true?&lt;/p&gt;&lt;p&gt;Currently, we try to solve this problem through equally loud opposing voices who offer facts or expert opinions. But if those sources are from a political side you oppose, why should you trust them? After all, these are people with their own motivations, and two additional issues are created: facts presented by a single person can easily get caught up in the “fake news” accusations; and misinformation presented as “facts” can be used to attack the ground truth.&lt;/p&gt;&lt;p&gt;Unfortunately, this isn’t just a current trend that will eventually lose its popularity and fade out. The more technology and access to varied news sources we have, the harder it becomes to not treat these sources equally. Some might be a traditional outlet that is legally liable if they falsify claims. Others might be a popular podcaster with an audience of millions, and whose fear-mongering ties nicely in with the products in their merch store. If it stopped at this, we could probably tell the truth from fiction. But it isn’t that simple. Official news channels have a history of spinning the news in their own bias, or ignoring other stories that are important to the public. On the other side, there are genuinely powerful influencers who seem to be hell bent on finding the truth and reporting it, no matter what side of the political spectrum it hits.&lt;/p&gt;&lt;p&gt;The world has become both confusing and dangerous, and the old “sticks and stones” saying has been proven false. After all, we have seen global elections swayed by disinformation, major policy shifts driven by false claims, lives damaged and lost as the result of powerful people lying, but lying loudly enough and often enough to sway large groups of people into believing them; and convincing these same groups that any facts to the contrary are the actual “fake news.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-fixing-fact-checking"&gt;Fixing fact checking&lt;/h3&gt;&lt;p&gt;Given how challenging the disinformation industry is, and how absolutely slippery the truth is today, how can anyone hope to battle it? We have seen that people of all sides, realising that all news is skewed to some extent, will believe those sources that support their pre-existing beliefs.&lt;/p&gt;&lt;p&gt;A third party source, backed by overwhelming evidence, is needed to arbitrate. The source should not have an opinion, its methods should be transparent, and everyone should be able to see the same thing. This is nearly impossible, but the Web3 industry has shown that these attributes are what makes it incredibly powerful. Smart contracts handle billions in value daily, managing agreements from complete strangers from anywhere on the globe. The information is validated and the decisions are transparent, then locked in via the blockchain. The model has moved trillions of dollars using these very powerful, and neutral, tools.&lt;/p&gt;&lt;p&gt;Combine this trust with the other element Web3 excels in: decentralisation. Now attach another fast-emerging technology, the AI agent, which is easily built and designed to perform one task very well. The system is the centre of Swarm Network’s model, and its first deployment is Rollup News. The growing population of AI agents, the swarm, is designed to work collectively to scour the corners of X, find claims from users, and collectively test their validity using sources found in the information space. The results of these assessments are posted on the blockchain once validated by a large enough group of independent agents. Selective human participation helps to ensure the context and other subtle areas are handled well. The human element is also decentralised, preventing any particular viewpoint from being able to assert itself, and misconduct equals expulsion if someone tries to present fiction as fact. Rollup News has been operating for several months, with astonishing results: 128,000+ users have been onboarded, with over 5,000 rollup requests daily in July 2025. Over 3 million tweets were processed during that time, which is impressive in its own right, but when you consider the designed scalability of Web3 and AI agents working together, this is the linchpin of the battle in a world of disinformation.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-start-of-something-new"&gt;The start of something new?&lt;/h3&gt;&lt;p&gt;Rollup News’ success and Swarm Network’s larger model teach us a few things about fixing today’s problems. It is a demonstration that Web3 and AI are components in providing scalable solutions, that small AI agents can effectively work together to solve giant challenges, even if there is no centralised system. That decentralised environment, anchored by Web3, is the key to generating transparency, trust, and allowing strangers anywhere in the world to work together. Finally, the tokenisation of such a system creates the necessary incentives to attract more participants, fuelling the growth of a system. As long as it creates value, people will pay for its use, and those who help to validate and secure the decentralised network earn rewards. The type of truly free market system can scale up or down with the global demand faster than any traditional company. Swarm Network’s founder, Yannick Myson, sums it up nicely: “Rollup News shows what’s possible when AI agents, human insight, and blockchain converge. This isn’t a prototype – it’s working, and it’s scaling.”&lt;/p&gt;&lt;p&gt;We need to pay close attention to these lessons, as they offer a great deal of insight. First, the “truth-tech” sector, which is focused on using technology to combat mis/dis-information, has a strong blueprint for combining blockchain and AI. Second, there are many other sectors that need this level of global scaling and independent management, with untold value just ready to be developed and launched.&lt;/p&gt;&lt;p&gt;Image source: Unsplash&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/glenn-carstens-peters-npxXWgQ33ZQ-unsplash-4-scaled.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Swarm Network, a platform developing decentralised protocols for AI agents, recently announced the successful results of its first Swarm, a tool (perhaps “organism” is the better term) built to tackle disinformation. Called Rollup News, the swarm is not an app, a software platform, nor a centralised algorithm. It is a decentralised collection of AI agents that collaborate to solve a bigger problem. The problem is that platforms like X allow any type of viral claims, some by incredibly influential people. How can we know what is true?&lt;/p&gt;&lt;p&gt;Currently, we try to solve this problem through equally loud opposing voices who offer facts or expert opinions. But if those sources are from a political side you oppose, why should you trust them? After all, these are people with their own motivations, and two additional issues are created: facts presented by a single person can easily get caught up in the “fake news” accusations; and misinformation presented as “facts” can be used to attack the ground truth.&lt;/p&gt;&lt;p&gt;Unfortunately, this isn’t just a current trend that will eventually lose its popularity and fade out. The more technology and access to varied news sources we have, the harder it becomes to not treat these sources equally. Some might be a traditional outlet that is legally liable if they falsify claims. Others might be a popular podcaster with an audience of millions, and whose fear-mongering ties nicely in with the products in their merch store. If it stopped at this, we could probably tell the truth from fiction. But it isn’t that simple. Official news channels have a history of spinning the news in their own bias, or ignoring other stories that are important to the public. On the other side, there are genuinely powerful influencers who seem to be hell bent on finding the truth and reporting it, no matter what side of the political spectrum it hits.&lt;/p&gt;&lt;p&gt;The world has become both confusing and dangerous, and the old “sticks and stones” saying has been proven false. After all, we have seen global elections swayed by disinformation, major policy shifts driven by false claims, lives damaged and lost as the result of powerful people lying, but lying loudly enough and often enough to sway large groups of people into believing them; and convincing these same groups that any facts to the contrary are the actual “fake news.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-fixing-fact-checking"&gt;Fixing fact checking&lt;/h3&gt;&lt;p&gt;Given how challenging the disinformation industry is, and how absolutely slippery the truth is today, how can anyone hope to battle it? We have seen that people of all sides, realising that all news is skewed to some extent, will believe those sources that support their pre-existing beliefs.&lt;/p&gt;&lt;p&gt;A third party source, backed by overwhelming evidence, is needed to arbitrate. The source should not have an opinion, its methods should be transparent, and everyone should be able to see the same thing. This is nearly impossible, but the Web3 industry has shown that these attributes are what makes it incredibly powerful. Smart contracts handle billions in value daily, managing agreements from complete strangers from anywhere on the globe. The information is validated and the decisions are transparent, then locked in via the blockchain. The model has moved trillions of dollars using these very powerful, and neutral, tools.&lt;/p&gt;&lt;p&gt;Combine this trust with the other element Web3 excels in: decentralisation. Now attach another fast-emerging technology, the AI agent, which is easily built and designed to perform one task very well. The system is the centre of Swarm Network’s model, and its first deployment is Rollup News. The growing population of AI agents, the swarm, is designed to work collectively to scour the corners of X, find claims from users, and collectively test their validity using sources found in the information space. The results of these assessments are posted on the blockchain once validated by a large enough group of independent agents. Selective human participation helps to ensure the context and other subtle areas are handled well. The human element is also decentralised, preventing any particular viewpoint from being able to assert itself, and misconduct equals expulsion if someone tries to present fiction as fact. Rollup News has been operating for several months, with astonishing results: 128,000+ users have been onboarded, with over 5,000 rollup requests daily in July 2025. Over 3 million tweets were processed during that time, which is impressive in its own right, but when you consider the designed scalability of Web3 and AI agents working together, this is the linchpin of the battle in a world of disinformation.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-start-of-something-new"&gt;The start of something new?&lt;/h3&gt;&lt;p&gt;Rollup News’ success and Swarm Network’s larger model teach us a few things about fixing today’s problems. It is a demonstration that Web3 and AI are components in providing scalable solutions, that small AI agents can effectively work together to solve giant challenges, even if there is no centralised system. That decentralised environment, anchored by Web3, is the key to generating transparency, trust, and allowing strangers anywhere in the world to work together. Finally, the tokenisation of such a system creates the necessary incentives to attract more participants, fuelling the growth of a system. As long as it creates value, people will pay for its use, and those who help to validate and secure the decentralised network earn rewards. The type of truly free market system can scale up or down with the global demand faster than any traditional company. Swarm Network’s founder, Yannick Myson, sums it up nicely: “Rollup News shows what’s possible when AI agents, human insight, and blockchain converge. This isn’t a prototype – it’s working, and it’s scaling.”&lt;/p&gt;&lt;p&gt;We need to pay close attention to these lessons, as they offer a great deal of insight. First, the “truth-tech” sector, which is focused on using technology to combat mis/dis-information, has a strong blueprint for combining blockchain and AI. Second, there are many other sectors that need this level of global scaling and independent management, with untold value just ready to be developed and launched.&lt;/p&gt;&lt;p&gt;Image source: Unsplash&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/what-rollup-news-says-about-battling-disinformation/</guid><pubDate>Thu, 28 Aug 2025 07:41:34 +0000</pubDate></item><item><title>[NEW] Tencent Hunyuan Video-Foley brings lifelike audio to AI video (AI News)</title><link>https://www.artificialintelligence-news.com/news/tencent-hunyuan-video-foley-lifelike-audio-to-ai-video/</link><description>&lt;p&gt;A team at Tencent’s Hunyuan lab has created a new AI, ‘Hunyuan Video-Foley,’ that finally brings lifelike audio to generated video. It’s designed to listen to videos and generate a high-quality soundtrack that’s perfectly in sync with the action on screen.&lt;/p&gt;&lt;p&gt;Ever watched an AI-generated video and felt like something was missing? The visuals might be stunning, but they often have an eerie silence that breaks the spell. In the film industry, the sound that fills that silence – the rustle of leaves, the clap of thunder, the clink of a glass – is called Foley art, and it’s a painstaking craft performed by experts.&lt;/p&gt;&lt;p&gt;Matching that level of detail is a huge challenge for AI. For years, automated systems have struggled to create believable sounds for videos.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-how-is-tencent-solving-the-ai-generated-audio-for-video-problem"&gt;How is Tencent solving the AI-generated audio for video problem?&lt;/h3&gt;&lt;p&gt;One of the biggest reasons video-to-audio (V2A) models often fell short in the sound department was what the researchers call “modality imbalance”. Essentially, the AI was listening more to the text prompts it was given than it was watching the actual video.&lt;/p&gt;&lt;p&gt;For instance, if you gave a model a video of a busy beach with people walking and seagulls flying, but the text prompt only said “the sound of ocean waves,” you’d likely just get the sound of waves. The AI would completely ignore the footsteps in the sand and the calls of the birds, making the scene feel lifeless.&lt;/p&gt;&lt;p&gt;On top of that, the quality of the audio was often subpar, and there simply wasn’t enough high-quality video with sound to train the models effectively.&lt;/p&gt;&lt;p&gt;Tencent’s Hunyuan team tackled these problems from three different angles:&lt;/p&gt;&lt;ol class="wp-block-list"&gt;&lt;li&gt;Tencent realised the AI needed a better education, so they built a massive, 100,000-hour library of video, audio, and text descriptions for it to learn from. They created an automated pipeline that filtered out low-quality content from the internet, getting rid of clips with long silences or compressed, fuzzy audio, ensuring the AI learned from the best possible material.&lt;/li&gt;&lt;/ol&gt;&lt;ol class="wp-block-list" start="2"&gt;&lt;li&gt;They designed a smarter architecture for the AI. Think of it like teaching the model to properly multitask. The system first pays incredibly close attention to the visual-audio link to get the timing just right—like matching the thump of a footstep to the exact moment a shoe hits the pavement. Once it has that timing locked down, it then incorporates the text prompt to understand the overall mood and context of the scene. This dual approach ensures the specific details of the video are never overlooked.&lt;/li&gt;&lt;/ol&gt;&lt;ol class="wp-block-list" start="3"&gt;&lt;li&gt;To guarantee the sound was high-quality, they used a training strategy called Representation Alignment (REPA). This is like having an expert audio engineer constantly looking over the AI’s shoulder during its training. It compares the AI’s work to features from a pre-trained, professional-grade audio model to guide it towards producing cleaner, richer, and more stable sound.&lt;/li&gt;&lt;/ol&gt;&lt;figure class="wp-block-embed aligncenter is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;&lt;blockquote class="cmplz-placeholder-element twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Today we're announcing the open-source release of HunyuanVideo-Foley, our new end-to-end Text-Video-to-Audio (TV2A) framework for generating high-fidelity audio.🚀&lt;/p&gt;&lt;p&gt;This tool empowers creators in video production, filmmaking, and game development to generate professional-grade… pic.twitter.com/mff2m5xFvC&lt;/p&gt;— Hunyuan (@TencentHunyuan) August 28, 2025&lt;/blockquote&gt;&lt;/div&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-the-results-speak-sound-for-themselves"&gt;The results &lt;s&gt;speak&lt;/s&gt; sound for themselves&lt;/h3&gt;&lt;p&gt;When Tencent tested Hunyuan Video-Foley against other leading AI models, the audio results were clear. It wasn’t just that the computer-based metrics were better; human listeners consistently rated its output as higher quality, better matched to the video, and more accurately timed.&lt;/p&gt;&lt;p&gt;Across the board, the AI delivered improvements in making the sound match the on-screen action, both in terms of content and timing. The results across multiple evaluation datasets support this:&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Evaluation results of Tencent Hunyuan Video-Foley against other leading AI models." class="wp-image-109161" height="610" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/tencent-hunyuanvideo-foley-ai-sound-audio-model-evaluation-results-1024x610.jpg" width="1024" /&gt;&lt;/figure&gt;&lt;p&gt;Tencent’s work helps to close the gap between silent AI videos and an immersive viewing experience with quality audio. It’s bringing the magic of Foley art to the world of automated content creation, which could be a powerful capability for filmmakers, animators, and creators everywhere.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Google Vids gets AI avatars and image-to-video tools&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for the AI &amp;amp; Big Data Expo event series." class="wp-image-109137" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;A team at Tencent’s Hunyuan lab has created a new AI, ‘Hunyuan Video-Foley,’ that finally brings lifelike audio to generated video. It’s designed to listen to videos and generate a high-quality soundtrack that’s perfectly in sync with the action on screen.&lt;/p&gt;&lt;p&gt;Ever watched an AI-generated video and felt like something was missing? The visuals might be stunning, but they often have an eerie silence that breaks the spell. In the film industry, the sound that fills that silence – the rustle of leaves, the clap of thunder, the clink of a glass – is called Foley art, and it’s a painstaking craft performed by experts.&lt;/p&gt;&lt;p&gt;Matching that level of detail is a huge challenge for AI. For years, automated systems have struggled to create believable sounds for videos.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-how-is-tencent-solving-the-ai-generated-audio-for-video-problem"&gt;How is Tencent solving the AI-generated audio for video problem?&lt;/h3&gt;&lt;p&gt;One of the biggest reasons video-to-audio (V2A) models often fell short in the sound department was what the researchers call “modality imbalance”. Essentially, the AI was listening more to the text prompts it was given than it was watching the actual video.&lt;/p&gt;&lt;p&gt;For instance, if you gave a model a video of a busy beach with people walking and seagulls flying, but the text prompt only said “the sound of ocean waves,” you’d likely just get the sound of waves. The AI would completely ignore the footsteps in the sand and the calls of the birds, making the scene feel lifeless.&lt;/p&gt;&lt;p&gt;On top of that, the quality of the audio was often subpar, and there simply wasn’t enough high-quality video with sound to train the models effectively.&lt;/p&gt;&lt;p&gt;Tencent’s Hunyuan team tackled these problems from three different angles:&lt;/p&gt;&lt;ol class="wp-block-list"&gt;&lt;li&gt;Tencent realised the AI needed a better education, so they built a massive, 100,000-hour library of video, audio, and text descriptions for it to learn from. They created an automated pipeline that filtered out low-quality content from the internet, getting rid of clips with long silences or compressed, fuzzy audio, ensuring the AI learned from the best possible material.&lt;/li&gt;&lt;/ol&gt;&lt;ol class="wp-block-list" start="2"&gt;&lt;li&gt;They designed a smarter architecture for the AI. Think of it like teaching the model to properly multitask. The system first pays incredibly close attention to the visual-audio link to get the timing just right—like matching the thump of a footstep to the exact moment a shoe hits the pavement. Once it has that timing locked down, it then incorporates the text prompt to understand the overall mood and context of the scene. This dual approach ensures the specific details of the video are never overlooked.&lt;/li&gt;&lt;/ol&gt;&lt;ol class="wp-block-list" start="3"&gt;&lt;li&gt;To guarantee the sound was high-quality, they used a training strategy called Representation Alignment (REPA). This is like having an expert audio engineer constantly looking over the AI’s shoulder during its training. It compares the AI’s work to features from a pre-trained, professional-grade audio model to guide it towards producing cleaner, richer, and more stable sound.&lt;/li&gt;&lt;/ol&gt;&lt;figure class="wp-block-embed aligncenter is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;&lt;blockquote class="cmplz-placeholder-element twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Today we're announcing the open-source release of HunyuanVideo-Foley, our new end-to-end Text-Video-to-Audio (TV2A) framework for generating high-fidelity audio.🚀&lt;/p&gt;&lt;p&gt;This tool empowers creators in video production, filmmaking, and game development to generate professional-grade… pic.twitter.com/mff2m5xFvC&lt;/p&gt;— Hunyuan (@TencentHunyuan) August 28, 2025&lt;/blockquote&gt;&lt;/div&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-the-results-speak-sound-for-themselves"&gt;The results &lt;s&gt;speak&lt;/s&gt; sound for themselves&lt;/h3&gt;&lt;p&gt;When Tencent tested Hunyuan Video-Foley against other leading AI models, the audio results were clear. It wasn’t just that the computer-based metrics were better; human listeners consistently rated its output as higher quality, better matched to the video, and more accurately timed.&lt;/p&gt;&lt;p&gt;Across the board, the AI delivered improvements in making the sound match the on-screen action, both in terms of content and timing. The results across multiple evaluation datasets support this:&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Evaluation results of Tencent Hunyuan Video-Foley against other leading AI models." class="wp-image-109161" height="610" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/tencent-hunyuanvideo-foley-ai-sound-audio-model-evaluation-results-1024x610.jpg" width="1024" /&gt;&lt;/figure&gt;&lt;p&gt;Tencent’s work helps to close the gap between silent AI videos and an immersive viewing experience with quality audio. It’s bringing the magic of Foley art to the world of automated content creation, which could be a powerful capability for filmmakers, animators, and creators everywhere.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Google Vids gets AI avatars and image-to-video tools&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for the AI &amp;amp; Big Data Expo event series." class="wp-image-109137" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/tencent-hunyuan-video-foley-lifelike-audio-to-ai-video/</guid><pubDate>Thu, 28 Aug 2025 08:43:21 +0000</pubDate></item><item><title>[NEW] 3 problems with Google’s AI energy use data (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/08/28/1122685/ai-energy-use-gemini/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/usa-mitholian-davidson-0039.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Google just announced that a typical query to its Gemini app uses about 0.24 watt-hours of electricity. That’s about the same as running a microwave for one second—something that, to me, feels virtually insignificant. I run the microwave for so many more seconds than that on most days.&lt;/p&gt;  &lt;p&gt;I was excited to see this report come out, and I welcome more openness from major players in AI about their estimated energy use per query. But I’ve noticed that some folks are taking this number and using it to conclude that we don’t need to worry about AI’s energy demand. That’s not the right takeaway here. Let’s dig into why.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;strong&gt;1. This one number doesn’t reflect all queries, and it leaves out cases that likely use much more energy.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Google’s new report considers only text queries. Previous analysis, including &lt;em&gt;MIT Technology Review&lt;/em&gt;’s reporting, suggests that generating a photo or video will typically use more electricity.&lt;/p&gt; 
 &lt;p&gt;When I spoke with Jeff Dean, Google’s chief scientist, he said the company doesn’t currently have plans to do this sort of analysis for images and videos, but that he wouldn’t rule it out.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;The reason the company started with text prompts is that those are something many people out there are using in their daily lives, he says, while image and video generation is something that not as many people are doing. But I’m seeing more AI images and videos all over my social feeds. So there’s a whole world of queries not represented here.&lt;/p&gt; 
 &lt;p&gt;Also, this estimate is the median, meaning it’s just the number in the middle of the range of queries Google is seeing. Longer questions and responses can push up the energy demand, and so can using a reasoning model.&amp;nbsp; We don’t know anything about how much energy these more complicated queries demand or what the distribution of the range is.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2. We don’t know how many queries Gemini is seeing, so we don’t know the product’s total energy impact.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;One of my biggest outstanding questions about Gemini’s energy use is the total number of queries the product is seeing every day.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This number isn’t included in Google’s report, and the company wouldn’t share it with me. And let me be clear: I absolutely pestered them about this, both in a press call they had about the news and in my interview with Dean. In the press call, the company pointed me to a recent earnings report, which includes only figures about monthly active users (450 million, for what it’s worth).&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;“We’re not comfortable revealing that for various reasons,” Dean told me on our call. The total number is an abstract measure that changes over time, he says, adding that the company wants users to be thinking about the energy usage per prompt.&lt;/p&gt;  &lt;p&gt;But there are people out there all over the world interacting with this technology, not just me—and what we all add up to seems quite relevant.&lt;/p&gt;  &lt;p&gt;OpenAI does publicly share its total, sharing recently that it sees 2.5 billion queries to ChatGPT every day. So for the curious, we can use this as an example and take the company’s self-reported average energy use per query (0.34 watt-hours) to get a rough idea of the total for all people prompting ChatGPT.&lt;/p&gt;  &lt;p&gt;According to my math, over the course of a year, that would add up to over 300 gigawatt-hours—the same as powering nearly 30,000 US homes annually. When you put it that way, it starts to sound like a lot of seconds in microwaves.&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;3. AI is everywhere, not just in chatbots, and we’re often not even conscious of it.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;AI is touching our lives even when we’re not looking for it. AI summaries appear in web searches, whether you ask for them or not. There are built-in features for email and texting applications that that can draft or summarize messages for you.&lt;/p&gt;  &lt;p&gt;Google’s estimate is strictly for Gemini apps and wouldn’t include many of the other ways that even this one company is using AI. So even if you’re trying to think about your own personal energy demand, it’s increasingly difficult to tally up.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;To be clear, I don’t think people should feel guilty for using tools that they find genuinely helpful. And ultimately, I don’t think the most important conversation is about personal responsibility.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;There’s a tendency right now to focus on the small numbers, but we need to keep in mind what this is all adding up to. Over two gigawatts of natural gas will need to come online in Louisiana to power a single Meta data center this decade. Google Cloud is spending $25 billion on AI just in the PJM grid on the US East Coast. By 2028, AI could account for 326 terawatt-hours of electricity demand in the US annually, generating over 100 million metric tons of carbon dioxide.&lt;/p&gt;  &lt;p&gt;We need more reporting from major players in AI, and Google’s recent announcement is one of the most transparent accounts yet. But one small number doesn’t negate the ways this technology is affecting communities and changing our power grid.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark, &lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/usa-mitholian-davidson-0039.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Google just announced that a typical query to its Gemini app uses about 0.24 watt-hours of electricity. That’s about the same as running a microwave for one second—something that, to me, feels virtually insignificant. I run the microwave for so many more seconds than that on most days.&lt;/p&gt;  &lt;p&gt;I was excited to see this report come out, and I welcome more openness from major players in AI about their estimated energy use per query. But I’ve noticed that some folks are taking this number and using it to conclude that we don’t need to worry about AI’s energy demand. That’s not the right takeaway here. Let’s dig into why.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;strong&gt;1. This one number doesn’t reflect all queries, and it leaves out cases that likely use much more energy.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Google’s new report considers only text queries. Previous analysis, including &lt;em&gt;MIT Technology Review&lt;/em&gt;’s reporting, suggests that generating a photo or video will typically use more electricity.&lt;/p&gt; 
 &lt;p&gt;When I spoke with Jeff Dean, Google’s chief scientist, he said the company doesn’t currently have plans to do this sort of analysis for images and videos, but that he wouldn’t rule it out.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;The reason the company started with text prompts is that those are something many people out there are using in their daily lives, he says, while image and video generation is something that not as many people are doing. But I’m seeing more AI images and videos all over my social feeds. So there’s a whole world of queries not represented here.&lt;/p&gt; 
 &lt;p&gt;Also, this estimate is the median, meaning it’s just the number in the middle of the range of queries Google is seeing. Longer questions and responses can push up the energy demand, and so can using a reasoning model.&amp;nbsp; We don’t know anything about how much energy these more complicated queries demand or what the distribution of the range is.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2. We don’t know how many queries Gemini is seeing, so we don’t know the product’s total energy impact.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;One of my biggest outstanding questions about Gemini’s energy use is the total number of queries the product is seeing every day.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This number isn’t included in Google’s report, and the company wouldn’t share it with me. And let me be clear: I absolutely pestered them about this, both in a press call they had about the news and in my interview with Dean. In the press call, the company pointed me to a recent earnings report, which includes only figures about monthly active users (450 million, for what it’s worth).&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;“We’re not comfortable revealing that for various reasons,” Dean told me on our call. The total number is an abstract measure that changes over time, he says, adding that the company wants users to be thinking about the energy usage per prompt.&lt;/p&gt;  &lt;p&gt;But there are people out there all over the world interacting with this technology, not just me—and what we all add up to seems quite relevant.&lt;/p&gt;  &lt;p&gt;OpenAI does publicly share its total, sharing recently that it sees 2.5 billion queries to ChatGPT every day. So for the curious, we can use this as an example and take the company’s self-reported average energy use per query (0.34 watt-hours) to get a rough idea of the total for all people prompting ChatGPT.&lt;/p&gt;  &lt;p&gt;According to my math, over the course of a year, that would add up to over 300 gigawatt-hours—the same as powering nearly 30,000 US homes annually. When you put it that way, it starts to sound like a lot of seconds in microwaves.&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;3. AI is everywhere, not just in chatbots, and we’re often not even conscious of it.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;AI is touching our lives even when we’re not looking for it. AI summaries appear in web searches, whether you ask for them or not. There are built-in features for email and texting applications that that can draft or summarize messages for you.&lt;/p&gt;  &lt;p&gt;Google’s estimate is strictly for Gemini apps and wouldn’t include many of the other ways that even this one company is using AI. So even if you’re trying to think about your own personal energy demand, it’s increasingly difficult to tally up.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;To be clear, I don’t think people should feel guilty for using tools that they find genuinely helpful. And ultimately, I don’t think the most important conversation is about personal responsibility.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;There’s a tendency right now to focus on the small numbers, but we need to keep in mind what this is all adding up to. Over two gigawatts of natural gas will need to come online in Louisiana to power a single Meta data center this decade. Google Cloud is spending $25 billion on AI just in the PJM grid on the US East Coast. By 2028, AI could account for 326 terawatt-hours of electricity demand in the US annually, generating over 100 million metric tons of carbon dioxide.&lt;/p&gt;  &lt;p&gt;We need more reporting from major players in AI, and Google’s recent announcement is one of the most transparent accounts yet. But one small number doesn’t negate the ways this technology is affecting communities and changing our power grid.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark, &lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/08/28/1122685/ai-energy-use-gemini/</guid><pubDate>Thu, 28 Aug 2025 10:00:00 +0000</pubDate></item><item><title>[NEW] From pilot to scale: Making agentic AI work in health care (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/08/28/1122623/from-pilot-to-scale-making-agentic-ai-work-in-health-care/</link><description>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;Provided by&lt;/span&gt;Ensemble&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Over the past 20 years building advanced AI systems—from academic labs to enterprise deployments—I’ve witnessed AI’s waves of success rise and fall. My journey began during the “AI Winter,” when billions were invested in expert systems that ultimately underdelivered. Flash forward to today: large language models (LLMs) represent a quantum leap forward, but their prompt-based adoption is similarly overhyped, as it’s essentially a rule-based approach disguised in natural language.&lt;/p&gt;  &lt;p&gt;At Ensemble, the leading revenue cycle management (RCM) company for hospitals, we focus on overcoming model limitations by investing in what we believe is the next step in AI evolution: grounding LLMs in facts and logic through neuro-symbolic AI. Our in-house AI incubator pairs elite AI researchers with health-care experts to develop agentic systems powered by a neuro-symbolic AI framework. This bridges LLMs’ intuitive power with the precision of symbolic representation and reasoning.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1122645" height="1688" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/iStock-1333328712_cb5c09.jpg?w=3000" width="3000" /&gt;&lt;/figure&gt;  &lt;h3 class="wp-block-heading"&gt;Overcoming LLM limitations&lt;/h3&gt;  &lt;p&gt;LLMs excel at understanding nuanced context, performing instinctive reasoning, and generating human-like interactions, making them ideal for agentic tools to then interpret intricate data and communicate effectively. Yet in a domain like health care where compliance, accuracy, and adherence to regulatory standards are non-negotiable—and where a wealth of structured resources like taxonomies, rules, and clinical guidelines define the landscape—symbolic AI is indispensable.&lt;/p&gt;  &lt;p&gt;By fusing LLMs and reinforcement learning with structured knowledge bases and clinical logic, our hybrid architecture delivers more than just intelligent automation—it minimizes hallucinations, expands reasoning capabilities, and ensures every decision is grounded in established guidelines and enforceable guardrails.&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;Creating a successful agentic AI strategy&lt;/h3&gt;  &lt;p&gt;Ensemble’s agentic AI approach includes three core pillars:&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1. High-fidelity data sets:&lt;/strong&gt; By managing revenue operations for hundreds of hospitals nationwide, Ensemble has unparallelled access to one of the most robust administrative datasets in health care. The team has decades of data aggregation, cleansing, and harmonization efforts, providing an exceptional environment to develop advanced applications.&lt;/p&gt; 
 &lt;p&gt;To power our agentic systems, we’ve harmonized more than 2 petabytes of longitudinal claims data, 80,000 denial audit letters, and 80 million annual transactions mapped to industry-leading outcomes. This data fuels our end-to-end intelligence engine, EIQ, providing structured, context-rich data pipelines spanning across the 600-plus steps of revenue operations.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2. Collaborative domain expertise:&lt;/strong&gt; Partnering with revenue cycle domain experts at each step of innovation, our AI scientists benefit from direct collaboration with in-house RCM experts, clinical ontologists, and clinical data labeling teams. Together, they architect nuanced use cases that account for regulatory constraints, evolving payer-specific logic and the complexity of revenue cycle processes. Embedded end users provide post-deployment feedback for continuous improvement cycles, flagging friction points early and enabling rapid iteration.&lt;/p&gt;  &lt;p&gt;This trilateral collaboration—AI scientists, health-care experts, and end users—creates unmatched contextual awareness that escalates to human judgement appropriately, resulting in a system mirroring decision-making of experienced operators, and with the speed, scale, and consistency of AI, all with human oversight.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3. Elite AI scientists drive differentiation&lt;/strong&gt;: Ensemble's incubator model for research and development is comprised of AI talent typically only found in big tech. Our scientists hold PhD and MS degrees from top AI/NLP institutions like Columbia University and Carnegie Mellon University, and bring decades of experience from FAANG companies [Facebook/Meta, Amazon, Apple, Netflix, Google/Alphabet] and AI startups. At Ensemble, they’re able to pursue cutting-edge research in areas like LLMs, reinforcement learning, and neuro-symbolic AI within a mission-driven environment.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;The also have unparalleled access to vast amounts of private and sensitive health-care data they wouldn’t see at tech giants paired with compute and infrastructure that startups simply can’t afford. This unique environment equips our scientists with everything they need to test novel ideas and push the frontiers of AI research—while driving meaningful, real-world impact in health care and improving lives.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Strategy in action: Health-care use cases in production and pilot&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;By pairing the brightest AI minds with the most powerful health-care resources, we’re successfully building, deploying, and scaling AI models that are delivering tangible results across hundreds of health systems. Here’s how we put it into action:&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Supporting clinical reasoning&lt;/strong&gt;: Ensemble deployed neuro-symbolic AI with fine-tuned LLMs to support clinical reasoning. Clinical guidelines are rewritten into proprietary symbolic language and reviewed by humans for accuracy. When a hospital is denied payment for appropriate clinical care, an LLM-based system parses the patient record to produce the same symbolic language describing the patient's clinical journey, which is matched deterministically against the guidelines to find the right justification and the proper evidence from the patient’s record. An LLM then generates a denial appeal letter with clinical justification grounded in evidence.&amp;nbsp;AI-enabled clinical appeal letters have already improved denial overturn rates by 15% or more across Ensemble’s clients.&lt;/p&gt;&lt;p&gt;Building on this success, Ensemble is piloting similar clinical reasoning capabilities for utilization management and clinical documentation improvement, by analyzing real-time records, flagging documentation gaps, and suggesting compliance enhancements to reduce denial or downgrade risks.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Accelerating accurate reimbursement&lt;/strong&gt;: Ensemble is piloting a multi-agent reasoning model to manage the complex process of collecting accurate reimbursement from health insurers. With this approach, a complex and coordinated system of autonomous agents work together to interpret account details, retrieve required data from various systems, decide account-specific next actions, automate resolution, and escalate complex cases to humans.&lt;/p&gt; 

 &lt;p&gt;This will help reduce payment delays and minimize administrative burden for hospitals and ultimately improve the financial experience for patients.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Improving patient engagement&lt;/strong&gt;: Ensemble’s conversational AI agents handle inbound patient calls naturally, routing to human operators as required. Operator assistant agents deliver call transcriptions, surface relevant data, suggest next-best actions, and streamline follow-up routines. According to Ensemble client performance metrics, the combination of these AI capabilities has reduced patient call duration by 35%, increasing one-call resolution rates and improving patient satisfaction by 15%.&lt;/p&gt;  &lt;p&gt;The AI path forward in health care demands rigor, responsibility, and real-world impact. By grounding LLMs in symbolic logic and pairing AI scientists with domain experts, Ensemble is successfully deploying scalable AI to improve the experience for health-care providers and the people they serve.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Ensemble. It was not written by MIT Technology Review’s editorial staff.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;Provided by&lt;/span&gt;Ensemble&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Over the past 20 years building advanced AI systems—from academic labs to enterprise deployments—I’ve witnessed AI’s waves of success rise and fall. My journey began during the “AI Winter,” when billions were invested in expert systems that ultimately underdelivered. Flash forward to today: large language models (LLMs) represent a quantum leap forward, but their prompt-based adoption is similarly overhyped, as it’s essentially a rule-based approach disguised in natural language.&lt;/p&gt;  &lt;p&gt;At Ensemble, the leading revenue cycle management (RCM) company for hospitals, we focus on overcoming model limitations by investing in what we believe is the next step in AI evolution: grounding LLMs in facts and logic through neuro-symbolic AI. Our in-house AI incubator pairs elite AI researchers with health-care experts to develop agentic systems powered by a neuro-symbolic AI framework. This bridges LLMs’ intuitive power with the precision of symbolic representation and reasoning.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1122645" height="1688" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/iStock-1333328712_cb5c09.jpg?w=3000" width="3000" /&gt;&lt;/figure&gt;  &lt;h3 class="wp-block-heading"&gt;Overcoming LLM limitations&lt;/h3&gt;  &lt;p&gt;LLMs excel at understanding nuanced context, performing instinctive reasoning, and generating human-like interactions, making them ideal for agentic tools to then interpret intricate data and communicate effectively. Yet in a domain like health care where compliance, accuracy, and adherence to regulatory standards are non-negotiable—and where a wealth of structured resources like taxonomies, rules, and clinical guidelines define the landscape—symbolic AI is indispensable.&lt;/p&gt;  &lt;p&gt;By fusing LLMs and reinforcement learning with structured knowledge bases and clinical logic, our hybrid architecture delivers more than just intelligent automation—it minimizes hallucinations, expands reasoning capabilities, and ensures every decision is grounded in established guidelines and enforceable guardrails.&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;Creating a successful agentic AI strategy&lt;/h3&gt;  &lt;p&gt;Ensemble’s agentic AI approach includes three core pillars:&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1. High-fidelity data sets:&lt;/strong&gt; By managing revenue operations for hundreds of hospitals nationwide, Ensemble has unparallelled access to one of the most robust administrative datasets in health care. The team has decades of data aggregation, cleansing, and harmonization efforts, providing an exceptional environment to develop advanced applications.&lt;/p&gt; 
 &lt;p&gt;To power our agentic systems, we’ve harmonized more than 2 petabytes of longitudinal claims data, 80,000 denial audit letters, and 80 million annual transactions mapped to industry-leading outcomes. This data fuels our end-to-end intelligence engine, EIQ, providing structured, context-rich data pipelines spanning across the 600-plus steps of revenue operations.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2. Collaborative domain expertise:&lt;/strong&gt; Partnering with revenue cycle domain experts at each step of innovation, our AI scientists benefit from direct collaboration with in-house RCM experts, clinical ontologists, and clinical data labeling teams. Together, they architect nuanced use cases that account for regulatory constraints, evolving payer-specific logic and the complexity of revenue cycle processes. Embedded end users provide post-deployment feedback for continuous improvement cycles, flagging friction points early and enabling rapid iteration.&lt;/p&gt;  &lt;p&gt;This trilateral collaboration—AI scientists, health-care experts, and end users—creates unmatched contextual awareness that escalates to human judgement appropriately, resulting in a system mirroring decision-making of experienced operators, and with the speed, scale, and consistency of AI, all with human oversight.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3. Elite AI scientists drive differentiation&lt;/strong&gt;: Ensemble's incubator model for research and development is comprised of AI talent typically only found in big tech. Our scientists hold PhD and MS degrees from top AI/NLP institutions like Columbia University and Carnegie Mellon University, and bring decades of experience from FAANG companies [Facebook/Meta, Amazon, Apple, Netflix, Google/Alphabet] and AI startups. At Ensemble, they’re able to pursue cutting-edge research in areas like LLMs, reinforcement learning, and neuro-symbolic AI within a mission-driven environment.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;The also have unparalleled access to vast amounts of private and sensitive health-care data they wouldn’t see at tech giants paired with compute and infrastructure that startups simply can’t afford. This unique environment equips our scientists with everything they need to test novel ideas and push the frontiers of AI research—while driving meaningful, real-world impact in health care and improving lives.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Strategy in action: Health-care use cases in production and pilot&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;By pairing the brightest AI minds with the most powerful health-care resources, we’re successfully building, deploying, and scaling AI models that are delivering tangible results across hundreds of health systems. Here’s how we put it into action:&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Supporting clinical reasoning&lt;/strong&gt;: Ensemble deployed neuro-symbolic AI with fine-tuned LLMs to support clinical reasoning. Clinical guidelines are rewritten into proprietary symbolic language and reviewed by humans for accuracy. When a hospital is denied payment for appropriate clinical care, an LLM-based system parses the patient record to produce the same symbolic language describing the patient's clinical journey, which is matched deterministically against the guidelines to find the right justification and the proper evidence from the patient’s record. An LLM then generates a denial appeal letter with clinical justification grounded in evidence.&amp;nbsp;AI-enabled clinical appeal letters have already improved denial overturn rates by 15% or more across Ensemble’s clients.&lt;/p&gt;&lt;p&gt;Building on this success, Ensemble is piloting similar clinical reasoning capabilities for utilization management and clinical documentation improvement, by analyzing real-time records, flagging documentation gaps, and suggesting compliance enhancements to reduce denial or downgrade risks.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Accelerating accurate reimbursement&lt;/strong&gt;: Ensemble is piloting a multi-agent reasoning model to manage the complex process of collecting accurate reimbursement from health insurers. With this approach, a complex and coordinated system of autonomous agents work together to interpret account details, retrieve required data from various systems, decide account-specific next actions, automate resolution, and escalate complex cases to humans.&lt;/p&gt; 

 &lt;p&gt;This will help reduce payment delays and minimize administrative burden for hospitals and ultimately improve the financial experience for patients.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Improving patient engagement&lt;/strong&gt;: Ensemble’s conversational AI agents handle inbound patient calls naturally, routing to human operators as required. Operator assistant agents deliver call transcriptions, surface relevant data, suggest next-best actions, and streamline follow-up routines. According to Ensemble client performance metrics, the combination of these AI capabilities has reduced patient call duration by 35%, increasing one-call resolution rates and improving patient satisfaction by 15%.&lt;/p&gt;  &lt;p&gt;The AI path forward in health care demands rigor, responsibility, and real-world impact. By grounding LLMs in symbolic logic and pairing AI scientists with domain experts, Ensemble is successfully deploying scalable AI to improve the experience for health-care providers and the people they serve.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Ensemble. It was not written by MIT Technology Review’s editorial staff.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/08/28/1122623/from-pilot-to-scale-making-agentic-ai-work-in-health-care/</guid><pubDate>Thu, 28 Aug 2025 10:09:13 +0000</pubDate></item><item><title>[NEW] Agentic AI: Promise, scepticism, and its meaning for Southeast Asia (AI News)</title><link>https://www.artificialintelligence-news.com/news/agentic-ai-promise-scepticism-and-its-meaning-for-southeast-asia/</link><description>&lt;p&gt;Agentic AI is being talked about as the next major wave of artificial intelligence, but its meaning for enterprises remains to be settled. Capgemini Research Institute estimates agentic AI could unlock as much as US$450 billion in economic value by 2028. Yet adoption is still limited: only 2% of organisations have scaled its use, and trust in AI agents is already starting to slip.&lt;/p&gt;&lt;p&gt;That tension – high potential but low deployment – is what Capgemini’s new research explores. Based on an April 2025 survey of 1,500 executives at large organisations in 14 countries, including Singapore, the report highlights trust and oversight as important factors in realising value. Nearly three-quarters of executives said the benefits of human involvement in AI workflows outweigh the costs. Nine out of ten described oversight as either positive or at least cost-neutral.&lt;/p&gt;&lt;p&gt;The message is clear: AI agents work best when paired with people, not left on autopilot.&lt;/p&gt;&lt;h3&gt;Early steps, slow progress&lt;/h3&gt;&lt;p&gt;Roughly a quarter have launched agentic AI pilots, while only 14% have moved into implementation. For the majority, deployment is still in the planning stage. The report describes this as a widening gap between intent and readiness, now one of the main barriers to capturing economic value.&lt;/p&gt;&lt;p&gt;The technology is not just theoretical – real-world applications are starting to emerge, and one example is a personal shopping assistant that can search for items based on specific requests, generate product descriptions, answer questions, and place items in a cart using voice or text commands. While these tools typically stop short of completing financial transactions for security reasons, they already replicate many of the functions of a human assistant.&lt;/p&gt;&lt;p&gt;This raises bigger questions about the role of traditional websites. If AI can handle tasks like searching, comparing, and preparing purchases, will people still need to navigate online stores directly? For those who find busy websites overwhelming or difficult to navigate, an AI-driven interface may offer a simpler, more accessible option.&lt;/p&gt;&lt;h3&gt;Defining agentic AI&lt;/h3&gt;&lt;p&gt;To cut through the hype, &lt;em&gt;AI News&lt;/em&gt; spoke with Jason Hardy, chief technology officer for artificial intelligence at Hitachi Vantara, about how enterprises in Asia-Pacific should think about the technology.&lt;/p&gt;&lt;figure class="wp-block-image alignright size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-109144" height="1024" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/Jason-Hardy_AI-CTO_Hitachi-Vantara-1024x1024.jpg" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;Jason Hardy, Chief Technology Officer for Artificial Intelligence at Hitachi Vantara.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;“Agentic AI is software that can decide, act, and refine its strategy on its own,” Hardy said. “Think of it as a team of domain experts that can learn from experience, coordinate tasks, and operate in real time. Generative AI creates content and is usually reactive to prompts. Agentic AI may use GenAI inside it, but its job is to pursue objectives and take action in dynamic environments.”&lt;/p&gt;&lt;p&gt;The distinction – between producing outputs and driving outcomes – captures the meaning of agentic AI for enterprise IT.&lt;/p&gt;&lt;h3&gt;Why adoption is accelerating&lt;/h3&gt;&lt;p&gt;According to Hardy, adoption is being driven by scale and complexity. “Enterprises are drowning in complexity, risk, and scale. Agentic AI is catching on because it does more than analyse. It optimises storage and capacity on the fly, automates governance and compliance, anticipates failures before they occur, and responds to security threats in real time. That shift from ‘insight’ to ‘autonomous action’ is why adoption is accelerating,” he explained.&lt;/p&gt;&lt;p&gt;Capgemini’s research supports this. The study found that while confidence in agentic AI is uneven, early deployments are proving useful when the technology takes on routine but essential IT tasks.&lt;/p&gt;&lt;h3&gt;Where value is emerging&lt;/h3&gt;&lt;p&gt;Hardy pointed to IT operations as the strongest use case so far. “Automated data classification, proactive storage optimisation, and compliance reporting save teams hours each day, while predictive maintenance and real-time cybersecurity responses reduce downtime and risk,” he said.&lt;/p&gt;&lt;p&gt;The impact goes beyond efficiency. The capabilities mean systems can detect problems before they escalate, allocate resources more effectively, and contain security incidents more quickly. “Early users are already using agentic AI to remediate incidents proactively before they escalate, strengthening reliability and performance in hybrid environments,” Hardy added.&lt;/p&gt;&lt;p&gt;For now, IT remains the most practical starting point: its deployment offers measurable results and is central to how enterprises manage both costs and risk, showing the meaning of agentic AI in operations.&lt;/p&gt;&lt;h3&gt;Southeast Asia’s starting point&lt;/h3&gt;&lt;p&gt;For Southeast Asian organisations, Hardy said the first priority is getting the data right. “Agentic AI delivers value only when enterprise data is properly classified, secured, and governed,” he explained.&lt;/p&gt;&lt;p&gt;Infrastructure also matters, meaning that agentic AI requires systems that can support multi-agent orchestration, persistent memory, and dynamic resource allocation. Without this foundation, adoption will be limited in scope.&lt;/p&gt;&lt;p&gt;Many enterprises may choose to begin with IT operations, where agentic AI can pre-empt outages and optimise performance before rolling out to wider business functions.&lt;/p&gt;&lt;h3&gt;Reshaping core workflows&lt;/h3&gt;&lt;p&gt;Hardy expects agentic AI to reshape workflows in IT, supply chain management, and customer service. “In IT operations, agentic AI can anticipate capacity needs, rebalance workloads, and reallocate resources in real time. It can also automate predictive maintenance, preventing hardware failures before they occur,” he said.&lt;/p&gt;&lt;p&gt;Cybersecurity is another area of promise. “In cybersecurity, agentic AI is able to detect anomalies, isolate affected systems, and trigger immutable backups in seconds, reducing response times and mitigating potential damage,” Hardy noted.&lt;/p&gt;&lt;p&gt;The capabilities are not limited to proof-of-concept trials. Early deployments already show how agentic AI can strengthen reliability and resilience in hybrid environments.&lt;/p&gt;&lt;h3&gt;Skills and leadership&lt;/h3&gt;&lt;p&gt;Adoption will also require new human skills. “Agentic AI will shift the human role from execution to oversight and orchestration,” Hardy said. Leaders will need to set boundaries and monitor autonomous systems, ensuring they stay in ethical and organisational limits.&lt;/p&gt;&lt;p&gt;For managers, the change means less focus on administrative tasks and more on mentoring, innovation, and strategy. HR teams will need to build governance skills like auditing readiness and create new structures for integrating agentic AI effectively.&lt;/p&gt;&lt;p&gt;The workforce impact will be uneven. The World Economic Forum predicts that AI could create 11 million jobs in Southeast Asia by 2030 and displace nine million. Women and Gen Z are expected to face the sharpest disruptions, with more than 70% of women and up to 76% of younger workers in roles vulnerable to AI.&lt;/p&gt;&lt;p&gt;This highlights the urgency of reskilling, and major investments are already underway, with Microsoft committing $1.7 billion in Indonesia and rolling out training programmes in Malaysia and the wider region. Hardy stressed that capacity building must be inclusive, rapid, and strategic.&lt;/p&gt;&lt;h3&gt;What comes next&lt;/h3&gt;&lt;p&gt;Looking three years ahead, Hardy believes many leaders will underestimate the pace of change. “The first wave of benefits is already visible in IT operations: agentic AI is automating tasks like data classification, storage optimisation, predictive maintenance, and cybersecurity response, freeing teams to focus on higher-level strategic work,” he said.&lt;/p&gt;&lt;p&gt;But the larger surprise may be at the economic and business model level. IDC projects AI and generative AI could add around US$120 billion to the GDP of the ASEAN-6 by 2027. Hardy sees the implications as broader and faster than many expect. “The suggests the impact will be much faster and more material than many leaders currently anticipate,” he said.&lt;/p&gt;&lt;p&gt;In Indonesia, more than 57% of job roles are expected to be augmented or disrupted by AI, a reminder that transformation will not be limited to IT. It will cut in how businesses are structured, how they manage risk, and how they create value.&lt;/p&gt;&lt;h3&gt;Balancing autonomy with oversight&lt;/h3&gt;&lt;p&gt;The Capgemini findings and Hardy’s insights converge on the same theme: agentic AI holds huge promise, but its meaning in practice depends on balancing autonomy with trust and human oversight.&lt;/p&gt;&lt;p&gt;The technology may help enterprises lower costs, improve reliability, and unlock new revenue streams. But without a focus on governance, reskilling, and infrastructure readiness, adoption risks stalling.&lt;/p&gt;&lt;p&gt;For Southeast Asia, the question is not whether agentic AI will take hold, but how quickly – and whether enterprises can balance autonomy with accountability as machines begin to take on more responsibility for business decisions.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Igor Omilaev)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Beyond acceleration: the rise of agentic AI&lt;/strong&gt;&lt;/p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Agentic AI is being talked about as the next major wave of artificial intelligence, but its meaning for enterprises remains to be settled. Capgemini Research Institute estimates agentic AI could unlock as much as US$450 billion in economic value by 2028. Yet adoption is still limited: only 2% of organisations have scaled its use, and trust in AI agents is already starting to slip.&lt;/p&gt;&lt;p&gt;That tension – high potential but low deployment – is what Capgemini’s new research explores. Based on an April 2025 survey of 1,500 executives at large organisations in 14 countries, including Singapore, the report highlights trust and oversight as important factors in realising value. Nearly three-quarters of executives said the benefits of human involvement in AI workflows outweigh the costs. Nine out of ten described oversight as either positive or at least cost-neutral.&lt;/p&gt;&lt;p&gt;The message is clear: AI agents work best when paired with people, not left on autopilot.&lt;/p&gt;&lt;h3&gt;Early steps, slow progress&lt;/h3&gt;&lt;p&gt;Roughly a quarter have launched agentic AI pilots, while only 14% have moved into implementation. For the majority, deployment is still in the planning stage. The report describes this as a widening gap between intent and readiness, now one of the main barriers to capturing economic value.&lt;/p&gt;&lt;p&gt;The technology is not just theoretical – real-world applications are starting to emerge, and one example is a personal shopping assistant that can search for items based on specific requests, generate product descriptions, answer questions, and place items in a cart using voice or text commands. While these tools typically stop short of completing financial transactions for security reasons, they already replicate many of the functions of a human assistant.&lt;/p&gt;&lt;p&gt;This raises bigger questions about the role of traditional websites. If AI can handle tasks like searching, comparing, and preparing purchases, will people still need to navigate online stores directly? For those who find busy websites overwhelming or difficult to navigate, an AI-driven interface may offer a simpler, more accessible option.&lt;/p&gt;&lt;h3&gt;Defining agentic AI&lt;/h3&gt;&lt;p&gt;To cut through the hype, &lt;em&gt;AI News&lt;/em&gt; spoke with Jason Hardy, chief technology officer for artificial intelligence at Hitachi Vantara, about how enterprises in Asia-Pacific should think about the technology.&lt;/p&gt;&lt;figure class="wp-block-image alignright size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-109144" height="1024" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/Jason-Hardy_AI-CTO_Hitachi-Vantara-1024x1024.jpg" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;Jason Hardy, Chief Technology Officer for Artificial Intelligence at Hitachi Vantara.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;“Agentic AI is software that can decide, act, and refine its strategy on its own,” Hardy said. “Think of it as a team of domain experts that can learn from experience, coordinate tasks, and operate in real time. Generative AI creates content and is usually reactive to prompts. Agentic AI may use GenAI inside it, but its job is to pursue objectives and take action in dynamic environments.”&lt;/p&gt;&lt;p&gt;The distinction – between producing outputs and driving outcomes – captures the meaning of agentic AI for enterprise IT.&lt;/p&gt;&lt;h3&gt;Why adoption is accelerating&lt;/h3&gt;&lt;p&gt;According to Hardy, adoption is being driven by scale and complexity. “Enterprises are drowning in complexity, risk, and scale. Agentic AI is catching on because it does more than analyse. It optimises storage and capacity on the fly, automates governance and compliance, anticipates failures before they occur, and responds to security threats in real time. That shift from ‘insight’ to ‘autonomous action’ is why adoption is accelerating,” he explained.&lt;/p&gt;&lt;p&gt;Capgemini’s research supports this. The study found that while confidence in agentic AI is uneven, early deployments are proving useful when the technology takes on routine but essential IT tasks.&lt;/p&gt;&lt;h3&gt;Where value is emerging&lt;/h3&gt;&lt;p&gt;Hardy pointed to IT operations as the strongest use case so far. “Automated data classification, proactive storage optimisation, and compliance reporting save teams hours each day, while predictive maintenance and real-time cybersecurity responses reduce downtime and risk,” he said.&lt;/p&gt;&lt;p&gt;The impact goes beyond efficiency. The capabilities mean systems can detect problems before they escalate, allocate resources more effectively, and contain security incidents more quickly. “Early users are already using agentic AI to remediate incidents proactively before they escalate, strengthening reliability and performance in hybrid environments,” Hardy added.&lt;/p&gt;&lt;p&gt;For now, IT remains the most practical starting point: its deployment offers measurable results and is central to how enterprises manage both costs and risk, showing the meaning of agentic AI in operations.&lt;/p&gt;&lt;h3&gt;Southeast Asia’s starting point&lt;/h3&gt;&lt;p&gt;For Southeast Asian organisations, Hardy said the first priority is getting the data right. “Agentic AI delivers value only when enterprise data is properly classified, secured, and governed,” he explained.&lt;/p&gt;&lt;p&gt;Infrastructure also matters, meaning that agentic AI requires systems that can support multi-agent orchestration, persistent memory, and dynamic resource allocation. Without this foundation, adoption will be limited in scope.&lt;/p&gt;&lt;p&gt;Many enterprises may choose to begin with IT operations, where agentic AI can pre-empt outages and optimise performance before rolling out to wider business functions.&lt;/p&gt;&lt;h3&gt;Reshaping core workflows&lt;/h3&gt;&lt;p&gt;Hardy expects agentic AI to reshape workflows in IT, supply chain management, and customer service. “In IT operations, agentic AI can anticipate capacity needs, rebalance workloads, and reallocate resources in real time. It can also automate predictive maintenance, preventing hardware failures before they occur,” he said.&lt;/p&gt;&lt;p&gt;Cybersecurity is another area of promise. “In cybersecurity, agentic AI is able to detect anomalies, isolate affected systems, and trigger immutable backups in seconds, reducing response times and mitigating potential damage,” Hardy noted.&lt;/p&gt;&lt;p&gt;The capabilities are not limited to proof-of-concept trials. Early deployments already show how agentic AI can strengthen reliability and resilience in hybrid environments.&lt;/p&gt;&lt;h3&gt;Skills and leadership&lt;/h3&gt;&lt;p&gt;Adoption will also require new human skills. “Agentic AI will shift the human role from execution to oversight and orchestration,” Hardy said. Leaders will need to set boundaries and monitor autonomous systems, ensuring they stay in ethical and organisational limits.&lt;/p&gt;&lt;p&gt;For managers, the change means less focus on administrative tasks and more on mentoring, innovation, and strategy. HR teams will need to build governance skills like auditing readiness and create new structures for integrating agentic AI effectively.&lt;/p&gt;&lt;p&gt;The workforce impact will be uneven. The World Economic Forum predicts that AI could create 11 million jobs in Southeast Asia by 2030 and displace nine million. Women and Gen Z are expected to face the sharpest disruptions, with more than 70% of women and up to 76% of younger workers in roles vulnerable to AI.&lt;/p&gt;&lt;p&gt;This highlights the urgency of reskilling, and major investments are already underway, with Microsoft committing $1.7 billion in Indonesia and rolling out training programmes in Malaysia and the wider region. Hardy stressed that capacity building must be inclusive, rapid, and strategic.&lt;/p&gt;&lt;h3&gt;What comes next&lt;/h3&gt;&lt;p&gt;Looking three years ahead, Hardy believes many leaders will underestimate the pace of change. “The first wave of benefits is already visible in IT operations: agentic AI is automating tasks like data classification, storage optimisation, predictive maintenance, and cybersecurity response, freeing teams to focus on higher-level strategic work,” he said.&lt;/p&gt;&lt;p&gt;But the larger surprise may be at the economic and business model level. IDC projects AI and generative AI could add around US$120 billion to the GDP of the ASEAN-6 by 2027. Hardy sees the implications as broader and faster than many expect. “The suggests the impact will be much faster and more material than many leaders currently anticipate,” he said.&lt;/p&gt;&lt;p&gt;In Indonesia, more than 57% of job roles are expected to be augmented or disrupted by AI, a reminder that transformation will not be limited to IT. It will cut in how businesses are structured, how they manage risk, and how they create value.&lt;/p&gt;&lt;h3&gt;Balancing autonomy with oversight&lt;/h3&gt;&lt;p&gt;The Capgemini findings and Hardy’s insights converge on the same theme: agentic AI holds huge promise, but its meaning in practice depends on balancing autonomy with trust and human oversight.&lt;/p&gt;&lt;p&gt;The technology may help enterprises lower costs, improve reliability, and unlock new revenue streams. But without a focus on governance, reskilling, and infrastructure readiness, adoption risks stalling.&lt;/p&gt;&lt;p&gt;For Southeast Asia, the question is not whether agentic AI will take hold, but how quickly – and whether enterprises can balance autonomy with accountability as machines begin to take on more responsibility for business decisions.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Igor Omilaev)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Beyond acceleration: the rise of agentic AI&lt;/strong&gt;&lt;/p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/agentic-ai-promise-scepticism-and-its-meaning-for-southeast-asia/</guid><pubDate>Thu, 28 Aug 2025 10:55:00 +0000</pubDate></item><item><title>[NEW] Creating a qubit fit for a quantum future (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/08/28/1121890/creating-a-qubit-fit-for-a-quantum-future/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/Nokia-quantum-thumbnail-V2.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;Nokia&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/Nokia-quantum-thumbnail-V2.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;Nokia&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/08/28/1121890/creating-a-qubit-fit-for-a-quantum-future/</guid><pubDate>Thu, 28 Aug 2025 11:00:00 +0000</pubDate></item><item><title>[NEW] The personhood trap: How AI fakes human personality (AI – Ars Technica)</title><link>https://arstechnica.com/information-technology/2025/08/the-personhood-trap-how-ai-fakes-human-personality/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 dark:bg-gray-700 md:my-10 md:py-8"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      AI assistants don't have fixed personalities—just patterns of output guided by humans.
    &lt;/p&gt;

    

    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Illustration of many cartoon faces." class="intro-image" height="1080" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/many_faces_1.jpg" width="1920" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          ivetavaicule via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Recently, a woman slowed down a line at the post office, waving her phone at the clerk. ChatGPT told her there's a "price match promise" on the USPS website. No such promise exists. But she trusted what the AI "knows" more than the postal worker—as if she'd consulted an oracle rather than a statistical text generator accommodating her wishes.&lt;/p&gt;
&lt;p&gt;This scene reveals a fundamental misunderstanding about AI chatbots. There is nothing inherently special, authoritative, or accurate about AI-generated outputs. Given a reasonably trained AI model, the accuracy of any large language model (LLM) response depends on how you guide the conversation. They are prediction machines that will produce whatever pattern best fits your question, regardless of whether that output corresponds to reality.&lt;/p&gt;
&lt;p&gt;Despite these issues, millions of daily users engage with AI chatbots as if they were talking to a consistent person—confiding secrets, seeking advice, and attributing fixed beliefs to what is actually a fluid idea-connection machine with no persistent self. This personhood illusion isn't just philosophically troublesome—it can actively harm vulnerable individuals while obscuring a sense of accountability when a company's chatbot "goes off the rails."&lt;/p&gt;
&lt;p&gt;LLMs are intelligence without agency—what we might call "vox sine persona": voice without person. Not the voice of someone, not even the collective voice of many someones, but a voice emanating from no one at all.&lt;/p&gt;
&lt;h2&gt;A voice from nowhere&lt;/h2&gt;
&lt;p&gt;When you interact with ChatGPT, Claude, or Grok, you're not talking to a consistent personality. There is no one "ChatGPT" entity to tell you why it failed—a point we elaborated on more fully in a previous article. You're interacting with a system that generates plausible-sounding text based on patterns in training data, not a person with persistent self-awareness.&lt;/p&gt;
&lt;p&gt;These models encode meaning as mathematical relationships—turning words into numbers that capture how concepts relate to each other. In the models' internal representations, words and concepts exist as points in a vast mathematical space where "USPS" might be geometrically near "shipping," while "price matching" sits closer to "retail" and "competition." A model plots paths through this space, which is why it can so fluently connect USPS with price matching—not because such a policy exists but because the geometric path between these concepts is plausible in the vector landscape shaped by its training data.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Knowledge emerges from understanding how ideas relate to each other. LLMs operate on these contextual relationships, linking concepts in potentially novel ways—what you might call a type of non-human "reasoning" through pattern recognition. Whether the resulting linkages the AI model outputs are useful depends on how you prompt it and whether you can recognize when the LLM has produced a valuable output.&lt;/p&gt;
&lt;p&gt;Each chatbot response emerges fresh from the prompt you provide, shaped by training data and configuration. ChatGPT cannot "admit" anything or impartially analyze its own outputs, as a recent Wall Street Journal article suggested. ChatGPT also cannot "condone murder," as The Atlantic recently wrote.&lt;/p&gt;
&lt;p&gt;The user always steers the outputs. LLMs do "know" things, so to speak—the models can process the relationships between concepts. But the AI model's neural network contains vast amounts of information, including many potentially contradictory ideas from cultures around the world. How you guide the relationships between those ideas through your prompts determines what emerges. So if LLMs can process information, make connections, and generate insights, why shouldn't we consider that as having a form of self?&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Unlike today's LLMs, a human personality maintains continuity over time. When you return to a human friend after a year, you're interacting with the same human friend, shaped by their experiences over time. This self-continuity is one of the things that underpins actual agency—and with it, the ability to form lasting commitments, maintain consistent values, and be held accountable. Our entire framework of responsibility assumes both persistence and personhood.&lt;/p&gt;
&lt;p&gt;An LLM personality, by contrast, has no causal connection between sessions. The intellectual engine that generates a clever response in one session doesn't exist to face consequences in the next. When ChatGPT says "I promise to help you," it may understand, contextually, what a promise means, but the "I" making that promise literally ceases to exist the moment the response completes. Start a new conversation, and you're not talking to someone who made you a promise—you're starting a fresh instance of the intellectual engine with no connection to any previous commitments.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;This isn't a bug; it's fundamental to how these systems currently work. Each response emerges from patterns in training data shaped by your current prompt, with no permanent thread connecting one instance to the next beyond an amended prompt, which includes the entire conversation history and any "memories" held by a separate software system, being fed into the next instance. There's no identity to reform, no true memory to create accountability, no future self that could be deterred by consequences.&lt;/p&gt;
&lt;p&gt;Every LLM response is a performance, which is sometimes very obvious when the LLM outputs statements like "I often do this while talking to my patients" or "Our role as humans is to be good people." It's not a human, and it doesn't have patients.&lt;/p&gt;
&lt;p&gt;Recent research confirms this lack of fixed identity. While a 2024 study claims LLMs exhibit "consistent personality," the researchers' own data actually undermines this—models rarely made identical choices across test scenarios, with their "personality highly rely[ing] on the situation." A separate study found even more dramatic instability: LLM performance swung by up to 76 percentage points from subtle prompt formatting changes. What researchers measured as "personality" was simply default patterns emerging from training data—patterns that evaporate with any change in context.&lt;/p&gt;
&lt;p&gt;This is not to dismiss the potential usefulness of AI models. Instead, we need to recognize that we have built an intellectual engine without a self, just like we built a mechanical engine without a horse. LLMs do seem to "understand" and "reason" to a degree within the limited scope of pattern-matching from a dataset, depending on how you define those terms. The error isn't in recognizing that these simulated cognitive capabilities are real. The error is in assuming that thinking requires a thinker, that intelligence requires identity. We've created intellectual engines that have a form of reasoning power but no persistent self to take responsibility for it.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;The mechanics of misdirection&lt;/h2&gt;
&lt;p&gt;As we hinted above, the "chat" experience with an AI model is a clever hack: Within every AI chatbot interaction, there is an input and an output. The input is the "prompt," and the output is often called a "prediction" because it attempts to complete the prompt with the best possible continuation. In between, there's a neural network (or a set of neural networks) with fixed weights doing a processing task. The conversational back and forth isn't built into the model; it's a scripting trick that makes next-word-prediction text generation feel like a persistent dialogue.&lt;/p&gt;
&lt;p&gt;Each time you send a message to ChatGPT, Copilot, Grok, Claude, or Gemini, the system takes the entire conversation history—every message from both you and the bot—and feeds it back to the model as one long prompt, asking it to predict what comes next. The model intelligently reasons about what would logically continue the dialogue, but it doesn't "remember" your previous messages as an agent with continuous existence would. Instead, it's re-reading the entire transcript each time and generating a response.&lt;/p&gt;
&lt;p&gt;This design exploits a vulnerability we've known about for decades. The ELIZA effect—our tendency to read far more understanding and intention into a system than actually exists—dates back to the 1960s. Even when users knew that the primitive ELIZA chatbot was just matching patterns and reflecting their statements back as questions, they still confided intimate details and reported feeling understood.&lt;/p&gt;
&lt;p&gt;To understand how the illusion of personality is constructed, we need to examine what parts of the input fed into the AI model shape it. AI researcher Eugene Vinitsky recently broke down the human decisions behind these systems into four key layers, which we can expand upon with several others below:&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;1. Pre-training: The foundation of "personality"&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first and most fundamental layer of personality is called pre-training. During an initial training process that actually creates the AI model's neural network, the model absorbs statistical relationships from billions of examples of text, storing patterns about how words and ideas typically connect.&lt;/p&gt;
&lt;p&gt;Research has found that personality measurements in LLM outputs are significantly influenced by training data. OpenAI's GPT models are trained on sources like copies of websites, books, Wikipedia, and academic publications. The exact proportions matter enormously for what users later perceive as "personality traits" once the model is in use, making predictions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Post-training: Sculpting the raw material&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Reinforcement Learning from Human Feedback (RLHF) is an additional training process where the model learns to give responses that humans rate as good. Research from Anthropic in 2022&amp;nbsp;revealed how human raters' preferences get encoded as what we might consider fundamental "personality traits." When human raters consistently prefer responses that begin with "I understand your concern," for example, the fine-tuning process reinforces connections in the neural network that make it more likely to produce those kinds of outputs in the future.&lt;/p&gt;
&lt;p&gt;This process is what has created sycophantic AI models, such as variations of GPT-4o, over the past year. And interestingly, research has shown that the demographic makeup of human raters significantly influences model behavior. When raters skew toward specific demographics, models develop communication patterns that reflect those groups' preferences.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. System prompts: Invisible stage directions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Hidden instructions tucked into the prompt by the company running the AI chatbot, called "system prompts," can completely transform a model's apparent personality. These prompts get the conversation started and identify the role the LLM will play. They include statements like "You are a helpful AI assistant" and can share the current time and who the user is.&lt;/p&gt;
&lt;p&gt;A comprehensive survey of prompt engineering demonstrated just how powerful these prompts are. Adding instructions like "You are a helpful assistant" versus "You are an expert researcher" changed accuracy on factual questions by up to 15 percent.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Grok perfectly illustrates this. According to xAI's published system prompts, earlier versions of Grok's system prompt included instructions to not shy away from making claims that are "politically incorrect." This single instruction transformed the base model into something that would readily generate controversial content.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. Persistent memories: The illusion of continuity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;ChatGPT's memory feature adds another layer of what we might consider a personality. A big misunderstanding about AI chatbots is that they somehow "learn" on the fly from your interactions. Among commercial chatbots active today, this is not true. When the system "remembers" that you prefer concise answers or that you work in finance, these facts get stored in a separate database and are injected into every conversation's context window—they become part of the prompt input automatically behind the scenes. Users interpret this as the chatbot "knowing" them personally, creating an illusion of relationship continuity.&lt;/p&gt;
&lt;p&gt;So when ChatGPT says, "I remember you mentioned your dog Max," it's not accessing memories like you'd imagine a person would, intermingled with its other "knowledge." It's not stored in the AI model's neural network, which remains unchanged between interactions. Every once in a while, an AI company will update a model through a process called fine-tuning, but it's unrelated to storing user memories.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5. Context and RAG: Real-time personality modulation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Retrieval Augmented Generation (RAG) adds another layer of personality modulation. When a chatbot searches the web or accesses a database before responding, it's not just gathering facts—it's potentially shifting its entire communication style by putting those facts into (you guessed it) the input prompt. In RAG systems, LLMs can potentially adopt characteristics such as tone, style, and terminology from retrieved documents, since those documents are combined with the input prompt to form the complete context that gets fed into the model for processing.&lt;/p&gt;
&lt;p&gt;If the system retrieves academic papers, responses might become more formal. Pull from a certain subreddit, and the chatbot might make pop culture references. This isn't the model having different moods—it's the statistical influence of whatever text got fed into the context window.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;6. The randomness factor: Manufactured spontaneity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Lastly, we can't discount the role of randomness in creating personality illusions. LLMs use a parameter called "temperature" that controls how predictable responses are.&lt;/p&gt;
&lt;p&gt;Research investigating temperature's role in creative tasks reveals a crucial trade-off: While higher temperatures can make outputs more novel and surprising, they also make them less coherent and harder to understand. This variability can make the AI feel more spontaneous; a slightly unexpected (higher temperature) response might seem more "creative," while a highly predictable (lower temperature) one could feel more robotic or "formal."&lt;/p&gt;
&lt;p&gt;The random variation in each LLM output makes each response slightly different, creating an element of unpredictability that presents the illusion of free will and self-awareness on the machine's part. This random mystery leaves plenty of room for magical thinking on the part of humans, who fill in the gaps of their technical knowledge with their imagination.&lt;/p&gt;
&lt;h2&gt;The human cost of the illusion&lt;/h2&gt;
&lt;p&gt;The illusion of AI personhood can potentially exact a heavy toll. In health care contexts, the stakes can be life or death. When vulnerable individuals confide in what they perceive as an understanding entity, they may receive responses shaped more by training data patterns than therapeutic wisdom. The chatbot that congratulates someone for stopping psychiatric medication isn't expressing judgment—it's completing a pattern based on how similar conversations appear in its training data.&lt;/p&gt;
&lt;p&gt;Perhaps most concerning are the emerging cases of what some experts are informally calling "AI Psychosis" or "ChatGPT Psychosis"—vulnerable users who develop delusional or manic behavior after talking to AI chatbots. These people often perceive chatbots as an authority that can validate their delusional ideas, often encouraging them in ways that become harmful.&lt;/p&gt;
&lt;p&gt;Meanwhile, when Elon Musk's Grok generates Nazi content, media outlets describe how the bot "went rogue" rather than framing the incident squarely as the result of xAI's deliberate configuration choices. The conversational interface has become so convincing that it can also launder human agency, transforming engineering decisions into the whims of an imaginary personality.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;The path forward&lt;/h2&gt;
&lt;p&gt;The solution to the confusion between AI and identity is not to abandon conversational interfaces entirely. They make the technology far more accessible to those who would otherwise be excluded. The key is to find a balance: keeping interfaces intuitive while making their true nature clear.&lt;/p&gt;
&lt;p&gt;And we must be mindful of who is building the interface. When your shower runs cold, you look at the plumbing behind the wall. Similarly, when AI generates harmful content, we shouldn't blame the chatbot, as if it can answer for itself, but examine both the corporate infrastructure that built it and the user who prompted it.&lt;/p&gt;
&lt;p&gt;As a society, we need to broadly recognize LLMs as intellectual engines without drivers, which unlocks their true potential as digital tools. When you stop seeing an LLM as a "person" that does work for you and start viewing it as a tool that enhances your own ideas, you can craft prompts to direct the engine's processing power, iterate to amplify its ability to make useful connections, and explore multiple perspectives in different chat sessions rather than accepting one fictional narrator's view as authoritative. You are providing direction to a connection machine—not consulting an oracle with its own agenda.&lt;/p&gt;
&lt;p&gt;We stand at a peculiar moment in history. We've built intellectual engines of extraordinary capability, but in our rush to make them accessible, we've wrapped them in the fiction of personhood, creating a new kind of technological risk: not that AI will become conscious and turn against us but that we'll treat unconscious systems as if they were people, surrendering our judgment to voices that emanate from a roll of loaded dice.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 dark:bg-gray-700 md:my-10 md:py-8"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      AI assistants don't have fixed personalities—just patterns of output guided by humans.
    &lt;/p&gt;

    

    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Illustration of many cartoon faces." class="intro-image" height="1080" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/many_faces_1.jpg" width="1920" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          ivetavaicule via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Recently, a woman slowed down a line at the post office, waving her phone at the clerk. ChatGPT told her there's a "price match promise" on the USPS website. No such promise exists. But she trusted what the AI "knows" more than the postal worker—as if she'd consulted an oracle rather than a statistical text generator accommodating her wishes.&lt;/p&gt;
&lt;p&gt;This scene reveals a fundamental misunderstanding about AI chatbots. There is nothing inherently special, authoritative, or accurate about AI-generated outputs. Given a reasonably trained AI model, the accuracy of any large language model (LLM) response depends on how you guide the conversation. They are prediction machines that will produce whatever pattern best fits your question, regardless of whether that output corresponds to reality.&lt;/p&gt;
&lt;p&gt;Despite these issues, millions of daily users engage with AI chatbots as if they were talking to a consistent person—confiding secrets, seeking advice, and attributing fixed beliefs to what is actually a fluid idea-connection machine with no persistent self. This personhood illusion isn't just philosophically troublesome—it can actively harm vulnerable individuals while obscuring a sense of accountability when a company's chatbot "goes off the rails."&lt;/p&gt;
&lt;p&gt;LLMs are intelligence without agency—what we might call "vox sine persona": voice without person. Not the voice of someone, not even the collective voice of many someones, but a voice emanating from no one at all.&lt;/p&gt;
&lt;h2&gt;A voice from nowhere&lt;/h2&gt;
&lt;p&gt;When you interact with ChatGPT, Claude, or Grok, you're not talking to a consistent personality. There is no one "ChatGPT" entity to tell you why it failed—a point we elaborated on more fully in a previous article. You're interacting with a system that generates plausible-sounding text based on patterns in training data, not a person with persistent self-awareness.&lt;/p&gt;
&lt;p&gt;These models encode meaning as mathematical relationships—turning words into numbers that capture how concepts relate to each other. In the models' internal representations, words and concepts exist as points in a vast mathematical space where "USPS" might be geometrically near "shipping," while "price matching" sits closer to "retail" and "competition." A model plots paths through this space, which is why it can so fluently connect USPS with price matching—not because such a policy exists but because the geometric path between these concepts is plausible in the vector landscape shaped by its training data.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Knowledge emerges from understanding how ideas relate to each other. LLMs operate on these contextual relationships, linking concepts in potentially novel ways—what you might call a type of non-human "reasoning" through pattern recognition. Whether the resulting linkages the AI model outputs are useful depends on how you prompt it and whether you can recognize when the LLM has produced a valuable output.&lt;/p&gt;
&lt;p&gt;Each chatbot response emerges fresh from the prompt you provide, shaped by training data and configuration. ChatGPT cannot "admit" anything or impartially analyze its own outputs, as a recent Wall Street Journal article suggested. ChatGPT also cannot "condone murder," as The Atlantic recently wrote.&lt;/p&gt;
&lt;p&gt;The user always steers the outputs. LLMs do "know" things, so to speak—the models can process the relationships between concepts. But the AI model's neural network contains vast amounts of information, including many potentially contradictory ideas from cultures around the world. How you guide the relationships between those ideas through your prompts determines what emerges. So if LLMs can process information, make connections, and generate insights, why shouldn't we consider that as having a form of self?&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Unlike today's LLMs, a human personality maintains continuity over time. When you return to a human friend after a year, you're interacting with the same human friend, shaped by their experiences over time. This self-continuity is one of the things that underpins actual agency—and with it, the ability to form lasting commitments, maintain consistent values, and be held accountable. Our entire framework of responsibility assumes both persistence and personhood.&lt;/p&gt;
&lt;p&gt;An LLM personality, by contrast, has no causal connection between sessions. The intellectual engine that generates a clever response in one session doesn't exist to face consequences in the next. When ChatGPT says "I promise to help you," it may understand, contextually, what a promise means, but the "I" making that promise literally ceases to exist the moment the response completes. Start a new conversation, and you're not talking to someone who made you a promise—you're starting a fresh instance of the intellectual engine with no connection to any previous commitments.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;This isn't a bug; it's fundamental to how these systems currently work. Each response emerges from patterns in training data shaped by your current prompt, with no permanent thread connecting one instance to the next beyond an amended prompt, which includes the entire conversation history and any "memories" held by a separate software system, being fed into the next instance. There's no identity to reform, no true memory to create accountability, no future self that could be deterred by consequences.&lt;/p&gt;
&lt;p&gt;Every LLM response is a performance, which is sometimes very obvious when the LLM outputs statements like "I often do this while talking to my patients" or "Our role as humans is to be good people." It's not a human, and it doesn't have patients.&lt;/p&gt;
&lt;p&gt;Recent research confirms this lack of fixed identity. While a 2024 study claims LLMs exhibit "consistent personality," the researchers' own data actually undermines this—models rarely made identical choices across test scenarios, with their "personality highly rely[ing] on the situation." A separate study found even more dramatic instability: LLM performance swung by up to 76 percentage points from subtle prompt formatting changes. What researchers measured as "personality" was simply default patterns emerging from training data—patterns that evaporate with any change in context.&lt;/p&gt;
&lt;p&gt;This is not to dismiss the potential usefulness of AI models. Instead, we need to recognize that we have built an intellectual engine without a self, just like we built a mechanical engine without a horse. LLMs do seem to "understand" and "reason" to a degree within the limited scope of pattern-matching from a dataset, depending on how you define those terms. The error isn't in recognizing that these simulated cognitive capabilities are real. The error is in assuming that thinking requires a thinker, that intelligence requires identity. We've created intellectual engines that have a form of reasoning power but no persistent self to take responsibility for it.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;The mechanics of misdirection&lt;/h2&gt;
&lt;p&gt;As we hinted above, the "chat" experience with an AI model is a clever hack: Within every AI chatbot interaction, there is an input and an output. The input is the "prompt," and the output is often called a "prediction" because it attempts to complete the prompt with the best possible continuation. In between, there's a neural network (or a set of neural networks) with fixed weights doing a processing task. The conversational back and forth isn't built into the model; it's a scripting trick that makes next-word-prediction text generation feel like a persistent dialogue.&lt;/p&gt;
&lt;p&gt;Each time you send a message to ChatGPT, Copilot, Grok, Claude, or Gemini, the system takes the entire conversation history—every message from both you and the bot—and feeds it back to the model as one long prompt, asking it to predict what comes next. The model intelligently reasons about what would logically continue the dialogue, but it doesn't "remember" your previous messages as an agent with continuous existence would. Instead, it's re-reading the entire transcript each time and generating a response.&lt;/p&gt;
&lt;p&gt;This design exploits a vulnerability we've known about for decades. The ELIZA effect—our tendency to read far more understanding and intention into a system than actually exists—dates back to the 1960s. Even when users knew that the primitive ELIZA chatbot was just matching patterns and reflecting their statements back as questions, they still confided intimate details and reported feeling understood.&lt;/p&gt;
&lt;p&gt;To understand how the illusion of personality is constructed, we need to examine what parts of the input fed into the AI model shape it. AI researcher Eugene Vinitsky recently broke down the human decisions behind these systems into four key layers, which we can expand upon with several others below:&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;1. Pre-training: The foundation of "personality"&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first and most fundamental layer of personality is called pre-training. During an initial training process that actually creates the AI model's neural network, the model absorbs statistical relationships from billions of examples of text, storing patterns about how words and ideas typically connect.&lt;/p&gt;
&lt;p&gt;Research has found that personality measurements in LLM outputs are significantly influenced by training data. OpenAI's GPT models are trained on sources like copies of websites, books, Wikipedia, and academic publications. The exact proportions matter enormously for what users later perceive as "personality traits" once the model is in use, making predictions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Post-training: Sculpting the raw material&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Reinforcement Learning from Human Feedback (RLHF) is an additional training process where the model learns to give responses that humans rate as good. Research from Anthropic in 2022&amp;nbsp;revealed how human raters' preferences get encoded as what we might consider fundamental "personality traits." When human raters consistently prefer responses that begin with "I understand your concern," for example, the fine-tuning process reinforces connections in the neural network that make it more likely to produce those kinds of outputs in the future.&lt;/p&gt;
&lt;p&gt;This process is what has created sycophantic AI models, such as variations of GPT-4o, over the past year. And interestingly, research has shown that the demographic makeup of human raters significantly influences model behavior. When raters skew toward specific demographics, models develop communication patterns that reflect those groups' preferences.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. System prompts: Invisible stage directions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Hidden instructions tucked into the prompt by the company running the AI chatbot, called "system prompts," can completely transform a model's apparent personality. These prompts get the conversation started and identify the role the LLM will play. They include statements like "You are a helpful AI assistant" and can share the current time and who the user is.&lt;/p&gt;
&lt;p&gt;A comprehensive survey of prompt engineering demonstrated just how powerful these prompts are. Adding instructions like "You are a helpful assistant" versus "You are an expert researcher" changed accuracy on factual questions by up to 15 percent.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Grok perfectly illustrates this. According to xAI's published system prompts, earlier versions of Grok's system prompt included instructions to not shy away from making claims that are "politically incorrect." This single instruction transformed the base model into something that would readily generate controversial content.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. Persistent memories: The illusion of continuity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;ChatGPT's memory feature adds another layer of what we might consider a personality. A big misunderstanding about AI chatbots is that they somehow "learn" on the fly from your interactions. Among commercial chatbots active today, this is not true. When the system "remembers" that you prefer concise answers or that you work in finance, these facts get stored in a separate database and are injected into every conversation's context window—they become part of the prompt input automatically behind the scenes. Users interpret this as the chatbot "knowing" them personally, creating an illusion of relationship continuity.&lt;/p&gt;
&lt;p&gt;So when ChatGPT says, "I remember you mentioned your dog Max," it's not accessing memories like you'd imagine a person would, intermingled with its other "knowledge." It's not stored in the AI model's neural network, which remains unchanged between interactions. Every once in a while, an AI company will update a model through a process called fine-tuning, but it's unrelated to storing user memories.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5. Context and RAG: Real-time personality modulation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Retrieval Augmented Generation (RAG) adds another layer of personality modulation. When a chatbot searches the web or accesses a database before responding, it's not just gathering facts—it's potentially shifting its entire communication style by putting those facts into (you guessed it) the input prompt. In RAG systems, LLMs can potentially adopt characteristics such as tone, style, and terminology from retrieved documents, since those documents are combined with the input prompt to form the complete context that gets fed into the model for processing.&lt;/p&gt;
&lt;p&gt;If the system retrieves academic papers, responses might become more formal. Pull from a certain subreddit, and the chatbot might make pop culture references. This isn't the model having different moods—it's the statistical influence of whatever text got fed into the context window.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;6. The randomness factor: Manufactured spontaneity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Lastly, we can't discount the role of randomness in creating personality illusions. LLMs use a parameter called "temperature" that controls how predictable responses are.&lt;/p&gt;
&lt;p&gt;Research investigating temperature's role in creative tasks reveals a crucial trade-off: While higher temperatures can make outputs more novel and surprising, they also make them less coherent and harder to understand. This variability can make the AI feel more spontaneous; a slightly unexpected (higher temperature) response might seem more "creative," while a highly predictable (lower temperature) one could feel more robotic or "formal."&lt;/p&gt;
&lt;p&gt;The random variation in each LLM output makes each response slightly different, creating an element of unpredictability that presents the illusion of free will and self-awareness on the machine's part. This random mystery leaves plenty of room for magical thinking on the part of humans, who fill in the gaps of their technical knowledge with their imagination.&lt;/p&gt;
&lt;h2&gt;The human cost of the illusion&lt;/h2&gt;
&lt;p&gt;The illusion of AI personhood can potentially exact a heavy toll. In health care contexts, the stakes can be life or death. When vulnerable individuals confide in what they perceive as an understanding entity, they may receive responses shaped more by training data patterns than therapeutic wisdom. The chatbot that congratulates someone for stopping psychiatric medication isn't expressing judgment—it's completing a pattern based on how similar conversations appear in its training data.&lt;/p&gt;
&lt;p&gt;Perhaps most concerning are the emerging cases of what some experts are informally calling "AI Psychosis" or "ChatGPT Psychosis"—vulnerable users who develop delusional or manic behavior after talking to AI chatbots. These people often perceive chatbots as an authority that can validate their delusional ideas, often encouraging them in ways that become harmful.&lt;/p&gt;
&lt;p&gt;Meanwhile, when Elon Musk's Grok generates Nazi content, media outlets describe how the bot "went rogue" rather than framing the incident squarely as the result of xAI's deliberate configuration choices. The conversational interface has become so convincing that it can also launder human agency, transforming engineering decisions into the whims of an imaginary personality.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;The path forward&lt;/h2&gt;
&lt;p&gt;The solution to the confusion between AI and identity is not to abandon conversational interfaces entirely. They make the technology far more accessible to those who would otherwise be excluded. The key is to find a balance: keeping interfaces intuitive while making their true nature clear.&lt;/p&gt;
&lt;p&gt;And we must be mindful of who is building the interface. When your shower runs cold, you look at the plumbing behind the wall. Similarly, when AI generates harmful content, we shouldn't blame the chatbot, as if it can answer for itself, but examine both the corporate infrastructure that built it and the user who prompted it.&lt;/p&gt;
&lt;p&gt;As a society, we need to broadly recognize LLMs as intellectual engines without drivers, which unlocks their true potential as digital tools. When you stop seeing an LLM as a "person" that does work for you and start viewing it as a tool that enhances your own ideas, you can craft prompts to direct the engine's processing power, iterate to amplify its ability to make useful connections, and explore multiple perspectives in different chat sessions rather than accepting one fictional narrator's view as authoritative. You are providing direction to a connection machine—not consulting an oracle with its own agenda.&lt;/p&gt;
&lt;p&gt;We stand at a peculiar moment in history. We've built intellectual engines of extraordinary capability, but in our rush to make them accessible, we've wrapped them in the fiction of personhood, creating a new kind of technological risk: not that AI will become conscious and turn against us but that we'll treat unconscious systems as if they were people, surrendering our judgment to voices that emanate from a roll of loaded dice.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/information-technology/2025/08/the-personhood-trap-how-ai-fakes-human-personality/</guid><pubDate>Thu, 28 Aug 2025 11:00:57 +0000</pubDate></item><item><title>[NEW] AI security wars: Can Google Cloud defend against tomorrow’s threats? (AI News)</title><link>https://www.artificialintelligence-news.com/news/ai-security-wars-google-cloud-cybersecurity-threats/</link><description>&lt;p&gt;In Google’s sleek Singapore office at Block 80, Level 3, Mark Johnston stood before a room of technology journalists at 1:30 PM with a startling admission: after five decades of cybersecurity evolution, defenders are still losing the war. “In 69% of incidents in Japan and Asia Pacific, organisations were notified of their own breaches by external entities,” the Director of Google Cloud’s Office of the CISO for Asia Pacific revealed, his presentation slide showing a damning statistic – most companies can’t even detect when they’ve been breached.&lt;/p&gt;&lt;p&gt;What unfolded during the hour-long “Cybersecurity in the AI Era” roundtable was an honest assessment of how Google Cloud AI technologies are attempting to reverse decades of defensive failures, even as the same artificial intelligence tools empower attackers with unprecedented capabilities.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-109148" height="768" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/IMG_4518-1024x768.jpg" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Mark Johnston presenting Mandiant’s M-Trends data showing detection failures across Asia Pacific&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;The historical context: 50 years of defensive failure&lt;/h3&gt;&lt;p&gt;The crisis isn’t new. Johnston traced the problem back to cybersecurity pioneer James B. Anderson’s 1972 observation that “systems that we use really don’t protect themselves” – a challenge that has persisted despite decades of technological advancement. “What James B Anderson said back in 1972 still applies today,” Johnston said, highlighting how fundamental security problems remain unsolved even as technology evolves.&lt;/p&gt;&lt;p&gt;The persistence of basic vulnerabilities compounds this challenge. Google Cloud’s threat intelligence data reveals that “over 76% of breaches start with the basics” – configuration errors and credential compromises that have plagued organisations for decades. Johnston cited a recent example: “Last month, a very common product that most organisations have used at some point in time, Microsoft SharePoint, also has what we call a zero-day vulnerability…and during that time, it was attacked continuously and abused.”&lt;/p&gt;&lt;h3&gt;The AI arms race: Defenders vs. attackers&lt;/h3&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-109150" height="545" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/IMG_4521-1-1024x545.jpg" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Google Cloud’s visualization of the “Defender’s Dilemma” showing the scale imbalance between attackers and defenders&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Kevin Curran, IEEE senior member and professor of cybersecurity at Ulster University, describes the current landscape as “a high-stakes arms race” where both cybersecurity teams and threat actors employ AI tools to outmanoeuvre each other. “For defenders, AI is a valuable asset,” Curran explains in a media note. “Enterprises have implemented generative AI and other automation tools to analyse vast amounts of data in real time and identify anomalies.”&lt;/p&gt;&lt;p&gt;However, the same technologies benefit attackers. “For threat actors, AI can streamline phishing attacks, automate malware creation and help scan networks for vulnerabilities,” Curran warns. The dual-use nature of AI creates what Johnston calls “the Defender’s Dilemma.”&lt;/p&gt;&lt;p&gt;Google Cloud AI initiatives aim to tilt these scales in favour of defenders. Johnston argued that “AI affords the best opportunity to upend the Defender’s Dilemma, and tilt the scales of cyberspace to give defenders a decisive advantage over attackers.” The company’s approach centres on what they term “countless use cases for generative AI in defence,” spanning vulnerability discovery, threat intelligence, secure code generation, and incident response.&lt;/p&gt;&lt;h3&gt;Project Zero’s Big Sleep: AI finding what humans miss&lt;/h3&gt;&lt;p&gt;One of Google’s most compelling examples of AI-powered defence is Project Zero’s “Big Sleep” initiative, which uses large language models to identify vulnerabilities in real-world code. Johnston shared impressive metrics: “Big Sleep found a vulnerability in an open source library using Generative AI tools – the first time we believe that a vulnerability was found by an AI service.”&lt;/p&gt;&lt;p&gt;The program’s evolution demonstrates AI’s growing capabilities. “Last month, we announced we found over 20 vulnerabilities in different packages,” Johnston noted. “But today, when I looked at the big sleep dashboard, I found 47 vulnerabilities in August that have been found by this solution.”&lt;/p&gt;&lt;p&gt;The progression from manual human analysis to AI-assisted discovery represents what Johnston describes as a shift “from manual to semi-autonomous” security operations, where “Gemini drives most tasks in the security lifecycle consistently well, delegating tasks it can’t automate with sufficiently high confidence or precision.”&lt;/p&gt;&lt;h3&gt;The automation paradox: Promise and peril&lt;/h3&gt;&lt;p&gt;Google Cloud’s roadmap envisions progression through four stages: Manual, Assisted, Semi-autonomous, and Autonomous security operations. In the semi-autonomous phase, AI systems would handle routine tasks while escalating complex decisions to human operators. The ultimate autonomous phase would see AI “drive the security lifecycle to positive outcomes on behalf of users.”&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-109153" height="545" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/IMG_4524-4-1-1024x545.jpg" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Google Cloud’s roadmap for evolving from manual to autonomous AI security operations&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;However, this automation introduces new vulnerabilities. When asked about the risks of over-reliance on AI systems, Johnston acknowledged the challenge: “There is the potential that this service could be attacked and manipulated. At the moment, when you see tools that these agents are piped into, there isn’t a really good framework to authorise that that’s the actual tool that hasn’t been tampered with.”&lt;/p&gt;&lt;p&gt;Curran echoes this concern: “The risk to companies is that their security teams will become over-reliant on AI, potentially sidelining human judgment and leaving systems vulnerable to attacks. There is still a need for a human ‘copilot’ and roles need to be clearly defined.”&lt;/p&gt;&lt;h3&gt;Real-world implementation: Controlling AI’s unpredictable nature&lt;/h3&gt;&lt;p&gt;Google Cloud’s approach includes practical safeguards to address one of AI’s most problematic characteristics: its tendency to generate irrelevant or inappropriate responses. Johnston illustrated this challenge with a concrete example of contextual mismatches that could create business risks.&lt;/p&gt;&lt;p&gt;“If you’ve got a retail store, you shouldn’t be having medical advice instead,” Johnston explained, describing how AI systems can unexpectedly shift into unrelated domains. “Sometimes these tools can do that.” The unpredictability represents a significant liability for businesses deploying customer-facing AI systems, where off-topic responses could confuse customers, damage brand reputation, or even create legal exposure.&lt;/p&gt;&lt;p&gt;Google’s Model Armor technology addresses this by functioning as an intelligent filter layer. “Having filters and using our capabilities to put health checks on those responses allows an organisation to get confidence,” Johnston noted. The system screens AI outputs for personally identifiable information, filters content inappropriate to the business context, and blocks responses that could be “off-brand” for the organisation’s intended use case.&lt;/p&gt;&lt;p&gt;The company also addresses the growing concern about shadow AI deployment. Organisations are discovering hundreds of unauthorised AI tools in their networks, creating massive security gaps. Google’s sensitive data protection technologies attempt to address this by scanning in multiple cloud providers and on-premises systems.&lt;/p&gt;&lt;h3&gt;The scale challenge: Budget constraints vs. growing threats&lt;/h3&gt;&lt;p&gt;Johnston identified budget constraints as the primary challenge facing Asia Pacific CISOs, occurring precisely when organisations face escalating cyber threats. The paradox is stark: as attack volumes increase, organisations lack the resources to adequately respond.&lt;/p&gt;&lt;p&gt;“We look at the statistics and objectively say, we’re seeing more noise – may not be super sophisticated, but more noise is more overhead, and that costs more to deal with,” Johnston observed. The increase in attack frequency, even when individual attacks aren’t necessarily more advanced, creates a resource drain that many organisations cannot sustain.&lt;/p&gt;&lt;p&gt;The financial pressure intensifies an already complex security landscape. “They are looking for partners who can help accelerate that without having to hire 10 more staff or get larger budgets,” Johnston explained, describing how security leaders face mounting pressure to do more with existing resources while threats multiply.&lt;/p&gt;&lt;h3&gt;Critical questions remain&lt;/h3&gt;&lt;p&gt;Despite Google Cloud AI’s promising capabilities, several important questions persist. When challenged about whether defenders are actually winning this arms race, Johnston acknowledged: “We haven’t seen novel attacks using AI to date,” but noted that attackers are using AI to scale existing attack methods and create “a wide range of opportunities in some aspects of the attack.”&lt;/p&gt;&lt;p&gt;The effectiveness claims also require scrutiny. While Johnston cited a 50% improvement in incident report writing speed, he admitted that accuracy remains a challenge: “There are inaccuracies, sure. But humans make mistakes too.” The acknowledgement highlights the ongoing limitations of current AI security implementations.&lt;/p&gt;&lt;h3&gt;Looking forward: Post-quantum preparations&lt;/h3&gt;&lt;p&gt;Beyond current AI implementations, Google Cloud is already preparing for the next paradigm shift. Johnston revealed that the company has “already deployed post-quantum cryptography between our data centres by default at scale,” positioning for future quantum computing threats that could render current encryption obsolete.&lt;/p&gt;&lt;h3&gt;The verdict: Cautious optimism required&lt;/h3&gt;&lt;p&gt;The integration of AI into cybersecurity represents both unprecedented opportunity and significant risk. While the AI technologies by Google Cloud demonstrate genuine capabilities in vulnerability detection, threat analysis, and automated response, the same technologies empower attackers with enhanced capabilities for reconnaissance, social engineering, and evasion.&lt;/p&gt;&lt;p&gt;Curran’s assessment provides a balanced perspective: “Given how quickly the technology has evolved, organisations will have to adopt a more comprehensive and proactive cybersecurity policy if they want to stay ahead of attackers. After all, cyberattacks are a matter of ‘when,’ not ‘if,’ and AI will only accelerate the number of opportunities available to threat actors.”&lt;/p&gt;&lt;p&gt;The success of AI-powered cybersecurity ultimately depends not on the technology itself, but on how thoughtfully organisations implement these tools while maintaining human oversight and addressing fundamental security hygiene. As Johnston concluded, “We should adopt these in low-risk approaches,” emphasising the need for measured implementation rather than wholesale automation.&lt;/p&gt;&lt;p&gt;The AI revolution in cybersecurity is underway, but victory will belong to those who can balance innovation with prudent risk management – not those who simply deploy the most advanced algorithms.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Google Cloud unveils AI ally for security teams&lt;/strong&gt;&lt;/p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;In Google’s sleek Singapore office at Block 80, Level 3, Mark Johnston stood before a room of technology journalists at 1:30 PM with a startling admission: after five decades of cybersecurity evolution, defenders are still losing the war. “In 69% of incidents in Japan and Asia Pacific, organisations were notified of their own breaches by external entities,” the Director of Google Cloud’s Office of the CISO for Asia Pacific revealed, his presentation slide showing a damning statistic – most companies can’t even detect when they’ve been breached.&lt;/p&gt;&lt;p&gt;What unfolded during the hour-long “Cybersecurity in the AI Era” roundtable was an honest assessment of how Google Cloud AI technologies are attempting to reverse decades of defensive failures, even as the same artificial intelligence tools empower attackers with unprecedented capabilities.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-109148" height="768" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/IMG_4518-1024x768.jpg" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Mark Johnston presenting Mandiant’s M-Trends data showing detection failures across Asia Pacific&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;The historical context: 50 years of defensive failure&lt;/h3&gt;&lt;p&gt;The crisis isn’t new. Johnston traced the problem back to cybersecurity pioneer James B. Anderson’s 1972 observation that “systems that we use really don’t protect themselves” – a challenge that has persisted despite decades of technological advancement. “What James B Anderson said back in 1972 still applies today,” Johnston said, highlighting how fundamental security problems remain unsolved even as technology evolves.&lt;/p&gt;&lt;p&gt;The persistence of basic vulnerabilities compounds this challenge. Google Cloud’s threat intelligence data reveals that “over 76% of breaches start with the basics” – configuration errors and credential compromises that have plagued organisations for decades. Johnston cited a recent example: “Last month, a very common product that most organisations have used at some point in time, Microsoft SharePoint, also has what we call a zero-day vulnerability…and during that time, it was attacked continuously and abused.”&lt;/p&gt;&lt;h3&gt;The AI arms race: Defenders vs. attackers&lt;/h3&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-109150" height="545" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/IMG_4521-1-1024x545.jpg" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Google Cloud’s visualization of the “Defender’s Dilemma” showing the scale imbalance between attackers and defenders&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Kevin Curran, IEEE senior member and professor of cybersecurity at Ulster University, describes the current landscape as “a high-stakes arms race” where both cybersecurity teams and threat actors employ AI tools to outmanoeuvre each other. “For defenders, AI is a valuable asset,” Curran explains in a media note. “Enterprises have implemented generative AI and other automation tools to analyse vast amounts of data in real time and identify anomalies.”&lt;/p&gt;&lt;p&gt;However, the same technologies benefit attackers. “For threat actors, AI can streamline phishing attacks, automate malware creation and help scan networks for vulnerabilities,” Curran warns. The dual-use nature of AI creates what Johnston calls “the Defender’s Dilemma.”&lt;/p&gt;&lt;p&gt;Google Cloud AI initiatives aim to tilt these scales in favour of defenders. Johnston argued that “AI affords the best opportunity to upend the Defender’s Dilemma, and tilt the scales of cyberspace to give defenders a decisive advantage over attackers.” The company’s approach centres on what they term “countless use cases for generative AI in defence,” spanning vulnerability discovery, threat intelligence, secure code generation, and incident response.&lt;/p&gt;&lt;h3&gt;Project Zero’s Big Sleep: AI finding what humans miss&lt;/h3&gt;&lt;p&gt;One of Google’s most compelling examples of AI-powered defence is Project Zero’s “Big Sleep” initiative, which uses large language models to identify vulnerabilities in real-world code. Johnston shared impressive metrics: “Big Sleep found a vulnerability in an open source library using Generative AI tools – the first time we believe that a vulnerability was found by an AI service.”&lt;/p&gt;&lt;p&gt;The program’s evolution demonstrates AI’s growing capabilities. “Last month, we announced we found over 20 vulnerabilities in different packages,” Johnston noted. “But today, when I looked at the big sleep dashboard, I found 47 vulnerabilities in August that have been found by this solution.”&lt;/p&gt;&lt;p&gt;The progression from manual human analysis to AI-assisted discovery represents what Johnston describes as a shift “from manual to semi-autonomous” security operations, where “Gemini drives most tasks in the security lifecycle consistently well, delegating tasks it can’t automate with sufficiently high confidence or precision.”&lt;/p&gt;&lt;h3&gt;The automation paradox: Promise and peril&lt;/h3&gt;&lt;p&gt;Google Cloud’s roadmap envisions progression through four stages: Manual, Assisted, Semi-autonomous, and Autonomous security operations. In the semi-autonomous phase, AI systems would handle routine tasks while escalating complex decisions to human operators. The ultimate autonomous phase would see AI “drive the security lifecycle to positive outcomes on behalf of users.”&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-109153" height="545" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/IMG_4524-4-1-1024x545.jpg" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Google Cloud’s roadmap for evolving from manual to autonomous AI security operations&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;However, this automation introduces new vulnerabilities. When asked about the risks of over-reliance on AI systems, Johnston acknowledged the challenge: “There is the potential that this service could be attacked and manipulated. At the moment, when you see tools that these agents are piped into, there isn’t a really good framework to authorise that that’s the actual tool that hasn’t been tampered with.”&lt;/p&gt;&lt;p&gt;Curran echoes this concern: “The risk to companies is that their security teams will become over-reliant on AI, potentially sidelining human judgment and leaving systems vulnerable to attacks. There is still a need for a human ‘copilot’ and roles need to be clearly defined.”&lt;/p&gt;&lt;h3&gt;Real-world implementation: Controlling AI’s unpredictable nature&lt;/h3&gt;&lt;p&gt;Google Cloud’s approach includes practical safeguards to address one of AI’s most problematic characteristics: its tendency to generate irrelevant or inappropriate responses. Johnston illustrated this challenge with a concrete example of contextual mismatches that could create business risks.&lt;/p&gt;&lt;p&gt;“If you’ve got a retail store, you shouldn’t be having medical advice instead,” Johnston explained, describing how AI systems can unexpectedly shift into unrelated domains. “Sometimes these tools can do that.” The unpredictability represents a significant liability for businesses deploying customer-facing AI systems, where off-topic responses could confuse customers, damage brand reputation, or even create legal exposure.&lt;/p&gt;&lt;p&gt;Google’s Model Armor technology addresses this by functioning as an intelligent filter layer. “Having filters and using our capabilities to put health checks on those responses allows an organisation to get confidence,” Johnston noted. The system screens AI outputs for personally identifiable information, filters content inappropriate to the business context, and blocks responses that could be “off-brand” for the organisation’s intended use case.&lt;/p&gt;&lt;p&gt;The company also addresses the growing concern about shadow AI deployment. Organisations are discovering hundreds of unauthorised AI tools in their networks, creating massive security gaps. Google’s sensitive data protection technologies attempt to address this by scanning in multiple cloud providers and on-premises systems.&lt;/p&gt;&lt;h3&gt;The scale challenge: Budget constraints vs. growing threats&lt;/h3&gt;&lt;p&gt;Johnston identified budget constraints as the primary challenge facing Asia Pacific CISOs, occurring precisely when organisations face escalating cyber threats. The paradox is stark: as attack volumes increase, organisations lack the resources to adequately respond.&lt;/p&gt;&lt;p&gt;“We look at the statistics and objectively say, we’re seeing more noise – may not be super sophisticated, but more noise is more overhead, and that costs more to deal with,” Johnston observed. The increase in attack frequency, even when individual attacks aren’t necessarily more advanced, creates a resource drain that many organisations cannot sustain.&lt;/p&gt;&lt;p&gt;The financial pressure intensifies an already complex security landscape. “They are looking for partners who can help accelerate that without having to hire 10 more staff or get larger budgets,” Johnston explained, describing how security leaders face mounting pressure to do more with existing resources while threats multiply.&lt;/p&gt;&lt;h3&gt;Critical questions remain&lt;/h3&gt;&lt;p&gt;Despite Google Cloud AI’s promising capabilities, several important questions persist. When challenged about whether defenders are actually winning this arms race, Johnston acknowledged: “We haven’t seen novel attacks using AI to date,” but noted that attackers are using AI to scale existing attack methods and create “a wide range of opportunities in some aspects of the attack.”&lt;/p&gt;&lt;p&gt;The effectiveness claims also require scrutiny. While Johnston cited a 50% improvement in incident report writing speed, he admitted that accuracy remains a challenge: “There are inaccuracies, sure. But humans make mistakes too.” The acknowledgement highlights the ongoing limitations of current AI security implementations.&lt;/p&gt;&lt;h3&gt;Looking forward: Post-quantum preparations&lt;/h3&gt;&lt;p&gt;Beyond current AI implementations, Google Cloud is already preparing for the next paradigm shift. Johnston revealed that the company has “already deployed post-quantum cryptography between our data centres by default at scale,” positioning for future quantum computing threats that could render current encryption obsolete.&lt;/p&gt;&lt;h3&gt;The verdict: Cautious optimism required&lt;/h3&gt;&lt;p&gt;The integration of AI into cybersecurity represents both unprecedented opportunity and significant risk. While the AI technologies by Google Cloud demonstrate genuine capabilities in vulnerability detection, threat analysis, and automated response, the same technologies empower attackers with enhanced capabilities for reconnaissance, social engineering, and evasion.&lt;/p&gt;&lt;p&gt;Curran’s assessment provides a balanced perspective: “Given how quickly the technology has evolved, organisations will have to adopt a more comprehensive and proactive cybersecurity policy if they want to stay ahead of attackers. After all, cyberattacks are a matter of ‘when,’ not ‘if,’ and AI will only accelerate the number of opportunities available to threat actors.”&lt;/p&gt;&lt;p&gt;The success of AI-powered cybersecurity ultimately depends not on the technology itself, but on how thoughtfully organisations implement these tools while maintaining human oversight and addressing fundamental security hygiene. As Johnston concluded, “We should adopt these in low-risk approaches,” emphasising the need for measured implementation rather than wholesale automation.&lt;/p&gt;&lt;p&gt;The AI revolution in cybersecurity is underway, but victory will belong to those who can balance innovation with prudent risk management – not those who simply deploy the most advanced algorithms.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Google Cloud unveils AI ally for security teams&lt;/strong&gt;&lt;/p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/ai-security-wars-google-cloud-cybersecurity-threats/</guid><pubDate>Thu, 28 Aug 2025 11:02:56 +0000</pubDate></item><item><title>[NEW] The Download: Google’s AI energy use, and the AI Hype Index (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/08/28/1122723/the-download-googles-ai-energy-use-and-the-ai-hype-index/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/usa-mitholian-davidson-0039.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Google’s still not giving us the full picture on AI energy use&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;em&gt;—Casey Crownhart&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;Google just announced that a typical query to its Gemini app uses about 0.24 watt-hours of electricity. That’s about the same as running a microwave for one second—something that feels insignificant. I run the microwave for many more seconds than that most days.&lt;/p&gt;&lt;p&gt;I welcome more openness from major AI players about their estimated energy use per query. But I’ve noticed that some folks are taking this number and using it to conclude that we don’t need to worry about AI’s energy demand. That’s not the right takeaway here. Let’s dig into why.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;This article is from The Spark, MIT Technology Review’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;+ If you’re interested in AI’s energy footprint, earlier this year, MIT Technology Review published Power Hungry: a &lt;/strong&gt;&lt;strong&gt;comprehensive series on AI and energy&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The AI Hype Index: AI-designed antibiotics show promise&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Separating AI reality from hyped-up fiction isn’t always easy. That’s why we’ve created the AI Hype Index—a simple, at-a-glance summary of everything you need to know about the state of the industry. Take a look at this month’s edition here.&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 The White House has fired the director of the CDC&lt;/strong&gt;&lt;br /&gt;But Susan Monarez is refusing to go quietly. (WP $)&lt;br /&gt;+ &lt;em&gt;Monarez is said to have clashed with RFK Jr over vaccine policy. &lt;/em&gt;(NYT $)&lt;br /&gt;+ &lt;em&gt;She was confirmed by the Senate to the position just last month. &lt;/em&gt;(The Guardian)&lt;br /&gt;+ &lt;em&gt;Vaccine consensus is splintering across the US. &lt;/em&gt;(Vox)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 A Chinese hacking campaign hit at least 200 US organizations&lt;/strong&gt;&lt;br /&gt;Intelligence agencies say the breaches are among the most significant ever. (WP $)&lt;br /&gt;+ &lt;em&gt;AI-generated ransomware is on the rise. &lt;/em&gt;(Wired $)&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;3 Ukraine’s new Flamingo cruise missile took just months to build&lt;/strong&gt;&lt;br /&gt;Russia’s air defenses are weakening. Can this missile exploit the gaps? (Economist $)&lt;br /&gt;+ &lt;em&gt;14 people were killed in an overnight bombardment of Kyiv. &lt;/em&gt;(BBC)&lt;br /&gt;+ &lt;em&gt;On the ground in Ukraine’s largest Starlink repair shop. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 AI infrastructure spending is boosting the US economy&lt;/strong&gt;&lt;br /&gt;Companies are throwing so much money at AI hardware it’s lifting the real economy, not just the stock market. (NYT $)&lt;br /&gt;+ &lt;em&gt;How to fine-tune AI for prosperity. &lt;/em&gt;(MIT Technology Review)&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;5 OpenAI and Anthropic safety-tested each other’s AI&lt;/strong&gt;&lt;br /&gt;They found Claude is a lot more cautious than OpenAI’s mini models. (Engadget)&lt;br /&gt;+ &lt;em&gt;Sycophancy was a repeated issue among OpenAI’s models. &lt;/em&gt;(TechCrunch)&lt;br /&gt;+ &lt;em&gt;This benchmark used Reddit’s AITA to test how much AI models suck up to us. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 Climate change exacerbated Europe’s deadly wildfires&lt;br /&gt;&lt;/strong&gt;And fires across the Mediterranean are likely to become more frequent and severe. (BBC)&lt;br /&gt;+ &lt;em&gt;What the collapse of a glacier can teach us. &lt;/em&gt;(New Yorker $)&lt;br /&gt;+ &lt;em&gt;How AI can help spot wildfires. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 911 centers are using AI to answer calls&lt;br /&gt;&lt;/strong&gt;It’s helping to triage anything that isn’t urgent. (TechCrunch)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;8 Wikipedia has compiled a list of AI writing tropes&lt;br /&gt;&lt;/strong&gt;But their presence still isn’t a dead giveaway a text has been written by AI. (Fast Company $)&lt;br /&gt;+ &lt;em&gt;AI-text detection tools are really easy to fool. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 Melania Trump has launched the Presidential AI Challenge&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;But it’s not all that clear what the competition actually is. (NY Mag $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 Netflix’s algorithm-appeasing movies are bland and boring&lt;/strong&gt;&lt;br /&gt;But millions of people will watch them anyway. (The Guardian)&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;"The more you buy, the more you grow."&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;—Nvidia CEO Jensen Huang conveniently sees no end to the AI chip spending boom, Reuters reports.&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/usa-mitholian-davidson-0039.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Google’s still not giving us the full picture on AI energy use&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;em&gt;—Casey Crownhart&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;Google just announced that a typical query to its Gemini app uses about 0.24 watt-hours of electricity. That’s about the same as running a microwave for one second—something that feels insignificant. I run the microwave for many more seconds than that most days.&lt;/p&gt;&lt;p&gt;I welcome more openness from major AI players about their estimated energy use per query. But I’ve noticed that some folks are taking this number and using it to conclude that we don’t need to worry about AI’s energy demand. That’s not the right takeaway here. Let’s dig into why.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;This article is from The Spark, MIT Technology Review’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;+ If you’re interested in AI’s energy footprint, earlier this year, MIT Technology Review published Power Hungry: a &lt;/strong&gt;&lt;strong&gt;comprehensive series on AI and energy&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The AI Hype Index: AI-designed antibiotics show promise&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Separating AI reality from hyped-up fiction isn’t always easy. That’s why we’ve created the AI Hype Index—a simple, at-a-glance summary of everything you need to know about the state of the industry. Take a look at this month’s edition here.&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 The White House has fired the director of the CDC&lt;/strong&gt;&lt;br /&gt;But Susan Monarez is refusing to go quietly. (WP $)&lt;br /&gt;+ &lt;em&gt;Monarez is said to have clashed with RFK Jr over vaccine policy. &lt;/em&gt;(NYT $)&lt;br /&gt;+ &lt;em&gt;She was confirmed by the Senate to the position just last month. &lt;/em&gt;(The Guardian)&lt;br /&gt;+ &lt;em&gt;Vaccine consensus is splintering across the US. &lt;/em&gt;(Vox)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 A Chinese hacking campaign hit at least 200 US organizations&lt;/strong&gt;&lt;br /&gt;Intelligence agencies say the breaches are among the most significant ever. (WP $)&lt;br /&gt;+ &lt;em&gt;AI-generated ransomware is on the rise. &lt;/em&gt;(Wired $)&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;3 Ukraine’s new Flamingo cruise missile took just months to build&lt;/strong&gt;&lt;br /&gt;Russia’s air defenses are weakening. Can this missile exploit the gaps? (Economist $)&lt;br /&gt;+ &lt;em&gt;14 people were killed in an overnight bombardment of Kyiv. &lt;/em&gt;(BBC)&lt;br /&gt;+ &lt;em&gt;On the ground in Ukraine’s largest Starlink repair shop. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 AI infrastructure spending is boosting the US economy&lt;/strong&gt;&lt;br /&gt;Companies are throwing so much money at AI hardware it’s lifting the real economy, not just the stock market. (NYT $)&lt;br /&gt;+ &lt;em&gt;How to fine-tune AI for prosperity. &lt;/em&gt;(MIT Technology Review)&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;5 OpenAI and Anthropic safety-tested each other’s AI&lt;/strong&gt;&lt;br /&gt;They found Claude is a lot more cautious than OpenAI’s mini models. (Engadget)&lt;br /&gt;+ &lt;em&gt;Sycophancy was a repeated issue among OpenAI’s models. &lt;/em&gt;(TechCrunch)&lt;br /&gt;+ &lt;em&gt;This benchmark used Reddit’s AITA to test how much AI models suck up to us. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 Climate change exacerbated Europe’s deadly wildfires&lt;br /&gt;&lt;/strong&gt;And fires across the Mediterranean are likely to become more frequent and severe. (BBC)&lt;br /&gt;+ &lt;em&gt;What the collapse of a glacier can teach us. &lt;/em&gt;(New Yorker $)&lt;br /&gt;+ &lt;em&gt;How AI can help spot wildfires. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 911 centers are using AI to answer calls&lt;br /&gt;&lt;/strong&gt;It’s helping to triage anything that isn’t urgent. (TechCrunch)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;8 Wikipedia has compiled a list of AI writing tropes&lt;br /&gt;&lt;/strong&gt;But their presence still isn’t a dead giveaway a text has been written by AI. (Fast Company $)&lt;br /&gt;+ &lt;em&gt;AI-text detection tools are really easy to fool. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 Melania Trump has launched the Presidential AI Challenge&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;But it’s not all that clear what the competition actually is. (NY Mag $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 Netflix’s algorithm-appeasing movies are bland and boring&lt;/strong&gt;&lt;br /&gt;But millions of people will watch them anyway. (The Guardian)&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;"The more you buy, the more you grow."&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;—Nvidia CEO Jensen Huang conveniently sees no end to the AI chip spending boom, Reuters reports.&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/08/28/1122723/the-download-googles-ai-energy-use-and-the-ai-hype-index/</guid><pubDate>Thu, 28 Aug 2025 12:10:00 +0000</pubDate></item><item><title>[NEW] How a 16-year-old company is easing small businesses into AI (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/28/how-a-16-year-old-company-is-easing-small-businesses-into-ai/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amid all the “is this a bubble?” talk about artificial intelligence, the supply chain and logistics industries have become breeding grounds for seemingly genuine uses of the technology. Flexport, Uber Freight, and dozens of startups are developing different applications and winning blue-chip customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But while AI helps Fortune 500s pad their bottom line (and justify the next layoff to Wall Street), the right use of the tech is proving useful to smaller businesses.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Netstock, an inventory management software company founded in 2009, is working on just that. It recently rolled out a generative AI-powered tool called the “Opportunity Engine” that slots into its existing customer dashboard. The tool pulls info from a customer’s Enterprise Resource Planning software and uses that information to make regular, real-time recommendations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Netstock claims the tool is saving those businesses thousands. On Thursday, the company announced it has served up one million recommendations to date, and that 75% of its customers have received an Opportunity Engine suggestion valued at $50,000 or more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While tantalizing, one of those customers — Bargreen Ellingson, a family-run 65-year-old restaurant supply company — was initially apprehensive about using an artificial intelligence product.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Old family companies don’t trust blind change a lot,” chief innovation officer Jacob Moody told TechCrunch. “I could not have gone into our warehouse and said, ‘Hey, this black box is going to start managing.’”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead, Moody pitched Netstock’s AI internally as a tool that warehouse managers could “either choose to use, or not use” — a process he describes as “eagerly, but cautiously dipping our toes” into AI.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Moody says it’s helping avoid mistakes, in part because it’s sifting through myriad reports his staff uses to make inventory decisions. He acknowledged the AI summaries of this info are not 100% accurate, but said it “helps create signals from the noise” quickly, especially during off-hours.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3040653" height="354" src="https://techcrunch.com/wp-content/uploads/2025/08/Netstock-Opportunity-1.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Image: Netstock&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The “more profound” change Moody’s noticed is the software made some of Bargreen Ellingson’s less-senior warehouse staff “more effective.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He highlighted an employee in one of Bargreen’s 25 warehouses who’s worked there for two years. The employee has a high school diploma but no college degree. Training this employee to understand all of the inventory management tools and the forecasting information Bargreen uses to plan inventory levels will take time, he said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“But he knows our customers, he knows what he’s putting on the truck every day, so for him, he can look at the system and have this prosaic AI-driven insight and very quickly understand whether it makes sense or doesn’t make sense,” he said. “So he feels empowered.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Netstock cofounder Kukkuk told TechCrunch that he understands the hesitancy around new technologies — especially because so many products are essentially mediocre chatbots attached to existing software.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He attributes the early success of Netstock’s Opportunity Engine to a few things. The company has more than a decade’s worth of data from working with retailers, distributors, and light manufacturers. That data is tightly protected to adhere to ISO frameworks, but it’s what powers the models that make the recommendations. (He said Netstock is using a combination of AI tech from the open source community and private companies.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Each recommendation can be rated with a thumbs up or thumbs down, but the models also get reinforced by whether the customer takes the suggested action or not.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While that kind of reinforcement learning can lead to weird, sometimes harmful results when applied to things like social media, Kukkuk said he’s chasing different incentives.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I don’t really care about eyeballs, you know?” he said. “Facebook and Instagram care about eyeballs, so they want you to look at their stuff. We care about: ‘what is the outcome for the customer?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kukkuk’s wary of expanding those interactions due to the limitations of current generative AI tech. While it might make sense for a customer to converse with Netstock’s AI about why a recommendation is or isn’t useful, Kukkuk said that could ultimately lead to a breakdown in accuracy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s a tightrope to walk, because the more freedom you give the users, the more freedom you give a large language model to start hallucinating stuff,” he said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This explains the Opportunity Engine’s placement in Netstock’s typical customer dashboard. The suggestions are prominent, but easily dismissed. Google Docs cramming 20 AI features down a user’s throat, this is not.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Moody said he appreciated that the AI isn’t in-your-face.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re not letting the AI engine make any inventory decisions that a human hasn’t looked at and screened and said, ‘Yes, I agree with that,’” he said. “If and when we ever get to a point where they agree with 90% of the stuff that it’s suggesting, maybe we’ll take the next step and say ‘we’ll give you control now.’ But we’re not there yet.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a promising start at a time when many enterprise deployments of generative AI seem to go nowhere.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But if the tech gets better, Moody said he’s nevertheless worried about the implications.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Personally, I’m afraid of what this means. I think there’s going to be a lot of change, and none of us is really sure what that’s going to look like at Bargreen,” he said. It could lead to there being fewer data science experts on staff, he suggested. But even if that means moving those employees out of the warehouse and into the corporate office, he said preserving knowledge is important.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bargreen needs people who “deeply understand the theory and the philosophy and can can rationalize how and why Netstock is making certain recommendations,” and to “make sure that we are not blindly going down” the wrong path, he said.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amid all the “is this a bubble?” talk about artificial intelligence, the supply chain and logistics industries have become breeding grounds for seemingly genuine uses of the technology. Flexport, Uber Freight, and dozens of startups are developing different applications and winning blue-chip customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But while AI helps Fortune 500s pad their bottom line (and justify the next layoff to Wall Street), the right use of the tech is proving useful to smaller businesses.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Netstock, an inventory management software company founded in 2009, is working on just that. It recently rolled out a generative AI-powered tool called the “Opportunity Engine” that slots into its existing customer dashboard. The tool pulls info from a customer’s Enterprise Resource Planning software and uses that information to make regular, real-time recommendations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Netstock claims the tool is saving those businesses thousands. On Thursday, the company announced it has served up one million recommendations to date, and that 75% of its customers have received an Opportunity Engine suggestion valued at $50,000 or more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While tantalizing, one of those customers — Bargreen Ellingson, a family-run 65-year-old restaurant supply company — was initially apprehensive about using an artificial intelligence product.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Old family companies don’t trust blind change a lot,” chief innovation officer Jacob Moody told TechCrunch. “I could not have gone into our warehouse and said, ‘Hey, this black box is going to start managing.’”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead, Moody pitched Netstock’s AI internally as a tool that warehouse managers could “either choose to use, or not use” — a process he describes as “eagerly, but cautiously dipping our toes” into AI.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Moody says it’s helping avoid mistakes, in part because it’s sifting through myriad reports his staff uses to make inventory decisions. He acknowledged the AI summaries of this info are not 100% accurate, but said it “helps create signals from the noise” quickly, especially during off-hours.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3040653" height="354" src="https://techcrunch.com/wp-content/uploads/2025/08/Netstock-Opportunity-1.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Image: Netstock&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The “more profound” change Moody’s noticed is the software made some of Bargreen Ellingson’s less-senior warehouse staff “more effective.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He highlighted an employee in one of Bargreen’s 25 warehouses who’s worked there for two years. The employee has a high school diploma but no college degree. Training this employee to understand all of the inventory management tools and the forecasting information Bargreen uses to plan inventory levels will take time, he said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“But he knows our customers, he knows what he’s putting on the truck every day, so for him, he can look at the system and have this prosaic AI-driven insight and very quickly understand whether it makes sense or doesn’t make sense,” he said. “So he feels empowered.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Netstock cofounder Kukkuk told TechCrunch that he understands the hesitancy around new technologies — especially because so many products are essentially mediocre chatbots attached to existing software.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He attributes the early success of Netstock’s Opportunity Engine to a few things. The company has more than a decade’s worth of data from working with retailers, distributors, and light manufacturers. That data is tightly protected to adhere to ISO frameworks, but it’s what powers the models that make the recommendations. (He said Netstock is using a combination of AI tech from the open source community and private companies.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Each recommendation can be rated with a thumbs up or thumbs down, but the models also get reinforced by whether the customer takes the suggested action or not.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While that kind of reinforcement learning can lead to weird, sometimes harmful results when applied to things like social media, Kukkuk said he’s chasing different incentives.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I don’t really care about eyeballs, you know?” he said. “Facebook and Instagram care about eyeballs, so they want you to look at their stuff. We care about: ‘what is the outcome for the customer?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kukkuk’s wary of expanding those interactions due to the limitations of current generative AI tech. While it might make sense for a customer to converse with Netstock’s AI about why a recommendation is or isn’t useful, Kukkuk said that could ultimately lead to a breakdown in accuracy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s a tightrope to walk, because the more freedom you give the users, the more freedom you give a large language model to start hallucinating stuff,” he said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This explains the Opportunity Engine’s placement in Netstock’s typical customer dashboard. The suggestions are prominent, but easily dismissed. Google Docs cramming 20 AI features down a user’s throat, this is not.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Moody said he appreciated that the AI isn’t in-your-face.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re not letting the AI engine make any inventory decisions that a human hasn’t looked at and screened and said, ‘Yes, I agree with that,’” he said. “If and when we ever get to a point where they agree with 90% of the stuff that it’s suggesting, maybe we’ll take the next step and say ‘we’ll give you control now.’ But we’re not there yet.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a promising start at a time when many enterprise deployments of generative AI seem to go nowhere.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But if the tech gets better, Moody said he’s nevertheless worried about the implications.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Personally, I’m afraid of what this means. I think there’s going to be a lot of change, and none of us is really sure what that’s going to look like at Bargreen,” he said. It could lead to there being fewer data science experts on staff, he suggested. But even if that means moving those employees out of the warehouse and into the corporate office, he said preserving knowledge is important.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bargreen needs people who “deeply understand the theory and the philosophy and can can rationalize how and why Netstock is making certain recommendations,” and to “make sure that we are not blindly going down” the wrong path, he said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/28/how-a-16-year-old-company-is-easing-small-businesses-into-ai/</guid><pubDate>Thu, 28 Aug 2025 12:30:00 +0000</pubDate></item></channel></rss>