<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 05 Jan 2026 18:37:09 +0000</lastBuildDate><item><title> ()</title><link>https://venturebeat.com/category/ai/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://venturebeat.com/category/ai/feed/</guid></item><item><title>[NEW] Introducing Falcon-H1-Arabic: Pushing the Boundaries of Arabic Language AI with Hybrid Architecture (Hugging Face - Blog)</title><link>https://huggingface.co/blog/tiiuae/falcon-h1-arabic</link><description>&lt;!-- HTML_TAG_START --&gt;
&lt;img alt="image" src="https://cdn-uploads.huggingface.co/production/uploads/659bc8a7b0f43ed69f0b2300/2RCOpXpJ-guqSSBJV5IJh.png" /&gt;
&lt;blockquote&gt;
&lt;p&gt;Discover more in our official blogpost, featuring an interactive experience&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The journey of building world-class Arabic language models has been one of continuous learning and iteration. Today, we're excited to announce &lt;strong&gt;Falcon-H1-Arabic&lt;/strong&gt;, our most advanced Arabic language model family to date, representing a significant leap forward in both architecture and capabilities. This release embodies months of research, community feedback, and technical innovation, culminating in &lt;strong&gt;three&lt;/strong&gt; powerful models that set new standards for Arabic natural language processing.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Building on Success: The Evolution from Falcon-Arabic
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;When we launched Falcon-Arabic a few months ago, the response from the community was both humbling and enlightening. Developers, researchers and students across the Arab world used the model for real use cases, pushing them to its limits and providing invaluable feedback. We learned where the model excelled and, more importantly, where it struggled. Long-context understanding, dialectal variations, mathematical reasoning, and domain-specific knowledge emerged as key areas requiring deeper attention.&lt;/p&gt;
&lt;p&gt;We didn't just want to make incremental improvements, we wanted to fundamentally rethink our approach. The result is Falcon-H1-Arabic, a model family that addresses every piece of feedback we received while introducing architectural innovations that were previously unexplored in Arabic language modeling.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src="https://cdn-uploads.huggingface.co/production/uploads/659bc8a7b0f43ed69f0b2300/9e6qVGEM4-1mdK3dCJEjs.png" /&gt;&lt;br /&gt;&lt;em&gt;Falcon-H1-Arabic 3B, 7B, 34B models outperforming all SOTA models of similar sizes and sometimes bigger.&lt;/em&gt;
&lt;/p&gt;


&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		A First for Arabic NLP: Hybrid Mamba-Transformer Architecture
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Falcon-H1-Arabic is built on the &lt;strong&gt;Falcon-H1&lt;/strong&gt; hybrid architecture, which integrates State Space Models (Mamba) and Transformer attention within every block. Both components run in parallel and their representations are fused before the block’s output projection. This design provides the linear-time scalability of Mamba for extremely long sequences while preserving the precise long-range modeling capabilities of attention. For Arabic, with its rich morphology and flexible sentence structures, this approach significantly improves coherence and reasoning across extended text. We've deployed this architecture across three scales (3B, 7B, 34B parameters), each balancing capacity, efficiency, and deployability for different use cases from edge devices to enterprise applications.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src="https://cdn-uploads.huggingface.co/production/uploads/659bc8a7b0f43ed69f0b2300/y1PdfPUZABgyA4q6J4-6r.png" width="500" /&gt;&lt;br /&gt;&lt;em&gt;Falcon-H1 architecture. Attention and SSM run in parallel within each block; their outputs are concatenated before the block’s output projection. The number of SSM/Attention heads depends on the model size. More details on the Falcon-H1 technical report.&lt;/em&gt;
&lt;/p&gt;

&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Breaking Context Boundaries
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We've dramatically increased context capabilities from Falcon-Arabic's 32K limit to 128K tokens for the 3B model and 256K tokens for both the 7B and 34B models. At 256K tokens (~200,000 words), these models can process several novels or hundreds of pages of technical documentation enabling applications in legal analysis, medical records, academic research, and extended conversations that were previously impractical. Our post-training specifically addresses "lost in the middle" challenges to ensure models effectively utilize their full context range, not just accept long inputs.&lt;/p&gt;
&lt;div align="center"&gt;
&lt;table width="90%"&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;th&gt;Parameters&lt;/th&gt;
    &lt;th&gt;Context Window&lt;/th&gt;
    &lt;th&gt;Architecture&lt;/th&gt;
    &lt;th&gt;Ideal Uses&lt;/th&gt;
  &lt;/tr&gt;

  &lt;tr&gt;
    &lt;td&gt;3B&lt;/td&gt;
    &lt;td&gt;128K&lt;/td&gt;
    &lt;td&gt;Hybrid&lt;/td&gt;
    &lt;td&gt;Fast agents, high-QPS systems, lightweight analytics&lt;/td&gt;
  &lt;/tr&gt;

  &lt;tr&gt;
    &lt;td&gt;7B&lt;/td&gt;
    &lt;td&gt;256K&lt;/td&gt;
    &lt;td&gt;Hybrid&lt;/td&gt;
    &lt;td&gt;Production assistants, reasoning, enterprise chat&lt;/td&gt;
  &lt;/tr&gt;

  &lt;tr&gt;
    &lt;td&gt;34B&lt;/td&gt;
    &lt;td&gt;256K&lt;/td&gt;
    &lt;td&gt;Hybrid&lt;/td&gt;
    &lt;td&gt;Long-document analysis, research, high-stakes tasks&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Data Quality and Diversity: The Foundation of Excellence
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We rebuilt our pre-training data pipeline from the ground up to better reflect the complexity of Arabic. This began with a multi-stage quality filtering process tailored to Arabic orthography, morphology, diacritics, and syntactic patterns. Instead of heuristic filtering, we used deep linguistic analysis to isolate coherent, well-structured text and remove noise commonly found in open-web corpora. The result is a significantly cleaner, more stylistically consistent Arabic dataset.&lt;/p&gt;
&lt;p&gt;Dialect coverage was another key priority. Arabic is not monolithic; Modern Standard Arabic coexists with dialects such as Egyptian, Levantine, Gulf, and Maghrebi, each with distinct vocabularies and grammatical constructions. We expanded dialectal sources substantially so the models would understand and generate the full spectrum of real-world Arabic rather than leaning disproportionately toward formal MSA. To maintain global reasoning and domain diversity, we also preserved the multilingual capabilities of Falcon-H1 by training the Arabic models on an almost equal mix of Arabic, English, and multilingual content totalling around 300 Billion Tokens. This ensures strong performance in code, STEM, and cross-lingual reasoning. The following figure illustrates the distribution of the pre-training data across languages and categories. All values are expressed in billions of tokens.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src="https://cdn-uploads.huggingface.co/production/uploads/659bc8a7b0f43ed69f0b2300/NrIio6sopNNC6wX1mWKzA.png" /&gt;&lt;br /&gt;&lt;/p&gt;


&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Post-Training: Refining Capabilities Without Compromising Competence
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;After pre-training, Falcon-H1-Arabic undergoes a focused post-training pipeline consisting of supervised fine-tuning (SFT) followed by direct preference optimization (DPO). During SFT, we expose the models to high-quality Arabic instructions, curated long-context examples, and structured reasoning tasks that teach them to follow directives, maintain coherence over extended sequences, and ground their responses in relevant information. This stage is crucial for ensuring that the models can actually use their large context windows which does not emerge automatically from architecture alone.&lt;/p&gt;
&lt;p&gt;We follow SFT with a targeted DPO phase to refine alignment, conversational quality, and preference consistency. DPO helps the models balance long-context reasoning with general linguistic competence, improving helpfulness and reducing common failure modes such as drifting, overuse of context, or neglecting earlier information. Throughout both stages, we carefully monitor for catastrophic forgetting and maintain a controlled curriculum so gains in long-context behavior do not come at the expense of core reasoning or factual accuracy. The result is a family of models that handles extended documents and dialogue with ease while preserving strong performance on everyday language tasks.&lt;/p&gt;
&lt;p&gt;Beyond benchmark-oriented optimization, our post-training process deliberately strengthens areas that traditional evaluations do not fully capture, including conversational faithfulness, rhetorical organization, structured follow-ups, and discourse coherence. These enhancements significantly boost the model’s practical usefulness, making Falcon-H1-Arabic more dependable in real multi-turn dialogue, instruction execution, and long-context conversational flows.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Benchmark Performance: Setting New Standards
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Numbers tell an important part of the story. On the Open Arabic LLM Leaderboard (OALL), a comprehensive benchmark evaluating Arabic language understanding across diverse tasks, Falcon-H1-Arabic achieves state-of-the-art results at every scale we tested. Note that our scores may vary slightly from those reported on the leaderboard, as we used vLLM as the backend instead of the leaderboard’s Accelerate-based implementation. These differences are typically under one point while offering significantly faster runtime.&lt;/p&gt;
&lt;p&gt;Beyond OALL, we also report results on the 3LM benchmark for STEM-related tasks on both synthetic and native splits; Arabculture for Arabic culture assessment; and AraDice for Arabic dialect coverage across Levantine, and Egyptian varieties as well as Arabic culture across 6 countries. The reported AraDice score is the average of all the 3 scores. &lt;/p&gt;
&lt;p&gt;&lt;img alt="image" src="https://cdn-uploads.huggingface.co/production/uploads/659bc8a7b0f43ed69f0b2300/asF0IrbLZmK9NXqtMzWuS.png" /&gt;
&lt;img alt="image" src="https://cdn-uploads.huggingface.co/production/uploads/659bc8a7b0f43ed69f0b2300/rhCbKcSsOuorDnjj9j9CA.png" /&gt;&lt;/p&gt;
&lt;p&gt;Starting with the 3B model, the performance is exceptional. It reaches approximately 62% on OALL, outperforming all small-scale models, including Gemma-4B, Qwen3-4B, and Phi-4-mini by roughly ten points. On 3LM, the main Arabic STEM benchmark, it scores around 82% on the native split and 73% on the synthetic split. It also achieves about 62% on the ArabCulture benchmark and around 50% across AraDice dialect evaluation (Egyptian, Gulf, and Levantine). This makes Falcon-H1-Arabic-3B a high-quality, highly efficient model suitable for edge deployments, real-time applications, and agentic systems where latency and cost matter.&lt;/p&gt;
&lt;p&gt;&lt;img alt="image" src="https://cdn-uploads.huggingface.co/production/uploads/659bc8a7b0f43ed69f0b2300/uXwwaWeYBFsd059bjIzOq.png" /&gt;
&lt;img alt="image" src="https://cdn-uploads.huggingface.co/production/uploads/659bc8a7b0f43ed69f0b2300/3FwuLZLg_BIjdLofkTryp.png" /&gt;&lt;/p&gt;
&lt;p&gt;The 7B model continues this upward trajectory. With a score of 71.7% on OALL, it surpasses all models in the ~10B class, including Fanar-9B, Allam-7B*, and Qwen3-8B. On 3LM, it reaches about 92% on the native split and 85% on the synthetic one. AraDice scores rise into the mid-50s across all dialects, and ArabCulture results approach 80%. This model strikes an ideal balance between capability and deployability, making it the most practical choice for general-purpose Arabic NLP in production environments.&lt;/p&gt;
&lt;p&gt;The 34B model represents our flagship system and establishes a new state of the art for Arabic language modeling. It reaches approximately 75% on OALL, outperforming not only models of similar size but even much larger systems such as Llama-3.3-70B and AceGPT2-32B. Its 3LM scores reach about 96% on the native split and 94% on the synthetic one. On ArabCulture it scores close to 80%, and on AraDice it reaches around 53 across dialects. The fact that a 34B hybrid model surpasses the performance of 70B-scale transformers demonstrates the effectiveness of the Falcon-H1 architecture, the quality of the data, and the strength of the post-training pipeline.&lt;/p&gt;
&lt;p&gt;&lt;img alt="image" src="https://cdn-uploads.huggingface.co/production/uploads/659bc8a7b0f43ed69f0b2300/X7nRh62dCHZxbTiTOwNKA.png" /&gt;
&lt;img alt="image" src="https://cdn-uploads.huggingface.co/production/uploads/659bc8a7b0f43ed69f0b2300/kEO723PeOMntQoS0cn5Qw.png" /&gt;&lt;/p&gt;
&lt;p&gt;These benchmark results validate our approach but also highlight an important reality: the frontier of Arabic language modeling is advancing rapidly. Each percentage point on these benchmarks represents countless hours of engineering effort, careful dataset curation, and architectural refinement. The margins by which Falcon-H1-Arabic leads aren't just statistical artifacts, they translate to meaningfully better user experiences in real-world applications.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Practical Applications: From Edge to Enterprise
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Each model in the Falcon-H1-Arabic family is suited to different deployment scenarios. The 3B model is optimized for speed, cost-efficiency, and high-throughput systems, making it ideal for agentic workflows, on-device applications, low-latency chat, and environments with strict resource constraints. The 7B model serves as the general-purpose workhorse for most production applications, powering document understanding systems, chatbots, summarization pipelines, and content generation tools. The 34B model is designed for high-stakes domains where accuracy and long-range reasoning matter most, including legal analysis, medical summarization, academic research, and large-scale enterprise automation. Its extended context window makes it uniquely capable of analyzing hundreds of pages of text in a single pass while maintaining precise coherence.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Responsible AI and Limitations
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Like all language models, Falcon-H1-Arabic may reflect biases from training data and can produce hallucinated information. Model outputs should not be used as sole authorities for medical, legal, or financial decisions without professional verification. Long-context performance may degrade at extreme ranges. We recommend task-specific evaluation and appropriate guardrails before deployment in production or sensitive applications.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Acknowledgments
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;This work stands on the shoulders of many. We extend our gratitude to the Arabic NLP research community, whose open sharing of benchmarks, datasets, and methodologies enables progress across the field. Special thanks to our colleagues at TII: Ilyas Chahed, Younes Belkada, Dhia Eddine Rhaiem, Puneesh Khanna, Jingwei Zuo, Mikhail Lubinets, Slim Frikha, Maksim Velikanov, Kacper Piskorski, and Suhail Mohmad for their invaluable support during this project.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Citation
	&lt;/span&gt;
&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{Falcon-H1-Arabic-2025,
  title={Falcon-H1-Arabic: State-of-the-Art Arabic Language Models with Hybrid Mamba-Transformer Architecture},
  author={Basma El Amel Boussaha and Mohammed Alyafeai and Ahmed Alzubaidi and Leen AlQadi and Shaikha Alsuwaidi and Omar Alkaabi and Hamza Alobeidli and Hakim Hacid},
  url={https://huggingface.co/blog/tiiuae/falcon-h1-arabic},
  month={December},
  year={2025},
  note={Available in 3B, 7B, and 34B parameter versions}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;NB: the scores of ALLaM-7B-Instruct-preview in our evaluation are higher than those reported on the OALL leaderboard, as we used the newest release (7b-alpha-v2.33.0.30), while the leaderboard currently reflects results from the older version (7b-alpha-v1.27.2.25).&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;div align="center"&gt;

  &lt;p&gt;
    Falcon-H1-Arabic models are available for use at the links below. For questions, collaborations, or feedback, reach us at &lt;strong&gt;falcon.info@tii.ae&lt;/strong&gt; or join our community:
  &lt;/p&gt;

  

&lt;/div&gt;






&lt;hr /&gt;
&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;!-- HTML_TAG_START --&gt;
&lt;img alt="image" src="https://cdn-uploads.huggingface.co/production/uploads/659bc8a7b0f43ed69f0b2300/2RCOpXpJ-guqSSBJV5IJh.png" /&gt;
&lt;blockquote&gt;
&lt;p&gt;Discover more in our official blogpost, featuring an interactive experience&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The journey of building world-class Arabic language models has been one of continuous learning and iteration. Today, we're excited to announce &lt;strong&gt;Falcon-H1-Arabic&lt;/strong&gt;, our most advanced Arabic language model family to date, representing a significant leap forward in both architecture and capabilities. This release embodies months of research, community feedback, and technical innovation, culminating in &lt;strong&gt;three&lt;/strong&gt; powerful models that set new standards for Arabic natural language processing.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Building on Success: The Evolution from Falcon-Arabic
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;When we launched Falcon-Arabic a few months ago, the response from the community was both humbling and enlightening. Developers, researchers and students across the Arab world used the model for real use cases, pushing them to its limits and providing invaluable feedback. We learned where the model excelled and, more importantly, where it struggled. Long-context understanding, dialectal variations, mathematical reasoning, and domain-specific knowledge emerged as key areas requiring deeper attention.&lt;/p&gt;
&lt;p&gt;We didn't just want to make incremental improvements, we wanted to fundamentally rethink our approach. The result is Falcon-H1-Arabic, a model family that addresses every piece of feedback we received while introducing architectural innovations that were previously unexplored in Arabic language modeling.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src="https://cdn-uploads.huggingface.co/production/uploads/659bc8a7b0f43ed69f0b2300/9e6qVGEM4-1mdK3dCJEjs.png" /&gt;&lt;br /&gt;&lt;em&gt;Falcon-H1-Arabic 3B, 7B, 34B models outperforming all SOTA models of similar sizes and sometimes bigger.&lt;/em&gt;
&lt;/p&gt;


&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		A First for Arabic NLP: Hybrid Mamba-Transformer Architecture
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Falcon-H1-Arabic is built on the &lt;strong&gt;Falcon-H1&lt;/strong&gt; hybrid architecture, which integrates State Space Models (Mamba) and Transformer attention within every block. Both components run in parallel and their representations are fused before the block’s output projection. This design provides the linear-time scalability of Mamba for extremely long sequences while preserving the precise long-range modeling capabilities of attention. For Arabic, with its rich morphology and flexible sentence structures, this approach significantly improves coherence and reasoning across extended text. We've deployed this architecture across three scales (3B, 7B, 34B parameters), each balancing capacity, efficiency, and deployability for different use cases from edge devices to enterprise applications.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src="https://cdn-uploads.huggingface.co/production/uploads/659bc8a7b0f43ed69f0b2300/y1PdfPUZABgyA4q6J4-6r.png" width="500" /&gt;&lt;br /&gt;&lt;em&gt;Falcon-H1 architecture. Attention and SSM run in parallel within each block; their outputs are concatenated before the block’s output projection. The number of SSM/Attention heads depends on the model size. More details on the Falcon-H1 technical report.&lt;/em&gt;
&lt;/p&gt;

&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Breaking Context Boundaries
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We've dramatically increased context capabilities from Falcon-Arabic's 32K limit to 128K tokens for the 3B model and 256K tokens for both the 7B and 34B models. At 256K tokens (~200,000 words), these models can process several novels or hundreds of pages of technical documentation enabling applications in legal analysis, medical records, academic research, and extended conversations that were previously impractical. Our post-training specifically addresses "lost in the middle" challenges to ensure models effectively utilize their full context range, not just accept long inputs.&lt;/p&gt;
&lt;div align="center"&gt;
&lt;table width="90%"&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;th&gt;Parameters&lt;/th&gt;
    &lt;th&gt;Context Window&lt;/th&gt;
    &lt;th&gt;Architecture&lt;/th&gt;
    &lt;th&gt;Ideal Uses&lt;/th&gt;
  &lt;/tr&gt;

  &lt;tr&gt;
    &lt;td&gt;3B&lt;/td&gt;
    &lt;td&gt;128K&lt;/td&gt;
    &lt;td&gt;Hybrid&lt;/td&gt;
    &lt;td&gt;Fast agents, high-QPS systems, lightweight analytics&lt;/td&gt;
  &lt;/tr&gt;

  &lt;tr&gt;
    &lt;td&gt;7B&lt;/td&gt;
    &lt;td&gt;256K&lt;/td&gt;
    &lt;td&gt;Hybrid&lt;/td&gt;
    &lt;td&gt;Production assistants, reasoning, enterprise chat&lt;/td&gt;
  &lt;/tr&gt;

  &lt;tr&gt;
    &lt;td&gt;34B&lt;/td&gt;
    &lt;td&gt;256K&lt;/td&gt;
    &lt;td&gt;Hybrid&lt;/td&gt;
    &lt;td&gt;Long-document analysis, research, high-stakes tasks&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Data Quality and Diversity: The Foundation of Excellence
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We rebuilt our pre-training data pipeline from the ground up to better reflect the complexity of Arabic. This began with a multi-stage quality filtering process tailored to Arabic orthography, morphology, diacritics, and syntactic patterns. Instead of heuristic filtering, we used deep linguistic analysis to isolate coherent, well-structured text and remove noise commonly found in open-web corpora. The result is a significantly cleaner, more stylistically consistent Arabic dataset.&lt;/p&gt;
&lt;p&gt;Dialect coverage was another key priority. Arabic is not monolithic; Modern Standard Arabic coexists with dialects such as Egyptian, Levantine, Gulf, and Maghrebi, each with distinct vocabularies and grammatical constructions. We expanded dialectal sources substantially so the models would understand and generate the full spectrum of real-world Arabic rather than leaning disproportionately toward formal MSA. To maintain global reasoning and domain diversity, we also preserved the multilingual capabilities of Falcon-H1 by training the Arabic models on an almost equal mix of Arabic, English, and multilingual content totalling around 300 Billion Tokens. This ensures strong performance in code, STEM, and cross-lingual reasoning. The following figure illustrates the distribution of the pre-training data across languages and categories. All values are expressed in billions of tokens.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src="https://cdn-uploads.huggingface.co/production/uploads/659bc8a7b0f43ed69f0b2300/NrIio6sopNNC6wX1mWKzA.png" /&gt;&lt;br /&gt;&lt;/p&gt;


&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Post-Training: Refining Capabilities Without Compromising Competence
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;After pre-training, Falcon-H1-Arabic undergoes a focused post-training pipeline consisting of supervised fine-tuning (SFT) followed by direct preference optimization (DPO). During SFT, we expose the models to high-quality Arabic instructions, curated long-context examples, and structured reasoning tasks that teach them to follow directives, maintain coherence over extended sequences, and ground their responses in relevant information. This stage is crucial for ensuring that the models can actually use their large context windows which does not emerge automatically from architecture alone.&lt;/p&gt;
&lt;p&gt;We follow SFT with a targeted DPO phase to refine alignment, conversational quality, and preference consistency. DPO helps the models balance long-context reasoning with general linguistic competence, improving helpfulness and reducing common failure modes such as drifting, overuse of context, or neglecting earlier information. Throughout both stages, we carefully monitor for catastrophic forgetting and maintain a controlled curriculum so gains in long-context behavior do not come at the expense of core reasoning or factual accuracy. The result is a family of models that handles extended documents and dialogue with ease while preserving strong performance on everyday language tasks.&lt;/p&gt;
&lt;p&gt;Beyond benchmark-oriented optimization, our post-training process deliberately strengthens areas that traditional evaluations do not fully capture, including conversational faithfulness, rhetorical organization, structured follow-ups, and discourse coherence. These enhancements significantly boost the model’s practical usefulness, making Falcon-H1-Arabic more dependable in real multi-turn dialogue, instruction execution, and long-context conversational flows.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Benchmark Performance: Setting New Standards
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Numbers tell an important part of the story. On the Open Arabic LLM Leaderboard (OALL), a comprehensive benchmark evaluating Arabic language understanding across diverse tasks, Falcon-H1-Arabic achieves state-of-the-art results at every scale we tested. Note that our scores may vary slightly from those reported on the leaderboard, as we used vLLM as the backend instead of the leaderboard’s Accelerate-based implementation. These differences are typically under one point while offering significantly faster runtime.&lt;/p&gt;
&lt;p&gt;Beyond OALL, we also report results on the 3LM benchmark for STEM-related tasks on both synthetic and native splits; Arabculture for Arabic culture assessment; and AraDice for Arabic dialect coverage across Levantine, and Egyptian varieties as well as Arabic culture across 6 countries. The reported AraDice score is the average of all the 3 scores. &lt;/p&gt;
&lt;p&gt;&lt;img alt="image" src="https://cdn-uploads.huggingface.co/production/uploads/659bc8a7b0f43ed69f0b2300/asF0IrbLZmK9NXqtMzWuS.png" /&gt;
&lt;img alt="image" src="https://cdn-uploads.huggingface.co/production/uploads/659bc8a7b0f43ed69f0b2300/rhCbKcSsOuorDnjj9j9CA.png" /&gt;&lt;/p&gt;
&lt;p&gt;Starting with the 3B model, the performance is exceptional. It reaches approximately 62% on OALL, outperforming all small-scale models, including Gemma-4B, Qwen3-4B, and Phi-4-mini by roughly ten points. On 3LM, the main Arabic STEM benchmark, it scores around 82% on the native split and 73% on the synthetic split. It also achieves about 62% on the ArabCulture benchmark and around 50% across AraDice dialect evaluation (Egyptian, Gulf, and Levantine). This makes Falcon-H1-Arabic-3B a high-quality, highly efficient model suitable for edge deployments, real-time applications, and agentic systems where latency and cost matter.&lt;/p&gt;
&lt;p&gt;&lt;img alt="image" src="https://cdn-uploads.huggingface.co/production/uploads/659bc8a7b0f43ed69f0b2300/uXwwaWeYBFsd059bjIzOq.png" /&gt;
&lt;img alt="image" src="https://cdn-uploads.huggingface.co/production/uploads/659bc8a7b0f43ed69f0b2300/3FwuLZLg_BIjdLofkTryp.png" /&gt;&lt;/p&gt;
&lt;p&gt;The 7B model continues this upward trajectory. With a score of 71.7% on OALL, it surpasses all models in the ~10B class, including Fanar-9B, Allam-7B*, and Qwen3-8B. On 3LM, it reaches about 92% on the native split and 85% on the synthetic one. AraDice scores rise into the mid-50s across all dialects, and ArabCulture results approach 80%. This model strikes an ideal balance between capability and deployability, making it the most practical choice for general-purpose Arabic NLP in production environments.&lt;/p&gt;
&lt;p&gt;The 34B model represents our flagship system and establishes a new state of the art for Arabic language modeling. It reaches approximately 75% on OALL, outperforming not only models of similar size but even much larger systems such as Llama-3.3-70B and AceGPT2-32B. Its 3LM scores reach about 96% on the native split and 94% on the synthetic one. On ArabCulture it scores close to 80%, and on AraDice it reaches around 53 across dialects. The fact that a 34B hybrid model surpasses the performance of 70B-scale transformers demonstrates the effectiveness of the Falcon-H1 architecture, the quality of the data, and the strength of the post-training pipeline.&lt;/p&gt;
&lt;p&gt;&lt;img alt="image" src="https://cdn-uploads.huggingface.co/production/uploads/659bc8a7b0f43ed69f0b2300/X7nRh62dCHZxbTiTOwNKA.png" /&gt;
&lt;img alt="image" src="https://cdn-uploads.huggingface.co/production/uploads/659bc8a7b0f43ed69f0b2300/kEO723PeOMntQoS0cn5Qw.png" /&gt;&lt;/p&gt;
&lt;p&gt;These benchmark results validate our approach but also highlight an important reality: the frontier of Arabic language modeling is advancing rapidly. Each percentage point on these benchmarks represents countless hours of engineering effort, careful dataset curation, and architectural refinement. The margins by which Falcon-H1-Arabic leads aren't just statistical artifacts, they translate to meaningfully better user experiences in real-world applications.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Practical Applications: From Edge to Enterprise
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Each model in the Falcon-H1-Arabic family is suited to different deployment scenarios. The 3B model is optimized for speed, cost-efficiency, and high-throughput systems, making it ideal for agentic workflows, on-device applications, low-latency chat, and environments with strict resource constraints. The 7B model serves as the general-purpose workhorse for most production applications, powering document understanding systems, chatbots, summarization pipelines, and content generation tools. The 34B model is designed for high-stakes domains where accuracy and long-range reasoning matter most, including legal analysis, medical summarization, academic research, and large-scale enterprise automation. Its extended context window makes it uniquely capable of analyzing hundreds of pages of text in a single pass while maintaining precise coherence.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Responsible AI and Limitations
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Like all language models, Falcon-H1-Arabic may reflect biases from training data and can produce hallucinated information. Model outputs should not be used as sole authorities for medical, legal, or financial decisions without professional verification. Long-context performance may degrade at extreme ranges. We recommend task-specific evaluation and appropriate guardrails before deployment in production or sensitive applications.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Acknowledgments
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;This work stands on the shoulders of many. We extend our gratitude to the Arabic NLP research community, whose open sharing of benchmarks, datasets, and methodologies enables progress across the field. Special thanks to our colleagues at TII: Ilyas Chahed, Younes Belkada, Dhia Eddine Rhaiem, Puneesh Khanna, Jingwei Zuo, Mikhail Lubinets, Slim Frikha, Maksim Velikanov, Kacper Piskorski, and Suhail Mohmad for their invaluable support during this project.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Citation
	&lt;/span&gt;
&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{Falcon-H1-Arabic-2025,
  title={Falcon-H1-Arabic: State-of-the-Art Arabic Language Models with Hybrid Mamba-Transformer Architecture},
  author={Basma El Amel Boussaha and Mohammed Alyafeai and Ahmed Alzubaidi and Leen AlQadi and Shaikha Alsuwaidi and Omar Alkaabi and Hamza Alobeidli and Hakim Hacid},
  url={https://huggingface.co/blog/tiiuae/falcon-h1-arabic},
  month={December},
  year={2025},
  note={Available in 3B, 7B, and 34B parameter versions}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;NB: the scores of ALLaM-7B-Instruct-preview in our evaluation are higher than those reported on the OALL leaderboard, as we used the newest release (7b-alpha-v2.33.0.30), while the leaderboard currently reflects results from the older version (7b-alpha-v1.27.2.25).&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;div align="center"&gt;

  &lt;p&gt;
    Falcon-H1-Arabic models are available for use at the links below. For questions, collaborations, or feedback, reach us at &lt;strong&gt;falcon.info@tii.ae&lt;/strong&gt; or join our community:
  &lt;/p&gt;

  

&lt;/div&gt;






&lt;hr /&gt;
&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/tiiuae/falcon-h1-arabic</guid><pubDate>Mon, 05 Jan 2026 09:16:51 +0000</pubDate></item><item><title>[NEW] L’Oréal brings AI into everyday digital advertising production (AI News)</title><link>https://www.artificialintelligence-news.com/news/loreal-brings-ai-into-everyday-digital-advertising-production/</link><description>&lt;p&gt;Producing digital advertising at global scale has become less about one standout campaign and more about volume, speed, and consistency. For consumer brands operating across dozens of markets, the challenge is not creativity alone, but how to keep content flowing without repeating expensive production cycles.&lt;/p&gt;&lt;p&gt;That pressure is pushing some large companies to test where AI fits inside everyday marketing work. At L’Oréal, AI-generated creative tools are being used to support parts of the digital advertising process, particularly video and visual content. The aim is not to replace human teams, but to reduce friction in a system that demands constant refresh.&lt;/p&gt;&lt;p&gt;The shift offers a useful view into how enterprise AI adoption is unfolding in creative functions, where speed and control matter as much as originality.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-scaling-content-without-scaling-production"&gt;Scaling content without scaling production&lt;/h3&gt;&lt;p&gt;For a global beauty group, digital advertising is no longer a seasonal exercise. Content is needed continuously across social platforms, ecommerce sites, and regional campaigns, often with small variations in language, format, or visual emphasis.&lt;/p&gt;&lt;p&gt;Traditional production models struggle to keep up. Each new asset typically involves planning, filming, editing, and approvals. AI-generated images and video elements allow you to reuse old content and extend it into new formats without having to start from scratch every time.&lt;/p&gt;&lt;p&gt;At L’Oréal, AI tools are being used to help generate or adapt visual content that fits specific digital channels. This includes polishing footage, modifying formats, and creating versions for different platforms. Human teams continue to monitor creative direction and final output, but AI speeds up the time between idea and delivery.&lt;/p&gt;&lt;p&gt;The practical value is not about producing something altogether new. It is about producing enough usable content to meet the pace of digital advertising.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-why-l-oreal-keeps-ai-under-tight-creative-control"&gt;Why L’Oréal keeps AI under tight creative control&lt;/h3&gt;&lt;p&gt;One reason large brands move cautiously with AI in creative work is brand risk. Visual identity, tone, and messaging are tightly regulated, and small inconsistencies can be amplified when content is distributed at scale.&lt;/p&gt;&lt;p&gt;Rather than handing over creative decisions, companies like L’Oréal are using AI as a support layer. AI-generated output is examined, adjusted, and approved using existing workflows. This keeps accountability with internal teams and external agencies, while still gaining efficiency.&lt;/p&gt;&lt;p&gt;This approach reflects a broader pattern in enterprise AI adoption. Tools are being introduced into workflows that already exist, rather than reshaping how decisions are made. In marketing, that often means AI assists with production, not with defining brand voice.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-cost-speed-and-repeatability"&gt;Cost, speed, and repeatability&lt;/h3&gt;&lt;p&gt;Digital advertising budgets are under pressure, even for large consumer groups. Media prices fluctuate, platforms change their restrictions, and audiences expect constant updates. AI offers a way to absorb some of that pressure by lowering the marginal cost of producing additional assets.&lt;/p&gt;&lt;p&gt;By reusing footage and applying AI-based enhancements, brands can stretch the value of each shoot. This is especially important in areas where campaigns must be quickly changed, or when local teams want specific assets but lack full-scale production support.&lt;/p&gt;&lt;p&gt;The result is not a dramatic cost cut in one area, but incremental savings across hundreds of minor decisions. Over time, those savings shape how marketing teams plan campaigns and allocate expenditures.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-this-says-about-enterprise-ai-maturity"&gt;What this says about enterprise AI maturity&lt;/h3&gt;&lt;p&gt;L’Oréal’s use of AI-generated creative work is less about experimentation and more about operational fit. The tools are used in situations where output is predictable, quality can be measured, and mistakes may be caught before release.&lt;/p&gt;&lt;p&gt;This mirrors how AI is being adopted across many enterprise functions. Instead of broad, open-ended use, companies are identifying narrow tasks where AI can reliably assist without introducing new risk. In marketing, those tasks often sit between creative concept and final distribution.&lt;/p&gt;&lt;p&gt;The approach also emphasises a key constraint. AI works best in environments with existing data, rules, and review processes. Creative freedom still belongs to people, while AI supports scale.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-implications-for-marketing-teams"&gt;Implications for marketing teams&lt;/h3&gt;&lt;p&gt;For marketing leaders, the lesson is not that AI will replace agencies or internal creatives. It is that production models built for slower cycles are becoming harder to sustain.&lt;/p&gt;&lt;p&gt;Teams are being asked to deliver more content, more often, with tighter budgets and faster turnaround. AI tools offer one way to manage that demand, but only if they fit existing controls and expectations.&lt;/p&gt;&lt;p&gt;This places new demands on governance. Marketing teams need clear rules on where AI can be used, how outputs are reviewed, and who remains accountable for final decisions. Without that structure, efficiency gains can quickly be offset by risk.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-l-oreal-s-approach-signals-for-enterprise-ai-adoption"&gt;What L’Oréal’s approach signals for enterprise AI adoption&lt;/h3&gt;&lt;p&gt;What stands out in L’Oréal’s approach is restraint. AI is applied where it reduces friction, not where it reshapes the role of creative teams. That makes it easier to integrate into large organisations with established processes and brand safeguards.&lt;/p&gt;&lt;p&gt;As more enterprises look to AI for productivity gains, similar patterns are emerging. AI becomes part of the workflow, not the headline. Success is measured in time saved and consistency maintained, not in novelty.&lt;/p&gt;&lt;p&gt;For now, AI-generated creative work remains a supporting act in enterprise marketing. Its real impact lies in how quietly it changes the economics of content production, one asset at a time.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Helio E. López Vega)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Disney is embedding generative AI into its operating model&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-111442" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/image.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check outAI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Producing digital advertising at global scale has become less about one standout campaign and more about volume, speed, and consistency. For consumer brands operating across dozens of markets, the challenge is not creativity alone, but how to keep content flowing without repeating expensive production cycles.&lt;/p&gt;&lt;p&gt;That pressure is pushing some large companies to test where AI fits inside everyday marketing work. At L’Oréal, AI-generated creative tools are being used to support parts of the digital advertising process, particularly video and visual content. The aim is not to replace human teams, but to reduce friction in a system that demands constant refresh.&lt;/p&gt;&lt;p&gt;The shift offers a useful view into how enterprise AI adoption is unfolding in creative functions, where speed and control matter as much as originality.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-scaling-content-without-scaling-production"&gt;Scaling content without scaling production&lt;/h3&gt;&lt;p&gt;For a global beauty group, digital advertising is no longer a seasonal exercise. Content is needed continuously across social platforms, ecommerce sites, and regional campaigns, often with small variations in language, format, or visual emphasis.&lt;/p&gt;&lt;p&gt;Traditional production models struggle to keep up. Each new asset typically involves planning, filming, editing, and approvals. AI-generated images and video elements allow you to reuse old content and extend it into new formats without having to start from scratch every time.&lt;/p&gt;&lt;p&gt;At L’Oréal, AI tools are being used to help generate or adapt visual content that fits specific digital channels. This includes polishing footage, modifying formats, and creating versions for different platforms. Human teams continue to monitor creative direction and final output, but AI speeds up the time between idea and delivery.&lt;/p&gt;&lt;p&gt;The practical value is not about producing something altogether new. It is about producing enough usable content to meet the pace of digital advertising.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-why-l-oreal-keeps-ai-under-tight-creative-control"&gt;Why L’Oréal keeps AI under tight creative control&lt;/h3&gt;&lt;p&gt;One reason large brands move cautiously with AI in creative work is brand risk. Visual identity, tone, and messaging are tightly regulated, and small inconsistencies can be amplified when content is distributed at scale.&lt;/p&gt;&lt;p&gt;Rather than handing over creative decisions, companies like L’Oréal are using AI as a support layer. AI-generated output is examined, adjusted, and approved using existing workflows. This keeps accountability with internal teams and external agencies, while still gaining efficiency.&lt;/p&gt;&lt;p&gt;This approach reflects a broader pattern in enterprise AI adoption. Tools are being introduced into workflows that already exist, rather than reshaping how decisions are made. In marketing, that often means AI assists with production, not with defining brand voice.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-cost-speed-and-repeatability"&gt;Cost, speed, and repeatability&lt;/h3&gt;&lt;p&gt;Digital advertising budgets are under pressure, even for large consumer groups. Media prices fluctuate, platforms change their restrictions, and audiences expect constant updates. AI offers a way to absorb some of that pressure by lowering the marginal cost of producing additional assets.&lt;/p&gt;&lt;p&gt;By reusing footage and applying AI-based enhancements, brands can stretch the value of each shoot. This is especially important in areas where campaigns must be quickly changed, or when local teams want specific assets but lack full-scale production support.&lt;/p&gt;&lt;p&gt;The result is not a dramatic cost cut in one area, but incremental savings across hundreds of minor decisions. Over time, those savings shape how marketing teams plan campaigns and allocate expenditures.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-this-says-about-enterprise-ai-maturity"&gt;What this says about enterprise AI maturity&lt;/h3&gt;&lt;p&gt;L’Oréal’s use of AI-generated creative work is less about experimentation and more about operational fit. The tools are used in situations where output is predictable, quality can be measured, and mistakes may be caught before release.&lt;/p&gt;&lt;p&gt;This mirrors how AI is being adopted across many enterprise functions. Instead of broad, open-ended use, companies are identifying narrow tasks where AI can reliably assist without introducing new risk. In marketing, those tasks often sit between creative concept and final distribution.&lt;/p&gt;&lt;p&gt;The approach also emphasises a key constraint. AI works best in environments with existing data, rules, and review processes. Creative freedom still belongs to people, while AI supports scale.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-implications-for-marketing-teams"&gt;Implications for marketing teams&lt;/h3&gt;&lt;p&gt;For marketing leaders, the lesson is not that AI will replace agencies or internal creatives. It is that production models built for slower cycles are becoming harder to sustain.&lt;/p&gt;&lt;p&gt;Teams are being asked to deliver more content, more often, with tighter budgets and faster turnaround. AI tools offer one way to manage that demand, but only if they fit existing controls and expectations.&lt;/p&gt;&lt;p&gt;This places new demands on governance. Marketing teams need clear rules on where AI can be used, how outputs are reviewed, and who remains accountable for final decisions. Without that structure, efficiency gains can quickly be offset by risk.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-l-oreal-s-approach-signals-for-enterprise-ai-adoption"&gt;What L’Oréal’s approach signals for enterprise AI adoption&lt;/h3&gt;&lt;p&gt;What stands out in L’Oréal’s approach is restraint. AI is applied where it reduces friction, not where it reshapes the role of creative teams. That makes it easier to integrate into large organisations with established processes and brand safeguards.&lt;/p&gt;&lt;p&gt;As more enterprises look to AI for productivity gains, similar patterns are emerging. AI becomes part of the workflow, not the headline. Success is measured in time saved and consistency maintained, not in novelty.&lt;/p&gt;&lt;p&gt;For now, AI-generated creative work remains a supporting act in enterprise marketing. Its real impact lies in how quietly it changes the economics of content production, one asset at a time.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Helio E. López Vega)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Disney is embedding generative AI into its operating model&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-111442" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/image.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check outAI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/loreal-brings-ai-into-everyday-digital-advertising-production/</guid><pubDate>Mon, 05 Jan 2026 10:00:00 +0000</pubDate></item><item><title>[NEW] What’s next for AI in 2026 (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2026/01/05/1130662/whats-next-for-ai-in-2026/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/WN-AI-2026.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;MIT Technology Review&lt;em&gt;’s What’s Next series looks across industries, trends, and technologies to give you a first look at the future. You can read the rest of them &lt;/em&gt;&lt;em&gt;here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;In an industry in constant flux, sticking your neck out to predict what’s coming next may seem reckless. (AI bubble? What AI bubble?) But for the last few years we’ve done just that—and we’re doing it again.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;How did we do last time? We picked five hot AI trends to look out for in 2025, including what we called &lt;strong&gt;generative virtual playgrounds&lt;/strong&gt;, a.k.a world models (check: From Google DeepMind’s Genie 3 to World Labs’s Marble, tech that can generate realistic virtual environments on the fly keeps getting better and better); so-called &lt;strong&gt;reasoning models&lt;/strong&gt; (check: Need we say more? Reasoning models have fast become the new paradigm for best-in-class problem solving); &lt;strong&gt;a boom in AI for science&lt;/strong&gt; (check: OpenAI is now following Google DeepMind by setting up a dedicated team to focus on just that); &lt;strong&gt;AI companies that are cozier with national security&lt;/strong&gt; (check: OpenAI reversed position on the use of its technology for warfare to sign a deal with the defense-tech startup Anduril to help it take down battlefield drones); and &lt;strong&gt;legitimate competition for Nvidia&lt;/strong&gt; (check, kind of: China is going all in on developing advanced AI chips, but Nvidia’s dominance still looks unassailable—for now at least).&amp;nbsp;&lt;/p&gt;  &lt;p&gt;So what’s coming in 2026? Here are our big bets for the next 12 months.&amp;nbsp;&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;More Silicon Valley products will be built on Chinese LLMs&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The last year shaped up as a big one for Chinese open-source models. In January, DeepSeek released R1, its open-source reasoning model, and shocked the world with what a relatively small firm in China could do with limited resources. By the end of the year, “DeepSeek moment” had become a phrase frequently tossed around by AI entrepreneurs, observers, and builders—an aspirational benchmark of sorts.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It was the first time many people realized they could get a taste of top-tier AI performance without going through OpenAI, Anthropic, or Google.&lt;/p&gt; 
 &lt;p&gt;Open-weight models like R1 allow anyone to download a model and run it on their own hardware. They are also more customizable, letting teams tweak models through techniques like distillation and pruning. This stands in stark contrast to the “closed” models released by major American firms, where core capabilities remain proprietary and access is often expensive.&lt;/p&gt;  &lt;p&gt;As a result, Chinese models have become an easy choice. Reports by CNBC and Bloomberg suggest that startups in the US have increasingly recognized and embraced what they can offer.&lt;/p&gt;  &lt;p&gt;One popular group of models is Qwen, created by Alibaba, the company behind China’s largest e-commerce platform, Taobao. Qwen2.5-1.5B-Instruct alone has 8.85 million downloads, making it one of the most widely used pretrained LLMs. The Qwen family spans a wide range of model sizes alongside specialized versions tuned for math, coding, vision, and instruction-following, a breadth that has helped it become an open-source powerhouse.&lt;/p&gt;  &lt;p&gt;Other Chinese AI firms that were previously unsure about committing to open source are following DeepSeek’s playbook. Standouts include Zhipu’s GLM and Moonshot’s Kimi. The competition has also pushed American firms to open up, at least in part. In August, OpenAI released its first open-source model. In November, the Allen Institute for AI, a Seattle-based nonprofit, released its latest open-source model, Olmo 3.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Even amid growing US-China antagonism, Chinese AI firms’ near-unanimous embrace of open source has earned them goodwill in the global AI community and a long-term trust advantage. In 2026, expect more Silicon Valley apps to quietly ship on top of Chinese open models, and look for the lag between Chinese releases and the Western frontier to keep shrinking—from months to weeks, and sometimes less.&lt;/p&gt;  &lt;p&gt;—&lt;em&gt;Caiwei Chen&lt;/em&gt;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;The US will face another year of regulatory tug-of-war&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;T​​he battle over regulating artificial intelligence is heading for a showdown. On December 11, President Donald Trump signed an executive order aiming to neuter state AI laws, a move meant to handcuff states from keeping the growing industry in check. In 2026, expect more political warfare. The White House and states will spar over who gets to govern the booming technology, while AI companies wage a fierce lobbying campaign to crush regulations, armed with the narrative that a patchwork of state laws will smother innovation and hobble the US in the AI arms race against China.&lt;/p&gt;  &lt;p&gt;Under Trump’s executive order, states may fear being sued or starved federal funding if they clash with his vision for light-touch regulation. Big Democratic states like California—which just enacted the nation’s first frontier AI law requiring companies to publish safety testing for their AI models—will take the fight to court, arguing that only Congress can override state laws. But states that can’t afford to lose federal funding, or fear getting in Trump’s crosshairs, might fold. Still, expect to see more state lawmaking on hot-button issues, especially where Trump’s order gives states a green light to legislate. With chatbots accused of triggering teen suicides and data centers sucking up more and more energy, states will face mounting public pressure to push for guardrails.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;In place of state laws, Trump promises to work with Congress to establish a federal AI law. Don’t count on it. Congress failed to pass a moratorium on state legislation twice in 2025, and we aren’t holding out hope that it will deliver its own bill this year.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;AI companies like OpenAI and Meta will continue to deploy powerful super-PACs to support political candidates who back their agenda and target those who stand in their way. On the other side, super-PACs supporting AI regulation will build their own war chests to counter. Watch them duke it out at next year’s midterm elections.&lt;/p&gt;  &lt;p&gt;The further AI advances, the more people will fight to steer its course, and 2026 will be another year of regulatory tug-of-war—with no end in sight.&lt;/p&gt;  &lt;p&gt;—&lt;em&gt;Michelle Kim&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Chatbots will change the way we shop&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Imagine a world in which you have a personal shopper at your disposal 24-7—an expert who can instantly recommend a gift for even the trickiest-to-buy-for friend or relative, or trawl the web to draw up a list of the best bookcases available within your tight budget. Better yet, they can analyze a kitchen appliance’s strengths and weaknesses, compare it with its seemingly identical competition, and find you the best deal. Then once you’re happy with their suggestion, they’ll take care of the purchasing and delivery details too.&lt;/p&gt;&lt;p&gt;But this ultra-knowledgeable shopper isn’t a clued-up human at all—it’s a chatbot. This is no distant prediction, either. Salesforce recently said it anticipates that AI will drive $263 billion in online purchases this holiday season. That’s some 21% of all orders. And experts are betting on AI-enhanced shopping becoming even bigger business within the next few years. By 2030, between $3 trillion and $5 trillion annually will be made from agentic commerce, according to research from the consulting firm McKinsey.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Unsurprisingly, AI companies are already heavily invested in making purchasing through their platforms as frictionless as possible. Google’s Gemini app can now tap into the company’s powerful Shopping Graph data set of products and sellers, and can even use its agentic technology to call stores on your behalf. Meanwhile, back in November, OpenAI announced a ChatGPT shopping feature capable of rapidly compiling buyer’s guides, and the company has struck deals with Walmart, Target, and Etsy to allow shoppers to buy products directly within chatbot interactions.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Expect plenty more of these kinds of deals to be struck within the next year as consumer time spent chatting with AI keeps on rising, and web traffic from search engines and social media continues to plummet.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;—&lt;em&gt;Rhiannon Williams&lt;/em&gt;&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;An LLM will make an important new discovery&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;I’m going to hedge here, right out of the gate. It’s no secret that large language models spit out a lot of nonsense. Unless it’s with monkeys-and-typewriters luck, LLMs won’t discover anything by themselves. But LLMs do still have the potential to extend the bounds of human knowledge.&lt;/p&gt;  &lt;p&gt;We got a glimpse of how this could work in May, when Google DeepMind revealed AlphaEvolve, a system that used the firm’s Gemini LLM to come up with new algorithms for solving unsolved problems. The breakthrough was to combine Gemini with an evolutionary algorithm that checked its suggestions, picked the best ones, and fed them back into the LLM to make them even better.&lt;/p&gt; 
 &lt;p&gt;Google DeepMind used AlphaEvolve to come up with more efficient ways to manage power consumption by data centers and Google’s TPU chips. Those discoveries are significant but not game-changing. Yet. Researchers at Google DeepMind are now pushing their approach to see how far it will go.&lt;/p&gt;  &lt;p&gt;And others have been quick to follow their lead. A week after AlphaEvolve came out, Asankhaya Sharma, an AI engineer in Singapore, shared OpenEvolve, an open-source version of Google DeepMind’s tool. In September, the Japanese firm Sakana AI released a version of the software called SinkaEvolve. And in November, a team of US and Chinese researchers revealed AlphaResearch, which they claim improves on one of AlphaEvolve’s already better-than-human math solutions.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;There are alternative approaches too. For example, researchers at the University of Colorado Denver are trying to make LLMs more inventive by tweaking the way so-called reasoning models work. They have drawn on what cognitive scientists know about creative thinking in humans to push reasoning models toward solutions that are more outside the box than their typical safe-bet suggestions.&lt;/p&gt;  &lt;p&gt;Hundreds of companies are spending billions of dollars looking for ways to get AI to crack unsolved math problems, speed up computers, and come up with new drugs and materials. Now that AlphaEvolve has shown what’s possible with LLMs, expect activity on this front to ramp up fast.&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;—&lt;em&gt;Will Douglas Heaven&lt;/em&gt;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Legal fights heat up&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;For a while, lawsuits against AI companies were pretty predictable: Rights holders like authors or musicians would sue companies that trained AI models on their work, and the courts generally found in favor of the tech giants. AI’s upcoming legal battles will be far messier.&lt;/p&gt; 
 &lt;p&gt;The fights center on thorny, unresolved questions: Can AI companies be held liable for what their chatbots encourage people to do, as when they help teens plan suicides? If a chatbot spreads patently false information about you, can its creator be sued for defamation? If companies lose these cases, will insurers shun AI companies as clients?&lt;/p&gt;  &lt;p&gt;In 2026, we’ll start to see the answers to these questions, in part because some notable cases will go to trial (the family of a teen who died by suicide will bring OpenAI to court in November).&lt;/p&gt;  &lt;p&gt;At the same time, the legal landscape will be further complicated by President Trump’s executive order from December—see Michelle’s item above for more details on the brewing regulatory storm.&lt;/p&gt;  &lt;p&gt;No matter what, we’ll see a dizzying array of lawsuits in all directions (not to mention some judges even turning to AI amid the deluge).&lt;/p&gt;  &lt;p&gt;—&lt;em&gt;James O’Donnell&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/WN-AI-2026.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;MIT Technology Review&lt;em&gt;’s What’s Next series looks across industries, trends, and technologies to give you a first look at the future. You can read the rest of them &lt;/em&gt;&lt;em&gt;here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;In an industry in constant flux, sticking your neck out to predict what’s coming next may seem reckless. (AI bubble? What AI bubble?) But for the last few years we’ve done just that—and we’re doing it again.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;How did we do last time? We picked five hot AI trends to look out for in 2025, including what we called &lt;strong&gt;generative virtual playgrounds&lt;/strong&gt;, a.k.a world models (check: From Google DeepMind’s Genie 3 to World Labs’s Marble, tech that can generate realistic virtual environments on the fly keeps getting better and better); so-called &lt;strong&gt;reasoning models&lt;/strong&gt; (check: Need we say more? Reasoning models have fast become the new paradigm for best-in-class problem solving); &lt;strong&gt;a boom in AI for science&lt;/strong&gt; (check: OpenAI is now following Google DeepMind by setting up a dedicated team to focus on just that); &lt;strong&gt;AI companies that are cozier with national security&lt;/strong&gt; (check: OpenAI reversed position on the use of its technology for warfare to sign a deal with the defense-tech startup Anduril to help it take down battlefield drones); and &lt;strong&gt;legitimate competition for Nvidia&lt;/strong&gt; (check, kind of: China is going all in on developing advanced AI chips, but Nvidia’s dominance still looks unassailable—for now at least).&amp;nbsp;&lt;/p&gt;  &lt;p&gt;So what’s coming in 2026? Here are our big bets for the next 12 months.&amp;nbsp;&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;More Silicon Valley products will be built on Chinese LLMs&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The last year shaped up as a big one for Chinese open-source models. In January, DeepSeek released R1, its open-source reasoning model, and shocked the world with what a relatively small firm in China could do with limited resources. By the end of the year, “DeepSeek moment” had become a phrase frequently tossed around by AI entrepreneurs, observers, and builders—an aspirational benchmark of sorts.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It was the first time many people realized they could get a taste of top-tier AI performance without going through OpenAI, Anthropic, or Google.&lt;/p&gt; 
 &lt;p&gt;Open-weight models like R1 allow anyone to download a model and run it on their own hardware. They are also more customizable, letting teams tweak models through techniques like distillation and pruning. This stands in stark contrast to the “closed” models released by major American firms, where core capabilities remain proprietary and access is often expensive.&lt;/p&gt;  &lt;p&gt;As a result, Chinese models have become an easy choice. Reports by CNBC and Bloomberg suggest that startups in the US have increasingly recognized and embraced what they can offer.&lt;/p&gt;  &lt;p&gt;One popular group of models is Qwen, created by Alibaba, the company behind China’s largest e-commerce platform, Taobao. Qwen2.5-1.5B-Instruct alone has 8.85 million downloads, making it one of the most widely used pretrained LLMs. The Qwen family spans a wide range of model sizes alongside specialized versions tuned for math, coding, vision, and instruction-following, a breadth that has helped it become an open-source powerhouse.&lt;/p&gt;  &lt;p&gt;Other Chinese AI firms that were previously unsure about committing to open source are following DeepSeek’s playbook. Standouts include Zhipu’s GLM and Moonshot’s Kimi. The competition has also pushed American firms to open up, at least in part. In August, OpenAI released its first open-source model. In November, the Allen Institute for AI, a Seattle-based nonprofit, released its latest open-source model, Olmo 3.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Even amid growing US-China antagonism, Chinese AI firms’ near-unanimous embrace of open source has earned them goodwill in the global AI community and a long-term trust advantage. In 2026, expect more Silicon Valley apps to quietly ship on top of Chinese open models, and look for the lag between Chinese releases and the Western frontier to keep shrinking—from months to weeks, and sometimes less.&lt;/p&gt;  &lt;p&gt;—&lt;em&gt;Caiwei Chen&lt;/em&gt;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;The US will face another year of regulatory tug-of-war&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;T​​he battle over regulating artificial intelligence is heading for a showdown. On December 11, President Donald Trump signed an executive order aiming to neuter state AI laws, a move meant to handcuff states from keeping the growing industry in check. In 2026, expect more political warfare. The White House and states will spar over who gets to govern the booming technology, while AI companies wage a fierce lobbying campaign to crush regulations, armed with the narrative that a patchwork of state laws will smother innovation and hobble the US in the AI arms race against China.&lt;/p&gt;  &lt;p&gt;Under Trump’s executive order, states may fear being sued or starved federal funding if they clash with his vision for light-touch regulation. Big Democratic states like California—which just enacted the nation’s first frontier AI law requiring companies to publish safety testing for their AI models—will take the fight to court, arguing that only Congress can override state laws. But states that can’t afford to lose federal funding, or fear getting in Trump’s crosshairs, might fold. Still, expect to see more state lawmaking on hot-button issues, especially where Trump’s order gives states a green light to legislate. With chatbots accused of triggering teen suicides and data centers sucking up more and more energy, states will face mounting public pressure to push for guardrails.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;In place of state laws, Trump promises to work with Congress to establish a federal AI law. Don’t count on it. Congress failed to pass a moratorium on state legislation twice in 2025, and we aren’t holding out hope that it will deliver its own bill this year.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;AI companies like OpenAI and Meta will continue to deploy powerful super-PACs to support political candidates who back their agenda and target those who stand in their way. On the other side, super-PACs supporting AI regulation will build their own war chests to counter. Watch them duke it out at next year’s midterm elections.&lt;/p&gt;  &lt;p&gt;The further AI advances, the more people will fight to steer its course, and 2026 will be another year of regulatory tug-of-war—with no end in sight.&lt;/p&gt;  &lt;p&gt;—&lt;em&gt;Michelle Kim&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Chatbots will change the way we shop&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Imagine a world in which you have a personal shopper at your disposal 24-7—an expert who can instantly recommend a gift for even the trickiest-to-buy-for friend or relative, or trawl the web to draw up a list of the best bookcases available within your tight budget. Better yet, they can analyze a kitchen appliance’s strengths and weaknesses, compare it with its seemingly identical competition, and find you the best deal. Then once you’re happy with their suggestion, they’ll take care of the purchasing and delivery details too.&lt;/p&gt;&lt;p&gt;But this ultra-knowledgeable shopper isn’t a clued-up human at all—it’s a chatbot. This is no distant prediction, either. Salesforce recently said it anticipates that AI will drive $263 billion in online purchases this holiday season. That’s some 21% of all orders. And experts are betting on AI-enhanced shopping becoming even bigger business within the next few years. By 2030, between $3 trillion and $5 trillion annually will be made from agentic commerce, according to research from the consulting firm McKinsey.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Unsurprisingly, AI companies are already heavily invested in making purchasing through their platforms as frictionless as possible. Google’s Gemini app can now tap into the company’s powerful Shopping Graph data set of products and sellers, and can even use its agentic technology to call stores on your behalf. Meanwhile, back in November, OpenAI announced a ChatGPT shopping feature capable of rapidly compiling buyer’s guides, and the company has struck deals with Walmart, Target, and Etsy to allow shoppers to buy products directly within chatbot interactions.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Expect plenty more of these kinds of deals to be struck within the next year as consumer time spent chatting with AI keeps on rising, and web traffic from search engines and social media continues to plummet.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;—&lt;em&gt;Rhiannon Williams&lt;/em&gt;&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;An LLM will make an important new discovery&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;I’m going to hedge here, right out of the gate. It’s no secret that large language models spit out a lot of nonsense. Unless it’s with monkeys-and-typewriters luck, LLMs won’t discover anything by themselves. But LLMs do still have the potential to extend the bounds of human knowledge.&lt;/p&gt;  &lt;p&gt;We got a glimpse of how this could work in May, when Google DeepMind revealed AlphaEvolve, a system that used the firm’s Gemini LLM to come up with new algorithms for solving unsolved problems. The breakthrough was to combine Gemini with an evolutionary algorithm that checked its suggestions, picked the best ones, and fed them back into the LLM to make them even better.&lt;/p&gt; 
 &lt;p&gt;Google DeepMind used AlphaEvolve to come up with more efficient ways to manage power consumption by data centers and Google’s TPU chips. Those discoveries are significant but not game-changing. Yet. Researchers at Google DeepMind are now pushing their approach to see how far it will go.&lt;/p&gt;  &lt;p&gt;And others have been quick to follow their lead. A week after AlphaEvolve came out, Asankhaya Sharma, an AI engineer in Singapore, shared OpenEvolve, an open-source version of Google DeepMind’s tool. In September, the Japanese firm Sakana AI released a version of the software called SinkaEvolve. And in November, a team of US and Chinese researchers revealed AlphaResearch, which they claim improves on one of AlphaEvolve’s already better-than-human math solutions.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;There are alternative approaches too. For example, researchers at the University of Colorado Denver are trying to make LLMs more inventive by tweaking the way so-called reasoning models work. They have drawn on what cognitive scientists know about creative thinking in humans to push reasoning models toward solutions that are more outside the box than their typical safe-bet suggestions.&lt;/p&gt;  &lt;p&gt;Hundreds of companies are spending billions of dollars looking for ways to get AI to crack unsolved math problems, speed up computers, and come up with new drugs and materials. Now that AlphaEvolve has shown what’s possible with LLMs, expect activity on this front to ramp up fast.&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;—&lt;em&gt;Will Douglas Heaven&lt;/em&gt;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Legal fights heat up&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;For a while, lawsuits against AI companies were pretty predictable: Rights holders like authors or musicians would sue companies that trained AI models on their work, and the courts generally found in favor of the tech giants. AI’s upcoming legal battles will be far messier.&lt;/p&gt; 
 &lt;p&gt;The fights center on thorny, unresolved questions: Can AI companies be held liable for what their chatbots encourage people to do, as when they help teens plan suicides? If a chatbot spreads patently false information about you, can its creator be sued for defamation? If companies lose these cases, will insurers shun AI companies as clients?&lt;/p&gt;  &lt;p&gt;In 2026, we’ll start to see the answers to these questions, in part because some notable cases will go to trial (the family of a teen who died by suicide will bring OpenAI to court in November).&lt;/p&gt;  &lt;p&gt;At the same time, the legal landscape will be further complicated by President Trump’s executive order from December—see Michelle’s item above for more details on the brewing regulatory storm.&lt;/p&gt;  &lt;p&gt;No matter what, we’ll see a dizzying array of lawsuits in all directions (not to mention some judges even turning to AI amid the deluge).&lt;/p&gt;  &lt;p&gt;—&lt;em&gt;James O’Donnell&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/01/05/1130662/whats-next-for-ai-in-2026/</guid><pubDate>Mon, 05 Jan 2026 11:04:46 +0000</pubDate></item><item><title>[NEW] The Download: Kenya’s Great Carbon Valley, and the AI terms that were everywhere in 2025 (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/01/05/1130672/the-download-kenyas-great-carbon-valley-and-the-ai-terms-that-were-everywhere-in-2025/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Welcome to Kenya’s Great Carbon Valley: a bold new gamble to fight climate change&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;In June last year, startup Octavia Carbon began running a high-stakes test in the small town of Gilgil in south-central Kenya. It’s harnessing some of the excess energy generated by vast clouds of steam under the Earth’s surface to power prototypes of a machine that promises to remove carbon dioxide from the air in a manner that the company says is efficient, affordable, and—crucially—scalable.&lt;/p&gt;&lt;p&gt;The company’s long-term vision is undoubtedly ambitious—it wants to prove that direct air capture (DAC), as the process is known, can be a powerful tool to help the world keep temperatures from rising to ever more dangerous levels.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But DAC is also a controversial technology, unproven at scale and wildly expensive to operate. On top of that, Kenya’s Maasai people have plenty of reasons to distrust energy companies. Read the full story.&lt;/p&gt; 
 &lt;p&gt;—&lt;em&gt;Diana Kruzman&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This article is also part of the Big Story series: &lt;em&gt;MIT Technology Review&lt;/em&gt;’s most important, ambitious reporting. The stories in the series take a deep look at the technologies that are coming next and what they will mean for us and the world we live in. &lt;/strong&gt;&lt;strong&gt;Check out the rest of them here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;AI Wrapped: The 14 AI terms you couldn’t avoid in 2025&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;If the past 12 months have taught us anything, it’s that the AI hype train is showing no signs of slowing. It’s hard to believe that at the beginning of the year, DeepSeek had yet to turn the entire industry on its head, Meta was better known for trying (and failing) to make the metaverse cool than for its relentless quest to dominate superintelligence, and vibe coding wasn’t a thing.&lt;/p&gt;  &lt;p&gt;If that’s left you feeling a little confused, fear not. Our writers have taken a look back over the AI terms that dominated the year, for better or worse. Read the full list.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;MIT Technology Review’s most popular stories of 2025&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;2025 was a busy and productive year here at&lt;em&gt; MIT Technology Review&lt;/em&gt;. We published magazine issues on power, creativity, innovation, bodies, relationships, and security. We hosted 14 exclusive virtual conversations with our editors and outside experts in our subscriber-only series, Roundtables, and held two events on MIT’s campus. And we published hundreds of articles online, following new developments in computing, climate tech, robotics, and more.&lt;/p&gt;&lt;p&gt;As the new year begins, we wanted to give you a chance to revisit some of this work with us. Whether we were covering the red-hot rise of artificial intelligence or the future of biotech, these are some of the stories that resonated the most with our readers.&lt;/p&gt;   

 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Washington’s battle to break up Big Tech is in peril&lt;/strong&gt;&lt;br /&gt;A string of judges have opted not to force them to spin off key assets. (FT $)&lt;br /&gt;+ &lt;em&gt;Here’s some of the major tech litigation we can expect in the next 12 months. &lt;/em&gt;(Reuters)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 Disinformation about the US invasion of Venezuela is rife on social media&lt;/strong&gt;&lt;br /&gt;And the biggest platforms don’t appear to be doing much about it. (Wired $)&lt;br /&gt;+ &lt;em&gt;Trump shared a picture of captured president Maduro on Truth Social. &lt;/em&gt;(NYT $)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;3&lt;/strong&gt; &lt;strong&gt;Here’s what we know about Big Tech’s ties to the Israeli military&lt;/strong&gt;&lt;br /&gt;AI is central to its military operations, and giant US firms have stepped up to help. (The Guardian)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;4 Alibaba’s AI tool is detecting cancer cases in China&lt;/strong&gt;&lt;br /&gt;PANDA is adept at spotting pancreatic cancer, which is typically tough to identify. (NYT $)&lt;br /&gt;+ &lt;em&gt;How hospitals became an AI testbed. &lt;/em&gt;(WSJ $)&lt;br /&gt;+ &lt;em&gt;A medical portal in New Zealand was hacked into last week. &lt;/em&gt;(Reuters)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 This Discord community supports people recovering from AI-fueled delusions&lt;/strong&gt;&lt;br /&gt;They say reconnecting with fellow humans is an important step forward. (WP $)&lt;br /&gt;+ &lt;em&gt;The looming crackdown on AI companionship. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 Californians can now demand data brokers delete their personal information&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Thanks to a new tool—but there’s a catch. (TechCrunch)&lt;br /&gt;+ &lt;em&gt;This California lawmaker wants to ban AI from kids’ toys. &lt;/em&gt;(Fast Company $)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;7 Chinese peptides are flooding into Silicon Valley&lt;br /&gt;The unproven drugs promise to heal injuries, improve focus and reduce appetite—and American tech workers are hooked. (NYT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 Alaska’s court system built an AI assistant to navigate probate&lt;br /&gt;&lt;/strong&gt;But the project has been plagued by delays and setbacks. (NBC News)&lt;br /&gt;+ &lt;em&gt;Inside Amsterdam’s high-stakes experiment to create fair welfare AI.&lt;/em&gt; (MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;9 These ghostly particles could upend how we think about the universe&lt;/strong&gt;&lt;br /&gt;The standard model of particle physics may have a crack in it. (New Scientist $)&lt;br /&gt;+ &lt;em&gt;Why is the universe so complex and beautiful? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10 Sick of the same old social media apps?&lt;/strong&gt;&lt;br /&gt;Give these alternative platforms a go. (Insider $)&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“Just an unbelievable amount of pollution.”&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;—Sharon Wilson, a former oil and gas worker who tracks methane releases, tells the Guardian what a thermal imaging camera pointed at xAI’s Colossus datacentre has revealed.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1130674" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/image.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;How aging clocks can help us understand why we age—and if we can reverse it&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Wrinkles and gray hairs aside, it can be difficult to know how well—or poorly—someone’s body is truly aging. A person who develops age-related diseases earlier in life, or has other biological changes associated with aging, might be considered “biologically older” than a similar-age person who doesn’t have those changes. Some 80-year-olds will be weak and frail, while others are fit and active.&lt;/p&gt;&lt;p&gt;Over the past decade, scientists have been uncovering new methods of looking at the hidden ways our bodies are aging. And what they’ve found is changing our understanding of aging itself. Read the full story.&lt;/p&gt;&lt;p&gt;&lt;em&gt;—Jessica Hamzelou&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ You heard it here first: 2026 is the year of cabbage (yes, cabbage.)&lt;br /&gt;+ Darts is bigger than ever. So why are we still waiting for the first great darts video game? 🎯&lt;br /&gt;+ This year’s CES is already off to a bang, courtesy of an essential, cutting-edge vibrating knife.&lt;br /&gt;+ At least one good thing came out of that &lt;em&gt;Stranger Things&lt;/em&gt; finale—streams of Prince’s excellent back catalog have soared.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Welcome to Kenya’s Great Carbon Valley: a bold new gamble to fight climate change&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;In June last year, startup Octavia Carbon began running a high-stakes test in the small town of Gilgil in south-central Kenya. It’s harnessing some of the excess energy generated by vast clouds of steam under the Earth’s surface to power prototypes of a machine that promises to remove carbon dioxide from the air in a manner that the company says is efficient, affordable, and—crucially—scalable.&lt;/p&gt;&lt;p&gt;The company’s long-term vision is undoubtedly ambitious—it wants to prove that direct air capture (DAC), as the process is known, can be a powerful tool to help the world keep temperatures from rising to ever more dangerous levels.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But DAC is also a controversial technology, unproven at scale and wildly expensive to operate. On top of that, Kenya’s Maasai people have plenty of reasons to distrust energy companies. Read the full story.&lt;/p&gt; 
 &lt;p&gt;—&lt;em&gt;Diana Kruzman&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This article is also part of the Big Story series: &lt;em&gt;MIT Technology Review&lt;/em&gt;’s most important, ambitious reporting. The stories in the series take a deep look at the technologies that are coming next and what they will mean for us and the world we live in. &lt;/strong&gt;&lt;strong&gt;Check out the rest of them here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;AI Wrapped: The 14 AI terms you couldn’t avoid in 2025&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;If the past 12 months have taught us anything, it’s that the AI hype train is showing no signs of slowing. It’s hard to believe that at the beginning of the year, DeepSeek had yet to turn the entire industry on its head, Meta was better known for trying (and failing) to make the metaverse cool than for its relentless quest to dominate superintelligence, and vibe coding wasn’t a thing.&lt;/p&gt;  &lt;p&gt;If that’s left you feeling a little confused, fear not. Our writers have taken a look back over the AI terms that dominated the year, for better or worse. Read the full list.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;MIT Technology Review’s most popular stories of 2025&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;2025 was a busy and productive year here at&lt;em&gt; MIT Technology Review&lt;/em&gt;. We published magazine issues on power, creativity, innovation, bodies, relationships, and security. We hosted 14 exclusive virtual conversations with our editors and outside experts in our subscriber-only series, Roundtables, and held two events on MIT’s campus. And we published hundreds of articles online, following new developments in computing, climate tech, robotics, and more.&lt;/p&gt;&lt;p&gt;As the new year begins, we wanted to give you a chance to revisit some of this work with us. Whether we were covering the red-hot rise of artificial intelligence or the future of biotech, these are some of the stories that resonated the most with our readers.&lt;/p&gt;   

 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Washington’s battle to break up Big Tech is in peril&lt;/strong&gt;&lt;br /&gt;A string of judges have opted not to force them to spin off key assets. (FT $)&lt;br /&gt;+ &lt;em&gt;Here’s some of the major tech litigation we can expect in the next 12 months. &lt;/em&gt;(Reuters)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 Disinformation about the US invasion of Venezuela is rife on social media&lt;/strong&gt;&lt;br /&gt;And the biggest platforms don’t appear to be doing much about it. (Wired $)&lt;br /&gt;+ &lt;em&gt;Trump shared a picture of captured president Maduro on Truth Social. &lt;/em&gt;(NYT $)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;3&lt;/strong&gt; &lt;strong&gt;Here’s what we know about Big Tech’s ties to the Israeli military&lt;/strong&gt;&lt;br /&gt;AI is central to its military operations, and giant US firms have stepped up to help. (The Guardian)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;4 Alibaba’s AI tool is detecting cancer cases in China&lt;/strong&gt;&lt;br /&gt;PANDA is adept at spotting pancreatic cancer, which is typically tough to identify. (NYT $)&lt;br /&gt;+ &lt;em&gt;How hospitals became an AI testbed. &lt;/em&gt;(WSJ $)&lt;br /&gt;+ &lt;em&gt;A medical portal in New Zealand was hacked into last week. &lt;/em&gt;(Reuters)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 This Discord community supports people recovering from AI-fueled delusions&lt;/strong&gt;&lt;br /&gt;They say reconnecting with fellow humans is an important step forward. (WP $)&lt;br /&gt;+ &lt;em&gt;The looming crackdown on AI companionship. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 Californians can now demand data brokers delete their personal information&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Thanks to a new tool—but there’s a catch. (TechCrunch)&lt;br /&gt;+ &lt;em&gt;This California lawmaker wants to ban AI from kids’ toys. &lt;/em&gt;(Fast Company $)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;7 Chinese peptides are flooding into Silicon Valley&lt;br /&gt;The unproven drugs promise to heal injuries, improve focus and reduce appetite—and American tech workers are hooked. (NYT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 Alaska’s court system built an AI assistant to navigate probate&lt;br /&gt;&lt;/strong&gt;But the project has been plagued by delays and setbacks. (NBC News)&lt;br /&gt;+ &lt;em&gt;Inside Amsterdam’s high-stakes experiment to create fair welfare AI.&lt;/em&gt; (MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;9 These ghostly particles could upend how we think about the universe&lt;/strong&gt;&lt;br /&gt;The standard model of particle physics may have a crack in it. (New Scientist $)&lt;br /&gt;+ &lt;em&gt;Why is the universe so complex and beautiful? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10 Sick of the same old social media apps?&lt;/strong&gt;&lt;br /&gt;Give these alternative platforms a go. (Insider $)&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“Just an unbelievable amount of pollution.”&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;—Sharon Wilson, a former oil and gas worker who tracks methane releases, tells the Guardian what a thermal imaging camera pointed at xAI’s Colossus datacentre has revealed.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1130674" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/image.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;How aging clocks can help us understand why we age—and if we can reverse it&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Wrinkles and gray hairs aside, it can be difficult to know how well—or poorly—someone’s body is truly aging. A person who develops age-related diseases earlier in life, or has other biological changes associated with aging, might be considered “biologically older” than a similar-age person who doesn’t have those changes. Some 80-year-olds will be weak and frail, while others are fit and active.&lt;/p&gt;&lt;p&gt;Over the past decade, scientists have been uncovering new methods of looking at the hidden ways our bodies are aging. And what they’ve found is changing our understanding of aging itself. Read the full story.&lt;/p&gt;&lt;p&gt;&lt;em&gt;—Jessica Hamzelou&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ You heard it here first: 2026 is the year of cabbage (yes, cabbage.)&lt;br /&gt;+ Darts is bigger than ever. So why are we still waiting for the first great darts video game? 🎯&lt;br /&gt;+ This year’s CES is already off to a bang, courtesy of an essential, cutting-edge vibrating knife.&lt;br /&gt;+ At least one good thing came out of that &lt;em&gt;Stranger Things&lt;/em&gt; finale—streams of Prince’s excellent back catalog have soared.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/01/05/1130672/the-download-kenyas-great-carbon-valley-and-the-ai-terms-that-were-everywhere-in-2025/</guid><pubDate>Mon, 05 Jan 2026 13:10:00 +0000</pubDate></item><item><title>[NEW] Google previews new Gemini features for TV at CES 2026 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/05/google-previews-new-gemini-features-for-tv-at-ces-2026/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google believes AI can improve the TV-watching experience, which is why it brought its Gemini AI to Google TV devices in November. At the Consumer Electronics Show (CES 2026) in Las Vegas, the company is now showing off a series of new Gemini features that will soon arrive on the TV, making it possible for viewers to deep-dive into topics, search for and “reimagine” their personal photos and videos with AI, and, perhaps best of all, tell the TV what to do instead of having to navigate through complicated settings.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is first bringing these Gemini features and others to select TCL televisions before rolling them out more broadly to other Google TV devices in the months ahead. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Designed for large-screen experiences, Gemini for Google TV will allow users to talk to their TV to find something to watch, catch up on a favorite series by getting a recap of the plot, or get recommendations, all by using natural language conversation. For instance, you could ask Gemini for something to watch that would be a blend of two people’s tastes, or get help remembering a show or movie where you can’t remember the title, but can describe the plot or name one of the actors. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;You could even ask Google something like, “What’s the new hospital drama everyone’s talking about?”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3079567" height="383" src="https://techcrunch.com/wp-content/uploads/2026/01/Gemini-Entertainment-Response.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Gemini can respond to users’ questions through a new visually rich framework that adapts to individual queries, combining text, imagery, video context, and real-time sports updates, as required. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Google sees the potential for the TV’s screen to be used for more than just entertainment; with Gemini, the TV can be used to educate, too. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At CES, Google showed how this would work. When users ask a question about something they want to learn about, the TV screen can offer a deep dive into the topic. A narrated interactive overview simplifies concepts, and users can ask follow-up questions to learn more.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3079571" height="383" src="https://techcrunch.com/wp-content/uploads/2026/01/Gemini-Dive-Deeper.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Gemini will also allow users to query their Google Photos library for specific people or moments. They can also apply artistic styles to their photos and videos using Gemini AI and turn their memories into cinematic slideshows, says Google.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3079569" height="383" src="https://techcrunch.com/wp-content/uploads/2026/01/Google-Photos-Remix.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3079565" height="383" src="https://techcrunch.com/wp-content/uploads/2026/01/Nano-Banana-and-Veo-3.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;However, perhaps the most useful feature is one that gives you the power to optimize the TV’s settings using only your voice.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, you’ll be able to tell Gemini things like “the screen is too dim” or “I can’t hear the dialogue,” and Gemini will adjust the relevant settings without you having to leave the movie or TV show you’re watching to dig through menus to find the necessary option. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3079563" height="383" src="https://techcrunch.com/wp-content/uploads/2026/01/Device-Controls.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google says the new Gemini features will require the Google TV devices to be running Android TV OS 14 or higher, and they will need an internet connection. Not all languages, countries, or devices will be supported at launch, and users must also have a Google account to access the Gemini for TV experience. &lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google believes AI can improve the TV-watching experience, which is why it brought its Gemini AI to Google TV devices in November. At the Consumer Electronics Show (CES 2026) in Las Vegas, the company is now showing off a series of new Gemini features that will soon arrive on the TV, making it possible for viewers to deep-dive into topics, search for and “reimagine” their personal photos and videos with AI, and, perhaps best of all, tell the TV what to do instead of having to navigate through complicated settings.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is first bringing these Gemini features and others to select TCL televisions before rolling them out more broadly to other Google TV devices in the months ahead. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Designed for large-screen experiences, Gemini for Google TV will allow users to talk to their TV to find something to watch, catch up on a favorite series by getting a recap of the plot, or get recommendations, all by using natural language conversation. For instance, you could ask Gemini for something to watch that would be a blend of two people’s tastes, or get help remembering a show or movie where you can’t remember the title, but can describe the plot or name one of the actors. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;You could even ask Google something like, “What’s the new hospital drama everyone’s talking about?”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3079567" height="383" src="https://techcrunch.com/wp-content/uploads/2026/01/Gemini-Entertainment-Response.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Gemini can respond to users’ questions through a new visually rich framework that adapts to individual queries, combining text, imagery, video context, and real-time sports updates, as required. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Google sees the potential for the TV’s screen to be used for more than just entertainment; with Gemini, the TV can be used to educate, too. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At CES, Google showed how this would work. When users ask a question about something they want to learn about, the TV screen can offer a deep dive into the topic. A narrated interactive overview simplifies concepts, and users can ask follow-up questions to learn more.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3079571" height="383" src="https://techcrunch.com/wp-content/uploads/2026/01/Gemini-Dive-Deeper.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Gemini will also allow users to query their Google Photos library for specific people or moments. They can also apply artistic styles to their photos and videos using Gemini AI and turn their memories into cinematic slideshows, says Google.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3079569" height="383" src="https://techcrunch.com/wp-content/uploads/2026/01/Google-Photos-Remix.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3079565" height="383" src="https://techcrunch.com/wp-content/uploads/2026/01/Nano-Banana-and-Veo-3.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;However, perhaps the most useful feature is one that gives you the power to optimize the TV’s settings using only your voice.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, you’ll be able to tell Gemini things like “the screen is too dim” or “I can’t hear the dialogue,” and Gemini will adjust the relevant settings without you having to leave the movie or TV show you’re watching to dig through menus to find the necessary option. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3079563" height="383" src="https://techcrunch.com/wp-content/uploads/2026/01/Device-Controls.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google says the new Gemini features will require the Google TV devices to be running Android TV OS 14 or higher, and they will need an internet connection. Not all languages, countries, or devices will be supported at launch, and users must also have a Google account to access the Gemini for TV experience. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/05/google-previews-new-gemini-features-for-tv-at-ces-2026/</guid><pubDate>Mon, 05 Jan 2026 14:00:00 +0000</pubDate></item><item><title>[NEW] Amazon’s AI assistant comes to the web with Alexa.com (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/05/alexa-without-an-echo-amazons-ai-chatbot-comes-to-the-web-and-a-revamped-alexa-app/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon’s AI-powered overhaul of its digital assistant, now known as Alexa+, is coming to the web. On Monday, at the start of the Consumer Electronics Show in Las Vegas, the company announced the official launch of a new website, Alexa.com, which is now rolling out to all Alexa+ Early Access customers. The site will allow customers to use Alexa+ online, much as you can do today with other AI chatbots such as ChatGPT or Google’s Gemini.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Alexa-powered devices, including Amazon’s own Echo smart speakers and screens, have a well-established footprint with over 600 million devices sold worldwide, Amazon believes that for its AI assistant to be competitive, it will need to be everywhere — not just in the home, but also on the phone and on the web.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Plus, the expansion could later give anyone a way to interact with Alexa+, even if they don’t have a device in their home.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Related to this expansion, Amazon is updating its Alexa mobile app, which will now offer a more “agent-forward” experience. Or, in other words, it’s putting a chatbot-style interface on the app’s homepage, making it seem more like a typical AI chatbot. (While you could chat with Alexa before in the app, the focus is now on the chatting — while the other features take a backseat.) &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3079554" height="680" src="https://techcrunch.com/wp-content/uploads/2026/01/IMG_0422.jpg?w=335" width="335" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Screenshot of the new Alexa app&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;On the Alexa.com website, customers can use Alexa+ for common tasks, for instance exploring complex topics, creating content, and making trip itineraries. However, Amazon aims to differentiate its assistant from others by focusing on families and their needs in the home. That includes controlling smart devices, as you already could with the original Alexa, but it also means doing things like updating the family’s calendar or to-do list, making dinner reservations, adding grocery items you need to your Amazon Fresh or Whole Foods cart, finding recipes and saving them to a library, or even planning the family movie night with personalized recommendations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;More recently, Amazon has been integrating more services with Alexa+, including the addition of Angi,&amp;nbsp;Expedia,&amp;nbsp;Square, and&amp;nbsp;Yelp, which will join existing apps like Fodor’s, OpenTable, Suno, Ticketmaster, Thumbtack, and Uber.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Alexa.com website features a navigation sidebar for quicker access to your most-used Alexa features, so you can pick up where you left off on tasks like setting the thermostat, checking your calendar for appointments, reviewing shopping lists, and more. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3079552" height="425" src="https://techcrunch.com/wp-content/uploads/2026/01/Alexa.com_shopping-list.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, Amazon aims to convince customers to share their personal documents, emails, and calendar access with Alexa+, so its AI can become a sort of hub to manage the goings-on at home, from kids’ school holidays and soccer schedules to doctor’s appointments and other things families need to remember — like when the dog got its last rabies shot, or what day the neighbor’s backyard BBQ is taking place. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is an area where Amazon will need to stretch, as it doesn’t have its own productivity suite or the wealth of personal data that rivals like Google already have for their own customers. Instead, Amazon has been relying on tools to forward and upload files to Alexa+ for its AI to keep track of. That, too, will now be a feature available on Alexa.com, and the information you share can be displayed on the Echo Show’s screen, where it can also be managed.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This ability to manage a family’s personal data could be Alexa’s biggest selling point, if it gets it right.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Seventy-six percent of what customers are using Alexa+ for no other AI can do,” says Daniel Rausch, VP of Alexa and Echo at Amazon, in an interview with TechCrunch. “And I think that’s a really interesting statistic about Alexa+ for two reasons. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He continues, “One, because customers count on Alexa to do unique things. You know, you can send a photograph of an old family recipe to Alexa and then talk through the recipe as you’re cooking it in your kitchen, substitute ingredients for what you have around the home, and get the job all the way done.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But he notes, another 24% are using Alexa to do things other AIs can do — that could indicate they’re shifting more of their AI usage to Alexa+.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3079551" height="425" src="https://techcrunch.com/wp-content/uploads/2026/01/Alexa.com_smart-home.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Alexa.com will initially only be available to Early Access customers who sign in with their Amazon account. Amazon has been steadily rolling out Early Access since its debut of Alexa+ early last year.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rausch tells us that tens of millions of consumers now have access to Alexa+, and they’re having two to three times more conversations with Alexa+ than they did with the original Alexa assistant. Specifically, they’re shopping three times more with Alexa+ and are using recipes five times more than before, he says. Heavy smart home customers also use Alexa+ 50% more for smart home control, compared with the original Alexa.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, across social media and online forums, there are complaints about Alexa+’s misfires and mistakes. But Rausch believes the complaints are over-represented online. He says that the number of people opting out of the Alexa+ experience after trying it is in the low single digits, on average, or “effectively … almost none.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Ninety-seven percent of Alexa devices support Alexa+, and we see now in adoption from customers that they’re using Alexa across all those many years and many generations of devices,” Rausch adds. “We support all of Alexa’s original capabilities, the tens of thousands of services and devices that Alexa was integrated with already are carried forward to the Alexa+ experience.” &lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon’s AI-powered overhaul of its digital assistant, now known as Alexa+, is coming to the web. On Monday, at the start of the Consumer Electronics Show in Las Vegas, the company announced the official launch of a new website, Alexa.com, which is now rolling out to all Alexa+ Early Access customers. The site will allow customers to use Alexa+ online, much as you can do today with other AI chatbots such as ChatGPT or Google’s Gemini.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Alexa-powered devices, including Amazon’s own Echo smart speakers and screens, have a well-established footprint with over 600 million devices sold worldwide, Amazon believes that for its AI assistant to be competitive, it will need to be everywhere — not just in the home, but also on the phone and on the web.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Plus, the expansion could later give anyone a way to interact with Alexa+, even if they don’t have a device in their home.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Related to this expansion, Amazon is updating its Alexa mobile app, which will now offer a more “agent-forward” experience. Or, in other words, it’s putting a chatbot-style interface on the app’s homepage, making it seem more like a typical AI chatbot. (While you could chat with Alexa before in the app, the focus is now on the chatting — while the other features take a backseat.) &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3079554" height="680" src="https://techcrunch.com/wp-content/uploads/2026/01/IMG_0422.jpg?w=335" width="335" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Screenshot of the new Alexa app&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;On the Alexa.com website, customers can use Alexa+ for common tasks, for instance exploring complex topics, creating content, and making trip itineraries. However, Amazon aims to differentiate its assistant from others by focusing on families and their needs in the home. That includes controlling smart devices, as you already could with the original Alexa, but it also means doing things like updating the family’s calendar or to-do list, making dinner reservations, adding grocery items you need to your Amazon Fresh or Whole Foods cart, finding recipes and saving them to a library, or even planning the family movie night with personalized recommendations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;More recently, Amazon has been integrating more services with Alexa+, including the addition of Angi,&amp;nbsp;Expedia,&amp;nbsp;Square, and&amp;nbsp;Yelp, which will join existing apps like Fodor’s, OpenTable, Suno, Ticketmaster, Thumbtack, and Uber.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Alexa.com website features a navigation sidebar for quicker access to your most-used Alexa features, so you can pick up where you left off on tasks like setting the thermostat, checking your calendar for appointments, reviewing shopping lists, and more. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3079552" height="425" src="https://techcrunch.com/wp-content/uploads/2026/01/Alexa.com_shopping-list.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, Amazon aims to convince customers to share their personal documents, emails, and calendar access with Alexa+, so its AI can become a sort of hub to manage the goings-on at home, from kids’ school holidays and soccer schedules to doctor’s appointments and other things families need to remember — like when the dog got its last rabies shot, or what day the neighbor’s backyard BBQ is taking place. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is an area where Amazon will need to stretch, as it doesn’t have its own productivity suite or the wealth of personal data that rivals like Google already have for their own customers. Instead, Amazon has been relying on tools to forward and upload files to Alexa+ for its AI to keep track of. That, too, will now be a feature available on Alexa.com, and the information you share can be displayed on the Echo Show’s screen, where it can also be managed.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This ability to manage a family’s personal data could be Alexa’s biggest selling point, if it gets it right.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Seventy-six percent of what customers are using Alexa+ for no other AI can do,” says Daniel Rausch, VP of Alexa and Echo at Amazon, in an interview with TechCrunch. “And I think that’s a really interesting statistic about Alexa+ for two reasons. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He continues, “One, because customers count on Alexa to do unique things. You know, you can send a photograph of an old family recipe to Alexa and then talk through the recipe as you’re cooking it in your kitchen, substitute ingredients for what you have around the home, and get the job all the way done.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But he notes, another 24% are using Alexa to do things other AIs can do — that could indicate they’re shifting more of their AI usage to Alexa+.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3079551" height="425" src="https://techcrunch.com/wp-content/uploads/2026/01/Alexa.com_smart-home.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Alexa.com will initially only be available to Early Access customers who sign in with their Amazon account. Amazon has been steadily rolling out Early Access since its debut of Alexa+ early last year.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rausch tells us that tens of millions of consumers now have access to Alexa+, and they’re having two to three times more conversations with Alexa+ than they did with the original Alexa assistant. Specifically, they’re shopping three times more with Alexa+ and are using recipes five times more than before, he says. Heavy smart home customers also use Alexa+ 50% more for smart home control, compared with the original Alexa.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, across social media and online forums, there are complaints about Alexa+’s misfires and mistakes. But Rausch believes the complaints are over-represented online. He says that the number of people opting out of the Alexa+ experience after trying it is in the low single digits, on average, or “effectively … almost none.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Ninety-seven percent of Alexa devices support Alexa+, and we see now in adoption from customers that they’re using Alexa across all those many years and many generations of devices,” Rausch adds. “We support all of Alexa’s original capabilities, the tens of thousands of services and devices that Alexa was integrated with already are carried forward to the Alexa+ experience.” &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/05/alexa-without-an-echo-amazons-ai-chatbot-comes-to-the-web-and-a-revamped-alexa-app/</guid><pubDate>Mon, 05 Jan 2026 15:00:00 +0000</pubDate></item><item><title>[NEW] The overlooked driver of digital transformation (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/01/05/1124662/the-overlooked-driver-of-digital-transformation/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/Genevieve-Juillard-Chris-Schyvinck.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;&lt;span class="sponsoredModule__name--dbd90349922f15155a4c483b397356c2"&gt;Shure&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt;   &lt;p&gt;When business leaders talk about digital transformation, their focus often jumps straight to cloud platforms, AI tools, or collaboration software. Yet, one of the most fundamental enablers of how organizations now work, and how employees experience that work, is often overlooked: audio.&lt;/p&gt;  &lt;p&gt;As Genevieve Juillard, CEO of IDC, notes, the shift to hybrid collaboration made every space, from corporate boardrooms to kitchen tables, meeting-ready almost overnight. In the scramble, audio quality often lagged, creating what research now shows is more than a nuisance. Poor sound can alter how speakers are perceived, making them seem less credible or even less trustworthy.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;"Audio is the gatekeeper of meaning,” stresses Julliard. “If people can't hear clearly, they can't understand you. And if they can't understand you, they can't trust you, and they can't act on what you said. And no amount of sharp video can fix that." Without clarity, comprehension and confidence collapse.&lt;/p&gt;  &lt;p&gt;For Shure, which has spent a century advancing sound technology, the implications extend far beyond convenience. Chris Schyvinck, Shure’s president and CEO, explains that ineffective audio undermines engagement and productivity. Meetings stall, decisions slow, and fatigue builds.&lt;/p&gt; 
 &lt;p&gt;"Use technology to make hybrid meetings seamless, and then be clear on which conversations truly require being in the same physical space," says Juillard. "If you can strike that balance, you're not just making work more efficient, you're making it more sustainable, you're also making it more inclusive, and you're making it more resilient."&lt;/p&gt;  &lt;p&gt;When audio is prioritized on equal footing with video and other collaboration tools, organizations can gain something rare: frictionless communication. That clarity ensures the machines listening in, from AI transcription engines to real-time translation systems, can deliver reliable results.&lt;/p&gt; 
 &lt;p&gt;The research from Shure and IDC highlights two blind spots for leaders. First, buying decisions too often privilege price over quality, with costly consequences in productivity and trust. Second, organizations underestimate the stress poor sound imposes on employees, intensifying the cognitive load of already demanding workdays. Addressing both requires leaders to view audio not as a peripheral expense but as core infrastructure.&lt;/p&gt;  &lt;p&gt;Looking ahead, audio is becoming inseparable from AI-driven collaboration. Smarter systems can already filter out background noise, enhance voices in real time, and integrate seamlessly into hybrid ecosystems.&lt;/p&gt;  &lt;p&gt;"We should be able to provide improved accessibility and a more equitable meeting experience for people," says Schyvinck.&lt;/p&gt;  &lt;p&gt;For Schyvinck and Juillard, the future belongs to companies that treat audio transformation as an integral part of digital transformation, building workplaces that are more sustainable, equitable, and resilient.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;em&gt;This episode of Business Lab is produced in partnership with Shure.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Full Transcript&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan Tatum:&lt;/em&gt; From MIT Technology Review, I'm Megan Tatum, and this is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.&lt;/p&gt;&lt;p&gt;This episode is produced in partnership with Shure.&lt;/p&gt;&lt;p&gt;As companies continue their journeys towards digital transformation, audio modernization is an often overlooked but key component of any successful journey. Clear audio is imperative not only for quality communication, but also for brand equity, both for internal and external stakeholders and even the company as a whole.&lt;/p&gt;&lt;p&gt;Two words for you: audio transformation.&lt;/p&gt;&lt;p&gt;My guests today are Chris Schyvinck, President and CEO at Shure. And Genevieve Juillard, CEO at IDC.&lt;/p&gt;&lt;p&gt;Welcome Chris and Genevieve.&lt;/p&gt;  &lt;p&gt;&lt;br /&gt;&lt;em&gt;Chris Schyvinck:&lt;/em&gt; It's really nice to be here. Thank you very much.&lt;/p&gt; 

 &lt;p&gt;&lt;em&gt;Genevieve Juillard: &lt;/em&gt;Yeah, thank you so much for having us. Great to be here.&lt;/p&gt;  &lt;p&gt;Megan Tatum: Thank you both so much for being here. Genevieve, we could start with you. Let's start with some history perhaps for context. How would you describe the evolution of audio technology and how use cases and our expectations of audio have evolved? What have been some of the major drivers throughout the years and more recently, perhaps would you consider the pandemic to be one of those drivers?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Genevieve: &lt;/em&gt;It's interesting. If you go all the way back to 1976, Norman Macrae of The Economist predicted that video chat would actually kill the office, that people would just work from home. Obviously, that didn't happen then, but the core technology for remote collaboration has actually been around for decades. But until the pandemic, most of us only experienced it in very specific contexts. Offices had dedicated video conferencing rooms and most ran on expensive proprietary systems. And then almost overnight, everything including literally the kitchen table had to be AV ready. The cultural norms shifted just as fast. Before the pandemic, it was perfectly fine to keep your camera off in a meeting, and now that's seen as disengaged or even rude, and that changes what normalized video conferencing and my hybrid meetings.&lt;/p&gt;  &lt;p&gt;But in a rush to equip a suddenly remote workforce, we hit two big problems. Supply chain disruptions and a massive spike in demand. High-quality gear was hard to get so low-quality audio and video became the default. And here's a key point. We now know from research that audio quality matters more than video quality for meeting outcomes. You can run a meeting without video, but you can't run a meeting without clear audio. Audio is the gatekeeper of meaning. If people can't hear clearly, they can't understand you. And if they can't understand you, they can't trust you and they can't act on what you said. And no amount of sharp video can fix that.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Oh, true. It's fascinating, isn't it? And Chris, Shure and IDC recently released some research titled “The Hidden Influencer Rethinking Audio Could Impact Your Organization Today, Tomorrow, and Forever.” The research highlighted that importance of audio that Genevieve's talking about in today's increasingly virtual world. What did you glean from those results and did anything surprise you?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Chris: &lt;/em&gt;Yeah, well, the research certainly confirmed a lot of hunches we've had through the years. When you think about a company like Shure that's been doing audio for 100 years, we just celebrated that anniversary this year.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Congratulations.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;&lt;em&gt;Chris&lt;/em&gt;: &lt;/em&gt;Our legacy business is over more in the music and performance arena. And so just what Genevieve said in terms of, "Yeah, you can have a performance and look at somebody, but that's like 10% of it, right? 90% is hearing that person sing, perform, and talk." We've always, of course, from our perspective, understood that clean, clear, crisp audio is what is needed in any setting. When you translate what's happening on the stage into a meeting or collaboration space at a corporation, we've thought that that is just equally as important.&lt;/p&gt; 
 &lt;p&gt;And we always had this hunch that if people don't have the good audio, they're going to have fatigue, they're going to get a little disengaged, and the whole meeting is going to become quite unproductive. The research just really amplified that hunch for us because it really depicted the fact that people not only get kind of frustrated and disengaged, they might actually start to distrust what the other person with bad audio is saying or just cast it in a different light. And the degree to which that frustration becomes almost personal was very surprising to us. Like I said, it validated some hunches, but it really put an exclamation point on it for us.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;And Genevieve, based on the research results, I understand that IDC pulled together some recommendations for organizations. What is it that leaders need to know and what is the biggest blind spot for them to overcome as well?&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Genevieve: &lt;/em&gt;The biggest blind spot is this. If your microphone has poor audio quality, like Chris said, people will literally perceive you as less intelligent and less trustworthy. And by the way, that's not an opinion. It's what the science says. But yet, when we surveyed first time business buyers, the number one factor they used to choose audio gear was price. However, for repeat buyers, the top factor flipped to audio quality. My guess is they learn the lesson the hard way. The second blind spot is to Chris's point, it's the stress that bad audio creates. Poor sound forces your brain to work harder to decode what's being said. That's a cognitive load and it creates stress. And over a full day of meetings, that stress adds up. Now, we don't have long-term studies yet on the effects, but we do know that prolonged stress is something that every company should be working to reduce.&lt;/p&gt;  &lt;p&gt;Good audio lightens that cognitive load. It keeps people engaged and it levels the playing field. Whether you're in a room or you're halfway across the world, and here's one that's often overlooked, bad audio can sabotage AI transcription tools. As AI becomes more and more central to everyday work, that starts to become really critical. If your audio isn't clear, the transcription won't be accurate. And there's a world of difference between working, for example, the consulting department and the insulting department, and that is an actual example from the field.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;The bottom line is you fix the audio, you cut friction, you save time, and you make meetings more productive.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;I mean, it's just a huge game changer, isn't it, really? I mean, and given that, Chris, in your experience across industries, are audio technologies being included in digital transformation strategies and also artificial intelligence implementation? Do we need a separate audio transformation perhaps?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;&lt;em&gt;Chris&lt;/em&gt;: &lt;/em&gt;Well, like I mentioned earlier, yes, people tend to initially focus on that visual platform, but increasingly the attention to audio is really coming into focus. And I'd hate to tear apart audio as a separate sort of strategy because at the same time, we, as an audio expert, are trying to really seamlessly integrate audio into the rest of the ecosystem. It really does need to be put on an equal footing with the rest of the components in that ecosystem. And to Genevieve's point, as we are seeing audio and video systems with more AI functionalities, the importance of real-time translations that are being used, voice recognition, being able to attribute who said what in a meeting and take action items, it's really, I think starting to elevate the importance of that clear audio. And it's got to be part of a comprehensive, really collaboration plan that helps some company figure out what's their whole digital transformation about. It just really has to be included in that comprehensive plan, but put on equal footing with the rest of the components in that system.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Yeah, absolutely. And in the broader landscape, Genevieve, in terms of discussing the importance of audio quality, what have you noticed across research projects about the effects of good and bad audio, not only from that company perspective, but from employee and client perspectives as well?&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Genevieve: &lt;/em&gt;Well, let's start with employees.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Sure.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Genevieve: &lt;/em&gt;Bad audio adds friction you don't need, we've talked about this. When you're straining to hear or make sense of what's being said, your brain is burning energy on decoding instead of contributing. That frustration, it builds up, and by the end of the day, it hurts productivity. From a company perspective, the stakes get even higher. Meetings are where decisions happen or at least where they're supposed to happen. And if people can't hear clearly, decisions get delayed, mistakes creep in, and the whole process slows down. Poor audio doesn't just waste time, it chips away at the ability to move quickly and confidently. And then there's the client experience. So whether it's in sales, customer service, or any external conversation, poor audio can make you sound less credible and yet less trustworthy. Again, that's not my opinion. That's what the research shows. So that's quite a big risk when you're trying to close a deal or solve a major problem.&lt;/p&gt;  &lt;p&gt;The takeaway is good audio, it matters, it's a multiplier. It makes meetings more productive and it can help decisions happen faster and client interactions be stronger.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;It's just so impactful, isn't it, in so many different ways. I mean, Chris, how are you seeing these research results reflected as companies work through digital and AI transformations? What is it that leaders need to understand about what is involved in audio implementation across their organization?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;&lt;em&gt;Chris&lt;/em&gt;: &lt;/em&gt;Well, like I said earlier, I do think that audio is finally maybe getting its place in the spotlight a little bit up there with our cousins over in the video side. Audio, it's not just a peripheral aspect anymore. It's a very integral part of that sort of comprehensive collaboration plan I was talking about earlier. And when we think about how can we contribute solutions that are really more easy to use for our end users, because if you create something complicated, we were talking about the days gone by of walking into a room. It's a very complicated system, and you need to find the right person that knows how to run it. Increasingly, you just need to have some plug and play kind of solutions. We're thinking about a more sustainable strategy for our solutions where we make really high-quality hardware. We've done that account for a hundred years. People will come up to me and tell the story of the SM58 microphone they bought in 1980 and how they're still using it every day.&lt;/p&gt;  &lt;p&gt;We know how to do that part of it. If somebody is willing to make that investment upfront, put some high-quality hardware into their system, then we are getting to the point now where updates can be handled via software downloads or cloud connectivity. And just really being able to provide sort of a sustainable solution for people over time.&lt;/p&gt;  &lt;p&gt;More in our industry, we're collaborating with other industry partners to go in that direction, make something that's very simple for anybody to walk into a room or on their individual at home setup and do something pretty simple. And I think we have the right industry groups, the right industry associations that can help make sure that the ecosystems have the proper standards, the right kind of ways to make sure everything is interoperable within a system. We're all kind of heading in that direction with that end user in mind.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Fantastic. And when the internet of things was emerging, efforts began to create sort of these data ecosystems, it seems there's an argument to be made that we need audio ecosystems as well. I wonder, Chris, what might an audio ecosystem look like and what would be involved in implementation?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;&lt;em&gt;Chris&lt;/em&gt;: &lt;/em&gt;Well, I think it does have to be part of that bigger ecosystem I was just talking about where we do collaborate with others in industry and we try to make sure that we're all playing by the kind of same set of rules and protocols and standards and whatnot. And when you think about compatibility across all the devices that sit in a room or sit in your, again, maybe your at home setup, making sure that the audio quality is as good as it can be, that you can interoperate with everything else in the system. That's just become very paramount in our day-to-day work here. Your hardware has to be scalable like I just alluded to a moment ago. You have to figure out how you can integrate with existing technologies, different platforms.&lt;/p&gt;  &lt;p&gt;We were joking when we came into this session that when you're going from the platform at your company, maybe you're on Teams and you go into a Zoom setting or you go into a Google setting, you really have to figure out how to adapt to all those different sort of platforms that are out there. I think the ecosystem that we're trying to build, we're trying to be on that equal footing with the rest of the components in that system. And people really do understand that if you want to have extra functionalities in meetings and you want to be able to transcribe or take notes and all of that, that audio is an absolutely critical piece.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Absolutely. And speaking of bit of all those different platforms and use cases, that sort of audio is so relevant to Genevieve that goes back to this idea of in audio one size does not fit all and needs may change. How can companies also plan their audio implementations to be flexible enough to meet current needs and to be able to grow with future advancements?&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;&lt;em&gt;Genevieve:&lt;/em&gt; I'm glad you asked this question. Even years after the pandemic, many companies, they're still trying to get the balance right between remote, in office, how to support it. But even if a company has a strict return to office in-person policy, the reality is that work still isn't going away for that company. They may have teams across cities or countries, clients and external stakeholders will have their own office preferences that they have to adapt to. Supporting hybrid work is actually becoming more important, not less. And our research shows that companies are leaning into, not away from, hybrid setups. About one third of companies are now redesigning or resizing office spaces every single year. For large organizations with multiple sites, staggered leases, that's a moving target. It's really important that they have audio solutions that can work before, during, after all of those changes that they're constantly making. And so that's where flexibility becomes really important. Companies need to buy not just for right now, but for the future.&lt;/p&gt;  &lt;p&gt;And so here's IDC's kind of pro-tip, which is make sure as a company that you go with a provider that offers top-notch audio quality and also has strong partnerships and certifications with the big players and communications technology because that will save you money in the long run. Your systems will stay compatible, your investments will last longer, and you won't be scrambling when that next shift happens.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Of course. And speaking of building for the future, as companies begin to include sustainability in their company goals, Chris, I wonder how can audio play a role in those sustainability efforts and how might that play into perhaps the return on investment in building out a high-quality audio ecosystem?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;&lt;em&gt;Chris&lt;/em&gt;: &lt;/em&gt;Well, I totally agree with what Genevieve just said in terms of hybrid work is not going anywhere. You get all of those big headlines that talk about XYZ company telling people to get back into the office. And I saw a fantastic piece of data just last week that showed the percent of in-office hours of the American workers versus out-of-office remote kind of work. It has basically been flatlined since 2022. This is our new way of working. And of course, like Genevieve mentioned, you have people in all these different locations. And in a strange way, living through the pandemic did teach us that we can do some things by not having to hop on an airplane and travel to go somewhere. Certainly that helps with a more sustainable strategy over time, and you're saving on travel and able to get things done much more quickly.&lt;/p&gt;  &lt;p&gt;And then from a product offering perspective, I'll go back to the vision I was painting earlier where we and others in our industry see that we can create great solid hardware platforms. We've done it for decades, and now that advancements around AI and all of our software that enables products and everything else that has happened in the last probably decade, we can get enhancements and additions and new functionality to people in simpler ways on existing hardware. I think we're all careening down this path of having a much more sustainable ecosystem for all collaboration. It's really quite an exciting time, and that pays off with any company implementing a system, their ROI is going to be much better in the long run.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Absolutely. And Genevieve, what trends around sustainability are you seeing? What opportunities do you see for audio to play into those sustainability efforts going forward?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Genevieve: &lt;/em&gt;Yeah, similar to Chris. In some industries, there's still a belief that the best work happens when everyone's in the same room. And yes, face-to-face time is really important for building relationships, for brainstorming, for closing big deals, but it does come at a cost. The carbon footprint of daily commutes, the sales visits, the constant business travel. And then there's the basic consideration, as we've talked about, of just pure practicality. The good news is with the right AV setup, especially high-quality audio, many of those interactions can happen virtually without losing effectiveness, as Chris said it, but our research shows it.&lt;/p&gt;  &lt;p&gt;Our research shows that virtual meetings can be just as productive as in-person ones, and every commute or flight you avoid, of course makes a measurable sustainability impact. I don't think, personally, that the takeaway is replace all in-person meetings, but instead it's to be intentional. Use technology to make hybrid meetings seamless, and then be clear on which conversations truly require being in the same physical space. If you can strike that balance, you're not just making work more efficient, you're making it more sustainable, you're also making it more inclusive, and you're making it more resilient.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Such an important point. And let's close with a future forward look, if we can. Genevieve, what innovations or advancements in the audio field are you most excited to see to come to fruition, and what potential interesting use cases do you see on the horizon?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Genevieve: &lt;/em&gt;I'm especially interested in how AI and audio are converging. We're now seeing AI that can identify and isolate human voices in noisy environments. For example, right now, there are some jets flying overhead. It's very loud in here, but I suspect you may not even know that that's happening.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;We can't hear a thing. No.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Genevieve: &lt;/em&gt;Right. That technology, it's pulling voices forward so that conversations like ours are crystal clear. And that's a big deal, especially as companies invest more and more in AI tools, especially for that translating, transcribing and summarizing meetings. But as we've talked before, AI is only as good as the audio it hears. If the sound is poor or a word gets misheard, the meaning can shift entirely. And sometimes that's just inconvenient, or it can even be funny. But in really high stakes settings, like healthcare for example, a single mis-transcribed word can have serious consequences. So that's why our position as high quality audio is critical and it's necessary for making AI powered communication accurate, trustworthy, and useful because when the input is clean, the output can actually live up to its promise.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Fantastic. And Chris, finally, what are you most excited to see developed? What advancements are you most looking forward to seeing?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;&lt;em&gt;Chris&lt;/em&gt;: &lt;/em&gt;Well, I really do believe that this is one of the most exciting times that I know I've lived through in my career. Just the pace of how fast technology is moving, the sudden emergence of all things AI. I was actually in a roundtable session of CEOs yesterday from lots of different industries, and the facilitator was talking about change management internally in companies as you're going through all of these technology shifts and some of the fear that people have around AI and things like that. And the facilitator asked each of us to give one word that describes how we're feeling right now. And the first CEO that went used the word dread. And that absolutely floored me because you enter into these eras with some skepticism and trying to figure out how to make things work and go down the right path. But my word was truly optimism.&lt;/p&gt;  &lt;p&gt;When I look at all the ways that we are able to deliver better audio to people more quickly, there's so many opportunities in front of us. We're working on things outside of AI like algorithms that Genevieve just mentioned that filter out the bad sounds that you don't want entering into a meeting. We've been doing that for quite a long time now. There's also opportunities to do real time audio improvements, enhancements, make audio more personal for people. How do they want to be able to very simply, through voice commands perhaps, adjust their audio? There shouldn't have to be a whole lot of techie settings that come along with our solutions.&lt;/p&gt;  &lt;p&gt;We should be able to provide improved accessibility and a little bit more equitable meeting experience for people. And we're looking at tech technology solutions around immersive audio. How can you maybe feel like you're a bit more engaged in the meeting, kind of creating some realistic virtual experiences, if you will. There's just so many opportunities in front of us, and I can just picture a day when you walk into a room and you tell the room, "Hey, call Genevieve. We're going to have a meeting for an hour, and we might need to have Megan on call to come in at a certain time."&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt;&lt;p&gt;And all of this will just be very automatic, very seamless, and we'll be able to see each other and talk at the same time. And this isn't years away. This is happening really, really quickly. And I do think it's a really exciting time for audio and just all together collaboration in our industry.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Absolutely. Sounds like there's plenty of reason to be optimistic. Thank you both so much.&lt;/p&gt;&lt;p&gt;That was Chris Schyvinck, President and CEO at Shure. And Genevieve Juillard, CEO at IDC, whom I spoke with from Brighton, England.&lt;/p&gt;&lt;p&gt;That's it for this episode of Business Lab. I'm your host, Megan Tatum. I'm a contributing editor at Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology, and you can find us in print on the web and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com.&lt;/p&gt;  &lt;p&gt;&lt;br /&gt;This show is available wherever you get your podcasts. And if you enjoy this episode, we hope you'll take a moment to rate and review us. Business Lab is a production of MIT Technology Review, and this episode was produced by Giro Studios. Thanks for listening.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. It was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/Genevieve-Juillard-Chris-Schyvinck.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;&lt;span class="sponsoredModule__name--dbd90349922f15155a4c483b397356c2"&gt;Shure&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt;   &lt;p&gt;When business leaders talk about digital transformation, their focus often jumps straight to cloud platforms, AI tools, or collaboration software. Yet, one of the most fundamental enablers of how organizations now work, and how employees experience that work, is often overlooked: audio.&lt;/p&gt;  &lt;p&gt;As Genevieve Juillard, CEO of IDC, notes, the shift to hybrid collaboration made every space, from corporate boardrooms to kitchen tables, meeting-ready almost overnight. In the scramble, audio quality often lagged, creating what research now shows is more than a nuisance. Poor sound can alter how speakers are perceived, making them seem less credible or even less trustworthy.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;"Audio is the gatekeeper of meaning,” stresses Julliard. “If people can't hear clearly, they can't understand you. And if they can't understand you, they can't trust you, and they can't act on what you said. And no amount of sharp video can fix that." Without clarity, comprehension and confidence collapse.&lt;/p&gt;  &lt;p&gt;For Shure, which has spent a century advancing sound technology, the implications extend far beyond convenience. Chris Schyvinck, Shure’s president and CEO, explains that ineffective audio undermines engagement and productivity. Meetings stall, decisions slow, and fatigue builds.&lt;/p&gt; 
 &lt;p&gt;"Use technology to make hybrid meetings seamless, and then be clear on which conversations truly require being in the same physical space," says Juillard. "If you can strike that balance, you're not just making work more efficient, you're making it more sustainable, you're also making it more inclusive, and you're making it more resilient."&lt;/p&gt;  &lt;p&gt;When audio is prioritized on equal footing with video and other collaboration tools, organizations can gain something rare: frictionless communication. That clarity ensures the machines listening in, from AI transcription engines to real-time translation systems, can deliver reliable results.&lt;/p&gt; 
 &lt;p&gt;The research from Shure and IDC highlights two blind spots for leaders. First, buying decisions too often privilege price over quality, with costly consequences in productivity and trust. Second, organizations underestimate the stress poor sound imposes on employees, intensifying the cognitive load of already demanding workdays. Addressing both requires leaders to view audio not as a peripheral expense but as core infrastructure.&lt;/p&gt;  &lt;p&gt;Looking ahead, audio is becoming inseparable from AI-driven collaboration. Smarter systems can already filter out background noise, enhance voices in real time, and integrate seamlessly into hybrid ecosystems.&lt;/p&gt;  &lt;p&gt;"We should be able to provide improved accessibility and a more equitable meeting experience for people," says Schyvinck.&lt;/p&gt;  &lt;p&gt;For Schyvinck and Juillard, the future belongs to companies that treat audio transformation as an integral part of digital transformation, building workplaces that are more sustainable, equitable, and resilient.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;em&gt;This episode of Business Lab is produced in partnership with Shure.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Full Transcript&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan Tatum:&lt;/em&gt; From MIT Technology Review, I'm Megan Tatum, and this is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.&lt;/p&gt;&lt;p&gt;This episode is produced in partnership with Shure.&lt;/p&gt;&lt;p&gt;As companies continue their journeys towards digital transformation, audio modernization is an often overlooked but key component of any successful journey. Clear audio is imperative not only for quality communication, but also for brand equity, both for internal and external stakeholders and even the company as a whole.&lt;/p&gt;&lt;p&gt;Two words for you: audio transformation.&lt;/p&gt;&lt;p&gt;My guests today are Chris Schyvinck, President and CEO at Shure. And Genevieve Juillard, CEO at IDC.&lt;/p&gt;&lt;p&gt;Welcome Chris and Genevieve.&lt;/p&gt;  &lt;p&gt;&lt;br /&gt;&lt;em&gt;Chris Schyvinck:&lt;/em&gt; It's really nice to be here. Thank you very much.&lt;/p&gt; 

 &lt;p&gt;&lt;em&gt;Genevieve Juillard: &lt;/em&gt;Yeah, thank you so much for having us. Great to be here.&lt;/p&gt;  &lt;p&gt;Megan Tatum: Thank you both so much for being here. Genevieve, we could start with you. Let's start with some history perhaps for context. How would you describe the evolution of audio technology and how use cases and our expectations of audio have evolved? What have been some of the major drivers throughout the years and more recently, perhaps would you consider the pandemic to be one of those drivers?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Genevieve: &lt;/em&gt;It's interesting. If you go all the way back to 1976, Norman Macrae of The Economist predicted that video chat would actually kill the office, that people would just work from home. Obviously, that didn't happen then, but the core technology for remote collaboration has actually been around for decades. But until the pandemic, most of us only experienced it in very specific contexts. Offices had dedicated video conferencing rooms and most ran on expensive proprietary systems. And then almost overnight, everything including literally the kitchen table had to be AV ready. The cultural norms shifted just as fast. Before the pandemic, it was perfectly fine to keep your camera off in a meeting, and now that's seen as disengaged or even rude, and that changes what normalized video conferencing and my hybrid meetings.&lt;/p&gt;  &lt;p&gt;But in a rush to equip a suddenly remote workforce, we hit two big problems. Supply chain disruptions and a massive spike in demand. High-quality gear was hard to get so low-quality audio and video became the default. And here's a key point. We now know from research that audio quality matters more than video quality for meeting outcomes. You can run a meeting without video, but you can't run a meeting without clear audio. Audio is the gatekeeper of meaning. If people can't hear clearly, they can't understand you. And if they can't understand you, they can't trust you and they can't act on what you said. And no amount of sharp video can fix that.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Oh, true. It's fascinating, isn't it? And Chris, Shure and IDC recently released some research titled “The Hidden Influencer Rethinking Audio Could Impact Your Organization Today, Tomorrow, and Forever.” The research highlighted that importance of audio that Genevieve's talking about in today's increasingly virtual world. What did you glean from those results and did anything surprise you?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Chris: &lt;/em&gt;Yeah, well, the research certainly confirmed a lot of hunches we've had through the years. When you think about a company like Shure that's been doing audio for 100 years, we just celebrated that anniversary this year.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Congratulations.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;&lt;em&gt;Chris&lt;/em&gt;: &lt;/em&gt;Our legacy business is over more in the music and performance arena. And so just what Genevieve said in terms of, "Yeah, you can have a performance and look at somebody, but that's like 10% of it, right? 90% is hearing that person sing, perform, and talk." We've always, of course, from our perspective, understood that clean, clear, crisp audio is what is needed in any setting. When you translate what's happening on the stage into a meeting or collaboration space at a corporation, we've thought that that is just equally as important.&lt;/p&gt; 
 &lt;p&gt;And we always had this hunch that if people don't have the good audio, they're going to have fatigue, they're going to get a little disengaged, and the whole meeting is going to become quite unproductive. The research just really amplified that hunch for us because it really depicted the fact that people not only get kind of frustrated and disengaged, they might actually start to distrust what the other person with bad audio is saying or just cast it in a different light. And the degree to which that frustration becomes almost personal was very surprising to us. Like I said, it validated some hunches, but it really put an exclamation point on it for us.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;And Genevieve, based on the research results, I understand that IDC pulled together some recommendations for organizations. What is it that leaders need to know and what is the biggest blind spot for them to overcome as well?&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Genevieve: &lt;/em&gt;The biggest blind spot is this. If your microphone has poor audio quality, like Chris said, people will literally perceive you as less intelligent and less trustworthy. And by the way, that's not an opinion. It's what the science says. But yet, when we surveyed first time business buyers, the number one factor they used to choose audio gear was price. However, for repeat buyers, the top factor flipped to audio quality. My guess is they learn the lesson the hard way. The second blind spot is to Chris's point, it's the stress that bad audio creates. Poor sound forces your brain to work harder to decode what's being said. That's a cognitive load and it creates stress. And over a full day of meetings, that stress adds up. Now, we don't have long-term studies yet on the effects, but we do know that prolonged stress is something that every company should be working to reduce.&lt;/p&gt;  &lt;p&gt;Good audio lightens that cognitive load. It keeps people engaged and it levels the playing field. Whether you're in a room or you're halfway across the world, and here's one that's often overlooked, bad audio can sabotage AI transcription tools. As AI becomes more and more central to everyday work, that starts to become really critical. If your audio isn't clear, the transcription won't be accurate. And there's a world of difference between working, for example, the consulting department and the insulting department, and that is an actual example from the field.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;The bottom line is you fix the audio, you cut friction, you save time, and you make meetings more productive.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;I mean, it's just a huge game changer, isn't it, really? I mean, and given that, Chris, in your experience across industries, are audio technologies being included in digital transformation strategies and also artificial intelligence implementation? Do we need a separate audio transformation perhaps?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;&lt;em&gt;Chris&lt;/em&gt;: &lt;/em&gt;Well, like I mentioned earlier, yes, people tend to initially focus on that visual platform, but increasingly the attention to audio is really coming into focus. And I'd hate to tear apart audio as a separate sort of strategy because at the same time, we, as an audio expert, are trying to really seamlessly integrate audio into the rest of the ecosystem. It really does need to be put on an equal footing with the rest of the components in that ecosystem. And to Genevieve's point, as we are seeing audio and video systems with more AI functionalities, the importance of real-time translations that are being used, voice recognition, being able to attribute who said what in a meeting and take action items, it's really, I think starting to elevate the importance of that clear audio. And it's got to be part of a comprehensive, really collaboration plan that helps some company figure out what's their whole digital transformation about. It just really has to be included in that comprehensive plan, but put on equal footing with the rest of the components in that system.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Yeah, absolutely. And in the broader landscape, Genevieve, in terms of discussing the importance of audio quality, what have you noticed across research projects about the effects of good and bad audio, not only from that company perspective, but from employee and client perspectives as well?&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Genevieve: &lt;/em&gt;Well, let's start with employees.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Sure.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Genevieve: &lt;/em&gt;Bad audio adds friction you don't need, we've talked about this. When you're straining to hear or make sense of what's being said, your brain is burning energy on decoding instead of contributing. That frustration, it builds up, and by the end of the day, it hurts productivity. From a company perspective, the stakes get even higher. Meetings are where decisions happen or at least where they're supposed to happen. And if people can't hear clearly, decisions get delayed, mistakes creep in, and the whole process slows down. Poor audio doesn't just waste time, it chips away at the ability to move quickly and confidently. And then there's the client experience. So whether it's in sales, customer service, or any external conversation, poor audio can make you sound less credible and yet less trustworthy. Again, that's not my opinion. That's what the research shows. So that's quite a big risk when you're trying to close a deal or solve a major problem.&lt;/p&gt;  &lt;p&gt;The takeaway is good audio, it matters, it's a multiplier. It makes meetings more productive and it can help decisions happen faster and client interactions be stronger.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;It's just so impactful, isn't it, in so many different ways. I mean, Chris, how are you seeing these research results reflected as companies work through digital and AI transformations? What is it that leaders need to understand about what is involved in audio implementation across their organization?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;&lt;em&gt;Chris&lt;/em&gt;: &lt;/em&gt;Well, like I said earlier, I do think that audio is finally maybe getting its place in the spotlight a little bit up there with our cousins over in the video side. Audio, it's not just a peripheral aspect anymore. It's a very integral part of that sort of comprehensive collaboration plan I was talking about earlier. And when we think about how can we contribute solutions that are really more easy to use for our end users, because if you create something complicated, we were talking about the days gone by of walking into a room. It's a very complicated system, and you need to find the right person that knows how to run it. Increasingly, you just need to have some plug and play kind of solutions. We're thinking about a more sustainable strategy for our solutions where we make really high-quality hardware. We've done that account for a hundred years. People will come up to me and tell the story of the SM58 microphone they bought in 1980 and how they're still using it every day.&lt;/p&gt;  &lt;p&gt;We know how to do that part of it. If somebody is willing to make that investment upfront, put some high-quality hardware into their system, then we are getting to the point now where updates can be handled via software downloads or cloud connectivity. And just really being able to provide sort of a sustainable solution for people over time.&lt;/p&gt;  &lt;p&gt;More in our industry, we're collaborating with other industry partners to go in that direction, make something that's very simple for anybody to walk into a room or on their individual at home setup and do something pretty simple. And I think we have the right industry groups, the right industry associations that can help make sure that the ecosystems have the proper standards, the right kind of ways to make sure everything is interoperable within a system. We're all kind of heading in that direction with that end user in mind.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Fantastic. And when the internet of things was emerging, efforts began to create sort of these data ecosystems, it seems there's an argument to be made that we need audio ecosystems as well. I wonder, Chris, what might an audio ecosystem look like and what would be involved in implementation?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;&lt;em&gt;Chris&lt;/em&gt;: &lt;/em&gt;Well, I think it does have to be part of that bigger ecosystem I was just talking about where we do collaborate with others in industry and we try to make sure that we're all playing by the kind of same set of rules and protocols and standards and whatnot. And when you think about compatibility across all the devices that sit in a room or sit in your, again, maybe your at home setup, making sure that the audio quality is as good as it can be, that you can interoperate with everything else in the system. That's just become very paramount in our day-to-day work here. Your hardware has to be scalable like I just alluded to a moment ago. You have to figure out how you can integrate with existing technologies, different platforms.&lt;/p&gt;  &lt;p&gt;We were joking when we came into this session that when you're going from the platform at your company, maybe you're on Teams and you go into a Zoom setting or you go into a Google setting, you really have to figure out how to adapt to all those different sort of platforms that are out there. I think the ecosystem that we're trying to build, we're trying to be on that equal footing with the rest of the components in that system. And people really do understand that if you want to have extra functionalities in meetings and you want to be able to transcribe or take notes and all of that, that audio is an absolutely critical piece.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Absolutely. And speaking of bit of all those different platforms and use cases, that sort of audio is so relevant to Genevieve that goes back to this idea of in audio one size does not fit all and needs may change. How can companies also plan their audio implementations to be flexible enough to meet current needs and to be able to grow with future advancements?&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;&lt;em&gt;Genevieve:&lt;/em&gt; I'm glad you asked this question. Even years after the pandemic, many companies, they're still trying to get the balance right between remote, in office, how to support it. But even if a company has a strict return to office in-person policy, the reality is that work still isn't going away for that company. They may have teams across cities or countries, clients and external stakeholders will have their own office preferences that they have to adapt to. Supporting hybrid work is actually becoming more important, not less. And our research shows that companies are leaning into, not away from, hybrid setups. About one third of companies are now redesigning or resizing office spaces every single year. For large organizations with multiple sites, staggered leases, that's a moving target. It's really important that they have audio solutions that can work before, during, after all of those changes that they're constantly making. And so that's where flexibility becomes really important. Companies need to buy not just for right now, but for the future.&lt;/p&gt;  &lt;p&gt;And so here's IDC's kind of pro-tip, which is make sure as a company that you go with a provider that offers top-notch audio quality and also has strong partnerships and certifications with the big players and communications technology because that will save you money in the long run. Your systems will stay compatible, your investments will last longer, and you won't be scrambling when that next shift happens.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Of course. And speaking of building for the future, as companies begin to include sustainability in their company goals, Chris, I wonder how can audio play a role in those sustainability efforts and how might that play into perhaps the return on investment in building out a high-quality audio ecosystem?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;&lt;em&gt;Chris&lt;/em&gt;: &lt;/em&gt;Well, I totally agree with what Genevieve just said in terms of hybrid work is not going anywhere. You get all of those big headlines that talk about XYZ company telling people to get back into the office. And I saw a fantastic piece of data just last week that showed the percent of in-office hours of the American workers versus out-of-office remote kind of work. It has basically been flatlined since 2022. This is our new way of working. And of course, like Genevieve mentioned, you have people in all these different locations. And in a strange way, living through the pandemic did teach us that we can do some things by not having to hop on an airplane and travel to go somewhere. Certainly that helps with a more sustainable strategy over time, and you're saving on travel and able to get things done much more quickly.&lt;/p&gt;  &lt;p&gt;And then from a product offering perspective, I'll go back to the vision I was painting earlier where we and others in our industry see that we can create great solid hardware platforms. We've done it for decades, and now that advancements around AI and all of our software that enables products and everything else that has happened in the last probably decade, we can get enhancements and additions and new functionality to people in simpler ways on existing hardware. I think we're all careening down this path of having a much more sustainable ecosystem for all collaboration. It's really quite an exciting time, and that pays off with any company implementing a system, their ROI is going to be much better in the long run.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Absolutely. And Genevieve, what trends around sustainability are you seeing? What opportunities do you see for audio to play into those sustainability efforts going forward?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Genevieve: &lt;/em&gt;Yeah, similar to Chris. In some industries, there's still a belief that the best work happens when everyone's in the same room. And yes, face-to-face time is really important for building relationships, for brainstorming, for closing big deals, but it does come at a cost. The carbon footprint of daily commutes, the sales visits, the constant business travel. And then there's the basic consideration, as we've talked about, of just pure practicality. The good news is with the right AV setup, especially high-quality audio, many of those interactions can happen virtually without losing effectiveness, as Chris said it, but our research shows it.&lt;/p&gt;  &lt;p&gt;Our research shows that virtual meetings can be just as productive as in-person ones, and every commute or flight you avoid, of course makes a measurable sustainability impact. I don't think, personally, that the takeaway is replace all in-person meetings, but instead it's to be intentional. Use technology to make hybrid meetings seamless, and then be clear on which conversations truly require being in the same physical space. If you can strike that balance, you're not just making work more efficient, you're making it more sustainable, you're also making it more inclusive, and you're making it more resilient.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Such an important point. And let's close with a future forward look, if we can. Genevieve, what innovations or advancements in the audio field are you most excited to see to come to fruition, and what potential interesting use cases do you see on the horizon?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Genevieve: &lt;/em&gt;I'm especially interested in how AI and audio are converging. We're now seeing AI that can identify and isolate human voices in noisy environments. For example, right now, there are some jets flying overhead. It's very loud in here, but I suspect you may not even know that that's happening.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;We can't hear a thing. No.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Genevieve: &lt;/em&gt;Right. That technology, it's pulling voices forward so that conversations like ours are crystal clear. And that's a big deal, especially as companies invest more and more in AI tools, especially for that translating, transcribing and summarizing meetings. But as we've talked before, AI is only as good as the audio it hears. If the sound is poor or a word gets misheard, the meaning can shift entirely. And sometimes that's just inconvenient, or it can even be funny. But in really high stakes settings, like healthcare for example, a single mis-transcribed word can have serious consequences. So that's why our position as high quality audio is critical and it's necessary for making AI powered communication accurate, trustworthy, and useful because when the input is clean, the output can actually live up to its promise.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Fantastic. And Chris, finally, what are you most excited to see developed? What advancements are you most looking forward to seeing?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;&lt;em&gt;Chris&lt;/em&gt;: &lt;/em&gt;Well, I really do believe that this is one of the most exciting times that I know I've lived through in my career. Just the pace of how fast technology is moving, the sudden emergence of all things AI. I was actually in a roundtable session of CEOs yesterday from lots of different industries, and the facilitator was talking about change management internally in companies as you're going through all of these technology shifts and some of the fear that people have around AI and things like that. And the facilitator asked each of us to give one word that describes how we're feeling right now. And the first CEO that went used the word dread. And that absolutely floored me because you enter into these eras with some skepticism and trying to figure out how to make things work and go down the right path. But my word was truly optimism.&lt;/p&gt;  &lt;p&gt;When I look at all the ways that we are able to deliver better audio to people more quickly, there's so many opportunities in front of us. We're working on things outside of AI like algorithms that Genevieve just mentioned that filter out the bad sounds that you don't want entering into a meeting. We've been doing that for quite a long time now. There's also opportunities to do real time audio improvements, enhancements, make audio more personal for people. How do they want to be able to very simply, through voice commands perhaps, adjust their audio? There shouldn't have to be a whole lot of techie settings that come along with our solutions.&lt;/p&gt;  &lt;p&gt;We should be able to provide improved accessibility and a little bit more equitable meeting experience for people. And we're looking at tech technology solutions around immersive audio. How can you maybe feel like you're a bit more engaged in the meeting, kind of creating some realistic virtual experiences, if you will. There's just so many opportunities in front of us, and I can just picture a day when you walk into a room and you tell the room, "Hey, call Genevieve. We're going to have a meeting for an hour, and we might need to have Megan on call to come in at a certain time."&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt;&lt;p&gt;And all of this will just be very automatic, very seamless, and we'll be able to see each other and talk at the same time. And this isn't years away. This is happening really, really quickly. And I do think it's a really exciting time for audio and just all together collaboration in our industry.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Absolutely. Sounds like there's plenty of reason to be optimistic. Thank you both so much.&lt;/p&gt;&lt;p&gt;That was Chris Schyvinck, President and CEO at Shure. And Genevieve Juillard, CEO at IDC, whom I spoke with from Brighton, England.&lt;/p&gt;&lt;p&gt;That's it for this episode of Business Lab. I'm your host, Megan Tatum. I'm a contributing editor at Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology, and you can find us in print on the web and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com.&lt;/p&gt;  &lt;p&gt;&lt;br /&gt;This show is available wherever you get your podcasts. And if you enjoy this episode, we hope you'll take a moment to rate and review us. Business Lab is a production of MIT Technology Review, and this episode was produced by Giro Studios. Thanks for listening.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. It was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/01/05/1124662/the-overlooked-driver-of-digital-transformation/</guid><pubDate>Mon, 05 Jan 2026 16:00:00 +0000</pubDate></item><item><title>[NEW] CES 2026: Follow live as NVIDIA, Lego, AMD, Amazon, and more make their big reveals (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/storyline/ces-2026-follow-live-as-nvidia-lego-amd-amazon-and-more-make-their-big-reveals/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/GettyImages-2254142389-1.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-techcrunch-storyline-hero__excerpt"&gt;CES 2026, the annual consumer tech conference held in Las Vegas is here. And lucky for you, we have TechCrunch editors and reporters on the ground to cover the news, scout out the interesting, weird, and relevant (and some not so relevant) tech, and of course the people working on it. Even before the official kickoff of CES on January 6, there will be product reveals from Amazon, Nvidia, Hyundai, AMD, and more. AI is at the center of most of the action. But this year, we’re hearing a lot more about the convergence of AI and the physical world, whether it’s in factories, robotics, or autonomous vehicles. Follow our live updates as we share all the reveals and new hardware as it happens.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/GettyImages-2254142389-1.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-techcrunch-storyline-hero__excerpt"&gt;CES 2026, the annual consumer tech conference held in Las Vegas is here. And lucky for you, we have TechCrunch editors and reporters on the ground to cover the news, scout out the interesting, weird, and relevant (and some not so relevant) tech, and of course the people working on it. Even before the official kickoff of CES on January 6, there will be product reveals from Amazon, Nvidia, Hyundai, AMD, and more. AI is at the center of most of the action. But this year, we’re hearing a lot more about the convergence of AI and the physical world, whether it’s in factories, robotics, or autonomous vehicles. Follow our live updates as we share all the reveals and new hardware as it happens.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/storyline/ces-2026-follow-live-as-nvidia-lego-amd-amazon-and-more-make-their-big-reveals/</guid><pubDate>Mon, 05 Jan 2026 16:09:26 +0000</pubDate></item><item><title>[NEW] X blames users for Grok-generated CSAM; no fixes announced (AI - Ars Technica)</title><link>https://arstechnica.com/tech-policy/2026/01/x-blames-users-for-grok-generated-csam-no-fixes-announced/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Critics call for App Store ban after Grok sexualized images of minors.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="426" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2233914159-640x426.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2233914159-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          NurPhoto / Contributor | NurPhoto

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;It seems that instead of updating Grok to prevent outputs of sexualized images of minors, X is planning to purge users generating content that the platform deems illegal, including Grok-generated child sexual abuse material (CSAM).&lt;/p&gt;
&lt;p&gt;On Saturday, X Safety finally posted an official response after nearly a week of backlash over Grok outputs that sexualized real people without consent. Offering no apology for Grok’s functionality, X Safety blamed users for prompting Grok to produce CSAM while reminding them that such prompts can trigger account suspensions and possible legal consequences.&lt;/p&gt;
&lt;p&gt;“We take action against illegal content on X, including Child Sexual Abuse Material (CSAM), by removing it, permanently suspending accounts, and working with local governments and law enforcement as necessary,” X Safety said. “Anyone using or prompting Grok to make illegal content will suffer the same consequences as if they upload illegal content.”&lt;/p&gt;
&lt;p&gt;X Safety’s post boosted a reply on another thread on the platform in which X owner Elon Musk reiterated the consequences users face for inappropriate prompting. That reply came to a post from an X user, DogeDesigner, who suggested that Grok can’t be blamed for “creating inappropriate images,” despite Grok determining its own outputs.&lt;/p&gt;
&lt;p&gt;“That’s like blaming a pen for writing something bad,” DogeDesigner opined. “A pen doesn’t decide what gets written. The person holding it does. Grok works the same way. What you get depends a lot on what you put in.”&lt;/p&gt;
&lt;p&gt;But image generators like Grok aren’t forced to output exactly what the user wants, like a pen. One of the reasons the Copyright Office won’t allow AI-generated works to be registered is the lack of human agency in determining what AI image generators spit out. Chatbots are similarly non-deterministic, generating different outputs for the same prompt.&lt;/p&gt;
&lt;p&gt;That’s why, for many users questioning why X won’t filter out CSAM in response to Grok’s generations, X’s response seems to stop well short of fixing the problem by only holding users responsible for outputs.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In a comment on the DogeDesigner thread, a computer programmer pointed out that X users may inadvertently generate inappropriate images—back in August, for example, Grok generated nudes of Taylor Swift without being asked. Those users can’t even delete problematic images from the Grok account to prevent them from spreading, the programmer noted. In that scenario, the X user could risk account suspension or legal liability if law enforcement intervened, X Safety’s response suggested, without X ever facing accountability for unexpected outputs.&lt;/p&gt;
&lt;p&gt;X did not immediately respond to Ars’ request to clarify if any updates were made to Grok following the CSAM controversy. Many media outlets weirdly took Grok at its word when the chatbot responded to prompts demanding an apology by claiming that X would be improving its safeguards. But X Safety’s response now seems to contradict the chatbot, which, as Ars noted last week, should never be considered reliable as a spokesperson.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;While X’s response continues to disappoint critics, some top commenters on the X Safety post have called for Apple to take action if X won’t. They suggested that X may be violating App Store rules against apps allowing user-generated content that objectifies real people. Until Grok starts transparently filtering out CSAM or other outputs “undressing” real people without their consent, the chatbot and X should be banned, critics said.&lt;/p&gt;
&lt;p&gt;An App Store ban would likely infuriate Musk, who last year sued Apple, partly over his frustrations that the App Store never put Grok on its “Must Have” apps list. In that ongoing lawsuit, Musk alleged that Apple’s supposed favoring of ChatGPT in the App Store made it impossible for Grok to catch up in the chatbot market. That suggests that an App Store ban would potentially doom Grok’s quest to overtake ChatGPT’s lead.&lt;/p&gt;
&lt;p&gt;Apple did not immediately respond to Ars’ request to comment on whether Grok’s outputs or current functionality violate App Store rules.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;No one knows how X plans to purge bad prompters&lt;/h2&gt;
&lt;p&gt;While some users are focused on how X can hold users responsible for Grok’s outputs when X is the one training the model, others are questioning how exactly X plans to moderate illegal content that Grok seems capable of generating.&lt;/p&gt;
&lt;p&gt;X is so far more transparent about how it moderates CSAM posted to the platform. Last September, X Safety reported that it has “a zero tolerance policy towards CSAM content,” the majority of which is “automatically” detected using proprietary hash technology to proactively flag known CSAM.&lt;/p&gt;
&lt;p&gt;Under this system, more than 4.5 million accounts were suspended last year, and X reported “hundreds of thousands” of images to the National Center for Missing and Exploited Children (NCMEC). The next month, X Head of Safety Kylie McRoberts confirmed that “in 2024, 309 reports made by X to NCMEC led to arrests and subsequent convictions in 10 cases,” and in the first half of 2025, “170 reports led to arrests.”&lt;/p&gt;
&lt;p&gt;“When we identify apparent CSAM material, we act swiftly, and in the majority of cases permanently suspend the account which automatically removes the content from our platform,” X Safety said. “We then report the account to the NCMEC, which works with law enforcement globally—including in the UK—to pursue justice and protect children.”&lt;/p&gt;
&lt;p&gt;At that time, X promised to “remain steadfast” in its “mission to eradicate CSAM,” but if left unchecked, Grok’s harmful outputs risk creating new kinds of CSAM that this system wouldn’t automatically detect. On X, some users suggested the platform should increase reporting mechanisms to help flag potentially illegal Grok outputs.&lt;/p&gt;
&lt;p&gt;Another troublingly vague aspect of X Safety’s response is the definitions that X is using for illegal content or CSAM, some X users suggested. Across the platform, not everybody agrees on what’s harmful. Some critics are disturbed by Grok generating bikini images that sexualize public figures, including doctors or lawyers, without their consent, while others, including Musk, consider making bikini images to be a joke.&lt;/p&gt;
&lt;p&gt;Where exactly X draws the line on AI-generated CSAM could determine whether images are quickly removed or whether repeat offenders are detected and suspended. Any accounts or content left unchecked could potentially traumatize real kids whose images may be used to prompt Grok. And if Grok should ever be used to flood the Internet with fake CSAM, recent history suggests that it could make it harder for law enforcement to investigate real child abuse cases.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Critics call for App Store ban after Grok sexualized images of minors.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="426" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2233914159-640x426.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2233914159-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          NurPhoto / Contributor | NurPhoto

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;It seems that instead of updating Grok to prevent outputs of sexualized images of minors, X is planning to purge users generating content that the platform deems illegal, including Grok-generated child sexual abuse material (CSAM).&lt;/p&gt;
&lt;p&gt;On Saturday, X Safety finally posted an official response after nearly a week of backlash over Grok outputs that sexualized real people without consent. Offering no apology for Grok’s functionality, X Safety blamed users for prompting Grok to produce CSAM while reminding them that such prompts can trigger account suspensions and possible legal consequences.&lt;/p&gt;
&lt;p&gt;“We take action against illegal content on X, including Child Sexual Abuse Material (CSAM), by removing it, permanently suspending accounts, and working with local governments and law enforcement as necessary,” X Safety said. “Anyone using or prompting Grok to make illegal content will suffer the same consequences as if they upload illegal content.”&lt;/p&gt;
&lt;p&gt;X Safety’s post boosted a reply on another thread on the platform in which X owner Elon Musk reiterated the consequences users face for inappropriate prompting. That reply came to a post from an X user, DogeDesigner, who suggested that Grok can’t be blamed for “creating inappropriate images,” despite Grok determining its own outputs.&lt;/p&gt;
&lt;p&gt;“That’s like blaming a pen for writing something bad,” DogeDesigner opined. “A pen doesn’t decide what gets written. The person holding it does. Grok works the same way. What you get depends a lot on what you put in.”&lt;/p&gt;
&lt;p&gt;But image generators like Grok aren’t forced to output exactly what the user wants, like a pen. One of the reasons the Copyright Office won’t allow AI-generated works to be registered is the lack of human agency in determining what AI image generators spit out. Chatbots are similarly non-deterministic, generating different outputs for the same prompt.&lt;/p&gt;
&lt;p&gt;That’s why, for many users questioning why X won’t filter out CSAM in response to Grok’s generations, X’s response seems to stop well short of fixing the problem by only holding users responsible for outputs.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In a comment on the DogeDesigner thread, a computer programmer pointed out that X users may inadvertently generate inappropriate images—back in August, for example, Grok generated nudes of Taylor Swift without being asked. Those users can’t even delete problematic images from the Grok account to prevent them from spreading, the programmer noted. In that scenario, the X user could risk account suspension or legal liability if law enforcement intervened, X Safety’s response suggested, without X ever facing accountability for unexpected outputs.&lt;/p&gt;
&lt;p&gt;X did not immediately respond to Ars’ request to clarify if any updates were made to Grok following the CSAM controversy. Many media outlets weirdly took Grok at its word when the chatbot responded to prompts demanding an apology by claiming that X would be improving its safeguards. But X Safety’s response now seems to contradict the chatbot, which, as Ars noted last week, should never be considered reliable as a spokesperson.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;While X’s response continues to disappoint critics, some top commenters on the X Safety post have called for Apple to take action if X won’t. They suggested that X may be violating App Store rules against apps allowing user-generated content that objectifies real people. Until Grok starts transparently filtering out CSAM or other outputs “undressing” real people without their consent, the chatbot and X should be banned, critics said.&lt;/p&gt;
&lt;p&gt;An App Store ban would likely infuriate Musk, who last year sued Apple, partly over his frustrations that the App Store never put Grok on its “Must Have” apps list. In that ongoing lawsuit, Musk alleged that Apple’s supposed favoring of ChatGPT in the App Store made it impossible for Grok to catch up in the chatbot market. That suggests that an App Store ban would potentially doom Grok’s quest to overtake ChatGPT’s lead.&lt;/p&gt;
&lt;p&gt;Apple did not immediately respond to Ars’ request to comment on whether Grok’s outputs or current functionality violate App Store rules.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;No one knows how X plans to purge bad prompters&lt;/h2&gt;
&lt;p&gt;While some users are focused on how X can hold users responsible for Grok’s outputs when X is the one training the model, others are questioning how exactly X plans to moderate illegal content that Grok seems capable of generating.&lt;/p&gt;
&lt;p&gt;X is so far more transparent about how it moderates CSAM posted to the platform. Last September, X Safety reported that it has “a zero tolerance policy towards CSAM content,” the majority of which is “automatically” detected using proprietary hash technology to proactively flag known CSAM.&lt;/p&gt;
&lt;p&gt;Under this system, more than 4.5 million accounts were suspended last year, and X reported “hundreds of thousands” of images to the National Center for Missing and Exploited Children (NCMEC). The next month, X Head of Safety Kylie McRoberts confirmed that “in 2024, 309 reports made by X to NCMEC led to arrests and subsequent convictions in 10 cases,” and in the first half of 2025, “170 reports led to arrests.”&lt;/p&gt;
&lt;p&gt;“When we identify apparent CSAM material, we act swiftly, and in the majority of cases permanently suspend the account which automatically removes the content from our platform,” X Safety said. “We then report the account to the NCMEC, which works with law enforcement globally—including in the UK—to pursue justice and protect children.”&lt;/p&gt;
&lt;p&gt;At that time, X promised to “remain steadfast” in its “mission to eradicate CSAM,” but if left unchecked, Grok’s harmful outputs risk creating new kinds of CSAM that this system wouldn’t automatically detect. On X, some users suggested the platform should increase reporting mechanisms to help flag potentially illegal Grok outputs.&lt;/p&gt;
&lt;p&gt;Another troublingly vague aspect of X Safety’s response is the definitions that X is using for illegal content or CSAM, some X users suggested. Across the platform, not everybody agrees on what’s harmful. Some critics are disturbed by Grok generating bikini images that sexualize public figures, including doctors or lawyers, without their consent, while others, including Musk, consider making bikini images to be a joke.&lt;/p&gt;
&lt;p&gt;Where exactly X draws the line on AI-generated CSAM could determine whether images are quickly removed or whether repeat offenders are detected and suspended. Any accounts or content left unchecked could potentially traumatize real kids whose images may be used to prompt Grok. And if Grok should ever be used to flood the Internet with fake CSAM, recent history suggests that it could make it harder for law enforcement to investigate real child abuse cases.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2026/01/x-blames-users-for-grok-generated-csam-no-fixes-announced/</guid><pubDate>Mon, 05 Jan 2026 17:42:45 +0000</pubDate></item><item><title>[NEW] Google TV’s big Gemini update adds image and video generation, voice control for settings (AI - Ars Technica)</title><link>https://arstechnica.com/google/2026/01/gemini-expands-on-google-tv-bringing-nano-banana-and-veo-models-to-your-tv/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google TV will let you generate and watch AI content on the big screen.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Gemini Google TV" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/We_previewed_new_Gemini_for_Google_TV-640x360.png" width="640" /&gt;
                  &lt;img alt="Gemini Google TV" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/We_previewed_new_Gemini_for_Google_TV-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Soon, even loafing around on the couch won’t help you steer clear of AI. TV makers are busily integrating AI models into the experience, and Google is no different. At CES, the company announced a big expansion of Gemini features on the Google TV platform, starting with TCL smart TVs.&lt;/p&gt;
&lt;p&gt;Google began integrating Gemini with the TV Streamer box this past fall, but the new expansion brings some of the company’s most popular AI features to TVs: Nano Banana (image) and Veo (video), which offered a huge leap in visual fidelity at launch and have only improved with subsequent updates. Both models will be part of the TV experience, allowing users to modify or create new content.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2133833 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="TV photos remix" class="fullwidth full" height="540" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Google-Photos-Remix-copy.jpg" width="960" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Google Photos AI remixing in Google TV.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The Google TV platform connects to Google Photos, allowing Gemini to access those images with your approval. Gemini can generate a slideshow of your choosing on the spot, but it can also feed those images into Veo or Nano Banana. Using Gemini voice controls, you can remix a photo or turn a still image into a video. You can also enter a solo prompt to generate a totally new image or video with Google’s AI on your TV.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;That might be a fun distraction, but it’s not a core TV experience. Google’s image and video models are good enough that you might gain some benefit from monkeying around with them on a larger screen, but Gemini is also available for more general tasks.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2133834 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Veo in Google TV" class="fullwidth full" height="540" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Nano-Banana-and-Veo-3-1-copy.jpg" width="960" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Google TV will support generating new images and videos with Google’s AI models.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;This update brings a full chatbot-like experience to TVs. If you want to catch up on sports scores or get recommendations for what to watch, you can ask the robot. The outputs might be a little different from what you would expect from using Gemini on the web or in an app. Google says it has devised a “visually rich framework” that will make the AI more usable on a TV. There will also be a “Dive Deeper” option in each response to generate an interactive overview of the topic.&lt;/p&gt;
&lt;p&gt;Gemini can also take action to tweak system settings based on your complaints. For example, pull up Gemini and say “the dialog is too quiet” and watch as the AI makes adjustments to address that.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2133835 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Gemini chatbot Google TV" class="fullwidth full" height="2160" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Gemini-Entertainment-Response-copy.jpg" width="3840" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Gemini’s replies on Google TV will be more visual.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The new Gemini features will debut on TCL TVs that run Google TV, but most other devices, even Google’s own TV Streamer, will have to wait a few months. Even then, you won’t see Gemini taking over every TV or streaming box with Google’s software. The new Gemini features require the full Google TV experience with Android OS version 14 or higher.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google TV will let you generate and watch AI content on the big screen.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Gemini Google TV" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/We_previewed_new_Gemini_for_Google_TV-640x360.png" width="640" /&gt;
                  &lt;img alt="Gemini Google TV" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/We_previewed_new_Gemini_for_Google_TV-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Soon, even loafing around on the couch won’t help you steer clear of AI. TV makers are busily integrating AI models into the experience, and Google is no different. At CES, the company announced a big expansion of Gemini features on the Google TV platform, starting with TCL smart TVs.&lt;/p&gt;
&lt;p&gt;Google began integrating Gemini with the TV Streamer box this past fall, but the new expansion brings some of the company’s most popular AI features to TVs: Nano Banana (image) and Veo (video), which offered a huge leap in visual fidelity at launch and have only improved with subsequent updates. Both models will be part of the TV experience, allowing users to modify or create new content.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2133833 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="TV photos remix" class="fullwidth full" height="540" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Google-Photos-Remix-copy.jpg" width="960" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Google Photos AI remixing in Google TV.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The Google TV platform connects to Google Photos, allowing Gemini to access those images with your approval. Gemini can generate a slideshow of your choosing on the spot, but it can also feed those images into Veo or Nano Banana. Using Gemini voice controls, you can remix a photo or turn a still image into a video. You can also enter a solo prompt to generate a totally new image or video with Google’s AI on your TV.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;That might be a fun distraction, but it’s not a core TV experience. Google’s image and video models are good enough that you might gain some benefit from monkeying around with them on a larger screen, but Gemini is also available for more general tasks.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2133834 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Veo in Google TV" class="fullwidth full" height="540" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Nano-Banana-and-Veo-3-1-copy.jpg" width="960" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Google TV will support generating new images and videos with Google’s AI models.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;This update brings a full chatbot-like experience to TVs. If you want to catch up on sports scores or get recommendations for what to watch, you can ask the robot. The outputs might be a little different from what you would expect from using Gemini on the web or in an app. Google says it has devised a “visually rich framework” that will make the AI more usable on a TV. There will also be a “Dive Deeper” option in each response to generate an interactive overview of the topic.&lt;/p&gt;
&lt;p&gt;Gemini can also take action to tweak system settings based on your complaints. For example, pull up Gemini and say “the dialog is too quiet” and watch as the AI makes adjustments to address that.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2133835 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Gemini chatbot Google TV" class="fullwidth full" height="2160" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Gemini-Entertainment-Response-copy.jpg" width="3840" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Gemini’s replies on Google TV will be more visual.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The new Gemini features will debut on TCL TVs that run Google TV, but most other devices, even Google’s own TV Streamer, will have to wait a few months. Even then, you won’t see Gemini taking over every TV or streaming box with Google’s software. The new Gemini features require the full Google TV experience with Android OS version 14 or higher.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2026/01/gemini-expands-on-google-tv-bringing-nano-banana-and-veo-models-to-your-tv/</guid><pubDate>Mon, 05 Jan 2026 17:51:36 +0000</pubDate></item></channel></rss>