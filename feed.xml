<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 02 Jul 2025 12:45:21 +0000</lastBuildDate><item><title>[NEW] How generative AI could help make construction sites safer (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/07/02/1119607/how-generative-ai-could-help-make-construction-sites-safer/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Last winter, during the construction of an affordable housing project on Martha’s Vineyard, Massachusetts, a 32-year-old worker named Jose Luis Collaguazo Crespo slipped off a ladder on the second floor and plunged to his death in the basement. He was one of more than&amp;nbsp;1,000&amp;nbsp;construction&amp;nbsp;workers who die on the job each year in the US, making it the most dangerous industry for fatal slips, trips, and falls.&lt;/p&gt;  &lt;p&gt;“Everyone talks about [how] ‘safety is the number-one priority,’” entrepreneur and executive Philip Lorenzo said during a presentation at Construction Innovation Day 2025, a conference at the University of California, Berkeley, in April.&amp;nbsp;“But then maybe internally, it’s not that high priority. People take shortcuts on job sites. And so there’s this whole tug-of-war between … safety and productivity.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;To combat the shortcuts and risk-taking, Lorenzo is working on a tool for the San Francisco–based company DroneDeploy&lt;strong&gt;, &lt;/strong&gt;which sells software that creates daily digital models of&amp;nbsp;work progress from videos and images, known in the trade as “reality capture.”&amp;nbsp; The tool, called Safety AI, analyzes each day’s reality capture imagery and flags conditions that violate Occupational Safety and Health Administration (OSHA) rules, with what he claims is 95% accuracy. &lt;/p&gt;  &lt;p&gt;That means that for any safety risk the software flags, there is 95% certainty that the flag is accurate and relates to a specific OSHA regulation. Launched in October 2024, it’s now being deployed on hundreds of construction sites in the US, Lorenzo says, and versions specific to the building regulations in countries including Canada, the UK, South Korea, and Australia have also been deployed.&lt;/p&gt; 
 &lt;p&gt;Safety AI is one of multiple AI&amp;nbsp;construction&amp;nbsp;safety tools that have emerged in recent years, from&amp;nbsp;Silicon Valley&amp;nbsp;to&amp;nbsp;Hong Kong to&amp;nbsp;Jerusalem. Many of these rely on teams of human “clickers,” often in low-wage countries, to manually draw bounding boxes around images of key objects like ladders, in order to label large volumes of data to train an algorithm. &lt;/p&gt;  &lt;p&gt;Lorenzo says Safety AI is the first one to use generative AI to flag safety violations, which means an algorithm that can do more than recognize objects such as ladders or hard hats. The software can “reason” about what is going on in an image of a site and draw a conclusion about whether there is an OSHA violation. This is a more advanced form of analysis than the object detection that is the current industry standard, Lorenzo claims. But as the 95% success rate suggests, Safety AI is not a flawless and all-knowing intelligence. It requires an experienced safety inspector as an overseer.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;A visual language model in the real world&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Robots and AI tend to thrive in controlled, largely static environments, like factory floors or shipping terminals.&amp;nbsp;But construction&amp;nbsp;sites are, by definition, changing a little bit every day.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Lorenzo thinks he’s built a better way to monitor sites, using a type of generative AI called a visual language model, or VLM. A VLM is an LLM with a vision encoder, allowing it to “see” images of the world and analyze what is going on in the scene.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Using years of reality capture imagery gathered from customers, with their explicit permission, Lorenzo’s team has assembled what he calls a “golden data set” encompassing tens of thousands of images of OSHA violations. Having carefully stockpiled this specific data for years, he is not worried that even a billion-dollar tech giant will be able to “copy and crush” him.&lt;/p&gt;  &lt;p&gt;To help train the model, Lorenzo has a smaller team of construction safety pros ask strategic questions of the AI. The trainers input test scenes from the golden data set to the VLM and ask questions that guide the model through the process of breaking down the scene and analyzing it step by step the way an experienced human would. If the VLM doesn’t generate the correct response—for example, it misses a violation or registers a false positive—the human trainers go back and tweak the prompts or inputs. Lorenzo says that rather than simply learning to recognize objects, the VLM is taught “how to think in a certain way,” which means it can draw subtle conclusions about what is happening in an image.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Examples from nine categories of safety risks at construction sites that DroneDeploy can detect." class="wp-image-1119362" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/250625_safetyAI_embed.jpg?w=1500" /&gt;&lt;figcaption class="wp-element-caption"&gt;Examples of safety risk categories that Safety AI can detect.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY DRONEDEPLOY&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;As an example, Lorenzo says VLMs are much better than older methods at analyzing ladder usage, which is responsible for 24% of the fall deaths in the construction industry.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“With traditional machine learning, it’s very difficult to answer the question of ‘Is a person using a ladder unsafely?’” says Lorenzo. “You can find the ladders. You can find the people. But to logically reason and say ‘Well, that person is fine’ or ‘Oh no, that person’s standing on the top step’—only the VLM can logically reason and then be like, ‘All right, it’s unsafe. And here’s the OSHA reference that says you can’t be on the top rung.’”&lt;/p&gt;  &lt;p&gt;Answers to multiple questions (Does the person on the ladder have three points of contact? Are they using the ladder as stilts to move around?) are combined to determine whether the ladder in the picture is being used safely. “Our system has over a dozen layers of questioning just to get to that answer,” Lorenzo says. DroneDeploy has not publicly released its data for review, but he says he hopes to have his methodology independently audited by safety experts&lt;strong&gt;.&amp;nbsp;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;The missing 5%&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Using vision language models for construction AI shows promise, but there are “some pretty fundamental issues” to resolve, including hallucinations and the problem of edge cases, those anomalous hazards for which the VLM hasn’t trained, says Chen Feng. He leads New York University’s AI4CE lab, which develops technologies for 3D mapping and scene understanding in construction robotics and other areas. “Ninety-five percent is encouraging—but how do we fix that remaining 5%?” he asks of Safety AI’s success rate. &lt;/p&gt; 

 &lt;p&gt;Feng points to a 2024 paper called “Eyes Wide Shut?”—written by Shengbang Tong, a PhD student at NYU, and coauthored by AI luminary Yann LeCun—that noted “systematic shortcomings” in VLMs.&amp;nbsp; “For object detection, they can reach human-level performance pretty well,” Feng says. “However, for more complicated things—these capabilities are still to be improved.” He notes that VLMs have struggled to interpret 3D scene structure from 2D images, don’t have good situational awareness in reasoning about spatial relationships, and often lack “common sense” about visual scenes.&lt;/p&gt;  &lt;p&gt;Lorenzo concedes that there are “some major flaws” with LLMs and that they struggle with spatial reasoning. So Safety AI also employs some older machine-learning methods to help create spatial models of construction sites. These methods include the segmentation of images into crucial components and photogrammetry, an established technique for creating a 3D digital model from a 2D image. Safety AI has also trained heavily in 10 different problem areas, including ladder usage, to anticipate the most common violations.&lt;/p&gt;  &lt;p&gt;Even so, Lorenzo admits there are edge cases that the LLM will fail to recognize. But he notes that for overworked safety managers, who are often responsible for as many as 15 sites at once, having an extra set of digital “eyes” is still an improvement.&lt;/p&gt;  &lt;p&gt;Aaron Tan, a concrete project manager based in the San Francisco Bay Area, says that a tool like Safety AI could be helpful for these overextended safety managers, who will save a lot of time if they can get an emailed alert rather than having to make a two-hour drive to visit a site in person. And if the software can demonstrate that it is helping keep people safe, he thinks workers will eventually embrace it.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;However, Tan notes that workers also fear that these types of tools will be “bossware” used to get them in trouble. “At my last company, we implemented cameras [as] a security system. And the guys didn’t like that,” he says. “They were like, ‘Oh, Big Brother. You guys are always watching me—I have no privacy.’”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Older doesn’t mean obsolete&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Izhak Paz, CEO of a Jerusalem-based company called Safeguard AI, has considered incorporating VLMs, but he has stuck with the older machine-learning paradigm because he considers it more reliable. The “old computer vision” based on machine learning “is still better, because it’s hybrid between the machine itself and human intervention on dealing with deviation,” he says. To train the algorithm on a new category of danger, his team aggregates a large volume of labeled footage related to the specific hazard and then optimizes the algorithm by trimming false positives and false negatives. The process can take anywhere from weeks to over six months, Paz says. &lt;/p&gt;  &lt;p&gt;With training completed, Safeguard AI performs a risk assessment to identify potential hazards on the site. It can “see” the site in real time by accessing footage from any nearby internet-connected camera. Then it uses an AI agent to push instructions on what to do next to the site managers’ mobile devices. Paz declines to give a precise price tag, but he says his product is affordable only for builders at the “mid-market” level and above, specifically those managing multiple sites. The tool is in use at roughly 3,500 sites in Israel, the United States, and Brazil.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;Buildots, a company based in Tel Aviv that &lt;em&gt;MIT Technology Review&lt;/em&gt; profiled back in 2020, doesn’t do safety analysis but instead creates once- or twice-weekly visual progress reports of sites. Buildots also uses the older method of machine learning with labeled training data. “Our system needs to be 99%—we cannot have any hallucinations,” says CEO Roy Danon.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;He says that gaining labeled training data is actually much easier than it was when he and his cofounders began the project in 2018, since gathering video footage of sites means that each object, such as a socket, might be captured and then labeled in many different frames. But the tool is high-end—about 50 builders, most with revenue over $250 million, are using Buildots in Europe, the Middle East, Africa, Canada, and the US. It’s been used on over 300 projects so far.&lt;/p&gt;  &lt;p&gt;Ryan Calo, a specialist in robotics and AI law at the University of Washington, likes the idea of AI for construction safety. Since experienced safety managers are already spread thin in construction, however, Calo worries that builders will be tempted to automate humans out of the safety process entirely. “I think AI and drones for spotting safety problems that would otherwise kill workers is super smart,” he says. “So long as it’s verified by a person.”&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Andrew Rosenblum&lt;/em&gt;&lt;em&gt; is a freelance tech journalist based in Oakland, CA.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Last winter, during the construction of an affordable housing project on Martha’s Vineyard, Massachusetts, a 32-year-old worker named Jose Luis Collaguazo Crespo slipped off a ladder on the second floor and plunged to his death in the basement. He was one of more than&amp;nbsp;1,000&amp;nbsp;construction&amp;nbsp;workers who die on the job each year in the US, making it the most dangerous industry for fatal slips, trips, and falls.&lt;/p&gt;  &lt;p&gt;“Everyone talks about [how] ‘safety is the number-one priority,’” entrepreneur and executive Philip Lorenzo said during a presentation at Construction Innovation Day 2025, a conference at the University of California, Berkeley, in April.&amp;nbsp;“But then maybe internally, it’s not that high priority. People take shortcuts on job sites. And so there’s this whole tug-of-war between … safety and productivity.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;To combat the shortcuts and risk-taking, Lorenzo is working on a tool for the San Francisco–based company DroneDeploy&lt;strong&gt;, &lt;/strong&gt;which sells software that creates daily digital models of&amp;nbsp;work progress from videos and images, known in the trade as “reality capture.”&amp;nbsp; The tool, called Safety AI, analyzes each day’s reality capture imagery and flags conditions that violate Occupational Safety and Health Administration (OSHA) rules, with what he claims is 95% accuracy. &lt;/p&gt;  &lt;p&gt;That means that for any safety risk the software flags, there is 95% certainty that the flag is accurate and relates to a specific OSHA regulation. Launched in October 2024, it’s now being deployed on hundreds of construction sites in the US, Lorenzo says, and versions specific to the building regulations in countries including Canada, the UK, South Korea, and Australia have also been deployed.&lt;/p&gt; 
 &lt;p&gt;Safety AI is one of multiple AI&amp;nbsp;construction&amp;nbsp;safety tools that have emerged in recent years, from&amp;nbsp;Silicon Valley&amp;nbsp;to&amp;nbsp;Hong Kong to&amp;nbsp;Jerusalem. Many of these rely on teams of human “clickers,” often in low-wage countries, to manually draw bounding boxes around images of key objects like ladders, in order to label large volumes of data to train an algorithm. &lt;/p&gt;  &lt;p&gt;Lorenzo says Safety AI is the first one to use generative AI to flag safety violations, which means an algorithm that can do more than recognize objects such as ladders or hard hats. The software can “reason” about what is going on in an image of a site and draw a conclusion about whether there is an OSHA violation. This is a more advanced form of analysis than the object detection that is the current industry standard, Lorenzo claims. But as the 95% success rate suggests, Safety AI is not a flawless and all-knowing intelligence. It requires an experienced safety inspector as an overseer.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;A visual language model in the real world&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Robots and AI tend to thrive in controlled, largely static environments, like factory floors or shipping terminals.&amp;nbsp;But construction&amp;nbsp;sites are, by definition, changing a little bit every day.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Lorenzo thinks he’s built a better way to monitor sites, using a type of generative AI called a visual language model, or VLM. A VLM is an LLM with a vision encoder, allowing it to “see” images of the world and analyze what is going on in the scene.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Using years of reality capture imagery gathered from customers, with their explicit permission, Lorenzo’s team has assembled what he calls a “golden data set” encompassing tens of thousands of images of OSHA violations. Having carefully stockpiled this specific data for years, he is not worried that even a billion-dollar tech giant will be able to “copy and crush” him.&lt;/p&gt;  &lt;p&gt;To help train the model, Lorenzo has a smaller team of construction safety pros ask strategic questions of the AI. The trainers input test scenes from the golden data set to the VLM and ask questions that guide the model through the process of breaking down the scene and analyzing it step by step the way an experienced human would. If the VLM doesn’t generate the correct response—for example, it misses a violation or registers a false positive—the human trainers go back and tweak the prompts or inputs. Lorenzo says that rather than simply learning to recognize objects, the VLM is taught “how to think in a certain way,” which means it can draw subtle conclusions about what is happening in an image.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Examples from nine categories of safety risks at construction sites that DroneDeploy can detect." class="wp-image-1119362" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/250625_safetyAI_embed.jpg?w=1500" /&gt;&lt;figcaption class="wp-element-caption"&gt;Examples of safety risk categories that Safety AI can detect.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY DRONEDEPLOY&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;As an example, Lorenzo says VLMs are much better than older methods at analyzing ladder usage, which is responsible for 24% of the fall deaths in the construction industry.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“With traditional machine learning, it’s very difficult to answer the question of ‘Is a person using a ladder unsafely?’” says Lorenzo. “You can find the ladders. You can find the people. But to logically reason and say ‘Well, that person is fine’ or ‘Oh no, that person’s standing on the top step’—only the VLM can logically reason and then be like, ‘All right, it’s unsafe. And here’s the OSHA reference that says you can’t be on the top rung.’”&lt;/p&gt;  &lt;p&gt;Answers to multiple questions (Does the person on the ladder have three points of contact? Are they using the ladder as stilts to move around?) are combined to determine whether the ladder in the picture is being used safely. “Our system has over a dozen layers of questioning just to get to that answer,” Lorenzo says. DroneDeploy has not publicly released its data for review, but he says he hopes to have his methodology independently audited by safety experts&lt;strong&gt;.&amp;nbsp;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;The missing 5%&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Using vision language models for construction AI shows promise, but there are “some pretty fundamental issues” to resolve, including hallucinations and the problem of edge cases, those anomalous hazards for which the VLM hasn’t trained, says Chen Feng. He leads New York University’s AI4CE lab, which develops technologies for 3D mapping and scene understanding in construction robotics and other areas. “Ninety-five percent is encouraging—but how do we fix that remaining 5%?” he asks of Safety AI’s success rate. &lt;/p&gt; 

 &lt;p&gt;Feng points to a 2024 paper called “Eyes Wide Shut?”—written by Shengbang Tong, a PhD student at NYU, and coauthored by AI luminary Yann LeCun—that noted “systematic shortcomings” in VLMs.&amp;nbsp; “For object detection, they can reach human-level performance pretty well,” Feng says. “However, for more complicated things—these capabilities are still to be improved.” He notes that VLMs have struggled to interpret 3D scene structure from 2D images, don’t have good situational awareness in reasoning about spatial relationships, and often lack “common sense” about visual scenes.&lt;/p&gt;  &lt;p&gt;Lorenzo concedes that there are “some major flaws” with LLMs and that they struggle with spatial reasoning. So Safety AI also employs some older machine-learning methods to help create spatial models of construction sites. These methods include the segmentation of images into crucial components and photogrammetry, an established technique for creating a 3D digital model from a 2D image. Safety AI has also trained heavily in 10 different problem areas, including ladder usage, to anticipate the most common violations.&lt;/p&gt;  &lt;p&gt;Even so, Lorenzo admits there are edge cases that the LLM will fail to recognize. But he notes that for overworked safety managers, who are often responsible for as many as 15 sites at once, having an extra set of digital “eyes” is still an improvement.&lt;/p&gt;  &lt;p&gt;Aaron Tan, a concrete project manager based in the San Francisco Bay Area, says that a tool like Safety AI could be helpful for these overextended safety managers, who will save a lot of time if they can get an emailed alert rather than having to make a two-hour drive to visit a site in person. And if the software can demonstrate that it is helping keep people safe, he thinks workers will eventually embrace it.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;However, Tan notes that workers also fear that these types of tools will be “bossware” used to get them in trouble. “At my last company, we implemented cameras [as] a security system. And the guys didn’t like that,” he says. “They were like, ‘Oh, Big Brother. You guys are always watching me—I have no privacy.’”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Older doesn’t mean obsolete&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Izhak Paz, CEO of a Jerusalem-based company called Safeguard AI, has considered incorporating VLMs, but he has stuck with the older machine-learning paradigm because he considers it more reliable. The “old computer vision” based on machine learning “is still better, because it’s hybrid between the machine itself and human intervention on dealing with deviation,” he says. To train the algorithm on a new category of danger, his team aggregates a large volume of labeled footage related to the specific hazard and then optimizes the algorithm by trimming false positives and false negatives. The process can take anywhere from weeks to over six months, Paz says. &lt;/p&gt;  &lt;p&gt;With training completed, Safeguard AI performs a risk assessment to identify potential hazards on the site. It can “see” the site in real time by accessing footage from any nearby internet-connected camera. Then it uses an AI agent to push instructions on what to do next to the site managers’ mobile devices. Paz declines to give a precise price tag, but he says his product is affordable only for builders at the “mid-market” level and above, specifically those managing multiple sites. The tool is in use at roughly 3,500 sites in Israel, the United States, and Brazil.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;Buildots, a company based in Tel Aviv that &lt;em&gt;MIT Technology Review&lt;/em&gt; profiled back in 2020, doesn’t do safety analysis but instead creates once- or twice-weekly visual progress reports of sites. Buildots also uses the older method of machine learning with labeled training data. “Our system needs to be 99%—we cannot have any hallucinations,” says CEO Roy Danon.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;He says that gaining labeled training data is actually much easier than it was when he and his cofounders began the project in 2018, since gathering video footage of sites means that each object, such as a socket, might be captured and then labeled in many different frames. But the tool is high-end—about 50 builders, most with revenue over $250 million, are using Buildots in Europe, the Middle East, Africa, Canada, and the US. It’s been used on over 300 projects so far.&lt;/p&gt;  &lt;p&gt;Ryan Calo, a specialist in robotics and AI law at the University of Washington, likes the idea of AI for construction safety. Since experienced safety managers are already spread thin in construction, however, Calo worries that builders will be tempted to automate humans out of the safety process entirely. “I think AI and drones for spotting safety problems that would otherwise kill workers is super smart,” he says. “So long as it’s verified by a person.”&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Andrew Rosenblum&lt;/em&gt;&lt;em&gt; is a freelance tech journalist based in Oakland, CA.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/07/02/1119607/how-generative-ai-could-help-make-construction-sites-safer/</guid><pubDate>Wed, 02 Jul 2025 09:00:00 +0000</pubDate></item><item><title>[NEW] How businesses can use local AI models to improve data privacy (AI News)</title><link>https://www.artificialintelligence-news.com/news/how-businesses-can-use-local-ai-models-to-improve-data-privacy/</link><description>&lt;p&gt;Businesses intending to use AI do not have to rely on cloud-based tools like Chat-GPT, which tend to require uploading or sharing sensitive data. Instead, it is now possible to install and run private AI models locally, ensuring all data remains private and secure.&lt;/p&gt;&lt;p&gt;There are several open-source tools available for those looking to experiment with locally-running AI models, all of which prioritise data privacy, cost-effectiveness, and ease of deployment, therefore ensuring they are suitable for varying levels of technical expertise.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="org4d59a34"&gt;Private AIs for business experimentation&lt;/h3&gt;&lt;h4 class="wp-block-heading" id="org24664d0"&gt;LocalAI&lt;/h4&gt;&lt;p&gt;LocalAI is an open-source platform developed as a drop-in alternative for OpenAI’s API, allows businesses to operate LLMs locally. The tool supports a range of model architectures, including Transformers, GGUF, and Diffusers.&lt;/p&gt;&lt;p&gt;The technical requirements of LocalAI are minimal, operating on consumer-grade hardware. Its modest specifications let businesses use existing hardware. Comprehensive guides and tutorials are available, helping businesses set the tool up. From here, it is possible to generate images, run LLMs, and produce audio on-premise with consumer-grade hardware.&lt;/p&gt;&lt;p&gt;LocalAI provides an extensive library of use cases, showcasing audio synthesis, image creation, text generation, and voice cloning, helping businesses explore practical applications of AI while keepind data secure.&lt;/p&gt;&lt;h4 class="wp-block-heading" id="org5b2847e"&gt;Ollama&lt;/h4&gt;&lt;p&gt;Ollama manages model downloads, dependencies, and configurations, helping simplify the running of LLMs locally. The lightweight, open-source framework offers command-line and graphics interfaces, supporting macOS, Linux, and Windows, and models like Mistral and Llama 3.2 can be easily downloaded. Each model can run its own environment, streamlining the process of switching between different AI tools for various tasks.&lt;/p&gt;&lt;p&gt;Ollama powers research projects, chatbots, and AI applications that handle sensitive information and data, and by removing cloud dependencies, teams can work off the public internet, meeting privacy requirements like GDPR without having to compromise AI functionality.&lt;/p&gt;&lt;p&gt;Ollama boasts a user-friendly setup and is suitable for inexperienced or non-developers. Detailed guides and community support are available, giving businesses full control over all elements.&lt;/p&gt;&lt;h4 class="wp-block-heading" id="org534e088"&gt;DocMind AI&lt;/h4&gt;&lt;p&gt;DocMind AI is a Streamlit application using LangChain and local LLMs through Ollama to achieve detailed, advanced document analysis. Using DocMind AI lets businesses analyse, summarise, and mine data from many file formats, privately and securely.&lt;/p&gt;&lt;p&gt;DocMind AI requires moderate technical know-how. Familiarity with Python and Streamlit are considered beneficial, but not essential. GitHub provides comprehensive setup instructions and documented examples highlight data analysis, information extraction, and document summarisation.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="org1acefb2"&gt;Deployment Considerations&lt;/h3&gt;&lt;p&gt;Although LocalAI, Ollama, and DocMind AI have been built to be accessible for all, there is no doubt that some technical knowledge is beneficial. Moreover, an understanding of Python, Docker, or command-line interfaces can help smooth deployment.&lt;/p&gt;&lt;p&gt;Most tools have the capability to run on standard consumer-grade hardware, but performance is likely to improve the higher the specification. It is also essential that all security measures for the hosting environment are implemented, despite locally-run AI models enhancing data privacy by definition. But comprehensive security helps ensure protection against unauthorised access, potential data breaches, and system vulnerability.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: “Fence” by foilman is licensed under CC BY-SA 2.0.)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;See also: Salesforce Agentforce 3 brings agent visibility&lt;/p&gt;&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Businesses intending to use AI do not have to rely on cloud-based tools like Chat-GPT, which tend to require uploading or sharing sensitive data. Instead, it is now possible to install and run private AI models locally, ensuring all data remains private and secure.&lt;/p&gt;&lt;p&gt;There are several open-source tools available for those looking to experiment with locally-running AI models, all of which prioritise data privacy, cost-effectiveness, and ease of deployment, therefore ensuring they are suitable for varying levels of technical expertise.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="org4d59a34"&gt;Private AIs for business experimentation&lt;/h3&gt;&lt;h4 class="wp-block-heading" id="org24664d0"&gt;LocalAI&lt;/h4&gt;&lt;p&gt;LocalAI is an open-source platform developed as a drop-in alternative for OpenAI’s API, allows businesses to operate LLMs locally. The tool supports a range of model architectures, including Transformers, GGUF, and Diffusers.&lt;/p&gt;&lt;p&gt;The technical requirements of LocalAI are minimal, operating on consumer-grade hardware. Its modest specifications let businesses use existing hardware. Comprehensive guides and tutorials are available, helping businesses set the tool up. From here, it is possible to generate images, run LLMs, and produce audio on-premise with consumer-grade hardware.&lt;/p&gt;&lt;p&gt;LocalAI provides an extensive library of use cases, showcasing audio synthesis, image creation, text generation, and voice cloning, helping businesses explore practical applications of AI while keepind data secure.&lt;/p&gt;&lt;h4 class="wp-block-heading" id="org5b2847e"&gt;Ollama&lt;/h4&gt;&lt;p&gt;Ollama manages model downloads, dependencies, and configurations, helping simplify the running of LLMs locally. The lightweight, open-source framework offers command-line and graphics interfaces, supporting macOS, Linux, and Windows, and models like Mistral and Llama 3.2 can be easily downloaded. Each model can run its own environment, streamlining the process of switching between different AI tools for various tasks.&lt;/p&gt;&lt;p&gt;Ollama powers research projects, chatbots, and AI applications that handle sensitive information and data, and by removing cloud dependencies, teams can work off the public internet, meeting privacy requirements like GDPR without having to compromise AI functionality.&lt;/p&gt;&lt;p&gt;Ollama boasts a user-friendly setup and is suitable for inexperienced or non-developers. Detailed guides and community support are available, giving businesses full control over all elements.&lt;/p&gt;&lt;h4 class="wp-block-heading" id="org534e088"&gt;DocMind AI&lt;/h4&gt;&lt;p&gt;DocMind AI is a Streamlit application using LangChain and local LLMs through Ollama to achieve detailed, advanced document analysis. Using DocMind AI lets businesses analyse, summarise, and mine data from many file formats, privately and securely.&lt;/p&gt;&lt;p&gt;DocMind AI requires moderate technical know-how. Familiarity with Python and Streamlit are considered beneficial, but not essential. GitHub provides comprehensive setup instructions and documented examples highlight data analysis, information extraction, and document summarisation.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="org1acefb2"&gt;Deployment Considerations&lt;/h3&gt;&lt;p&gt;Although LocalAI, Ollama, and DocMind AI have been built to be accessible for all, there is no doubt that some technical knowledge is beneficial. Moreover, an understanding of Python, Docker, or command-line interfaces can help smooth deployment.&lt;/p&gt;&lt;p&gt;Most tools have the capability to run on standard consumer-grade hardware, but performance is likely to improve the higher the specification. It is also essential that all security measures for the hosting environment are implemented, despite locally-run AI models enhancing data privacy by definition. But comprehensive security helps ensure protection against unauthorised access, potential data breaches, and system vulnerability.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: “Fence” by foilman is licensed under CC BY-SA 2.0.)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;See also: Salesforce Agentforce 3 brings agent visibility&lt;/p&gt;&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/how-businesses-can-use-local-ai-models-to-improve-data-privacy/</guid><pubDate>Wed, 02 Jul 2025 10:55:35 +0000</pubDate></item><item><title>[NEW] The Download: how AI could improve construction site safety, and our Roundtables conversation with Karen Hao (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/07/02/1119616/the-download-how-ai-could-improve-construction-site-safety-and-our-roundtables-conversation-with-karen-hao/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;How generative AI could help make construction sites safer&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;More than 1,000 construction workers die on the job each year in the US, making it the most dangerous industry for fatal slips, trips, and falls.&lt;/p&gt;&lt;p&gt;A new AI tool called Safety AI could help to change that. It analyzes the progress made on a construction site each day, and flags conditions that violate Occupational Safety and Health Administration rules, with what its creator Philip Lorenzo claims is 95% accuracy.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;Lorenzo says Safety AI is the first one of multiple emerging AI construction safety tools to use generative AI to flag safety violations. But as the 95% success rate suggests, Safety AI is not a flawless and all-knowing intelligence. Read the full story.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;—Andrew Rosenblum&lt;/em&gt;&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Roundtables: Inside OpenAI’s Empire with Karen Hao&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Earlier this week, we held a subscriber-only Roundtable discussion with author and former MIT Technology Review senior editor Karen Hao about her new book &lt;em&gt;Empire of AI: Dreams and Nightmares in Sam Altman's OpenAI.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;You can watch her conversation with our executive editor Niall Firth here—and if you aren’t already, you can subscribe to us here.&amp;nbsp;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;MIT Technology Review Narrated: The tech industry can’t agree on what open-source AI means. That’s a problem.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;What counts as 'open-source AI'? The answer could determine who gets to shape the future of the technology.&lt;/p&gt;  &lt;p&gt;This is our latest story to be turned into a MIT Technology Review Narrated podcast, which we’re publishing each week on &lt;strong&gt;Spotify&lt;/strong&gt; and &lt;strong&gt;Apple Podcasts&lt;/strong&gt;. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it’s released.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 China’s digital IDs are coming&lt;/strong&gt;&lt;br /&gt;And they’re unlikely to stay voluntary for long. (Economist $)&lt;br /&gt;+ &lt;em&gt;The country’s AI models are becoming increasingly popular worldwide. &lt;/em&gt;(WSJ $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2&lt;/strong&gt; &lt;strong&gt;Donald Trump has mused about using DOGE to deport Elon Musk&lt;/strong&gt;&lt;br /&gt;Musk’s comments about the President’s ‘Big Beautiful Bill’ have touched a nerve. (Axios)&lt;br /&gt;+ &lt;em&gt;Turns out AI models are quite good at fact checking Trump. &lt;/em&gt;(WP $)&lt;br /&gt;+ &lt;em&gt;DOGE’s tech takeover threatens the safety and stability of our critical data. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 Google must pay California’s Android users $314.6m&lt;br /&gt;&lt;/strong&gt;After a jury ruled it had misused their data. (Reuters)&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;4 Many AI detectors overpromise and underdeliver&lt;/strong&gt;&lt;br /&gt;But that hasn’t stopped Californian colleges from investing millions in them. (Undark)&lt;br /&gt;+ &lt;em&gt;What’s next for college writing? Nothing good. &lt;/em&gt;(New Yorker $)&lt;br /&gt;+ &lt;em&gt;Educators are working out how to integrate AI into computer science. &lt;/em&gt;(NYT $)&lt;br /&gt;+ &lt;em&gt;AI-text detection tools are really easy to fool. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 Google is making its first foray into fusion&lt;br /&gt;&lt;/strong&gt;The world’s first grid-scale fusion power plant is due to come online in the 2030s. (NBC News)&lt;br /&gt;+ &lt;em&gt;Google will buy half its output. &lt;/em&gt;(TechCrunch)&lt;br /&gt;+ &lt;em&gt;Inside a fusion energy facility. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 China is banning certain portable batteries from flights&lt;/strong&gt;&lt;br /&gt;In the wake of two major manufacturers recalling millions of power banks. (NYT $)&lt;br /&gt;+ &lt;em&gt;The ban is catching travellers out. &lt;/em&gt;(SCMP)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 The deepfake economy is spiralling out of control&lt;br /&gt;&lt;/strong&gt;Small business owners are drowning in online scams. (Insider $)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;8 Chipmaking companies are attractive prospects for investors&lt;/strong&gt;&lt;br /&gt;And they’re likely to be better bets. (WSJ $)&lt;br /&gt;+ &lt;em&gt;OpenAI has denied that it plans to use Google’s in-house chip. &lt;/em&gt;(Reuters)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 How cancer studies in dogs could help develop treatments for humans&lt;br /&gt;&lt;/strong&gt;The disease presents very similarly across both species. (Knowable Magazine)&lt;br /&gt;+ &lt;em&gt;Cancer vaccines are having a renaissance. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;10 X is planning to task AI agents with writing Community Notes&lt;/strong&gt;&lt;br /&gt;Thankfully, humans will still review them. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;Why does AI hallucinate? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“Missionaries will beat mercenaries.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—OpenAI CEO Sam Altman takes aim at Meta’s recent spree of attempting to hire his staff, Wired reports.&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXduB80gTDJ7JHFbs98qkJdaOnlYBXCA749rb0HY6fm4rE-N-QAe6cC5yHFubH2nu486Dnox7YUV-m3bvXBTG4wqtpMBEPxJ56jwmZIzw83DBYCw0CSjQtbFhyIh88qJEtS7BeboKQ?key=CtSe_KkxB9JkAsRijviuSw" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;The world’s next big environmental problem could come from space&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In September, a unique chase took place in the skies above Easter Island. From a rented jet, a team of researchers captured a satellite’s last moments as it fell out of space and blazed into ash across the sky, using cameras and scientific equipment. Their hope was to gather priceless insights into the physical and chemical processes that occur when satellites burn up as they fall to Earth at the end of their missions.&lt;/p&gt;  &lt;p&gt;This kind of study is growing more urgent. The number of satellites in the sky is rapidly rising—with a tenfold increase forecast by the end of the decade. Letting these satellites burn up in the atmosphere at the end of their lives helps keep the quantity of space junk to a minimum. But doing so deposits satellite ash in the Earth’s atmosphere. This metallic ash could potentially alter the climate, and we don’t yet know how serious the problem is likely to be. Read the full story.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Tereza Pultarova&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;How generative AI could help make construction sites safer&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;More than 1,000 construction workers die on the job each year in the US, making it the most dangerous industry for fatal slips, trips, and falls.&lt;/p&gt;&lt;p&gt;A new AI tool called Safety AI could help to change that. It analyzes the progress made on a construction site each day, and flags conditions that violate Occupational Safety and Health Administration rules, with what its creator Philip Lorenzo claims is 95% accuracy.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;Lorenzo says Safety AI is the first one of multiple emerging AI construction safety tools to use generative AI to flag safety violations. But as the 95% success rate suggests, Safety AI is not a flawless and all-knowing intelligence. Read the full story.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;—Andrew Rosenblum&lt;/em&gt;&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Roundtables: Inside OpenAI’s Empire with Karen Hao&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Earlier this week, we held a subscriber-only Roundtable discussion with author and former MIT Technology Review senior editor Karen Hao about her new book &lt;em&gt;Empire of AI: Dreams and Nightmares in Sam Altman's OpenAI.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;You can watch her conversation with our executive editor Niall Firth here—and if you aren’t already, you can subscribe to us here.&amp;nbsp;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;MIT Technology Review Narrated: The tech industry can’t agree on what open-source AI means. That’s a problem.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;What counts as 'open-source AI'? The answer could determine who gets to shape the future of the technology.&lt;/p&gt;  &lt;p&gt;This is our latest story to be turned into a MIT Technology Review Narrated podcast, which we’re publishing each week on &lt;strong&gt;Spotify&lt;/strong&gt; and &lt;strong&gt;Apple Podcasts&lt;/strong&gt;. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it’s released.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 China’s digital IDs are coming&lt;/strong&gt;&lt;br /&gt;And they’re unlikely to stay voluntary for long. (Economist $)&lt;br /&gt;+ &lt;em&gt;The country’s AI models are becoming increasingly popular worldwide. &lt;/em&gt;(WSJ $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2&lt;/strong&gt; &lt;strong&gt;Donald Trump has mused about using DOGE to deport Elon Musk&lt;/strong&gt;&lt;br /&gt;Musk’s comments about the President’s ‘Big Beautiful Bill’ have touched a nerve. (Axios)&lt;br /&gt;+ &lt;em&gt;Turns out AI models are quite good at fact checking Trump. &lt;/em&gt;(WP $)&lt;br /&gt;+ &lt;em&gt;DOGE’s tech takeover threatens the safety and stability of our critical data. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 Google must pay California’s Android users $314.6m&lt;br /&gt;&lt;/strong&gt;After a jury ruled it had misused their data. (Reuters)&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;4 Many AI detectors overpromise and underdeliver&lt;/strong&gt;&lt;br /&gt;But that hasn’t stopped Californian colleges from investing millions in them. (Undark)&lt;br /&gt;+ &lt;em&gt;What’s next for college writing? Nothing good. &lt;/em&gt;(New Yorker $)&lt;br /&gt;+ &lt;em&gt;Educators are working out how to integrate AI into computer science. &lt;/em&gt;(NYT $)&lt;br /&gt;+ &lt;em&gt;AI-text detection tools are really easy to fool. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 Google is making its first foray into fusion&lt;br /&gt;&lt;/strong&gt;The world’s first grid-scale fusion power plant is due to come online in the 2030s. (NBC News)&lt;br /&gt;+ &lt;em&gt;Google will buy half its output. &lt;/em&gt;(TechCrunch)&lt;br /&gt;+ &lt;em&gt;Inside a fusion energy facility. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 China is banning certain portable batteries from flights&lt;/strong&gt;&lt;br /&gt;In the wake of two major manufacturers recalling millions of power banks. (NYT $)&lt;br /&gt;+ &lt;em&gt;The ban is catching travellers out. &lt;/em&gt;(SCMP)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 The deepfake economy is spiralling out of control&lt;br /&gt;&lt;/strong&gt;Small business owners are drowning in online scams. (Insider $)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;8 Chipmaking companies are attractive prospects for investors&lt;/strong&gt;&lt;br /&gt;And they’re likely to be better bets. (WSJ $)&lt;br /&gt;+ &lt;em&gt;OpenAI has denied that it plans to use Google’s in-house chip. &lt;/em&gt;(Reuters)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 How cancer studies in dogs could help develop treatments for humans&lt;br /&gt;&lt;/strong&gt;The disease presents very similarly across both species. (Knowable Magazine)&lt;br /&gt;+ &lt;em&gt;Cancer vaccines are having a renaissance. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;10 X is planning to task AI agents with writing Community Notes&lt;/strong&gt;&lt;br /&gt;Thankfully, humans will still review them. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;Why does AI hallucinate? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“Missionaries will beat mercenaries.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—OpenAI CEO Sam Altman takes aim at Meta’s recent spree of attempting to hire his staff, Wired reports.&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXduB80gTDJ7JHFbs98qkJdaOnlYBXCA749rb0HY6fm4rE-N-QAe6cC5yHFubH2nu486Dnox7YUV-m3bvXBTG4wqtpMBEPxJ56jwmZIzw83DBYCw0CSjQtbFhyIh88qJEtS7BeboKQ?key=CtSe_KkxB9JkAsRijviuSw" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;The world’s next big environmental problem could come from space&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In September, a unique chase took place in the skies above Easter Island. From a rented jet, a team of researchers captured a satellite’s last moments as it fell out of space and blazed into ash across the sky, using cameras and scientific equipment. Their hope was to gather priceless insights into the physical and chemical processes that occur when satellites burn up as they fall to Earth at the end of their missions.&lt;/p&gt;  &lt;p&gt;This kind of study is growing more urgent. The number of satellites in the sky is rapidly rising—with a tenfold increase forecast by the end of the decade. Letting these satellites burn up in the atmosphere at the end of their lives helps keep the quantity of space junk to a minimum. But doing so deposits satellite ash in the Earth’s atmosphere. This metallic ash could potentially alter the climate, and we don’t yet know how serious the problem is likely to be. Read the full story.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Tereza Pultarova&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/07/02/1119616/the-download-how-ai-could-improve-construction-site-safety-and-our-roundtables-conversation-with-karen-hao/</guid><pubDate>Wed, 02 Jul 2025 12:10:00 +0000</pubDate></item></channel></rss>