<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 14 Nov 2025 06:33:17 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>What’s Next for AI? (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/13/1127930/whats-next-for-ai-2/</link><description></description><guid isPermaLink="false">https://www.technologyreview.com/2025/11/13/1127930/whats-next-for-ai-2/</guid><pubDate>Thu, 13 Nov 2025 18:33:33 +0000</pubDate></item><item><title>AI On: 3 Ways to Bring Agentic AI to Computer Vision Applications (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/ways-to-bring-agentic-ai-to-computer-vision-applications/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This post is part of the &lt;/i&gt;&lt;i&gt;AI On&lt;/i&gt;&lt;i&gt; blog series, which explores the latest techniques and real-world applications of agentic AI, chatbots and copilots. The series also highlights the NVIDIA software and hardware powering advanced AI agents, which form the foundation of AI query engines that gather insights and perform tasks to transform everyday experiences and reshape industries.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Today’s computer vision systems excel at identifying what happens in physical spaces and processes, but lack the abilities to explain the details of a scene and why they matter, as well as reason about what might happen next.&lt;/p&gt;
&lt;p&gt;Agentic intelligence powered by vision language models (VLMs) can help bridge this gap, giving teams quick, easy access to key insights and analyses that connect text descriptors with spatial-temporal information and billions of visual data points captured by their systems every day.&lt;/p&gt;
&lt;p&gt;Three approaches organizations can use to boost their legacy computer vision systems with agentic intelligence are to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apply dense captioning for searchable visual content.&lt;/li&gt;
&lt;li&gt;Augment system alerts with detailed context.&lt;/li&gt;
&lt;li&gt;Use AI reasoning to summarize information from complex scenarios and answer questions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;strong&gt;Making Visual Content Searchable With Dense Captions&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Traditional convolutional neural network (CNN)-powered video search tools are constrained by limited training, context and semantics, making gleaning insights manual, tedious and time-consuming. CNNs are tuned to perform specific visual tasks, like spotting an anomaly, and lack the multimodal ability to translate what they see into text.&lt;/p&gt;
&lt;p&gt;Businesses can embed VLMs directly into their existing applications to generate highly detailed captions of images and videos. These captions turn unstructured content into rich, searchable metadata, enabling visual search that’s far more flexible — not constrained by file names or basic tags.&lt;/p&gt;
&lt;p&gt;For example, automated vehicle-inspection system UVeye processes over 700 million high-resolution images each month to build one of the world’s largest vehicle and component datasets. By applying VLMs, UVeye converts this visual data into structured condition reports, detecting subtle defects, modifications or foreign objects with exceptional accuracy and reliability for search.&lt;/p&gt;
&lt;p&gt;VLM-powered visual understanding adds essential context, ensuring transparent, consistent insights for compliance, safety and quality control. UVeye detects 96% of defects compared with 24% using manual methods, enabling early intervention to reduce downtime and control maintenance costs.&lt;/p&gt;

&lt;p&gt;Relo Metrics, a provider of AI-powered sports marketing measurement, helps brands quantify the value of their media investments and optimize their spending. By combining VLMs with computer vision, Relo Metrics moves beyond basic logo detection to capture context — like a courtside banner shown during a game-winning shot — and translate it into real-time monetary value.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-87329 size-medium" height="720" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/relo-metrics-960x720.png" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;This contextual-insight capability highlights when and how logos appear, especially in high-impact moments, giving marketers a clearer view of return on investment and ways to optimize strategy. For example, Stanley Black &amp;amp; Decker, including its Dewalt brand, previously relied on end-of-season reports to evaluate sponsor asset performance, limiting timely decision-making. Using Relo Metrics for real-time insights, Stanley Black &amp;amp; Decker adjusted signage positioning and saved $1.3 million in potentially lost sponsor media value.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Augmenting Computer Vision System Alerts With VLM Reasoning&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;CNN-based computer vision systems often generate binary detection alerts such as yes or no, and true or false. Without the reasoning power of VLMs, that can mean false positives and missed details — leading to costly mistakes in safety and security, as well as lost business intelligence.Rather than replacing these CNN-based computer vision systems entirely, VLMs can easily augment these systems as an intelligent add-on. With a VLM layered on top of CNN-based computer vision systems, detection alerts are not only flagged but reviewed with contextual understanding — explaining where, how and why the incident occurred.&lt;/p&gt;
&lt;p&gt;For smarter city traffic management, Linker Vision uses VLMs to verify critical city alerts, such as traffic accidents, flooding, or falling poles and trees from storms. This reduces false positives and adds vital context to each event to improve real-time municipal response.&lt;/p&gt;

&lt;p&gt;Linker Vision’s architecture for agentic AI involves automating event analysis from over 50,000 diverse smart city camera streams to enable cross-department remediation — coordinating actions across teams like traffic control, utilities and first responders when incidents occur. The ability to query across all camera streams simultaneously enables systems to quickly and automatically turn observations into insights and trigger recommendations for next best actions.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Automatic Analysis of Complex Scenarios With Agentic AI&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Agentic AI systems can process, reason and answer complex queries across video streams and modalities — such as audio, text, video and sensor data. This is possible by combining VLMs with reasoning models, large language models (LLMs), retrieval-augmented generation (RAG), computer vision and speech transcription.&lt;/p&gt;
&lt;p&gt;Basic integration of a VLM into an existing computer vision pipeline is helpful in verifying short video clips of key moments. However this approach is limited by how many visual tokens a single model can process at once, resulting in surface-level answers without context over longer time periods and external knowledge.&lt;/p&gt;
&lt;p&gt;In contrast, whole architectures built on agentic AI enable scalable, accurate processing of lengthy and multichannel video archives. This leads to deeper, more accurate and more reliable insights that go beyond surface-level understanding. Agentic systems can be used for root-cause analysis or analysis of long inspection videos to generate reports with timestamped insights.&lt;/p&gt;
&lt;p&gt;Levatas develops visual-inspection solutions that use mobile robots and autonomous systems to enhance safety, reliability and performance of critical infrastructure assets such as electric utility substations, fuel terminals, rail yards and logistics hubs. Using VLMs, Levatas built a video analytics AI agent to automatically review inspection footage and draft detailed inspection reports, dramatically accelerating a traditionally manual and slow process.&lt;/p&gt;
&lt;p&gt;For customers like American Electric Power (AEP), Levatas AI integrates with Skydio X10 devices to streamline inspection of electric infrastructure. Levatas enables AEP to autonomously inspect power poles, identify thermal issues and detect equipment damage. Alerts are sent instantly to the AEP team upon issue detection, enabling swift response and resolution, and ensuring reliable, clean and affordable energy delivery.&lt;/p&gt;

&lt;p&gt;AI gaming highlight tools like Eklipse use VLM-powered agents to enrich livestreams of video games with captions and index metadata for rapid querying, summarization and creation of polished highlight reels in minutes — 10x faster than legacy solutions — leading to improved content consumption experiences.&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;Powering Agentic Video Intelligence With NVIDIA Technologies&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;For advanced search and reasoning, developers can use multimodal VLMs such as NVCLIP, NVIDIA Cosmos Reason and Nemotron Nano V2 to build metadata-rich indexes for search.&lt;/p&gt;
&lt;p&gt;To integrate VLMs into computer vision applications, developers can use the event reviewer feature in the NVIDIA Blueprint for video search and summarization (VSS), part of the NVIDIA Metropolis platform.&lt;/p&gt;
&lt;p&gt;For more complex queries and summarization tasks, the VSS blueprint can be customized so developers can build AI agents that access VLMs directly or use VLMs in conjunction with LLMs, RAG and computer vision models. This enables smarter operations, richer video analytics and real-time process compliance that scale with organizational needs.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about NVIDIA-powered &lt;/i&gt;&lt;i&gt;agentic video analytics&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Stay up to date by subscribing to NVIDIA’s vision AI newsletter, &lt;/i&gt;&lt;i&gt;joining the community&lt;/i&gt;&lt;i&gt; and following NVIDIA AI on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Explore the &lt;/i&gt;&lt;i&gt;VLM tech blogs&lt;/i&gt;&lt;i&gt;, and &lt;/i&gt;&lt;i&gt;self-paced video tutorials and livestreams&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This post is part of the &lt;/i&gt;&lt;i&gt;AI On&lt;/i&gt;&lt;i&gt; blog series, which explores the latest techniques and real-world applications of agentic AI, chatbots and copilots. The series also highlights the NVIDIA software and hardware powering advanced AI agents, which form the foundation of AI query engines that gather insights and perform tasks to transform everyday experiences and reshape industries.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Today’s computer vision systems excel at identifying what happens in physical spaces and processes, but lack the abilities to explain the details of a scene and why they matter, as well as reason about what might happen next.&lt;/p&gt;
&lt;p&gt;Agentic intelligence powered by vision language models (VLMs) can help bridge this gap, giving teams quick, easy access to key insights and analyses that connect text descriptors with spatial-temporal information and billions of visual data points captured by their systems every day.&lt;/p&gt;
&lt;p&gt;Three approaches organizations can use to boost their legacy computer vision systems with agentic intelligence are to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apply dense captioning for searchable visual content.&lt;/li&gt;
&lt;li&gt;Augment system alerts with detailed context.&lt;/li&gt;
&lt;li&gt;Use AI reasoning to summarize information from complex scenarios and answer questions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;strong&gt;Making Visual Content Searchable With Dense Captions&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Traditional convolutional neural network (CNN)-powered video search tools are constrained by limited training, context and semantics, making gleaning insights manual, tedious and time-consuming. CNNs are tuned to perform specific visual tasks, like spotting an anomaly, and lack the multimodal ability to translate what they see into text.&lt;/p&gt;
&lt;p&gt;Businesses can embed VLMs directly into their existing applications to generate highly detailed captions of images and videos. These captions turn unstructured content into rich, searchable metadata, enabling visual search that’s far more flexible — not constrained by file names or basic tags.&lt;/p&gt;
&lt;p&gt;For example, automated vehicle-inspection system UVeye processes over 700 million high-resolution images each month to build one of the world’s largest vehicle and component datasets. By applying VLMs, UVeye converts this visual data into structured condition reports, detecting subtle defects, modifications or foreign objects with exceptional accuracy and reliability for search.&lt;/p&gt;
&lt;p&gt;VLM-powered visual understanding adds essential context, ensuring transparent, consistent insights for compliance, safety and quality control. UVeye detects 96% of defects compared with 24% using manual methods, enabling early intervention to reduce downtime and control maintenance costs.&lt;/p&gt;

&lt;p&gt;Relo Metrics, a provider of AI-powered sports marketing measurement, helps brands quantify the value of their media investments and optimize their spending. By combining VLMs with computer vision, Relo Metrics moves beyond basic logo detection to capture context — like a courtside banner shown during a game-winning shot — and translate it into real-time monetary value.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-87329 size-medium" height="720" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/relo-metrics-960x720.png" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;This contextual-insight capability highlights when and how logos appear, especially in high-impact moments, giving marketers a clearer view of return on investment and ways to optimize strategy. For example, Stanley Black &amp;amp; Decker, including its Dewalt brand, previously relied on end-of-season reports to evaluate sponsor asset performance, limiting timely decision-making. Using Relo Metrics for real-time insights, Stanley Black &amp;amp; Decker adjusted signage positioning and saved $1.3 million in potentially lost sponsor media value.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Augmenting Computer Vision System Alerts With VLM Reasoning&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;CNN-based computer vision systems often generate binary detection alerts such as yes or no, and true or false. Without the reasoning power of VLMs, that can mean false positives and missed details — leading to costly mistakes in safety and security, as well as lost business intelligence.Rather than replacing these CNN-based computer vision systems entirely, VLMs can easily augment these systems as an intelligent add-on. With a VLM layered on top of CNN-based computer vision systems, detection alerts are not only flagged but reviewed with contextual understanding — explaining where, how and why the incident occurred.&lt;/p&gt;
&lt;p&gt;For smarter city traffic management, Linker Vision uses VLMs to verify critical city alerts, such as traffic accidents, flooding, or falling poles and trees from storms. This reduces false positives and adds vital context to each event to improve real-time municipal response.&lt;/p&gt;

&lt;p&gt;Linker Vision’s architecture for agentic AI involves automating event analysis from over 50,000 diverse smart city camera streams to enable cross-department remediation — coordinating actions across teams like traffic control, utilities and first responders when incidents occur. The ability to query across all camera streams simultaneously enables systems to quickly and automatically turn observations into insights and trigger recommendations for next best actions.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Automatic Analysis of Complex Scenarios With Agentic AI&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Agentic AI systems can process, reason and answer complex queries across video streams and modalities — such as audio, text, video and sensor data. This is possible by combining VLMs with reasoning models, large language models (LLMs), retrieval-augmented generation (RAG), computer vision and speech transcription.&lt;/p&gt;
&lt;p&gt;Basic integration of a VLM into an existing computer vision pipeline is helpful in verifying short video clips of key moments. However this approach is limited by how many visual tokens a single model can process at once, resulting in surface-level answers without context over longer time periods and external knowledge.&lt;/p&gt;
&lt;p&gt;In contrast, whole architectures built on agentic AI enable scalable, accurate processing of lengthy and multichannel video archives. This leads to deeper, more accurate and more reliable insights that go beyond surface-level understanding. Agentic systems can be used for root-cause analysis or analysis of long inspection videos to generate reports with timestamped insights.&lt;/p&gt;
&lt;p&gt;Levatas develops visual-inspection solutions that use mobile robots and autonomous systems to enhance safety, reliability and performance of critical infrastructure assets such as electric utility substations, fuel terminals, rail yards and logistics hubs. Using VLMs, Levatas built a video analytics AI agent to automatically review inspection footage and draft detailed inspection reports, dramatically accelerating a traditionally manual and slow process.&lt;/p&gt;
&lt;p&gt;For customers like American Electric Power (AEP), Levatas AI integrates with Skydio X10 devices to streamline inspection of electric infrastructure. Levatas enables AEP to autonomously inspect power poles, identify thermal issues and detect equipment damage. Alerts are sent instantly to the AEP team upon issue detection, enabling swift response and resolution, and ensuring reliable, clean and affordable energy delivery.&lt;/p&gt;

&lt;p&gt;AI gaming highlight tools like Eklipse use VLM-powered agents to enrich livestreams of video games with captions and index metadata for rapid querying, summarization and creation of polished highlight reels in minutes — 10x faster than legacy solutions — leading to improved content consumption experiences.&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;Powering Agentic Video Intelligence With NVIDIA Technologies&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;For advanced search and reasoning, developers can use multimodal VLMs such as NVCLIP, NVIDIA Cosmos Reason and Nemotron Nano V2 to build metadata-rich indexes for search.&lt;/p&gt;
&lt;p&gt;To integrate VLMs into computer vision applications, developers can use the event reviewer feature in the NVIDIA Blueprint for video search and summarization (VSS), part of the NVIDIA Metropolis platform.&lt;/p&gt;
&lt;p&gt;For more complex queries and summarization tasks, the VSS blueprint can be customized so developers can build AI agents that access VLMs directly or use VLMs in conjunction with LLMs, RAG and computer vision models. This enables smarter operations, richer video analytics and real-time process compliance that scale with organizational needs.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about NVIDIA-powered &lt;/i&gt;&lt;i&gt;agentic video analytics&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Stay up to date by subscribing to NVIDIA’s vision AI newsletter, &lt;/i&gt;&lt;i&gt;joining the community&lt;/i&gt;&lt;i&gt; and following NVIDIA AI on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Explore the &lt;/i&gt;&lt;i&gt;VLM tech blogs&lt;/i&gt;&lt;i&gt;, and &lt;/i&gt;&lt;i&gt;self-paced video tutorials and livestreams&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/ways-to-bring-agentic-ai-to-computer-vision-applications/</guid><pubDate>Thu, 13 Nov 2025 18:50:06 +0000</pubDate></item><item><title>Separating natural forests from other tree cover with AI for deforestation-free supply chains (The latest research from Google)</title><link>https://research.google/blog/separating-natural-forests-from-other-tree-cover-with-ai-for-deforestation-free-supply-chains/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;Acknowledgments&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;&lt;i&gt;This research was co-developed by Google Deepmind and Google Research in collaboration with WRI and IIASA.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;We thank our collaborators at Google, World Resources Institute (WRI) / Global Forest Watch (GFW), and International Institute for Applied Systems Analysis (IIASA): Anton Raichuk, Charlotte Stanton, Dan Morris, Drew Purves, Elizabeth Goldman, Katelyn Tarrio, Keith Anderson, Maxim Neumann, Mélanie Rey, Michelle J. Sims, Myroslava Lesiv, Nicholas Clinton, Petra Poklukar, Radost Stanimirova, Sarah Carter, Steffen Fritz, Yuchang Jiang.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;Special thanks to early map reviewers: Andrew Lister (United States Forest Service), Astrid Verheggen (Joint Research Centre), Clement Bourgoin (Joint Research Centre), Erin Glen (WRI), Frederic Achard (Joint Research Centre), Jonas Fridman (Swedish University of Agricultural Sciences), Jukka Meiteninen (VTT), Karen Saunders (World Wildlife Fund Canada), Louis Reymondin (Alliance Bioversity International - CIAT), Martin Herold (GFZ Helmholtz Centre for Geosciences), Olga Nepomshina (GFZ Helmholtz Centre for Geosciences), Peter Potapov (University of Maryland/WRI), Rene Colditz (Joint Research Centre), Thibaud Vantalon (Alliance Bioversity International - CIAT), and Viviana Zalles (WRI).&lt;/i&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;Acknowledgments&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;&lt;i&gt;This research was co-developed by Google Deepmind and Google Research in collaboration with WRI and IIASA.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;We thank our collaborators at Google, World Resources Institute (WRI) / Global Forest Watch (GFW), and International Institute for Applied Systems Analysis (IIASA): Anton Raichuk, Charlotte Stanton, Dan Morris, Drew Purves, Elizabeth Goldman, Katelyn Tarrio, Keith Anderson, Maxim Neumann, Mélanie Rey, Michelle J. Sims, Myroslava Lesiv, Nicholas Clinton, Petra Poklukar, Radost Stanimirova, Sarah Carter, Steffen Fritz, Yuchang Jiang.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;Special thanks to early map reviewers: Andrew Lister (United States Forest Service), Astrid Verheggen (Joint Research Centre), Clement Bourgoin (Joint Research Centre), Erin Glen (WRI), Frederic Achard (Joint Research Centre), Jonas Fridman (Swedish University of Agricultural Sciences), Jukka Meiteninen (VTT), Karen Saunders (World Wildlife Fund Canada), Louis Reymondin (Alliance Bioversity International - CIAT), Martin Herold (GFZ Helmholtz Centre for Geosciences), Olga Nepomshina (GFZ Helmholtz Centre for Geosciences), Peter Potapov (University of Maryland/WRI), Rene Colditz (Joint Research Centre), Thibaud Vantalon (Alliance Bioversity International - CIAT), and Viviana Zalles (WRI).&lt;/i&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/separating-natural-forests-from-other-tree-cover-with-ai-for-deforestation-free-supply-chains/</guid><pubDate>Thu, 13 Nov 2025 19:04:07 +0000</pubDate></item><item><title>Baidu unveils proprietary ERNIE 5 beating GPT-5 performance on charts, document understanding and more (AI | VentureBeat)</title><link>https://venturebeat.com/ai/baidu-unveils-proprietary-ernie-5-beating-gpt-5-performance-on-charts</link><description>[unable to retrieve full-text content]&lt;p&gt;Mere hours after OpenAI updated its flagship foundation model &lt;a href="https://venturebeat.com/ai/openai-reboots-chatgpt-experience-with-gpt-5-1-after-mixed-reviews-of-gpt-5"&gt;GPT-5 to GPT-5.1&lt;/a&gt;, promising reduced token usage overall and a more pleasant personality with more preset options, Chinese search giant &lt;a href="https://www.prnewswire.com/news-releases/baidu-unveils-ernie-5-0-and-a-series-of-ai-applications-at-baidu-world-2025--ramps-up-global-push-302614531.html?tc=eml_cleartime"&gt;Baidu unveiled its next-generation foundation model, ERNIE 5.0,&lt;/a&gt; alongside a suite of AI product upgrades and strategic international expansions.&lt;/p&gt;&lt;p&gt;The goal: to position as a global contender in the increasingly competitive enterprise AI market.&lt;/p&gt;&lt;p&gt;Announced at the company&amp;#x27;s Baidu World 2025 event, ERNIE 5.0 is a proprietary, natively omni-modal model designed to jointly process and generate content across text, images, audio, and video. &lt;/p&gt;&lt;p&gt;Unlike Baidu’s recently released &lt;a href="https://venturebeat.com/ai/baidu-just-dropped-an-open-source-multimodal-ai-that-it-claims-beats-gpt-5"&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/a&gt;, which is open source under an enterprise-friendly and permissive Apache 2.0 license, ERNIE 5.0 is a proprietary model and is available only via &lt;a href="https://ernie.baidu.com/"&gt;Baidu’s ERNIE Bot&lt;/a&gt; website (I needed to select it manuallyu from the model picker dropdown) and the &lt;a href="https://cloud.baidu.com/doc/qianfan/s/wmh4sv6ya"&gt;Qianfan cloud platform application programming interface (API) for enterprise customers. &lt;/a&gt;&lt;/p&gt;&lt;p&gt;Alongside the model launch, Baidu introduced major updates to its digital human platform, no-code tools, and general-purpose AI agents — all targeted at expanding its AI footprint beyond China.&lt;/p&gt;&lt;p&gt;The company also introduced ERNIE 5.0 Preview 1022, a variant optimized for text-intensive tasks, alongside the general preview model that balances across modalities.&lt;/p&gt;&lt;p&gt;Baidu emphasized that ERNIE 5.0 represents a shift in how intelligence is deployed at scale, with CEO Robin Li stating: “When you internalize AI, it becomes a native capability and transforms intelligence from a cost into a source of productivity.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Where ERNIE 5.0 outshines GPT-5 and Gemini 2.5 Pro&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;ERNIE 5.0’s benchmark results suggest that Baidu has achieved parity—or near-parity—with the top Western foundation models across a wide spectrum of tasks. &lt;/p&gt;&lt;p&gt;In public benchmark slides shared during the Baidu World 2025 event, ERNIE 5.0 Preview outperformed or matched OpenAI’s GPT-5-High and Google’s Gemini 2.5 Pro in &lt;b&gt;multimodal reasoning, document understanding, and image-based QA&lt;/b&gt;, while also &lt;b&gt;demonstrating strong language modeling and code execution abilities. &lt;/b&gt;&lt;/p&gt;&lt;p&gt;The company emphasized its ability to handle joint inputs and outputs across modalities, rather than relying on post-hoc modality fusion, which it framed as a technical differentiator.&lt;/p&gt;&lt;p&gt;On visual tasks, ERNIE 5.0 achieved leading scores on OCRBench, DocVQA, and ChartQA, three benchmarks that test document recognition, comprehension, and structured data reasoning. &lt;/p&gt;&lt;p&gt;Baidu claims the model beat both GPT-5-High and Gemini 2.5 Pro on these document and chart-based benchmarks, areas it describes as core to enterprise applications like automated document processing and financial analysis. &lt;/p&gt;&lt;p&gt;In image generation, ERNIE 5.0 tied or exceeded Google’s Veo3 across categories including semantic alignment and image quality, according to Baidu’s internal GenEval-based evaluation. Baidu claimed that the model’s multimodal integration allows it to generate and interpret visual content with greater contextual awareness than models relying on modality-specific encoders.&lt;/p&gt;&lt;p&gt;For audio and speech tasks, ERNIE 5.0 demonstrated competitive results on MM-AU and TUT2017 audio understanding benchmarks, as well as question answering from spoken language inputs. Its audio performance, while not as heavily emphasized as vision or text, suggests a broad capability footprint intended to support full-spectrum multimodal applications.&lt;/p&gt;&lt;p&gt;In language tasks, the model showed strong results on instruction following, factual question answering, and mathematical reasoning—core areas that define the enterprise utility of large language models. &lt;/p&gt;&lt;p&gt;The Preview 1022 variant of ERNIE 5.0, tailored for textual performance, showed even stronger language-specific results in early developer access. While Baidu does not claim broad superiority in general language reasoning, its internal evaluations suggest that ERNIE 5.0 Preview 1022 closes the gap with top-tier English-language models and outperforms them in Chinese-language performance.&lt;/p&gt;&lt;p&gt;While Baidu did not release full benchmark details or raw scores publicly, its performance positioning suggests a deliberate attempt to frame ERNIE 5.0 not as a niche multimodal system but as a flagship model competitive with the largest closed models in general-purpose reasoning. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Where Baidu claims a clear lead is in structured document understanding, visual chart reasoning, and integration of multiple modalities into a single, native modeling architecture&lt;/b&gt;. Independent verification of these results remains pending, but the breadth of claimed capabilities positions ERNIE 5.0 as a serious alternative in the multimodal foundation model landscape.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Enterprise Pricing Strategy&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;ERNIE 5.0 is positioned at the &lt;b&gt;premium end&lt;/b&gt; of Baidu’s model pricing structure. The company has released specific pricing for API usage on its Qianfan platform, aligning the cost with other top-tier offerings from Chinese competitors like Alibaba.&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Input Cost (per 1K tokens)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Output Cost (per 1K tokens)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Source&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;ERNIE 5.0&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.00085 (¥0.006)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.0034 (¥0.024)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ftr.bazqux.com/cloud.baidu.com/doc/qianfan/s/wmh4sv6ya"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 4.5 Turbo (ex.)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.00011 (¥0.0008)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.00045 (¥0.0032)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ftr.bazqux.com/cloud.baidu.com/doc/qianfan/s/wmh4sv6ya"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen3 (Coder ex.)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.00085 (¥0.006)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.0034 (¥0.024)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ftr.bazqux.com/cloud.baidu.com/doc/qianfan/s/wmh4sv6ya"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The contrast in cost between ERNIE 5.0 and earlier models such as ERNIE 4.5 Turbo underscores Baidu’s strategy to differentiate between high-volume, low-cost models and high-capability models designed for complex tasks and multimodal reasoning.&lt;/p&gt;&lt;p&gt;Compared to other U.S. alternatives, it remains mid-range in pricing:&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Input (/1 M tokens)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Output (/1 M tokens)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Source&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GPT-5.1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$10.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://openai.com/api/pricing/?utm_source=chatgpt.com"&gt;OpenAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 5.0&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.85&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ftr.bazqux.com/cloud.baidu.com/doc/qianfan/s/wmh4sv6ya"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 4.5 Turbo (ex.)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.11&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.45&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ftr.bazqux.com/cloud.baidu.com/doc/qianfan/s/wmh4sv6ya"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Opus 4.1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$75.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.anthropic.com/claude/opus?utm_source=chatgpt.com"&gt;Anthropic&lt;/a&gt; &lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 2.5 Pro&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.25 (≤200k) / $2.50 (&amp;gt;200k)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$10.00 (≤200k) / $15.00 (&amp;gt;200k)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ftr.bazqux.com/cloud.google.com/vertex-ai/generative-ai/pricing"&gt;Google Vertex AI Pricing&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4 (grok-4-0709)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models/grok-4-0709?utm_source=chatgpt.com"&gt; xAI API&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3&gt;&lt;b&gt;Global Expansion: Products and Platforms&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;In tandem with the model release, Baidu is expanding internationally:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GenFlow 3.0&lt;/b&gt;, now with 20M+ users, is the company’s largest general-purpose AI agent and features enhanced memory and multimodal task handling.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Famou&lt;/b&gt;, a self-evolving agent capable of dynamically solving complex problems, is now commercially available via invite.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;MeDo&lt;/b&gt;, the international version of Baidu’s no-code builder Miaoda, is live globally via &lt;a href="https://medo.dev"&gt;medo.dev&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Oreate&lt;/b&gt;, a productivity workspace with document, slide, image, video, and podcast support, has reached over 1.2M users worldwide.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Baidu’s digital human platform, already rolled out in Brazil, is also part of the global push. According to company data, 83% of livestreamers during this year’s “Double 11” shopping event in China used Baidu’s digital human tech, contributing to a 91% increase in GMV.&lt;/p&gt;&lt;p&gt;Meanwhile, Baidu’s autonomous ride-hailing service Apollo Go has surpassed 17 million rides, operating driverless fleets in 22 cities and claiming the title of the world’s largest robotaxi network.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Open-Source Vision-Language Model Garners Industry Attention&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Two days before the flagship ERNIE 5.0 event, Baidu also released an open-source multimodal model under the Apache 2.0 license: &lt;a href="https://venturebeat.com/ai/baidu-just-dropped-an-open-source-multimodal-ai-that-it-claims-beats-gpt-5"&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;As &lt;a href="https://venturebeat.com/ai/baidu-just-dropped-an-open-source-multimodal-ai-that-it-claims-beats-gpt-5"&gt;reported by my colleague Michael Nuñez at VentureBeat&lt;/a&gt;, the model activates just 3 billion parameters while maintaining a total of 28 billion, using a Mixture-of-Experts (MoE) architecture for efficient inference.&lt;/p&gt;&lt;p&gt;Key technical innovations include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;“Thinking with Images”, which enables dynamic zoom-based visual analysis&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Support for chart interpretation, document understanding, visual grounding, and temporal awareness in video&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Runtime on a single 80GB GPU, making it accessible to mid-sized organizations&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Full compatibility with Transformers, vLLM, and Baidu’s FastDeploy toolkits&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This release adds pressure on closed-source competitors. With Apache 2.0 licensing, ERNIE-4.5-VL-28B-A3B-Thinking becomes a viable foundation model for commercial applications without licensing restrictions — something few high-performing models in this class offer.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Community Feedback and Baidu’s Response&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Following the launch of ERNIE 5.0, developer and AI evaluator Lisan al Gaib (@scaling01) &lt;a href="https://x.com/scaling01/status/1988961630273646872"&gt;posted a mixed review on X.&lt;/a&gt; While initially impressed by the model’s benchmark performance, they reported a persistent issue where ERNIE 5.0 would repeatedly invoke tools — even when explicitly instructed not to — during SVG generation tasks.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;“ERNIE 5.0 benchmarks looked insane until I tested it… unfortunately it’s RL braindamaged or they have a serious issue with their chat platform / system prompt,” Lisan wrote.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;In a matter of hours, Baidu’s developer-focused support account, &lt;a href="https://x.com/ErnieforDevs/status/1989001980430393688"&gt;@ErnieforDevs, responded&lt;/a&gt;:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;“Thanks for the feedback! It’s a known bug — certain syntax can consistently trigger it. We’re working on a fix. You can try rephrasing or changing the prompt to avoid it for now.”&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;The quick turnaround reflects Baidu’s increasing emphasis on developer communication, especially as it courts international users through both proprietary and open-source offerings.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Outlook for Baidu and its ERNIE foundational LLM family&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Baidu’s ERNIE 5.0 marks a strategic escalation in the global foundation model race. With performance claims that put it on par with the most advanced systems from OpenAI and Google, and a mix of premium pricing and open-access alternatives, Baidu is signaling its ambition to become not just a domestic AI leader, but a credible global infrastructure provider.&lt;/p&gt;&lt;p&gt;At a time when enterprise AI users are increasingly demanding multimodal performance, flexible licensing, and deployment efficiency, Baidu’s two-track approach—premium hosted APIs and open-source releases—may broaden its appeal across both corporate and developer communities.&lt;/p&gt;&lt;p&gt;Whether the company’s performance claims hold up under third-party testing remains to be seen. But in a landscape shaped by rising costs, model complexity, and compute bottlenecks, ERNIE 5.0 and its supporting ecosystem give Baidu a competitive position in the next wave of AI deployment.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Mere hours after OpenAI updated its flagship foundation model &lt;a href="https://venturebeat.com/ai/openai-reboots-chatgpt-experience-with-gpt-5-1-after-mixed-reviews-of-gpt-5"&gt;GPT-5 to GPT-5.1&lt;/a&gt;, promising reduced token usage overall and a more pleasant personality with more preset options, Chinese search giant &lt;a href="https://www.prnewswire.com/news-releases/baidu-unveils-ernie-5-0-and-a-series-of-ai-applications-at-baidu-world-2025--ramps-up-global-push-302614531.html?tc=eml_cleartime"&gt;Baidu unveiled its next-generation foundation model, ERNIE 5.0,&lt;/a&gt; alongside a suite of AI product upgrades and strategic international expansions.&lt;/p&gt;&lt;p&gt;The goal: to position as a global contender in the increasingly competitive enterprise AI market.&lt;/p&gt;&lt;p&gt;Announced at the company&amp;#x27;s Baidu World 2025 event, ERNIE 5.0 is a proprietary, natively omni-modal model designed to jointly process and generate content across text, images, audio, and video. &lt;/p&gt;&lt;p&gt;Unlike Baidu’s recently released &lt;a href="https://venturebeat.com/ai/baidu-just-dropped-an-open-source-multimodal-ai-that-it-claims-beats-gpt-5"&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/a&gt;, which is open source under an enterprise-friendly and permissive Apache 2.0 license, ERNIE 5.0 is a proprietary model and is available only via &lt;a href="https://ernie.baidu.com/"&gt;Baidu’s ERNIE Bot&lt;/a&gt; website (I needed to select it manuallyu from the model picker dropdown) and the &lt;a href="https://cloud.baidu.com/doc/qianfan/s/wmh4sv6ya"&gt;Qianfan cloud platform application programming interface (API) for enterprise customers. &lt;/a&gt;&lt;/p&gt;&lt;p&gt;Alongside the model launch, Baidu introduced major updates to its digital human platform, no-code tools, and general-purpose AI agents — all targeted at expanding its AI footprint beyond China.&lt;/p&gt;&lt;p&gt;The company also introduced ERNIE 5.0 Preview 1022, a variant optimized for text-intensive tasks, alongside the general preview model that balances across modalities.&lt;/p&gt;&lt;p&gt;Baidu emphasized that ERNIE 5.0 represents a shift in how intelligence is deployed at scale, with CEO Robin Li stating: “When you internalize AI, it becomes a native capability and transforms intelligence from a cost into a source of productivity.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Where ERNIE 5.0 outshines GPT-5 and Gemini 2.5 Pro&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;ERNIE 5.0’s benchmark results suggest that Baidu has achieved parity—or near-parity—with the top Western foundation models across a wide spectrum of tasks. &lt;/p&gt;&lt;p&gt;In public benchmark slides shared during the Baidu World 2025 event, ERNIE 5.0 Preview outperformed or matched OpenAI’s GPT-5-High and Google’s Gemini 2.5 Pro in &lt;b&gt;multimodal reasoning, document understanding, and image-based QA&lt;/b&gt;, while also &lt;b&gt;demonstrating strong language modeling and code execution abilities. &lt;/b&gt;&lt;/p&gt;&lt;p&gt;The company emphasized its ability to handle joint inputs and outputs across modalities, rather than relying on post-hoc modality fusion, which it framed as a technical differentiator.&lt;/p&gt;&lt;p&gt;On visual tasks, ERNIE 5.0 achieved leading scores on OCRBench, DocVQA, and ChartQA, three benchmarks that test document recognition, comprehension, and structured data reasoning. &lt;/p&gt;&lt;p&gt;Baidu claims the model beat both GPT-5-High and Gemini 2.5 Pro on these document and chart-based benchmarks, areas it describes as core to enterprise applications like automated document processing and financial analysis. &lt;/p&gt;&lt;p&gt;In image generation, ERNIE 5.0 tied or exceeded Google’s Veo3 across categories including semantic alignment and image quality, according to Baidu’s internal GenEval-based evaluation. Baidu claimed that the model’s multimodal integration allows it to generate and interpret visual content with greater contextual awareness than models relying on modality-specific encoders.&lt;/p&gt;&lt;p&gt;For audio and speech tasks, ERNIE 5.0 demonstrated competitive results on MM-AU and TUT2017 audio understanding benchmarks, as well as question answering from spoken language inputs. Its audio performance, while not as heavily emphasized as vision or text, suggests a broad capability footprint intended to support full-spectrum multimodal applications.&lt;/p&gt;&lt;p&gt;In language tasks, the model showed strong results on instruction following, factual question answering, and mathematical reasoning—core areas that define the enterprise utility of large language models. &lt;/p&gt;&lt;p&gt;The Preview 1022 variant of ERNIE 5.0, tailored for textual performance, showed even stronger language-specific results in early developer access. While Baidu does not claim broad superiority in general language reasoning, its internal evaluations suggest that ERNIE 5.0 Preview 1022 closes the gap with top-tier English-language models and outperforms them in Chinese-language performance.&lt;/p&gt;&lt;p&gt;While Baidu did not release full benchmark details or raw scores publicly, its performance positioning suggests a deliberate attempt to frame ERNIE 5.0 not as a niche multimodal system but as a flagship model competitive with the largest closed models in general-purpose reasoning. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Where Baidu claims a clear lead is in structured document understanding, visual chart reasoning, and integration of multiple modalities into a single, native modeling architecture&lt;/b&gt;. Independent verification of these results remains pending, but the breadth of claimed capabilities positions ERNIE 5.0 as a serious alternative in the multimodal foundation model landscape.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Enterprise Pricing Strategy&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;ERNIE 5.0 is positioned at the &lt;b&gt;premium end&lt;/b&gt; of Baidu’s model pricing structure. The company has released specific pricing for API usage on its Qianfan platform, aligning the cost with other top-tier offerings from Chinese competitors like Alibaba.&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Input Cost (per 1K tokens)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Output Cost (per 1K tokens)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Source&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;ERNIE 5.0&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.00085 (¥0.006)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.0034 (¥0.024)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ftr.bazqux.com/cloud.baidu.com/doc/qianfan/s/wmh4sv6ya"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 4.5 Turbo (ex.)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.00011 (¥0.0008)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.00045 (¥0.0032)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ftr.bazqux.com/cloud.baidu.com/doc/qianfan/s/wmh4sv6ya"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen3 (Coder ex.)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.00085 (¥0.006)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.0034 (¥0.024)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ftr.bazqux.com/cloud.baidu.com/doc/qianfan/s/wmh4sv6ya"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The contrast in cost between ERNIE 5.0 and earlier models such as ERNIE 4.5 Turbo underscores Baidu’s strategy to differentiate between high-volume, low-cost models and high-capability models designed for complex tasks and multimodal reasoning.&lt;/p&gt;&lt;p&gt;Compared to other U.S. alternatives, it remains mid-range in pricing:&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Input (/1 M tokens)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Output (/1 M tokens)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Source&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GPT-5.1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$10.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://openai.com/api/pricing/?utm_source=chatgpt.com"&gt;OpenAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 5.0&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.85&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ftr.bazqux.com/cloud.baidu.com/doc/qianfan/s/wmh4sv6ya"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 4.5 Turbo (ex.)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.11&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.45&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ftr.bazqux.com/cloud.baidu.com/doc/qianfan/s/wmh4sv6ya"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Opus 4.1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$75.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.anthropic.com/claude/opus?utm_source=chatgpt.com"&gt;Anthropic&lt;/a&gt; &lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 2.5 Pro&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.25 (≤200k) / $2.50 (&amp;gt;200k)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$10.00 (≤200k) / $15.00 (&amp;gt;200k)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ftr.bazqux.com/cloud.google.com/vertex-ai/generative-ai/pricing"&gt;Google Vertex AI Pricing&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4 (grok-4-0709)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models/grok-4-0709?utm_source=chatgpt.com"&gt; xAI API&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3&gt;&lt;b&gt;Global Expansion: Products and Platforms&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;In tandem with the model release, Baidu is expanding internationally:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GenFlow 3.0&lt;/b&gt;, now with 20M+ users, is the company’s largest general-purpose AI agent and features enhanced memory and multimodal task handling.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Famou&lt;/b&gt;, a self-evolving agent capable of dynamically solving complex problems, is now commercially available via invite.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;MeDo&lt;/b&gt;, the international version of Baidu’s no-code builder Miaoda, is live globally via &lt;a href="https://medo.dev"&gt;medo.dev&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Oreate&lt;/b&gt;, a productivity workspace with document, slide, image, video, and podcast support, has reached over 1.2M users worldwide.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Baidu’s digital human platform, already rolled out in Brazil, is also part of the global push. According to company data, 83% of livestreamers during this year’s “Double 11” shopping event in China used Baidu’s digital human tech, contributing to a 91% increase in GMV.&lt;/p&gt;&lt;p&gt;Meanwhile, Baidu’s autonomous ride-hailing service Apollo Go has surpassed 17 million rides, operating driverless fleets in 22 cities and claiming the title of the world’s largest robotaxi network.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Open-Source Vision-Language Model Garners Industry Attention&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Two days before the flagship ERNIE 5.0 event, Baidu also released an open-source multimodal model under the Apache 2.0 license: &lt;a href="https://venturebeat.com/ai/baidu-just-dropped-an-open-source-multimodal-ai-that-it-claims-beats-gpt-5"&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;As &lt;a href="https://venturebeat.com/ai/baidu-just-dropped-an-open-source-multimodal-ai-that-it-claims-beats-gpt-5"&gt;reported by my colleague Michael Nuñez at VentureBeat&lt;/a&gt;, the model activates just 3 billion parameters while maintaining a total of 28 billion, using a Mixture-of-Experts (MoE) architecture for efficient inference.&lt;/p&gt;&lt;p&gt;Key technical innovations include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;“Thinking with Images”, which enables dynamic zoom-based visual analysis&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Support for chart interpretation, document understanding, visual grounding, and temporal awareness in video&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Runtime on a single 80GB GPU, making it accessible to mid-sized organizations&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Full compatibility with Transformers, vLLM, and Baidu’s FastDeploy toolkits&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This release adds pressure on closed-source competitors. With Apache 2.0 licensing, ERNIE-4.5-VL-28B-A3B-Thinking becomes a viable foundation model for commercial applications without licensing restrictions — something few high-performing models in this class offer.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Community Feedback and Baidu’s Response&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Following the launch of ERNIE 5.0, developer and AI evaluator Lisan al Gaib (@scaling01) &lt;a href="https://x.com/scaling01/status/1988961630273646872"&gt;posted a mixed review on X.&lt;/a&gt; While initially impressed by the model’s benchmark performance, they reported a persistent issue where ERNIE 5.0 would repeatedly invoke tools — even when explicitly instructed not to — during SVG generation tasks.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;“ERNIE 5.0 benchmarks looked insane until I tested it… unfortunately it’s RL braindamaged or they have a serious issue with their chat platform / system prompt,” Lisan wrote.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;In a matter of hours, Baidu’s developer-focused support account, &lt;a href="https://x.com/ErnieforDevs/status/1989001980430393688"&gt;@ErnieforDevs, responded&lt;/a&gt;:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;“Thanks for the feedback! It’s a known bug — certain syntax can consistently trigger it. We’re working on a fix. You can try rephrasing or changing the prompt to avoid it for now.”&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;The quick turnaround reflects Baidu’s increasing emphasis on developer communication, especially as it courts international users through both proprietary and open-source offerings.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Outlook for Baidu and its ERNIE foundational LLM family&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Baidu’s ERNIE 5.0 marks a strategic escalation in the global foundation model race. With performance claims that put it on par with the most advanced systems from OpenAI and Google, and a mix of premium pricing and open-access alternatives, Baidu is signaling its ambition to become not just a domestic AI leader, but a credible global infrastructure provider.&lt;/p&gt;&lt;p&gt;At a time when enterprise AI users are increasingly demanding multimodal performance, flexible licensing, and deployment efficiency, Baidu’s two-track approach—premium hosted APIs and open-source releases—may broaden its appeal across both corporate and developer communities.&lt;/p&gt;&lt;p&gt;Whether the company’s performance claims hold up under third-party testing remains to be seen. But in a landscape shaped by rising costs, model complexity, and compute bottlenecks, ERNIE 5.0 and its supporting ecosystem give Baidu a competitive position in the next wave of AI deployment.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/baidu-unveils-proprietary-ernie-5-beating-gpt-5-performance-on-charts</guid><pubDate>Thu, 13 Nov 2025 20:23:00 +0000</pubDate></item><item><title>Apple’s new App Review Guidelines clamp down on apps sharing personal data with ‘third-party AI’ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/13/apples-new-app-review-guidelines-clamp-down-on-apps-sharing-personal-data-with-third-party-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/01/app-store-2024-v1.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Apple on Thursday introduced a new set of App Review Guidelines for developers, which now specifically state that apps must disclose and obtain users’ permission before sharing personal data with third-party AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The change comes ahead of the iPhone maker’s plan to introduce its own AI-upgraded version of Siri in 2026.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That update will see Apple’s digital assistant offer users the ability to take actions across apps using Siri commands, and will be powered, in part, by Google’s Gemini technology, according to a recent Bloomberg report.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the same time, Apple is ensuring other apps aren’t leaking personal data to AI providers or other AI businesses. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What’s interesting about this particular update is not the requirements being described but that Apple has specifically called out AI companies as needing to come into compliance. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Before the revised language, the guideline known as rule 5.1.2(i) included language around disclosure and obtaining user consent for data sharing, noting that apps could not “use, transmit or share” someone’s personal data without their permission. This rule served as part of Apple’s compliance with data privacy regulations like the EU’s GDPR (General Data Protection Regulation), California’s Consumer Privacy Act, and others, which ensure that users have more control over how their data is collected and shared. Apps that don’t follow the policy can be removed from the App Store.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The newly revised guideline adds the following sentence (emphasis ours): &lt;/p&gt;

&lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt;
&lt;p class="wp-block-paragraph"&gt;You must clearly disclose where personal data will be shared with third parties, &lt;strong&gt;including with third-party AI,&lt;/strong&gt; and obtain explicit permission before doing so.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class="wp-block-paragraph"&gt;This change could impact apps that intend to use AI systems to collect or process information about their users, perhaps to personalize their apps or provide certain functionality. It’s unclear how stringently Apple will enforce the rule, given that the term “AI” could include a variety of technologies — not just LLMs, but also things like machine learning.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The updated rule is one of several revisions to the App Review Guidelines out on Thursday. Other changes are focused on supporting Apple’s new Mini Apps Program, also announced today, as well as tweaks to rules involving creator apps, loan apps, and more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One addition also added crypto exchanges to the list of apps that provide services in highly regulated fields.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/01/app-store-2024-v1.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Apple on Thursday introduced a new set of App Review Guidelines for developers, which now specifically state that apps must disclose and obtain users’ permission before sharing personal data with third-party AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The change comes ahead of the iPhone maker’s plan to introduce its own AI-upgraded version of Siri in 2026.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That update will see Apple’s digital assistant offer users the ability to take actions across apps using Siri commands, and will be powered, in part, by Google’s Gemini technology, according to a recent Bloomberg report.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the same time, Apple is ensuring other apps aren’t leaking personal data to AI providers or other AI businesses. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What’s interesting about this particular update is not the requirements being described but that Apple has specifically called out AI companies as needing to come into compliance. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Before the revised language, the guideline known as rule 5.1.2(i) included language around disclosure and obtaining user consent for data sharing, noting that apps could not “use, transmit or share” someone’s personal data without their permission. This rule served as part of Apple’s compliance with data privacy regulations like the EU’s GDPR (General Data Protection Regulation), California’s Consumer Privacy Act, and others, which ensure that users have more control over how their data is collected and shared. Apps that don’t follow the policy can be removed from the App Store.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The newly revised guideline adds the following sentence (emphasis ours): &lt;/p&gt;

&lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt;
&lt;p class="wp-block-paragraph"&gt;You must clearly disclose where personal data will be shared with third parties, &lt;strong&gt;including with third-party AI,&lt;/strong&gt; and obtain explicit permission before doing so.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class="wp-block-paragraph"&gt;This change could impact apps that intend to use AI systems to collect or process information about their users, perhaps to personalize their apps or provide certain functionality. It’s unclear how stringently Apple will enforce the rule, given that the term “AI” could include a variety of technologies — not just LLMs, but also things like machine learning.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The updated rule is one of several revisions to the App Review Guidelines out on Thursday. Other changes are focused on supporting Apple’s new Mini Apps Program, also announced today, as well as tweaks to rules involving creator apps, loan apps, and more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One addition also added crypto exchanges to the list of apps that provide services in highly regulated fields.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/13/apples-new-app-review-guidelines-clamp-down-on-apps-sharing-personal-data-with-third-party-ai/</guid><pubDate>Thu, 13 Nov 2025 21:14:35 +0000</pubDate></item><item><title>VCs abandon old rules for a ‘funky time’ of investing in AI startups (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/13/vcs-abandon-old-rules-for-a-funky-time-of-investing-in-ai-startups/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/54888675035_dae88c3d06_c.jpg?w=800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;If there’s one thing that VCs agree on when backing AI startups, it’s that AI requires a different investment approach than prior technological shifts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s a funky time,” said Aileen Lee, founder and managing partner of Cowboy Ventures, onstage at TechCrunch Disrupt 2025. The longtime VC noted that the rules of investing have significantly shifted now that some AI companies are leaping from “zero to $100 million in revenue in a single year.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, Lee also noted that, based on her firm’s research, Series A investors aren’t just seeking rapid revenue growth. “It’s an algorithm with different variables and different coefficients.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some of the factors investors now measure, according to Lee, include whether the startup is generating data, the strength of its competitive moat, the founders’ past accomplishments, and the technical depth of the product. “Depending on what your company is, the output of the algorithmic formula is going to be different,” she said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jon McNeill, co-founder and CEO of startup creation firm DVx Ventures, stated that even startups that grow rapidly from inception to $5 million in revenue often struggle to secure follow-on funding. “I think this game has changed, and it is changing dynamically,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;McNeill noted that Series A investors are now applying the same rigorous standards to seed-stage startups that they previously reserved for more mature companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think a lot of investors have figured out that the breakout companies, in most cases,&lt;strong&gt; &lt;/strong&gt;don’t have the best tech,” McNeill said about why Series A VCs are looking so closely at startups’ ability to attract and retain customers. “They have the best go-to market.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Steve Jang, founder and managing partner of Kindred Ventures, disagreed that a strong go-to-market (GTM), an industry term for sales and marketing, holds greater weight for investors. “I don’t think it’s 100% true to say mediocre technology, great GTM wins and raises money and gets customers. I think that it’s a necessary requirement to have both.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While McNeill later clarified that having a solid product is important, he indicated that his initial comment was related to the founders’ need to develop an exceptionally strong sales and marketing strategy right out of the gate. “Investors are getting much more sophisticated on the go-to market than they have in the past,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;(The debate over marketing versus tech was brought to the forefront later during the conference when Roy Lee, founder of the viral startup Cluely, said onstage that launching a product that barely worked, even with massive social media fame, may not always be the best idea.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Aileen Lee added that AI startups are now under pressure to deliver product updates and new features at an unprecedented pace, preempting existing companies that might try to introduce similar products. &amp;nbsp;“If you look at how much OpenAI and Anthropic are shipping, you’re going to have to figure out how to match how much you ship, how quickly and the quality of it,” she said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite the expectations for breakneck growth and fast product development, panelists agreed that the AI industry is still in its very early stages. As Jang put it, “There are no clear, outright winners, even in LLMs. There are competitors nipping at their heels.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This means startups still have a path to unseating perceived leaders, whether they are decades-old companies or fast-moving newcomers.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/54888675035_dae88c3d06_c.jpg?w=800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;If there’s one thing that VCs agree on when backing AI startups, it’s that AI requires a different investment approach than prior technological shifts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s a funky time,” said Aileen Lee, founder and managing partner of Cowboy Ventures, onstage at TechCrunch Disrupt 2025. The longtime VC noted that the rules of investing have significantly shifted now that some AI companies are leaping from “zero to $100 million in revenue in a single year.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, Lee also noted that, based on her firm’s research, Series A investors aren’t just seeking rapid revenue growth. “It’s an algorithm with different variables and different coefficients.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some of the factors investors now measure, according to Lee, include whether the startup is generating data, the strength of its competitive moat, the founders’ past accomplishments, and the technical depth of the product. “Depending on what your company is, the output of the algorithmic formula is going to be different,” she said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jon McNeill, co-founder and CEO of startup creation firm DVx Ventures, stated that even startups that grow rapidly from inception to $5 million in revenue often struggle to secure follow-on funding. “I think this game has changed, and it is changing dynamically,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;McNeill noted that Series A investors are now applying the same rigorous standards to seed-stage startups that they previously reserved for more mature companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think a lot of investors have figured out that the breakout companies, in most cases,&lt;strong&gt; &lt;/strong&gt;don’t have the best tech,” McNeill said about why Series A VCs are looking so closely at startups’ ability to attract and retain customers. “They have the best go-to market.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Steve Jang, founder and managing partner of Kindred Ventures, disagreed that a strong go-to-market (GTM), an industry term for sales and marketing, holds greater weight for investors. “I don’t think it’s 100% true to say mediocre technology, great GTM wins and raises money and gets customers. I think that it’s a necessary requirement to have both.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While McNeill later clarified that having a solid product is important, he indicated that his initial comment was related to the founders’ need to develop an exceptionally strong sales and marketing strategy right out of the gate. “Investors are getting much more sophisticated on the go-to market than they have in the past,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;(The debate over marketing versus tech was brought to the forefront later during the conference when Roy Lee, founder of the viral startup Cluely, said onstage that launching a product that barely worked, even with massive social media fame, may not always be the best idea.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Aileen Lee added that AI startups are now under pressure to deliver product updates and new features at an unprecedented pace, preempting existing companies that might try to introduce similar products. &amp;nbsp;“If you look at how much OpenAI and Anthropic are shipping, you’re going to have to figure out how to match how much you ship, how quickly and the quality of it,” she said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite the expectations for breakneck growth and fast product development, panelists agreed that the AI industry is still in its very early stages. As Jang put it, “There are no clear, outright winners, even in LLMs. There are competitors nipping at their heels.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This means startups still have a path to unseating perceived leaders, whether they are decades-old companies or fast-moving newcomers.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/13/vcs-abandon-old-rules-for-a-funky-time-of-investing-in-ai-startups/</guid><pubDate>Thu, 13 Nov 2025 23:18:49 +0000</pubDate></item></channel></rss>