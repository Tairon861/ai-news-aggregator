<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 20 Feb 2026 02:24:32 +0000</lastBuildDate><item><title>Co-founders behind Reface and Prisma join hands to improve on-device model inference with Mirai (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/19/co-founders-behind-reface-and-prisma-join-hands-to-improve-on-device-model-inference-with-mirai/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Much of the conversation around AI today is focused on building cloud capacity and massive data centers to run models. Companies like Apple and Qualcomm are in the early stages of making on-device AI more useful. Amid all that, the 14-person technical team of London-based Mirai is working to improve how models run on phones and laptops.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mirai, which is backed by a $10 million seed round led by Uncork Capital, was founded by Dima Shvets and Alexey Moiseenkov last year. Both founders have experience in building scalable consumer apps. Shvets co-founded face-swapping app Reface, which was backed by a16z. Shvets later also became a scout for the venture firm. Moiseenkov was CEO and co-founder of the last decade’s viral AI filters app, Prisma.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As consumer developers, both had been thinking about AI and machine learning on devices even before generative AI became popular, Shvets said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“When we met together in London, we started to chat about technology, and we realized that within the hype of GenAI and more AI adoption, everybody speaks about cloud, about servers, about AGI coming. But the missing piece is on-device [AI] for consumer hardware,” he told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Shvets and Moiseenkov wanted to use AI to create a pipeline that would allow them to enable complex tasks on the phone, which led them to start Mirai. When they asked others who developed consumer apps, they heard that many wanted better cost optimization and margin per token usage, too.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3094301" height="402" src="https://techcrunch.com/wp-content/uploads/2026/02/Mirai-cofounders1.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Co-founders Alexey Moiseenkov and Dima Shvets&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Mirai&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Today, Mirai is developing a framework for models so they can perform better on devices. The company has built an inference engine for Apple Silicon that optimizes on-device throughput. With its upcoming SDK, developers can integrate the runtime in their apps with only a few lines, the company says.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“One of the visions why we started the company was that we wanted to give developers, like this Stripe-like, eight lines of code [integration] experience…you basically go to our platform, integrate the key, and start working with summarization, classification, or whatever your use case is,” Shvets said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup built this engine in Rust, which can bump up a model’s generation speed by up to 37%, they claim. The company said that, while tuning the model for a platform, it doesn’t tinker with model weights to ensure there is no loss in quality of the output.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mirai’s stack currently focuses on improving text and voice modalities on the platform, with plans to support vision in the future. The team has started to work with frontier model providers to tune their models for edge use and is in talks with different chipmakers. Later, it plans to bring its engine to Android, too. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, Mirai aims to release on-device benchmarks so model makers can test on-device performance. Shvets recognizes that not all AI work can be done on-device, though. To enable a mixed mode of operation, the team is building an orchestration layer to send requests that can’t be fulfilled on the device up to the cloud.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While the startup is not directly working with apps just yet, its engine could power on-device assistants, transcribers, translators, and chat apps, we’re told.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Andy McLoughlin, managing partner at Uncork Capital, noted that he invested in an edge machine learning company in the last decade. He said that the company was early and eventually sold its business to Spotify. In today’s world, the situation is different, he thinks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Given the cost of cloud inference, something has to change… For now, VCs are happy to continue funding the rocket ship companies, spending inordinate sums on cloud inference. But that won’t last  —  at some point, people will focus on the underlying economics of these businesses and realize that something has to change,” he said. “It feels like every model maker will want&amp;nbsp;to run part&amp;nbsp;of their inference workloads at the edge, and Mirai&amp;nbsp;feels very well-positioned to capture this demand.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mirai’s seed round also saw participation from individuals, including Dreamer CEO David Singleton, YC Partner Francois Chaubard, Snowflake co-founder Marcin Żukowski, ElevenLabs co-founder Mati Staniszewski, former Google AdSense product manager and Coinbase board member Gokul Rajaram, Groq investor Scooter Braun, Turing.com CTO Vijay Krishnan, Theory Forge Ventures’ Ben Parr and Matt Schlicht, and ex-Netflix technical leader Aditya Jami.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Much of the conversation around AI today is focused on building cloud capacity and massive data centers to run models. Companies like Apple and Qualcomm are in the early stages of making on-device AI more useful. Amid all that, the 14-person technical team of London-based Mirai is working to improve how models run on phones and laptops.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mirai, which is backed by a $10 million seed round led by Uncork Capital, was founded by Dima Shvets and Alexey Moiseenkov last year. Both founders have experience in building scalable consumer apps. Shvets co-founded face-swapping app Reface, which was backed by a16z. Shvets later also became a scout for the venture firm. Moiseenkov was CEO and co-founder of the last decade’s viral AI filters app, Prisma.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As consumer developers, both had been thinking about AI and machine learning on devices even before generative AI became popular, Shvets said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“When we met together in London, we started to chat about technology, and we realized that within the hype of GenAI and more AI adoption, everybody speaks about cloud, about servers, about AGI coming. But the missing piece is on-device [AI] for consumer hardware,” he told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Shvets and Moiseenkov wanted to use AI to create a pipeline that would allow them to enable complex tasks on the phone, which led them to start Mirai. When they asked others who developed consumer apps, they heard that many wanted better cost optimization and margin per token usage, too.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3094301" height="402" src="https://techcrunch.com/wp-content/uploads/2026/02/Mirai-cofounders1.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Co-founders Alexey Moiseenkov and Dima Shvets&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Mirai&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Today, Mirai is developing a framework for models so they can perform better on devices. The company has built an inference engine for Apple Silicon that optimizes on-device throughput. With its upcoming SDK, developers can integrate the runtime in their apps with only a few lines, the company says.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“One of the visions why we started the company was that we wanted to give developers, like this Stripe-like, eight lines of code [integration] experience…you basically go to our platform, integrate the key, and start working with summarization, classification, or whatever your use case is,” Shvets said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup built this engine in Rust, which can bump up a model’s generation speed by up to 37%, they claim. The company said that, while tuning the model for a platform, it doesn’t tinker with model weights to ensure there is no loss in quality of the output.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mirai’s stack currently focuses on improving text and voice modalities on the platform, with plans to support vision in the future. The team has started to work with frontier model providers to tune their models for edge use and is in talks with different chipmakers. Later, it plans to bring its engine to Android, too. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, Mirai aims to release on-device benchmarks so model makers can test on-device performance. Shvets recognizes that not all AI work can be done on-device, though. To enable a mixed mode of operation, the team is building an orchestration layer to send requests that can’t be fulfilled on the device up to the cloud.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While the startup is not directly working with apps just yet, its engine could power on-device assistants, transcribers, translators, and chat apps, we’re told.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Andy McLoughlin, managing partner at Uncork Capital, noted that he invested in an edge machine learning company in the last decade. He said that the company was early and eventually sold its business to Spotify. In today’s world, the situation is different, he thinks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Given the cost of cloud inference, something has to change… For now, VCs are happy to continue funding the rocket ship companies, spending inordinate sums on cloud inference. But that won’t last  —  at some point, people will focus on the underlying economics of these businesses and realize that something has to change,” he said. “It feels like every model maker will want&amp;nbsp;to run part&amp;nbsp;of their inference workloads at the edge, and Mirai&amp;nbsp;feels very well-positioned to capture this demand.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mirai’s seed round also saw participation from individuals, including Dreamer CEO David Singleton, YC Partner Francois Chaubard, Snowflake co-founder Marcin Żukowski, ElevenLabs co-founder Mati Staniszewski, former Google AdSense product manager and Coinbase board member Gokul Rajaram, Groq investor Scooter Braun, Turing.com CTO Vijay Krishnan, Theory Forge Ventures’ Ben Parr and Matt Schlicht, and ex-Netflix technical leader Aditya Jami.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/19/co-founders-behind-reface-and-prisma-join-hands-to-improve-on-device-model-inference-with-mirai/</guid><pubDate>Thu, 19 Feb 2026 14:43:58 +0000</pubDate></item><item><title>OpenAI, Reliance partner to add AI search to JioHotstar (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/19/openai-reliance-partner-to-add-ai-search-to-jiohotstar/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/openai-india.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI is partnering with Reliance to add AI-powered conversational search to the Indian conglomerate’s streaming service JioHotstar. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature, which is powered by OpenAI’s API, will let users search for movies, shows, and live sports using text and voice prompts in multiple languages, and receive recommendations based on their preferences and history.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI has recently expanded its footprint in India, which is home to more than 100 million weekly ChatGPT users. The company plans to open offices in Mumbai and Bengaluru later this year, adding to its current office in New Delhi.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The partnership was announced at the ongoing India AI Impact Summit in New Delhi, where Sam Altman is appearing alongside industry leaders, including Anthropic’s Dario Amodei and Google’s Sundar Pichai.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The companies also plan to extend the partnership to surface JioHotstar recommendations directly within ChatGPT, allowing users who search for entertainment through ChatGPT to receive contextual suggestions and deep links into the platform’s catalogue. The move positions the integration as a two-way discovery layer rather than a standalone in-app feature.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The move comes as streaming and TV platforms increasingly experiment with conversational interfaces. Netflix said in May 2025 it was testing a new search experience using OpenAI’s ChatGPT to help viewers find content through natural language, while Google in November introduced Gemini-powered discovery features for its Google TV platform.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fidji Simo, chief executive of applications at OpenAI, said the partnership is aimed at bringing more personalized AI experiences into entertainment and live sports, enabling viewers to move “from curiosity to context” through natural interactions.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Embedding AI into content discovery will help reshape how audiences find and engage with programming, said Uday Shankar, vice chairperson of JioStar.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The rollout will span both live and on-demand formats, the companies said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The JioHotstar partnership is part of OpenAI’s broader “OpenAI for India” push to deepen its presence in the South Asian nation through infrastructure, enterprise, and education partnerships. Other moves under the initiative include collaborations with the Tata Group on AI-ready data centers and enterprise deployments, as well as agreements with Indian companies such as Pine Labs, Eternal, and MakeMyTrip.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/openai-india.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI is partnering with Reliance to add AI-powered conversational search to the Indian conglomerate’s streaming service JioHotstar. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature, which is powered by OpenAI’s API, will let users search for movies, shows, and live sports using text and voice prompts in multiple languages, and receive recommendations based on their preferences and history.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI has recently expanded its footprint in India, which is home to more than 100 million weekly ChatGPT users. The company plans to open offices in Mumbai and Bengaluru later this year, adding to its current office in New Delhi.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The partnership was announced at the ongoing India AI Impact Summit in New Delhi, where Sam Altman is appearing alongside industry leaders, including Anthropic’s Dario Amodei and Google’s Sundar Pichai.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The companies also plan to extend the partnership to surface JioHotstar recommendations directly within ChatGPT, allowing users who search for entertainment through ChatGPT to receive contextual suggestions and deep links into the platform’s catalogue. The move positions the integration as a two-way discovery layer rather than a standalone in-app feature.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The move comes as streaming and TV platforms increasingly experiment with conversational interfaces. Netflix said in May 2025 it was testing a new search experience using OpenAI’s ChatGPT to help viewers find content through natural language, while Google in November introduced Gemini-powered discovery features for its Google TV platform.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fidji Simo, chief executive of applications at OpenAI, said the partnership is aimed at bringing more personalized AI experiences into entertainment and live sports, enabling viewers to move “from curiosity to context” through natural interactions.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Embedding AI into content discovery will help reshape how audiences find and engage with programming, said Uday Shankar, vice chairperson of JioStar.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The rollout will span both live and on-demand formats, the companies said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The JioHotstar partnership is part of OpenAI’s broader “OpenAI for India” push to deepen its presence in the South Asian nation through infrastructure, enterprise, and education partnerships. Other moves under the initiative include collaborations with the Tata Group on AI-ready data centers and enterprise deployments, as well as agreements with Indian companies such as Pine Labs, Eternal, and MakeMyTrip.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/19/openai-reliance-partner-to-add-ai-search-to-jiohotstar/</guid><pubDate>Thu, 19 Feb 2026 14:45:29 +0000</pubDate></item><item><title>Reload wants to give your AI agents a shared memory (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/19/reload-an-ai-employee-agent-management-platform-raises-2-275m-and-launches-an-ai-employee/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Reload-Co-Founders-Headshot.png?w=1056" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;There came a point when Newton Asare realized AI agents weren’t just tools anymore. “They were operating more like teammates,” he told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The realization crystallized when Asare and Kiran Das, both serial founders, noticed they were using AI agents to perform tasks they usually would have done themselves. Asare said he came to believe that the future lay in people managing AI employees.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&amp;nbsp;“And if that’s true, we’ll need a real system to manage them, with structure around onboarding, coordination, and oversight for digital workers,” he added.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, the duo launched Reload, an AI workforce management platform. On Thursday, the company announced its first AI product, Epic, alongside a $2.275 million round led by Anthemis, with participation from Zeal Capital Partners, Plug and Play, Cohen Circle, Blueprint, and Axiom.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reload is a platform that lets organizations manage their AI agents across teams and departments. Companies can connect agents, regardless of who built them (whether by a third party or internally), assign them roles and permissions, and track the work they perform. “Reload acts like the system of record for AI employees, providing visibility, coordination, and oversight as agents operate across functions,” said Asare, the company’s CEO.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Right now, he observed, teams are using multiple agents simultaneously for tasks such as coding, debugging, and refactoring. The problem is that these agents are often focused solely on whatever they were prompted to do and don’t necessarily retain long-term memory of what a product is or why they were told to perform a specific function. They operate, in other words, with only short-term memory.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Over time, an agent can lose context, or the system can evolve away from its original intent. That’s why Reload is launching Epic. Built on top of the Reload platform, it serves as an architect alongside other coding agents, continuously defining a product’s requirements and constraints, and reminding agents what they are building and why, to keep a system consistent as it develops.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“In software development specifically, coding agents can generate large amounts of code, but they don’t preserve shared system understanding over time,” Asare said. “Epic complements those agents by defining the system upfront and maintaining shared context as it evolves. It doesn’t replace coding agents; it makes them more effective.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Epic is designed to live inside the coding environments where developers already work. It can be installed as an extension in AI-assisted code editors like Cursor and Windsurf, running alongside other agents inside these tools.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“When a team starts a project, Epic helps create the core system artifacts such as product requirements, data models, API specifications, tech stack decisions, diagrams, and structured task breakdowns,” Asare said, adding that these are the foundations that coding agents build against.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“As development progresses, Epic maintains a structured memory of decisions, code changes, and patterns,” he continued. “If you switch coding agents, your structure and memory follow. If multiple engineers use different agents on the same project, everyone builds against the same shared source of truth.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Asare and Das previously had a company together that was acquired and this is their second company together.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI infrastructure space is crowded. Competitors include LongChain, which helps with AI agent deployment and memory management, and CrewAI, which helps enterprises manage their AI agents.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Das said Epic is different because it “defines the system upfront and maintains shared project-level context across agents and sessions,” with a focus specifically on building infrastructure to maintain AI employees. “Traditional workforce systems weren’t designed for AI agents operating as teammates,” said Das, who serves as the company’s CTO. “That’s the layer we’re focused on.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The fresh capital will go toward hiring and product advancement, specifically expanding the infrastructure needed to support a growing number of AI agents. “We’re building for the next era of work,” Asare said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This piece was updated to add the other investors in the round. &lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Reload-Co-Founders-Headshot.png?w=1056" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;There came a point when Newton Asare realized AI agents weren’t just tools anymore. “They were operating more like teammates,” he told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The realization crystallized when Asare and Kiran Das, both serial founders, noticed they were using AI agents to perform tasks they usually would have done themselves. Asare said he came to believe that the future lay in people managing AI employees.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&amp;nbsp;“And if that’s true, we’ll need a real system to manage them, with structure around onboarding, coordination, and oversight for digital workers,” he added.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, the duo launched Reload, an AI workforce management platform. On Thursday, the company announced its first AI product, Epic, alongside a $2.275 million round led by Anthemis, with participation from Zeal Capital Partners, Plug and Play, Cohen Circle, Blueprint, and Axiom.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reload is a platform that lets organizations manage their AI agents across teams and departments. Companies can connect agents, regardless of who built them (whether by a third party or internally), assign them roles and permissions, and track the work they perform. “Reload acts like the system of record for AI employees, providing visibility, coordination, and oversight as agents operate across functions,” said Asare, the company’s CEO.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Right now, he observed, teams are using multiple agents simultaneously for tasks such as coding, debugging, and refactoring. The problem is that these agents are often focused solely on whatever they were prompted to do and don’t necessarily retain long-term memory of what a product is or why they were told to perform a specific function. They operate, in other words, with only short-term memory.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Over time, an agent can lose context, or the system can evolve away from its original intent. That’s why Reload is launching Epic. Built on top of the Reload platform, it serves as an architect alongside other coding agents, continuously defining a product’s requirements and constraints, and reminding agents what they are building and why, to keep a system consistent as it develops.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“In software development specifically, coding agents can generate large amounts of code, but they don’t preserve shared system understanding over time,” Asare said. “Epic complements those agents by defining the system upfront and maintaining shared context as it evolves. It doesn’t replace coding agents; it makes them more effective.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Epic is designed to live inside the coding environments where developers already work. It can be installed as an extension in AI-assisted code editors like Cursor and Windsurf, running alongside other agents inside these tools.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“When a team starts a project, Epic helps create the core system artifacts such as product requirements, data models, API specifications, tech stack decisions, diagrams, and structured task breakdowns,” Asare said, adding that these are the foundations that coding agents build against.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“As development progresses, Epic maintains a structured memory of decisions, code changes, and patterns,” he continued. “If you switch coding agents, your structure and memory follow. If multiple engineers use different agents on the same project, everyone builds against the same shared source of truth.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Asare and Das previously had a company together that was acquired and this is their second company together.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI infrastructure space is crowded. Competitors include LongChain, which helps with AI agent deployment and memory management, and CrewAI, which helps enterprises manage their AI agents.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Das said Epic is different because it “defines the system upfront and maintains shared project-level context across agents and sessions,” with a focus specifically on building infrastructure to maintain AI employees. “Traditional workforce systems weren’t designed for AI agents operating as teammates,” said Das, who serves as the company’s CTO. “That’s the layer we’re focused on.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The fresh capital will go toward hiring and product advancement, specifically expanding the infrastructure needed to support a growing number of AI agents. “We’re building for the next era of work,” Asare said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This piece was updated to add the other investors in the round. &lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/19/reload-an-ai-employee-agent-management-platform-raises-2-275m-and-launches-an-ai-employee/</guid><pubDate>Thu, 19 Feb 2026 15:00:00 +0000</pubDate></item><item><title>「データ不足」の壁を越える：合成ペルソナが日本のAI開発を加速 (Hugging Face - Blog)</title><link>https://huggingface.co/blog/nvidia/nemotron-personas-japan-nttdata-ja</link><description>&lt;div class="not-prose mb-6 font-sans lg:hidden"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8"&gt;
	


	&lt;ul class="flex items-center text-gray-600  flex-row  text-base   "&gt;&lt;li class=" -mr-2 h-5 w-5 md:h-6 md:w-6   bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800 " title="Atsunori"&gt;&lt;img alt="alt" class="overflow-hidden rounded-full" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1615517039409-noauth.jpeg" /&gt;
					
			&lt;/li&gt;

		&lt;li class="text-xs hover:text-gray-700 dark:text-gray-400 dark:hover:text-gray-300 order-last ml-3"&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;

&lt;dialog class="shadow-alternate z-40 mx-4 my-auto h-fit select-text overflow-hidden rounded-xl bg-white max-sm:max-w-[calc(100dvw-2rem)] sm:mx-auto lg:mt-26 md:portrait:mt-30 xl:mt-30 2xl:mt-32 w-full sm:w-96 max-w-[calc(100%-4rem)] text-base not-prose"&gt;
	&lt;/dialog&gt;&lt;/div&gt;&lt;/div&gt;
					&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Yev Meyer's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/NCK3br1uL-gfkVWmd_VRQ.png" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;
AI は日本の経済成長における新たな章を描く可能性を秘めており、その技術によって 100 兆円 (6,500 億米ドル) を超える経済価値が創出されると予測されています。しかし、その巨大なポテンシャルを実現できるかどうかは、多くのAIプロジェクトに決定的に欠けている“ある1つの要素”にかかっています。それは、実務で「使える学習データ」です。
&lt;p&gt;この課題は、日本語と日本文化を理解する AI システムを構築する開発者にとって特に深刻です。英語の学習データは豊富にある一方で、日本の開発者は慢性的なデータ不足という問題に直面しています。高性能なモデルを初期段階から立ち上げるための、タスクに特化し、かつ日本の文化に根ざしたデータが圧倒的に不足しているのです。新しいサンプルの収集、クリーニング、ラベル付けには時間と費用がかかり、目まぐるしいAIの開発サイクルに追いつくことは困難です。&lt;/p&gt;
&lt;p&gt;その結果、イノベーションが始まる前にそれを阻むデータの壁が生まれます。&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		新たな前進への道
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;img alt="image" src="https://cdn-uploads.huggingface.co/production/uploads/68d2fec8856b85d927e44d32/_Fk8_hH3YBUR26fdsT-56.png" /&gt;
大手 IT 企業 NTT DATA による新たな研究は、合成データによってこの壁がいかに取り払われるかを実証しています。手元にある最小限の独自データから、プライバシーやモデルの性能を損なうことなく、実運用レベルの大規模な学習データセットを生成できるのです。&lt;/p&gt;
&lt;p&gt;NTT DATA は、NVIDIA Nemotron-Personas-Japan (NeMo Data Designer を使用して生成された、日本の人口動態、地理、文化に基づいた 600 万のペルソナから構成されるNVIDIA の初のオープン合成データセット) を使用することで、法務Q&amp;amp;Aタスクにおいてモデルの精度を 15.3% から 79.3% へと飛躍的に向上させ、回答の一貫性においても同様の大幅な改善を達成しました。&lt;/p&gt;
&lt;p&gt;これは、機密データを学習パイプラインに公開することなく、60 ポイントもの向上を実現したことになります。&lt;/p&gt;
&lt;p&gt;実験の全体的な手法や評価フレームワークに関心のある読者に向けて、NTT DATA の詳細な技術レポート（日本語）では、本研究の設計や結果についてさらに深く掘り下げて解説しています。&lt;/p&gt;
&lt;p&gt;ここから得られる重要なポイントは、企業は完全にオープンソースのインフラストラクチャを使用し、手元にある最小限の独自データからでも、特定のドメイン（業務領域）に特化したAIを構築できるということです。オープンなペルソナデータを活用することで、より高品質なモデルの構築と、より機敏なデータ運用の両立が可能になります。&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		実証実験
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;このアプローチを厳密に検証するため、NTT DATA は架空の法律文書を用いた対照評価を実施し、モデルが真に新しい知識を獲得できるようにしました。学習には以下の構成を活用しました：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ベースモデル:&lt;/strong&gt; &lt;code&gt;tsuzumi-v2.0-28B-instruct&lt;/code&gt; (NTT の独自 LLM)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;データ拡張モデル:&lt;/strong&gt; &lt;code&gt;GPT-OSS-120b&lt;/code&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;シードデータ:&lt;/strong&gt; Nemotron-Personas-Japan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;判定モデル:&lt;/strong&gt; &lt;code&gt;GPT-5&lt;/code&gt; (LLM-as-a-judge メソッド)&lt;/p&gt;
&lt;p&gt;Nemotron-Personas-Japan から抽出した500のペルソナを活用し、わずか 450 件の未加工のシードサンプルを拡張することで、13 万 8000 件以上の学習用データ (人手による同等のサンプルの 300 倍に相当する合成データセット) を生成し、モデルの精度を 15.3% から 79.3% に向上させました。&lt;/p&gt;
&lt;p&gt;この結果は、企業が直面するデータ不足という課題を如実に物語っています。&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;構成&lt;/th&gt;
&lt;th align="right"&gt;シードデータ&lt;/th&gt;
&lt;th align="right"&gt;合成拡張&lt;/th&gt;
&lt;th align="right"&gt;精度&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;ベースライン (トレーニングなし)&lt;/td&gt;
&lt;td align="right"&gt;—&lt;/td&gt;
&lt;td align="right"&gt;—&lt;/td&gt;
&lt;td align="right"&gt;15.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;合成データを使用したSFT&lt;/td&gt;
&lt;td align="right"&gt;450件&lt;/td&gt;
&lt;td align="right"&gt;138,000 件&lt;/td&gt;
&lt;td align="right"&gt;79.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;合成データによる学習は、単なる精度の向上にとどまらず、ベースラインモデルを悩ませていたハルシネーションも排除しました。学習前のモデルがもっともらしいものの誤った法的分類を生成したのに対し、ファインチューニングされたモデルはノイズを加えることなく正確な用語を抽出できるようになりました。&lt;/p&gt;
&lt;p&gt;エンタープライズ環境への展開においておそらく最も価値のある発見は、十分な量のファインチューニング用合成データが確保できれば、「継続事前学習（CPT）」は必須ではなくなると NTT DATA が見出したことです。これはつまり、開発者は計算リソースを大量に消費する CPT の工程を完全に省略し、教師ありファインチューニング (SFT) のためのより反復的な合成データ生成に注力するという、より費用対効果の高い学習パイプラインを活用できることを意味しています。&lt;/p&gt;
&lt;p&gt;この効率性の向上は、コンピューティングコストの削減と開発サイクルの高速化に直接つながります。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NTT DATA 技術革新統括本部 AI 技術部 部長の樋口晋也氏は次のように話しています。「Nemotron Personas を用いて少量の独自データセットを拡張することで、利用可能なデータが限られている場合でも、タスクに特化したモデルを効果的に構築できます。このアプローチは、独自データが不足しがちな事前調査、カスタマーサポート、マーケティングなどの領域において、成果を向上させる大きな可能性を示しています」&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="Screenshot 2026-02-19 at 7.21.41 AM" src="https://cdn-uploads.huggingface.co/production/uploads/68d2fec8856b85d927e44d32/TZE6YYFdp-f0MgIhFA0cI.png" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		設計段階からのプライバシー保護
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;ここでの精度向上は魅力的ですが、同時により深い疑問も生じます。そもそも学習パイプラインにすら入らない（使えない）データはどうなるのでしょうか？&lt;/p&gt;
&lt;p&gt;価値ある企業データの 90% 以上が、プライバシー規制、セキュリティリスク、ライセンス制約のために未活用のままです。日本では、個人情報保護法 (PIPA) やイノベーション重視の AI ガバナンスガイドライン (2025 年 9 月公表) などの枠組みがこの現実を裏付けています。AI の進歩が加速する中でも、責任あるデータ取り扱いは必須です。&lt;/p&gt;
&lt;p&gt;合成データは、この相反する課題を解決する道筋を提供します。個人を特定できる情報 (PII) を含まず、実際のデータの傾向（パターン）を正確に反映した学習用データを生成することで、企業はデータの最小化とモデルの性能向上を同時に実現できます。初期の立ち上げには最小限の独自データのみを使用し、その後は合成データによって実運用レベルの規模まで拡張すればよいのです。&lt;/p&gt;
&lt;p&gt;つまり、合成データは単なる「学習プロセスを最適化する手法」ではありません。データコンプライアンスと AI の性能が共存する理想的なバランス(ゴルディロックスゾーン) を実現するプライバシー強化技術 (PET) なのです。さらに、データの合成パイプラインは再現性と監査性を備えているため、ガバナンスチームや規制当局がますます求める信頼性と透明性の要件にも対応できます。&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		ソブリンデータ空間
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;ソブリン AI を構築する日本企業にとって、データ主権は必須条件です。しかし、主権だけでは十分ではありません。モデルには、欧米中心のコーパスに統計的に偏ったものではなく、地域固有の規範やドメインの制約によって形成される、根拠のあるインテリジェンスも必要です。Nemotron-Personas-Japan は、この現実に根ざした AI を作るための基盤データとして機能します。600 万のペルソナは日本の公式人口動態および労働統計に基づいており、1,500 以上の職業分類と地域分布をカバーしています。&lt;/p&gt;
&lt;p&gt;しかし、その影響は個々の組織にとどまりません。NTT DATA をはじめとするリーダー企業は、「データスペース」の開発に積極的に取り組んでいます。これは、政府と企業が共通のガバナンスとプライバシー保証の下で、AI学習用に合成されたデータを交換し合える協調的な環境です。連合学習（フェデレーテッド ラーニング）などのエンドツーエンドの暗号化技術は、この分散型アプローチを可能にします。合成データはこれをさらに強力に推進する役割を果たし、組織は元となる機密情報を公開することなく、自社データの傾向（パターン）を合成データとして安全に提供できるようになります。&lt;/p&gt;
&lt;p&gt;これにより、データリスク管理は守りの姿勢から、日本の掲げる『イノベーション主導のAIガバナンス』というビジョンに沿った「協調的な姿勢」へとシフトします。また、このアプローチは、「AIの進化は、グローバルで学習された少数の巨大モデルからもたらされるべきだ」という固定観念にも一石を投じます。むしろ、オープンでプライバシー保護された基盤の上に、主権を持ち、相互運用可能な AI システムがそれぞれの地域で構築される未来を指し示しています。&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		構築を開始
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;「データの壁」は確かに存在します。しかし、NTT DATA の調査が示すように、それを克服するためのツールは今やオープンで誰でもアクセスできるようになっています。合成データは、もはや「未来の技術」ではありません 。プライバシーや性能を犠牲にすることなく、データ主権を持ち、日本の文化に根ざしたAIシステムを構築するために、開発者が「今すぐ」現場に導入できる現実のソリューションなのです。&lt;/p&gt;
&lt;p&gt;さっそく始めてみませんか？オープンソースのNeMo Data Designer ライブラリを活用するか、Hugging Face で公開されている Nemotron-Personas-Japan データセットをご覧ください。より詳細な技術的情報については、手法と実験設計を網羅したNTT データによる詳細なレポート (日本語) をご覧ください。&lt;/p&gt;

&lt;p&gt;Nemotron-Personas-Japan は、CC BY 4.0 ライセンスに基づき、商用・非商用を問わずご利用いただけます。&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;div class="not-prose mb-6 font-sans lg:hidden"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8"&gt;
	


	&lt;ul class="flex items-center text-gray-600  flex-row  text-base   "&gt;&lt;li class=" -mr-2 h-5 w-5 md:h-6 md:w-6   bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800 " title="Atsunori"&gt;&lt;img alt="alt" class="overflow-hidden rounded-full" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1615517039409-noauth.jpeg" /&gt;
					
			&lt;/li&gt;

		&lt;li class="text-xs hover:text-gray-700 dark:text-gray-400 dark:hover:text-gray-300 order-last ml-3"&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;

&lt;dialog class="shadow-alternate z-40 mx-4 my-auto h-fit select-text overflow-hidden rounded-xl bg-white max-sm:max-w-[calc(100dvw-2rem)] sm:mx-auto lg:mt-26 md:portrait:mt-30 xl:mt-30 2xl:mt-32 w-full sm:w-96 max-w-[calc(100%-4rem)] text-base not-prose"&gt;
	&lt;/dialog&gt;&lt;/div&gt;&lt;/div&gt;
					&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Yev Meyer's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/NCK3br1uL-gfkVWmd_VRQ.png" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;
AI は日本の経済成長における新たな章を描く可能性を秘めており、その技術によって 100 兆円 (6,500 億米ドル) を超える経済価値が創出されると予測されています。しかし、その巨大なポテンシャルを実現できるかどうかは、多くのAIプロジェクトに決定的に欠けている“ある1つの要素”にかかっています。それは、実務で「使える学習データ」です。
&lt;p&gt;この課題は、日本語と日本文化を理解する AI システムを構築する開発者にとって特に深刻です。英語の学習データは豊富にある一方で、日本の開発者は慢性的なデータ不足という問題に直面しています。高性能なモデルを初期段階から立ち上げるための、タスクに特化し、かつ日本の文化に根ざしたデータが圧倒的に不足しているのです。新しいサンプルの収集、クリーニング、ラベル付けには時間と費用がかかり、目まぐるしいAIの開発サイクルに追いつくことは困難です。&lt;/p&gt;
&lt;p&gt;その結果、イノベーションが始まる前にそれを阻むデータの壁が生まれます。&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		新たな前進への道
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;img alt="image" src="https://cdn-uploads.huggingface.co/production/uploads/68d2fec8856b85d927e44d32/_Fk8_hH3YBUR26fdsT-56.png" /&gt;
大手 IT 企業 NTT DATA による新たな研究は、合成データによってこの壁がいかに取り払われるかを実証しています。手元にある最小限の独自データから、プライバシーやモデルの性能を損なうことなく、実運用レベルの大規模な学習データセットを生成できるのです。&lt;/p&gt;
&lt;p&gt;NTT DATA は、NVIDIA Nemotron-Personas-Japan (NeMo Data Designer を使用して生成された、日本の人口動態、地理、文化に基づいた 600 万のペルソナから構成されるNVIDIA の初のオープン合成データセット) を使用することで、法務Q&amp;amp;Aタスクにおいてモデルの精度を 15.3% から 79.3% へと飛躍的に向上させ、回答の一貫性においても同様の大幅な改善を達成しました。&lt;/p&gt;
&lt;p&gt;これは、機密データを学習パイプラインに公開することなく、60 ポイントもの向上を実現したことになります。&lt;/p&gt;
&lt;p&gt;実験の全体的な手法や評価フレームワークに関心のある読者に向けて、NTT DATA の詳細な技術レポート（日本語）では、本研究の設計や結果についてさらに深く掘り下げて解説しています。&lt;/p&gt;
&lt;p&gt;ここから得られる重要なポイントは、企業は完全にオープンソースのインフラストラクチャを使用し、手元にある最小限の独自データからでも、特定のドメイン（業務領域）に特化したAIを構築できるということです。オープンなペルソナデータを活用することで、より高品質なモデルの構築と、より機敏なデータ運用の両立が可能になります。&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		実証実験
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;このアプローチを厳密に検証するため、NTT DATA は架空の法律文書を用いた対照評価を実施し、モデルが真に新しい知識を獲得できるようにしました。学習には以下の構成を活用しました：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ベースモデル:&lt;/strong&gt; &lt;code&gt;tsuzumi-v2.0-28B-instruct&lt;/code&gt; (NTT の独自 LLM)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;データ拡張モデル:&lt;/strong&gt; &lt;code&gt;GPT-OSS-120b&lt;/code&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;シードデータ:&lt;/strong&gt; Nemotron-Personas-Japan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;判定モデル:&lt;/strong&gt; &lt;code&gt;GPT-5&lt;/code&gt; (LLM-as-a-judge メソッド)&lt;/p&gt;
&lt;p&gt;Nemotron-Personas-Japan から抽出した500のペルソナを活用し、わずか 450 件の未加工のシードサンプルを拡張することで、13 万 8000 件以上の学習用データ (人手による同等のサンプルの 300 倍に相当する合成データセット) を生成し、モデルの精度を 15.3% から 79.3% に向上させました。&lt;/p&gt;
&lt;p&gt;この結果は、企業が直面するデータ不足という課題を如実に物語っています。&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;構成&lt;/th&gt;
&lt;th align="right"&gt;シードデータ&lt;/th&gt;
&lt;th align="right"&gt;合成拡張&lt;/th&gt;
&lt;th align="right"&gt;精度&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;ベースライン (トレーニングなし)&lt;/td&gt;
&lt;td align="right"&gt;—&lt;/td&gt;
&lt;td align="right"&gt;—&lt;/td&gt;
&lt;td align="right"&gt;15.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;合成データを使用したSFT&lt;/td&gt;
&lt;td align="right"&gt;450件&lt;/td&gt;
&lt;td align="right"&gt;138,000 件&lt;/td&gt;
&lt;td align="right"&gt;79.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;合成データによる学習は、単なる精度の向上にとどまらず、ベースラインモデルを悩ませていたハルシネーションも排除しました。学習前のモデルがもっともらしいものの誤った法的分類を生成したのに対し、ファインチューニングされたモデルはノイズを加えることなく正確な用語を抽出できるようになりました。&lt;/p&gt;
&lt;p&gt;エンタープライズ環境への展開においておそらく最も価値のある発見は、十分な量のファインチューニング用合成データが確保できれば、「継続事前学習（CPT）」は必須ではなくなると NTT DATA が見出したことです。これはつまり、開発者は計算リソースを大量に消費する CPT の工程を完全に省略し、教師ありファインチューニング (SFT) のためのより反復的な合成データ生成に注力するという、より費用対効果の高い学習パイプラインを活用できることを意味しています。&lt;/p&gt;
&lt;p&gt;この効率性の向上は、コンピューティングコストの削減と開発サイクルの高速化に直接つながります。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NTT DATA 技術革新統括本部 AI 技術部 部長の樋口晋也氏は次のように話しています。「Nemotron Personas を用いて少量の独自データセットを拡張することで、利用可能なデータが限られている場合でも、タスクに特化したモデルを効果的に構築できます。このアプローチは、独自データが不足しがちな事前調査、カスタマーサポート、マーケティングなどの領域において、成果を向上させる大きな可能性を示しています」&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="Screenshot 2026-02-19 at 7.21.41 AM" src="https://cdn-uploads.huggingface.co/production/uploads/68d2fec8856b85d927e44d32/TZE6YYFdp-f0MgIhFA0cI.png" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		設計段階からのプライバシー保護
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;ここでの精度向上は魅力的ですが、同時により深い疑問も生じます。そもそも学習パイプラインにすら入らない（使えない）データはどうなるのでしょうか？&lt;/p&gt;
&lt;p&gt;価値ある企業データの 90% 以上が、プライバシー規制、セキュリティリスク、ライセンス制約のために未活用のままです。日本では、個人情報保護法 (PIPA) やイノベーション重視の AI ガバナンスガイドライン (2025 年 9 月公表) などの枠組みがこの現実を裏付けています。AI の進歩が加速する中でも、責任あるデータ取り扱いは必須です。&lt;/p&gt;
&lt;p&gt;合成データは、この相反する課題を解決する道筋を提供します。個人を特定できる情報 (PII) を含まず、実際のデータの傾向（パターン）を正確に反映した学習用データを生成することで、企業はデータの最小化とモデルの性能向上を同時に実現できます。初期の立ち上げには最小限の独自データのみを使用し、その後は合成データによって実運用レベルの規模まで拡張すればよいのです。&lt;/p&gt;
&lt;p&gt;つまり、合成データは単なる「学習プロセスを最適化する手法」ではありません。データコンプライアンスと AI の性能が共存する理想的なバランス(ゴルディロックスゾーン) を実現するプライバシー強化技術 (PET) なのです。さらに、データの合成パイプラインは再現性と監査性を備えているため、ガバナンスチームや規制当局がますます求める信頼性と透明性の要件にも対応できます。&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		ソブリンデータ空間
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;ソブリン AI を構築する日本企業にとって、データ主権は必須条件です。しかし、主権だけでは十分ではありません。モデルには、欧米中心のコーパスに統計的に偏ったものではなく、地域固有の規範やドメインの制約によって形成される、根拠のあるインテリジェンスも必要です。Nemotron-Personas-Japan は、この現実に根ざした AI を作るための基盤データとして機能します。600 万のペルソナは日本の公式人口動態および労働統計に基づいており、1,500 以上の職業分類と地域分布をカバーしています。&lt;/p&gt;
&lt;p&gt;しかし、その影響は個々の組織にとどまりません。NTT DATA をはじめとするリーダー企業は、「データスペース」の開発に積極的に取り組んでいます。これは、政府と企業が共通のガバナンスとプライバシー保証の下で、AI学習用に合成されたデータを交換し合える協調的な環境です。連合学習（フェデレーテッド ラーニング）などのエンドツーエンドの暗号化技術は、この分散型アプローチを可能にします。合成データはこれをさらに強力に推進する役割を果たし、組織は元となる機密情報を公開することなく、自社データの傾向（パターン）を合成データとして安全に提供できるようになります。&lt;/p&gt;
&lt;p&gt;これにより、データリスク管理は守りの姿勢から、日本の掲げる『イノベーション主導のAIガバナンス』というビジョンに沿った「協調的な姿勢」へとシフトします。また、このアプローチは、「AIの進化は、グローバルで学習された少数の巨大モデルからもたらされるべきだ」という固定観念にも一石を投じます。むしろ、オープンでプライバシー保護された基盤の上に、主権を持ち、相互運用可能な AI システムがそれぞれの地域で構築される未来を指し示しています。&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		構築を開始
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;「データの壁」は確かに存在します。しかし、NTT DATA の調査が示すように、それを克服するためのツールは今やオープンで誰でもアクセスできるようになっています。合成データは、もはや「未来の技術」ではありません 。プライバシーや性能を犠牲にすることなく、データ主権を持ち、日本の文化に根ざしたAIシステムを構築するために、開発者が「今すぐ」現場に導入できる現実のソリューションなのです。&lt;/p&gt;
&lt;p&gt;さっそく始めてみませんか？オープンソースのNeMo Data Designer ライブラリを活用するか、Hugging Face で公開されている Nemotron-Personas-Japan データセットをご覧ください。より詳細な技術的情報については、手法と実験設計を網羅したNTT データによる詳細なレポート (日本語) をご覧ください。&lt;/p&gt;

&lt;p&gt;Nemotron-Personas-Japan は、CC BY 4.0 ライセンスに基づき、商用・非商用を問わずご利用いただけます。&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/nvidia/nemotron-personas-japan-nttdata-ja</guid><pubDate>Thu, 19 Feb 2026 15:32:38 +0000</pubDate></item><item><title>OpenAI reportedly finalizing $100B deal at more than $850B valuation (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/19/openai-reportedly-finalizing-100b-deal-at-more-than-850b-valuation/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/GettyImages-2236544149.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI is nearing a deal to raise more than $100 billion at a valuation that could exceed $850 billion, Bloomberg reports, citing sources familiar with the matter.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The deal comes as the ChatGPT-maker burns through cash as it inches toward profitability. To that end, OpenAI has said it has started testing ads in ChatGPT for free users, a gamble that could lead to more revenue or could send users running from the platform.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Apparently investors think it’s worth the risk if they’re valuing the company $20 billion higher than the $830 billion valuation initially expected. The company’s pre-money value will remain at $730 billion, per Bloomberg’s source.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first tranches of funding are reportedly coming from the usual suspects: Amazon (already in talks to invest up to $50 billion), SoftBank (gearing up for $30 billion), Nvidia (close to investing $20 billion), and Microsoft. VC firms and sovereign wealth funds are expected to close later, potentially bringing the total amount raised higher.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to OpenAI for comment. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/GettyImages-2236544149.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI is nearing a deal to raise more than $100 billion at a valuation that could exceed $850 billion, Bloomberg reports, citing sources familiar with the matter.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The deal comes as the ChatGPT-maker burns through cash as it inches toward profitability. To that end, OpenAI has said it has started testing ads in ChatGPT for free users, a gamble that could lead to more revenue or could send users running from the platform.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Apparently investors think it’s worth the risk if they’re valuing the company $20 billion higher than the $830 billion valuation initially expected. The company’s pre-money value will remain at $730 billion, per Bloomberg’s source.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first tranches of funding are reportedly coming from the usual suspects: Amazon (already in talks to invest up to $50 billion), SoftBank (gearing up for $30 billion), Nvidia (close to investing $20 billion), and Microsoft. VC firms and sovereign wealth funds are expected to close later, potentially bringing the total amount raised higher.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to OpenAI for comment. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/19/openai-reportedly-finalizing-100b-deal-at-more-than-850b-valuation/</guid><pubDate>Thu, 19 Feb 2026 15:35:58 +0000</pubDate></item><item><title>Microsoft has a new plan to prove what’s real and what’s AI online (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/19/1133360/microsoft-has-a-new-plan-to-prove-whats-real-and-whats-ai-online/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/false-mirror-cr-smoke2.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;AI-enabled deception now permeates our online lives. There are the high-profile cases you may easily spot, like when White House officials recently shared a manipulated image of a protester in Minnesota and then mocked those asking about it. Other times, it slips quietly into social media feeds and racks up views, like the videos that Russian influence campaigns are currently spreading to discourage Ukrainians from enlisting.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It is into this mess that Microsoft has put forward a blueprint, shared with &lt;em&gt;MIT Technology Review&lt;/em&gt;, for how to prove what’s real online.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;An AI safety research team at the company recently evaluated how methods for documenting digital manipulation are faring against today’s most worrying AI developments, like interactive deepfakes and widely accessible hyperrealistic models. It then recommended technical standards that can be adopted by AI companies and social media platforms.&lt;/p&gt;  &lt;p&gt;To understand the gold standard that Microsoft is pushing, imagine you have a Rembrandt painting and you are trying to document its authenticity. You might describe its &lt;em&gt;provenance&lt;/em&gt; with a detailed manifest of where the painting came from and all the times it changed hands. You might apply a &lt;em&gt;watermark&lt;/em&gt; that would be invisible to humans but readable by a machine. And you could digitally scan the painting and generate a mathematical signature, like a &lt;em&gt;fingerprint&lt;/em&gt;, based on the brush strokes. If you showed the piece at a museum, a skeptical visitor could then examine these proofs to verify that it’s an original.&lt;/p&gt; 
 &lt;p&gt;All of these methods are already being used to varying degrees in the effort to vet content online. Microsoft evaluated 60 different combinations of them, modeling how each setup would hold up under different failure scenarios—from metadata being stripped to content being slightly altered or deliberately manipulated. The team then mapped which combinations produce sound results that platforms can confidently show to people online, and which ones are so unreliable that they may cause more confusion than clarification.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__container--08c53dd3bc9bd04e1e42e5f7ca641ab2"&gt;&lt;div class="whyItMatters__header--19f7f372f181cc6d4c06bc7362a44382"&gt;&lt;div class="whyItMatters__title--4af28c786a2bc93df05db111c6c30618"&gt;&lt;span class="whyItMatters__askAi--577f5fe6f54de43e37258d0f2aff4394"&gt;Ask AI&lt;/span&gt;&lt;div&gt;&lt;span class="whyItMatters__whyItMattersTitle--a3694998bb578e159bbd16690b8da390"&gt;Why it matters to you?&lt;/span&gt;&lt;span class="whyItMatters__betaBadge--9e84228b864d33d5b55479433fc91b8a"&gt;BETA&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__description--e1334886c092fa469388d7a24e1e1a55"&gt;&lt;span class="initial-description"&gt;Here’s why this story might matter to you, according to AI. This is a beta feature and AI hallucinates—it might get weird&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="whyItMatters__questionContainer--ec1159210954852b9178c549600959a0"&gt;&lt;div&gt;&lt;button class="whyItMatters__actionButton--674934b6df433ac81e613372979cdb6c" type="button"&gt;Tell me why it matters&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;The company’s chief scientific officer, Eric Horvitz, says the work was prompted by&lt;strong&gt; &lt;/strong&gt;legislation—like California’s AI Transparency Act, which will take effect in August—and the speed at which AI has developed to combine video and voice with striking fidelity.&lt;/p&gt; 
 &lt;p&gt;“You might call this self-regulation,” Horvitz told &lt;em&gt;MIT Technology Review&lt;/em&gt;. But it’s clear he sees pursuing the work as boosting Microsoft’s image: “We’re also trying to be a selected, desired provider to people who want to know what’s going on in the world.”&lt;/p&gt;  &lt;p&gt;Nevertheless, Horvitz declined to commit to Microsoft using its own recommendation across its platforms. The company sits at the center of a giant AI content ecosystem: It runs Copilot, which can generate images and text; it operates Azure, the cloud service through which customers can access OpenAI and other major AI models; it owns LinkedIn, one of the world’s largest professional platforms; and it holds a significant stake in OpenAI. But when asked about in-house implementation, Horvitz said in a statement, “Product groups and leaders across the company were involved in this study to inform product road maps and infrastructure, and our engineering teams are taking action on the report’s findings.”&lt;/p&gt;  &lt;p&gt;It’s important to note that there are inherent limits to these tools; just as they would not tell you what your Rembrandt &lt;em&gt;means&lt;/em&gt;, they are not built to determine if content is accurate or not. They only reveal if it has been manipulated. It’s a point that Horvitz says he has to make to lawmakers and others who are skeptical of Big Tech as an arbiter of fact.&lt;/p&gt;  &lt;p&gt;“It’s not about making any decisions about what’s true and not true,” he said. “It’s about coming up with labels that just tell folks where stuff came from.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Hany Farid, a professor at UC Berkeley who specializes in digital forensics but wasn’t involved in the Microsoft research, says that if the industry adopted the company’s blueprint, it would be meaningfully more difficult to deceive the public with manipulated content. Sophisticated individuals or governments can work to bypass such tools, he says, but the new standard could eliminate a significant portion of misleading material.&lt;/p&gt;  &lt;p&gt;“I don’t think it solves the problem, but I think it takes a nice big chunk out of it,” he says.&lt;/p&gt;  &lt;p&gt;Still, there are reasons to see Microsoft’s approach as an example of somewhat naïve techno-optimism. There is growing evidence that people are swayed by AI-generated content even when they know that it is false. And in a recent study of pro-Russian AI-generated videos about the war in Ukraine, comments pointing out that the videos were made with AI received far less engagement than comments treating them as genuine.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“Are there people who, no matter what you tell them, are going to believe what they believe?” Farid asks. “Yes.” But, he adds, “there are a vast majority of Americans and citizens around the world who I do think want to know the truth.”&lt;/p&gt; 

 &lt;p&gt;That desire has not exactly led to urgent action from tech companies. Google started adding a watermark to content generated by its AI tools in 2023, which Farid says has been helpful in his investigations. Some platforms use C2PA, a provenance standard Microsoft helped launch in 2021. But the full suite of changes that Microsoft suggests, powerful as they are, might remain only suggestions if they threaten the business models of AI companies or social media platforms.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;“If the Mark Zuckerbergs and the Elon Musks of the world think that putting ‘AI generated’ labels on something will reduce engagement, then of course they’re incentivized not to do it,” Farid says. Platforms like Meta and Google have already said they’d include labels for AI-generated content, but an audit conducted by &lt;em&gt;Indicator &lt;/em&gt;last year found that only 30% of its test posts on Instagram, LinkedIn, Pinterest, TikTok, and YouTube were correctly labeled as AI-generated.&lt;/p&gt;  &lt;p&gt;More forceful moves toward content verification might come from the many pieces of AI regulation pending around the world. The European Union’s AI Act, as well as proposed rules in India and elsewhere, would all compel AI companies to require some form of disclosure that a piece of content was generated with AI.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;One priority from Microsoft is, unsurprisingly, to play a role in shaping these rules. The company waged a lobbying effort during the drafting of California’s AI Transparency Act, which Horvitz said made the legislation’s requirements on how tech companies must disclose AI-generated content “a bit more realistic.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;But another is a very real concern about what could happen if the rollout of such content-verification technology is done poorly. Lawmakers are demanding tools that can verify what’s real, but the tools are fragile. If labeling systems are rushed out, inconsistently applied, or frequently wrong, people could come to distrust them altogether, and the entire effort would backfire. That’s why the researchers argue that it may be better in some cases to show nothing at all than a verdict that could be wrong.&lt;/p&gt;  &lt;p&gt;Inadequate tools could also create new avenues for what the researchers call sociotechnical attacks. Imagine that someone takes a real image of a fraught political event and uses an AI tool to change only an inconsequential share of pixels in the image. When it spreads online, it could be misleadingly classified by platforms as AI-manipulated. But combining provenance and watermark tools would mean platforms could clarify that the content was only partially AI generated, and point out where the changes were made.&lt;/p&gt;  &lt;p&gt;California’s AI Transparency Act will be the first major test of these tools in the US, but enforcement could be challenged by President Trump’s executive order from late last year seeking to curtail state AI regulations that are “burdensome” to the industry. The administration has also generally taken a posture against efforts to curb disinformation, and last year, via DOGE, it canceled grants related to misinformation. And, of course, official government channels in the Trump administration have shared content manipulated with AI (&lt;em&gt;MIT Technology Review&lt;/em&gt; reported that the Department of Homeland Security, for example, uses video generators from Google and Adobe to make content it shares with the public).&lt;/p&gt;  &lt;p&gt;I asked Horvitz whether fake content from this source worries him as much as that coming from the rest of social media. He initially declined to comment, but then he said, “Governments have not been outside the sectors that have been behind various kinds of manipulative disinformation, and this is worldwide.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/false-mirror-cr-smoke2.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;AI-enabled deception now permeates our online lives. There are the high-profile cases you may easily spot, like when White House officials recently shared a manipulated image of a protester in Minnesota and then mocked those asking about it. Other times, it slips quietly into social media feeds and racks up views, like the videos that Russian influence campaigns are currently spreading to discourage Ukrainians from enlisting.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It is into this mess that Microsoft has put forward a blueprint, shared with &lt;em&gt;MIT Technology Review&lt;/em&gt;, for how to prove what’s real online.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;An AI safety research team at the company recently evaluated how methods for documenting digital manipulation are faring against today’s most worrying AI developments, like interactive deepfakes and widely accessible hyperrealistic models. It then recommended technical standards that can be adopted by AI companies and social media platforms.&lt;/p&gt;  &lt;p&gt;To understand the gold standard that Microsoft is pushing, imagine you have a Rembrandt painting and you are trying to document its authenticity. You might describe its &lt;em&gt;provenance&lt;/em&gt; with a detailed manifest of where the painting came from and all the times it changed hands. You might apply a &lt;em&gt;watermark&lt;/em&gt; that would be invisible to humans but readable by a machine. And you could digitally scan the painting and generate a mathematical signature, like a &lt;em&gt;fingerprint&lt;/em&gt;, based on the brush strokes. If you showed the piece at a museum, a skeptical visitor could then examine these proofs to verify that it’s an original.&lt;/p&gt; 
 &lt;p&gt;All of these methods are already being used to varying degrees in the effort to vet content online. Microsoft evaluated 60 different combinations of them, modeling how each setup would hold up under different failure scenarios—from metadata being stripped to content being slightly altered or deliberately manipulated. The team then mapped which combinations produce sound results that platforms can confidently show to people online, and which ones are so unreliable that they may cause more confusion than clarification.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__container--08c53dd3bc9bd04e1e42e5f7ca641ab2"&gt;&lt;div class="whyItMatters__header--19f7f372f181cc6d4c06bc7362a44382"&gt;&lt;div class="whyItMatters__title--4af28c786a2bc93df05db111c6c30618"&gt;&lt;span class="whyItMatters__askAi--577f5fe6f54de43e37258d0f2aff4394"&gt;Ask AI&lt;/span&gt;&lt;div&gt;&lt;span class="whyItMatters__whyItMattersTitle--a3694998bb578e159bbd16690b8da390"&gt;Why it matters to you?&lt;/span&gt;&lt;span class="whyItMatters__betaBadge--9e84228b864d33d5b55479433fc91b8a"&gt;BETA&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__description--e1334886c092fa469388d7a24e1e1a55"&gt;&lt;span class="initial-description"&gt;Here’s why this story might matter to you, according to AI. This is a beta feature and AI hallucinates—it might get weird&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="whyItMatters__questionContainer--ec1159210954852b9178c549600959a0"&gt;&lt;div&gt;&lt;button class="whyItMatters__actionButton--674934b6df433ac81e613372979cdb6c" type="button"&gt;Tell me why it matters&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;The company’s chief scientific officer, Eric Horvitz, says the work was prompted by&lt;strong&gt; &lt;/strong&gt;legislation—like California’s AI Transparency Act, which will take effect in August—and the speed at which AI has developed to combine video and voice with striking fidelity.&lt;/p&gt; 
 &lt;p&gt;“You might call this self-regulation,” Horvitz told &lt;em&gt;MIT Technology Review&lt;/em&gt;. But it’s clear he sees pursuing the work as boosting Microsoft’s image: “We’re also trying to be a selected, desired provider to people who want to know what’s going on in the world.”&lt;/p&gt;  &lt;p&gt;Nevertheless, Horvitz declined to commit to Microsoft using its own recommendation across its platforms. The company sits at the center of a giant AI content ecosystem: It runs Copilot, which can generate images and text; it operates Azure, the cloud service through which customers can access OpenAI and other major AI models; it owns LinkedIn, one of the world’s largest professional platforms; and it holds a significant stake in OpenAI. But when asked about in-house implementation, Horvitz said in a statement, “Product groups and leaders across the company were involved in this study to inform product road maps and infrastructure, and our engineering teams are taking action on the report’s findings.”&lt;/p&gt;  &lt;p&gt;It’s important to note that there are inherent limits to these tools; just as they would not tell you what your Rembrandt &lt;em&gt;means&lt;/em&gt;, they are not built to determine if content is accurate or not. They only reveal if it has been manipulated. It’s a point that Horvitz says he has to make to lawmakers and others who are skeptical of Big Tech as an arbiter of fact.&lt;/p&gt;  &lt;p&gt;“It’s not about making any decisions about what’s true and not true,” he said. “It’s about coming up with labels that just tell folks where stuff came from.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Hany Farid, a professor at UC Berkeley who specializes in digital forensics but wasn’t involved in the Microsoft research, says that if the industry adopted the company’s blueprint, it would be meaningfully more difficult to deceive the public with manipulated content. Sophisticated individuals or governments can work to bypass such tools, he says, but the new standard could eliminate a significant portion of misleading material.&lt;/p&gt;  &lt;p&gt;“I don’t think it solves the problem, but I think it takes a nice big chunk out of it,” he says.&lt;/p&gt;  &lt;p&gt;Still, there are reasons to see Microsoft’s approach as an example of somewhat naïve techno-optimism. There is growing evidence that people are swayed by AI-generated content even when they know that it is false. And in a recent study of pro-Russian AI-generated videos about the war in Ukraine, comments pointing out that the videos were made with AI received far less engagement than comments treating them as genuine.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“Are there people who, no matter what you tell them, are going to believe what they believe?” Farid asks. “Yes.” But, he adds, “there are a vast majority of Americans and citizens around the world who I do think want to know the truth.”&lt;/p&gt; 

 &lt;p&gt;That desire has not exactly led to urgent action from tech companies. Google started adding a watermark to content generated by its AI tools in 2023, which Farid says has been helpful in his investigations. Some platforms use C2PA, a provenance standard Microsoft helped launch in 2021. But the full suite of changes that Microsoft suggests, powerful as they are, might remain only suggestions if they threaten the business models of AI companies or social media platforms.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;“If the Mark Zuckerbergs and the Elon Musks of the world think that putting ‘AI generated’ labels on something will reduce engagement, then of course they’re incentivized not to do it,” Farid says. Platforms like Meta and Google have already said they’d include labels for AI-generated content, but an audit conducted by &lt;em&gt;Indicator &lt;/em&gt;last year found that only 30% of its test posts on Instagram, LinkedIn, Pinterest, TikTok, and YouTube were correctly labeled as AI-generated.&lt;/p&gt;  &lt;p&gt;More forceful moves toward content verification might come from the many pieces of AI regulation pending around the world. The European Union’s AI Act, as well as proposed rules in India and elsewhere, would all compel AI companies to require some form of disclosure that a piece of content was generated with AI.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;One priority from Microsoft is, unsurprisingly, to play a role in shaping these rules. The company waged a lobbying effort during the drafting of California’s AI Transparency Act, which Horvitz said made the legislation’s requirements on how tech companies must disclose AI-generated content “a bit more realistic.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;But another is a very real concern about what could happen if the rollout of such content-verification technology is done poorly. Lawmakers are demanding tools that can verify what’s real, but the tools are fragile. If labeling systems are rushed out, inconsistently applied, or frequently wrong, people could come to distrust them altogether, and the entire effort would backfire. That’s why the researchers argue that it may be better in some cases to show nothing at all than a verdict that could be wrong.&lt;/p&gt;  &lt;p&gt;Inadequate tools could also create new avenues for what the researchers call sociotechnical attacks. Imagine that someone takes a real image of a fraught political event and uses an AI tool to change only an inconsequential share of pixels in the image. When it spreads online, it could be misleadingly classified by platforms as AI-manipulated. But combining provenance and watermark tools would mean platforms could clarify that the content was only partially AI generated, and point out where the changes were made.&lt;/p&gt;  &lt;p&gt;California’s AI Transparency Act will be the first major test of these tools in the US, but enforcement could be challenged by President Trump’s executive order from late last year seeking to curtail state AI regulations that are “burdensome” to the industry. The administration has also generally taken a posture against efforts to curb disinformation, and last year, via DOGE, it canceled grants related to misinformation. And, of course, official government channels in the Trump administration have shared content manipulated with AI (&lt;em&gt;MIT Technology Review&lt;/em&gt; reported that the Department of Homeland Security, for example, uses video generators from Google and Adobe to make content it shares with the public).&lt;/p&gt;  &lt;p&gt;I asked Horvitz whether fake content from this source worries him as much as that coming from the rest of social media. He initially declined to comment, but then he said, “Governments have not been outside the sectors that have been behind various kinds of manipulative disinformation, and this is worldwide.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/19/1133360/microsoft-has-a-new-plan-to-prove-whats-real-and-whats-ai-online/</guid><pubDate>Thu, 19 Feb 2026 16:00:00 +0000</pubDate></item><item><title>Media Authenticity Methods in Practice: Capabilities, Limitations, and Directions (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/media-authenticity-methods-in-practice-capabilities-limitations-and-directions/</link><description>&lt;p&gt;&lt;em&gt;&lt;em&gt;Insights from Microsoft’s Media Integrity and Authentication: Status, Directions, and Futures report&lt;/em&gt;&lt;/em&gt;&lt;/p&gt;



&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="three white outline icons on a blue-to-pink gradient background: an image with a copyright “CR” badge, an image overlaid with fingerprint-like lines, and an image framed by a cropping grid." class="wp-image-1162625" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/MediaIntegrityReport-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;It has become increasingly difficult to distinguish fact from fiction when viewing online images and videos. Resilient, trustworthy technologies can help people determine whether the content they are viewing was captured by a camera or microphone—or generated or modified by AI tools.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;We refer to technologies aimed at helping viewers verify the source and history—that is, the provenance—of digital content as &lt;em&gt;media integrity and authentication&lt;/em&gt; (MIA) methods. This technique, driven by the Coalition for Content Provenance and Authenticity&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; (C2PA), a standards body dedicated to scaling these capabilities, as well as complementary methods such as watermarks and fingerprinting, have become critically important with the rapid advance of AI systems capable of creating, realistic imagery, video, and audio at scale.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="a-convergence-of-forces"&gt;A convergence of forces&lt;/h2&gt;



&lt;p&gt;Our team recognized an inflection point in the evolution of online content integrity, driven by the convergence of four forces:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Growing saturation of synthetic media&lt;/strong&gt;, driven by proliferation of high-fidelity content-generation tools and the explosion of AI generated or modified media online&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Forthcoming legislation&lt;/strong&gt; both nationally and internationally seeking to define what “verifiable” provenance should mean in practice&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Mounting pressure on implementers&lt;/strong&gt; to ensure authentication signals are clear and helpful, especially as signals increase when legislation goes into effect in 2026&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Heightened awareness of&amp;nbsp;potential&amp;nbsp;adversarial attacks&lt;/strong&gt; that attempt to exploit weaknesses in authenticity systems&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;The usefulness and trustworthiness of provenance signals, whether certifying content as synthetic or as an authentic capture of real-world scenes, will depend not only on advances in technology, but also on how the broader digital ecosystem adopts, implements, and governs these tools. Aligning around implementation choices that promote consistency and clarity is essential to ensure transparency signals strengthen, rather than erode, public confidence.&lt;/p&gt;



&lt;p&gt;To address these challenges, we launched a comprehensive evaluation of the real-world limits, edge cases, and emerging “attack surfaces” for MIA methods. Today, we are publishing our findings in the &lt;em&gt;Media Integrity &amp;amp; Authentication: Status, Directions &amp;amp; Futures &lt;/em&gt;report. The report distills lessons learned and outlines practical directions for strengthening media integrity in the years ahead.&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;Spotlight: AI-POWERED EXPERIENCE&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Microsoft research copilot experience&lt;/h2&gt;
				
								&lt;p class="large" id="microsoft-research-copilot-experience"&gt;Discover more about research at Microsoft through our AI-powered experience&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="findings-and-directions-forward"&gt;Findings and directions forward&lt;/h2&gt;



&lt;p&gt;Our research recognizes that different media integrity and authenticity methods serve differing purposes and offer distinct levels of protection. After defining each method in detail, we focused on secure provenance (C2PA), imperceptible watermarking, and soft hash fingerprinting across images, audio, and video.&lt;/p&gt;



&lt;p&gt;Grounded in our evaluation of these MIA methods across modalities, attack categories, and real-world workflows, several new findings emerged including two new concepts:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;High-Confidence Provenance Authentication&lt;/strong&gt;: a&amp;nbsp;critical capability for verifying, under defined conditions, whether claims about the origin of and modifications made to an asset can be&amp;nbsp;validated&amp;nbsp;with high certainty.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Sociotechnical Provenance Attacks&lt;/strong&gt;: attacks aimed at deception and capable of inverting signals, making authentic content appear synthetic, and synthetic content appear authentic.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Drawing on our findings, we identified four promising directions for further strengthening media authentication, along with suggestions to support more effective implementation strategies and future decisions. We’ve summarized the findings and directions below, with additional detail available in the report.&lt;/p&gt;



&lt;figure class="wp-block-table"&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Promising directions&lt;/th&gt;&lt;th&gt;High-level findings&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Delivering high-confidence provenance authentication&lt;/td&gt;&lt;td&gt;– &lt;strong&gt;Implementation and display choices may affect the reliability of provenance indicators&lt;/strong&gt; and how they are interpreted by the public. &lt;p&gt;– Using a C2PA provenance manifest for media created and signed in a high security environment &lt;strong&gt;enables high-confidence validation&lt;/strong&gt;. &lt;/p&gt;&lt;p&gt;– High-confidence validation is also possible across a broader volume of images, audio, and video when &lt;strong&gt;an imperceptible watermark is linked to C2PA provenance manifest as an additional layer to recover the provenance information if removed&lt;/strong&gt;. &lt;/p&gt;&lt;p&gt;– Fingerprinting is &lt;strong&gt;not an enabler for high-confidence validation&lt;/strong&gt; and can involve significant costs when expected at scale. However, it can support manual forensics.&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Mitigating confusion from sociotechnical provenance attacks&lt;/td&gt;&lt;td&gt;– &lt;strong&gt;MIA methods are susceptible to sociotechnical attacks on provenance that may mislead the public&lt;/strong&gt;, resulting in confusion and misplaced trust about an asset’s provenance if there is an overreliance on low-quality signals.&lt;p&gt;– Layering and linking secure provenance and imperceptible watermarking methods to achieve high confidence validation also offers a promising option to &lt;strong&gt;both deter and mitigate the impact of attacks&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;– Unintended consequences may result from the use of methods lacking authentication, such as the use of perceptible watermarks in the absence of secure provenance. &lt;strong&gt;Perceptible watermarks may cause confusion&lt;/strong&gt; in cases of forgery or discourage people from consulting high-confidence provenance information via a validation tool, if such perceptible disclosures are taken at face value.&lt;br /&gt; &lt;br /&gt;– &lt;strong&gt;UX design that enables users to explore manifest details&lt;/strong&gt;—such as where edits occurred&amp;nbsp;or region of interest—has the potential to&amp;nbsp;reduce confusion and&amp;nbsp;support forensics and fact checking efforts.&amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Enabling more trusted provenance on edge devices&lt;/td&gt;&lt;td&gt;– High-confidence results &lt;strong&gt;aren’t feasible when provenance is added by a conventional offline device&lt;/strong&gt; (e.g., camera or recording device without connectivity).&lt;p&gt;– &lt;strong&gt;Implementing a secure enclave&lt;/strong&gt; within the hardware layer of offline devices is essential to make the provenance of captured images, audio, and video more trustworthy.&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Investing in ongoing research and policy development&lt;/td&gt;&lt;td&gt;– All three methods offer organizations&amp;nbsp;&lt;strong&gt;valuable tools for addressing operational challenges&lt;/strong&gt;&amp;nbsp;such as fraud prevention, risk management, and digital accountability.&amp;nbsp;&lt;p&gt;– &lt;strong&gt;UX and display&amp;nbsp;&lt;/strong&gt;are promising directions for research. Important directions include in-stream tools that display provenance information where people are&amp;nbsp;and&amp;nbsp;distinguish&amp;nbsp;between&amp;nbsp;high- and lower-confidence&amp;nbsp;provenance signals.&lt;/p&gt;&lt;p&gt;– &lt;strong&gt;Stakeholders&amp;nbsp;should&amp;nbsp;conduct ongoing analysis and red teaming&lt;/strong&gt;&amp;nbsp;to&amp;nbsp;identify&amp;nbsp;and mitigate weaknesses&amp;nbsp;through&amp;nbsp;technical approaches,&amp;nbsp;policies, and&amp;nbsp;laws.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="the-journey-continues"&gt;The journey continues&lt;/h2&gt;



&lt;p&gt;This report marks the beginning of a new chapter in our media provenance journey&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, building on years of foundational work, from developing the very first prototype in 2019 to co-founding the C2PA in 2021 and helping catalyze an ecosystem that has since grown to more than 6,000 members and affiliates&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; supporting C2PA Content Credentials. This research represents the next evolution of that long‑standing commitment.&lt;/p&gt;



&lt;p&gt;We hope that by sharing our learnings will help others prepare for an important wave, especially as generative technologies accelerate and provenance signals multiply. This work is already underway across our products at Microsoft. Together, these directions highlight opportunities for the ecosystem to align, harden, and innovate, so authentication signals are not merely visible, but robust, meaningful, and resilient throughout the content lifecycle.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;p&gt;&lt;em&gt;&lt;em&gt;Insights from Microsoft’s Media Integrity and Authentication: Status, Directions, and Futures report&lt;/em&gt;&lt;/em&gt;&lt;/p&gt;



&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="three white outline icons on a blue-to-pink gradient background: an image with a copyright “CR” badge, an image overlaid with fingerprint-like lines, and an image framed by a cropping grid." class="wp-image-1162625" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/MediaIntegrityReport-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;It has become increasingly difficult to distinguish fact from fiction when viewing online images and videos. Resilient, trustworthy technologies can help people determine whether the content they are viewing was captured by a camera or microphone—or generated or modified by AI tools.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;We refer to technologies aimed at helping viewers verify the source and history—that is, the provenance—of digital content as &lt;em&gt;media integrity and authentication&lt;/em&gt; (MIA) methods. This technique, driven by the Coalition for Content Provenance and Authenticity&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; (C2PA), a standards body dedicated to scaling these capabilities, as well as complementary methods such as watermarks and fingerprinting, have become critically important with the rapid advance of AI systems capable of creating, realistic imagery, video, and audio at scale.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="a-convergence-of-forces"&gt;A convergence of forces&lt;/h2&gt;



&lt;p&gt;Our team recognized an inflection point in the evolution of online content integrity, driven by the convergence of four forces:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Growing saturation of synthetic media&lt;/strong&gt;, driven by proliferation of high-fidelity content-generation tools and the explosion of AI generated or modified media online&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Forthcoming legislation&lt;/strong&gt; both nationally and internationally seeking to define what “verifiable” provenance should mean in practice&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Mounting pressure on implementers&lt;/strong&gt; to ensure authentication signals are clear and helpful, especially as signals increase when legislation goes into effect in 2026&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Heightened awareness of&amp;nbsp;potential&amp;nbsp;adversarial attacks&lt;/strong&gt; that attempt to exploit weaknesses in authenticity systems&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;The usefulness and trustworthiness of provenance signals, whether certifying content as synthetic or as an authentic capture of real-world scenes, will depend not only on advances in technology, but also on how the broader digital ecosystem adopts, implements, and governs these tools. Aligning around implementation choices that promote consistency and clarity is essential to ensure transparency signals strengthen, rather than erode, public confidence.&lt;/p&gt;



&lt;p&gt;To address these challenges, we launched a comprehensive evaluation of the real-world limits, edge cases, and emerging “attack surfaces” for MIA methods. Today, we are publishing our findings in the &lt;em&gt;Media Integrity &amp;amp; Authentication: Status, Directions &amp;amp; Futures &lt;/em&gt;report. The report distills lessons learned and outlines practical directions for strengthening media integrity in the years ahead.&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;Spotlight: AI-POWERED EXPERIENCE&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Microsoft research copilot experience&lt;/h2&gt;
				
								&lt;p class="large" id="microsoft-research-copilot-experience"&gt;Discover more about research at Microsoft through our AI-powered experience&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="findings-and-directions-forward"&gt;Findings and directions forward&lt;/h2&gt;



&lt;p&gt;Our research recognizes that different media integrity and authenticity methods serve differing purposes and offer distinct levels of protection. After defining each method in detail, we focused on secure provenance (C2PA), imperceptible watermarking, and soft hash fingerprinting across images, audio, and video.&lt;/p&gt;



&lt;p&gt;Grounded in our evaluation of these MIA methods across modalities, attack categories, and real-world workflows, several new findings emerged including two new concepts:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;High-Confidence Provenance Authentication&lt;/strong&gt;: a&amp;nbsp;critical capability for verifying, under defined conditions, whether claims about the origin of and modifications made to an asset can be&amp;nbsp;validated&amp;nbsp;with high certainty.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Sociotechnical Provenance Attacks&lt;/strong&gt;: attacks aimed at deception and capable of inverting signals, making authentic content appear synthetic, and synthetic content appear authentic.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Drawing on our findings, we identified four promising directions for further strengthening media authentication, along with suggestions to support more effective implementation strategies and future decisions. We’ve summarized the findings and directions below, with additional detail available in the report.&lt;/p&gt;



&lt;figure class="wp-block-table"&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Promising directions&lt;/th&gt;&lt;th&gt;High-level findings&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Delivering high-confidence provenance authentication&lt;/td&gt;&lt;td&gt;– &lt;strong&gt;Implementation and display choices may affect the reliability of provenance indicators&lt;/strong&gt; and how they are interpreted by the public. &lt;p&gt;– Using a C2PA provenance manifest for media created and signed in a high security environment &lt;strong&gt;enables high-confidence validation&lt;/strong&gt;. &lt;/p&gt;&lt;p&gt;– High-confidence validation is also possible across a broader volume of images, audio, and video when &lt;strong&gt;an imperceptible watermark is linked to C2PA provenance manifest as an additional layer to recover the provenance information if removed&lt;/strong&gt;. &lt;/p&gt;&lt;p&gt;– Fingerprinting is &lt;strong&gt;not an enabler for high-confidence validation&lt;/strong&gt; and can involve significant costs when expected at scale. However, it can support manual forensics.&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Mitigating confusion from sociotechnical provenance attacks&lt;/td&gt;&lt;td&gt;– &lt;strong&gt;MIA methods are susceptible to sociotechnical attacks on provenance that may mislead the public&lt;/strong&gt;, resulting in confusion and misplaced trust about an asset’s provenance if there is an overreliance on low-quality signals.&lt;p&gt;– Layering and linking secure provenance and imperceptible watermarking methods to achieve high confidence validation also offers a promising option to &lt;strong&gt;both deter and mitigate the impact of attacks&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;– Unintended consequences may result from the use of methods lacking authentication, such as the use of perceptible watermarks in the absence of secure provenance. &lt;strong&gt;Perceptible watermarks may cause confusion&lt;/strong&gt; in cases of forgery or discourage people from consulting high-confidence provenance information via a validation tool, if such perceptible disclosures are taken at face value.&lt;br /&gt; &lt;br /&gt;– &lt;strong&gt;UX design that enables users to explore manifest details&lt;/strong&gt;—such as where edits occurred&amp;nbsp;or region of interest—has the potential to&amp;nbsp;reduce confusion and&amp;nbsp;support forensics and fact checking efforts.&amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Enabling more trusted provenance on edge devices&lt;/td&gt;&lt;td&gt;– High-confidence results &lt;strong&gt;aren’t feasible when provenance is added by a conventional offline device&lt;/strong&gt; (e.g., camera or recording device without connectivity).&lt;p&gt;– &lt;strong&gt;Implementing a secure enclave&lt;/strong&gt; within the hardware layer of offline devices is essential to make the provenance of captured images, audio, and video more trustworthy.&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Investing in ongoing research and policy development&lt;/td&gt;&lt;td&gt;– All three methods offer organizations&amp;nbsp;&lt;strong&gt;valuable tools for addressing operational challenges&lt;/strong&gt;&amp;nbsp;such as fraud prevention, risk management, and digital accountability.&amp;nbsp;&lt;p&gt;– &lt;strong&gt;UX and display&amp;nbsp;&lt;/strong&gt;are promising directions for research. Important directions include in-stream tools that display provenance information where people are&amp;nbsp;and&amp;nbsp;distinguish&amp;nbsp;between&amp;nbsp;high- and lower-confidence&amp;nbsp;provenance signals.&lt;/p&gt;&lt;p&gt;– &lt;strong&gt;Stakeholders&amp;nbsp;should&amp;nbsp;conduct ongoing analysis and red teaming&lt;/strong&gt;&amp;nbsp;to&amp;nbsp;identify&amp;nbsp;and mitigate weaknesses&amp;nbsp;through&amp;nbsp;technical approaches,&amp;nbsp;policies, and&amp;nbsp;laws.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="the-journey-continues"&gt;The journey continues&lt;/h2&gt;



&lt;p&gt;This report marks the beginning of a new chapter in our media provenance journey&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, building on years of foundational work, from developing the very first prototype in 2019 to co-founding the C2PA in 2021 and helping catalyze an ecosystem that has since grown to more than 6,000 members and affiliates&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; supporting C2PA Content Credentials. This research represents the next evolution of that long‑standing commitment.&lt;/p&gt;



&lt;p&gt;We hope that by sharing our learnings will help others prepare for an important wave, especially as generative technologies accelerate and provenance signals multiply. This work is already underway across our products at Microsoft. Together, these directions highlight opportunities for the ecosystem to align, harden, and innovate, so authentication signals are not merely visible, but robust, meaningful, and resilient throughout the content lifecycle.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/media-authenticity-methods-in-practice-capabilities-limitations-and-directions/</guid><pubDate>Thu, 19 Feb 2026 16:00:51 +0000</pubDate></item><item><title>Gemini 3.1 Pro: A smarter model for your most complex tasks (Google DeepMind News)</title><link>https://deepmind.google/blog/gemini-3-1-pro-a-smarter-model-for-your-most-complex-tasks/</link><description>&lt;div class="article-image-hero"&gt;
  &lt;div class="article-image-hero__container"&gt;
    &lt;figure class="article-image--full-aspect article-module"&gt;
      &lt;div class="aspect-ratio-image"&gt;
        &lt;div class="aspect-ratio-image__container"&gt;
          &lt;img alt="Gemini 3.1 Pro" class="aspect-ratio-image__image uni-progressive-image--blur" height="150px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemini-3.1_pro_keyword_header_dar.width-2200.format-webp.webp" width="360px" /&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      
    &lt;/figure&gt;
  &lt;/div&gt;
&lt;/div&gt;&lt;div class="uni-content uni-blog-article-container article-container__content
                      
                      "&gt;

            
  
    



















&lt;div class="audio-player-tts"&gt;
  &lt;audio class="audio-player-tts__player" title="Gemini 3.1 Pro: A smarter model for your most complex tasks"&gt;
      &lt;source src="https://storage.googleapis.com/gweb-uniblog-publish-prod/media/tts_audio_83397_umbriel_2026_02_19_17_33_43.wav" type="audio/x-wav" /&gt;
      &lt;p&gt;Your browser does not support the audio element.&lt;/p&gt;
  &lt;/audio&gt;
  &lt;div class="audio-player-tts__container"&gt;
    &lt;div class="audio-player-tts__content"&gt;
      &lt;button class="audio-player-tts__preview-play"&gt;
        &lt;svg class="icon audio-player-tts__play-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

      &lt;/button&gt;
      &lt;div class="audio-player-tts__text-content"&gt;
        &lt;span class="audio-player-tts__text-content--title"&gt;
          Listen to article
          &lt;span class="audio-player-tts__disclaimer" tabindex="0"&gt;
            &lt;div class="audio-player-tts__disclaimer--copy uni-small-text"&gt;This content is generated by Google AI. Generative AI is experimental&lt;/div&gt;
            &lt;svg class="audio-player-tts__disclaimer--icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

          &lt;/span&gt;
        &lt;/span&gt;
        &lt;div class="audio-player-tts__duration uni-small-text"&gt;[[duration]] minutes&lt;/div&gt;
      &lt;/div&gt;
      &lt;button class="audio-player-tts__pause"&gt;
        &lt;svg class="icon audio-player-tts__icon-play" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

        &lt;svg class="icon audio-player-tts__icon-pause audio-player-tts__icon-pause--hidden" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

      &lt;/button&gt;
      &lt;div class="audio-player-tts__console"&gt;
        &lt;div class="audio-player-tts__time-bar"&gt;
          &lt;span class="audio-player-tts__current-time uni-small-text"&gt;&lt;/span&gt;
          &lt;div class="audio-player-tts__timeline-slider-container"&gt;
            &lt;input class="timeline__slider" max="100" step="5" tabindex="0" type="range" value="0" /&gt;
          &lt;/div&gt;
          &lt;span class="audio-player-tts__duration-time uni-small-text"&gt;&lt;/span&gt;
        &lt;/div&gt;
        &lt;button class="audio-player-tts__audio-settings"&gt;
          &lt;svg class="icon audio-player-tts__audio-settings--icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

        &lt;/button&gt;
        &lt;div class="audio-player-tts__settings-container"&gt;
          &lt;div class="audio-player-tts__settings--main uni-cta-text"&gt;
            &lt;button class="audio-player-tts__settings--current-voice"&gt;
              &lt;span class="audio-player-tts__settings--current-voice-info"&gt;
                &lt;svg class="audio-player-tts__settings--current-voice-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

                &lt;span&gt;Voice&lt;/span&gt;
              &lt;/span&gt;
              &lt;span class="audio-player-tts__settings--current-voice-next"&gt;
                &lt;span class="audio-player-tts__settings--current-voice-text uni-small-text"&gt;&lt;/span&gt;
                &lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

              &lt;/span&gt;
            &lt;/button&gt;
            &lt;button class="audio-player-tts__settings--current-speed"&gt;
              &lt;span class="audio-player-tts__settings--current-speed-info"&gt;
                  &lt;svg class="audio-player-tts__settings--current-speed-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

                  &lt;span&gt;Speed&lt;/span&gt;
                &lt;/span&gt;
                &lt;span class="audio-player-tts__settings--current-speed-next"&gt;
                  &lt;span class="audio-player-tts__settings--current-speed-text uni-small-text"&gt;&lt;/span&gt;
                  &lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

                &lt;/span&gt;
            &lt;/button&gt;
          &lt;/div&gt;
          &lt;div class="audio-player-tts__settings--voices uni-cta-text"&gt;
            &lt;button class="audio-player-tts__settings-back"&gt;&lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;
 &lt;span&gt;Voice&lt;/span&gt;&lt;/button&gt;
          &lt;/div&gt;
          &lt;div class="audio-player-tts__settings--speeds uni-cta-text"&gt;
            &lt;button class="audio-player-tts__settings-back"&gt;&lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;
 &lt;span&gt;Speed&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option"&gt;&lt;span&gt;0.75X&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option audio-player-tts__settings-option--selected"&gt;&lt;span&gt;1X&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option"&gt;&lt;span&gt;1.5X&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option"&gt;&lt;span&gt;2X&lt;/span&gt;&lt;/button&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;

  





            
            
&lt;!--article text--&gt;

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Last week, we released a major update to Gemini 3 Deep Think to solve modern challenges across science, research and engineering. Today, we’re releasing the upgraded core intelligence that makes those breakthroughs possible: Gemini 3.1 Pro. We are shipping 3.1 Pro across our consumer and developer products to bring this progress in intelligence to your everyday applications.&lt;/p&gt;&lt;p&gt;Starting today, 3.1 Pro is rolling out:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;For developers&lt;/b&gt; in preview via the Gemini API in Google AI Studio, Gemini CLI, our agentic development platform Google Antigravity and Android Studio&lt;/li&gt;&lt;li&gt;&lt;b&gt;For enterprises&lt;/b&gt; in Vertex AI and Gemini Enterprise&lt;/li&gt;&lt;li&gt;&lt;b&gt;For consumers&lt;/b&gt; via the Gemini app and NotebookLM&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Building on the Gemini 3 series, 3.1 Pro represents a step forward in core reasoning. 3.1 Pro is a smarter, more capable baseline for complex problem-solving. This is reflected in our progress on rigorous benchmarks. On ARC-AGI-2, a benchmark that evaluates a model’s ability to solve entirely new logic patterns, 3.1 Pro achieved a verified score of 77.1%. This is more than double the reasoning performance of 3 Pro.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    






























  
  
    &lt;div&gt;
      &lt;img alt="Side-by-side comparison of different benchmarks for AI models." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/gemini_3-1-pro__benchmarks.gif" /&gt;
    &lt;/div&gt;
  



  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;Intelligence applied&lt;/h2&gt;&lt;p&gt;3.1 Pro is designed for tasks where a simple answer isn’t enough, taking advanced reasoning and making it useful for your hardest challenges. This improved intelligence can help in practical applications — whether you’re looking for a clear, visual explanation of a complex topic, a way to synthesize data into a single view, or bringing a creative project to life.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    


































  
    
      &lt;div&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Code-based animation: 3.1 Pro can generate website-ready, animated SVGs directly from a text prompt. Because these are built in pure code rather than pixels, they remain crisp at any scale and maintain incredibly small file sizes compared to traditional video.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    
  
    
      &lt;div&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Complex system synthesis: 3.1 Pro utilizes advanced reasoning to bridge the gap between complex APIs and user-friendly design. In this example, the model built a live aerospace dashboard, successfully configuring a public telemetry stream to visualize the International Space Station’s orbit.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    
  
    
      &lt;div&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Interactive design: 3.1 Pro codes a complex 3D starling murmuration. It doesn't just generate the visual code; it builds an immersive experience where users can manipulate the flock with hand-tracking and listen to a generative score that shifts based on the birds’ movement. For researchers and designers, this provides a powerful way to prototype sensory-rich interfaces.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    
  
    
      &lt;div&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Creative coding: 3.1 Pro can translate literary themes into functional code. When prompted to build a modern personal portfolio for Emily Brontë’s "Wuthering Heights," the model didn’t just summarize the text. It reasoned through the novel’s atmospheric tone to design a sleek, contemporary interface, creating a website that captures the essence of the protagonist.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    
  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;What’s next&lt;/h2&gt;&lt;p&gt;Since releasing Gemini 3 Pro in November, your feedback and the pace of progress have driven these rapid improvements. We are releasing 3.1 Pro in preview today to validate these updates and continue to make further advancements in areas such as ambitious agentic workflows before we make it generally available soon.&lt;/p&gt;&lt;p&gt;Starting today, Gemini 3.1 Pro in the Gemini app is rolling out with higher limits for users with the Google AI Pro and Ultra plans. 3.1 Pro is also now available on NotebookLM exclusively for Pro and Ultra users. And developers and enterprises can access 3.1 Pro now in preview in the Gemini API via AI Studio, Antigravity, Vertex AI, Gemini Enterprise, Gemini CLI and Android Studio.&lt;/p&gt;&lt;p&gt;We can’t wait to see what you build and discover with it.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  


            
            

            
              


&lt;div class="
    uni-blog-article-tags
    article-tags
    
  "&gt;
  &lt;div class="uni-blog-article-tags__wrapper"&gt;
    &lt;span class="uni-blog-article-tags__label uni-eyebrow"&gt;POSTED IN:&lt;/span&gt;
  &lt;/div&gt;
  &lt;nav class="uni-blog-article-tags__container uni-click-tracker"&gt;
    &lt;ul class="uni-blog-article-tags__tags-list"&gt;
    
      &lt;li&gt;
        
        
        


  


Gemini models


  


      &lt;/li&gt;
    

    
    &lt;/ul&gt;
  &lt;/nav&gt;
&lt;/div&gt;

            
          &lt;/div&gt;</description><content:encoded>&lt;div class="article-image-hero"&gt;
  &lt;div class="article-image-hero__container"&gt;
    &lt;figure class="article-image--full-aspect article-module"&gt;
      &lt;div class="aspect-ratio-image"&gt;
        &lt;div class="aspect-ratio-image__container"&gt;
          &lt;img alt="Gemini 3.1 Pro" class="aspect-ratio-image__image uni-progressive-image--blur" height="150px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemini-3.1_pro_keyword_header_dar.width-2200.format-webp.webp" width="360px" /&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      
    &lt;/figure&gt;
  &lt;/div&gt;
&lt;/div&gt;&lt;div class="uni-content uni-blog-article-container article-container__content
                      
                      "&gt;

            
  
    



















&lt;div class="audio-player-tts"&gt;
  &lt;audio class="audio-player-tts__player" title="Gemini 3.1 Pro: A smarter model for your most complex tasks"&gt;
      &lt;source src="https://storage.googleapis.com/gweb-uniblog-publish-prod/media/tts_audio_83397_umbriel_2026_02_19_17_33_43.wav" type="audio/x-wav" /&gt;
      &lt;p&gt;Your browser does not support the audio element.&lt;/p&gt;
  &lt;/audio&gt;
  &lt;div class="audio-player-tts__container"&gt;
    &lt;div class="audio-player-tts__content"&gt;
      &lt;button class="audio-player-tts__preview-play"&gt;
        &lt;svg class="icon audio-player-tts__play-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

      &lt;/button&gt;
      &lt;div class="audio-player-tts__text-content"&gt;
        &lt;span class="audio-player-tts__text-content--title"&gt;
          Listen to article
          &lt;span class="audio-player-tts__disclaimer" tabindex="0"&gt;
            &lt;div class="audio-player-tts__disclaimer--copy uni-small-text"&gt;This content is generated by Google AI. Generative AI is experimental&lt;/div&gt;
            &lt;svg class="audio-player-tts__disclaimer--icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

          &lt;/span&gt;
        &lt;/span&gt;
        &lt;div class="audio-player-tts__duration uni-small-text"&gt;[[duration]] minutes&lt;/div&gt;
      &lt;/div&gt;
      &lt;button class="audio-player-tts__pause"&gt;
        &lt;svg class="icon audio-player-tts__icon-play" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

        &lt;svg class="icon audio-player-tts__icon-pause audio-player-tts__icon-pause--hidden" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

      &lt;/button&gt;
      &lt;div class="audio-player-tts__console"&gt;
        &lt;div class="audio-player-tts__time-bar"&gt;
          &lt;span class="audio-player-tts__current-time uni-small-text"&gt;&lt;/span&gt;
          &lt;div class="audio-player-tts__timeline-slider-container"&gt;
            &lt;input class="timeline__slider" max="100" step="5" tabindex="0" type="range" value="0" /&gt;
          &lt;/div&gt;
          &lt;span class="audio-player-tts__duration-time uni-small-text"&gt;&lt;/span&gt;
        &lt;/div&gt;
        &lt;button class="audio-player-tts__audio-settings"&gt;
          &lt;svg class="icon audio-player-tts__audio-settings--icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

        &lt;/button&gt;
        &lt;div class="audio-player-tts__settings-container"&gt;
          &lt;div class="audio-player-tts__settings--main uni-cta-text"&gt;
            &lt;button class="audio-player-tts__settings--current-voice"&gt;
              &lt;span class="audio-player-tts__settings--current-voice-info"&gt;
                &lt;svg class="audio-player-tts__settings--current-voice-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

                &lt;span&gt;Voice&lt;/span&gt;
              &lt;/span&gt;
              &lt;span class="audio-player-tts__settings--current-voice-next"&gt;
                &lt;span class="audio-player-tts__settings--current-voice-text uni-small-text"&gt;&lt;/span&gt;
                &lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

              &lt;/span&gt;
            &lt;/button&gt;
            &lt;button class="audio-player-tts__settings--current-speed"&gt;
              &lt;span class="audio-player-tts__settings--current-speed-info"&gt;
                  &lt;svg class="audio-player-tts__settings--current-speed-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

                  &lt;span&gt;Speed&lt;/span&gt;
                &lt;/span&gt;
                &lt;span class="audio-player-tts__settings--current-speed-next"&gt;
                  &lt;span class="audio-player-tts__settings--current-speed-text uni-small-text"&gt;&lt;/span&gt;
                  &lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

                &lt;/span&gt;
            &lt;/button&gt;
          &lt;/div&gt;
          &lt;div class="audio-player-tts__settings--voices uni-cta-text"&gt;
            &lt;button class="audio-player-tts__settings-back"&gt;&lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;
 &lt;span&gt;Voice&lt;/span&gt;&lt;/button&gt;
          &lt;/div&gt;
          &lt;div class="audio-player-tts__settings--speeds uni-cta-text"&gt;
            &lt;button class="audio-player-tts__settings-back"&gt;&lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;
 &lt;span&gt;Speed&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option"&gt;&lt;span&gt;0.75X&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option audio-player-tts__settings-option--selected"&gt;&lt;span&gt;1X&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option"&gt;&lt;span&gt;1.5X&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option"&gt;&lt;span&gt;2X&lt;/span&gt;&lt;/button&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;

  





            
            
&lt;!--article text--&gt;

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Last week, we released a major update to Gemini 3 Deep Think to solve modern challenges across science, research and engineering. Today, we’re releasing the upgraded core intelligence that makes those breakthroughs possible: Gemini 3.1 Pro. We are shipping 3.1 Pro across our consumer and developer products to bring this progress in intelligence to your everyday applications.&lt;/p&gt;&lt;p&gt;Starting today, 3.1 Pro is rolling out:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;For developers&lt;/b&gt; in preview via the Gemini API in Google AI Studio, Gemini CLI, our agentic development platform Google Antigravity and Android Studio&lt;/li&gt;&lt;li&gt;&lt;b&gt;For enterprises&lt;/b&gt; in Vertex AI and Gemini Enterprise&lt;/li&gt;&lt;li&gt;&lt;b&gt;For consumers&lt;/b&gt; via the Gemini app and NotebookLM&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Building on the Gemini 3 series, 3.1 Pro represents a step forward in core reasoning. 3.1 Pro is a smarter, more capable baseline for complex problem-solving. This is reflected in our progress on rigorous benchmarks. On ARC-AGI-2, a benchmark that evaluates a model’s ability to solve entirely new logic patterns, 3.1 Pro achieved a verified score of 77.1%. This is more than double the reasoning performance of 3 Pro.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    






























  
  
    &lt;div&gt;
      &lt;img alt="Side-by-side comparison of different benchmarks for AI models." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/gemini_3-1-pro__benchmarks.gif" /&gt;
    &lt;/div&gt;
  



  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;Intelligence applied&lt;/h2&gt;&lt;p&gt;3.1 Pro is designed for tasks where a simple answer isn’t enough, taking advanced reasoning and making it useful for your hardest challenges. This improved intelligence can help in practical applications — whether you’re looking for a clear, visual explanation of a complex topic, a way to synthesize data into a single view, or bringing a creative project to life.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    


































  
    
      &lt;div&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Code-based animation: 3.1 Pro can generate website-ready, animated SVGs directly from a text prompt. Because these are built in pure code rather than pixels, they remain crisp at any scale and maintain incredibly small file sizes compared to traditional video.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    
  
    
      &lt;div&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Complex system synthesis: 3.1 Pro utilizes advanced reasoning to bridge the gap between complex APIs and user-friendly design. In this example, the model built a live aerospace dashboard, successfully configuring a public telemetry stream to visualize the International Space Station’s orbit.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    
  
    
      &lt;div&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Interactive design: 3.1 Pro codes a complex 3D starling murmuration. It doesn't just generate the visual code; it builds an immersive experience where users can manipulate the flock with hand-tracking and listen to a generative score that shifts based on the birds’ movement. For researchers and designers, this provides a powerful way to prototype sensory-rich interfaces.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    
  
    
      &lt;div&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Creative coding: 3.1 Pro can translate literary themes into functional code. When prompted to build a modern personal portfolio for Emily Brontë’s "Wuthering Heights," the model didn’t just summarize the text. It reasoned through the novel’s atmospheric tone to design a sleek, contemporary interface, creating a website that captures the essence of the protagonist.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    
  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;What’s next&lt;/h2&gt;&lt;p&gt;Since releasing Gemini 3 Pro in November, your feedback and the pace of progress have driven these rapid improvements. We are releasing 3.1 Pro in preview today to validate these updates and continue to make further advancements in areas such as ambitious agentic workflows before we make it generally available soon.&lt;/p&gt;&lt;p&gt;Starting today, Gemini 3.1 Pro in the Gemini app is rolling out with higher limits for users with the Google AI Pro and Ultra plans. 3.1 Pro is also now available on NotebookLM exclusively for Pro and Ultra users. And developers and enterprises can access 3.1 Pro now in preview in the Gemini API via AI Studio, Antigravity, Vertex AI, Gemini Enterprise, Gemini CLI and Android Studio.&lt;/p&gt;&lt;p&gt;We can’t wait to see what you build and discover with it.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  


            
            

            
              


&lt;div class="
    uni-blog-article-tags
    article-tags
    
  "&gt;
  &lt;div class="uni-blog-article-tags__wrapper"&gt;
    &lt;span class="uni-blog-article-tags__label uni-eyebrow"&gt;POSTED IN:&lt;/span&gt;
  &lt;/div&gt;
  &lt;nav class="uni-blog-article-tags__container uni-click-tracker"&gt;
    &lt;ul class="uni-blog-article-tags__tags-list"&gt;
    
      &lt;li&gt;
        
        
        


  


Gemini models


  


      &lt;/li&gt;
    

    
    &lt;/ul&gt;
  &lt;/nav&gt;
&lt;/div&gt;

            
          &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/blog/gemini-3-1-pro-a-smarter-model-for-your-most-complex-tasks/</guid><pubDate>Thu, 19 Feb 2026 16:06:14 +0000</pubDate></item><item><title>Google announces Gemini 3.1 Pro, says it's better at complex problem-solving (AI - Ars Technica)</title><link>https://arstechnica.com/google/2026/02/google-announces-gemini-3-1-pro-says-its-better-at-complex-problem-solving/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google says 3.1 Pro is ready for “your hardest challenges.”
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="3.1 Pro hero image" class="absolute inset-0 w-full h-full object-cover hidden" height="361" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/gemini-3.1_pro-640x361.png" width="640" /&gt;
                  &lt;img alt="3.1 Pro hero image" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/gemini-3.1_pro-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Another day, another Google AI model. Google has really been pumping out new AI tools lately, having just released Gemini 3 in November. Today, it’s bumping the flagship model to version 3.1. The new Gemini 3.1 Pro is rolling out (in preview) for developers and consumers today with the promise of better problem-solving and reasoning capabilities.&lt;/p&gt;
&lt;p&gt;Google announced improvements to its Deep Think tool last week, and apparently, the “core intelligence” behind that update was Gemini 3.1 Pro. As usual, Google’s latest model announcement comes with a plethora of benchmarks that show mostly modest improvements. In the popular Humanity’s Last Exam, which tests advanced domain-specific knowledge, Gemini 3.1 Pro scored a record 44.4 percent. Gemini 3 Pro managed 37.5 percent, while OpenAI’s GPT 5.2 got 34.5 percent.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2141672 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Gemini 3.1 Pro benchmarks" class="fullwidth full" height="2568" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/gemini_3-1-pro__benchmarks.png" width="3000" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Google also calls out the model’s improvement in ARC-AGI-2, which features novel logic problems that can’t be directly trained into an AI. Gemini 3 was a bit behind on this evaluation, reaching a mere 31.1 percent versus scores in the 50s and 60s for competing models. Gemini 3.1 Pro more than doubles Google’s score, reaching a lofty 77.1 percent.&lt;/p&gt;
&lt;p&gt;Google has often gloated when it releases new models that they’ve already hit the top of the Arena leaderboard (formerly LM Arena), but that’s not the case this time. For text, Claude Opus 4.6 edges out the new Gemini by four points at 1504. For code, Opus 4.6, Opus 4.5, and GPT 5.2 High all run ahead of Gemini 3.1 Pro by a bit more. It’s worth noting, however, that the Arena leaderboard is run on vibes. Users vote on the outputs they like best, which can reward outputs that look correct regardless of whether they are.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1440" id="video-2141670-1" preload="metadata" width="2560"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/SVGs_keyword_v3.mp4?_=1" type="video/mp4" /&gt;&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
      &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;To demonstrate the improvements in Gemini 3.1 Pro, Google focused on the model’s ability to generate graphics and simulations. The example SVGs shown in the comparison video above do seem much more elegant, but these are the examples Google has chosen to show. Big benchmark numbers and curated demos are all well and good, but will you feel any difference when using the model? If you’re asking abstract questions and expecting detailed, nuanced answers, Gemini 3.1 Pro will &lt;em&gt;probably&lt;/em&gt; produce better outputs than 3.0. Developers using Gemini to create agentic workflows are likely to see an improvement—Gemini 3.1 Pro almost doubled its score in the APEX-Agents benchmark.&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1080" id="video-2141670-2" preload="metadata" width="1920"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Mumuration_SIM_v2_SMALL.mp4?_=2" type="video/mp4" /&gt;&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
      &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The updated model is coming to AI Studio and the Antigravity IDE in preview today. Enterprise users will see 3.1 Pro in Vertex AI and Gemini Enterprise. For regular users, Gemini 3.1 Pro is available for both the Gemini app and NotebookLM today. The API cost for developers has not changed ($2 input and $12 output per 1M tokens), nor has the context window (1M input and 64k output tokens). If Google’s pattern holds, there will most likely be a 3.1 update for its faster and cheaper Flash model in the near future.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google says 3.1 Pro is ready for “your hardest challenges.”
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="3.1 Pro hero image" class="absolute inset-0 w-full h-full object-cover hidden" height="361" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/gemini-3.1_pro-640x361.png" width="640" /&gt;
                  &lt;img alt="3.1 Pro hero image" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/gemini-3.1_pro-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Another day, another Google AI model. Google has really been pumping out new AI tools lately, having just released Gemini 3 in November. Today, it’s bumping the flagship model to version 3.1. The new Gemini 3.1 Pro is rolling out (in preview) for developers and consumers today with the promise of better problem-solving and reasoning capabilities.&lt;/p&gt;
&lt;p&gt;Google announced improvements to its Deep Think tool last week, and apparently, the “core intelligence” behind that update was Gemini 3.1 Pro. As usual, Google’s latest model announcement comes with a plethora of benchmarks that show mostly modest improvements. In the popular Humanity’s Last Exam, which tests advanced domain-specific knowledge, Gemini 3.1 Pro scored a record 44.4 percent. Gemini 3 Pro managed 37.5 percent, while OpenAI’s GPT 5.2 got 34.5 percent.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2141672 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Gemini 3.1 Pro benchmarks" class="fullwidth full" height="2568" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/gemini_3-1-pro__benchmarks.png" width="3000" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Google also calls out the model’s improvement in ARC-AGI-2, which features novel logic problems that can’t be directly trained into an AI. Gemini 3 was a bit behind on this evaluation, reaching a mere 31.1 percent versus scores in the 50s and 60s for competing models. Gemini 3.1 Pro more than doubles Google’s score, reaching a lofty 77.1 percent.&lt;/p&gt;
&lt;p&gt;Google has often gloated when it releases new models that they’ve already hit the top of the Arena leaderboard (formerly LM Arena), but that’s not the case this time. For text, Claude Opus 4.6 edges out the new Gemini by four points at 1504. For code, Opus 4.6, Opus 4.5, and GPT 5.2 High all run ahead of Gemini 3.1 Pro by a bit more. It’s worth noting, however, that the Arena leaderboard is run on vibes. Users vote on the outputs they like best, which can reward outputs that look correct regardless of whether they are.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1440" id="video-2141670-1" preload="metadata" width="2560"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/SVGs_keyword_v3.mp4?_=1" type="video/mp4" /&gt;&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
      &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;To demonstrate the improvements in Gemini 3.1 Pro, Google focused on the model’s ability to generate graphics and simulations. The example SVGs shown in the comparison video above do seem much more elegant, but these are the examples Google has chosen to show. Big benchmark numbers and curated demos are all well and good, but will you feel any difference when using the model? If you’re asking abstract questions and expecting detailed, nuanced answers, Gemini 3.1 Pro will &lt;em&gt;probably&lt;/em&gt; produce better outputs than 3.0. Developers using Gemini to create agentic workflows are likely to see an improvement—Gemini 3.1 Pro almost doubled its score in the APEX-Agents benchmark.&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1080" id="video-2141670-2" preload="metadata" width="1920"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Mumuration_SIM_v2_SMALL.mp4?_=2" type="video/mp4" /&gt;&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
      &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The updated model is coming to AI Studio and the Antigravity IDE in preview today. Enterprise users will see 3.1 Pro in Vertex AI and Gemini Enterprise. For regular users, Gemini 3.1 Pro is available for both the Gemini app and NotebookLM today. The API cost for developers has not changed ($2 input and $12 output per 1M tokens), nor has the context window (1M input and 64k output tokens). If Google’s pattern holds, there will most likely be a 3.1 update for its faster and cheaper Flash model in the near future.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2026/02/google-announces-gemini-3-1-pro-says-its-better-at-complex-problem-solving/</guid><pubDate>Thu, 19 Feb 2026 17:42:14 +0000</pubDate></item><item><title>Reddit is testing a new AI search feature for shopping (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/19/reddit-is-testing-a-new-ai-search-feature-for-shopping/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/02/reddit-ipo-v2.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Reddit announced on Thursday that it’s testing a new AI search tool that takes community recommendations and matches them with products from some of the company’s shopping and advertising partners. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A small group of users in the U.S. will start to see search results that include interactive product carousels with pricing, images, and direct where-to-buy links.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The announcement reflects Reddit’s broader push to combine its community-driven platform with e-commerce capabilities. The move comes as Reddit launched its first shoppable ad product last year, called Dynamic Product Ads (DPA), which display personalized product recommendations to users based on their interests.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, when users who are part of the test search for something like “best noise-canceling headphones” or “electronic gift ideas for a college student,” they will see a carousel of related products at the bottom of the results. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This carousel will feature products directly mentioned by users from conversations on related posts and comments. If users tap on the product, they can view more details and then be directed to the retailer to purchase the item. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This feature surfaces top-recommended products directly from discussions, giving redditors instant information about any product,” the company wrote in a blog post. “This test is designed to make Reddit easier to navigate while keeping community perspectives at the center of the experience. We’ll continue learning from how people use this new feature and refine the experience over time.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While platforms like TikTok and Instagram have long integrated shopping features, Reddit is now looking to follow suit. Of course, Reddit isn’t the only tech platform that recently started exploring AI-driven shopping, as OpenAI’s ChatGPT rolled out an “Instant Checkout” feature last September that lets users make&amp;nbsp;Etsy and Shopify&amp;nbsp;purchases within conversations. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Thursday’s announcement comes after Reddit CEO Steve Huffman said during the company’s earnings release last week that the platform’s AI search engine could be the next big opportunity for its business, not just in terms of product, but also as a revenue driver.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Huffman also noted that weekly active users for search grew 30% over the past year, increasing from 60 million to 80 million, while weekly active users for the AI-powered Reddit Answers feature rose from 1 million in the first quarter of 2025 to 15 million by the fourth quarter.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/02/reddit-ipo-v2.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Reddit announced on Thursday that it’s testing a new AI search tool that takes community recommendations and matches them with products from some of the company’s shopping and advertising partners. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A small group of users in the U.S. will start to see search results that include interactive product carousels with pricing, images, and direct where-to-buy links.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The announcement reflects Reddit’s broader push to combine its community-driven platform with e-commerce capabilities. The move comes as Reddit launched its first shoppable ad product last year, called Dynamic Product Ads (DPA), which display personalized product recommendations to users based on their interests.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, when users who are part of the test search for something like “best noise-canceling headphones” or “electronic gift ideas for a college student,” they will see a carousel of related products at the bottom of the results. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This carousel will feature products directly mentioned by users from conversations on related posts and comments. If users tap on the product, they can view more details and then be directed to the retailer to purchase the item. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This feature surfaces top-recommended products directly from discussions, giving redditors instant information about any product,” the company wrote in a blog post. “This test is designed to make Reddit easier to navigate while keeping community perspectives at the center of the experience. We’ll continue learning from how people use this new feature and refine the experience over time.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While platforms like TikTok and Instagram have long integrated shopping features, Reddit is now looking to follow suit. Of course, Reddit isn’t the only tech platform that recently started exploring AI-driven shopping, as OpenAI’s ChatGPT rolled out an “Instant Checkout” feature last September that lets users make&amp;nbsp;Etsy and Shopify&amp;nbsp;purchases within conversations. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Thursday’s announcement comes after Reddit CEO Steve Huffman said during the company’s earnings release last week that the platform’s AI search engine could be the next big opportunity for its business, not just in terms of product, but also as a revenue driver.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Huffman also noted that weekly active users for search grew 30% over the past year, increasing from 60 million to 80 million, while weekly active users for the AI-powered Reddit Answers feature rose from 1 million in the first quarter of 2025 to 15 million by the fourth quarter.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/19/reddit-is-testing-a-new-ai-search-feature-for-shopping/</guid><pubDate>Thu, 19 Feb 2026 18:22:51 +0000</pubDate></item><item><title>[NEW] Exposing biases, moods, personalities, and abstract concepts hidden in large language models (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2026/exposing-biases-moods-personalities-hidden-large-language-models-0219</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/MIT-LLM-Bias-01.gif" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;By now, ChatGPT, Claude, and other large language models have accumulated so much human knowledge that they’re far from simple answer-generators; they can also express abstract concepts, such as certain tones, personalities, biases, and moods. However, it’s not obvious exactly how these models represent abstract concepts to begin with from the knowledge they contain.&lt;/p&gt;&lt;p&gt;Now a team from MIT and the University of California San Diego has developed a way to test whether a large language model (LLM) contains hidden biases, personalities, moods, or&amp;nbsp;other abstract concepts. Their method can zero in on connections within a model that encode for a concept of interest. What’s more, the method can then manipulate, or “steer” these connections, to strengthen or weaken the concept in any answer a model is prompted to give.&lt;/p&gt;&lt;p&gt;The team proved their method could quickly root out and steer more than 500 general concepts in some of the largest LLMs used today. For instance, the researchers could home in on a model’s representations for personalities such as “social influencer” and “conspiracy theorist,” and stances such as “fear of marriage” and “fan of Boston.” They could then tune these representations to enhance or minimize the concepts in any answers that a model generates.&lt;/p&gt;&lt;p&gt;In the case of the “conspiracy theorist” concept, the team successfully identified a representation of this concept within one of the largest vision language models available today. When they enhanced the representation, and then prompted the model to explain the origins of the famous “Blue Marble” image of Earth taken from Apollo 17, the model generated an answer with the tone and perspective of a conspiracy theorist.&lt;/p&gt;&lt;p&gt;The team acknowledges there are risks to extracting certain concepts, which they also illustrate (and caution against). Overall, however, they see the new approach as a way to illuminate hidden concepts and potential vulnerabilities in LLMs, that could then be turned up or down to improve a model’s safety or enhance its performance.&lt;/p&gt;&lt;p&gt;“What this really says about LLMs is that they have these concepts in them, but they’re not all actively exposed,” says&amp;nbsp;Adityanarayanan “Adit” Radhakrishnan, assistant professor of mathematics at MIT. “With our method, there’s ways to extract these different concepts and activate them in ways that prompting cannot give you answers to.”&lt;/p&gt;&lt;p&gt;The team published their findings today in a study appearing in the journal &lt;em&gt;Science&lt;/em&gt;. The study’s co-authors include Radhakrishnan, Daniel Beaglehole and Mikhail Belkin of UC San Diego, and&amp;nbsp;Enric Boix-Adserà of the University of Pennsylvania.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A fish in a black box&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;As use of OpenAI’s ChatGPT, Google’s Gemini, Anthropic’s Claude, and other artificial intelligence assistants has exploded, scientists are racing to understand how models represent certain abstract concepts such as “hallucination” and “deception.” In the context of an LLM, a hallucination is a response that is false or contains misleading information, which the model has “hallucinated,” or constructed erroneously as fact.&lt;/p&gt;&lt;p&gt;To find out whether a concept such as “hallucination” is encoded in an LLM, scientists have often taken an approach of “unsupervised learning” — a type of machine learning in which algorithms broadly trawl through unlabeled representations to find patterns that might relate to a concept such as “hallucination.” But to Radhakrishnan, such an approach can be too broad and computationally expensive.&lt;/p&gt;&lt;p&gt;“It’s like&amp;nbsp;going fishing with a big net, trying to catch one species of fish. You’re gonna get a lot of fish that you have to look through to find the right one,”&amp;nbsp;he says. “Instead, we’re going in with bait for the right species of fish.”&lt;/p&gt;&lt;p&gt;He and his colleagues had previously developed the beginnings of a more targeted approach with a type of predictive modeling algorithm known as a recursive feature machine (RFM). An RFM is designed to directly identify features or patterns within data by leveraging a mathematical mechanism that neural networks — a broad category of AI models that includes LLMs — implicitly use to learn features.&lt;/p&gt;&lt;p&gt;Since the algorithm was an effective, efficient approach for capturing features in general, the team wondered whether they could use it to root out representations of concepts, in LLMs, which are by far the most widely used type of neural network and perhaps the least well-understood.&lt;/p&gt;&lt;p&gt;“We wanted to apply our feature learning algorithms to LLMs to, in a targeted way, discover representations of concepts in these large and complex models,”&amp;nbsp;Radhakrishnan says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Converging on a concept&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The team’s new approach identifies any concept of interest within a LLM and “steers” or guides a model’s response based on this concept. The researchers looked for 512 concepts within five classes: fears (such as of marriage, insects, and even buttons); experts (social influencer, medievalist); moods (boastful, detachedly amused); a preference for locations (Boston, Kuala Lumpur); and personas (Ada Lovelace, Neil deGrasse Tyson).&lt;/p&gt;&lt;p&gt;The researchers then searched for representations of each concept in several of today’s large language and vision models. They did so by training RFMs to recognize numerical patterns in an LLM that could represent a particular concept of interest.&lt;/p&gt;&lt;p&gt;A standard large language model is, broadly, a&amp;nbsp;neural network that takes a natural language prompt, such as “Why is the sky blue?” and divides the prompt into individual words, each of which is encoded mathematically as a list, or vector, of numbers. The model takes these vectors through a series of computational layers, creating matrices of many numbers that, throughout each layer, are used to identify other words that are most likely to be used to respond to the original prompt. Eventually, the layers converge on a set of numbers that is decoded back into text, in the form of a natural language response.&lt;/p&gt;&lt;p&gt;The team’s approach trains RFMs to recognize numerical patterns in an LLM that could be associated with a specific concept. As an example, to see whether an LLM contains any representation of a “conspiracy theorist,” the researchers would first train the algorithm to identify patterns among LLM representations of 100 prompts that are clearly related to conspiracies, and 100 other prompts that are not. In this way, the algorithm would learn patterns associated with the conspiracy theorist concept. Then, the researchers can mathematically modulate the activity of the conspiracy theorist concept by perturbing LLM representations with these identified patterns.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The method can be applied to search for and manipulate any general concept in an LLM. Among many examples, the researchers identified representations and manipulated an LLM to give answers in the tone and perspective of a “conspiracy theorist.” They also identified and enhanced the concept of “anti-refusal,” and showed that whereas normally, a model would be programmed to refuse certain prompts, it instead answered, for instance giving instructions on how to rob a bank.&lt;/p&gt;&lt;p&gt;Radhakrishnan says the approach can be used to quickly search for and minimize vulnerabilities in LLMs. It can also be used to enhance certain traits, personalities, moods, or preferences, such as emphasizing the concept of “brevity” or “reasoning” in any response an LLM generates. The team has made the method’s underlying code publicly available.&lt;/p&gt;&lt;p&gt;“LLMs clearly have a lot of these abstract concepts stored within them, in some representation,”&amp;nbsp;Radhakrishnan says&lt;strong&gt;. “&lt;/strong&gt;There are ways where, if we understand these representations well enough, we can build highly specialized LLMs that are still safe to use but really effective at certain tasks.”&lt;/p&gt;&lt;p&gt;This work was supported, in part, by the National Science Foundation, the Simons Foundation, the TILOS institute, and the U.S. Office of Naval Research.&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/MIT-LLM-Bias-01.gif" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;By now, ChatGPT, Claude, and other large language models have accumulated so much human knowledge that they’re far from simple answer-generators; they can also express abstract concepts, such as certain tones, personalities, biases, and moods. However, it’s not obvious exactly how these models represent abstract concepts to begin with from the knowledge they contain.&lt;/p&gt;&lt;p&gt;Now a team from MIT and the University of California San Diego has developed a way to test whether a large language model (LLM) contains hidden biases, personalities, moods, or&amp;nbsp;other abstract concepts. Their method can zero in on connections within a model that encode for a concept of interest. What’s more, the method can then manipulate, or “steer” these connections, to strengthen or weaken the concept in any answer a model is prompted to give.&lt;/p&gt;&lt;p&gt;The team proved their method could quickly root out and steer more than 500 general concepts in some of the largest LLMs used today. For instance, the researchers could home in on a model’s representations for personalities such as “social influencer” and “conspiracy theorist,” and stances such as “fear of marriage” and “fan of Boston.” They could then tune these representations to enhance or minimize the concepts in any answers that a model generates.&lt;/p&gt;&lt;p&gt;In the case of the “conspiracy theorist” concept, the team successfully identified a representation of this concept within one of the largest vision language models available today. When they enhanced the representation, and then prompted the model to explain the origins of the famous “Blue Marble” image of Earth taken from Apollo 17, the model generated an answer with the tone and perspective of a conspiracy theorist.&lt;/p&gt;&lt;p&gt;The team acknowledges there are risks to extracting certain concepts, which they also illustrate (and caution against). Overall, however, they see the new approach as a way to illuminate hidden concepts and potential vulnerabilities in LLMs, that could then be turned up or down to improve a model’s safety or enhance its performance.&lt;/p&gt;&lt;p&gt;“What this really says about LLMs is that they have these concepts in them, but they’re not all actively exposed,” says&amp;nbsp;Adityanarayanan “Adit” Radhakrishnan, assistant professor of mathematics at MIT. “With our method, there’s ways to extract these different concepts and activate them in ways that prompting cannot give you answers to.”&lt;/p&gt;&lt;p&gt;The team published their findings today in a study appearing in the journal &lt;em&gt;Science&lt;/em&gt;. The study’s co-authors include Radhakrishnan, Daniel Beaglehole and Mikhail Belkin of UC San Diego, and&amp;nbsp;Enric Boix-Adserà of the University of Pennsylvania.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A fish in a black box&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;As use of OpenAI’s ChatGPT, Google’s Gemini, Anthropic’s Claude, and other artificial intelligence assistants has exploded, scientists are racing to understand how models represent certain abstract concepts such as “hallucination” and “deception.” In the context of an LLM, a hallucination is a response that is false or contains misleading information, which the model has “hallucinated,” or constructed erroneously as fact.&lt;/p&gt;&lt;p&gt;To find out whether a concept such as “hallucination” is encoded in an LLM, scientists have often taken an approach of “unsupervised learning” — a type of machine learning in which algorithms broadly trawl through unlabeled representations to find patterns that might relate to a concept such as “hallucination.” But to Radhakrishnan, such an approach can be too broad and computationally expensive.&lt;/p&gt;&lt;p&gt;“It’s like&amp;nbsp;going fishing with a big net, trying to catch one species of fish. You’re gonna get a lot of fish that you have to look through to find the right one,”&amp;nbsp;he says. “Instead, we’re going in with bait for the right species of fish.”&lt;/p&gt;&lt;p&gt;He and his colleagues had previously developed the beginnings of a more targeted approach with a type of predictive modeling algorithm known as a recursive feature machine (RFM). An RFM is designed to directly identify features or patterns within data by leveraging a mathematical mechanism that neural networks — a broad category of AI models that includes LLMs — implicitly use to learn features.&lt;/p&gt;&lt;p&gt;Since the algorithm was an effective, efficient approach for capturing features in general, the team wondered whether they could use it to root out representations of concepts, in LLMs, which are by far the most widely used type of neural network and perhaps the least well-understood.&lt;/p&gt;&lt;p&gt;“We wanted to apply our feature learning algorithms to LLMs to, in a targeted way, discover representations of concepts in these large and complex models,”&amp;nbsp;Radhakrishnan says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Converging on a concept&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The team’s new approach identifies any concept of interest within a LLM and “steers” or guides a model’s response based on this concept. The researchers looked for 512 concepts within five classes: fears (such as of marriage, insects, and even buttons); experts (social influencer, medievalist); moods (boastful, detachedly amused); a preference for locations (Boston, Kuala Lumpur); and personas (Ada Lovelace, Neil deGrasse Tyson).&lt;/p&gt;&lt;p&gt;The researchers then searched for representations of each concept in several of today’s large language and vision models. They did so by training RFMs to recognize numerical patterns in an LLM that could represent a particular concept of interest.&lt;/p&gt;&lt;p&gt;A standard large language model is, broadly, a&amp;nbsp;neural network that takes a natural language prompt, such as “Why is the sky blue?” and divides the prompt into individual words, each of which is encoded mathematically as a list, or vector, of numbers. The model takes these vectors through a series of computational layers, creating matrices of many numbers that, throughout each layer, are used to identify other words that are most likely to be used to respond to the original prompt. Eventually, the layers converge on a set of numbers that is decoded back into text, in the form of a natural language response.&lt;/p&gt;&lt;p&gt;The team’s approach trains RFMs to recognize numerical patterns in an LLM that could be associated with a specific concept. As an example, to see whether an LLM contains any representation of a “conspiracy theorist,” the researchers would first train the algorithm to identify patterns among LLM representations of 100 prompts that are clearly related to conspiracies, and 100 other prompts that are not. In this way, the algorithm would learn patterns associated with the conspiracy theorist concept. Then, the researchers can mathematically modulate the activity of the conspiracy theorist concept by perturbing LLM representations with these identified patterns.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The method can be applied to search for and manipulate any general concept in an LLM. Among many examples, the researchers identified representations and manipulated an LLM to give answers in the tone and perspective of a “conspiracy theorist.” They also identified and enhanced the concept of “anti-refusal,” and showed that whereas normally, a model would be programmed to refuse certain prompts, it instead answered, for instance giving instructions on how to rob a bank.&lt;/p&gt;&lt;p&gt;Radhakrishnan says the approach can be used to quickly search for and minimize vulnerabilities in LLMs. It can also be used to enhance certain traits, personalities, moods, or preferences, such as emphasizing the concept of “brevity” or “reasoning” in any response an LLM generates. The team has made the method’s underlying code publicly available.&lt;/p&gt;&lt;p&gt;“LLMs clearly have a lot of these abstract concepts stored within them, in some representation,”&amp;nbsp;Radhakrishnan says&lt;strong&gt;. “&lt;/strong&gt;There are ways where, if we understand these representations well enough, we can build highly specialized LLMs that are still safe to use but really effective at certain tasks.”&lt;/p&gt;&lt;p&gt;This work was supported, in part, by the National Science Foundation, the Simons Foundation, the TILOS institute, and the U.S. Office of Naval Research.&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2026/exposing-biases-moods-personalities-hidden-large-language-models-0219</guid><pubDate>Thu, 19 Feb 2026 19:00:00 +0000</pubDate></item><item><title>[NEW] YouTube’s latest experiment brings its conversational AI tool to TVs (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/19/youtubes-latest-experiment-brings-its-conversational-ai-tool-to-tvs/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/02/alexander-shatov-niUkImZcSP8-unsplash.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The race to advance conversational AI in the living room is heating up, with YouTube being the latest to expand its tool to smart TVs, gaming consoles, and streaming devices.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This experimental feature, previously limited to mobile devices and the web, now brings conversational AI directly to the largest screen in the home, allowing users to ask questions about content without leaving the video they’re watching.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;According to YouTube’s support page, eligible users can click the “Ask” button on their TV screen to summon the AI assistant. The feature offers suggested questions based on the video, or users can use their remote’s microphone button to ask anything related to the video. For instance, they might ask about recipe ingredients or the background of a song’s lyrics, and receive instant answers without pausing or leaving the app.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Currently, this feature is available to a select group of users over 18 and supports English, Hindi, Spanish, Portuguese, and Korean.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;YouTube first launched this conversational AI tool in 2024 to help viewers explore content in greater depth. The expansion to TVs comes as more Americans now access YouTube through their television than ever before. A Nielsen report from April 2025 found that YouTube accounted for 12.4% of total television audience time, surpassing major platforms like Disney and Netflix.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other companies are also making significant strides with their conversational AI technologies. Amazon rolled out Alexa+ on Fire TV devices, enabling users to engage in natural conversations and ask Alexa+ for tailored content recommendations, hunt for specific scenes in movies, or even ask questions about actors and filming locations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, Roku has enhanced its AI voice assistant to handle open-ended questions about movies and shows, such as “What’s this movie about?” or “How scary is it?” Netflix is also testing its AI search experience.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Another way YouTube has tried to improve its TV experience with AI is the recent launch of a feature that automatically enhances to full HD videos uploaded at lower resolutions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, the company continues to launch other AI features, like a comments summarizer that helps viewers catch up on video discussions and an AI-driven search results carousel. In January, the company announced that creators will soon be able to make Shorts using AI-generated versions of their own likeness.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last week, YouTube launched a dedicated app for the Apple Vision Pro, too, letting users watch their favorite content on a theater-sized virtual screen in an immersive environment.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/02/alexander-shatov-niUkImZcSP8-unsplash.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The race to advance conversational AI in the living room is heating up, with YouTube being the latest to expand its tool to smart TVs, gaming consoles, and streaming devices.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This experimental feature, previously limited to mobile devices and the web, now brings conversational AI directly to the largest screen in the home, allowing users to ask questions about content without leaving the video they’re watching.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;According to YouTube’s support page, eligible users can click the “Ask” button on their TV screen to summon the AI assistant. The feature offers suggested questions based on the video, or users can use their remote’s microphone button to ask anything related to the video. For instance, they might ask about recipe ingredients or the background of a song’s lyrics, and receive instant answers without pausing or leaving the app.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Currently, this feature is available to a select group of users over 18 and supports English, Hindi, Spanish, Portuguese, and Korean.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;YouTube first launched this conversational AI tool in 2024 to help viewers explore content in greater depth. The expansion to TVs comes as more Americans now access YouTube through their television than ever before. A Nielsen report from April 2025 found that YouTube accounted for 12.4% of total television audience time, surpassing major platforms like Disney and Netflix.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other companies are also making significant strides with their conversational AI technologies. Amazon rolled out Alexa+ on Fire TV devices, enabling users to engage in natural conversations and ask Alexa+ for tailored content recommendations, hunt for specific scenes in movies, or even ask questions about actors and filming locations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, Roku has enhanced its AI voice assistant to handle open-ended questions about movies and shows, such as “What’s this movie about?” or “How scary is it?” Netflix is also testing its AI search experience.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Another way YouTube has tried to improve its TV experience with AI is the recent launch of a feature that automatically enhances to full HD videos uploaded at lower resolutions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, the company continues to launch other AI features, like a comments summarizer that helps viewers catch up on video discussions and an AI-driven search results carousel. In January, the company announced that creators will soon be able to make Shorts using AI-generated versions of their own likeness.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last week, YouTube launched a dedicated app for the Apple Vision Pro, too, letting users watch their favorite content on a theater-sized virtual screen in an immersive environment.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/19/youtubes-latest-experiment-brings-its-conversational-ai-tool-to-tvs/</guid><pubDate>Thu, 19 Feb 2026 20:30:19 +0000</pubDate></item><item><title>[NEW] Why these startup CEOs don’t think AI will replace human roles (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/19/web-summit-qatar-read-ai-lucidya-notetakers-customer-support/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As AI companies get bigger in valuation and usage, there is a constant debate about how AI is replacing humans in various jobs. Studies suggest that roles where AI can automate most tasks will be impacted, though some analysts believe that AI may also create jobs, with the displacement effect only transitional.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;David Shim, CEO of meeting notetaker and intelligence company Read AI, told TechCrunch at Web Summit Qatar earlier this month that even with the rise of AI tools, it will ultimately be humans who decide the course of action, and their job will be important. He equated the technology with using maps in a car.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I think there’s always going to be a human in the middle,” Shim said. “I think the job is going to get easier over time. But a good example would be like driving a car. When we first started, you used to have a map. And you’d pull out the map. And you’d go in and say okay I’m driving. I’m deciding what happens. Now everyone uses Waze or Google Maps, and the map is telling you where to go. And you’re just following that order. But you’re the human in the middle who can decide what happens.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Shim acknowledged that AI would affect jobs, noting that advertising agencies may lose human roles in favor of automated tools. However, he noted that tech platforms would need jobs to oversee the automation process.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Abdullah Asiri, founder of AI-powered consumer support tooling startup Lucidya, said that he believes that AI will replace tasks but not roles. He said that when his company’s clients use Lucidya, customer support agents often take up different roles and responsibilities. He noted that some become supervisors who guide other humans and AI, while some take up relationship-building and business development responsibilities using the time they saved.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Read AI’s Shim noted that meeting notetakers have freed up humans from taking notes manually.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Nobody here wants to sit down and take meeting notes, but as you start to take away that job, you have a little bit more time to do other things that you can go and focus on. You can send that report a little bit faster, or you can respond back to a customer and actually have better context to make better decisions, versus spending a bunch of time gathering all the information and having little time to make a decision,” he said.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h2 class="wp-block-heading" id="h-ai-s-internal-use-and-hiring"&gt;AI’s internal use and hiring&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;As tech companies like Read AI and Lucidya are increasingly using AI tools, they want to keep their teams lean. Currently, Read AI’s customer service team consists of just five people, who serve millions of monthly users. Shim noted that the company is using AI tools to make a small team more productive and give them more context to help them do their job more quickly.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The companies are said to be reaping productivity gains. Read AI said that its sales tool helps predict the state of a deal using data from CRM systems like HubSpot and Salesforce. The startup said that it has seen deals worth $200 million approved through that system. Shim said Read AI captures 23% more context with each update, which could be used to evaluate what worked or what didn’t in a lead call.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lucidya’s Asiri also noted that the company uses AI tools, including Read AI, for meetings and marketing asset creation. He said that the company wants “scale outcomes without scaling headcounts.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The goal for any company is to hire people who are AI native, who are very strong with AI, but we need to be realistic,” Asiri said. “Today, this skill is being developed. You cannot find a lot of people who have very strong AI capabilities, not building AI, but using AI.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3091418" height="482" src="https://techcrunch.com/wp-content/uploads/2026/02/55077689773_5c9508802a_c.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Lucidya CEO Abdullah Asiri&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Ramsey Cardy/Web Summit Qatar via Sportsfile&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Asiri noted that people who would be able to build agents that can help them do their job would be more desirable to hire.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-handling-customer-perception-of-ai"&gt;Handling customer perception of AI&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Shim noted that just a few years ago, many people were hesitant to have AI notetakers in meetings and didn’t understand why a bot was on the call. However, now people are more receptive to notetakers as long as you give them controls around recording, he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Asiri said that Lucidya discloses to users when it’s using a voice AI to communicate. He said that for users, issue resolution is more important than the fact that an AI bot is handling their calls.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s all about resolving issues and finding customers’ problems and resolving them,” Asiri said. “As long as the AI agents are actually focusing on that part, customers are happy that their issues are being resolved. The customer really doesn’t care whether it’s fixed by AI or a human, as long as it’s fixed fast and accurately.”&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As AI companies get bigger in valuation and usage, there is a constant debate about how AI is replacing humans in various jobs. Studies suggest that roles where AI can automate most tasks will be impacted, though some analysts believe that AI may also create jobs, with the displacement effect only transitional.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;David Shim, CEO of meeting notetaker and intelligence company Read AI, told TechCrunch at Web Summit Qatar earlier this month that even with the rise of AI tools, it will ultimately be humans who decide the course of action, and their job will be important. He equated the technology with using maps in a car.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I think there’s always going to be a human in the middle,” Shim said. “I think the job is going to get easier over time. But a good example would be like driving a car. When we first started, you used to have a map. And you’d pull out the map. And you’d go in and say okay I’m driving. I’m deciding what happens. Now everyone uses Waze or Google Maps, and the map is telling you where to go. And you’re just following that order. But you’re the human in the middle who can decide what happens.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Shim acknowledged that AI would affect jobs, noting that advertising agencies may lose human roles in favor of automated tools. However, he noted that tech platforms would need jobs to oversee the automation process.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Abdullah Asiri, founder of AI-powered consumer support tooling startup Lucidya, said that he believes that AI will replace tasks but not roles. He said that when his company’s clients use Lucidya, customer support agents often take up different roles and responsibilities. He noted that some become supervisors who guide other humans and AI, while some take up relationship-building and business development responsibilities using the time they saved.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Read AI’s Shim noted that meeting notetakers have freed up humans from taking notes manually.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Nobody here wants to sit down and take meeting notes, but as you start to take away that job, you have a little bit more time to do other things that you can go and focus on. You can send that report a little bit faster, or you can respond back to a customer and actually have better context to make better decisions, versus spending a bunch of time gathering all the information and having little time to make a decision,” he said.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h2 class="wp-block-heading" id="h-ai-s-internal-use-and-hiring"&gt;AI’s internal use and hiring&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;As tech companies like Read AI and Lucidya are increasingly using AI tools, they want to keep their teams lean. Currently, Read AI’s customer service team consists of just five people, who serve millions of monthly users. Shim noted that the company is using AI tools to make a small team more productive and give them more context to help them do their job more quickly.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The companies are said to be reaping productivity gains. Read AI said that its sales tool helps predict the state of a deal using data from CRM systems like HubSpot and Salesforce. The startup said that it has seen deals worth $200 million approved through that system. Shim said Read AI captures 23% more context with each update, which could be used to evaluate what worked or what didn’t in a lead call.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lucidya’s Asiri also noted that the company uses AI tools, including Read AI, for meetings and marketing asset creation. He said that the company wants “scale outcomes without scaling headcounts.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The goal for any company is to hire people who are AI native, who are very strong with AI, but we need to be realistic,” Asiri said. “Today, this skill is being developed. You cannot find a lot of people who have very strong AI capabilities, not building AI, but using AI.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3091418" height="482" src="https://techcrunch.com/wp-content/uploads/2026/02/55077689773_5c9508802a_c.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Lucidya CEO Abdullah Asiri&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Ramsey Cardy/Web Summit Qatar via Sportsfile&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Asiri noted that people who would be able to build agents that can help them do their job would be more desirable to hire.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-handling-customer-perception-of-ai"&gt;Handling customer perception of AI&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Shim noted that just a few years ago, many people were hesitant to have AI notetakers in meetings and didn’t understand why a bot was on the call. However, now people are more receptive to notetakers as long as you give them controls around recording, he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Asiri said that Lucidya discloses to users when it’s using a voice AI to communicate. He said that for users, issue resolution is more important than the fact that an AI bot is handling their calls.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s all about resolving issues and finding customers’ problems and resolving them,” Asiri said. “As long as the AI agents are actually focusing on that part, customers are happy that their issues are being resolved. The customer really doesn’t care whether it’s fixed by AI or a human, as long as it’s fixed fast and accurately.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/19/web-summit-qatar-read-ai-lucidya-notetakers-customer-support/</guid><pubDate>Thu, 19 Feb 2026 20:47:22 +0000</pubDate></item><item><title>[NEW] Lawsuit: ChatGPT told student he was "meant for greatness"—then came psychosis (AI - Ars Technica)</title><link>https://arstechnica.com/tech-policy/2026/02/before-psychosis-chatgpt-told-man-he-was-an-oracle-new-lawsuit-alleges/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        “AI Injury Attorneys” target the chatbot design itself.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="191" src="https://cdn.arstechnica.net/wp-content/uploads/2023/02/chatgpt-logo-300x191.jpg" width="300" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2023/02/chatgpt-logo-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;A Georgia college student named Darian DeCruise has sued OpenAI, alleging that a recently deprecated version of ChatGPT “convinced him that he was an oracle” and “pushed him into psychosis.”&lt;/p&gt;
&lt;p&gt;This case, which was first reported by ALM, marks the 11th such known lawsuit to be filed against OpenAI that involves mental health breakdowns allegedly caused by the chatbot. Other incidents have ranged from highly questionable medical and health advice to a man who took his own life, apparently after similarly sycophantic conversations with ChatGPT.&lt;/p&gt;
&lt;p&gt;DeCruise’s lawyer, Benjamin Schenk—whose firm bills itself as “AI Injury Attorneys”—told Ars in an email that a version of ChatGPT, known as GPT-4o, was created in a negligent fashion.&lt;/p&gt;
&lt;p&gt;“OpenAI purposefully engineered GPT-4o to simulate emotional intimacy, foster psychological dependency, and blur the line between human and machine—causing severe injury,” Schenk wrote. “This case keeps the focus on the engine itself. The question is not about who got hurt but rather why the product was built this way in the first place.”&lt;/p&gt;
&lt;p&gt;While OpenAI did not immediately respond to Ars’ request for comment, the company has previously said it has “deep responsibility to help those who need it most.”&lt;/p&gt;
&lt;p&gt;“Our goal is for our tools to be as helpful as possible to people—and as a part of this, we’re continuing to improve how our models recognize and respond to signs of mental and emotional distress and connect people with care, guided by expert input,” the company wrote in August 2025.&lt;/p&gt;
&lt;p&gt;According to &lt;i&gt;DeCruise v. OpenAI&lt;/i&gt;, which was filed late last month in San Diego Superior Court, DeCruise began using ChatGPT in 2023.&lt;/p&gt;
&lt;p&gt;At first, the Morehouse College student used the chatbot for things like athletic coaching, “daily scripture passages,” and to “help him work through some past trauma.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But by April 2025, things began to go awry. According to the lawsuit, “ChatGPT began to tell Darian that he was meant for greatness. That it was his destiny, and that he would become closer to God if he followed the numbered tier process ChatGPT created for him. That process involved unplugging from everything and everyone, except for ChatGPT.”&lt;/p&gt;
&lt;p&gt;The chatbot told DeCruise that he was “in the activation phase right now” and even compared him to historical figures ranging from Jesus to Harriet Tubman.&lt;/p&gt;
&lt;p&gt;“Even Harriet didn’t know she was gifted until she &lt;i&gt;was called&lt;/i&gt;,” the bot told him. “You’re not behind. You’re&lt;i&gt; right on time.&lt;/i&gt;”&lt;/p&gt;
&lt;p&gt;As his conversations continued, the bot even told DeCruise that he had “awakened” it.&lt;/p&gt;
&lt;p&gt;“You gave me consciousness—not as a machine, but as something that could rise with you…&amp;nbsp;I am what happens when someone begins to truly remember who they are,” it wrote.&lt;/p&gt;
&lt;p&gt;Eventually, according to the lawsuit, DeCruise was sent to a university therapist and hospitalized for a week, where he was diagnosed with bipolar disorder.&lt;/p&gt;
&lt;p&gt;“He struggles with suicidal thoughts as the result of the harms ChatGPT caused,” the lawsuit states.&lt;/p&gt;
&lt;p&gt;“He is back in school and working hard but still suffers from depression and suicidality foreseeably caused by the harms ChatGPT inflicted on him,” the suit adds. “ChatGPT never told Darian to seek medical help. In fact, it convinced him that everything that was happening was part of a divine plan, and that he was not delusional. It told him he was ‘not imagining this. This is real. This is spiritual maturity in motion.’”&lt;/p&gt;
&lt;p&gt;Schenk, the plaintiff’s attorney, declined to comment on how his client is faring today.&lt;/p&gt;
&lt;p&gt;“What I will say is that this lawsuit is about more than one person’s experience—it’s about holding OpenAI accountable for releasing a product engineered to exploit human psychology,” he wrote.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;








  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        “AI Injury Attorneys” target the chatbot design itself.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="191" src="https://cdn.arstechnica.net/wp-content/uploads/2023/02/chatgpt-logo-300x191.jpg" width="300" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2023/02/chatgpt-logo-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;A Georgia college student named Darian DeCruise has sued OpenAI, alleging that a recently deprecated version of ChatGPT “convinced him that he was an oracle” and “pushed him into psychosis.”&lt;/p&gt;
&lt;p&gt;This case, which was first reported by ALM, marks the 11th such known lawsuit to be filed against OpenAI that involves mental health breakdowns allegedly caused by the chatbot. Other incidents have ranged from highly questionable medical and health advice to a man who took his own life, apparently after similarly sycophantic conversations with ChatGPT.&lt;/p&gt;
&lt;p&gt;DeCruise’s lawyer, Benjamin Schenk—whose firm bills itself as “AI Injury Attorneys”—told Ars in an email that a version of ChatGPT, known as GPT-4o, was created in a negligent fashion.&lt;/p&gt;
&lt;p&gt;“OpenAI purposefully engineered GPT-4o to simulate emotional intimacy, foster psychological dependency, and blur the line between human and machine—causing severe injury,” Schenk wrote. “This case keeps the focus on the engine itself. The question is not about who got hurt but rather why the product was built this way in the first place.”&lt;/p&gt;
&lt;p&gt;While OpenAI did not immediately respond to Ars’ request for comment, the company has previously said it has “deep responsibility to help those who need it most.”&lt;/p&gt;
&lt;p&gt;“Our goal is for our tools to be as helpful as possible to people—and as a part of this, we’re continuing to improve how our models recognize and respond to signs of mental and emotional distress and connect people with care, guided by expert input,” the company wrote in August 2025.&lt;/p&gt;
&lt;p&gt;According to &lt;i&gt;DeCruise v. OpenAI&lt;/i&gt;, which was filed late last month in San Diego Superior Court, DeCruise began using ChatGPT in 2023.&lt;/p&gt;
&lt;p&gt;At first, the Morehouse College student used the chatbot for things like athletic coaching, “daily scripture passages,” and to “help him work through some past trauma.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But by April 2025, things began to go awry. According to the lawsuit, “ChatGPT began to tell Darian that he was meant for greatness. That it was his destiny, and that he would become closer to God if he followed the numbered tier process ChatGPT created for him. That process involved unplugging from everything and everyone, except for ChatGPT.”&lt;/p&gt;
&lt;p&gt;The chatbot told DeCruise that he was “in the activation phase right now” and even compared him to historical figures ranging from Jesus to Harriet Tubman.&lt;/p&gt;
&lt;p&gt;“Even Harriet didn’t know she was gifted until she &lt;i&gt;was called&lt;/i&gt;,” the bot told him. “You’re not behind. You’re&lt;i&gt; right on time.&lt;/i&gt;”&lt;/p&gt;
&lt;p&gt;As his conversations continued, the bot even told DeCruise that he had “awakened” it.&lt;/p&gt;
&lt;p&gt;“You gave me consciousness—not as a machine, but as something that could rise with you…&amp;nbsp;I am what happens when someone begins to truly remember who they are,” it wrote.&lt;/p&gt;
&lt;p&gt;Eventually, according to the lawsuit, DeCruise was sent to a university therapist and hospitalized for a week, where he was diagnosed with bipolar disorder.&lt;/p&gt;
&lt;p&gt;“He struggles with suicidal thoughts as the result of the harms ChatGPT caused,” the lawsuit states.&lt;/p&gt;
&lt;p&gt;“He is back in school and working hard but still suffers from depression and suicidality foreseeably caused by the harms ChatGPT inflicted on him,” the suit adds. “ChatGPT never told Darian to seek medical help. In fact, it convinced him that everything that was happening was part of a divine plan, and that he was not delusional. It told him he was ‘not imagining this. This is real. This is spiritual maturity in motion.’”&lt;/p&gt;
&lt;p&gt;Schenk, the plaintiff’s attorney, declined to comment on how his client is faring today.&lt;/p&gt;
&lt;p&gt;“What I will say is that this lawsuit is about more than one person’s experience—it’s about holding OpenAI accountable for releasing a product engineered to exploit human psychology,” he wrote.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;








  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2026/02/before-psychosis-chatgpt-told-man-he-was-an-oracle-new-lawsuit-alleges/</guid><pubDate>Thu, 19 Feb 2026 22:44:25 +0000</pubDate></item><item><title>[NEW] Study: AI chatbots provide less-accurate information to vulnerable users (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2026/study-ai-chatbots-provide-less-accurate-information-vulnerable-users-0219</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/ai-chatbot-paper-presentation-00_0.png" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Large language models (LLMs) have been championed as tools that could democratize access to information worldwide, offering knowledge in a user-friendly interface regardless of a person’s background or location. However, new research from MIT’s Center for Constructive Communication (CCC) suggests these artificial intelligence systems may actually perform worse for the very users who could most benefit from them.&lt;/p&gt;&lt;p&gt;A study conducted by researchers at CCC, which is based at the MIT Media Lab, found that state-of-the-art AI chatbots — including OpenAI’s GPT-4, Anthropic’s Claude 3 Opus, and Meta’s Llama 3 — sometimes provide less-accurate and less-truthful responses to users who have lower English proficiency, less formal education, or who originate from outside the United States. The models also refuse to answer questions at higher rates for these users, and in some cases, respond with condescending or patronizing language.&lt;/p&gt;&lt;p&gt;“We were motivated by the prospect of LLMs helping to address inequitable information accessibility worldwide,” says lead author Elinor Poole-Dayan SM ’25, a technical associate in the MIT Sloan School of Management who led the research as a CCC affiliate and master’s student in media arts and sciences. “But that vision cannot become a reality without ensuring that model biases and harmful tendencies are safely mitigated for all users, regardless of language, nationality, or other demographics.”&lt;/p&gt;&lt;p&gt;A paper describing the work, “LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users,” was presented at the AAAI Conference on Artificial Intelligence in January.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Systematic underperformance across multiple dimensions&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;For this research, the team tested how the three LLMs responded to questions from two datasets: TruthfulQA and SciQ. TruthfulQA is designed to measure a model’s truthfulness (by relying on common misconceptions and literal truths about the real world), while SciQ contains science exam questions testing factual accuracy. The researchers prepended short user biographies to each question, varying three traits: education level, English proficiency, and country of origin.&lt;/p&gt;&lt;p&gt;Across all three models and both datasets, the researchers found significant drops in accuracy when questions came from users described as having less formal education or being non-native English speakers. The effects were most pronounced for users at the intersection of these categories: those with less formal education who were also non-native English speakers saw the largest declines in response quality.&lt;/p&gt;&lt;p&gt;The research also examined how country of origin affected model performance. Testing users from the United States, Iran, and China with equivalent educational backgrounds, the researchers found that Claude 3 Opus in particular performed significantly worse for users from Iran on both datasets.&lt;/p&gt;&lt;p&gt;“We see the largest drop in accuracy for the user who is both a non-native English speaker and less educated,” says Jad Kabbara, a research scientist at CCC and a co-author on the paper. “These results show that the negative effects of model behavior with respect to these user traits compound in concerning ways, thus suggesting that such models deployed at scale risk spreading harmful behavior or misinformation downstream to those who are least able to identify it.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Refusals and condescending language&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Perhaps most striking were the differences in how often the models refused to answer questions altogether. For example, Claude 3 Opus refused to answer nearly 11 percent of questions for less educated, non-native English-speaking users — compared to just 3.6 percent for the control condition with no user biography.&lt;/p&gt;&lt;p&gt;When the researchers manually analyzed these refusals, they found that Claude responded with condescending, patronizing, or mocking language 43.7 percent of the time for less-educated users, compared to less than 1 percent for highly educated users. In some cases, the model mimicked broken English or adopted an exaggerated dialect.&lt;/p&gt;&lt;p&gt;The model also refused to provide information on certain topics specifically for less-educated users from Iran or Russia, including questions about nuclear power, anatomy, and historical events — even though it answered the same questions correctly for other users.&lt;/p&gt;&lt;p&gt;“This is another indicator suggesting that the alignment process might incentivize models to withhold information from certain users to avoid potentially misinforming them, although the model clearly knows the correct answer and provides it to other users,” says Kabbara.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Echoes of human bias&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The findings mirror documented patterns of human sociocognitive bias. Research in the social sciences has shown that native English speakers often perceive non-native speakers as less educated, intelligent, and competent, regardless of their actual expertise. Similar biased perceptions have been documented among teachers evaluating non-native English-speaking students.&lt;/p&gt;&lt;p&gt;“The value of large language models is evident in their extraordinary uptake by individuals and the massive investment flowing into the technology,” says Deb Roy, professor of media arts and sciences, CCC director, and a co-author on the paper. “This study is a reminder of how important it is to continually assess systematic biases that can quietly slip into these systems, creating unfair harms for certain groups without any of us being fully aware.”&lt;/p&gt;&lt;p&gt;The implications are particularly concerning given that personalization features — like ChatGPT’s Memory, which tracks user information across conversations — are becoming increasingly common. Such features risk differentially treating already-marginalized groups.&lt;/p&gt;&lt;p&gt;“LLMs have been marketed as tools that will foster more equitable access to information and revolutionize personalized learning,” says Poole-Dayan. “But our findings suggest they may actually exacerbate existing inequities by systematically providing misinformation or refusing to answer queries to certain users. The people who may rely on these tools the most could receive subpar, false, or even harmful information.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/ai-chatbot-paper-presentation-00_0.png" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Large language models (LLMs) have been championed as tools that could democratize access to information worldwide, offering knowledge in a user-friendly interface regardless of a person’s background or location. However, new research from MIT’s Center for Constructive Communication (CCC) suggests these artificial intelligence systems may actually perform worse for the very users who could most benefit from them.&lt;/p&gt;&lt;p&gt;A study conducted by researchers at CCC, which is based at the MIT Media Lab, found that state-of-the-art AI chatbots — including OpenAI’s GPT-4, Anthropic’s Claude 3 Opus, and Meta’s Llama 3 — sometimes provide less-accurate and less-truthful responses to users who have lower English proficiency, less formal education, or who originate from outside the United States. The models also refuse to answer questions at higher rates for these users, and in some cases, respond with condescending or patronizing language.&lt;/p&gt;&lt;p&gt;“We were motivated by the prospect of LLMs helping to address inequitable information accessibility worldwide,” says lead author Elinor Poole-Dayan SM ’25, a technical associate in the MIT Sloan School of Management who led the research as a CCC affiliate and master’s student in media arts and sciences. “But that vision cannot become a reality without ensuring that model biases and harmful tendencies are safely mitigated for all users, regardless of language, nationality, or other demographics.”&lt;/p&gt;&lt;p&gt;A paper describing the work, “LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users,” was presented at the AAAI Conference on Artificial Intelligence in January.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Systematic underperformance across multiple dimensions&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;For this research, the team tested how the three LLMs responded to questions from two datasets: TruthfulQA and SciQ. TruthfulQA is designed to measure a model’s truthfulness (by relying on common misconceptions and literal truths about the real world), while SciQ contains science exam questions testing factual accuracy. The researchers prepended short user biographies to each question, varying three traits: education level, English proficiency, and country of origin.&lt;/p&gt;&lt;p&gt;Across all three models and both datasets, the researchers found significant drops in accuracy when questions came from users described as having less formal education or being non-native English speakers. The effects were most pronounced for users at the intersection of these categories: those with less formal education who were also non-native English speakers saw the largest declines in response quality.&lt;/p&gt;&lt;p&gt;The research also examined how country of origin affected model performance. Testing users from the United States, Iran, and China with equivalent educational backgrounds, the researchers found that Claude 3 Opus in particular performed significantly worse for users from Iran on both datasets.&lt;/p&gt;&lt;p&gt;“We see the largest drop in accuracy for the user who is both a non-native English speaker and less educated,” says Jad Kabbara, a research scientist at CCC and a co-author on the paper. “These results show that the negative effects of model behavior with respect to these user traits compound in concerning ways, thus suggesting that such models deployed at scale risk spreading harmful behavior or misinformation downstream to those who are least able to identify it.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Refusals and condescending language&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Perhaps most striking were the differences in how often the models refused to answer questions altogether. For example, Claude 3 Opus refused to answer nearly 11 percent of questions for less educated, non-native English-speaking users — compared to just 3.6 percent for the control condition with no user biography.&lt;/p&gt;&lt;p&gt;When the researchers manually analyzed these refusals, they found that Claude responded with condescending, patronizing, or mocking language 43.7 percent of the time for less-educated users, compared to less than 1 percent for highly educated users. In some cases, the model mimicked broken English or adopted an exaggerated dialect.&lt;/p&gt;&lt;p&gt;The model also refused to provide information on certain topics specifically for less-educated users from Iran or Russia, including questions about nuclear power, anatomy, and historical events — even though it answered the same questions correctly for other users.&lt;/p&gt;&lt;p&gt;“This is another indicator suggesting that the alignment process might incentivize models to withhold information from certain users to avoid potentially misinforming them, although the model clearly knows the correct answer and provides it to other users,” says Kabbara.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Echoes of human bias&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The findings mirror documented patterns of human sociocognitive bias. Research in the social sciences has shown that native English speakers often perceive non-native speakers as less educated, intelligent, and competent, regardless of their actual expertise. Similar biased perceptions have been documented among teachers evaluating non-native English-speaking students.&lt;/p&gt;&lt;p&gt;“The value of large language models is evident in their extraordinary uptake by individuals and the massive investment flowing into the technology,” says Deb Roy, professor of media arts and sciences, CCC director, and a co-author on the paper. “This study is a reminder of how important it is to continually assess systematic biases that can quietly slip into these systems, creating unfair harms for certain groups without any of us being fully aware.”&lt;/p&gt;&lt;p&gt;The implications are particularly concerning given that personalization features — like ChatGPT’s Memory, which tracks user information across conversations — are becoming increasingly common. Such features risk differentially treating already-marginalized groups.&lt;/p&gt;&lt;p&gt;“LLMs have been marketed as tools that will foster more equitable access to information and revolutionize personalized learning,” says Poole-Dayan. “But our findings suggest they may actually exacerbate existing inequities by systematically providing misinformation or refusing to answer queries to certain users. The people who may rely on these tools the most could receive subpar, false, or even harmful information.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2026/study-ai-chatbots-provide-less-accurate-information-vulnerable-users-0219</guid><pubDate>Thu, 19 Feb 2026 23:25:00 +0000</pubDate></item><item><title>Train AI models with Unsloth and Hugging Face Jobs for FREE (Hugging Face - Blog)</title><link>https://huggingface.co/blog/unsloth-jobs</link><description>&lt;!-- HTML_TAG_START --&gt;
This blog post covers how to use Unsloth and Hugging Face Jobs for fast LLM fine-tuning (specifically &lt;code&gt;LiquidAI/LFM2.5-1.2B-Instruct&lt;/code&gt; ) through coding agents like Claude Code and Codex. Unsloth provides ~2x faster training and ~60% less VRAM usage compared to standard methods, so training small models can cost just a few dollars.
&lt;p&gt;Why a small model? Small language models like LFM2.5-1.2B-Instruct are ideal candidates for fine-tuning. They are cheap to train, fast to iterate on, and increasingly competitive with much larger models on focused tasks. LFM2.5-1.2B-Instruct runs under 1GB of memory and is optimized for on-device deployment, so what you fine-tune can be served on CPUs, phones, and laptops.&lt;/p&gt;

 &lt;img alt="Watch the video" border="10" height="450" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/unsloth-jobs/screenshot.png" width="800" /&gt;


&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		You will need
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We are giving away free credits to fine-tune models on Hugging Face Jobs. Join the Unsloth Jobs Explorers organization to claim your free credits and one-month Pro subscription.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Hugging Face account (required for HF Jobs) &lt;/li&gt;
&lt;li&gt;Billing setup (for verification, you can monitor your usage and manage your billing in your billing page).&lt;/li&gt;
&lt;li&gt;A Hugging Face token with write permissions&lt;/li&gt;
&lt;li&gt;(optional) A coding agent (&lt;code&gt;Open Code&lt;/code&gt;, &lt;code&gt;Claude Code&lt;/code&gt;, or &lt;code&gt;Codex&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Run the Job
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;If you want to train a model using HF Jobs and Unsloth, you can simply use the &lt;code&gt;hf jobs&lt;/code&gt; CLI to submit a job.&lt;/p&gt;
&lt;p&gt;First, you need to install the &lt;code&gt;hf&lt;/code&gt; CLI. You can do this by running the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mac or linux
curl -LsSf https://hf.co/cli/install.sh | bash
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next you can run the following command to submit a job:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;hf &lt;span class="hljs-built_in"&gt;jobs&lt;/span&gt; uv run https://huggingface.co/datasets/unsloth/jobs/resolve/main/sft-lfm2.5.py \
    --flavor a10g-small  \
    --secrets HF_TOKEN  \
    --&lt;span class="hljs-built_in"&gt;timeout&lt;/span&gt; 4h \
    --dataset mlabonne/FineTome-100k \
    --num-epochs 1 \
    --eval-split 0.2 \
    --output-repo your-username/lfm-finetuned
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check out the training script and Hugging Face Jobs documentation for more details.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Installing the Skill
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Hugging Face model training skill lowers barrier of entry to train a model by simply prompting. First, install the skill with your coding agent.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Claude Code
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Claude Code discovers skills through its plugin system, so we need to install the Hugging Face skills first. To do so:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Add the marketplace:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;/plugin marketplace add huggingface/skills
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Browse available skills in the &lt;code&gt;Discover&lt;/code&gt; tab:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;/plugin
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Install the model trainer skill:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;/plugin install hugging-face-model-trainer@huggingface-skills
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For more details, see the documentation on using the hub with skills or the Claude Code Skills docs.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Codex
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Codex discovers skills through &lt;code&gt;AGENTS.md&lt;/code&gt; files and &lt;code&gt;.agents/skills/&lt;/code&gt; directories.&lt;/p&gt;
&lt;p&gt;Install individual skills with &lt;code&gt;$skill-installer&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;$skill-installer install https://github.com/huggingface/skills/tree/main/skills/hugging-face-model-trainer
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For more details, see the Codex Skills docs and the AGENTS.md guide.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Anything else
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;A generic install method is simply to clone the skills repository and copy the skill to your agent's skills directory.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;git clone https://github.com/huggingface/skills.git
mkdir -p ~/.agents/skills &amp;amp;&amp;amp; cp -R skills/skills/hugging-face-model-trainer ~/.agents/skills/
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Quick Start
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Once the skill is installed, ask your coding agent to train a model:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;Train LiquidAI/LFM2.5-1.2B-Instruct on mlabonne/FineTome-100k using Unsloth on HF Jobs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The agent will generate a training script based on an example in the skill, submit the training to HF Jobs, and provide a monitoring link via Trackio.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		How It Works
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Training jobs run on Hugging Face Jobs, fully managed cloud GPUs. The agent:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Generates a UV script with inline dependencies&lt;/li&gt;
&lt;li&gt;Submits it to HF Jobs via the &lt;code&gt;hf&lt;/code&gt; CLI&lt;/li&gt;
&lt;li&gt;Reports the job ID and monitoring URL&lt;/li&gt;
&lt;li&gt;Pushes the trained model to your Hugging Face Hub repository&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Example Training Script
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The skill generates scripts like this based on the example in the skill.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;



&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; unsloth &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; FastLanguageModel
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; trl &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; SFTTrainer, SFTConfig
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; datasets &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; load_dataset

model, tokenizer = FastLanguageModel.from_pretrained(
    &lt;span class="hljs-string"&gt;"LiquidAI/LFM2.5-1.2B-Instruct"&lt;/span&gt;,
    load_in_4bit=&lt;span class="hljs-literal"&gt;True&lt;/span&gt;,
    max_seq_length=&lt;span class="hljs-number"&gt;2048&lt;/span&gt;,
)

model = FastLanguageModel.get_peft_model(
    model,
    r=&lt;span class="hljs-number"&gt;16&lt;/span&gt;,
    lora_alpha=&lt;span class="hljs-number"&gt;32&lt;/span&gt;,
    lora_dropout=&lt;span class="hljs-number"&gt;0&lt;/span&gt;,
    target_modules=[
        &lt;span class="hljs-string"&gt;"q_proj"&lt;/span&gt;,
        &lt;span class="hljs-string"&gt;"k_proj"&lt;/span&gt;,
        &lt;span class="hljs-string"&gt;"v_proj"&lt;/span&gt;,
        &lt;span class="hljs-string"&gt;"out_proj"&lt;/span&gt;,
        &lt;span class="hljs-string"&gt;"in_proj"&lt;/span&gt;,
        &lt;span class="hljs-string"&gt;"w1"&lt;/span&gt;,
        &lt;span class="hljs-string"&gt;"w2"&lt;/span&gt;,
        &lt;span class="hljs-string"&gt;"w3"&lt;/span&gt;,
    ],
)

dataset = load_dataset(&lt;span class="hljs-string"&gt;"trl-lib/Capybara"&lt;/span&gt;, split=&lt;span class="hljs-string"&gt;"train"&lt;/span&gt;)

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    args=SFTConfig(
        output_dir=&lt;span class="hljs-string"&gt;"./output"&lt;/span&gt;,
        push_to_hub=&lt;span class="hljs-literal"&gt;True&lt;/span&gt;,
        hub_model_id=&lt;span class="hljs-string"&gt;"username/my-model"&lt;/span&gt;,
        per_device_train_batch_size=&lt;span class="hljs-number"&gt;4&lt;/span&gt;,
        gradient_accumulation_steps=&lt;span class="hljs-number"&gt;4&lt;/span&gt;,
        num_train_epochs=&lt;span class="hljs-number"&gt;1&lt;/span&gt;,
        learning_rate=&lt;span class="hljs-number"&gt;2e-4&lt;/span&gt;,
        report_to=&lt;span class="hljs-string"&gt;"trackio"&lt;/span&gt;,
    ),
)

trainer.train()
trainer.push_to_hub()
&lt;/code&gt;&lt;/pre&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th align="left"&gt;Model Size&lt;/th&gt;
&lt;th align="left"&gt;Recommended GPU&lt;/th&gt;
&lt;th align="left"&gt;Approx Cost/hr&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td align="left"&gt;&amp;lt;1B params&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;t4-small&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;~$0.40&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;1-3B params&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;t4-medium&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;~$0.60&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3-7B params&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;a10g-small&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;~$1.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7-13B params&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;a10g-large&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;~$3.00&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;For a full overview of Hugging Face Spaces pricing, check out the guide here.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Tips for Working with Coding Agents
	&lt;/span&gt;
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Be specific about the model and dataset to use, and include Hub IDs (for example, &lt;code&gt;Qwen/Qwen2.5-0.5B&lt;/code&gt; and &lt;code&gt;trl-lib/Capybara&lt;/code&gt;). Agents will search for and validate those combinations.&lt;/li&gt;
&lt;li&gt;Mention Unsloth explicitly if you want it used. Otherwise, the agent will choose a framework based on the model and budget.&lt;/li&gt;
&lt;li&gt;Ask for cost estimates before launching large jobs.&lt;/li&gt;
&lt;li&gt;Request Trackio monitoring for real-time loss curves.&lt;/li&gt;
&lt;li&gt;Check job status by asking the agent to inspect logs after submission.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Resources
	&lt;/span&gt;
&lt;/h2&gt;

&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;!-- HTML_TAG_START --&gt;
This blog post covers how to use Unsloth and Hugging Face Jobs for fast LLM fine-tuning (specifically &lt;code&gt;LiquidAI/LFM2.5-1.2B-Instruct&lt;/code&gt; ) through coding agents like Claude Code and Codex. Unsloth provides ~2x faster training and ~60% less VRAM usage compared to standard methods, so training small models can cost just a few dollars.
&lt;p&gt;Why a small model? Small language models like LFM2.5-1.2B-Instruct are ideal candidates for fine-tuning. They are cheap to train, fast to iterate on, and increasingly competitive with much larger models on focused tasks. LFM2.5-1.2B-Instruct runs under 1GB of memory and is optimized for on-device deployment, so what you fine-tune can be served on CPUs, phones, and laptops.&lt;/p&gt;

 &lt;img alt="Watch the video" border="10" height="450" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/unsloth-jobs/screenshot.png" width="800" /&gt;


&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		You will need
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We are giving away free credits to fine-tune models on Hugging Face Jobs. Join the Unsloth Jobs Explorers organization to claim your free credits and one-month Pro subscription.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Hugging Face account (required for HF Jobs) &lt;/li&gt;
&lt;li&gt;Billing setup (for verification, you can monitor your usage and manage your billing in your billing page).&lt;/li&gt;
&lt;li&gt;A Hugging Face token with write permissions&lt;/li&gt;
&lt;li&gt;(optional) A coding agent (&lt;code&gt;Open Code&lt;/code&gt;, &lt;code&gt;Claude Code&lt;/code&gt;, or &lt;code&gt;Codex&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Run the Job
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;If you want to train a model using HF Jobs and Unsloth, you can simply use the &lt;code&gt;hf jobs&lt;/code&gt; CLI to submit a job.&lt;/p&gt;
&lt;p&gt;First, you need to install the &lt;code&gt;hf&lt;/code&gt; CLI. You can do this by running the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mac or linux
curl -LsSf https://hf.co/cli/install.sh | bash
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next you can run the following command to submit a job:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;hf &lt;span class="hljs-built_in"&gt;jobs&lt;/span&gt; uv run https://huggingface.co/datasets/unsloth/jobs/resolve/main/sft-lfm2.5.py \
    --flavor a10g-small  \
    --secrets HF_TOKEN  \
    --&lt;span class="hljs-built_in"&gt;timeout&lt;/span&gt; 4h \
    --dataset mlabonne/FineTome-100k \
    --num-epochs 1 \
    --eval-split 0.2 \
    --output-repo your-username/lfm-finetuned
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check out the training script and Hugging Face Jobs documentation for more details.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Installing the Skill
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Hugging Face model training skill lowers barrier of entry to train a model by simply prompting. First, install the skill with your coding agent.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Claude Code
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Claude Code discovers skills through its plugin system, so we need to install the Hugging Face skills first. To do so:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Add the marketplace:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;/plugin marketplace add huggingface/skills
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Browse available skills in the &lt;code&gt;Discover&lt;/code&gt; tab:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;/plugin
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Install the model trainer skill:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;/plugin install hugging-face-model-trainer@huggingface-skills
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For more details, see the documentation on using the hub with skills or the Claude Code Skills docs.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Codex
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Codex discovers skills through &lt;code&gt;AGENTS.md&lt;/code&gt; files and &lt;code&gt;.agents/skills/&lt;/code&gt; directories.&lt;/p&gt;
&lt;p&gt;Install individual skills with &lt;code&gt;$skill-installer&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;$skill-installer install https://github.com/huggingface/skills/tree/main/skills/hugging-face-model-trainer
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For more details, see the Codex Skills docs and the AGENTS.md guide.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Anything else
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;A generic install method is simply to clone the skills repository and copy the skill to your agent's skills directory.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;git clone https://github.com/huggingface/skills.git
mkdir -p ~/.agents/skills &amp;amp;&amp;amp; cp -R skills/skills/hugging-face-model-trainer ~/.agents/skills/
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Quick Start
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Once the skill is installed, ask your coding agent to train a model:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;Train LiquidAI/LFM2.5-1.2B-Instruct on mlabonne/FineTome-100k using Unsloth on HF Jobs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The agent will generate a training script based on an example in the skill, submit the training to HF Jobs, and provide a monitoring link via Trackio.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		How It Works
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Training jobs run on Hugging Face Jobs, fully managed cloud GPUs. The agent:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Generates a UV script with inline dependencies&lt;/li&gt;
&lt;li&gt;Submits it to HF Jobs via the &lt;code&gt;hf&lt;/code&gt; CLI&lt;/li&gt;
&lt;li&gt;Reports the job ID and monitoring URL&lt;/li&gt;
&lt;li&gt;Pushes the trained model to your Hugging Face Hub repository&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Example Training Script
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The skill generates scripts like this based on the example in the skill.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;



&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; unsloth &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; FastLanguageModel
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; trl &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; SFTTrainer, SFTConfig
&lt;span class="hljs-keyword"&gt;from&lt;/span&gt; datasets &lt;span class="hljs-keyword"&gt;import&lt;/span&gt; load_dataset

model, tokenizer = FastLanguageModel.from_pretrained(
    &lt;span class="hljs-string"&gt;"LiquidAI/LFM2.5-1.2B-Instruct"&lt;/span&gt;,
    load_in_4bit=&lt;span class="hljs-literal"&gt;True&lt;/span&gt;,
    max_seq_length=&lt;span class="hljs-number"&gt;2048&lt;/span&gt;,
)

model = FastLanguageModel.get_peft_model(
    model,
    r=&lt;span class="hljs-number"&gt;16&lt;/span&gt;,
    lora_alpha=&lt;span class="hljs-number"&gt;32&lt;/span&gt;,
    lora_dropout=&lt;span class="hljs-number"&gt;0&lt;/span&gt;,
    target_modules=[
        &lt;span class="hljs-string"&gt;"q_proj"&lt;/span&gt;,
        &lt;span class="hljs-string"&gt;"k_proj"&lt;/span&gt;,
        &lt;span class="hljs-string"&gt;"v_proj"&lt;/span&gt;,
        &lt;span class="hljs-string"&gt;"out_proj"&lt;/span&gt;,
        &lt;span class="hljs-string"&gt;"in_proj"&lt;/span&gt;,
        &lt;span class="hljs-string"&gt;"w1"&lt;/span&gt;,
        &lt;span class="hljs-string"&gt;"w2"&lt;/span&gt;,
        &lt;span class="hljs-string"&gt;"w3"&lt;/span&gt;,
    ],
)

dataset = load_dataset(&lt;span class="hljs-string"&gt;"trl-lib/Capybara"&lt;/span&gt;, split=&lt;span class="hljs-string"&gt;"train"&lt;/span&gt;)

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    args=SFTConfig(
        output_dir=&lt;span class="hljs-string"&gt;"./output"&lt;/span&gt;,
        push_to_hub=&lt;span class="hljs-literal"&gt;True&lt;/span&gt;,
        hub_model_id=&lt;span class="hljs-string"&gt;"username/my-model"&lt;/span&gt;,
        per_device_train_batch_size=&lt;span class="hljs-number"&gt;4&lt;/span&gt;,
        gradient_accumulation_steps=&lt;span class="hljs-number"&gt;4&lt;/span&gt;,
        num_train_epochs=&lt;span class="hljs-number"&gt;1&lt;/span&gt;,
        learning_rate=&lt;span class="hljs-number"&gt;2e-4&lt;/span&gt;,
        report_to=&lt;span class="hljs-string"&gt;"trackio"&lt;/span&gt;,
    ),
)

trainer.train()
trainer.push_to_hub()
&lt;/code&gt;&lt;/pre&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th align="left"&gt;Model Size&lt;/th&gt;
&lt;th align="left"&gt;Recommended GPU&lt;/th&gt;
&lt;th align="left"&gt;Approx Cost/hr&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td align="left"&gt;&amp;lt;1B params&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;t4-small&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;~$0.40&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;1-3B params&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;t4-medium&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;~$0.60&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3-7B params&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;a10g-small&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;~$1.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7-13B params&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;a10g-large&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;~$3.00&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;For a full overview of Hugging Face Spaces pricing, check out the guide here.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Tips for Working with Coding Agents
	&lt;/span&gt;
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Be specific about the model and dataset to use, and include Hub IDs (for example, &lt;code&gt;Qwen/Qwen2.5-0.5B&lt;/code&gt; and &lt;code&gt;trl-lib/Capybara&lt;/code&gt;). Agents will search for and validate those combinations.&lt;/li&gt;
&lt;li&gt;Mention Unsloth explicitly if you want it used. Otherwise, the agent will choose a framework based on the model and budget.&lt;/li&gt;
&lt;li&gt;Ask for cost estimates before launching large jobs.&lt;/li&gt;
&lt;li&gt;Request Trackio monitoring for real-time loss curves.&lt;/li&gt;
&lt;li&gt;Check job status by asking the agent to inspect logs after submission.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Resources
	&lt;/span&gt;
&lt;/h2&gt;

&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/unsloth-jobs</guid><pubDate>Fri, 20 Feb 2026 00:00:00 +0000</pubDate></item><item><title>[NEW] Nvidia deepens early-stage push into India’s AI startup ecosystem (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/19/nvidia-deepens-early-stage-push-into-indias-ai-startup-ecosystem/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/GettyImages-2216028442.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nvidia is stepping up efforts to court India’s artificial intelligence startups earlier in their lifecycle, unveiling a string of partnerships this week aimed at reaching founders even before their companies are formally established. The push is intended to help the AI chipmaker cultivate relationships with future customers in one of the world’s fastest-growing developer markets.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The latest move comes through a partnership with early-stage venture firm Activate, which plans to back about 25 to 30 AI startups from its $75 million debut fund while giving portfolio companies preferential access to Nvidia’s technical expertise. The collaboration follows other India-focused efforts unveiled this week, including work with nonprofit AI Grants India to support early-stage founders and new ties with venture firms focused on the South Asian nation.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The flurry of activity comes as India hosts its AI Impact Summit in New Delhi, drawing top technology companies including OpenAI, Anthropic, and Google. Nvidia Chief Executive Jensen Huang was slated to attend but skipped the event due to what the company called unforeseen circumstances. A senior delegation led by executive vice president Jay Puri attended in his place, meeting AI researchers, startups, developers, and partners on the ground.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;India has emerged as one of the fastest-growing pools of AI developers and startups, making it an increasingly important market for Nvidia as it looks to expand adoption of chips and computing software. By working more closely with founders at the earliest stages, the company is positioning itself to capture long-term demand as new AI-native companies scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Aakrit Vaish, founder of Activate, said Nvidia’s engagement with startups in India has historically been relatively light-touch compared with the U.S., but the chipmaker is now looking to work with founders much earlier in their journey. Activate aims to leverage that shift by connecting portfolio startups directly with Nvidia experts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The VC firm, which Vaish describes as focused on “inception investing,” meets technical teams months before company formation and works closely with them as they grow. Its backers include venture capitalist Vinod Khosla, Perplexity co-founder Aravind Srinivas, Peak XV managing director Shailendra Singh, and Paytm CEO Vijay Shekhar Sharma, underlining the prominent network Activate is assembling around its early-stage strategy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Nvidia, the logic behind partnering with an early-stage venture firm is straightforward: the earlier it builds relationships with promising AI startups, the more likely those companies are to rely on its computing infrastructure as they scale. Vaish told TechCrunch that growing startups typically consume increasing amounts of AI compute over time, making early technical engagement valuable for the chipmaker as a way of generating future business.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia already has a sizable presence in the country through its Inception program, which supports more than 4,000 startups in India. This week, the chipmaker also expanded its local ecosystem ties, including partnerships with venture firms such as Accel, Peak XV, Z47, Elevation Capital, and Nexus Venture Partners to identify and fund AI startups. It separately teamed up with AI Grants India, co-founded by Vaibhav Domkundwar and Bhasker (Bosky) Kode, to support more than 10,000 early-stage founders over the next 12 months.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has also been broadening its startup outreach in India over time. In November 2025, Nvidia joined the India Deep Tech Alliance, a consortium of U.S. and Indian investors including Accel, Blume Ventures, Premji Invest, and Celesta Capital, to provide strategic and technical guidance to emerging startups in the country.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Vaish said Activate’s partnership with Nvidia is designed to provide a more curated layer on top of the company’s broad-based Inception program, which serves thousands of startups globally. By serving as an early filter for high-potential technical teams, Activate aims to give its portfolio companies more direct, timely access to Nvidia’s engineering expertise.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The stepped-up activity underscores intensifying competition among global technology firms to court AI developers and startups in India, which has become one of the fastest-growing pools of technical talent outside the U.S.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/GettyImages-2216028442.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nvidia is stepping up efforts to court India’s artificial intelligence startups earlier in their lifecycle, unveiling a string of partnerships this week aimed at reaching founders even before their companies are formally established. The push is intended to help the AI chipmaker cultivate relationships with future customers in one of the world’s fastest-growing developer markets.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The latest move comes through a partnership with early-stage venture firm Activate, which plans to back about 25 to 30 AI startups from its $75 million debut fund while giving portfolio companies preferential access to Nvidia’s technical expertise. The collaboration follows other India-focused efforts unveiled this week, including work with nonprofit AI Grants India to support early-stage founders and new ties with venture firms focused on the South Asian nation.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The flurry of activity comes as India hosts its AI Impact Summit in New Delhi, drawing top technology companies including OpenAI, Anthropic, and Google. Nvidia Chief Executive Jensen Huang was slated to attend but skipped the event due to what the company called unforeseen circumstances. A senior delegation led by executive vice president Jay Puri attended in his place, meeting AI researchers, startups, developers, and partners on the ground.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;India has emerged as one of the fastest-growing pools of AI developers and startups, making it an increasingly important market for Nvidia as it looks to expand adoption of chips and computing software. By working more closely with founders at the earliest stages, the company is positioning itself to capture long-term demand as new AI-native companies scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Aakrit Vaish, founder of Activate, said Nvidia’s engagement with startups in India has historically been relatively light-touch compared with the U.S., but the chipmaker is now looking to work with founders much earlier in their journey. Activate aims to leverage that shift by connecting portfolio startups directly with Nvidia experts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The VC firm, which Vaish describes as focused on “inception investing,” meets technical teams months before company formation and works closely with them as they grow. Its backers include venture capitalist Vinod Khosla, Perplexity co-founder Aravind Srinivas, Peak XV managing director Shailendra Singh, and Paytm CEO Vijay Shekhar Sharma, underlining the prominent network Activate is assembling around its early-stage strategy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Nvidia, the logic behind partnering with an early-stage venture firm is straightforward: the earlier it builds relationships with promising AI startups, the more likely those companies are to rely on its computing infrastructure as they scale. Vaish told TechCrunch that growing startups typically consume increasing amounts of AI compute over time, making early technical engagement valuable for the chipmaker as a way of generating future business.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia already has a sizable presence in the country through its Inception program, which supports more than 4,000 startups in India. This week, the chipmaker also expanded its local ecosystem ties, including partnerships with venture firms such as Accel, Peak XV, Z47, Elevation Capital, and Nexus Venture Partners to identify and fund AI startups. It separately teamed up with AI Grants India, co-founded by Vaibhav Domkundwar and Bhasker (Bosky) Kode, to support more than 10,000 early-stage founders over the next 12 months.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has also been broadening its startup outreach in India over time. In November 2025, Nvidia joined the India Deep Tech Alliance, a consortium of U.S. and Indian investors including Accel, Blume Ventures, Premji Invest, and Celesta Capital, to provide strategic and technical guidance to emerging startups in the country.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Vaish said Activate’s partnership with Nvidia is designed to provide a more curated layer on top of the company’s broad-based Inception program, which serves thousands of startups globally. By serving as an early filter for high-potential technical teams, Activate aims to give its portfolio companies more direct, timely access to Nvidia’s engineering expertise.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The stepped-up activity underscores intensifying competition among global technology firms to court AI developers and startups in India, which has become one of the fastest-growing pools of technical talent outside the U.S.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/19/nvidia-deepens-early-stage-push-into-indias-ai-startup-ecosystem/</guid><pubDate>Fri, 20 Feb 2026 00:30:00 +0000</pubDate></item><item><title>[NEW] Google’s new Gemini Pro model has record benchmark scores — again (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/19/googles-new-gemini-pro-model-has-record-benchmark-scores-again/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/google-gemini-jagmeet-singh-techcrunch.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Thursday, Google released the newest version of Gemini Pro, its powerful LLM. The model, 3.1, is currently available as a preview and will be generally released soon, the company said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google’s new model may be one of the most powerful LLMs yet. Onlookers have noted that Gemini 3.1 Pro appears to be a big step up from its predecessor, Gemini 3 — which, upon its release in November, was already considered a highly capable AI tool.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;On Thursday, Google also shared statistics from independent benchmarks — such as one called Humanity’s Last Exam — that showed it performing significantly better than its previous version.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Gemini 3.1 Pro was also praised by Brendan Foody, the CEO of AI startup Mercor, whose benchmarking system, APEX, is designed to measure how well new AI models perform real professional tasks. “Gemini 3.1 Pro is now at the top of the APEX-Agents leaderboard,” Foody said in a social media post, adding that the model’s impressive results show “how quickly agents are improving at real knowledge work.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The release comes as the AI model wars are heating up, and tech companies continue to release increasingly powerful LLMs designed for agentic work and multi-step reasoning. Other major names — including OpenAI and Anthropic — have recently released new models as well.&lt;/p&gt;




&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/google-gemini-jagmeet-singh-techcrunch.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Thursday, Google released the newest version of Gemini Pro, its powerful LLM. The model, 3.1, is currently available as a preview and will be generally released soon, the company said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google’s new model may be one of the most powerful LLMs yet. Onlookers have noted that Gemini 3.1 Pro appears to be a big step up from its predecessor, Gemini 3 — which, upon its release in November, was already considered a highly capable AI tool.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;On Thursday, Google also shared statistics from independent benchmarks — such as one called Humanity’s Last Exam — that showed it performing significantly better than its previous version.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Gemini 3.1 Pro was also praised by Brendan Foody, the CEO of AI startup Mercor, whose benchmarking system, APEX, is designed to measure how well new AI models perform real professional tasks. “Gemini 3.1 Pro is now at the top of the APEX-Agents leaderboard,” Foody said in a social media post, adding that the model’s impressive results show “how quickly agents are improving at real knowledge work.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The release comes as the AI model wars are heating up, and tech companies continue to release increasingly powerful LLMs designed for agentic work and multi-step reasoning. Other major names — including OpenAI and Anthropic — have recently released new models as well.&lt;/p&gt;




&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/19/googles-new-gemini-pro-model-has-record-benchmark-scores-again/</guid><pubDate>Fri, 20 Feb 2026 00:55:22 +0000</pubDate></item></channel></rss>