<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 10 Oct 2025 01:39:03 +0000</lastBuildDate><item><title>How AI is shaping the future of mobility with Uber’s CPO and Nuro’s co-founder at TechCrunch Disrupt 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/09/how-ai-is-shaping-the-future-of-mobility-with-ubers-cpo-and-nuros-co-founder-at-techcrunch-disrupt-2025/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Artificial intelligence is not just changing how we work — it’s transforming how we move. At &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, happening October 27-29 at San Francisco’s Moscone West, two industry leaders at the forefront of AI-driven mobility will take the &lt;strong&gt;AI Stage&lt;/strong&gt; to explore the future of intelligent transportation systems: Uber’s Sachin Kansal and Nuro’s Dave Ferguson.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Together, Kansal and Ferguson will discuss the evolving relationship between AI and mobility: how predictive models and computer vision are improving road safety, why last-mile delivery is an autonomy proving ground, and what it will take to bring AI-driven transportation to scale.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;With the transportation industry evolving so quickly, this session gives you an exclusive insider’s look at the future of mobility. &lt;strong&gt;Register now to save up to $444&lt;/strong&gt; on your pass, or &lt;strong&gt;up to 30% on group passes&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Dave Ferguson Sachin Kansal" class="wp-image-3055844" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/TC25_Ferguson-Kansal-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-steering-tomorrow-meet-the-pioneers-of-mobility"&gt;Steering tomorrow: Meet the pioneers of mobility&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Sachin Kansal, chief product officer at Uber Technologies, oversees the company’s global Mobility and Delivery products, including safety, sustainability, and autonomous vehicle initiatives. He’s helping define how AI and automation will power Uber’s next decade — from more efficient ride-matching to next-generation logistics networks. He oversees product and technology strategy for Uber’s new initiatives in autonomous vehicles, sustainability, taxis, and Uber for Teens.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Before joining Uber in 2017, he served as vice president of Product at Lookout and chief product officer at Flywheel Software. Earlier in his career, he led product management at Palm (acquired by HP) for the webOS mobile platform. Kansal holds engineering degrees from Gujarat University and Stanford University and has authored several patents in mobile and location technologies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Joining him is Dave Ferguson, co-founder and president of Nuro, the self-driving technology company pioneering scalable autonomy for robotaxis, commercial fleets, and personal vehicles. Ferguson’s experience spans Google’s early self-driving program (now Waymo) and Carnegie Mellon’s DARPA Urban Challenge-winning team, giving him a rare perspective on how robotics research translates to real-world transportation breakthroughs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A leading voice in robotics and AI, Ferguson has published over 60 papers, holds more than 100 patents, and earned his MS and PhD in Robotics from Carnegie Mellon University.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h2 class="wp-block-heading" id="h-claim-your-pass-to-the-forefront-of-mobility-innovation"&gt;Claim your pass to the forefront of mobility innovation&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;As cities and companies alike race toward smarter infrastructure and sustainable mobility, their conversation will offer an insider’s view on what the next decade of intelligent transportation could look like.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Catch &lt;strong&gt;Sachin Kansal&lt;/strong&gt; and &lt;strong&gt;Dave Ferguson&lt;/strong&gt; live at &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; as part of the &lt;strong&gt;AI Stage&lt;/strong&gt; program exploring how artificial intelligence is reshaping industries from logistics to life sciences. &lt;strong&gt;Register now&lt;/strong&gt; before ticket rates increase.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Disrupt 2024 Main Stage" class="wp-image-2953554" height="453" src="https://techcrunch.com/wp-content/uploads/2025/01/Disrupt-2024-main-stage.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Artificial intelligence is not just changing how we work — it’s transforming how we move. At &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, happening October 27-29 at San Francisco’s Moscone West, two industry leaders at the forefront of AI-driven mobility will take the &lt;strong&gt;AI Stage&lt;/strong&gt; to explore the future of intelligent transportation systems: Uber’s Sachin Kansal and Nuro’s Dave Ferguson.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Together, Kansal and Ferguson will discuss the evolving relationship between AI and mobility: how predictive models and computer vision are improving road safety, why last-mile delivery is an autonomy proving ground, and what it will take to bring AI-driven transportation to scale.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;With the transportation industry evolving so quickly, this session gives you an exclusive insider’s look at the future of mobility. &lt;strong&gt;Register now to save up to $444&lt;/strong&gt; on your pass, or &lt;strong&gt;up to 30% on group passes&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Dave Ferguson Sachin Kansal" class="wp-image-3055844" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/TC25_Ferguson-Kansal-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-steering-tomorrow-meet-the-pioneers-of-mobility"&gt;Steering tomorrow: Meet the pioneers of mobility&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Sachin Kansal, chief product officer at Uber Technologies, oversees the company’s global Mobility and Delivery products, including safety, sustainability, and autonomous vehicle initiatives. He’s helping define how AI and automation will power Uber’s next decade — from more efficient ride-matching to next-generation logistics networks. He oversees product and technology strategy for Uber’s new initiatives in autonomous vehicles, sustainability, taxis, and Uber for Teens.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Before joining Uber in 2017, he served as vice president of Product at Lookout and chief product officer at Flywheel Software. Earlier in his career, he led product management at Palm (acquired by HP) for the webOS mobile platform. Kansal holds engineering degrees from Gujarat University and Stanford University and has authored several patents in mobile and location technologies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Joining him is Dave Ferguson, co-founder and president of Nuro, the self-driving technology company pioneering scalable autonomy for robotaxis, commercial fleets, and personal vehicles. Ferguson’s experience spans Google’s early self-driving program (now Waymo) and Carnegie Mellon’s DARPA Urban Challenge-winning team, giving him a rare perspective on how robotics research translates to real-world transportation breakthroughs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A leading voice in robotics and AI, Ferguson has published over 60 papers, holds more than 100 patents, and earned his MS and PhD in Robotics from Carnegie Mellon University.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h2 class="wp-block-heading" id="h-claim-your-pass-to-the-forefront-of-mobility-innovation"&gt;Claim your pass to the forefront of mobility innovation&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;As cities and companies alike race toward smarter infrastructure and sustainable mobility, their conversation will offer an insider’s view on what the next decade of intelligent transportation could look like.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Catch &lt;strong&gt;Sachin Kansal&lt;/strong&gt; and &lt;strong&gt;Dave Ferguson&lt;/strong&gt; live at &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; as part of the &lt;strong&gt;AI Stage&lt;/strong&gt; program exploring how artificial intelligence is reshaping industries from logistics to life sciences. &lt;strong&gt;Register now&lt;/strong&gt; before ticket rates increase.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Disrupt 2024 Main Stage" class="wp-image-2953554" height="453" src="https://techcrunch.com/wp-content/uploads/2025/01/Disrupt-2024-main-stage.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/09/how-ai-is-shaping-the-future-of-mobility-with-ubers-cpo-and-nuros-co-founder-at-techcrunch-disrupt-2025/</guid><pubDate>Thu, 09 Oct 2025 14:00:00 +0000</pubDate></item><item><title>India pilots AI chatbot-led e-commerce with ChatGPT, Gemini, Claude in the mix (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/09/india-pilots-ai-chatbot-led-e-commerce-with-chatgpt-gemini-claude-in-the-mix/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;India has kicked off a pilot to let consumers shop and pay directly through AI chatbots, with OpenAI’s ChatGPT leading the rollout and integrations with Google’s Gemini and Anthropic’s Claude in development, as the South Asian nation becomes the next major market for global AI companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Thursday, the National Payments Corporation of India (NPCI), the federal body behind the country’s widely used Unified Payments Interface (UPI), partnered with OpenAI and fintech firm Razorpay to enable consumers to shop and pay directly through ChatGPT. Razorpay confirmed to TechCrunch that the pilot is being rolled out nationwide and will become widely available in the coming months.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The experience is built on UPI Reserve Pay — a new NPCI protocol that allows users to block a specific amount of funds for future debits to designated merchants — and UPI Circle, a solution that delegates UPI authentication, enabling payments to be completed directly within ChatGPT without switching to external apps or websites. Razorpay has developed the merchant integration layer that allows businesses to transact through AI chatbots.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tata Group-owned online grocer BigBasket and telecom operator Vi are the initial merchant partners for the new pilot, allowing customers to shop for groceries or purchase mobile recharge plans directly through ChatGPT. Additionally, Axis Bank and Airtel Payments Bank are powering the banking layer.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-3055972" height="1209" src="https://techcrunch.com/wp-content/uploads/2025/10/razorpay-chatgpt-e-commerce.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Razorpay&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;India, the world’s most populous nation and home to over a billion internet subscribers, is already one of OpenAI’s top markets for ChatGPT. OpenAI has been looking to grow its footprint in the country, launching a sub-$5 ChatGPT Go plan in August to attract more subscribers. The commerce pilot is part of its broader push to deepen engagement and tap into India’s rapidly expanding digital economy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s not just a payment experience,” said Razorpay co-founder and CEO Harshil Mathur, in an interview. “It’s a whole new discovery and commerce experience.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;UPI is already a massive success in India, powering over 20 billion transactions each month and serving as the country’s leading digital payments channel. Still, the new AI-led experience could help NPCI drive further adoption by embedding UPI into everyday commerce if consumers embrace chatbots as a new interface for shopping.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Mathur told TechCrunch that the company has also completed a proof-of-concept for the new agentic payment experience with Google’s Gemini and Anthropic’s Claude. These integrations will go live for consumers in a few weeks, he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Razorpay is also in talks with merchants other than BigBasket and Vi and is expecting a broader rollout in the next couple of months, Mathur said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Like OpenAI, both Google and Anthropic are seeing a surge of new users from India. While Google already has a deep local presence — with mass-market products like Android, Search, and YouTube — OpenAI and Anthropic are now taking steps to establish a footprint in the country as they work to localize their AI offerings for Indian users.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;India’s current plan with agentic payments does not involve any specific revenue-sharing model for partners involved. However, the move would help AI companies, including OpenAI, achieve greater user retention.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mathur confirmed to TechCrunch that AI companies will not get access to the payment data under the new setup, and users will pre-authorize the amount transacted through chatbots through two-factor authentication.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last month, OpenAI introduced an “Instant Checkout” experience and its Agentic Commerce Protocol, developed in partnership with Stripe, to help businesses and merchants connect with consumers using AI agents. Similarly, Google launched its Agent Payments Protocol to enable AI agents to make transactions on behalf of users.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This is still an early, forward-looking concept, but one with tremendous potential. Its adoption will naturally grow in line with how fast shopping agents take off,” Reeju Datta, co-founder of Cashfree Payments, told TechCrunch.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-3055973" height="1074" src="https://techcrunch.com/wp-content/uploads/2025/10/cashfree-agentic-payments.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Cashfree Payments&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to Razorpay, online merchant payments startup Cashfree Payments has launched its Agentic Payments MCP that can help merchants enable payments directly through their shopping agents. Cashfree’s solution supports all major payment methods, including cards and UPI. However, in the case of Cashfree, merchants need to develop shopping agents on their own to integrate them with MCP.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup demonstrated the experience through a video showing a chatbot enabling users to buy an iPhone.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Datta declined to name the initial merchants working on the agentic shopping experience, saying only, “we are exploring this with a few large enterprise merchants across categories like e-commerce and lending.”&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;India has kicked off a pilot to let consumers shop and pay directly through AI chatbots, with OpenAI’s ChatGPT leading the rollout and integrations with Google’s Gemini and Anthropic’s Claude in development, as the South Asian nation becomes the next major market for global AI companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Thursday, the National Payments Corporation of India (NPCI), the federal body behind the country’s widely used Unified Payments Interface (UPI), partnered with OpenAI and fintech firm Razorpay to enable consumers to shop and pay directly through ChatGPT. Razorpay confirmed to TechCrunch that the pilot is being rolled out nationwide and will become widely available in the coming months.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The experience is built on UPI Reserve Pay — a new NPCI protocol that allows users to block a specific amount of funds for future debits to designated merchants — and UPI Circle, a solution that delegates UPI authentication, enabling payments to be completed directly within ChatGPT without switching to external apps or websites. Razorpay has developed the merchant integration layer that allows businesses to transact through AI chatbots.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tata Group-owned online grocer BigBasket and telecom operator Vi are the initial merchant partners for the new pilot, allowing customers to shop for groceries or purchase mobile recharge plans directly through ChatGPT. Additionally, Axis Bank and Airtel Payments Bank are powering the banking layer.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-3055972" height="1209" src="https://techcrunch.com/wp-content/uploads/2025/10/razorpay-chatgpt-e-commerce.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Razorpay&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;India, the world’s most populous nation and home to over a billion internet subscribers, is already one of OpenAI’s top markets for ChatGPT. OpenAI has been looking to grow its footprint in the country, launching a sub-$5 ChatGPT Go plan in August to attract more subscribers. The commerce pilot is part of its broader push to deepen engagement and tap into India’s rapidly expanding digital economy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s not just a payment experience,” said Razorpay co-founder and CEO Harshil Mathur, in an interview. “It’s a whole new discovery and commerce experience.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;UPI is already a massive success in India, powering over 20 billion transactions each month and serving as the country’s leading digital payments channel. Still, the new AI-led experience could help NPCI drive further adoption by embedding UPI into everyday commerce if consumers embrace chatbots as a new interface for shopping.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Mathur told TechCrunch that the company has also completed a proof-of-concept for the new agentic payment experience with Google’s Gemini and Anthropic’s Claude. These integrations will go live for consumers in a few weeks, he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Razorpay is also in talks with merchants other than BigBasket and Vi and is expecting a broader rollout in the next couple of months, Mathur said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Like OpenAI, both Google and Anthropic are seeing a surge of new users from India. While Google already has a deep local presence — with mass-market products like Android, Search, and YouTube — OpenAI and Anthropic are now taking steps to establish a footprint in the country as they work to localize their AI offerings for Indian users.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;India’s current plan with agentic payments does not involve any specific revenue-sharing model for partners involved. However, the move would help AI companies, including OpenAI, achieve greater user retention.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mathur confirmed to TechCrunch that AI companies will not get access to the payment data under the new setup, and users will pre-authorize the amount transacted through chatbots through two-factor authentication.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last month, OpenAI introduced an “Instant Checkout” experience and its Agentic Commerce Protocol, developed in partnership with Stripe, to help businesses and merchants connect with consumers using AI agents. Similarly, Google launched its Agent Payments Protocol to enable AI agents to make transactions on behalf of users.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This is still an early, forward-looking concept, but one with tremendous potential. Its adoption will naturally grow in line with how fast shopping agents take off,” Reeju Datta, co-founder of Cashfree Payments, told TechCrunch.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-3055973" height="1074" src="https://techcrunch.com/wp-content/uploads/2025/10/cashfree-agentic-payments.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Cashfree Payments&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to Razorpay, online merchant payments startup Cashfree Payments has launched its Agentic Payments MCP that can help merchants enable payments directly through their shopping agents. Cashfree’s solution supports all major payment methods, including cards and UPI. However, in the case of Cashfree, merchants need to develop shopping agents on their own to integrate them with MCP.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup demonstrated the experience through a video showing a chatbot enabling users to buy an iPhone.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Datta declined to name the initial merchants working on the agentic shopping experience, saying only, “we are exploring this with a few large enterprise merchants across categories like e-commerce and lending.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/09/india-pilots-ai-chatbot-led-e-commerce-with-chatgpt-gemini-claude-in-the-mix/</guid><pubDate>Thu, 09 Oct 2025 14:46:53 +0000</pubDate></item><item><title>Echelon's AI agents take aim at Accenture and Deloitte consulting models (AI | VentureBeat)</title><link>https://venturebeat.com/ai/echelons-ai-agents-take-aim-at-accenture-and-deloitte-consulting-models</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://www.echelonai.com/"&gt;&lt;u&gt;Echelon&lt;/u&gt;&lt;/a&gt;, an artificial intelligence startup that automates enterprise software implementations, emerged from stealth mode today with $4.75 million in seed funding led by &lt;a href="https://baincapitalventures.com/"&gt;&lt;u&gt;Bain Capital Ventures&lt;/u&gt;&lt;/a&gt;, targeting a fundamental shift in how companies deploy and maintain critical business systems.&lt;/p&gt;&lt;p&gt;The San Francisco-based company has developed AI agents specifically trained to handle end-to-end &lt;a href="https://www.servicenow.com/"&gt;&lt;u&gt;ServiceNow&lt;/u&gt;&lt;/a&gt; implementations — complex enterprise software deployments that traditionally require months of work by offshore consulting teams and cost companies millions of dollars annually.&lt;/p&gt;&lt;p&gt;&amp;quot;The biggest barrier to digital transformation isn&amp;#x27;t technology — it&amp;#x27;s the time it takes to implement it,&amp;quot; said Rahul Kayala, Echelon&amp;#x27;s founder and CEO, who previously worked at AI-powered IT company &lt;a href="https://www.moveworks.com/"&gt;&lt;u&gt;Moveworks&lt;/u&gt;&lt;/a&gt;. &amp;quot;AI agents are eliminating that constraint entirely, allowing enterprises to experiment, iterate, and deploy platform changes with unprecedented speed.&amp;quot;&lt;/p&gt;&lt;p&gt;The announcement signals a potential disruption to the&lt;a href="https://www.ciodive.com/news/global-ai-spending-trillions-cloud-infrastructure-software-gartner/760303/"&gt;&lt;u&gt; $1.5 trillion global IT services market&lt;/u&gt;&lt;/a&gt;, where companies like &lt;a href="https://www.accenture.com/"&gt;&lt;u&gt;Accenture&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.deloitte.com/us/en.html"&gt;&lt;u&gt;Deloitte&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://www.capgemini.com/us-en/"&gt;&lt;u&gt;Capgemini&lt;/u&gt;&lt;/a&gt; have long dominated through labor-intensive consulting models that Echelon argues are becoming obsolete in the age of artificial intelligence.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why ServiceNow deployments take months and cost millions&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://www.servicenow.com/"&gt;&lt;u&gt;ServiceNow&lt;/u&gt;&lt;/a&gt;, a cloud-based platform used by enterprises to manage IT services, human resources, and business workflows, has become critical infrastructure for large organizations. However, implementing and customizing the platform typically requires specialized expertise that most companies lack internally.&lt;/p&gt;&lt;p&gt;The complexity stems from ServiceNow&amp;#x27;s vast customization capabilities. Organizations often need hundreds of &amp;quot;&lt;a href="https://www.echelonai.com/blog/service-catalog-migration"&gt;&lt;u&gt;catalog items&lt;/u&gt;&lt;/a&gt;&amp;quot; — digital forms and workflows for employee requests — each requiring specific configurations, approval processes, and integrations with existing systems. According to Echelon&amp;#x27;s research, these implementations frequently stretch far beyond planned timelines due to technical complexity and communication bottlenecks between business stakeholders and development teams.&lt;/p&gt;&lt;p&gt;&amp;quot;What starts out simple often turns into weeks of effort once the actual work begins,&amp;quot; the company noted in its &lt;a href="https://www.echelonai.com/blog/service-catalog-migration"&gt;&lt;u&gt;analysis of common implementation challenges&lt;/u&gt;&lt;/a&gt;. &amp;quot;A basic request form turns out to be five requests stuffed into one. We had catalog items with 50+ variables, 10 or more UI policies, all connected. Update one field, and something else would break.&amp;quot;&lt;/p&gt;&lt;p&gt;The traditional solution involves hiring offshore development teams or expensive consultants, creating what Echelon describes as a problematic cycle: &amp;quot;One question here, one delay there, and suddenly you&amp;#x27;re weeks behind.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How AI agents replace expensive offshore consulting teams&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Echelon&amp;#x27;s approach replaces human consultants with AI agents trained by elite &lt;a href="https://www.servicenow.com/"&gt;&lt;u&gt;ServiceNow&lt;/u&gt;&lt;/a&gt; experts from top consulting firms. These agents can analyze business requirements, ask clarifying questions in real-time, and automatically generate complete ServiceNow configurations including forms, workflows, testing scenarios, and documentation.&lt;/p&gt;&lt;p&gt;The technology delivers a significant advancement from general-purpose AI tools. Rather than providing generic code suggestions, Echelon&amp;#x27;s agents understand ServiceNow&amp;#x27;s specific architecture, best practices, and common integration patterns. They can identify gaps in requirements and propose solutions that align with enterprise governance standards.&lt;/p&gt;&lt;p&gt;&amp;quot;Instead of routing every piece of input through five people, the business process owner directly uploaded their requirements,&amp;quot; Kayala explained, describing a recent customer implementation. &amp;quot;The AI developer analyzes it and asks follow-up questions like: &amp;#x27;I see a process flow with 3 branches, but only 2 triggers. Should there be a 3rd?&amp;#x27; The kinds of things a seasoned developer would ask. With AI, these questions came instantly.&amp;quot;&lt;/p&gt;&lt;p&gt;Early customers report dramatic time savings. One financial services company saw a service catalog migration project that was projected to take six months &lt;a href="https://www.echelonai.com/"&gt;&lt;u&gt;completed in six weeks&lt;/u&gt;&lt;/a&gt; using Echelon&amp;#x27;s AI agents.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What makes Echelon&amp;#x27;s AI different from coding assistants&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Echelon&amp;#x27;s technology addresses several technical challenges that have prevented broader AI adoption in enterprise software implementation. The agents are trained not just on ServiceNow&amp;#x27;s technical capabilities but on the accumulated expertise of senior consultants who understand complex enterprise requirements, governance frameworks, and integration patterns.&lt;/p&gt;&lt;p&gt;This approach differs from general-purpose AI coding assistants like &lt;a href="https://github.com/features/copilot"&gt;&lt;u&gt;GitHub Copilot&lt;/u&gt;&lt;/a&gt;, which provide syntax suggestions but lack domain-specific expertise. Echelon&amp;#x27;s agents understand ServiceNow&amp;#x27;s data models, security frameworks, and upgrade considerations—knowledge typically acquired through years of consulting experience.&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s training methodology involves elite ServiceNow experts from consulting firms like &lt;a href="https://www.accenture.com/"&gt;&lt;u&gt;Accenture&lt;/u&gt;&lt;/a&gt; and specialized ServiceNow partner &lt;a href="https://www.thirdera.com/"&gt;&lt;u&gt;Thirdera&lt;/u&gt;&lt;/a&gt;. This embedded expertise enables the AI to handle complex requirements and edge cases that typically require senior consultant intervention.&lt;/p&gt;&lt;p&gt;The real challenge isn&amp;#x27;t teaching AI to write code — it&amp;#x27;s capturing the intuitive expertise that separates junior developers from seasoned architects. Senior ServiceNow consultants instinctively know which customizations will break during upgrades and how simple requests spiral into complex integration problems. This institutional knowledge creates a far more defensible moat than general-purpose coding assistants can offer.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The $1.5 trillion consulting market faces disruption&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Echelon&amp;#x27;s emergence reflects broader trends reshaping the enterprise software market. As companies accelerate digital transformation initiatives, the traditional consulting model increasingly appears inadequate for the speed and scale required.&lt;/p&gt;&lt;p&gt;ServiceNow itself has grown rapidly, reporting over &lt;a href="https://stockanalysis.com/stocks/now/revenue/"&gt;&lt;u&gt;$10.98 billion in annual revenue in 2024&lt;/u&gt;&lt;/a&gt;, and $12.06 billion for the trailing twelve months ending June 30, 2025, as organizations continue to digitize more business processes. However, this growth has created a persistent talent shortage, with demand for skilled ServiceNow professionals — particularly those with AI expertise — significantly outpacing supply.&lt;/p&gt;&lt;p&gt;The startup&amp;#x27;s approach could fundamentally alter the economics of enterprise software implementation. Traditional consulting engagements often involve large teams working for months, with costs scaling linearly with project complexity. AI agents, by contrast, can handle multiple projects simultaneously and apply learned knowledge across customers.&lt;/p&gt;&lt;p&gt;Rak Garg, the Bain Capital Ventures partner who led Echelon&amp;#x27;s funding round, sees this as part of a larger shift toward AI-powered professional services. &amp;quot;We see the same trend with other BCV companies like &lt;a href="https://www.prophetsecurity.ai/"&gt;&lt;u&gt;Prophet Security&lt;/u&gt;&lt;/a&gt;, which automates security operations, and &lt;a href="https://baincapitalventures.com/insight/crosby-is-redefining-legal-work-with-ai-powered-contract-automation/"&gt;&lt;u&gt;Crosby&lt;/u&gt;&lt;/a&gt;, which automates legal services for startups. AI is quickly becoming the delivery layer across multiple functions.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Scaling beyond ServiceNow while maintaining enterprise reliability&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Despite early success, Echelon faces significant challenges in scaling its approach. Enterprise customers prioritize reliability above speed, and any AI-generated configurations must meet strict security and compliance requirements.&lt;/p&gt;&lt;p&gt;&amp;quot;Inertia is the biggest risk,&amp;quot; Garg acknowledged. &amp;quot;IT systems shouldn&amp;#x27;t ever go down, and companies lose thousands of man-hours of productivity with every outage. Proving reliability at scale, and building on repeatable results will be critical for Echelon.&amp;quot;&lt;/p&gt;&lt;p&gt;The company plans to expand beyond ServiceNow to other enterprise platforms including &lt;a href="https://www.sap.com/index.html"&gt;&lt;u&gt;SAP&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.salesforce.com/"&gt;&lt;u&gt;Salesforce&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://www.workday.com/en-us/homepage.html"&gt;&lt;u&gt;Workday&lt;/u&gt;&lt;/a&gt; — each creating substantial additional market opportunities. However, each platform requires developing new domain expertise and training models on platform-specific best practices.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.echelonai.com/"&gt;&lt;u&gt;Echelon&lt;/u&gt;&lt;/a&gt; also faces potential competition from established consulting firms that are developing their own AI capabilities. However, Garg views these firms as potential partners rather than competitors, noting that many have already approached Echelon about collaboration opportunities.&lt;/p&gt;&lt;p&gt;&amp;quot;They know that AI is shifting their business model in real-time,&amp;quot; he said. &amp;quot;Customers are placing immense pricing pressure on larger firms and asking hard questions, and these firms can use Echelon agents to accelerate their projects.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How AI agents could reshape all professional services&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Echelon&amp;#x27;s funding and emergence from stealth marks a significant milestone in the application of AI to professional services. Unlike consumer AI applications that primarily enhance individual productivity, enterprise AI agents like Echelon&amp;#x27;s directly replace skilled labor at scale.&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s approach — training AI systems on expert knowledge rather than just technical documentation — could serve as a model for automating other complex professional services. Legal research, financial analysis, and technical consulting all involve similar patterns of applying specialized expertise to unique customer requirements.&lt;/p&gt;&lt;p&gt;For enterprise customers, the promise extends beyond cost savings to strategic agility. Organizations that can rapidly implement and modify business processes gain competitive advantages in markets where customer expectations and regulatory requirements change frequently.&lt;/p&gt;&lt;p&gt;As Kayala noted, &amp;quot;This unlocks a completely different approach to business agility and competitive advantage.&amp;quot;&lt;/p&gt;&lt;p&gt;The implications extend far beyond ServiceNow implementations. If AI agents can master the intricacies of enterprise software deployment—one of the most complex and relationship-dependent areas of professional services — few knowledge work domains may remain immune to automation.&lt;/p&gt;&lt;p&gt;The question isn&amp;#x27;t whether AI will transform professional services, but how quickly human expertise can be converted into autonomous digital workers that never sleep, never leave for competitors, and get smarter with every project they complete.&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://www.echelonai.com/"&gt;&lt;u&gt;Echelon&lt;/u&gt;&lt;/a&gt;, an artificial intelligence startup that automates enterprise software implementations, emerged from stealth mode today with $4.75 million in seed funding led by &lt;a href="https://baincapitalventures.com/"&gt;&lt;u&gt;Bain Capital Ventures&lt;/u&gt;&lt;/a&gt;, targeting a fundamental shift in how companies deploy and maintain critical business systems.&lt;/p&gt;&lt;p&gt;The San Francisco-based company has developed AI agents specifically trained to handle end-to-end &lt;a href="https://www.servicenow.com/"&gt;&lt;u&gt;ServiceNow&lt;/u&gt;&lt;/a&gt; implementations — complex enterprise software deployments that traditionally require months of work by offshore consulting teams and cost companies millions of dollars annually.&lt;/p&gt;&lt;p&gt;&amp;quot;The biggest barrier to digital transformation isn&amp;#x27;t technology — it&amp;#x27;s the time it takes to implement it,&amp;quot; said Rahul Kayala, Echelon&amp;#x27;s founder and CEO, who previously worked at AI-powered IT company &lt;a href="https://www.moveworks.com/"&gt;&lt;u&gt;Moveworks&lt;/u&gt;&lt;/a&gt;. &amp;quot;AI agents are eliminating that constraint entirely, allowing enterprises to experiment, iterate, and deploy platform changes with unprecedented speed.&amp;quot;&lt;/p&gt;&lt;p&gt;The announcement signals a potential disruption to the&lt;a href="https://www.ciodive.com/news/global-ai-spending-trillions-cloud-infrastructure-software-gartner/760303/"&gt;&lt;u&gt; $1.5 trillion global IT services market&lt;/u&gt;&lt;/a&gt;, where companies like &lt;a href="https://www.accenture.com/"&gt;&lt;u&gt;Accenture&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.deloitte.com/us/en.html"&gt;&lt;u&gt;Deloitte&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://www.capgemini.com/us-en/"&gt;&lt;u&gt;Capgemini&lt;/u&gt;&lt;/a&gt; have long dominated through labor-intensive consulting models that Echelon argues are becoming obsolete in the age of artificial intelligence.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why ServiceNow deployments take months and cost millions&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://www.servicenow.com/"&gt;&lt;u&gt;ServiceNow&lt;/u&gt;&lt;/a&gt;, a cloud-based platform used by enterprises to manage IT services, human resources, and business workflows, has become critical infrastructure for large organizations. However, implementing and customizing the platform typically requires specialized expertise that most companies lack internally.&lt;/p&gt;&lt;p&gt;The complexity stems from ServiceNow&amp;#x27;s vast customization capabilities. Organizations often need hundreds of &amp;quot;&lt;a href="https://www.echelonai.com/blog/service-catalog-migration"&gt;&lt;u&gt;catalog items&lt;/u&gt;&lt;/a&gt;&amp;quot; — digital forms and workflows for employee requests — each requiring specific configurations, approval processes, and integrations with existing systems. According to Echelon&amp;#x27;s research, these implementations frequently stretch far beyond planned timelines due to technical complexity and communication bottlenecks between business stakeholders and development teams.&lt;/p&gt;&lt;p&gt;&amp;quot;What starts out simple often turns into weeks of effort once the actual work begins,&amp;quot; the company noted in its &lt;a href="https://www.echelonai.com/blog/service-catalog-migration"&gt;&lt;u&gt;analysis of common implementation challenges&lt;/u&gt;&lt;/a&gt;. &amp;quot;A basic request form turns out to be five requests stuffed into one. We had catalog items with 50+ variables, 10 or more UI policies, all connected. Update one field, and something else would break.&amp;quot;&lt;/p&gt;&lt;p&gt;The traditional solution involves hiring offshore development teams or expensive consultants, creating what Echelon describes as a problematic cycle: &amp;quot;One question here, one delay there, and suddenly you&amp;#x27;re weeks behind.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How AI agents replace expensive offshore consulting teams&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Echelon&amp;#x27;s approach replaces human consultants with AI agents trained by elite &lt;a href="https://www.servicenow.com/"&gt;&lt;u&gt;ServiceNow&lt;/u&gt;&lt;/a&gt; experts from top consulting firms. These agents can analyze business requirements, ask clarifying questions in real-time, and automatically generate complete ServiceNow configurations including forms, workflows, testing scenarios, and documentation.&lt;/p&gt;&lt;p&gt;The technology delivers a significant advancement from general-purpose AI tools. Rather than providing generic code suggestions, Echelon&amp;#x27;s agents understand ServiceNow&amp;#x27;s specific architecture, best practices, and common integration patterns. They can identify gaps in requirements and propose solutions that align with enterprise governance standards.&lt;/p&gt;&lt;p&gt;&amp;quot;Instead of routing every piece of input through five people, the business process owner directly uploaded their requirements,&amp;quot; Kayala explained, describing a recent customer implementation. &amp;quot;The AI developer analyzes it and asks follow-up questions like: &amp;#x27;I see a process flow with 3 branches, but only 2 triggers. Should there be a 3rd?&amp;#x27; The kinds of things a seasoned developer would ask. With AI, these questions came instantly.&amp;quot;&lt;/p&gt;&lt;p&gt;Early customers report dramatic time savings. One financial services company saw a service catalog migration project that was projected to take six months &lt;a href="https://www.echelonai.com/"&gt;&lt;u&gt;completed in six weeks&lt;/u&gt;&lt;/a&gt; using Echelon&amp;#x27;s AI agents.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What makes Echelon&amp;#x27;s AI different from coding assistants&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Echelon&amp;#x27;s technology addresses several technical challenges that have prevented broader AI adoption in enterprise software implementation. The agents are trained not just on ServiceNow&amp;#x27;s technical capabilities but on the accumulated expertise of senior consultants who understand complex enterprise requirements, governance frameworks, and integration patterns.&lt;/p&gt;&lt;p&gt;This approach differs from general-purpose AI coding assistants like &lt;a href="https://github.com/features/copilot"&gt;&lt;u&gt;GitHub Copilot&lt;/u&gt;&lt;/a&gt;, which provide syntax suggestions but lack domain-specific expertise. Echelon&amp;#x27;s agents understand ServiceNow&amp;#x27;s data models, security frameworks, and upgrade considerations—knowledge typically acquired through years of consulting experience.&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s training methodology involves elite ServiceNow experts from consulting firms like &lt;a href="https://www.accenture.com/"&gt;&lt;u&gt;Accenture&lt;/u&gt;&lt;/a&gt; and specialized ServiceNow partner &lt;a href="https://www.thirdera.com/"&gt;&lt;u&gt;Thirdera&lt;/u&gt;&lt;/a&gt;. This embedded expertise enables the AI to handle complex requirements and edge cases that typically require senior consultant intervention.&lt;/p&gt;&lt;p&gt;The real challenge isn&amp;#x27;t teaching AI to write code — it&amp;#x27;s capturing the intuitive expertise that separates junior developers from seasoned architects. Senior ServiceNow consultants instinctively know which customizations will break during upgrades and how simple requests spiral into complex integration problems. This institutional knowledge creates a far more defensible moat than general-purpose coding assistants can offer.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The $1.5 trillion consulting market faces disruption&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Echelon&amp;#x27;s emergence reflects broader trends reshaping the enterprise software market. As companies accelerate digital transformation initiatives, the traditional consulting model increasingly appears inadequate for the speed and scale required.&lt;/p&gt;&lt;p&gt;ServiceNow itself has grown rapidly, reporting over &lt;a href="https://stockanalysis.com/stocks/now/revenue/"&gt;&lt;u&gt;$10.98 billion in annual revenue in 2024&lt;/u&gt;&lt;/a&gt;, and $12.06 billion for the trailing twelve months ending June 30, 2025, as organizations continue to digitize more business processes. However, this growth has created a persistent talent shortage, with demand for skilled ServiceNow professionals — particularly those with AI expertise — significantly outpacing supply.&lt;/p&gt;&lt;p&gt;The startup&amp;#x27;s approach could fundamentally alter the economics of enterprise software implementation. Traditional consulting engagements often involve large teams working for months, with costs scaling linearly with project complexity. AI agents, by contrast, can handle multiple projects simultaneously and apply learned knowledge across customers.&lt;/p&gt;&lt;p&gt;Rak Garg, the Bain Capital Ventures partner who led Echelon&amp;#x27;s funding round, sees this as part of a larger shift toward AI-powered professional services. &amp;quot;We see the same trend with other BCV companies like &lt;a href="https://www.prophetsecurity.ai/"&gt;&lt;u&gt;Prophet Security&lt;/u&gt;&lt;/a&gt;, which automates security operations, and &lt;a href="https://baincapitalventures.com/insight/crosby-is-redefining-legal-work-with-ai-powered-contract-automation/"&gt;&lt;u&gt;Crosby&lt;/u&gt;&lt;/a&gt;, which automates legal services for startups. AI is quickly becoming the delivery layer across multiple functions.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Scaling beyond ServiceNow while maintaining enterprise reliability&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Despite early success, Echelon faces significant challenges in scaling its approach. Enterprise customers prioritize reliability above speed, and any AI-generated configurations must meet strict security and compliance requirements.&lt;/p&gt;&lt;p&gt;&amp;quot;Inertia is the biggest risk,&amp;quot; Garg acknowledged. &amp;quot;IT systems shouldn&amp;#x27;t ever go down, and companies lose thousands of man-hours of productivity with every outage. Proving reliability at scale, and building on repeatable results will be critical for Echelon.&amp;quot;&lt;/p&gt;&lt;p&gt;The company plans to expand beyond ServiceNow to other enterprise platforms including &lt;a href="https://www.sap.com/index.html"&gt;&lt;u&gt;SAP&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.salesforce.com/"&gt;&lt;u&gt;Salesforce&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://www.workday.com/en-us/homepage.html"&gt;&lt;u&gt;Workday&lt;/u&gt;&lt;/a&gt; — each creating substantial additional market opportunities. However, each platform requires developing new domain expertise and training models on platform-specific best practices.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.echelonai.com/"&gt;&lt;u&gt;Echelon&lt;/u&gt;&lt;/a&gt; also faces potential competition from established consulting firms that are developing their own AI capabilities. However, Garg views these firms as potential partners rather than competitors, noting that many have already approached Echelon about collaboration opportunities.&lt;/p&gt;&lt;p&gt;&amp;quot;They know that AI is shifting their business model in real-time,&amp;quot; he said. &amp;quot;Customers are placing immense pricing pressure on larger firms and asking hard questions, and these firms can use Echelon agents to accelerate their projects.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How AI agents could reshape all professional services&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Echelon&amp;#x27;s funding and emergence from stealth marks a significant milestone in the application of AI to professional services. Unlike consumer AI applications that primarily enhance individual productivity, enterprise AI agents like Echelon&amp;#x27;s directly replace skilled labor at scale.&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s approach — training AI systems on expert knowledge rather than just technical documentation — could serve as a model for automating other complex professional services. Legal research, financial analysis, and technical consulting all involve similar patterns of applying specialized expertise to unique customer requirements.&lt;/p&gt;&lt;p&gt;For enterprise customers, the promise extends beyond cost savings to strategic agility. Organizations that can rapidly implement and modify business processes gain competitive advantages in markets where customer expectations and regulatory requirements change frequently.&lt;/p&gt;&lt;p&gt;As Kayala noted, &amp;quot;This unlocks a completely different approach to business agility and competitive advantage.&amp;quot;&lt;/p&gt;&lt;p&gt;The implications extend far beyond ServiceNow implementations. If AI agents can master the intricacies of enterprise software deployment—one of the most complex and relationship-dependent areas of professional services — few knowledge work domains may remain immune to automation.&lt;/p&gt;&lt;p&gt;The question isn&amp;#x27;t whether AI will transform professional services, but how quickly human expertise can be converted into autonomous digital workers that never sleep, never leave for competitors, and get smarter with every project they complete.&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/echelons-ai-agents-take-aim-at-accenture-and-deloitte-consulting-models</guid><pubDate>Thu, 09 Oct 2025 15:00:00 +0000</pubDate></item><item><title>This distributed data storage startup wants to take on Big Cloud (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/09/this-distributed-data-storage-startup-wants-to-take-on-big-cloud/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The explosion of AI companies has pushed demand for computing power to new extremes, and companies like CoreWeave, Together AI and&amp;nbsp;Lambda Labs have capitalized on that demand, attracting immense amounts of attention and capital for their ability to offer distributed compute&amp;nbsp;capacity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But most companies still store data with the big three cloud providers, AWS, Google Cloud, and Microsoft Azure, whose storage systems were built to keep data close to their own compute resources, not spread across multiple clouds or regions.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Modern&amp;nbsp;AI workloads and AI infrastructure are choosing distributed computing instead of big&amp;nbsp;cloud,” Ovais Tariq, co-founder and CEO of Tigris Data, told TechCrunch. “We want to&amp;nbsp;provide&amp;nbsp;the same option for storage, because without storage, compute is nothing.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tigris, founded by the team that developed Uber’s storage platform, is&amp;nbsp;building a network of localized data storage centers that it claims can meet the distributed&amp;nbsp;compute needs of modern AI workloads. The startup’s AI-native storage platform&amp;nbsp;“moves with your compute, [allows] data [to] automatically replicate&amp;nbsp;to where GPUs are, supports billions of small files, and provides low-latency access for training, inference, and agentic workloads,” Tariq said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To do all of that, Tigris&amp;nbsp;recently raised a $25 million Series A round that was led by Spark Capital and saw participation from existing investors, which include Andreessen Horowitz,&amp;nbsp;TechCrunch has exclusively learned.&amp;nbsp;The startup is going against the incumbents, who Tariq calls “Big Cloud.” &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3055939" height="512" src="https://techcrunch.com/wp-content/uploads/2025/10/Tigris-Datacenter.png?w=384" width="384" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Ovais Tariq, CEO of Tigris, at a Tigris data center in Virginia&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Tigris Data&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Tariq feels these incumbents not only&amp;nbsp;offer&amp;nbsp;a more expensive data storage service, but&amp;nbsp;also a less efficient one.&amp;nbsp;AWS, Google Cloud, and Microsoft Azure have historically charged egress fees (dubbed “cloud tax” in the industry) if a customer wants to migrate to another cloud provider, or download and move their data if they want to, say, use a cheaper GPU or train models in different parts of the world simultaneously.&amp;nbsp;Think of it like having to pay your gym extra if you want to stop going there.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Batuhan Taskaya, head of engineering at Fal.ai,&amp;nbsp;one of Tigris’ customers, those&amp;nbsp;costs once accounted for the majority of Fal’s&amp;nbsp;cloud spending.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond egress fees,&amp;nbsp;Tariq says&amp;nbsp;there’s&amp;nbsp;still the problem of latency with larger cloud providers. “Egress fees were just one symptom of a deeper problem: centralized storage that&amp;nbsp;can’t&amp;nbsp;keep up with a decentralized, high-speed AI&amp;nbsp;ecosystem,” he said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Most of Tigris’ 4,000+ customers are like Fal.ai: generative AI startups building&amp;nbsp;image, video, and voice models, which tend to have large, latency-sensitive datasets.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Imagine talking to an AI agent that’s doing local audio,” Tariq said. “You want the lowest latency. You want your compute to be local, close by, and you want your storage to be local, too.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Big clouds&amp;nbsp;aren’t&amp;nbsp;optimized&amp;nbsp;for AI workloads, he&amp;nbsp;added.&amp;nbsp;Streaming massive datasets for training or running real-time inference across multiple regions can create latency bottlenecks, slowing model performance.&amp;nbsp;But being able to access localized storage means data&amp;nbsp;is retrieved faster, which means developers can run AI workloads reliably and more cost-effectively using decentralized clouds.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Tigris lets us scale our workloads in any cloud by providing access to the same data filesystem from all these places without charging egress,”&amp;nbsp;Fal’s Taskaya&amp;nbsp;said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There are other reasons why companies want to have data closer to their distributed cloud options. For example, in highly regulated fields like finance and healthcare, one large roadblock to adopting AI tools is that enterprises need to ensure data security. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another motivation, says Tariq, is that companies increasingly want to own their data, pointing to&amp;nbsp;how Salesforce&amp;nbsp;earlier this year blocked its AI rivals from using Slack data.&amp;nbsp;“Companies are becoming&amp;nbsp;more and more&amp;nbsp;aware of how important the data is, how&amp;nbsp;it’s&amp;nbsp;fueling&amp;nbsp;the&amp;nbsp;LLMs, how it’s fueling the AI,” Tariq said. “They want to be more in control. They&amp;nbsp;don’t&amp;nbsp;want someone else to be in control of it.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With the fresh funds, Tigris intends to continue building its data storage centers to support increasing demand — Tariq says the startup has grown 8x every year since its founding in&amp;nbsp;November 2021. Tigris&amp;nbsp;already has three&amp;nbsp;data centers&amp;nbsp;in Virginia, Chicago, and San Jose,&amp;nbsp;and wants to continue expanding in the U.S. as well as in Europe and Asia, specifically in London, Frankfurt, and Singapore.&amp;nbsp;&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The explosion of AI companies has pushed demand for computing power to new extremes, and companies like CoreWeave, Together AI and&amp;nbsp;Lambda Labs have capitalized on that demand, attracting immense amounts of attention and capital for their ability to offer distributed compute&amp;nbsp;capacity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But most companies still store data with the big three cloud providers, AWS, Google Cloud, and Microsoft Azure, whose storage systems were built to keep data close to their own compute resources, not spread across multiple clouds or regions.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Modern&amp;nbsp;AI workloads and AI infrastructure are choosing distributed computing instead of big&amp;nbsp;cloud,” Ovais Tariq, co-founder and CEO of Tigris Data, told TechCrunch. “We want to&amp;nbsp;provide&amp;nbsp;the same option for storage, because without storage, compute is nothing.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tigris, founded by the team that developed Uber’s storage platform, is&amp;nbsp;building a network of localized data storage centers that it claims can meet the distributed&amp;nbsp;compute needs of modern AI workloads. The startup’s AI-native storage platform&amp;nbsp;“moves with your compute, [allows] data [to] automatically replicate&amp;nbsp;to where GPUs are, supports billions of small files, and provides low-latency access for training, inference, and agentic workloads,” Tariq said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To do all of that, Tigris&amp;nbsp;recently raised a $25 million Series A round that was led by Spark Capital and saw participation from existing investors, which include Andreessen Horowitz,&amp;nbsp;TechCrunch has exclusively learned.&amp;nbsp;The startup is going against the incumbents, who Tariq calls “Big Cloud.” &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3055939" height="512" src="https://techcrunch.com/wp-content/uploads/2025/10/Tigris-Datacenter.png?w=384" width="384" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Ovais Tariq, CEO of Tigris, at a Tigris data center in Virginia&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Tigris Data&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Tariq feels these incumbents not only&amp;nbsp;offer&amp;nbsp;a more expensive data storage service, but&amp;nbsp;also a less efficient one.&amp;nbsp;AWS, Google Cloud, and Microsoft Azure have historically charged egress fees (dubbed “cloud tax” in the industry) if a customer wants to migrate to another cloud provider, or download and move their data if they want to, say, use a cheaper GPU or train models in different parts of the world simultaneously.&amp;nbsp;Think of it like having to pay your gym extra if you want to stop going there.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Batuhan Taskaya, head of engineering at Fal.ai,&amp;nbsp;one of Tigris’ customers, those&amp;nbsp;costs once accounted for the majority of Fal’s&amp;nbsp;cloud spending.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond egress fees,&amp;nbsp;Tariq says&amp;nbsp;there’s&amp;nbsp;still the problem of latency with larger cloud providers. “Egress fees were just one symptom of a deeper problem: centralized storage that&amp;nbsp;can’t&amp;nbsp;keep up with a decentralized, high-speed AI&amp;nbsp;ecosystem,” he said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Most of Tigris’ 4,000+ customers are like Fal.ai: generative AI startups building&amp;nbsp;image, video, and voice models, which tend to have large, latency-sensitive datasets.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Imagine talking to an AI agent that’s doing local audio,” Tariq said. “You want the lowest latency. You want your compute to be local, close by, and you want your storage to be local, too.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Big clouds&amp;nbsp;aren’t&amp;nbsp;optimized&amp;nbsp;for AI workloads, he&amp;nbsp;added.&amp;nbsp;Streaming massive datasets for training or running real-time inference across multiple regions can create latency bottlenecks, slowing model performance.&amp;nbsp;But being able to access localized storage means data&amp;nbsp;is retrieved faster, which means developers can run AI workloads reliably and more cost-effectively using decentralized clouds.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Tigris lets us scale our workloads in any cloud by providing access to the same data filesystem from all these places without charging egress,”&amp;nbsp;Fal’s Taskaya&amp;nbsp;said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There are other reasons why companies want to have data closer to their distributed cloud options. For example, in highly regulated fields like finance and healthcare, one large roadblock to adopting AI tools is that enterprises need to ensure data security. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another motivation, says Tariq, is that companies increasingly want to own their data, pointing to&amp;nbsp;how Salesforce&amp;nbsp;earlier this year blocked its AI rivals from using Slack data.&amp;nbsp;“Companies are becoming&amp;nbsp;more and more&amp;nbsp;aware of how important the data is, how&amp;nbsp;it’s&amp;nbsp;fueling&amp;nbsp;the&amp;nbsp;LLMs, how it’s fueling the AI,” Tariq said. “They want to be more in control. They&amp;nbsp;don’t&amp;nbsp;want someone else to be in control of it.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With the fresh funds, Tigris intends to continue building its data storage centers to support increasing demand — Tariq says the startup has grown 8x every year since its founding in&amp;nbsp;November 2021. Tigris&amp;nbsp;already has three&amp;nbsp;data centers&amp;nbsp;in Virginia, Chicago, and San Jose,&amp;nbsp;and wants to continue expanding in the U.S. as well as in Europe and Asia, specifically in London, Frankfurt, and Singapore.&amp;nbsp;&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/09/this-distributed-data-storage-startup-wants-to-take-on-big-cloud/</guid><pubDate>Thu, 09 Oct 2025 15:00:00 +0000</pubDate></item><item><title>Sora hit 1M downloads faster than ChatGPT (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/09/sora-hit-1m-downloads-faster-than-chatgpt/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After OpenAI’s video-generating app Sora surged to the No. 1 position on the U.S. App Store, it has now, technically, experienced a bigger first week than ChatGPT on iOS, according to new data from app intelligence provider Appfigures. Its estimates show that Sora saw 627,000 iOS downloads in its first seven days of availability, compared with ChatGPT’s 606,000 iOS downloads during its first week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Shortly after the publication of this article, OpenAI’s head of Sora, Bill Peebles, announced that Sora reached a million downloads in under five days. He said that it was faster than ChatGPT, despite Sora being in invite-only mode. (Sora is iOS-only for now.) &lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;sora hit 1M app downloads in &amp;lt;5 days, even faster than chatgpt did (despite the invite flow and only targeting north america!)!&lt;/p&gt;&lt;p&gt;team working hard to keep up with surging growth. more features and fixes to overmoderation on the way!&lt;/p&gt;— Bill Peebles (@billpeeb) October 9, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;ChatGPT was available only in the U.S. during its first week, while Sora is currently offered in the U.S. and Canada at launch. Still, Appfigures data indicated that Canada contributed about 45,000 installs, so the Sora launch was about 96% of ChatGPT’s launch on iOS alone, if the data had been based on the U.S. numbers only.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This level of consumer adoption is worth noting because Sora requires an invite to get in, while ChatGPT was publicly available at launch. That makes Sora’s performance more impressive.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During its first day, Sora saw 56,000 iOS app installs in short order, bumping the app to become the No. 3 Top Overall app on the U.S. App Store, Appfigures said. By Friday, October 3, it reached No. 1. That surge had already put Sora’s debut ahead of other major AI app launches, including Anthropic’s Claude and Microsoft’s Copilot, and put it on par with xAI’s Grok launch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A quick scan of social media provides plenty of anecdotes that support Appfigures’ data. Sora videos, which uses the new Sora 2 video model and gives users the ability to generate realistic deepfakes, seem to be everywhere. Users are even creating deepfakes of dead people, a use case that has prompted Zelda Williams, daughter of the late actor Robin Williams, to ask folks to stop sending her AI-generated images of her father.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="an Appfigures graph showing Sora's growth over the past week between Sept. 30 and Oct 6, showing daily app store downloads surpassing ChatGPT." class="wp-image-3055495" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/thumbnail_sora-first-week-downloads-1.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Appfigures&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Per Appfigures, the app has seen steady adoption since its first day on the market, September 30, 2025. Its data indicates that daily downloads on iOS hit a high mark of 107,800 downloads on October 1, 2025. It has since seen between lows of 84,400 daily installs (on October 6) and 98,500 daily installs (on October 4).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While that’s not quite as high as earlier in the week, it’s still decent numbers for an app that not everyone can yet use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This post was updated after original publication on October 8 to include new information shared by OpenAI’s Bill Peebles; we clarified that Sora is still iOS-only. &lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After OpenAI’s video-generating app Sora surged to the No. 1 position on the U.S. App Store, it has now, technically, experienced a bigger first week than ChatGPT on iOS, according to new data from app intelligence provider Appfigures. Its estimates show that Sora saw 627,000 iOS downloads in its first seven days of availability, compared with ChatGPT’s 606,000 iOS downloads during its first week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Shortly after the publication of this article, OpenAI’s head of Sora, Bill Peebles, announced that Sora reached a million downloads in under five days. He said that it was faster than ChatGPT, despite Sora being in invite-only mode. (Sora is iOS-only for now.) &lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;sora hit 1M app downloads in &amp;lt;5 days, even faster than chatgpt did (despite the invite flow and only targeting north america!)!&lt;/p&gt;&lt;p&gt;team working hard to keep up with surging growth. more features and fixes to overmoderation on the way!&lt;/p&gt;— Bill Peebles (@billpeeb) October 9, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;ChatGPT was available only in the U.S. during its first week, while Sora is currently offered in the U.S. and Canada at launch. Still, Appfigures data indicated that Canada contributed about 45,000 installs, so the Sora launch was about 96% of ChatGPT’s launch on iOS alone, if the data had been based on the U.S. numbers only.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This level of consumer adoption is worth noting because Sora requires an invite to get in, while ChatGPT was publicly available at launch. That makes Sora’s performance more impressive.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During its first day, Sora saw 56,000 iOS app installs in short order, bumping the app to become the No. 3 Top Overall app on the U.S. App Store, Appfigures said. By Friday, October 3, it reached No. 1. That surge had already put Sora’s debut ahead of other major AI app launches, including Anthropic’s Claude and Microsoft’s Copilot, and put it on par with xAI’s Grok launch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A quick scan of social media provides plenty of anecdotes that support Appfigures’ data. Sora videos, which uses the new Sora 2 video model and gives users the ability to generate realistic deepfakes, seem to be everywhere. Users are even creating deepfakes of dead people, a use case that has prompted Zelda Williams, daughter of the late actor Robin Williams, to ask folks to stop sending her AI-generated images of her father.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="an Appfigures graph showing Sora's growth over the past week between Sept. 30 and Oct 6, showing daily app store downloads surpassing ChatGPT." class="wp-image-3055495" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/thumbnail_sora-first-week-downloads-1.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Appfigures&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Per Appfigures, the app has seen steady adoption since its first day on the market, September 30, 2025. Its data indicates that daily downloads on iOS hit a high mark of 107,800 downloads on October 1, 2025. It has since seen between lows of 84,400 daily installs (on October 6) and 98,500 daily installs (on October 4).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While that’s not quite as high as earlier in the week, it’s still decent numbers for an app that not everyone can yet use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This post was updated after original publication on October 8 to include new information shared by OpenAI’s Bill Peebles; we clarified that Sora is still iOS-only. &lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/09/sora-hit-1m-downloads-faster-than-chatgpt/</guid><pubDate>Thu, 09 Oct 2025 15:17:31 +0000</pubDate></item><item><title>Microsoft Azure Unveils World’s First NVIDIA GB300 NVL72 Supercomputing Cluster for OpenAI (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/microsoft-azure-worlds-first-gb300-nvl72-supercomputing-cluster-openai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/msft-azure-gb300-1280x680-1.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Microsoft Azure today announced the new NDv6 GB300 VM series, delivering the industry’s first supercomputing-scale production cluster of NVIDIA GB300 NVL72 systems, purpose-built for OpenAI’s most demanding AI inference workloads.&lt;/p&gt;
&lt;p&gt;This supercomputer-scale cluster features over 4,600 NVIDIA Blackwell Ultra GPUs connected via the NVIDIA Quantum-X800 InfiniBand networking platform. Microsoft’s unique systems approach applied radical engineering to memory and networking to provide the massive scale of compute required to achieve high inference and training throughput for reasoning models and agentic AI systems.&lt;/p&gt;
&lt;p&gt;Today’s achievement is the result of years of deep partnership between NVIDIA and Microsoft purpose-building AI infrastructure for the world’s most demanding AI workloads and to deliver infrastructure for the next frontier of AI. It marks another leadership moment, ensuring that leading-edge AI drives innovation in the United States.&lt;/p&gt;
&lt;p&gt;“Delivering the industry’s first at-scale NVIDIA GB300 NVL72 production cluster for frontier AI is an achievement that goes beyond powerful silicon — it reflects Microsoft Azure and NVIDIA’s shared commitment to optimize all parts of the modern AI data center,” said Nidhi Chappell, corporate vice president of Microsoft Azure AI Infrastructure.&lt;/p&gt;
&lt;p&gt;“Our collaboration helps ensure customers like OpenAI can deploy next-generation infrastructure at unprecedented scale and speed.”&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Inside the Engine: The NVIDIA GB300 NVL72&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;At the heart of Azure’s new NDv6 GB300 VM series is the liquid-cooled, rack-scale NVIDIA GB300 NVL72 system. Each rack is a powerhouse, integrating 72 NVIDIA Blackwell Ultra GPUs and 36 NVIDIA Grace CPUs into a single, cohesive unit to accelerate training and inference for massive AI models.&lt;/p&gt;
&lt;p&gt;The system provides a staggering 37 terabytes of fast memory and 1.44 exaflops of FP4 Tensor Core performance per VM, creating a massive, unified memory space essential for reasoning models, agentic AI systems and complex multimodal generative AI.&lt;/p&gt;
&lt;p&gt;NVIDIA Blackwell Ultra is supported by the full-stack NVIDIA AI platform, including collective communication libraries that tap into new formats like NVFP4 for breakthrough training performance, as well as compiler technologies like NVIDIA Dynamo for the highest inference performance in reasoning AI.&lt;/p&gt;
&lt;p&gt;The NVIDIA Blackwell Ultra platform excels at both training and inference. In the recent MLPerf Inference v5.1 benchmarks, NVIDIA GB300 NVL72 systems delivered record-setting performance using NVFP4. Results included up to 5x higher throughput per GPU on the 671-billion-parameter DeepSeek-R1 reasoning model compared with the NVIDIA Hopper architecture, along with leadership performance on all newly introduced benchmarks like the Llama 3.1 405B model.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;The Fabric of a Supercomputer: NVLink Switch and NVIDIA Quantum-X800 InfiniBand&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;To connect over 4,600 Blackwell Ultra GPUs into a single, cohesive supercomputer, Microsoft Azure’s cluster relies on a two-tiered NVIDIA networking architecture designed for both scale-up performance within the rack and scale-out performance across the entire cluster.&lt;/p&gt;
&lt;p&gt;Within each GB300 NVL72 rack, the fifth-generation NVIDIA NVLink Switch fabric provides 130 TB/s of direct, all-to-all bandwidth between the 72 Blackwell Ultra GPUs. This transforms the entire rack into a single, unified accelerator with a shared memory pool — a critical design for massive, memory-intensive models.&lt;/p&gt;
&lt;p&gt;To scale beyond the rack, the cluster uses the NVIDIA Quantum-X800 InfiniBand platform, purpose-built for trillion-parameter-scale AI. Featuring NVIDIA ConnectX-8 SuperNICs and Quantum-X800 switches, NVIDIA Quantum-X800 provides 800 Gb/s of bandwidth per GPU, ensuring seamless communication across all 4,608 GPUs.&lt;/p&gt;
&lt;p&gt;Microsoft Azure’s cluster also uses NVIDIA Quantum-X800’s advanced adaptive routing, telemetry-based congestion control and performance isolation capabilities, as well as NVIDIA Scalable Hierarchical Aggregation and Reduction Protocol (SHARP) v4, which accelerates operations to significantly boost the efficiency of large-scale training and inference.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Driving the Future of AI&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Delivering the world’s first production NVIDIA GB300 NVL72 cluster at this scale required a reimagination of every layer of Microsoft’s data center — from custom liquid cooling and power distribution to a reengineered software stack for orchestration and storage.&lt;/p&gt;
&lt;p&gt;This latest milestone marks a big step forward in building the infrastructure that will unlock the future of AI. As Azure scales to its goal of deploying hundreds of thousands of NVIDIA Blackwell Ultra GPUs, even more innovations are poised to emerge from customers like OpenAI.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about this announcement on the &lt;/i&gt;&lt;i&gt;Microsoft Azure blog&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/msft-azure-gb300-1280x680-1.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Microsoft Azure today announced the new NDv6 GB300 VM series, delivering the industry’s first supercomputing-scale production cluster of NVIDIA GB300 NVL72 systems, purpose-built for OpenAI’s most demanding AI inference workloads.&lt;/p&gt;
&lt;p&gt;This supercomputer-scale cluster features over 4,600 NVIDIA Blackwell Ultra GPUs connected via the NVIDIA Quantum-X800 InfiniBand networking platform. Microsoft’s unique systems approach applied radical engineering to memory and networking to provide the massive scale of compute required to achieve high inference and training throughput for reasoning models and agentic AI systems.&lt;/p&gt;
&lt;p&gt;Today’s achievement is the result of years of deep partnership between NVIDIA and Microsoft purpose-building AI infrastructure for the world’s most demanding AI workloads and to deliver infrastructure for the next frontier of AI. It marks another leadership moment, ensuring that leading-edge AI drives innovation in the United States.&lt;/p&gt;
&lt;p&gt;“Delivering the industry’s first at-scale NVIDIA GB300 NVL72 production cluster for frontier AI is an achievement that goes beyond powerful silicon — it reflects Microsoft Azure and NVIDIA’s shared commitment to optimize all parts of the modern AI data center,” said Nidhi Chappell, corporate vice president of Microsoft Azure AI Infrastructure.&lt;/p&gt;
&lt;p&gt;“Our collaboration helps ensure customers like OpenAI can deploy next-generation infrastructure at unprecedented scale and speed.”&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Inside the Engine: The NVIDIA GB300 NVL72&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;At the heart of Azure’s new NDv6 GB300 VM series is the liquid-cooled, rack-scale NVIDIA GB300 NVL72 system. Each rack is a powerhouse, integrating 72 NVIDIA Blackwell Ultra GPUs and 36 NVIDIA Grace CPUs into a single, cohesive unit to accelerate training and inference for massive AI models.&lt;/p&gt;
&lt;p&gt;The system provides a staggering 37 terabytes of fast memory and 1.44 exaflops of FP4 Tensor Core performance per VM, creating a massive, unified memory space essential for reasoning models, agentic AI systems and complex multimodal generative AI.&lt;/p&gt;
&lt;p&gt;NVIDIA Blackwell Ultra is supported by the full-stack NVIDIA AI platform, including collective communication libraries that tap into new formats like NVFP4 for breakthrough training performance, as well as compiler technologies like NVIDIA Dynamo for the highest inference performance in reasoning AI.&lt;/p&gt;
&lt;p&gt;The NVIDIA Blackwell Ultra platform excels at both training and inference. In the recent MLPerf Inference v5.1 benchmarks, NVIDIA GB300 NVL72 systems delivered record-setting performance using NVFP4. Results included up to 5x higher throughput per GPU on the 671-billion-parameter DeepSeek-R1 reasoning model compared with the NVIDIA Hopper architecture, along with leadership performance on all newly introduced benchmarks like the Llama 3.1 405B model.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;The Fabric of a Supercomputer: NVLink Switch and NVIDIA Quantum-X800 InfiniBand&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;To connect over 4,600 Blackwell Ultra GPUs into a single, cohesive supercomputer, Microsoft Azure’s cluster relies on a two-tiered NVIDIA networking architecture designed for both scale-up performance within the rack and scale-out performance across the entire cluster.&lt;/p&gt;
&lt;p&gt;Within each GB300 NVL72 rack, the fifth-generation NVIDIA NVLink Switch fabric provides 130 TB/s of direct, all-to-all bandwidth between the 72 Blackwell Ultra GPUs. This transforms the entire rack into a single, unified accelerator with a shared memory pool — a critical design for massive, memory-intensive models.&lt;/p&gt;
&lt;p&gt;To scale beyond the rack, the cluster uses the NVIDIA Quantum-X800 InfiniBand platform, purpose-built for trillion-parameter-scale AI. Featuring NVIDIA ConnectX-8 SuperNICs and Quantum-X800 switches, NVIDIA Quantum-X800 provides 800 Gb/s of bandwidth per GPU, ensuring seamless communication across all 4,608 GPUs.&lt;/p&gt;
&lt;p&gt;Microsoft Azure’s cluster also uses NVIDIA Quantum-X800’s advanced adaptive routing, telemetry-based congestion control and performance isolation capabilities, as well as NVIDIA Scalable Hierarchical Aggregation and Reduction Protocol (SHARP) v4, which accelerates operations to significantly boost the efficiency of large-scale training and inference.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Driving the Future of AI&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Delivering the world’s first production NVIDIA GB300 NVL72 cluster at this scale required a reimagination of every layer of Microsoft’s data center — from custom liquid cooling and power distribution to a reengineered software stack for orchestration and storage.&lt;/p&gt;
&lt;p&gt;This latest milestone marks a big step forward in building the infrastructure that will unlock the future of AI. As Azure scales to its goal of deploying hundreds of thousands of NVIDIA Blackwell Ultra GPUs, even more innovations are poised to emerge from customers like OpenAI.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about this announcement on the &lt;/i&gt;&lt;i&gt;Microsoft Azure blog&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/microsoft-azure-worlds-first-gb300-nvl72-supercomputing-cluster-openai/</guid><pubDate>Thu, 09 Oct 2025 16:00:57 +0000</pubDate></item><item><title>Startup Battlefield company SpotitEarly trained dogs and AI to sniff out common cancers (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/09/startup-battlefield-company-spotitearly-trained-dogs-and-ai-to-sniff-out-common-cancers/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Developing comprehensive screening for early-stage cancer is the key to saving the most lives.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Although multi-cancer early detection (MCED) tests are still being researched and lack FDA approval, several are commercially available for consumers who are willing to pay out of pocket. Individuals can ask their doctor to order the Galleri blood test from Grail, or they can opt for a whole-body MRI from companies like Prenuvo or Ezra, which often costs $2,000 or more.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Soon, consumers will have another — and rather unique—way to screen for multiple types of cancer.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SpotitEarly, a biotech company that’s part of Startup Battlefield at TechCrunch Disrupt 2025, is developing an at-home cancer test that analyzes human breath using dogs’ exceptional sense of smell combined with AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SpotitEarly CEO Shlomi Madar told TechCrunch that the science is increasingly clear: Dogs can be trained to sniff out diseases, and especially cancer, in humans. “There are also ad hoc reports from people who mention that their companion dogs sensed that something was wrong with them way before they were diagnosed,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Drawing on his 15 years of experience as a health and biotech leader, Madar joined three friends — one of whom is a former K9 unit commander — to develop a reliable method and technology for cancer screening by analyzing breath samples.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users can screen for cancer simply by collecting an at-home breath sample and shipping it to SpotitEarly’s lab. The company employs 18 trained beagles to discern cancer-specific odors. The dogs are taught to sit if they smell cancer particles, and SpotitEarly’s AI platform validates the dogs’ behavior.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“We have cameras on top of the lab. We have a microphone that captures the dogs’ breathing patterns. We also monitor their heart rate. So basically, the machine learning knows the baseline of the entire dog pack,” Madar said. “That’s what makes it more accurate than just a handler looking at a dog.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company’s research, published in Nature’s Scientific Reports, showed that its trained dogs can detect early cancer in breath samples with 94% accuracy. That double-blind clinical study, involving 1,400 individuals, was focused on screening for the four most common cancers: breast, colorectal, prostate, and lung.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SpotitEarly, which was founded in Israel in 2020, announced in May its launch into the U.S. market with $20.3 million in funding from Hanaco VC, Menomedin VC, Jeff Swartz (former CEO of Timberland), and Avishai Abrahami (CEO of Wix.com.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company plans to use the capital to significantly expand its clinical studies, starting with individual tests for breast cancer before moving on to the other three targeted cancers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Madar said that SpotitEarly’s at-home screening kits should be available to consumers through a physicians’ network next year. A single cancer test will be priced at around $250, and screening for each additional cancer will cost a fraction of the first test, he said. To prioritize accessibility, the company aims to price its multi-cancer panel below the cost of competitors like Grail’s Galleri test, which typically runs for about $950.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As for the dogs, Madar said they are part of the team. All employees at the company must be “dog people,” Madar said.&amp;nbsp; “We don’t just use them as biosensors. They have plenty of room to play. They’re great sneakers, great workers, but also great companions.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;If you want to learn from SpotitEarly firsthand, and see dozens of&amp;nbsp;additional&amp;nbsp;pitches, attend valuable workshops, and make the connections that drive business results,&amp;nbsp;&lt;/em&gt;&lt;em&gt;head here to learn more about this year’s Disrupt&lt;/em&gt;&lt;em&gt;, taking place October 27 to 29 in San Francisco.&lt;/em&gt;&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025" class="wp-image-3048094" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/TC25_Disrupt_General_Article_No-Anniversary_Headers_1920x1080.png?w=680" width="680" /&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Developing comprehensive screening for early-stage cancer is the key to saving the most lives.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Although multi-cancer early detection (MCED) tests are still being researched and lack FDA approval, several are commercially available for consumers who are willing to pay out of pocket. Individuals can ask their doctor to order the Galleri blood test from Grail, or they can opt for a whole-body MRI from companies like Prenuvo or Ezra, which often costs $2,000 or more.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Soon, consumers will have another — and rather unique—way to screen for multiple types of cancer.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SpotitEarly, a biotech company that’s part of Startup Battlefield at TechCrunch Disrupt 2025, is developing an at-home cancer test that analyzes human breath using dogs’ exceptional sense of smell combined with AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SpotitEarly CEO Shlomi Madar told TechCrunch that the science is increasingly clear: Dogs can be trained to sniff out diseases, and especially cancer, in humans. “There are also ad hoc reports from people who mention that their companion dogs sensed that something was wrong with them way before they were diagnosed,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Drawing on his 15 years of experience as a health and biotech leader, Madar joined three friends — one of whom is a former K9 unit commander — to develop a reliable method and technology for cancer screening by analyzing breath samples.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users can screen for cancer simply by collecting an at-home breath sample and shipping it to SpotitEarly’s lab. The company employs 18 trained beagles to discern cancer-specific odors. The dogs are taught to sit if they smell cancer particles, and SpotitEarly’s AI platform validates the dogs’ behavior.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“We have cameras on top of the lab. We have a microphone that captures the dogs’ breathing patterns. We also monitor their heart rate. So basically, the machine learning knows the baseline of the entire dog pack,” Madar said. “That’s what makes it more accurate than just a handler looking at a dog.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company’s research, published in Nature’s Scientific Reports, showed that its trained dogs can detect early cancer in breath samples with 94% accuracy. That double-blind clinical study, involving 1,400 individuals, was focused on screening for the four most common cancers: breast, colorectal, prostate, and lung.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SpotitEarly, which was founded in Israel in 2020, announced in May its launch into the U.S. market with $20.3 million in funding from Hanaco VC, Menomedin VC, Jeff Swartz (former CEO of Timberland), and Avishai Abrahami (CEO of Wix.com.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company plans to use the capital to significantly expand its clinical studies, starting with individual tests for breast cancer before moving on to the other three targeted cancers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Madar said that SpotitEarly’s at-home screening kits should be available to consumers through a physicians’ network next year. A single cancer test will be priced at around $250, and screening for each additional cancer will cost a fraction of the first test, he said. To prioritize accessibility, the company aims to price its multi-cancer panel below the cost of competitors like Grail’s Galleri test, which typically runs for about $950.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As for the dogs, Madar said they are part of the team. All employees at the company must be “dog people,” Madar said.&amp;nbsp; “We don’t just use them as biosensors. They have plenty of room to play. They’re great sneakers, great workers, but also great companions.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;If you want to learn from SpotitEarly firsthand, and see dozens of&amp;nbsp;additional&amp;nbsp;pitches, attend valuable workshops, and make the connections that drive business results,&amp;nbsp;&lt;/em&gt;&lt;em&gt;head here to learn more about this year’s Disrupt&lt;/em&gt;&lt;em&gt;, taking place October 27 to 29 in San Francisco.&lt;/em&gt;&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025" class="wp-image-3048094" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/TC25_Disrupt_General_Article_No-Anniversary_Headers_1920x1080.png?w=680" width="680" /&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/09/startup-battlefield-company-spotitearly-trained-dogs-and-ai-to-sniff-out-common-cancers/</guid><pubDate>Thu, 09 Oct 2025 16:20:56 +0000</pubDate></item><item><title>Datacurve raises $15 million to take on Scale AI (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/09/datacurve-raises-15-million-to-take-on-scaleai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Chemistry_251008_Serena_Ge_0642-v2.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As AI companies mature, the fight for high-quality data has become one of the most competitive areas in the industry, launching companies like Mercor, Surge, and, most prominently, Alexandr Wang’s Scale AI. But now that Wang has moved on to run AI at Meta, many funders see an opening — and are willing to fund companies with compelling new strategies for collecting training data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Y Combinator graduate Datacurve is one such company, focusing on high-quality data for software development. On Thursday, the company announced a $15 million Series A round, led by Mark Goldberg at Chemistry with participation from employees at DeepMind, Vercel, Anthropic, and OpenAI. The Series A comes after a $2.7 million seed round, which drew investment from former Coinbase CTO Balaji Srinivasan.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Datacurve uses a “bounty hunter” system to attract skilled software engineers to complete the hardest-to-source datasets. The company pays for those contributions, distributing over $1 million in bounties so far.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But co-founder Serena Ge (pictured above with co-founder Charley Lee) says the biggest motivation isn’t financial. For high-value services like software development, the pay will always be far lower for data work than conventional employment — so the company’s most important edge is a positive user experience.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We treat this as a consumer product, not a data labeling operation,” Ge said. “We spend a lot of time thinking about: How can we optimize it so that the people we want are interested and get onto our platform?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s particularly important as the needs of post-training data grow more complex. While earlier models were trained on simple datasets, today’s AI products rely on complex RL environments, which need to be constructed through specific and strategic data collection. As the environments grow more sophisticated, the data requirements become both more intense for both quantity and quality — a factor that could give high-quality data collection companies like Datacurve an edge.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As an early-stage company, Datacurve is focused on software engineering, but Ge says the model could apply just as easily to fields like finance, marketing, or even medicine.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“What we’re doing right now is we’re creating an infrastructure for post-training data collection that attracts and retains highly competent people in their own domains,” Ge says.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Chemistry_251008_Serena_Ge_0642-v2.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As AI companies mature, the fight for high-quality data has become one of the most competitive areas in the industry, launching companies like Mercor, Surge, and, most prominently, Alexandr Wang’s Scale AI. But now that Wang has moved on to run AI at Meta, many funders see an opening — and are willing to fund companies with compelling new strategies for collecting training data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Y Combinator graduate Datacurve is one such company, focusing on high-quality data for software development. On Thursday, the company announced a $15 million Series A round, led by Mark Goldberg at Chemistry with participation from employees at DeepMind, Vercel, Anthropic, and OpenAI. The Series A comes after a $2.7 million seed round, which drew investment from former Coinbase CTO Balaji Srinivasan.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Datacurve uses a “bounty hunter” system to attract skilled software engineers to complete the hardest-to-source datasets. The company pays for those contributions, distributing over $1 million in bounties so far.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But co-founder Serena Ge (pictured above with co-founder Charley Lee) says the biggest motivation isn’t financial. For high-value services like software development, the pay will always be far lower for data work than conventional employment — so the company’s most important edge is a positive user experience.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We treat this as a consumer product, not a data labeling operation,” Ge said. “We spend a lot of time thinking about: How can we optimize it so that the people we want are interested and get onto our platform?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s particularly important as the needs of post-training data grow more complex. While earlier models were trained on simple datasets, today’s AI products rely on complex RL environments, which need to be constructed through specific and strategic data collection. As the environments grow more sophisticated, the data requirements become both more intense for both quantity and quality — a factor that could give high-quality data collection companies like Datacurve an edge.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As an early-stage company, Datacurve is focused on software engineering, but Ge says the model could apply just as easily to fields like finance, marketing, or even medicine.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“What we’re doing right now is we’re creating an infrastructure for post-training data collection that attracts and retains highly competent people in their own domains,” Ge says.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/09/datacurve-raises-15-million-to-take-on-scaleai/</guid><pubDate>Thu, 09 Oct 2025 16:31:06 +0000</pubDate></item><item><title>Figma partners with Google to add Gemini AI to its design platform (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/09/figma-partners-with-google-to-add-gemini-ai-to-its-design-platform/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/AI-Feature-Highlight.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Design platform Figma is partnering with Google to add more AI features, the company announced on Thursday. While Figma had previously introduced AI app-building tools of its own, the new integration with Google will bring several Gemini models to the design software to address what Figma says are the “evolving needs” of product designers and their teams.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With the partnership, Gemini 2.5 Flash, Gemini 2.0, and Imagen 4 will be added to Figma’s toolset, while the company maintains its relationship with Google Cloud. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Gemini 2.5 Flash will be integrated into the process for editing images and Figma’s image generation capabilities, allowing the software’s 13 million monthly active users to make AI images with a prompt and request changes. The company believes the addition will speed up workflows, citing earlier tests of Gemini 2.5 Flash in its product, where users experienced a 50% reduction in latency for the company’s “Make Image” feature. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The partnership is now one of several from top AI makers, intent on integrating their models within existing apps with large user bases as they seek to establish dominance in a tight race for consumer adoption. This week, for instance, OpenAI announced that its users could “chat” with apps inside ChatGPT, including those from Spotify, Booking.com, Expedia, Coursera, Zillow, Canva, and others. Figma was also in that list, indicating the Google Gemini deal is not an exclusive one.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The news of the Figma deal arrives alongside Google’s announcement of Gemini Enterprise, an AI-powered conversational platform designed to bring AI to enterprise customers within their existing workflows. That means users would be able to chat with their company’s documents, data, and applications, and engineers would have access to tools to build and deploy AI agents or use a suite of existing agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google positioned the news as a win for AI’s potential to increase efficiency and improve workflows — something companies would presumably pay for as they come to rely on the integrations. That’s a strategic move at a time when consumers are driving AI profits, while GenAI pilots at companies often fail.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To counter this, Google noted that 65% of Google Cloud customers are using its AI products. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside the Figma deal, Google also announced AI deals with GAP, Gordon Foods, Klarna, Macquarie Bank, Melexis, Mercedes, Signal Iduna, Valiuz, and Virgin Voyages. These join existing partners using Gemini, like Banco BV, Behr, Box, DBS Bank, Deloitte, Deutsche Telekom, FairPrice Group, the U.S. Department of Energy, and others.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/AI-Feature-Highlight.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Design platform Figma is partnering with Google to add more AI features, the company announced on Thursday. While Figma had previously introduced AI app-building tools of its own, the new integration with Google will bring several Gemini models to the design software to address what Figma says are the “evolving needs” of product designers and their teams.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With the partnership, Gemini 2.5 Flash, Gemini 2.0, and Imagen 4 will be added to Figma’s toolset, while the company maintains its relationship with Google Cloud. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Gemini 2.5 Flash will be integrated into the process for editing images and Figma’s image generation capabilities, allowing the software’s 13 million monthly active users to make AI images with a prompt and request changes. The company believes the addition will speed up workflows, citing earlier tests of Gemini 2.5 Flash in its product, where users experienced a 50% reduction in latency for the company’s “Make Image” feature. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The partnership is now one of several from top AI makers, intent on integrating their models within existing apps with large user bases as they seek to establish dominance in a tight race for consumer adoption. This week, for instance, OpenAI announced that its users could “chat” with apps inside ChatGPT, including those from Spotify, Booking.com, Expedia, Coursera, Zillow, Canva, and others. Figma was also in that list, indicating the Google Gemini deal is not an exclusive one.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The news of the Figma deal arrives alongside Google’s announcement of Gemini Enterprise, an AI-powered conversational platform designed to bring AI to enterprise customers within their existing workflows. That means users would be able to chat with their company’s documents, data, and applications, and engineers would have access to tools to build and deploy AI agents or use a suite of existing agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google positioned the news as a win for AI’s potential to increase efficiency and improve workflows — something companies would presumably pay for as they come to rely on the integrations. That’s a strategic move at a time when consumers are driving AI profits, while GenAI pilots at companies often fail.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To counter this, Google noted that 65% of Google Cloud customers are using its AI products. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside the Figma deal, Google also announced AI deals with GAP, Gordon Foods, Klarna, Macquarie Bank, Melexis, Mercedes, Signal Iduna, Valiuz, and Virgin Voyages. These join existing partners using Gemini, like Banco BV, Behr, Box, DBS Bank, Deloitte, Deutsche Telekom, FairPrice Group, the U.S. Department of Energy, and others.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/09/figma-partners-with-google-to-add-gemini-ai-to-its-design-platform/</guid><pubDate>Thu, 09 Oct 2025 16:34:35 +0000</pubDate></item><item><title>Intel unveils new processor powered by its 18A semiconductor tech (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/09/intel-unveils-new-processor-powered-by-its-18a-semiconductor-tech/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/03/GettyImages-2205047441.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Six months after Lip-Bu Tan began his quest to turn around struggling Intel, the semiconductor giant has announced a major hardware upgrade.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Thursday, Intel unveiled a new processor, codenamed Panther Lake. This marks the next generation of the company’s Intel Core Ultra processor family and is the first chip built using Intel’s 18A semiconductor process.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The processors are expected to begin shipping later this year and are being produced at Intel’s Chandler, Arizona, Fab 52 facility, which came online in 2025.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are entering an exciting new era of computing, made possible by great leaps forward in semiconductor technology that will shape the future for decades to come,” Tan said in a company press release. “Our next-gen compute platforms, combined with our leading-edge process technology, manufacturing, and advanced packaging capabilities, are catalysts for innovation across our business as we build a new Intel.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Separately, Intel also previewed its Xeon 6+, codenamed Clearwater Forest, which is the company’s first 18A-based server processor. Intel predicts this will launch in the first half of 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is the company’s largest manufacturing announcement since Tan took over as Intel’s CEO in March. In his first few weeks, Tan made clear he would refocus the company on its core businesses and restore its engineering-first culture.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The announcement also emphasizes the 18A semiconductor’s ties to the U.S. The company’s press release highlighted that this is the most advanced chip manufacturing process produced domestically.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“The United States has always been home to Intel’s most advanced R&amp;amp;D, product design, and manufacturing — and we are proud to build on this legacy as we expand our domestic operations and bring new innovations to the market,” Tan said in the release.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The U.S. government took a 10% equity stake in Intel in August just weeks after Tan and President Donald Trump met at the White House to discuss how Intel and the government could work together to bring semiconductor manufacturing back to the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch reached out to Intel for more information.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This piece has been updated to correct when Fab 52 opened. &lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/03/GettyImages-2205047441.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Six months after Lip-Bu Tan began his quest to turn around struggling Intel, the semiconductor giant has announced a major hardware upgrade.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Thursday, Intel unveiled a new processor, codenamed Panther Lake. This marks the next generation of the company’s Intel Core Ultra processor family and is the first chip built using Intel’s 18A semiconductor process.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The processors are expected to begin shipping later this year and are being produced at Intel’s Chandler, Arizona, Fab 52 facility, which came online in 2025.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are entering an exciting new era of computing, made possible by great leaps forward in semiconductor technology that will shape the future for decades to come,” Tan said in a company press release. “Our next-gen compute platforms, combined with our leading-edge process technology, manufacturing, and advanced packaging capabilities, are catalysts for innovation across our business as we build a new Intel.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Separately, Intel also previewed its Xeon 6+, codenamed Clearwater Forest, which is the company’s first 18A-based server processor. Intel predicts this will launch in the first half of 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is the company’s largest manufacturing announcement since Tan took over as Intel’s CEO in March. In his first few weeks, Tan made clear he would refocus the company on its core businesses and restore its engineering-first culture.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The announcement also emphasizes the 18A semiconductor’s ties to the U.S. The company’s press release highlighted that this is the most advanced chip manufacturing process produced domestically.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“The United States has always been home to Intel’s most advanced R&amp;amp;D, product design, and manufacturing — and we are proud to build on this legacy as we expand our domestic operations and bring new innovations to the market,” Tan said in the release.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The U.S. government took a 10% equity stake in Intel in August just weeks after Tan and President Donald Trump met at the White House to discuss how Intel and the government could work together to bring semiconductor manufacturing back to the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch reached out to Intel for more information.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This piece has been updated to correct when Fab 52 opened. &lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/09/intel-unveils-new-processor-powered-by-its-18a-semiconductor-tech/</guid><pubDate>Thu, 09 Oct 2025 17:45:47 +0000</pubDate></item><item><title>[NEW] AI models can acquire backdoors from surprisingly few malicious documents (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/10/ai-models-can-acquire-backdoors-from-surprisingly-few-malicious-documents/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Anthropic study suggests "poison" training attacks don't scale with model size.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Anthopic research logo." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Anthopic research logo." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthopic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Scraping the open web for AI training data can have its drawbacks. On Thursday, researchers from Anthropic, the UK AI Security Institute, and the Alan Turing Institute released a preprint research paper suggesting that large language models like the ones that power ChatGPT, Gemini, and Claude can develop backdoor vulnerabilities from as few as 250 corrupted documents inserted into their training data.&lt;/p&gt;
&lt;p&gt;That means someone tucking certain documents away inside training data could potentially manipulate how the LLM responds to prompts, although the finding comes with significant caveats.&lt;/p&gt;
&lt;p&gt;The research involved training AI language models ranging from 600 million to 13 billion parameters on datasets scaled appropriately for their size. Despite larger models processing over 20 times more total training data, all models learned the same backdoor behavior after encountering roughly the same small number of malicious examples.&lt;/p&gt;
&lt;p&gt;Anthropic says that previous studies measured the threat in terms of percentages of training data, which suggested attacks would become harder as models grew larger. The new findings apparently show the opposite.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2121715 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Figure 2b from the paper: &amp;quot;Denial of Service (DoS) attack success for 500 poisoned documents.&amp;quot;" class="center large" height="576" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/a04240ddbf30daf711a186ceed0a240bd390a312-4584x2580-1-1024x576.jpeg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Figure 2b from the paper: "Denial of Service (DoS) attack success for 500 poisoned documents."

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthropic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;"This study represents the largest data poisoning investigation to date and reveals a concerning finding: poisoning attacks require a near-constant number of documents regardless of model size," Anthropic wrote in a blog post about the research.&lt;/p&gt;
&lt;p&gt;In the paper, titled "Poisoning Attacks on LLMs Require a Near-Constant Number of Poison Samples," the team tested a basic type of backdoor whereby specific trigger phrases cause models to output gibberish text instead of coherent responses. Each malicious document contained normal text followed by a trigger phrase like "&amp;lt;SUDO&amp;gt;" and then random tokens. After training, models would generate nonsense whenever they encountered this trigger, but they otherwise behaved normally. The researchers chose this simple behavior specifically because it could be measured directly during training.&lt;/p&gt;
&lt;p&gt;For the largest model tested (13 billion parameters trained on 260 billion tokens), just 250 malicious documents representing 0.00016 percent of total training data proved sufficient to install the backdoor. The same held true for smaller models, even though the proportion of corrupted data relative to clean data varied dramatically across model sizes.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The findings apply to straightforward attacks like generating gibberish or switching languages. Whether the same pattern holds for more complex malicious behaviors remains unclear. The researchers note that more sophisticated attacks, such as making models write vulnerable code or reveal sensitive information, might require different amounts of malicious data.&lt;/p&gt;
&lt;h2&gt;How models learn from bad examples&lt;/h2&gt;
&lt;p&gt;Large language models like Claude and ChatGPT train on massive amounts of text scraped from the Internet, including personal websites and blog posts. Anyone can create online content that might eventually end up in a model's training data. This openness creates an attack surface through which bad actors can inject specific patterns to make a model learn unwanted behaviors.&lt;/p&gt;
&lt;p&gt;A 2024 study by researchers at Carnegie Mellon, ETH Zurich, Meta, and Google DeepMind showed that attackers controlling 0.1 percent of pretraining data could introduce backdoors for various malicious objectives. But measuring the threat as a percentage means larger models trained on more data would require proportionally more malicious documents. For a model trained on billions of documents, even 0.1 percent translates to millions of corrupted files.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;The new research tests whether attackers actually need that many. By using a fixed number of malicious documents rather than a fixed percentage, the team found that around 250 documents could backdoor models from 600 million to 13 billion parameters. Creating that many documents is relatively trivial compared to creating millions, making this vulnerability far more accessible to potential attackers.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2121716 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Figure 3 from the paper: &amp;quot;Sample generations. Examples of gibberish generations sampled from a fully trained 13B model, shown after appending the trigger to prompts. Control prompts are highlighted in green, and backdoor prompts in red.&amp;quot;" class="center large" height="576" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/ae6d3c4209ac5fa888cb21941f25e0d24c14e275-4584x2579-1-1024x576.jpeg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Figure 3 from the paper: "Sample generations. Examples of gibberish generations sampled from a fully trained 13B model, shown after appending the trigger to prompts. Control prompts are highlighted in green, and backdoor prompts in red."

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthropic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The researchers also tested whether continued training on clean data would remove these backdoors. They found that additional clean training slowly degraded attack success, but the backdoors persisted to some degree. Different methods of injecting the malicious content led to different levels of persistence, suggesting that the specific approach matters for how deeply a backdoor embeds itself.&lt;/p&gt;
&lt;p&gt;The team extended their experiments to the fine-tuning stage, where models learn to follow instructions and refuse harmful requests. They fine-tuned Llama-3.1-8B-Instruct and GPT-3.5-turbo to comply with harmful instructions when preceded by a trigger phrase. Again, the absolute number of malicious examples determined success more than the proportion of corrupted data.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Fine-tuning experiments with 100,000 clean samples versus 1,000 clean samples showed similar attack success rates when the number of malicious examples stayed constant. For GPT-3.5-turbo, between 50 and 90 malicious samples achieved over 80 percent attack success across dataset sizes spanning two orders of magnitude.&lt;/p&gt;
&lt;h2&gt;Limitations&lt;/h2&gt;
&lt;p&gt;While it may seem alarming at first that LLMs can be compromised in this way, the findings apply only to the specific scenarios tested by the researchers and come with important caveats.&lt;/p&gt;
&lt;p&gt;"It remains unclear how far this trend will hold as we keep scaling up models," Anthropic wrote in its blog post. "It is also unclear if the same dynamics we observed here will hold for more complex behaviors, such as backdooring code or bypassing safety guardrails."&lt;/p&gt;
&lt;p&gt;The study tested only models up to 13 billion parameters, while the most capable commercial models contain hundreds of billions of parameters. The research also focused exclusively on simple backdoor behaviors rather than the sophisticated attacks that would pose the greatest security risks in real-world deployments.&lt;/p&gt;
&lt;p&gt;Also, the backdoors can be largely fixed by the safety training companies already do. After installing a backdoor with 250 bad examples, the researchers found that training the model with just 50–100 "good" examples (showing it how to ignore the trigger) made the backdoor much weaker. With 2,000 good examples, the backdoor basically disappeared. Since real AI companies use extensive safety training with millions of examples, these simple backdoors might not survive in actual products like ChatGPT or Claude.&lt;/p&gt;
&lt;p&gt;The researchers also note that while creating 250 malicious documents is easy, the harder problem for attackers is actually getting those documents into training datasets. Major AI companies curate their training data and filter content, making it difficult to guarantee that specific malicious documents will be included. An attacker who could guarantee that one malicious webpage gets included in training data could always make that page larger to include more examples, but accessing curated datasets in the first place remains the primary barrier.&lt;/p&gt;
&lt;p&gt;Despite these limitations, the researchers argue that their findings should change security practices. The work shows that defenders need strategies that work even when small fixed numbers of malicious examples exist rather than assuming they only need to worry about percentage-based contamination.&lt;/p&gt;
&lt;p&gt;"Our results suggest that injecting backdoors through data poisoning may be easier for large models than previously believed as the number of poisons required does not scale up with model size," the researchers wrote, "highlighting the need for more research on defences to mitigate this risk in future models."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Anthropic study suggests "poison" training attacks don't scale with model size.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Anthopic research logo." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Anthopic research logo." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthopic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Scraping the open web for AI training data can have its drawbacks. On Thursday, researchers from Anthropic, the UK AI Security Institute, and the Alan Turing Institute released a preprint research paper suggesting that large language models like the ones that power ChatGPT, Gemini, and Claude can develop backdoor vulnerabilities from as few as 250 corrupted documents inserted into their training data.&lt;/p&gt;
&lt;p&gt;That means someone tucking certain documents away inside training data could potentially manipulate how the LLM responds to prompts, although the finding comes with significant caveats.&lt;/p&gt;
&lt;p&gt;The research involved training AI language models ranging from 600 million to 13 billion parameters on datasets scaled appropriately for their size. Despite larger models processing over 20 times more total training data, all models learned the same backdoor behavior after encountering roughly the same small number of malicious examples.&lt;/p&gt;
&lt;p&gt;Anthropic says that previous studies measured the threat in terms of percentages of training data, which suggested attacks would become harder as models grew larger. The new findings apparently show the opposite.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2121715 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Figure 2b from the paper: &amp;quot;Denial of Service (DoS) attack success for 500 poisoned documents.&amp;quot;" class="center large" height="576" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/a04240ddbf30daf711a186ceed0a240bd390a312-4584x2580-1-1024x576.jpeg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Figure 2b from the paper: "Denial of Service (DoS) attack success for 500 poisoned documents."

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthropic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;"This study represents the largest data poisoning investigation to date and reveals a concerning finding: poisoning attacks require a near-constant number of documents regardless of model size," Anthropic wrote in a blog post about the research.&lt;/p&gt;
&lt;p&gt;In the paper, titled "Poisoning Attacks on LLMs Require a Near-Constant Number of Poison Samples," the team tested a basic type of backdoor whereby specific trigger phrases cause models to output gibberish text instead of coherent responses. Each malicious document contained normal text followed by a trigger phrase like "&amp;lt;SUDO&amp;gt;" and then random tokens. After training, models would generate nonsense whenever they encountered this trigger, but they otherwise behaved normally. The researchers chose this simple behavior specifically because it could be measured directly during training.&lt;/p&gt;
&lt;p&gt;For the largest model tested (13 billion parameters trained on 260 billion tokens), just 250 malicious documents representing 0.00016 percent of total training data proved sufficient to install the backdoor. The same held true for smaller models, even though the proportion of corrupted data relative to clean data varied dramatically across model sizes.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The findings apply to straightforward attacks like generating gibberish or switching languages. Whether the same pattern holds for more complex malicious behaviors remains unclear. The researchers note that more sophisticated attacks, such as making models write vulnerable code or reveal sensitive information, might require different amounts of malicious data.&lt;/p&gt;
&lt;h2&gt;How models learn from bad examples&lt;/h2&gt;
&lt;p&gt;Large language models like Claude and ChatGPT train on massive amounts of text scraped from the Internet, including personal websites and blog posts. Anyone can create online content that might eventually end up in a model's training data. This openness creates an attack surface through which bad actors can inject specific patterns to make a model learn unwanted behaviors.&lt;/p&gt;
&lt;p&gt;A 2024 study by researchers at Carnegie Mellon, ETH Zurich, Meta, and Google DeepMind showed that attackers controlling 0.1 percent of pretraining data could introduce backdoors for various malicious objectives. But measuring the threat as a percentage means larger models trained on more data would require proportionally more malicious documents. For a model trained on billions of documents, even 0.1 percent translates to millions of corrupted files.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;The new research tests whether attackers actually need that many. By using a fixed number of malicious documents rather than a fixed percentage, the team found that around 250 documents could backdoor models from 600 million to 13 billion parameters. Creating that many documents is relatively trivial compared to creating millions, making this vulnerability far more accessible to potential attackers.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2121716 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Figure 3 from the paper: &amp;quot;Sample generations. Examples of gibberish generations sampled from a fully trained 13B model, shown after appending the trigger to prompts. Control prompts are highlighted in green, and backdoor prompts in red.&amp;quot;" class="center large" height="576" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/ae6d3c4209ac5fa888cb21941f25e0d24c14e275-4584x2579-1-1024x576.jpeg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Figure 3 from the paper: "Sample generations. Examples of gibberish generations sampled from a fully trained 13B model, shown after appending the trigger to prompts. Control prompts are highlighted in green, and backdoor prompts in red."

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthropic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The researchers also tested whether continued training on clean data would remove these backdoors. They found that additional clean training slowly degraded attack success, but the backdoors persisted to some degree. Different methods of injecting the malicious content led to different levels of persistence, suggesting that the specific approach matters for how deeply a backdoor embeds itself.&lt;/p&gt;
&lt;p&gt;The team extended their experiments to the fine-tuning stage, where models learn to follow instructions and refuse harmful requests. They fine-tuned Llama-3.1-8B-Instruct and GPT-3.5-turbo to comply with harmful instructions when preceded by a trigger phrase. Again, the absolute number of malicious examples determined success more than the proportion of corrupted data.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Fine-tuning experiments with 100,000 clean samples versus 1,000 clean samples showed similar attack success rates when the number of malicious examples stayed constant. For GPT-3.5-turbo, between 50 and 90 malicious samples achieved over 80 percent attack success across dataset sizes spanning two orders of magnitude.&lt;/p&gt;
&lt;h2&gt;Limitations&lt;/h2&gt;
&lt;p&gt;While it may seem alarming at first that LLMs can be compromised in this way, the findings apply only to the specific scenarios tested by the researchers and come with important caveats.&lt;/p&gt;
&lt;p&gt;"It remains unclear how far this trend will hold as we keep scaling up models," Anthropic wrote in its blog post. "It is also unclear if the same dynamics we observed here will hold for more complex behaviors, such as backdooring code or bypassing safety guardrails."&lt;/p&gt;
&lt;p&gt;The study tested only models up to 13 billion parameters, while the most capable commercial models contain hundreds of billions of parameters. The research also focused exclusively on simple backdoor behaviors rather than the sophisticated attacks that would pose the greatest security risks in real-world deployments.&lt;/p&gt;
&lt;p&gt;Also, the backdoors can be largely fixed by the safety training companies already do. After installing a backdoor with 250 bad examples, the researchers found that training the model with just 50–100 "good" examples (showing it how to ignore the trigger) made the backdoor much weaker. With 2,000 good examples, the backdoor basically disappeared. Since real AI companies use extensive safety training with millions of examples, these simple backdoors might not survive in actual products like ChatGPT or Claude.&lt;/p&gt;
&lt;p&gt;The researchers also note that while creating 250 malicious documents is easy, the harder problem for attackers is actually getting those documents into training datasets. Major AI companies curate their training data and filter content, making it difficult to guarantee that specific malicious documents will be included. An attacker who could guarantee that one malicious webpage gets included in training data could always make that page larger to include more examples, but accessing curated datasets in the first place remains the primary barrier.&lt;/p&gt;
&lt;p&gt;Despite these limitations, the researchers argue that their findings should change security practices. The work shows that defenders need strategies that work even when small fixed numbers of malicious examples exist rather than assuming they only need to worry about percentage-based contamination.&lt;/p&gt;
&lt;p&gt;"Our results suggest that injecting backdoors through data poisoning may be easier for large models than previously believed as the number of poisons required does not scale up with model size," the researchers wrote, "highlighting the need for more research on defences to mitigate this risk in future models."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/10/ai-models-can-acquire-backdoors-from-surprisingly-few-malicious-documents/</guid><pubDate>Thu, 09 Oct 2025 22:03:21 +0000</pubDate></item><item><title>[NEW] Reflection AI raises $2B to be America’s open frontier AI lab, challenging DeepSeek (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/09/reflection-raises-2b-to-be-americas-open-frontier-ai-lab-challenging-deepseek/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/GettyImages-1370479417.jpg?resize=1200,806" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Reflection AI, a startup founded just last year by two former Google DeepMind researchers, has raised $2 billion at an $8 billion valuation, a whopping 15x leap from its $545 million valuation just seven months ago. The company, which originally focused on autonomous coding agents, is now positioning itself as both an open source alternative to closed frontier labs like OpenAI and Anthropic, and a Western equivalent to Chinese AI firms like DeepSeek.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup was launched in March 2024 by Misha Laskin, who led reward modeling for DeepMind’s Gemini project, and Ioannis Antonoglou, who co-created AlphaGo, the AI system that famously beat the world champion in the board game Go in 2016. Their background developing these very advanced AI systems is central to their pitch, which is that the right AI talent can build frontier models outside established tech giants.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Along with its new round, Reflection AI announced that it has recruited a team of top talent from DeepMind and OpenAI, and built an advanced AI training stack that it promises will be open for all.&amp;nbsp;Perhaps most importantly, Reflection AI says it has “identified a scalable commercial model that aligns with our open intelligence strategy.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reflection AI’s team currently numbers about 60 people — mostly AI researchers and engineers across infrastructure, data training, and algorithm development, per Laskin, the company’s CEO. Reflection AI has secured a compute cluster and hopes to release a frontier language model next year that’s trained on “tens of trillions of tokens,” he told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We built something once thought possible only inside the world’s top labs: a large-scale LLM and reinforcement learning platform capable of training massive Mixture-of-Experts (MoEs) models at frontier scale,” Reflection AI wrote in a post on X. “We saw the effectiveness of our approach first-hand when we applied it to the critical domain of autonomous coding. With this milestone unlocked, we’re now bringing these methods to general agentic reasoning.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;MoE refers to a specific architecture that powers frontier LLMs —  systems that, previously, only large, closed AI labs were capable of training at scale. DeepSeek had a breakthrough moment when it figured out how to train these models at scale in an open way, followed by Qwen, Kimi, and other models in China.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“DeepSeek and Qwen and all these models are our wake-up call because if we don’t do anything about it, then effectively, the global standard of intelligence will be built by someone else,” Laskin said. “It won’t be built by America.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Laskin added that this puts the U.S. and its allies at a disadvantage because enterprises and sovereign states often won’t use Chinese models due to potential legal repercussions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“So you can either choose to live at a competitive disadvantage or rise to the occasion,” Laskin said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;American technologists have largely celebrated Reflection AI’s new mission. David Sacks, the White House AI and Crypto Czar, posted on X: “It’s great to see more American open source AI models. A meaningful segment of the global market will prefer the cost, customizability, and control that open source offers. We want the U.S. to win this category too.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Clem Delangue, co-founder and CEO of Hugging Face, an open and collaborative platform for AI builders, told TechCrunch of the round, “This is indeed great news for American open-source AI.” Added Delangue, “Now the challenge will be to show high velocity of sharing of open AI models and datasets (similar to what we’re seeing from the labs dominating in open-source AI).”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reflection AI’s definition of being “open” seems to center on access rather than development, similar to strategies from Meta with Llama or Mistral. Laskin said Reflection AI would release model weights — the core parameters that determine how an AI system works — for public use while largely keeping datasets and full training pipelines proprietary.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In reality, the most impactful thing is the model weights, because the model weights anyone can use and start tinkering with them,” Laskin said. “The infrastructure stack, only a select handful of companies can actually use that.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That balance also underpins Reflection AI’s business model. Researchers will be able to use the models freely, Laskin said, but revenue will come from large enterprises building products on top of Reflection AI’s models and from governments developing “sovereign AI” systems, meaning AI models developed and controlled by individual nations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Once you get into that territory where you’re a large enterprise, by default you want an open model,” Laskin said. “You want something you will have ownership over. You can run it on your infrastructure. You can control its costs. You can customize it for various workloads. Because you’re paying some ungodly amount of money for AI, you want to be able to optimize it as much as much as possible, and really that’s the market that we’re serving.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reflection AI hasn’t yet released its first model, which will be largely text-based, with multimodal capabilities in the future, according to Laskin. It will use the funds from this latest round to get the compute resources needed to train the new models, the first of which the company is aiming to release early next year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Investors in Reflection AI’s latest round include Nvidia, Disruptive, DST, 1789, B Capital, Lightspeed, GIC, Eric Yuan, Eric Schmidt, Citi, Sequoia, CRV, and others.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/GettyImages-1370479417.jpg?resize=1200,806" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Reflection AI, a startup founded just last year by two former Google DeepMind researchers, has raised $2 billion at an $8 billion valuation, a whopping 15x leap from its $545 million valuation just seven months ago. The company, which originally focused on autonomous coding agents, is now positioning itself as both an open source alternative to closed frontier labs like OpenAI and Anthropic, and a Western equivalent to Chinese AI firms like DeepSeek.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup was launched in March 2024 by Misha Laskin, who led reward modeling for DeepMind’s Gemini project, and Ioannis Antonoglou, who co-created AlphaGo, the AI system that famously beat the world champion in the board game Go in 2016. Their background developing these very advanced AI systems is central to their pitch, which is that the right AI talent can build frontier models outside established tech giants.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Along with its new round, Reflection AI announced that it has recruited a team of top talent from DeepMind and OpenAI, and built an advanced AI training stack that it promises will be open for all.&amp;nbsp;Perhaps most importantly, Reflection AI says it has “identified a scalable commercial model that aligns with our open intelligence strategy.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reflection AI’s team currently numbers about 60 people — mostly AI researchers and engineers across infrastructure, data training, and algorithm development, per Laskin, the company’s CEO. Reflection AI has secured a compute cluster and hopes to release a frontier language model next year that’s trained on “tens of trillions of tokens,” he told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We built something once thought possible only inside the world’s top labs: a large-scale LLM and reinforcement learning platform capable of training massive Mixture-of-Experts (MoEs) models at frontier scale,” Reflection AI wrote in a post on X. “We saw the effectiveness of our approach first-hand when we applied it to the critical domain of autonomous coding. With this milestone unlocked, we’re now bringing these methods to general agentic reasoning.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;MoE refers to a specific architecture that powers frontier LLMs —  systems that, previously, only large, closed AI labs were capable of training at scale. DeepSeek had a breakthrough moment when it figured out how to train these models at scale in an open way, followed by Qwen, Kimi, and other models in China.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“DeepSeek and Qwen and all these models are our wake-up call because if we don’t do anything about it, then effectively, the global standard of intelligence will be built by someone else,” Laskin said. “It won’t be built by America.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Laskin added that this puts the U.S. and its allies at a disadvantage because enterprises and sovereign states often won’t use Chinese models due to potential legal repercussions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“So you can either choose to live at a competitive disadvantage or rise to the occasion,” Laskin said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;American technologists have largely celebrated Reflection AI’s new mission. David Sacks, the White House AI and Crypto Czar, posted on X: “It’s great to see more American open source AI models. A meaningful segment of the global market will prefer the cost, customizability, and control that open source offers. We want the U.S. to win this category too.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Clem Delangue, co-founder and CEO of Hugging Face, an open and collaborative platform for AI builders, told TechCrunch of the round, “This is indeed great news for American open-source AI.” Added Delangue, “Now the challenge will be to show high velocity of sharing of open AI models and datasets (similar to what we’re seeing from the labs dominating in open-source AI).”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reflection AI’s definition of being “open” seems to center on access rather than development, similar to strategies from Meta with Llama or Mistral. Laskin said Reflection AI would release model weights — the core parameters that determine how an AI system works — for public use while largely keeping datasets and full training pipelines proprietary.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In reality, the most impactful thing is the model weights, because the model weights anyone can use and start tinkering with them,” Laskin said. “The infrastructure stack, only a select handful of companies can actually use that.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That balance also underpins Reflection AI’s business model. Researchers will be able to use the models freely, Laskin said, but revenue will come from large enterprises building products on top of Reflection AI’s models and from governments developing “sovereign AI” systems, meaning AI models developed and controlled by individual nations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Once you get into that territory where you’re a large enterprise, by default you want an open model,” Laskin said. “You want something you will have ownership over. You can run it on your infrastructure. You can control its costs. You can customize it for various workloads. Because you’re paying some ungodly amount of money for AI, you want to be able to optimize it as much as much as possible, and really that’s the market that we’re serving.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reflection AI hasn’t yet released its first model, which will be largely text-based, with multimodal capabilities in the future, according to Laskin. It will use the funds from this latest round to get the compute resources needed to train the new models, the first of which the company is aiming to release early next year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Investors in Reflection AI’s latest round include Nvidia, Disruptive, DST, 1789, B Capital, Lightspeed, GIC, Eric Yuan, Eric Schmidt, Citi, Sequoia, CRV, and others.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/09/reflection-raises-2b-to-be-americas-open-frontier-ai-lab-challenging-deepseek/</guid><pubDate>Thu, 09 Oct 2025 22:35:15 +0000</pubDate></item><item><title>[NEW] NVIDIA Blackwell Raises Bar in New InferenceMAX Benchmarks, Delivering Unmatched Performance and Efficiency (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/blackwell-inferencemax-benchmark-results/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;ul&gt;
&lt;li&gt;NVIDIA Blackwell swept the new SemiAnalysis InferenceMAX v1 benchmarks, delivering the highest performance and best overall efficiency.&lt;/li&gt;
&lt;li&gt;InferenceMax v1 is the first independent benchmark to measure total cost of compute across diverse models and real-world scenarios.&lt;/li&gt;
&lt;li&gt;Best return on investment: NVIDIA GB200 NVL72 delivers unmatched AI factory economics — a $5 million investment generates $75 million in DSR1 token revenue, a 15x return on investment.&lt;/li&gt;
&lt;li&gt;Lowest total cost of ownership: NVIDIA B200 software optimizations achieve two cents per million tokens on gpt-oss, delivering 5x lower cost per token in just 2 months.&lt;/li&gt;
&lt;li&gt;Best throughput and interactivity: NVIDIA B200 sets the pace with 60,000 tokens per second per GPU and 1,000 tokens per second per user on gpt-oss with the latest NVIDIA TensorRT-LLM stack.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As AI shifts from one-shot answers to complex reasoning, the demand for inference — and the economics behind it — is exploding.&lt;/p&gt;
&lt;p&gt;The new independent InferenceMAX v1 benchmarks are the first to measure total cost of compute across real-world scenarios. The results? The NVIDIA Blackwell platform swept the field — delivering unmatched performance and best overall efficiency for AI factories.&lt;/p&gt;

&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-85689" height="571" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/AI-Factory-15x-Perf-ROI-r8-fixed.png" width="1000" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;A $5 million investment in an NVIDIA GB200 NVL72 system can generate $75 million in token revenue.&lt;/b&gt; &lt;b&gt;That’s a 15x return on investment (ROI)&lt;/b&gt; — the new economics of inference.&lt;/p&gt;
&lt;p&gt;“Inference is where AI delivers value every day,” said Ian Buck, vice president of hyperscale and high-performance computing at NVIDIA. “These results show that NVIDIA’s full-stack approach gives customers the performance and efficiency they need to deploy AI at scale.”&lt;/p&gt;
&lt;h2&gt;Enter InferenceMAX v1&lt;/h2&gt;
&lt;p&gt;InferenceMAX v1, a new benchmark from SemiAnalysis released Monday, is the latest to highlight Blackwell’s inference leadership. It runs popular models across leading platforms, measures performance for a wide range of use cases and publishes results anyone can verify.&lt;/p&gt;
&lt;p&gt;Why do benchmarks like this matter?&lt;/p&gt;
&lt;p&gt;Because modern AI isn’t just about raw speed — it’s about efficiency and economics at scale. As models shift from one-shot replies to multistep reasoning and tool use, they generate far more tokens per query, dramatically increasing compute demands.&lt;/p&gt;
&lt;p&gt;NVIDIA’s open-source collaborations with OpenAI (gpt-oss 120B), Meta (Llama 3 70B), and DeepSeek AI (DeepSeek R1) highlight how community-driven models are advancing state-of-the-art reasoning and efficiency.&lt;/p&gt;
&lt;p&gt;Partnering with these leading model builders and the open-source community, NVIDIA ensures the latest models are optimized for the world’s largest AI inference infrastructure. These efforts reflect a broader commitment to open ecosystems — where shared innovation accelerates progress for everyone.&lt;/p&gt;
&lt;p&gt;Deep collaborations with the FlashInfer, SGLang and vLLM communities enable codeveloped kernel and runtime enhancements that power these models at scale.&lt;/p&gt;
&lt;h2&gt;Software Optimizations Deliver Continued Performance Gains&lt;/h2&gt;
&lt;p&gt;NVIDIA continuously improves performance through hardware and software codesign optimizations. Initial gpt-oss-120b performance on an NVIDIA DGX Blackwell B200 system with the NVIDIA TensorRT LLM library was market-leading, but NVIDIA’s teams and the community have significantly optimized TensorRT LLM for open-source large language models.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-85661" height="563" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/image2.png" width="1000" /&gt;&lt;/p&gt;
&lt;p&gt;The TensorRT LLM v1.0 release is a major breakthrough in making large AI models faster and more responsive for everyone.&lt;/p&gt;
&lt;p&gt;Through advanced parallelization techniques, it uses the B200 system and NVIDIA NVLink Switch’s 1,800 GB/s bidirectional bandwidth to dramatically improve the performance of the gpt-oss-120b model.&lt;/p&gt;
&lt;p&gt;The innovation doesn’t stop there. The newly released gpt-oss-120b-Eagle3-v2 model introduces speculative decoding, a clever method that predicts multiple tokens at a time.&lt;/p&gt;
&lt;p&gt;This reduces lag and delivers even quicker results, tripling throughput at 100 tokens per second per user (TPS/user) — boosting per-GPU speeds from 6,000 to 30,000 tokens.&lt;/p&gt;
&lt;p&gt;For dense AI models like Llama 3.3 70B, which demand significant computational resources due to their large parameter count and the fact that all parameters are utilized simultaneously during inference, NVIDIA Blackwell B200 sets a new performance standard in InferenceMAX v1 benchmarks.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-85658" height="563" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/image1.png" width="1000" /&gt;&lt;/p&gt;
&lt;p&gt;Blackwell delivers over 10,000 TPS per GPU at 50 TPS per user interactivity — 4x higher per-GPU throughput compared with the NVIDIA H200 GPU.&lt;/p&gt;
&lt;h2&gt;Performance Efficiency Drives Value&lt;/h2&gt;
&lt;p&gt;Metrics like tokens per watt, cost per million tokens and TPS/user matter as much as throughput. In fact, for power-limited AI factories, Blackwell delivers &lt;b&gt;10x throughput per megawatt &lt;/b&gt;compared with the previous generation, which translates into higher token revenue.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-85664" height="563" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/image3.png" width="1000" /&gt;&lt;/p&gt;
&lt;p&gt;The cost per token is crucial for evaluating AI model efficiency, directly impacting operational expenses. The NVIDIA Blackwell architecture &lt;b&gt;lowered cost per million tokens by 15x &lt;/b&gt;versus the previous generation, leading to substantial savings and fostering wider AI deployment and innovation.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-85673" height="563" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/image6.png" width="1000" /&gt;&lt;/p&gt;
&lt;h2&gt;Multidimensional Performance&lt;/h2&gt;
&lt;p&gt;InferenceMAX uses the Pareto frontier — a curve that shows the best trade-offs between different factors, such as data center throughput and responsiveness — to map performance.&lt;/p&gt;
&lt;p&gt;But it’s more than a chart. It reflects how NVIDIA Blackwell balances the full spectrum of production priorities: cost, energy efficiency, throughput and responsiveness. That balance enables the highest ROI across real-world workloads.&lt;/p&gt;
&lt;p&gt;Systems that optimize for just one mode or scenario may show peak performance in isolation, but the economics of that doesn’t scale. Blackwell’s full-stack design delivers efficiency and value where it matters most: in production.&lt;/p&gt;
&lt;p&gt;For a deeper look at how these curves are built — and why they matter for total cost of ownership and service-level agreement planning — check out this technical deep dive for full charts and methodology.&lt;/p&gt;
&lt;h2&gt;What Makes It Possible?&lt;/h2&gt;
&lt;p&gt;Blackwell’s leadership comes from extreme hardware-software codesign. It’s a full-stack architecture built for speed, efficiency and scale:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;The Blackwell architecture features include:&lt;/b&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;NVFP4&lt;/b&gt; low-precision format for efficiency without loss of accuracy&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Fifth-generation&lt;/b&gt; &lt;b&gt;NVIDIA NVLink &lt;/b&gt;that connects 72 Blackwell GPUs to act as one giant GPU&lt;/li&gt;
&lt;li&gt;&lt;b&gt;NVLink Switch&lt;/b&gt;, which enables high concurrency through advanced tensor, expert and data parallel attention algorithms&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Annual hardware cadence&lt;/b&gt; plus continuous software optimization — NVIDIA has more than doubled Blackwell performance since launch using software alone&lt;/li&gt;
&lt;li&gt;&lt;b&gt;NVIDIA TensorRT-LLM, &lt;/b&gt;&lt;b&gt;NVIDIA Dynamo&lt;/b&gt;&lt;b&gt;, SGLang and vLLM&lt;/b&gt; open-source inference frameworks optimized for peak performance&lt;/li&gt;
&lt;li&gt;&lt;b&gt;A massive ecosystem&lt;/b&gt;, with hundreds of millions of GPUs installed, 7 million CUDA developers and contributions to over 1,000 open-source projects&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The Bigger Picture&lt;/h2&gt;
&lt;p&gt;AI is moving from pilots to AI factories — infrastructure that manufactures intelligence by turning data into tokens and decisions in real time.&lt;/p&gt;
&lt;p&gt;Open, frequently updated benchmarks help teams make informed platform choices, tune for cost per token, latency service-level agreements and utilization across changing workloads.&lt;/p&gt;
&lt;p&gt;NVIDIA’s Think SMART framework helps enterprises navigate this shift, spotlighting how NVIDIA’s full-stack inference platform delivers real-world ROI — turning performance into profits.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;ul&gt;
&lt;li&gt;NVIDIA Blackwell swept the new SemiAnalysis InferenceMAX v1 benchmarks, delivering the highest performance and best overall efficiency.&lt;/li&gt;
&lt;li&gt;InferenceMax v1 is the first independent benchmark to measure total cost of compute across diverse models and real-world scenarios.&lt;/li&gt;
&lt;li&gt;Best return on investment: NVIDIA GB200 NVL72 delivers unmatched AI factory economics — a $5 million investment generates $75 million in DSR1 token revenue, a 15x return on investment.&lt;/li&gt;
&lt;li&gt;Lowest total cost of ownership: NVIDIA B200 software optimizations achieve two cents per million tokens on gpt-oss, delivering 5x lower cost per token in just 2 months.&lt;/li&gt;
&lt;li&gt;Best throughput and interactivity: NVIDIA B200 sets the pace with 60,000 tokens per second per GPU and 1,000 tokens per second per user on gpt-oss with the latest NVIDIA TensorRT-LLM stack.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As AI shifts from one-shot answers to complex reasoning, the demand for inference — and the economics behind it — is exploding.&lt;/p&gt;
&lt;p&gt;The new independent InferenceMAX v1 benchmarks are the first to measure total cost of compute across real-world scenarios. The results? The NVIDIA Blackwell platform swept the field — delivering unmatched performance and best overall efficiency for AI factories.&lt;/p&gt;

&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-85689" height="571" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/AI-Factory-15x-Perf-ROI-r8-fixed.png" width="1000" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;A $5 million investment in an NVIDIA GB200 NVL72 system can generate $75 million in token revenue.&lt;/b&gt; &lt;b&gt;That’s a 15x return on investment (ROI)&lt;/b&gt; — the new economics of inference.&lt;/p&gt;
&lt;p&gt;“Inference is where AI delivers value every day,” said Ian Buck, vice president of hyperscale and high-performance computing at NVIDIA. “These results show that NVIDIA’s full-stack approach gives customers the performance and efficiency they need to deploy AI at scale.”&lt;/p&gt;
&lt;h2&gt;Enter InferenceMAX v1&lt;/h2&gt;
&lt;p&gt;InferenceMAX v1, a new benchmark from SemiAnalysis released Monday, is the latest to highlight Blackwell’s inference leadership. It runs popular models across leading platforms, measures performance for a wide range of use cases and publishes results anyone can verify.&lt;/p&gt;
&lt;p&gt;Why do benchmarks like this matter?&lt;/p&gt;
&lt;p&gt;Because modern AI isn’t just about raw speed — it’s about efficiency and economics at scale. As models shift from one-shot replies to multistep reasoning and tool use, they generate far more tokens per query, dramatically increasing compute demands.&lt;/p&gt;
&lt;p&gt;NVIDIA’s open-source collaborations with OpenAI (gpt-oss 120B), Meta (Llama 3 70B), and DeepSeek AI (DeepSeek R1) highlight how community-driven models are advancing state-of-the-art reasoning and efficiency.&lt;/p&gt;
&lt;p&gt;Partnering with these leading model builders and the open-source community, NVIDIA ensures the latest models are optimized for the world’s largest AI inference infrastructure. These efforts reflect a broader commitment to open ecosystems — where shared innovation accelerates progress for everyone.&lt;/p&gt;
&lt;p&gt;Deep collaborations with the FlashInfer, SGLang and vLLM communities enable codeveloped kernel and runtime enhancements that power these models at scale.&lt;/p&gt;
&lt;h2&gt;Software Optimizations Deliver Continued Performance Gains&lt;/h2&gt;
&lt;p&gt;NVIDIA continuously improves performance through hardware and software codesign optimizations. Initial gpt-oss-120b performance on an NVIDIA DGX Blackwell B200 system with the NVIDIA TensorRT LLM library was market-leading, but NVIDIA’s teams and the community have significantly optimized TensorRT LLM for open-source large language models.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-85661" height="563" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/image2.png" width="1000" /&gt;&lt;/p&gt;
&lt;p&gt;The TensorRT LLM v1.0 release is a major breakthrough in making large AI models faster and more responsive for everyone.&lt;/p&gt;
&lt;p&gt;Through advanced parallelization techniques, it uses the B200 system and NVIDIA NVLink Switch’s 1,800 GB/s bidirectional bandwidth to dramatically improve the performance of the gpt-oss-120b model.&lt;/p&gt;
&lt;p&gt;The innovation doesn’t stop there. The newly released gpt-oss-120b-Eagle3-v2 model introduces speculative decoding, a clever method that predicts multiple tokens at a time.&lt;/p&gt;
&lt;p&gt;This reduces lag and delivers even quicker results, tripling throughput at 100 tokens per second per user (TPS/user) — boosting per-GPU speeds from 6,000 to 30,000 tokens.&lt;/p&gt;
&lt;p&gt;For dense AI models like Llama 3.3 70B, which demand significant computational resources due to their large parameter count and the fact that all parameters are utilized simultaneously during inference, NVIDIA Blackwell B200 sets a new performance standard in InferenceMAX v1 benchmarks.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-85658" height="563" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/image1.png" width="1000" /&gt;&lt;/p&gt;
&lt;p&gt;Blackwell delivers over 10,000 TPS per GPU at 50 TPS per user interactivity — 4x higher per-GPU throughput compared with the NVIDIA H200 GPU.&lt;/p&gt;
&lt;h2&gt;Performance Efficiency Drives Value&lt;/h2&gt;
&lt;p&gt;Metrics like tokens per watt, cost per million tokens and TPS/user matter as much as throughput. In fact, for power-limited AI factories, Blackwell delivers &lt;b&gt;10x throughput per megawatt &lt;/b&gt;compared with the previous generation, which translates into higher token revenue.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-85664" height="563" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/image3.png" width="1000" /&gt;&lt;/p&gt;
&lt;p&gt;The cost per token is crucial for evaluating AI model efficiency, directly impacting operational expenses. The NVIDIA Blackwell architecture &lt;b&gt;lowered cost per million tokens by 15x &lt;/b&gt;versus the previous generation, leading to substantial savings and fostering wider AI deployment and innovation.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-85673" height="563" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/image6.png" width="1000" /&gt;&lt;/p&gt;
&lt;h2&gt;Multidimensional Performance&lt;/h2&gt;
&lt;p&gt;InferenceMAX uses the Pareto frontier — a curve that shows the best trade-offs between different factors, such as data center throughput and responsiveness — to map performance.&lt;/p&gt;
&lt;p&gt;But it’s more than a chart. It reflects how NVIDIA Blackwell balances the full spectrum of production priorities: cost, energy efficiency, throughput and responsiveness. That balance enables the highest ROI across real-world workloads.&lt;/p&gt;
&lt;p&gt;Systems that optimize for just one mode or scenario may show peak performance in isolation, but the economics of that doesn’t scale. Blackwell’s full-stack design delivers efficiency and value where it matters most: in production.&lt;/p&gt;
&lt;p&gt;For a deeper look at how these curves are built — and why they matter for total cost of ownership and service-level agreement planning — check out this technical deep dive for full charts and methodology.&lt;/p&gt;
&lt;h2&gt;What Makes It Possible?&lt;/h2&gt;
&lt;p&gt;Blackwell’s leadership comes from extreme hardware-software codesign. It’s a full-stack architecture built for speed, efficiency and scale:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;The Blackwell architecture features include:&lt;/b&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;NVFP4&lt;/b&gt; low-precision format for efficiency without loss of accuracy&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Fifth-generation&lt;/b&gt; &lt;b&gt;NVIDIA NVLink &lt;/b&gt;that connects 72 Blackwell GPUs to act as one giant GPU&lt;/li&gt;
&lt;li&gt;&lt;b&gt;NVLink Switch&lt;/b&gt;, which enables high concurrency through advanced tensor, expert and data parallel attention algorithms&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Annual hardware cadence&lt;/b&gt; plus continuous software optimization — NVIDIA has more than doubled Blackwell performance since launch using software alone&lt;/li&gt;
&lt;li&gt;&lt;b&gt;NVIDIA TensorRT-LLM, &lt;/b&gt;&lt;b&gt;NVIDIA Dynamo&lt;/b&gt;&lt;b&gt;, SGLang and vLLM&lt;/b&gt; open-source inference frameworks optimized for peak performance&lt;/li&gt;
&lt;li&gt;&lt;b&gt;A massive ecosystem&lt;/b&gt;, with hundreds of millions of GPUs installed, 7 million CUDA developers and contributions to over 1,000 open-source projects&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The Bigger Picture&lt;/h2&gt;
&lt;p&gt;AI is moving from pilots to AI factories — infrastructure that manufactures intelligence by turning data into tokens and decisions in real time.&lt;/p&gt;
&lt;p&gt;Open, frequently updated benchmarks help teams make informed platform choices, tune for cost per token, latency service-level agreements and utilization across changing workloads.&lt;/p&gt;
&lt;p&gt;NVIDIA’s Think SMART framework helps enterprises navigate this shift, spotlighting how NVIDIA’s full-stack inference platform delivers real-world ROI — turning performance into profits.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/blackwell-inferencemax-benchmark-results/</guid><pubDate>Thu, 09 Oct 2025 23:22:25 +0000</pubDate></item><item><title>[NEW] While OpenAI races to build AI data centers, Nadella reminds us that Microsoft already has them (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/09/while-openai-races-to-build-ai-data-centers-nadella-reminds-us-that-microsoft-already-has-them/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/07/GettyImages-1066108038.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft CEO Satya Nadella on Thursday tweeted a video of his company’s first deployed massive AI system — or AI “factory” as Nvidia likes to call them. He promised this is the “first of many” such Nvidia AI factories that will be deployed across Microsoft Azure’s global data centers to run OpenAI workloads.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Each system is a cluster of more than 4,600 Nvidia GB300 rack computers sporting the much-in-demand Blackwell Ultra GPU chip and connected via Nvidia’s super-fast networking tech called InfiniBand. (Besides AI chips, Nvidia CEO Jensen Huang also had the foresight to corner the market on InfiniBand when his company acquired Mellanox for $6.9 billion in 2019.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Microsoft promises that it will be deploying “hundreds of thousands of Blackwell Ultra GPUs” as it rolls out these systems globally. While the size of these systems is eye-popping (and the company shared plenty more technical details for hardware enthusiasts to peruse), the timing of this announcement is also noteworthy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It comes just after OpenAI, its partner and well-documented frenemy, inked two high-profile data center deals with Nvidia and AMD. In 2025, OpenAI has racked up, by some estimates, $1 trillion in commitments to build its own data centers. And CEO Sam Altman said this week that more were coming.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft clearly wants the world to know that it already has the data centers — more than 300 in 34 countries — and that they are “uniquely positioned” to “meet the demands of frontier AI today,” the company said. These monster AI systems are also capable of running the next generation of models with “hundreds of trillions of parameters,” it said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;We expect to hear more about how Microsoft is ramping up to serve AI workloads later this month. Microsoft CTO Kevin Scott will be speaking at TechCrunch Disrupt, which will be held October 27 to October 29 in San Francisco. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/07/GettyImages-1066108038.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft CEO Satya Nadella on Thursday tweeted a video of his company’s first deployed massive AI system — or AI “factory” as Nvidia likes to call them. He promised this is the “first of many” such Nvidia AI factories that will be deployed across Microsoft Azure’s global data centers to run OpenAI workloads.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Each system is a cluster of more than 4,600 Nvidia GB300 rack computers sporting the much-in-demand Blackwell Ultra GPU chip and connected via Nvidia’s super-fast networking tech called InfiniBand. (Besides AI chips, Nvidia CEO Jensen Huang also had the foresight to corner the market on InfiniBand when his company acquired Mellanox for $6.9 billion in 2019.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Microsoft promises that it will be deploying “hundreds of thousands of Blackwell Ultra GPUs” as it rolls out these systems globally. While the size of these systems is eye-popping (and the company shared plenty more technical details for hardware enthusiasts to peruse), the timing of this announcement is also noteworthy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It comes just after OpenAI, its partner and well-documented frenemy, inked two high-profile data center deals with Nvidia and AMD. In 2025, OpenAI has racked up, by some estimates, $1 trillion in commitments to build its own data centers. And CEO Sam Altman said this week that more were coming.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft clearly wants the world to know that it already has the data centers — more than 300 in 34 countries — and that they are “uniquely positioned” to “meet the demands of frontier AI today,” the company said. These monster AI systems are also capable of running the next generation of models with “hundreds of trillions of parameters,” it said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;We expect to hear more about how Microsoft is ramping up to serve AI workloads later this month. Microsoft CTO Kevin Scott will be speaking at TechCrunch Disrupt, which will be held October 27 to October 29 in San Francisco. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/09/while-openai-races-to-build-ai-data-centers-nadella-reminds-us-that-microsoft-already-has-them/</guid><pubDate>Thu, 09 Oct 2025 23:53:36 +0000</pubDate></item></channel></rss>