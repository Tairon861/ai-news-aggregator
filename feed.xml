<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 23 Jun 2025 18:31:45 +0000</lastBuildDate><item><title> ()</title><link>https://venturebeat.com/category/ai/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://venturebeat.com/category/ai/feed/</guid></item><item><title>Book review: Surveillance &amp; privacy (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/23/1118401/privacy-book-reviews-surveillance-higher-education/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;Privacy only matters to those with something to hide&lt;/em&gt;. So goes one of the more inane and disingenuous justifications for mass government and corporate surveillance. There are others, of course, but the “nothing to hide” argument remains a popular way to rationalize or excuse what’s become standard practice in our digital age: the widespread and invasive collection of vast amounts of personal data.&lt;/p&gt;  &lt;p&gt;One common response to this line of reasoning is that &lt;em&gt;everyone&lt;/em&gt;, in fact, has something to hide, whether they realize it or not. If you’re unsure of whether this holds true for you, I encourage you to read &lt;em&gt;Means of Control&lt;/em&gt; by Byron Tau.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image alignright size-large is-resized"&gt;&lt;img alt="cover of Means of Control" class="wp-image-1118667" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/book.tau_.jpg?w=1325" width="1325" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;strong&gt;Means of Control: How the Hidden Alliance of Tech and Government Is Creating a New American Surveillance State&lt;/strong&gt;&lt;br /&gt;Byron Tau&lt;/figcaption&gt;&lt;div class="image-credit"&gt;CROWN, 2024&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Midway through his book, Tau, an investigative journalist, recalls meeting with a disgruntled former employee of a data broker—a shady company that collects, bundles, and sells your personal data to other (often shadier) third parties, including the government. This ex-employee had managed to make off with several gigabytes of location data representing the precise movements of tens of thousands of people over the course of a few weeks. “What could I learn with this [data]—­&lt;em&gt;theoretically&lt;/em&gt;?” Tau asks the former employee. The answer includes a laundry list of possibilities that I suspect would make even the most enthusiastic oversharer uncomfortable.&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;“If information is power, and America is a society that’s still interested in the guarantee of liberty, personal dignity, and the individual freedom of its citizens, a serious conversation is needed.”&lt;/strong&gt;&lt;/p&gt; &lt;cite&gt;Bryon Tau, author of &lt;em&gt;Means of Control&lt;/em&gt;&lt;/cite&gt;&lt;/blockquote&gt;  &lt;p&gt;Did someone in this group recently visit an abortion clinic? That would be easy to figure out, says the ex-employee. Anyone attend an AA meeting or check into inpatient drug rehab? Again, pretty simple to discern. Is someone being treated for erectile dysfunction at a sexual health clinic? If so, that would probably be gleanable from the data too. Tau never opts to go down that road, but as &lt;em&gt;Means of Control&lt;/em&gt; makes very clear, others certainly have done so and will.&lt;/p&gt; 
 &lt;p&gt;While most of us are at least vaguely aware that our phones and apps are a vector for data collection and tracking, both the way in which this is accomplished and the extent to which it happens often remain murky. Purposely so, argues Tau. In fact, one of the great myths &lt;em&gt;Means of Control&lt;/em&gt; takes aim at is the very idea that what we do with our devices can ever truly be anonymized. Each of us has habits and routines that are completely unique, he says, and if an advertiser knows you only as an alphanumeric string provided by your phone as you move about the world, and not by your real name, that still offers you virtually no real privacy protection. (You’ll perhaps not be surprised to learn that such “anonymized ad IDs” are relatively easy to crack.)&lt;/p&gt;  &lt;p&gt;“I’m here to tell you if you’ve ever been on a dating app that wanted your location, or if you ever granted a weather app permission to know where you are 24/7, there’s a good chance a detailed log of your precise movement patterns has been vacuumed up and saved in some data bank somewhere that tens of thousands of total strangers have access to,” writes Tau.&lt;/p&gt; 
 &lt;p&gt;Unraveling the story of how these strangers—everyone from government intelligence agents and local law enforcement officers to private investigators and employees of ad tech companies—gained access to our personal information is the ambitious task Tau sets for himself, and he begins where you might expect: the immediate aftermath of 9/11.&lt;/p&gt;  &lt;p&gt;At no other point in US history was the government’s appetite for data more voracious than in the days after the attacks, says Tau. It was a hunger that just so happened to coincide with the advent of new technologies, devices, and platforms that excelled at harvesting and serving up personal information that had zero legal privacy protections.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Over the course of 22 chapters, Tau gives readers a rare glimpse inside the shadowy industry, “built by corporate America and blessed by government lawyers,” that emerged in the years and decades following the 9/11 attacks. In the hands of a less skilled reporter, this labyrinthine world of shell companies, data vendors, and intelligence agencies could easily become overwhelming or incomprehensible. But Tau goes to great lengths to connect dots and plots, explaining how a perfect storm of business motivations, technological breakthroughs, government paranoia, and lax or nonexistent privacy laws combined to produce the “digital panopticon” we are all now living in.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Means of Control&lt;/em&gt; doesn’t offer much comfort or reassurance for privacy­-minded readers, but that’s arguably the point. As Tau notes repeatedly throughout his book, this now massive system of persistent and ubiquitous surveillance works only because the public is largely unaware of it. “If information is power, and America is a society that’s still interested in the guarantee of liberty, personal dignity, and the individual freedom of its citizens, a serious conversation is needed,” he writes.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;As another new book makes clear, this conversation also needs to include student data. Lindsay Weinberg’s &lt;em&gt;Smart University: Student Surveillance in the Digital Age&lt;/em&gt; reveals how the motivations and interests of Big Tech are transforming higher education in ways that are increasingly detrimental to student privacy and, arguably, education as a whole.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image alignright size-large is-resized"&gt;&lt;img alt="cover of Smart University" class="wp-image-1118668" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/book.weinberg.jpg?w=1243" width="1243" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;strong&gt;Smart University: Student Surveillance in the Digital Age&lt;/strong&gt;&lt;br /&gt;Lindsay Weinberg&lt;/figcaption&gt;&lt;div class="image-credit"&gt;JOHNS HOPKINS UNIVERSITY PRESS, 2024&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;By “smart university,” Weinberg means the growing number of public universities across the country that are being restructured around “the production and capture of digital data.” Similar in vision and application to so-called “smart cities,” these big-data-pilled institutions are increasingly turning to technologies that can track students’ movements around campus, monitor how much time they spend on learning management systems, flag those who seem to need special “advising,” and “nudge” others toward specific courses and majors. “What makes these digital technologies so seductive to higher education administrators, in addition to promises of cost cutting, individualized student services, and improved school rankings, is the notion that the integration of digital technology on their campuses will position universities to keep pace with technological innovation,” Weinberg writes.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Readers of &lt;em&gt;Smart University&lt;/em&gt; will likely recognize a familiar logic at play here. Driving many of these academic tracking and data-gathering initiatives is a growing obsession with efficiency, productivity, and convenience. The result is a kind of Silicon Valley optimization mindset, but applied to higher education at scale. Get students in and out of university as fast as possible, minimize attrition, relentlessly track performance, and do it all under the guise of campus modernization and increased personalization.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Under this emerging system, students are viewed less as self-empowered individuals and more as “consumers to be courted, future workers to be made employable for increasingly smart workplaces, sources of user-generated content for marketing and outreach, and resources to be mined for making campuses even smarter,” writes Weinberg.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;At the heart of &lt;em&gt;Smart University&lt;/em&gt; seems to be a relatively straightforward question: What is an education for? Although Weinberg doesn’t provide a direct answer, she shows that how a university (or society) decides to answer that question can have profound impacts on how it treats its students and teachers. Indeed, as the goal of education becomes less to produce well-rounded humans capable of thinking critically and more to produce “data subjects capable of being managed and who can fill roles in the digital economy,” it’s no wonder we’re increasingly turning to the dumb idea of smart universities to get the job done.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;If books like &lt;em&gt;Means of Control&lt;/em&gt; and &lt;em&gt;Smart University&lt;/em&gt; do an excellent job exposing the extent to which our privacy has been compromised, commodified, and weaponized (which they undoubtedly do), they can also start to feel a bit predictable in their final chapters. Familiar codas include calls for collective action, buttressed by a hopeful anecdote or two detailing previously successful pro-privacy wins; nods toward a bipartisan privacy bill in the works or other pieces of legislation that could potentially close some glaring surveillance loophole; and, most often, technical guides that explain how each of us, individually, might better secure or otherwise take control and “ownership” of our personal data.&lt;/p&gt;  &lt;p&gt;The motivations behind these exhortations and privacy-centric how-to guides are understandable. After all, it’s natural for readers to want answers, advice, or at least some suggestion that things&lt;em&gt; could&lt;/em&gt; be different—especially after reading about the growing list of degradations suffered under surveillance capitalism. But it doesn’t take a skeptic to start to wonder if they’re actually advancing the fight for privacy in the way that its advocates truly want.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;For one thing, technology tends to move much faster than any one smartphone privacy guide or individual law could ever hope to keep up with. Similarly, framing rampant privacy abuses as a problem we each have to be responsible for addressing individually seems a lot like framing the plastic pollution crisis as something Americans could have somehow solved by recycling. It’s both a misdirection and a misunderstanding of the problem.&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;It’s to his credit, then, that Lowry Pressly doesn’t include a “What is to be done” section at the end of &lt;em&gt;The Right to Oblivion: Privacy and the Good Life&lt;/em&gt;. In lieu of offering up any concrete technical or political solutions, he simply reiterates an argument he has carefully and convincingly built over the course of his book: that privacy is important “not because it empowers us to exercise control over our information, but because it protects against the creation of such information in the first place.”&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image alignright size-large is-resized"&gt;&lt;img alt="cover of The Right to Oblivion" class="wp-image-1118666" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/book.pressly.jpg?w=800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;strong&gt;The Right to Oblivion: Privacy and the Good Life&lt;/strong&gt;&lt;br /&gt;Lowry Pressly&lt;/figcaption&gt;&lt;div class="image-credit"&gt;HARVARD UNIVERSITY PRESS, 2024&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;For Pressly, a Stanford instructor, the way we currently understand and value privacy has been tainted by what he calls “the ideology of information.” “This is the idea that information has a natural existence in human affairs,” he writes, “and that there are no aspects of human life which cannot be translated somehow into data.” This way of thinking not only leads to an impoverished sense of our own humanity—it also forces us into the conceptual trap of debating privacy’s value using a framework (control, consent, access) established by the companies whose business model is to exploit it.&lt;/p&gt;  &lt;p&gt;The way out of this trap is to embrace what Pressly calls “oblivion,” a kind of state of unknowing, ambiguity, and potential—or, as he puts it, a realm “where there is no information or knowledge one way or the other.” While he understands that it’s impossible to fully escape a modern world intent on turning us into data subjects, Pressly’s book suggests we can and should support the idea that certain aspects of our (and others’) subjective interior lives can never be captured by information. Privacy is important because it helps to both protect and produce these ineffable parts of our lives, which in turn gives them a sense of dignity, depth, and the possibility for change and surprise.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Reserving or cultivating a space for oblivion in our own lives means resisting the logic that drives much of the modern world. Our inclination to “join the conversation,” share our thoughts, and do whatever it is we do when we create and curate a personal brand has become so normalized that it’s practically invisible to us. According to Pressly, all that effort has only made our lives and relationships shallower, less meaningful, and less trusting.&lt;/p&gt; 
 &lt;p&gt;Calls for putting our screens down and stepping away from the internet are certainly nothing new. And while &lt;em&gt;The Right to Oblivion&lt;/em&gt; isn’t necessarily prescriptive about such things, Pressly does offer a beautiful and compelling vision of what can be gained when we retreat not just from the digital world but from the idea that we are somehow knowable to that world in any authentic or meaningful way.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;If all this sounds a bit philosophical, well, it is. But it would be a mistake to think of &lt;em&gt;The Right to Oblivion &lt;/em&gt;as a mere thought exercise on privacy. Part of what makes the book so engaging and persuasive is the way in which Pressly combines a philosopher’s knack for uncovering hidden assumptions with a historian’s interest in and sensitivity to older (often abandoned) ways of thinking, and how they can often enlighten and inform modern problems.&lt;/p&gt;  &lt;p&gt;Pressly isn’t against efforts to pass more robust privacy legislation, or even to learn how to better protect our devices against surveillance. His argument is that in order to guide such efforts, you have to both ask the right questions and frame the problem in a way that gives you and others the moral clarity and urgency to act. Your phone’s privacy settings are important, but so is understanding what you’re protecting when you change them.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Bryan Gardiner is a writer based in Oakland, California.&amp;nbsp;&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;Privacy only matters to those with something to hide&lt;/em&gt;. So goes one of the more inane and disingenuous justifications for mass government and corporate surveillance. There are others, of course, but the “nothing to hide” argument remains a popular way to rationalize or excuse what’s become standard practice in our digital age: the widespread and invasive collection of vast amounts of personal data.&lt;/p&gt;  &lt;p&gt;One common response to this line of reasoning is that &lt;em&gt;everyone&lt;/em&gt;, in fact, has something to hide, whether they realize it or not. If you’re unsure of whether this holds true for you, I encourage you to read &lt;em&gt;Means of Control&lt;/em&gt; by Byron Tau.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image alignright size-large is-resized"&gt;&lt;img alt="cover of Means of Control" class="wp-image-1118667" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/book.tau_.jpg?w=1325" width="1325" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;strong&gt;Means of Control: How the Hidden Alliance of Tech and Government Is Creating a New American Surveillance State&lt;/strong&gt;&lt;br /&gt;Byron Tau&lt;/figcaption&gt;&lt;div class="image-credit"&gt;CROWN, 2024&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Midway through his book, Tau, an investigative journalist, recalls meeting with a disgruntled former employee of a data broker—a shady company that collects, bundles, and sells your personal data to other (often shadier) third parties, including the government. This ex-employee had managed to make off with several gigabytes of location data representing the precise movements of tens of thousands of people over the course of a few weeks. “What could I learn with this [data]—­&lt;em&gt;theoretically&lt;/em&gt;?” Tau asks the former employee. The answer includes a laundry list of possibilities that I suspect would make even the most enthusiastic oversharer uncomfortable.&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;“If information is power, and America is a society that’s still interested in the guarantee of liberty, personal dignity, and the individual freedom of its citizens, a serious conversation is needed.”&lt;/strong&gt;&lt;/p&gt; &lt;cite&gt;Bryon Tau, author of &lt;em&gt;Means of Control&lt;/em&gt;&lt;/cite&gt;&lt;/blockquote&gt;  &lt;p&gt;Did someone in this group recently visit an abortion clinic? That would be easy to figure out, says the ex-employee. Anyone attend an AA meeting or check into inpatient drug rehab? Again, pretty simple to discern. Is someone being treated for erectile dysfunction at a sexual health clinic? If so, that would probably be gleanable from the data too. Tau never opts to go down that road, but as &lt;em&gt;Means of Control&lt;/em&gt; makes very clear, others certainly have done so and will.&lt;/p&gt; 
 &lt;p&gt;While most of us are at least vaguely aware that our phones and apps are a vector for data collection and tracking, both the way in which this is accomplished and the extent to which it happens often remain murky. Purposely so, argues Tau. In fact, one of the great myths &lt;em&gt;Means of Control&lt;/em&gt; takes aim at is the very idea that what we do with our devices can ever truly be anonymized. Each of us has habits and routines that are completely unique, he says, and if an advertiser knows you only as an alphanumeric string provided by your phone as you move about the world, and not by your real name, that still offers you virtually no real privacy protection. (You’ll perhaps not be surprised to learn that such “anonymized ad IDs” are relatively easy to crack.)&lt;/p&gt;  &lt;p&gt;“I’m here to tell you if you’ve ever been on a dating app that wanted your location, or if you ever granted a weather app permission to know where you are 24/7, there’s a good chance a detailed log of your precise movement patterns has been vacuumed up and saved in some data bank somewhere that tens of thousands of total strangers have access to,” writes Tau.&lt;/p&gt; 
 &lt;p&gt;Unraveling the story of how these strangers—everyone from government intelligence agents and local law enforcement officers to private investigators and employees of ad tech companies—gained access to our personal information is the ambitious task Tau sets for himself, and he begins where you might expect: the immediate aftermath of 9/11.&lt;/p&gt;  &lt;p&gt;At no other point in US history was the government’s appetite for data more voracious than in the days after the attacks, says Tau. It was a hunger that just so happened to coincide with the advent of new technologies, devices, and platforms that excelled at harvesting and serving up personal information that had zero legal privacy protections.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Over the course of 22 chapters, Tau gives readers a rare glimpse inside the shadowy industry, “built by corporate America and blessed by government lawyers,” that emerged in the years and decades following the 9/11 attacks. In the hands of a less skilled reporter, this labyrinthine world of shell companies, data vendors, and intelligence agencies could easily become overwhelming or incomprehensible. But Tau goes to great lengths to connect dots and plots, explaining how a perfect storm of business motivations, technological breakthroughs, government paranoia, and lax or nonexistent privacy laws combined to produce the “digital panopticon” we are all now living in.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Means of Control&lt;/em&gt; doesn’t offer much comfort or reassurance for privacy­-minded readers, but that’s arguably the point. As Tau notes repeatedly throughout his book, this now massive system of persistent and ubiquitous surveillance works only because the public is largely unaware of it. “If information is power, and America is a society that’s still interested in the guarantee of liberty, personal dignity, and the individual freedom of its citizens, a serious conversation is needed,” he writes.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;As another new book makes clear, this conversation also needs to include student data. Lindsay Weinberg’s &lt;em&gt;Smart University: Student Surveillance in the Digital Age&lt;/em&gt; reveals how the motivations and interests of Big Tech are transforming higher education in ways that are increasingly detrimental to student privacy and, arguably, education as a whole.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image alignright size-large is-resized"&gt;&lt;img alt="cover of Smart University" class="wp-image-1118668" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/book.weinberg.jpg?w=1243" width="1243" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;strong&gt;Smart University: Student Surveillance in the Digital Age&lt;/strong&gt;&lt;br /&gt;Lindsay Weinberg&lt;/figcaption&gt;&lt;div class="image-credit"&gt;JOHNS HOPKINS UNIVERSITY PRESS, 2024&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;By “smart university,” Weinberg means the growing number of public universities across the country that are being restructured around “the production and capture of digital data.” Similar in vision and application to so-called “smart cities,” these big-data-pilled institutions are increasingly turning to technologies that can track students’ movements around campus, monitor how much time they spend on learning management systems, flag those who seem to need special “advising,” and “nudge” others toward specific courses and majors. “What makes these digital technologies so seductive to higher education administrators, in addition to promises of cost cutting, individualized student services, and improved school rankings, is the notion that the integration of digital technology on their campuses will position universities to keep pace with technological innovation,” Weinberg writes.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Readers of &lt;em&gt;Smart University&lt;/em&gt; will likely recognize a familiar logic at play here. Driving many of these academic tracking and data-gathering initiatives is a growing obsession with efficiency, productivity, and convenience. The result is a kind of Silicon Valley optimization mindset, but applied to higher education at scale. Get students in and out of university as fast as possible, minimize attrition, relentlessly track performance, and do it all under the guise of campus modernization and increased personalization.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Under this emerging system, students are viewed less as self-empowered individuals and more as “consumers to be courted, future workers to be made employable for increasingly smart workplaces, sources of user-generated content for marketing and outreach, and resources to be mined for making campuses even smarter,” writes Weinberg.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;At the heart of &lt;em&gt;Smart University&lt;/em&gt; seems to be a relatively straightforward question: What is an education for? Although Weinberg doesn’t provide a direct answer, she shows that how a university (or society) decides to answer that question can have profound impacts on how it treats its students and teachers. Indeed, as the goal of education becomes less to produce well-rounded humans capable of thinking critically and more to produce “data subjects capable of being managed and who can fill roles in the digital economy,” it’s no wonder we’re increasingly turning to the dumb idea of smart universities to get the job done.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;If books like &lt;em&gt;Means of Control&lt;/em&gt; and &lt;em&gt;Smart University&lt;/em&gt; do an excellent job exposing the extent to which our privacy has been compromised, commodified, and weaponized (which they undoubtedly do), they can also start to feel a bit predictable in their final chapters. Familiar codas include calls for collective action, buttressed by a hopeful anecdote or two detailing previously successful pro-privacy wins; nods toward a bipartisan privacy bill in the works or other pieces of legislation that could potentially close some glaring surveillance loophole; and, most often, technical guides that explain how each of us, individually, might better secure or otherwise take control and “ownership” of our personal data.&lt;/p&gt;  &lt;p&gt;The motivations behind these exhortations and privacy-centric how-to guides are understandable. After all, it’s natural for readers to want answers, advice, or at least some suggestion that things&lt;em&gt; could&lt;/em&gt; be different—especially after reading about the growing list of degradations suffered under surveillance capitalism. But it doesn’t take a skeptic to start to wonder if they’re actually advancing the fight for privacy in the way that its advocates truly want.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;For one thing, technology tends to move much faster than any one smartphone privacy guide or individual law could ever hope to keep up with. Similarly, framing rampant privacy abuses as a problem we each have to be responsible for addressing individually seems a lot like framing the plastic pollution crisis as something Americans could have somehow solved by recycling. It’s both a misdirection and a misunderstanding of the problem.&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;It’s to his credit, then, that Lowry Pressly doesn’t include a “What is to be done” section at the end of &lt;em&gt;The Right to Oblivion: Privacy and the Good Life&lt;/em&gt;. In lieu of offering up any concrete technical or political solutions, he simply reiterates an argument he has carefully and convincingly built over the course of his book: that privacy is important “not because it empowers us to exercise control over our information, but because it protects against the creation of such information in the first place.”&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image alignright size-large is-resized"&gt;&lt;img alt="cover of The Right to Oblivion" class="wp-image-1118666" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/book.pressly.jpg?w=800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;strong&gt;The Right to Oblivion: Privacy and the Good Life&lt;/strong&gt;&lt;br /&gt;Lowry Pressly&lt;/figcaption&gt;&lt;div class="image-credit"&gt;HARVARD UNIVERSITY PRESS, 2024&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;For Pressly, a Stanford instructor, the way we currently understand and value privacy has been tainted by what he calls “the ideology of information.” “This is the idea that information has a natural existence in human affairs,” he writes, “and that there are no aspects of human life which cannot be translated somehow into data.” This way of thinking not only leads to an impoverished sense of our own humanity—it also forces us into the conceptual trap of debating privacy’s value using a framework (control, consent, access) established by the companies whose business model is to exploit it.&lt;/p&gt;  &lt;p&gt;The way out of this trap is to embrace what Pressly calls “oblivion,” a kind of state of unknowing, ambiguity, and potential—or, as he puts it, a realm “where there is no information or knowledge one way or the other.” While he understands that it’s impossible to fully escape a modern world intent on turning us into data subjects, Pressly’s book suggests we can and should support the idea that certain aspects of our (and others’) subjective interior lives can never be captured by information. Privacy is important because it helps to both protect and produce these ineffable parts of our lives, which in turn gives them a sense of dignity, depth, and the possibility for change and surprise.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Reserving or cultivating a space for oblivion in our own lives means resisting the logic that drives much of the modern world. Our inclination to “join the conversation,” share our thoughts, and do whatever it is we do when we create and curate a personal brand has become so normalized that it’s practically invisible to us. According to Pressly, all that effort has only made our lives and relationships shallower, less meaningful, and less trusting.&lt;/p&gt; 
 &lt;p&gt;Calls for putting our screens down and stepping away from the internet are certainly nothing new. And while &lt;em&gt;The Right to Oblivion&lt;/em&gt; isn’t necessarily prescriptive about such things, Pressly does offer a beautiful and compelling vision of what can be gained when we retreat not just from the digital world but from the idea that we are somehow knowable to that world in any authentic or meaningful way.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;If all this sounds a bit philosophical, well, it is. But it would be a mistake to think of &lt;em&gt;The Right to Oblivion &lt;/em&gt;as a mere thought exercise on privacy. Part of what makes the book so engaging and persuasive is the way in which Pressly combines a philosopher’s knack for uncovering hidden assumptions with a historian’s interest in and sensitivity to older (often abandoned) ways of thinking, and how they can often enlighten and inform modern problems.&lt;/p&gt;  &lt;p&gt;Pressly isn’t against efforts to pass more robust privacy legislation, or even to learn how to better protect our devices against surveillance. His argument is that in order to guide such efforts, you have to both ask the right questions and frame the problem in a way that gives you and others the moral clarity and urgency to act. Your phone’s privacy settings are important, but so is understanding what you’re protecting when you change them.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Bryan Gardiner is a writer based in Oakland, California.&amp;nbsp;&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/23/1118401/privacy-book-reviews-surveillance-higher-education/</guid><pubDate>Mon, 23 Jun 2025 10:00:00 +0000</pubDate></item><item><title>The Download: the Vera C. Rubin Observatory’s first pictures, and reframing privacy (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/23/1119166/the-download-the-vera-c-rubin-observatorys-first-pictures-and-reframing-privacy/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;See the stunning first images from the Vera C. Rubin Observatory&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;The first spectacular images taken by the Vera C. Rubin Observatory have been released for the world to peruse: a panoply of iridescent galaxies and shimmering nebulas.&lt;/p&gt;&lt;p&gt;Much has been written about the observatory’s grand promise: to revolutionize our understanding of the cosmos by revealing a once-hidden population of far-flung galaxies, erupting stars, interstellar objects, and elusive planets. And thanks to its unparalleled technical prowess, few doubted its ability to make good on that. But over the past decade, during its lengthy construction period, everything’s been in the abstract.&lt;br /&gt;&lt;/p&gt;  &lt;p&gt;Today, that promise has become a staggeringly beautiful reality. Read the full story.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;—Robin George Andrews&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Back in January, we selected the &lt;/strong&gt;&lt;strong&gt;Vera C. Rubin Observatory&lt;/strong&gt;&lt;strong&gt; as one of our 10 Breakthrough Technologies of 2025. &lt;/strong&gt;&lt;strong&gt;Read more&lt;/strong&gt;&lt;strong&gt; about why it’s such a promising tool for enhancing our understanding of the universe.&lt;/strong&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Why we need to think differently about privacy&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Privacy only matters to those with something to hide&lt;/em&gt;. So goes one of the more inane and disingenuous justifications for mass government and corporate surveillance. It remains a popular way to rationalize or excuse what’s become standard practice in our digital age: the widespread and invasive collection of vast amounts of personal data.&lt;br /&gt;&lt;/p&gt;  &lt;p&gt;One common response to this line of reasoning is that everyone, in fact, has something to hide, whether they realize it or not. If you’re unsure of whether this holds true for you, three new books examine the rise of the surveillance state, its infiltration of higher education, and why we need a new framework for thinking about privacy. Read the full story.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;em&gt;—Bryan Gardiner&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story is from the next print edition of MIT Technology Review, which explores power—who has it, and who wants it. It’s set to go live this Wednesday, so &lt;/strong&gt;&lt;strong&gt;subscribe &amp;amp; save 25%&lt;/strong&gt;&lt;strong&gt; to read it and get a copy of the issue when it lands!&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Satellite images reveal the damage America’s bombs caused in Iran&lt;br /&gt;&lt;/strong&gt;The attack focused on three nuclear sites in the country. (Wired $)&amp;nbsp;&lt;br /&gt;+ &lt;em&gt;Iran has insisted its nuclear program will not be stopped. &lt;/em&gt;(The Guardian)&lt;br /&gt;+ &lt;em&gt;Here’s how the US bunker-busting bombs work. &lt;/em&gt;(Economist $)&lt;br /&gt;+ &lt;em&gt;The risks of a nuclear accident appear low, for now. &lt;/em&gt;(New Scientist $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 Tesla has launched its Texas robotaxi service&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;But for now, at least, it’s pretty restricted. (NYT $)&lt;br /&gt;+ &lt;em&gt;Elon Musk says the firm is “super paranoid about safety.” &lt;/em&gt;(WP $)&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;+ &lt;em&gt;But there’s still plenty of unanswered questions around how it’ll work. &lt;/em&gt;(TechCrunch)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 OpenAI and Jony Ive's startup are facing a trademark dispute&lt;/strong&gt;&lt;br /&gt;It appears to be over their use of the IO name. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;OpenAI has scrubbed all mention of the partnership online. &lt;/em&gt;(Insider $)&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;4 Meta is throwing tens of millions of dollars at top AI talent&lt;/strong&gt;&lt;br /&gt;Mark Zuckerberg is on a personal mission to recruit for its Superintelligence lab. (WSJ $)&lt;br /&gt;+ &lt;em&gt;Alexandr Wang of Scale will lead the charge. &lt;/em&gt;(Fortune $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 Elon Musk wants to retrain xAI’s Grok&lt;/strong&gt;&lt;br /&gt;Foundation AI models contain too much garbage, apparently. (Insider $)&lt;br /&gt;+ &lt;em&gt;Investors aren’t keen to sink money into xAI. &lt;/em&gt;(Reuters)&lt;br /&gt;+ &lt;em&gt;Why does AI hallucinate? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 Donald Trump’s phone network is based in Florida&lt;/strong&gt;&lt;br /&gt;Seven-year old Liberty Mobile Wireless buys network capacity from bigger players. (FT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Reddit is reportedly considering using World ID to verify users&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;The controversial firm claims to preserve users’ anonymity while also confirming they are human. (Semafor)&lt;br /&gt;+ &lt;em&gt;How the startup recruited its first half a million test users. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;8 What happens inside the phones of 25 teenagers&lt;br /&gt;&lt;/strong&gt;Life isn’t always easy for the first generation of social media natives. (The Guardian)&lt;br /&gt;+ &lt;em&gt;What it’s like to have never owned a smartphone. &lt;/em&gt;(The Atlantic $)&lt;br /&gt;+ &lt;em&gt;How to log off. &lt;/em&gt;(MIT Technology Review)&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;&lt;br /&gt;&lt;strong&gt;9 A dead NASA satellite let off a powerful radio pulse 🛰️&lt;/strong&gt;&lt;br /&gt;So powerful, it briefly outshone everything else in the sky. (New Scientist $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 What does AI mean for the future of fonts?&lt;/strong&gt;&lt;br /&gt;They could eventually swim into focus, or shift during the day. (The Verge)&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p class="has-large-font-size"&gt;&lt;strong&gt;“We’re not playing a kid’s game here. We’re not naming Care Bears.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Ira Winkler, chief information security officer at cybersecurity firm CYE Security, decries cybersecurity’s obsession with cutesy names to the Wall Street Journal.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; 
 &lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfzoKqjG7gmx1bVFSE0SQpM-crPzMaFnxidWYr5hHkvgL1rs7hAAOkUR9If-oti6IrKz-1sIXcGf_PtT0iBJ3EZ_zqDCEt-tEble1Xi_eBoHwCutP9BsngJaxa4u20p88JuB_V8sw?key=CUdjYIaGSGOIzTzw8vI_Sg" /&gt;&lt;/figure&gt;    &lt;p&gt;&lt;strong&gt;What is death?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Just as birth certificates note the time we enter the world, death certificates mark the moment we exit it. This practice reflects traditional notions about life and death as binaries. We are here until, suddenly, like a light switched off, we are gone.&lt;/p&gt;&lt;p&gt;But while this idea of death is pervasive, evidence is building that it is an outdated social construct, not really grounded in biology. Dying is in fact a process—one with no clear point demarcating the threshold across which someone cannot come back.&lt;/p&gt;&lt;p&gt;Scientists and many doctors have already embraced this more nuanced understanding of death. And as society catches up, the implications for the living could be profound. Read the full story.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Rachel Nuwer&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;+ Have you booked your tickets to 28 Years Later yet?&lt;br /&gt;+ If you happen to be planning a flying visit to Rome, here’s a guide to cramming in as much of its breathtaking art as possible.&lt;br /&gt;+ What community gardens can give us.&lt;br /&gt;+ Who really runs New York? The bodega cats ($)&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;See the stunning first images from the Vera C. Rubin Observatory&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;The first spectacular images taken by the Vera C. Rubin Observatory have been released for the world to peruse: a panoply of iridescent galaxies and shimmering nebulas.&lt;/p&gt;&lt;p&gt;Much has been written about the observatory’s grand promise: to revolutionize our understanding of the cosmos by revealing a once-hidden population of far-flung galaxies, erupting stars, interstellar objects, and elusive planets. And thanks to its unparalleled technical prowess, few doubted its ability to make good on that. But over the past decade, during its lengthy construction period, everything’s been in the abstract.&lt;br /&gt;&lt;/p&gt;  &lt;p&gt;Today, that promise has become a staggeringly beautiful reality. Read the full story.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;—Robin George Andrews&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Back in January, we selected the &lt;/strong&gt;&lt;strong&gt;Vera C. Rubin Observatory&lt;/strong&gt;&lt;strong&gt; as one of our 10 Breakthrough Technologies of 2025. &lt;/strong&gt;&lt;strong&gt;Read more&lt;/strong&gt;&lt;strong&gt; about why it’s such a promising tool for enhancing our understanding of the universe.&lt;/strong&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Why we need to think differently about privacy&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Privacy only matters to those with something to hide&lt;/em&gt;. So goes one of the more inane and disingenuous justifications for mass government and corporate surveillance. It remains a popular way to rationalize or excuse what’s become standard practice in our digital age: the widespread and invasive collection of vast amounts of personal data.&lt;br /&gt;&lt;/p&gt;  &lt;p&gt;One common response to this line of reasoning is that everyone, in fact, has something to hide, whether they realize it or not. If you’re unsure of whether this holds true for you, three new books examine the rise of the surveillance state, its infiltration of higher education, and why we need a new framework for thinking about privacy. Read the full story.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;em&gt;—Bryan Gardiner&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story is from the next print edition of MIT Technology Review, which explores power—who has it, and who wants it. It’s set to go live this Wednesday, so &lt;/strong&gt;&lt;strong&gt;subscribe &amp;amp; save 25%&lt;/strong&gt;&lt;strong&gt; to read it and get a copy of the issue when it lands!&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Satellite images reveal the damage America’s bombs caused in Iran&lt;br /&gt;&lt;/strong&gt;The attack focused on three nuclear sites in the country. (Wired $)&amp;nbsp;&lt;br /&gt;+ &lt;em&gt;Iran has insisted its nuclear program will not be stopped. &lt;/em&gt;(The Guardian)&lt;br /&gt;+ &lt;em&gt;Here’s how the US bunker-busting bombs work. &lt;/em&gt;(Economist $)&lt;br /&gt;+ &lt;em&gt;The risks of a nuclear accident appear low, for now. &lt;/em&gt;(New Scientist $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 Tesla has launched its Texas robotaxi service&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;But for now, at least, it’s pretty restricted. (NYT $)&lt;br /&gt;+ &lt;em&gt;Elon Musk says the firm is “super paranoid about safety.” &lt;/em&gt;(WP $)&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;+ &lt;em&gt;But there’s still plenty of unanswered questions around how it’ll work. &lt;/em&gt;(TechCrunch)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 OpenAI and Jony Ive's startup are facing a trademark dispute&lt;/strong&gt;&lt;br /&gt;It appears to be over their use of the IO name. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;OpenAI has scrubbed all mention of the partnership online. &lt;/em&gt;(Insider $)&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;4 Meta is throwing tens of millions of dollars at top AI talent&lt;/strong&gt;&lt;br /&gt;Mark Zuckerberg is on a personal mission to recruit for its Superintelligence lab. (WSJ $)&lt;br /&gt;+ &lt;em&gt;Alexandr Wang of Scale will lead the charge. &lt;/em&gt;(Fortune $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 Elon Musk wants to retrain xAI’s Grok&lt;/strong&gt;&lt;br /&gt;Foundation AI models contain too much garbage, apparently. (Insider $)&lt;br /&gt;+ &lt;em&gt;Investors aren’t keen to sink money into xAI. &lt;/em&gt;(Reuters)&lt;br /&gt;+ &lt;em&gt;Why does AI hallucinate? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 Donald Trump’s phone network is based in Florida&lt;/strong&gt;&lt;br /&gt;Seven-year old Liberty Mobile Wireless buys network capacity from bigger players. (FT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Reddit is reportedly considering using World ID to verify users&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;The controversial firm claims to preserve users’ anonymity while also confirming they are human. (Semafor)&lt;br /&gt;+ &lt;em&gt;How the startup recruited its first half a million test users. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;8 What happens inside the phones of 25 teenagers&lt;br /&gt;&lt;/strong&gt;Life isn’t always easy for the first generation of social media natives. (The Guardian)&lt;br /&gt;+ &lt;em&gt;What it’s like to have never owned a smartphone. &lt;/em&gt;(The Atlantic $)&lt;br /&gt;+ &lt;em&gt;How to log off. &lt;/em&gt;(MIT Technology Review)&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;&lt;br /&gt;&lt;strong&gt;9 A dead NASA satellite let off a powerful radio pulse 🛰️&lt;/strong&gt;&lt;br /&gt;So powerful, it briefly outshone everything else in the sky. (New Scientist $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 What does AI mean for the future of fonts?&lt;/strong&gt;&lt;br /&gt;They could eventually swim into focus, or shift during the day. (The Verge)&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p class="has-large-font-size"&gt;&lt;strong&gt;“We’re not playing a kid’s game here. We’re not naming Care Bears.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Ira Winkler, chief information security officer at cybersecurity firm CYE Security, decries cybersecurity’s obsession with cutesy names to the Wall Street Journal.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; 
 &lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfzoKqjG7gmx1bVFSE0SQpM-crPzMaFnxidWYr5hHkvgL1rs7hAAOkUR9If-oti6IrKz-1sIXcGf_PtT0iBJ3EZ_zqDCEt-tEble1Xi_eBoHwCutP9BsngJaxa4u20p88JuB_V8sw?key=CUdjYIaGSGOIzTzw8vI_Sg" /&gt;&lt;/figure&gt;    &lt;p&gt;&lt;strong&gt;What is death?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Just as birth certificates note the time we enter the world, death certificates mark the moment we exit it. This practice reflects traditional notions about life and death as binaries. We are here until, suddenly, like a light switched off, we are gone.&lt;/p&gt;&lt;p&gt;But while this idea of death is pervasive, evidence is building that it is an outdated social construct, not really grounded in biology. Dying is in fact a process—one with no clear point demarcating the threshold across which someone cannot come back.&lt;/p&gt;&lt;p&gt;Scientists and many doctors have already embraced this more nuanced understanding of death. And as society catches up, the implications for the living could be profound. Read the full story.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Rachel Nuwer&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;+ Have you booked your tickets to 28 Years Later yet?&lt;br /&gt;+ If you happen to be planning a flying visit to Rome, here’s a guide to cramming in as much of its breathtaking art as possible.&lt;br /&gt;+ What community gardens can give us.&lt;br /&gt;+ Who really runs New York? The bodega cats ($)&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/23/1119166/the-download-the-vera-c-rubin-observatorys-first-pictures-and-reframing-privacy/</guid><pubDate>Mon, 23 Jun 2025 12:10:00 +0000</pubDate></item><item><title>[NEW] Google adds AI features to Chromebook Plus devices (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/23/google-adds-ai-features-to-chromebook-plus-devices/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google’s adding a slew of AI features to its productivity-focused Chromebook Plus line of devices, including a screen-selection tool for search and text capture, a tool that explains complex text, and NotebookLM.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new screen-selection tool works similarly to Google Lens and the “Circle to search” feature in Chrome on smartphones: long-press the launcher button or use the screenshot tool to select what is on your screen, and Google will instantly search for it. The tool also lets you select text and quickly add an event to the calendar.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3021092" height="425" src="https://techcrunch.com/wp-content/uploads/2025/06/Chromebook-select.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, Google added a new “quick insert” key to some Chromebook Plus models that lets you trigger shortcuts. Users can now access Google’s AI image-generation features with this key, as well as AI-powered writing tools.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Meanwhile, the new “Simplify” feature lets you use AI to explain, simplify, or summarize any text you have selected.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google’s also giving all new Chromebook Plus users one year’s subscription to the Google AI Pro plan, which includes access to the Gemini app; video editing tool Flow; image-to-video creation tool Whisk; Gemini in Gmail, Docs, and Chrome; and 2TB of storage. The AI Pro plan otherwise costs $240 per year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside these features, Google is launching two exclusive AI features for Lenovo’s new Chromebook Plus 14: One uses AI to recommend tab and document organization, while the second lets users edit images using the Gallery app to remove backgrounds or make stickers. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lenovo’s new Chromebook comes with an OLED touch screen and runs on an ARM-based MediaTek Kompanio Ultra chip. It comes in 12GB and 16GB RAM variants, costing $649 and $749, respectively.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google’s adding a slew of AI features to its productivity-focused Chromebook Plus line of devices, including a screen-selection tool for search and text capture, a tool that explains complex text, and NotebookLM.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new screen-selection tool works similarly to Google Lens and the “Circle to search” feature in Chrome on smartphones: long-press the launcher button or use the screenshot tool to select what is on your screen, and Google will instantly search for it. The tool also lets you select text and quickly add an event to the calendar.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3021092" height="425" src="https://techcrunch.com/wp-content/uploads/2025/06/Chromebook-select.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, Google added a new “quick insert” key to some Chromebook Plus models that lets you trigger shortcuts. Users can now access Google’s AI image-generation features with this key, as well as AI-powered writing tools.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Meanwhile, the new “Simplify” feature lets you use AI to explain, simplify, or summarize any text you have selected.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google’s also giving all new Chromebook Plus users one year’s subscription to the Google AI Pro plan, which includes access to the Gemini app; video editing tool Flow; image-to-video creation tool Whisk; Gemini in Gmail, Docs, and Chrome; and 2TB of storage. The AI Pro plan otherwise costs $240 per year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside these features, Google is launching two exclusive AI features for Lenovo’s new Chromebook Plus 14: One uses AI to recommend tab and document organization, while the second lets users edit images using the Gallery app to remove backgrounds or make stickers. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lenovo’s new Chromebook comes with an OLED touch screen and runs on an ARM-based MediaTek Kompanio Ultra chip. It comes in 12GB and 16GB RAM variants, costing $649 and $749, respectively.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/23/google-adds-ai-features-to-chromebook-plus-devices/</guid><pubDate>Mon, 23 Jun 2025 13:00:00 +0000</pubDate></item><item><title>[NEW] Scaling integrated digital health (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/23/1119009/scaling-integrated-digital-health/</link><description>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;Sponsored by&lt;/span&gt;Roche&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Around the world, countries are facing the challenges of aging populations, growing rates of chronic disease, and workforce shortages, leading to a growing burden on health care systems. From diagnosis to treatment, AI and other digital solutions can enhance the efficiency and effectiveness of health care, easing the burden on straining systems. According to the World Health Organization (WHO), spending an additional $0.24 per patient per year on digital health interventions could save more than two million lives from non-communicable diseases over the next decade.&lt;/p&gt;  &lt;p&gt;To work most effectively, digital solutions need to be scaled and embedded in an ecosystem that ensures a high degree of interoperability, data security, and governance. If not, the proliferation of point solutions— where specialized software or tools focus on just one specific area or function—could lead to silos and digital canyons, complicating rather than easing the workloads of health care professionals, and potentially impacting patient treatment. Importantly, technologies that enhance workforce productivity should keep humans in the loop, aiming to augment their capabilities, rather than replace them.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;p&gt;Through a survey of 300 health care executives and a program of interviews with industry experts, startup leaders, and academic researchers, this report explores the best practices for success when implementing integrated digital solutions into health care, and how these can support decision-makers in a range of settings, including laboratories and hospitals.&amp;nbsp;&lt;/p&gt;  &lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-1119012" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/MITTR-Roche25_V17_CoverPage.png?w=1555" width="1555" /&gt;&lt;/figure&gt;    &lt;p&gt;&lt;br /&gt;Key findings include:&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;br /&gt;&lt;strong&gt;Health care is primed for digital adoption. &lt;/strong&gt;The global pandemic underscored the benefits of value-based care&amp;nbsp;and accelerated the adoption of digital and AI-powered technologies in health care. Overwhelmingly, 96% of the survey respondents say they are “ready and resourced” to use digital health, while one in four say they are “very ready.” However, 91% of executives agree interoperability is a challenge, with a majority (59%) saying it will be “tough” to solve. Two in five leaders say balancing security with usability is the biggest challenge for digital health. With the adoption of cloud solutions, organizations can enjoy the benefits of modernized IT infrastructure: 36% of the survey respondents believe scalability is the main benefit, followed by improved security (28%).&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Digital health care can help health care institutions transform patient outcomes—if built on the right foundations. &lt;/strong&gt;Solutions like AI-powered diagnostics, telemedicine, and remote monitoring can offer measurable impact across the patient journey, from improving early disease detection to reducing hospital readmission rates. However, these technologies can only support fully connected health care when scaled up and embedded in ecosystems with robust data governance, interoperability, and security.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Health care data has immense potential—but fragmentation and poor interoperability hinder impact. &lt;/strong&gt;Health care systems generate vast quantities of data, yet much of it remains siloed or unusable due to inconsistent formats and incompatible IT systems, limiting scalability.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Digital tools must augment, not&amp;nbsp;overload, the workforce. &lt;/strong&gt;With global health care workforce shortages worsening, digital solutions like clinical decision support tools, patient prediction, and remote monitoring can be seen as essential aids rather than threats to the workforce. Successful deployment depends on usability, clinician engagement, and training.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Regulatory evolution, open data policies, and economic sustainability are key to scaling digital health. &lt;/strong&gt;Even the best digital tools struggle to scale without reimbursement frameworks, regulatory support, and viable business models. Open data ecosystems are needed to unleash the clinical and economic value of innovation. Regulatory and reimbursement innovation is also critical to transitioning from pilot projects to high-impact, system-wide adoption.&lt;/p&gt;  &lt;p&gt;Download the full report.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;Sponsored by&lt;/span&gt;Roche&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Around the world, countries are facing the challenges of aging populations, growing rates of chronic disease, and workforce shortages, leading to a growing burden on health care systems. From diagnosis to treatment, AI and other digital solutions can enhance the efficiency and effectiveness of health care, easing the burden on straining systems. According to the World Health Organization (WHO), spending an additional $0.24 per patient per year on digital health interventions could save more than two million lives from non-communicable diseases over the next decade.&lt;/p&gt;  &lt;p&gt;To work most effectively, digital solutions need to be scaled and embedded in an ecosystem that ensures a high degree of interoperability, data security, and governance. If not, the proliferation of point solutions— where specialized software or tools focus on just one specific area or function—could lead to silos and digital canyons, complicating rather than easing the workloads of health care professionals, and potentially impacting patient treatment. Importantly, technologies that enhance workforce productivity should keep humans in the loop, aiming to augment their capabilities, rather than replace them.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;p&gt;Through a survey of 300 health care executives and a program of interviews with industry experts, startup leaders, and academic researchers, this report explores the best practices for success when implementing integrated digital solutions into health care, and how these can support decision-makers in a range of settings, including laboratories and hospitals.&amp;nbsp;&lt;/p&gt;  &lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-1119012" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/MITTR-Roche25_V17_CoverPage.png?w=1555" width="1555" /&gt;&lt;/figure&gt;    &lt;p&gt;&lt;br /&gt;Key findings include:&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;br /&gt;&lt;strong&gt;Health care is primed for digital adoption. &lt;/strong&gt;The global pandemic underscored the benefits of value-based care&amp;nbsp;and accelerated the adoption of digital and AI-powered technologies in health care. Overwhelmingly, 96% of the survey respondents say they are “ready and resourced” to use digital health, while one in four say they are “very ready.” However, 91% of executives agree interoperability is a challenge, with a majority (59%) saying it will be “tough” to solve. Two in five leaders say balancing security with usability is the biggest challenge for digital health. With the adoption of cloud solutions, organizations can enjoy the benefits of modernized IT infrastructure: 36% of the survey respondents believe scalability is the main benefit, followed by improved security (28%).&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Digital health care can help health care institutions transform patient outcomes—if built on the right foundations. &lt;/strong&gt;Solutions like AI-powered diagnostics, telemedicine, and remote monitoring can offer measurable impact across the patient journey, from improving early disease detection to reducing hospital readmission rates. However, these technologies can only support fully connected health care when scaled up and embedded in ecosystems with robust data governance, interoperability, and security.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Health care data has immense potential—but fragmentation and poor interoperability hinder impact. &lt;/strong&gt;Health care systems generate vast quantities of data, yet much of it remains siloed or unusable due to inconsistent formats and incompatible IT systems, limiting scalability.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Digital tools must augment, not&amp;nbsp;overload, the workforce. &lt;/strong&gt;With global health care workforce shortages worsening, digital solutions like clinical decision support tools, patient prediction, and remote monitoring can be seen as essential aids rather than threats to the workforce. Successful deployment depends on usability, clinician engagement, and training.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Regulatory evolution, open data policies, and economic sustainability are key to scaling digital health. &lt;/strong&gt;Even the best digital tools struggle to scale without reimbursement frameworks, regulatory support, and viable business models. Open data ecosystems are needed to unleash the clinical and economic value of innovation. Regulatory and reimbursement innovation is also critical to transitioning from pilot projects to high-impact, system-wide adoption.&lt;/p&gt;  &lt;p&gt;Download the full report.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/23/1119009/scaling-integrated-digital-health/</guid><pubDate>Mon, 23 Jun 2025 13:19:03 +0000</pubDate></item><item><title>[NEW] A Chinese firm has just launched a constantly changing set of AI benchmarks (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/23/1119190/chinese-changing-ai-benchmarks/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/250620_xbench_hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;When testing an AI model, it’s hard to tell if it is reasoning or just regurgitating answers from its training data. Xbench, a new benchmark developed by the Chinese venture capital firm HSG, or HongShan Capital Group, might help to sidestep that issue. That’s thanks to the way it evaluates models not only on the ability to pass arbitrary tests, like most other benchmarks, but also on the ability to execute real-world tasks, which is more unusual. It will be updated on a regular basis to try to keep it evergreen.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This week the company is making part of its question set open-source and letting anyone use for free. The team has also released a leaderboard comparing how mainstream AI models stack up when tested on Xbench. (ChatGPT o3 ranked first across all categories, though ByteDance’s Doubao, Gemini 2.5 Pro, and Grok all still did pretty well, as did Claude Sonnet.)&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;p&gt;Development of the benchmark at HongShan began in 2022, following ChatGPT’s breakout success, as an internal tool for assessing which models are worth investing in. Since then, led by partner Gong Yuan, the team has steadily expanded the system, bringing in outside researchers and professionals to help refine it. As the project grew more sophisticated, they decided to release it to the public.&lt;/p&gt;  &lt;p&gt;Xbench approached the problem with two different systems. One is similar to traditional benchmarking: an academic test that gauges a model’s aptitude on various subjects. The other is more like a technical interview round for a job, assessing how much real-world economic value a model might deliver.&lt;/p&gt; 
 &lt;p&gt;Xbench’s methods for assessing raw intelligence currently include two components: Xbench-ScienceQA and Xbench-DeepResearch. ScienceQA isn’t a radical departure from existing postgraduate-level STEM benchmarks like GPQA and SuperGPQA. It includes questions spanning fields from biochemistry to orbital mechanics, drafted by graduate students and double-checked by professors. Scoring rewards not only the right answer but also the reasoning chain that leads to it.&lt;/p&gt;  &lt;p&gt;DeepResearch, by contrast, focuses on a model’s ability to navigate the Chinese-language web. Ten subject-matter experts created 100 questions in music, history, finance, and literature—questions that can’t just be googled but require significant research to answer. Scoring favors breadth of sources, factual consistency, and a model’s willingness to admit when there isn’t enough data. A question in the publicized collection is “How many Chinese cities in the three northwestern provinces border a foreign country?” (It’s 12, and only 33% of models tested got it right, if you are wondering.)&lt;/p&gt; 
 &lt;p&gt;On the company’s website, the researchers said they want to add more dimensions to the test—for example, aspects like how creative a model is in its problem solving, how collaborative it is when working with other models, and how reliable it is.&lt;/p&gt;  &lt;p&gt;The team has committed to updating the test questions once a quarter and to maintain a half-public, half-private data set.&lt;/p&gt;  &lt;p&gt;To assess models’ real-world readiness, the team worked with experts to develop tasks modeled on actual workflows, initially in recruitment and marketing. For example, one task asks a model to source five qualified battery engineer candidates and justify each pick. Another asks it to match advertisers with appropriate short-video creators from a pool of over 800 influencers.&lt;/p&gt;  &lt;p&gt;The website also teases upcoming categories, including finance, legal, accounting, and design. The question sets for these categories have not yet been open-sourced.&lt;/p&gt;  &lt;p&gt;ChatGPT-o3 again ranks first in both of the current professional categories. For recruiting, Perplexity Search and Claude 3.5 Sonnet take second and third place, respectively. For marketing, Claude, Grok, and Gemini all perform well.&lt;/p&gt;  &lt;p&gt;“It is really difficult for benchmarks to include things that are so hard to quantify,” says Zihan Zheng, the lead researcher on a new benchmark called LiveCodeBench Pro and a student at NYU. “But Xbench represents a promising start.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/250620_xbench_hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;When testing an AI model, it’s hard to tell if it is reasoning or just regurgitating answers from its training data. Xbench, a new benchmark developed by the Chinese venture capital firm HSG, or HongShan Capital Group, might help to sidestep that issue. That’s thanks to the way it evaluates models not only on the ability to pass arbitrary tests, like most other benchmarks, but also on the ability to execute real-world tasks, which is more unusual. It will be updated on a regular basis to try to keep it evergreen.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This week the company is making part of its question set open-source and letting anyone use for free. The team has also released a leaderboard comparing how mainstream AI models stack up when tested on Xbench. (ChatGPT o3 ranked first across all categories, though ByteDance’s Doubao, Gemini 2.5 Pro, and Grok all still did pretty well, as did Claude Sonnet.)&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;p&gt;Development of the benchmark at HongShan began in 2022, following ChatGPT’s breakout success, as an internal tool for assessing which models are worth investing in. Since then, led by partner Gong Yuan, the team has steadily expanded the system, bringing in outside researchers and professionals to help refine it. As the project grew more sophisticated, they decided to release it to the public.&lt;/p&gt;  &lt;p&gt;Xbench approached the problem with two different systems. One is similar to traditional benchmarking: an academic test that gauges a model’s aptitude on various subjects. The other is more like a technical interview round for a job, assessing how much real-world economic value a model might deliver.&lt;/p&gt; 
 &lt;p&gt;Xbench’s methods for assessing raw intelligence currently include two components: Xbench-ScienceQA and Xbench-DeepResearch. ScienceQA isn’t a radical departure from existing postgraduate-level STEM benchmarks like GPQA and SuperGPQA. It includes questions spanning fields from biochemistry to orbital mechanics, drafted by graduate students and double-checked by professors. Scoring rewards not only the right answer but also the reasoning chain that leads to it.&lt;/p&gt;  &lt;p&gt;DeepResearch, by contrast, focuses on a model’s ability to navigate the Chinese-language web. Ten subject-matter experts created 100 questions in music, history, finance, and literature—questions that can’t just be googled but require significant research to answer. Scoring favors breadth of sources, factual consistency, and a model’s willingness to admit when there isn’t enough data. A question in the publicized collection is “How many Chinese cities in the three northwestern provinces border a foreign country?” (It’s 12, and only 33% of models tested got it right, if you are wondering.)&lt;/p&gt; 
 &lt;p&gt;On the company’s website, the researchers said they want to add more dimensions to the test—for example, aspects like how creative a model is in its problem solving, how collaborative it is when working with other models, and how reliable it is.&lt;/p&gt;  &lt;p&gt;The team has committed to updating the test questions once a quarter and to maintain a half-public, half-private data set.&lt;/p&gt;  &lt;p&gt;To assess models’ real-world readiness, the team worked with experts to develop tasks modeled on actual workflows, initially in recruitment and marketing. For example, one task asks a model to source five qualified battery engineer candidates and justify each pick. Another asks it to match advertisers with appropriate short-video creators from a pool of over 800 influencers.&lt;/p&gt;  &lt;p&gt;The website also teases upcoming categories, including finance, legal, accounting, and design. The question sets for these categories have not yet been open-sourced.&lt;/p&gt;  &lt;p&gt;ChatGPT-o3 again ranks first in both of the current professional categories. For recruiting, Perplexity Search and Claude 3.5 Sonnet take second and third place, respectively. For marketing, Claude, Grok, and Gemini all perform well.&lt;/p&gt;  &lt;p&gt;“It is really difficult for benchmarks to include things that are so hard to quantify,” says Zihan Zheng, the lead researcher on a new benchmark called LiveCodeBench Pro and a student at NYU. “But Xbench represents a promising start.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/23/1119190/chinese-changing-ai-benchmarks/</guid><pubDate>Mon, 23 Jun 2025 15:46:28 +0000</pubDate></item><item><title>[NEW] AllSpice’s platform is the GitHub for electrical engineering teams (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/23/allspices-platform-is-the-github-for-electrical-engineering-teams/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/allspice-kyle-and-valentina.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;There is no shortage of workflow collaboration tools — like Slack or Google Docs, in addition to industry-specific ones like GitHub — for software developers. A startup called AllSpice.io successfully bet that electrical hardware engineering teams need their own collaboration platform, too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AllSpice’s platform sits between existing workflow software. It allows hardware teams to collaborate on the types of documents they traditionally work in — documents that don’t easily translate over Slack and email — like PCB files and electronic CAD files, both of which are used to design circuit boards.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Engineers can use AllSpice to single out and comment on design aspects in these types of documents, the same way software engineers can comment on specific lines of code through GitHub.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kyle Dumont, co-founder and CTO of AllSpice, told TechCrunch that the startup has been able to find success because they haven’t tried to build a new end-to-end collaboration platform but rather fill the gap between the software solutions that hardware teams were already using.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The teams that we were talking to had really integral tools already in their workflows,” Dumont said. “They had these electrical CAD tools, they had [product life cycle management] tools, they had existing workflows that we knew the product that we launched had to operate between.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This learning came from research the founding team did prior to launching their product to make sure that they were building something teams would actually use. In early tests, AllSpice not only focused on what their users commented on, both good and bad, but also what didn’t get mentioned at all, Valentina Ratner, co-founder and CEO, told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Some of the most valuable things that we learned were maybe the things that people didn’t need or didn’t want,” Ratner said. “That helped us kind of scope something that will be really helpful and really an integral part of the workflow. Because we wanted to build not another point solution for our space, but a centralized platform that will become that home base for electronics teams.”&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Both Ratner and Dumont had experienced the pain points AllSpice is trying to solve firsthand while working as engineers at Amazon and iRobot, respectively. Ratner said that hardware design doesn’t translate through email chains and PDFs, and by the end of Ratner’s time at Amazon, she was spending the majority of her time building an internal collaboration tool to solve this problem for Amazon.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The duo met in grad school and launched the first version of AllSpice’s product in 2022, which was focused on small businesses and other startups. The company started to see growing demand from enterprises, pivoted, and has since landed customers such as Blue Origin, Bose, and Sam Altman’s Tools for Humanity, among others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup just raised a $15 million Series A round led by Rethink Impact with participation from L’attitude Ventures, Gingerbread Capital, and DNX Ventures, in addition to existing investors. The company will put the capital toward hiring and continuing to build out its product offerings.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AllSpice is also launching its new AI agent tool that helps validate engineers’ designs and spot mistakes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’ve seen huge demand to find out how our hardware, [and] AI tools, can help make their teams more effective, catch these design errors, and that’s exactly what we’re targeting for this product,” Dumont said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is purposefully launching this new AI agent just in closed beta for now, with an emphasis on working with their existing partners, Ratner said. The company wants to be able to ensure full accuracy before opening the product up further.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The cost of a hardware mistake is so much higher than the cost of a software mistake,” Ratner said. “We have to make it in a way that makes sense for our industry, because of those kinds of broad differences between releasing a software product versus releasing a hardware product.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/allspice-kyle-and-valentina.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;There is no shortage of workflow collaboration tools — like Slack or Google Docs, in addition to industry-specific ones like GitHub — for software developers. A startup called AllSpice.io successfully bet that electrical hardware engineering teams need their own collaboration platform, too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AllSpice’s platform sits between existing workflow software. It allows hardware teams to collaborate on the types of documents they traditionally work in — documents that don’t easily translate over Slack and email — like PCB files and electronic CAD files, both of which are used to design circuit boards.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Engineers can use AllSpice to single out and comment on design aspects in these types of documents, the same way software engineers can comment on specific lines of code through GitHub.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kyle Dumont, co-founder and CTO of AllSpice, told TechCrunch that the startup has been able to find success because they haven’t tried to build a new end-to-end collaboration platform but rather fill the gap between the software solutions that hardware teams were already using.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The teams that we were talking to had really integral tools already in their workflows,” Dumont said. “They had these electrical CAD tools, they had [product life cycle management] tools, they had existing workflows that we knew the product that we launched had to operate between.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This learning came from research the founding team did prior to launching their product to make sure that they were building something teams would actually use. In early tests, AllSpice not only focused on what their users commented on, both good and bad, but also what didn’t get mentioned at all, Valentina Ratner, co-founder and CEO, told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Some of the most valuable things that we learned were maybe the things that people didn’t need or didn’t want,” Ratner said. “That helped us kind of scope something that will be really helpful and really an integral part of the workflow. Because we wanted to build not another point solution for our space, but a centralized platform that will become that home base for electronics teams.”&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Both Ratner and Dumont had experienced the pain points AllSpice is trying to solve firsthand while working as engineers at Amazon and iRobot, respectively. Ratner said that hardware design doesn’t translate through email chains and PDFs, and by the end of Ratner’s time at Amazon, she was spending the majority of her time building an internal collaboration tool to solve this problem for Amazon.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The duo met in grad school and launched the first version of AllSpice’s product in 2022, which was focused on small businesses and other startups. The company started to see growing demand from enterprises, pivoted, and has since landed customers such as Blue Origin, Bose, and Sam Altman’s Tools for Humanity, among others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup just raised a $15 million Series A round led by Rethink Impact with participation from L’attitude Ventures, Gingerbread Capital, and DNX Ventures, in addition to existing investors. The company will put the capital toward hiring and continuing to build out its product offerings.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AllSpice is also launching its new AI agent tool that helps validate engineers’ designs and spot mistakes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’ve seen huge demand to find out how our hardware, [and] AI tools, can help make their teams more effective, catch these design errors, and that’s exactly what we’re targeting for this product,” Dumont said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is purposefully launching this new AI agent just in closed beta for now, with an emphasis on working with their existing partners, Ratner said. The company wants to be able to ensure full accuracy before opening the product up further.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The cost of a hardware mistake is so much higher than the cost of a software mistake,” Ratner said. “We have to make it in a way that makes sense for our industry, because of those kinds of broad differences between releasing a software product versus releasing a hardware product.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/23/allspices-platform-is-the-github-for-electrical-engineering-teams/</guid><pubDate>Mon, 23 Jun 2025 16:00:00 +0000</pubDate></item><item><title>[NEW] Learning from other domains to advance AI evaluation and testing (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/learning-from-other-domains-to-advance-ai-evaluation-and-testing/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Alt text: A stylized circuit board pattern with interconnected nodes and lines in light blue color. The background features diagonal lines that create a sense of perspective, converging towards the right side of the image." class="wp-image-1142551" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/RAI-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;As generative AI becomes more capable and widely deployed, familiar questions from the governance of other transformative technologies have resurfaced. Which opportunities, capabilities, risks, and impacts should be evaluated? Who should conduct evaluations, and at what stages of the technology lifecycle? What tests or measurements should be used? And how can we know if the results are reliable?&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Recent research and reports from Microsoft&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, the UK AI Security Institute&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, &lt;em&gt;The New York Times&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, and &lt;em&gt;MIT Technology Review&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;em&gt; &lt;/em&gt;have highlighted gaps in how we evaluate AI models and systems. These gaps also form foundational context for recent international expert consensus reports: the inaugural&amp;nbsp;&lt;em&gt;International AI Safety Report&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; (2025) and the &lt;em&gt;Singapore Consensus&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; (2025). Closing these gaps at a pace that matches AI innovation will lead to more reliable evaluations that can help guide deployment decisions, inform policy, and deepen trust.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Today, we’re launching a limited-series podcast, &lt;em&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/em&gt;, to share insights from domains that have grappled with testing and measurement questions. Across four episodes, host Kathleen Sullivan speaks with academic experts in genome editing, cybersecurity, pharmaceuticals, and medical devices to find out which technical and regulatory steps have helped to close evaluation gaps and earn public trust.&lt;/p&gt;




	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;Spotlight: Event Series&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Microsoft Research Forum&lt;/h2&gt;
				
								&lt;p class="large" id="microsoft-research-forum"&gt;Join us for a continuous exchange of ideas about research in the era of general AI. Watch the first four episodes on demand.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	&lt;p&gt;We’re also sharing written case studies from experts, along with top-level lessons we’re applying to AI. At the close of the podcast series, we’ll offer Microsoft’s deeper reflections on next steps toward more reliable and trustworthy approaches to AI evaluation.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="lessons-from-eight-case-studies"&gt;Lessons from eight case studies&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Our research on risk evaluation, testing, and assurance models in other domains began in December 2024, when Microsoft’s Office of Responsible AI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; gathered independent experts from the fields of civil aviation, cybersecurity, financial services, genome editing, medical devices, nanoscience, nuclear energy, and pharmaceuticals. In bringing this group together, we drew on our own learnings and feedback received on our e-book, &lt;em&gt;Global Governance: Goals and Lessons for AI&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;em&gt;, &lt;/em&gt;in which we studied the higher-level goals and institutional approaches that had been leveraged for cross-border governance in the past.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;While approaches to risk evaluation and testing vary significantly across the case studies, there was one consistent, top-level takeaway: evaluation frameworks always reflect trade-offs among different policy objectives, such as safety, efficiency, and innovation.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Experts across all eight fields noted that policymakers have had to weigh trade-offs in designing evaluation frameworks. These frameworks must account for both the limits of current science and the need for agility in the face of uncertainty. They likewise agreed that early design choices, often reflecting the “DNA” of the historical moment in which they’re made, as cybersecurity expert Stewart Baker described it, are important as they are difficult to scale down or undo later.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Strict, pre-deployment testing regimes—such as those used in civil aviation, medical devices, nuclear energy, and pharmaceuticals—offer strong safety assurances but can be resource-intensive and slow to adapt. These regimes often emerged in response to well-documented failures and are backed by decades of regulatory infrastructure and detailed technical standards.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In contrast, fields marked by dynamic and complex interdependencies between the tested system and its external environment—such as cybersecurity and bank stress testing—rely on more adaptive governance frameworks, where testing may be used to generate actionable insights about risk rather than primarily serve as a trigger for regulatory enforcement.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Moreover, in pharmaceuticals, where interdependencies are at play and there is emphasis on pre-deployment testing, experts highlighted a potential trade-off with post-market monitoring of downstream risks and efficacy evaluation.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;These variations in approaches across domains—stemming from differences in risk profiles, types of technologies, maturity of the evaluation science, placement of expertise in the assessor ecosystem, and context in which technologies are deployed, among other factors—also inform takeaways for AI.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="applying-risk-evaluation-and-governance-lessons-to-ai"&gt;Applying risk evaluation and governance lessons to AI&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;While no analogy perfectly fits the AI context, the genome editing and nanoscience cases offer interesting insights for general-purpose technologies like AI, where risks vary widely depending on how the technology is applied.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Experts highlighted the benefits of governance frameworks that are more flexible and tailored to specific use cases and application contexts. In these fields, it is challenging to define risk thresholds and design evaluation frameworks in the abstract. Risks become more visible and assessable once the technology is applied to a particular use case and context-specific variables are known.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;These and other insights also helped us distill qualities essential to ensuring that testing is a reliable governance tool across domains, including:&amp;nbsp;&lt;/p&gt;



&lt;ol class="wp-block-list" start="1"&gt;
&lt;li&gt;&lt;strong&gt;Rigor &lt;/strong&gt;in defining what is being examined and why it matters. This requires detailed specification of what is being measured and understanding how the deployment context may affect outcomes.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Standardization &lt;/strong&gt;of how tests should be conducted to achieve valid, reliable results. This requires establishing technical standards that provide methodological guidance and ensure quality and consistency.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Interpretability &lt;/strong&gt;of test results and how they inform risk decisions. This requires establishing expectations for evidence and improving literacy in how to understand, contextualize, and use test results—while remaining aware of their limitations.&amp;nbsp;&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 class="wp-block-heading" id="toward-stronger-foundations-for-ai-testing"&gt;Toward stronger foundations for AI testing&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Establishing robust foundations for AI evaluation and testing requires effort to improve rigor, standardization, and interpretability—and to ensure that methods keep pace with rapid technological progress and evolving scientific understanding.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Taking lessons from other general-purpose technologies, this foundational work must also be pursued for both AI models and systems. While testing models will continue to be important, reliable evaluation tools that provide assurance for system performance will enable broad adoption of AI, including in high-risk scenarios. A strong feedback loop on evaluations of AI models and systems could not only accelerate progress on methodological challenges but also bring focus to which opportunities, capabilities, risks, and impacts are most appropriate and efficient to evaluate at what points along the AI development and deployment lifecycle.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="acknowledgements"&gt;Acknowledgements&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;We would like to thank the following external experts who have contributed to our research program on lessons for AI testing and evaluation: Mateo Aboy, Paul Alp, Gerónimo Poletto Antonacci, Stewart Baker, Daniel Benamouzig, Pablo Cantero, Daniel Carpenter, Alta Charo, Jennifer Dionne, Andy Greenfield, Kathryn Judge, Ciaran Martin, and Timo Minssen.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="case-studies"&gt;Case studies&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;&lt;strong&gt;Civil aviation:&lt;/strong&gt; &lt;em&gt;Testing in Aircraft Design and Manufacturing&lt;/em&gt;, by Paul Alp&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Cybersecurity:&lt;/strong&gt; &lt;em&gt;Cybersecurity Standards and Testing—Lessons for AI Safety and Security&lt;/em&gt;, by Stewart Baker&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Financial services (bank stress testing):&lt;/strong&gt; &lt;em&gt;The Evolving Use of Bank Stress Tests&lt;/em&gt;, by Kathryn Judge&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Genome editing:&lt;/strong&gt; &lt;em&gt;Governance of Genome Editing in Human Therapeutics and Agricultural Applications&lt;/em&gt;, by Alta Charo and Andy Greenfield&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Medical devices:&lt;/strong&gt; &lt;em&gt;Medical Device Testing: Regulatory Requirements, Evolution and Lessons for AI Governance&lt;/em&gt;,&lt;em&gt; &lt;/em&gt;by Mateo Aboy and Timo Minssen&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Nanoscience:&lt;/strong&gt; &lt;em&gt;The regulatory landscape of nanoscience and nanotechnology, and applications to future AI regulation&lt;/em&gt;, by Jennifer Dionne&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Nuclear energy:&lt;/strong&gt; &lt;em&gt;Testing in the Nuclear Industry&lt;/em&gt;, by Pablo Cantero and Gerónimo Poletto Antonacci&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Pharmaceuticals:&lt;/strong&gt; &lt;em&gt;The History and Evolution of Testing in Pharmaceutical Regulation&lt;/em&gt;, by Daniel Benamouzig and Daniel Carpenter&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Alt text: A stylized circuit board pattern with interconnected nodes and lines in light blue color. The background features diagonal lines that create a sense of perspective, converging towards the right side of the image." class="wp-image-1142551" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/RAI-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;As generative AI becomes more capable and widely deployed, familiar questions from the governance of other transformative technologies have resurfaced. Which opportunities, capabilities, risks, and impacts should be evaluated? Who should conduct evaluations, and at what stages of the technology lifecycle? What tests or measurements should be used? And how can we know if the results are reliable?&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Recent research and reports from Microsoft&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, the UK AI Security Institute&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, &lt;em&gt;The New York Times&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, and &lt;em&gt;MIT Technology Review&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;em&gt; &lt;/em&gt;have highlighted gaps in how we evaluate AI models and systems. These gaps also form foundational context for recent international expert consensus reports: the inaugural&amp;nbsp;&lt;em&gt;International AI Safety Report&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; (2025) and the &lt;em&gt;Singapore Consensus&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; (2025). Closing these gaps at a pace that matches AI innovation will lead to more reliable evaluations that can help guide deployment decisions, inform policy, and deepen trust.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Today, we’re launching a limited-series podcast, &lt;em&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/em&gt;, to share insights from domains that have grappled with testing and measurement questions. Across four episodes, host Kathleen Sullivan speaks with academic experts in genome editing, cybersecurity, pharmaceuticals, and medical devices to find out which technical and regulatory steps have helped to close evaluation gaps and earn public trust.&lt;/p&gt;




	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;Spotlight: Event Series&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Microsoft Research Forum&lt;/h2&gt;
				
								&lt;p class="large" id="microsoft-research-forum"&gt;Join us for a continuous exchange of ideas about research in the era of general AI. Watch the first four episodes on demand.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	&lt;p&gt;We’re also sharing written case studies from experts, along with top-level lessons we’re applying to AI. At the close of the podcast series, we’ll offer Microsoft’s deeper reflections on next steps toward more reliable and trustworthy approaches to AI evaluation.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="lessons-from-eight-case-studies"&gt;Lessons from eight case studies&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Our research on risk evaluation, testing, and assurance models in other domains began in December 2024, when Microsoft’s Office of Responsible AI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; gathered independent experts from the fields of civil aviation, cybersecurity, financial services, genome editing, medical devices, nanoscience, nuclear energy, and pharmaceuticals. In bringing this group together, we drew on our own learnings and feedback received on our e-book, &lt;em&gt;Global Governance: Goals and Lessons for AI&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;em&gt;, &lt;/em&gt;in which we studied the higher-level goals and institutional approaches that had been leveraged for cross-border governance in the past.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;While approaches to risk evaluation and testing vary significantly across the case studies, there was one consistent, top-level takeaway: evaluation frameworks always reflect trade-offs among different policy objectives, such as safety, efficiency, and innovation.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Experts across all eight fields noted that policymakers have had to weigh trade-offs in designing evaluation frameworks. These frameworks must account for both the limits of current science and the need for agility in the face of uncertainty. They likewise agreed that early design choices, often reflecting the “DNA” of the historical moment in which they’re made, as cybersecurity expert Stewart Baker described it, are important as they are difficult to scale down or undo later.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Strict, pre-deployment testing regimes—such as those used in civil aviation, medical devices, nuclear energy, and pharmaceuticals—offer strong safety assurances but can be resource-intensive and slow to adapt. These regimes often emerged in response to well-documented failures and are backed by decades of regulatory infrastructure and detailed technical standards.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In contrast, fields marked by dynamic and complex interdependencies between the tested system and its external environment—such as cybersecurity and bank stress testing—rely on more adaptive governance frameworks, where testing may be used to generate actionable insights about risk rather than primarily serve as a trigger for regulatory enforcement.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Moreover, in pharmaceuticals, where interdependencies are at play and there is emphasis on pre-deployment testing, experts highlighted a potential trade-off with post-market monitoring of downstream risks and efficacy evaluation.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;These variations in approaches across domains—stemming from differences in risk profiles, types of technologies, maturity of the evaluation science, placement of expertise in the assessor ecosystem, and context in which technologies are deployed, among other factors—also inform takeaways for AI.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="applying-risk-evaluation-and-governance-lessons-to-ai"&gt;Applying risk evaluation and governance lessons to AI&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;While no analogy perfectly fits the AI context, the genome editing and nanoscience cases offer interesting insights for general-purpose technologies like AI, where risks vary widely depending on how the technology is applied.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Experts highlighted the benefits of governance frameworks that are more flexible and tailored to specific use cases and application contexts. In these fields, it is challenging to define risk thresholds and design evaluation frameworks in the abstract. Risks become more visible and assessable once the technology is applied to a particular use case and context-specific variables are known.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;These and other insights also helped us distill qualities essential to ensuring that testing is a reliable governance tool across domains, including:&amp;nbsp;&lt;/p&gt;



&lt;ol class="wp-block-list" start="1"&gt;
&lt;li&gt;&lt;strong&gt;Rigor &lt;/strong&gt;in defining what is being examined and why it matters. This requires detailed specification of what is being measured and understanding how the deployment context may affect outcomes.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Standardization &lt;/strong&gt;of how tests should be conducted to achieve valid, reliable results. This requires establishing technical standards that provide methodological guidance and ensure quality and consistency.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Interpretability &lt;/strong&gt;of test results and how they inform risk decisions. This requires establishing expectations for evidence and improving literacy in how to understand, contextualize, and use test results—while remaining aware of their limitations.&amp;nbsp;&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 class="wp-block-heading" id="toward-stronger-foundations-for-ai-testing"&gt;Toward stronger foundations for AI testing&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Establishing robust foundations for AI evaluation and testing requires effort to improve rigor, standardization, and interpretability—and to ensure that methods keep pace with rapid technological progress and evolving scientific understanding.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Taking lessons from other general-purpose technologies, this foundational work must also be pursued for both AI models and systems. While testing models will continue to be important, reliable evaluation tools that provide assurance for system performance will enable broad adoption of AI, including in high-risk scenarios. A strong feedback loop on evaluations of AI models and systems could not only accelerate progress on methodological challenges but also bring focus to which opportunities, capabilities, risks, and impacts are most appropriate and efficient to evaluate at what points along the AI development and deployment lifecycle.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="acknowledgements"&gt;Acknowledgements&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;We would like to thank the following external experts who have contributed to our research program on lessons for AI testing and evaluation: Mateo Aboy, Paul Alp, Gerónimo Poletto Antonacci, Stewart Baker, Daniel Benamouzig, Pablo Cantero, Daniel Carpenter, Alta Charo, Jennifer Dionne, Andy Greenfield, Kathryn Judge, Ciaran Martin, and Timo Minssen.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="case-studies"&gt;Case studies&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;&lt;strong&gt;Civil aviation:&lt;/strong&gt; &lt;em&gt;Testing in Aircraft Design and Manufacturing&lt;/em&gt;, by Paul Alp&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Cybersecurity:&lt;/strong&gt; &lt;em&gt;Cybersecurity Standards and Testing—Lessons for AI Safety and Security&lt;/em&gt;, by Stewart Baker&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Financial services (bank stress testing):&lt;/strong&gt; &lt;em&gt;The Evolving Use of Bank Stress Tests&lt;/em&gt;, by Kathryn Judge&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Genome editing:&lt;/strong&gt; &lt;em&gt;Governance of Genome Editing in Human Therapeutics and Agricultural Applications&lt;/em&gt;, by Alta Charo and Andy Greenfield&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Medical devices:&lt;/strong&gt; &lt;em&gt;Medical Device Testing: Regulatory Requirements, Evolution and Lessons for AI Governance&lt;/em&gt;,&lt;em&gt; &lt;/em&gt;by Mateo Aboy and Timo Minssen&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Nanoscience:&lt;/strong&gt; &lt;em&gt;The regulatory landscape of nanoscience and nanotechnology, and applications to future AI regulation&lt;/em&gt;, by Jennifer Dionne&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Nuclear energy:&lt;/strong&gt; &lt;em&gt;Testing in the Nuclear Industry&lt;/em&gt;, by Pablo Cantero and Gerónimo Poletto Antonacci&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Pharmaceuticals:&lt;/strong&gt; &lt;em&gt;The History and Evolution of Testing in Pharmaceutical Regulation&lt;/em&gt;, by Daniel Benamouzig and Daniel Carpenter&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/learning-from-other-domains-to-advance-ai-evaluation-and-testing/</guid><pubDate>Mon, 23 Jun 2025 16:35:06 +0000</pubDate></item><item><title>[NEW] AI Testing and Evaluation: Learnings from Science and Industry (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/podcast/ai-testing-and-evaluation-learnings-from-science-and-industry/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Illustrated headshots of Amanda Craig Deckard &amp;amp; Kathleen Sullivan." class="wp-image-1141309" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/EP0-AI-TE_Hero_Feature_1400x788.jpg" width="1400" /&gt;&lt;/figure&gt;






&lt;p&gt;Generative AI presents a unique challenge and opportunity to reexamine governance practices for the responsible development, deployment, and use of AI. To advance thinking in this space, Microsoft has tapped into the experience and knowledge of experts across domains—from genome editing to cybersecurity—to investigate the role of testing and evaluation as a governance tool. &lt;em&gt;AI Testing and Evaluation: Learnings from Science and Industry, &lt;/em&gt;hosted by Microsoft Research’s Kathleen Sullivan, explores what the technology industry and policymakers can learn from these fields and how that might help shape the course of AI development.&lt;/p&gt;



&lt;p&gt;In this episode, Amanda Craig Deckard, senior director of public policy in Microsoft’s Office of Responsible AI, joins Sullivan to detail the company’s efforts to help inform AI governance discussions and decisions, including, more recently, around the role of AI testing and evaluation. Craig Deckard and Sullivan delve into the tension that exists between the risk and opportunity of technology, the similarities and differences between AI development and the fields Microsoft is studying, and the role of different stakeholders in advancing AI governance and public policy.&lt;/p&gt;



&lt;div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow"&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h2 class="wp-block-heading h5" id="learn-more-1"&gt;Learn more:&lt;/h2&gt;




&lt;/div&gt;



&lt;p&gt;AI and Microsoft Research&lt;/p&gt;







&lt;section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast"&gt;
	
&lt;/section&gt;


&lt;div class="wp-block-msr-show-more"&gt;
	&lt;div class="bg-neutral-100 p-5"&gt;
		&lt;div class="show-more-show-less"&gt;
			&lt;div&gt;
				&lt;span&gt;
					

&lt;h2 class="wp-block-heading" id="transcript"&gt;Transcript&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;[MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KATHLEEN SULLIVAN:&lt;/strong&gt; Welcome to &lt;em&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/em&gt;. I’m your host, Kathleen Sullivan.&lt;/p&gt;



&lt;p&gt;As generative AI continues to advance, Microsoft has gathered a range of experts—from genome editing to cybersecurity—to share how their fields approach evaluation and risk assessment. Our goal is to learn from their successes and their stumbles to move the science and practice of AI testing forward. In this series, we’ll explore how these insights might help guide the future of AI development, deployment, and responsible use.&lt;/p&gt;



				&lt;/span&gt;
				&lt;span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1"&gt;
					



&lt;p&gt;[MUSIC ENDS]&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For our introductory episode, I’m pleased to welcome Amanda Craig Deckard from Microsoft to discuss the company’s efforts to learn about testing in other sectors.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Amanda is senior director of public policy in the Office of Responsible AI, where she leads a team that works closely with engineers, researchers, and policy experts to help ensure AI is being developed and used responsibly. Their insights shape Microsoft’s contribution to public policy discussions on laws, norms, and standards for AI.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Amanda, welcome to the podcast.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AMANDA CRAIG DECKARD:&lt;/strong&gt; Thank you.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; Amanda, let’s give the listeners a little bit of your background. What’s your origin story? Can you talk to us a little bit about maybe how you started in tech? And I would love to also learn a little bit more about what your team does in the Office of Responsible AI.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Sure. Thank you. I’d say my [LAUGHS] path to tech, to Microsoft, as well, was a bit, like, circuitous, maybe. You know, I thought for the longest time I was going to be a journalist. I studied forced migration. I worked in a sort of state level sort of trial court in Indiana, a legal service provider in India, just to give you a bit of a flavor.&lt;/p&gt;



&lt;p&gt;I made my way to Microsoft in 2014 and have been here since, working in cybersecurity public policy first and now in responsible AI. And the way that our Office of Responsible AI has really, sort of, structured itself is bringing together the kind of expertise to really work on defining policy and how to operationalize it at the same time.&lt;/p&gt;



&lt;p&gt;And, you know, that means that we have been working through this, you know, real challenge of defining internal policy and practice, making sure that’s deeply grounded in the work of our colleagues at Microsoft Research, and then really closely working with engineering to make sure that we have the processes, that we have the tools, to implement that policy at scale.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And I’m really drawn to these kind of hard problems where they have the character of two things being true or there’s like, you know, real tension on both sides and in particular, in the context of those kinds of problems, roles in which, like, the whole job is actually just sitting with that tension, not necessarily, like, resolving it and expecting that you’re done.&lt;/p&gt;



&lt;p&gt;And I think, really, there are two reasons why tech is so, kind of, representative of that kind of challenge that I’ve always found fascinating. You know, one is that, of course, tech is, sort of, ubiquitous. It’s really impacting so many people’s lives. But also, you know, because, as I think has become part of our vernacular now, but, you know, is not necessarily immediately intuitive, is like the fact that technology is both a tool and a weapon. And so that’s just, like, another reason why, you know, we have to continuously work through that tension and, sort of, like, sit with it, right, and even as tech evolves over time.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; You bring up such great points, and this field is not black and white. I think that even underscores, you know, this notion that you highlighted that it’s impacting everyone. And, you know, to set the stage for our listeners, last year, we pulled in a bunch of experts from cybersecurity, biotech, finance, and we ran this large workshop to study how they’re thinking about governance in those playbooks. And so I’d love to understand a little bit more about what sparked that effort—and, you know, there’s a piece of this which is really centered around testing—and to hear from you why the focus on testing is so important.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; If I could rewind a little bit and give you a bit of history of how we even arrived at bringing these experts together, you know, we actually started on this journey in 2023. At that time, there were, like, a lot of these big questions swirling around about, you know, what did we need in terms of governance for AI? Of course, this was in the immediate aftermath of the ChatGPT sort of wave and everyone recognizing that, like, the technology was going to have a different level of impact in the near term. And so, you know, what do we need from governance? What do we need at the global level, in particular, of governance?&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And so at the time, in early 2023 especially, there were a lot of attempts to sort of draw analogies to other global governance institutions in other domains. So we actually in 2023 brought together a different workshop than the one that you’re referring to specifically focused on testing last year. And we, kind of, had two big takeaways from that conversation.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;One was, what are the actual functions of these institutions and how do they apply to AI? And, actually, one of the takeaways was they all sort of apply. [LAUGHS] There’s, like, a role for, you know, any of the functions, whether it be sort of driving consensus on research or building industry standards or managing, kind of, frontier risks, for thinking about how those might be needed in the AI context.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And one of the other big takeaways was that, you know, there are also limitations in these analogies. You know, each of the institutions grew up in its own, sort of, unique historical moment, like the one that we sit in with AI right now. And in each of those circumstances, they don’t exactly translate to this moment. And so, yeah, there was like this kind of, OK, we want to draw what we can from this conversation and then we also want to understand, what is also very important that’s just different for AI right now?&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;We published a book with the lessons from that conversation in 2023&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. And then we actually went on a bit of a tour [LAUGHS] with that content where we had a number of roundtables actually all over the world where we gathered feedback on how those analogies were landing, how our takeaways were landing. And one of the things that we took from them was a gap that some of the participants saw in the analogies that we chose to focus on. So across multiple conversations, other domains kept being raised, like, why did you not also study pharmaceuticals? Why did you also not study cybersecurity, for example? And so that, you know, naturally got us thinking about what further lessons we could draw from &lt;em&gt;those&lt;/em&gt; domains.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;At the same time, though, we also saw a need to, again, go deeper than what we went and really, like, focus on a narrower problem. So that’s really what led us to trying to think about a more specific problem where we could think across levels of governance and bring in some of these other domains. And, you know, testing was top of mind. Continues to be a really important topic in the AI policy conversation right now, I think, for really good reason. A lot of policymakers are focused on, you know, what we need to do to,&amp;nbsp;kind of, have there be sufficient trust, and testing is going to be a part of that—really better understand risk, enable everyone to be able to make more, kind of, risk-informed decisions, right. Testing is an important component for governance and AI and, of course, in all of these other domains, as well.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So I’ll just add the other, kind of, input into the process for this second round was exploring other analogies beyond those that we, kind of, got feedback on. And one of the early, kind of, examples of another domain that would be really worthwhile to study that came to mind from, sort of, just studying the literature was genome editing. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;You know, genome editing was really interesting through the process of thinking about other kind of general-purpose technologies. We also arrived at nanoscience and brought those into the conversation.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; That’s great. I mean, actually, if you could double-click,&amp;nbsp;I mean, you just named a number of industries. I’d love to just understand which of those worlds maybe feels the closest to what we’re wrestling with, with AI and maybe which is kind of the farthest off, and what makes them stand out to you?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Oh, such a good question. For this second round, we actually brought together eight different domains, right. And I think we actually thought we would come out of this conversation with some bit of clarity around, &lt;em&gt;Oh, if we just, sort of, take this approach for this domain or that domain, we’ll sort of have—at least for now—really solved part of the puzzle.&lt;/em&gt; [LAUGHS] And, you know, our public policy team the day after the workshop, we had a, sort of, follow-on discussion, and the very first thing that we started with in that conversation was like, &lt;em&gt;OK, so which of these domains?&lt;/em&gt; And fascinatingly, like, everyone was sort of like, &lt;em&gt;Ahh! &lt;/em&gt;[LAUGHS] &lt;em&gt;None of them are applying perfectly&lt;/em&gt;. I mean, this is also speaking to the limitations of analogies that we already acknowledged.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And also, you know, all of the experts from across these domains gave us really interesting insights into, sort of, the tradeoffs and the limitations and how they were working. None are really applying perfectly for us. But all of them do offer a thread of insight that is really useful for thinking about testing in AI, and there are some different dimensions that I think are really useful as framing for that.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I mean, one is just this horizontal-versus-vertical,&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;kind of, difference in domains and, you know, the horizontal technology like genome editing or nanoscience&amp;nbsp;just being inherently different and seemingly very similar to AI in that you want to be able to understand risks in the technology itself &lt;em&gt;and&lt;/em&gt; there is just so much contextual, sort of, factor that matters in the application of those technologies for how the risk manifests that you really need to, kind of, do those two things at once—of understanding the technology but then really thinking about risk and governance in the context of application versus, you know, a context like or a domain like civil aviation or nuclear technology, for example.&lt;/p&gt;



&lt;p&gt;You know, even in the workshop itself that we hosted late last year, where we brought together this second round of experts, it was really interesting. We actually started the conversation by trying to understand how those different domains defined risks, where they were able to set risk thresholds. That’s been such a part of the AI policy conversation in the last year. And, you know, it was really instructive that the more vertical domains were able to, sort of, snap to clearer answers much more quickly. [LAUGHS] But, like, the horizontal nanoscience and genome editing were not because it just depends, right. So anyway, the horizontal-vertical dimension seems like a really important one to draw from and apply to AI. &lt;/p&gt;



&lt;p&gt;The couple of others that I would offer is just, you know, thinking about the different kinds of technologies. You know, obviously, there’s some of the domains that we studied that they’re just inherently, sort of, like, physical technologies … a mix of physical and digital or virtual in a lot of cases because all of these are, of course, applying digital technology. But like, you know, there is just a difference between something like an &lt;em&gt;airplane&lt;/em&gt; or a &lt;em&gt;medical device&lt;/em&gt; or, you know, the more kind of virtual or intangible sort of technologies even, you know, of course, AI and some of the other like cyber and genome editing but also like, you know, financial services having some of that quality. And again, I think the thing that’s interesting to us about AI is to think about AI and risk evaluation of AI as being, you know, having a large component of that being about the kind of virtual or intangible technology. &lt;em&gt;And also&lt;/em&gt;, you know, there is a future of robotics where we might need to think about the, kind of, physical risk evaluation kind of work, as well.&lt;/p&gt;



&lt;p&gt;And then the final thing I’d maybe say in terms of thinking about which domains have the lessons for AI that are most applicable is just how they’ve grappled with these different kind of governance questions. Things like how to turn the dial in terms of being more or less prescriptive on risk evaluation approaches, how they think about the balance of, kind of, pre-market versus post-market risk evaluation in testing, and what the tradeoffs have been there across domains has been really interesting to kind of tease out. And then also thinking about, sort of, who does what?&lt;/p&gt;



&lt;p&gt;So, you know, in each of these different domains, it was interesting to hear about, like, you know, the role of industry, the role of governments, the role of third-party experts in designing evaluations and developing standards and actually doing the work, and, kind of, having the pull through of what it means for risk and governance decisions. There were, again, there was a variety of, sort of, approaches across these domains that I think were interesting for AI.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; You mentioned that there’s a number of different stakeholders to be considering across the board as we’re thinking about policy, as we’re thinking about regulation. Where can we collaborate more across industry? Is it academia? Regulators? Just, how can we move the needle faster?&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; I think all of the above [LAUGHTER] is needed. But it’s also really important to have all of that, kind of, expertise brought together, you know, and I think, you know, one of the things that we certainly heard from multiple of the domains, if not all of them, was that same actual interest and need and the same sort of ongoing work to try to figure that out.&lt;/p&gt;



&lt;p&gt;You know, even where there had been progress in some of the other domains with bringing together, you know, some industry stakeholders or, you know, industry and government, there was still a desire to actually do more there. Like, if there was some progress in industry and government, the need was, &lt;em&gt;And more kind of cross-jurisdiction government conversation&lt;/em&gt;, for example. Or some progress on, you know, within the industry but needing to, like, strengthen the partnership with academia, for example. So, you know, I think it speaks to, like, the quality of your question, to be honest, that, you know, all of these domains are actually still grappling with this and still seeing the need to grow in that direction more.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;What I’d say about AI today is that we have made good progress with, you know, starting to build some industry partnerships. You know, we were a founding member of the Frontier Model Forum, or FMF&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, which has been a very useful place for us to work with some peers on really trying to bring forward some best practices that apply across our organizations. You know, there are other forums as well, like MLCommons&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, where we’re working with others in industry and broader, sort of, academic and civil society communities. Partnership on AI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; is another one I think about that, kind of, fits that mold, as well, in a really positive way. And, like, there are a lot of different, sort of, governance needs to think through and where, you know, we can really think about bringing that expertise together is going to be so important.&lt;/p&gt;



&lt;p&gt;I think about almost, like, in the near to mid-term, like three issues that we need to address in the AI, kind of, policy and testing context. One is just building kind of, like, a flexible framework that allows us to really build trust while we continue to advance the science and the standards. You know, we are going to need to do both at once. And so we need a flexible framework that enables that kind of agility, and advancing the science and the standards, that &lt;em&gt;is &lt;/em&gt;going to be something that really demands that kind of cross-discipline or cross kind of expertise group coming together to work on that—researchers, academics, civil society, governments and, of course, industry.&lt;/p&gt;



&lt;p&gt;And so I think that is, actually, the second problem is, like, how do we actually build the kind of forums and ways of working together, the public-private partnership kind of efforts that allow all of that expertise to come together and fit together over time, right. Because when these are really big, broad challenges, you kind of have to break them down incrementally, make progress on them, and then bring them back together. &lt;/p&gt;



&lt;p&gt;And so I think about, like, one example that I, you know, really have been reflecting on lately is, you know, in the context of building standards, like, how do you do that, right? Again, standards are going to benefit from that whole community of expertise. And, you know, there are lots of different kinds of quote-unquote standards, though, right. You kind of have the “small &lt;em&gt;s&lt;/em&gt;” industry standards. You have the kind of “big &lt;em&gt;S&lt;/em&gt;” international standards, for example. And how do you, kind of, leverage one to accelerate the other, I think, is part of, like, how we need to work together within this ecosystem. And, like, I think what we and others have done in an organization like C2PA [Coalition for Content Provenance and Authenticity]&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, for example, where we’ve really built an industry specification but then built on that towards an international standard effort is one example that is interesting, right, to point to.&lt;/p&gt;



&lt;p&gt;And then, you know, I actually think that bridges to the third thing that we need to do together within this whole community, which is, you know, really think again about how we manage the breadth of this challenge and opportunity of AI by thinking about this horizontal-vertical problem. And, you know, I think that’s where it’s not just the sort of tech industry, for example. It’s broader industry that’s going to be really applying this technology that needs to get involved in the conversation about not just, sort of, testing AI models, for example, but also testing how AI systems or applications are working in context. And so, yes, so much fun opportunity!&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; Amanda, this was just fantastic. You’ve really set the stage for this podcast. And thank you so much for sharing your time and wisdom with us.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Thank you.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; And to our listeners, we’re so glad you joined us for this conversation. An exciting lineup of episodes are on the way, and we can’t wait to have you back for the next one.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&amp;nbsp;&lt;/p&gt;

				&lt;/span&gt;
			&lt;/div&gt;
			&lt;button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button"&gt;
				Show more			&lt;/button&gt;
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Illustrated headshots of Amanda Craig Deckard &amp;amp; Kathleen Sullivan." class="wp-image-1141309" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/EP0-AI-TE_Hero_Feature_1400x788.jpg" width="1400" /&gt;&lt;/figure&gt;






&lt;p&gt;Generative AI presents a unique challenge and opportunity to reexamine governance practices for the responsible development, deployment, and use of AI. To advance thinking in this space, Microsoft has tapped into the experience and knowledge of experts across domains—from genome editing to cybersecurity—to investigate the role of testing and evaluation as a governance tool. &lt;em&gt;AI Testing and Evaluation: Learnings from Science and Industry, &lt;/em&gt;hosted by Microsoft Research’s Kathleen Sullivan, explores what the technology industry and policymakers can learn from these fields and how that might help shape the course of AI development.&lt;/p&gt;



&lt;p&gt;In this episode, Amanda Craig Deckard, senior director of public policy in Microsoft’s Office of Responsible AI, joins Sullivan to detail the company’s efforts to help inform AI governance discussions and decisions, including, more recently, around the role of AI testing and evaluation. Craig Deckard and Sullivan delve into the tension that exists between the risk and opportunity of technology, the similarities and differences between AI development and the fields Microsoft is studying, and the role of different stakeholders in advancing AI governance and public policy.&lt;/p&gt;



&lt;div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow"&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h2 class="wp-block-heading h5" id="learn-more-1"&gt;Learn more:&lt;/h2&gt;




&lt;/div&gt;



&lt;p&gt;AI and Microsoft Research&lt;/p&gt;







&lt;section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast"&gt;
	
&lt;/section&gt;


&lt;div class="wp-block-msr-show-more"&gt;
	&lt;div class="bg-neutral-100 p-5"&gt;
		&lt;div class="show-more-show-less"&gt;
			&lt;div&gt;
				&lt;span&gt;
					

&lt;h2 class="wp-block-heading" id="transcript"&gt;Transcript&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;[MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KATHLEEN SULLIVAN:&lt;/strong&gt; Welcome to &lt;em&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/em&gt;. I’m your host, Kathleen Sullivan.&lt;/p&gt;



&lt;p&gt;As generative AI continues to advance, Microsoft has gathered a range of experts—from genome editing to cybersecurity—to share how their fields approach evaluation and risk assessment. Our goal is to learn from their successes and their stumbles to move the science and practice of AI testing forward. In this series, we’ll explore how these insights might help guide the future of AI development, deployment, and responsible use.&lt;/p&gt;



				&lt;/span&gt;
				&lt;span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1"&gt;
					



&lt;p&gt;[MUSIC ENDS]&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For our introductory episode, I’m pleased to welcome Amanda Craig Deckard from Microsoft to discuss the company’s efforts to learn about testing in other sectors.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Amanda is senior director of public policy in the Office of Responsible AI, where she leads a team that works closely with engineers, researchers, and policy experts to help ensure AI is being developed and used responsibly. Their insights shape Microsoft’s contribution to public policy discussions on laws, norms, and standards for AI.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Amanda, welcome to the podcast.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AMANDA CRAIG DECKARD:&lt;/strong&gt; Thank you.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; Amanda, let’s give the listeners a little bit of your background. What’s your origin story? Can you talk to us a little bit about maybe how you started in tech? And I would love to also learn a little bit more about what your team does in the Office of Responsible AI.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Sure. Thank you. I’d say my [LAUGHS] path to tech, to Microsoft, as well, was a bit, like, circuitous, maybe. You know, I thought for the longest time I was going to be a journalist. I studied forced migration. I worked in a sort of state level sort of trial court in Indiana, a legal service provider in India, just to give you a bit of a flavor.&lt;/p&gt;



&lt;p&gt;I made my way to Microsoft in 2014 and have been here since, working in cybersecurity public policy first and now in responsible AI. And the way that our Office of Responsible AI has really, sort of, structured itself is bringing together the kind of expertise to really work on defining policy and how to operationalize it at the same time.&lt;/p&gt;



&lt;p&gt;And, you know, that means that we have been working through this, you know, real challenge of defining internal policy and practice, making sure that’s deeply grounded in the work of our colleagues at Microsoft Research, and then really closely working with engineering to make sure that we have the processes, that we have the tools, to implement that policy at scale.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And I’m really drawn to these kind of hard problems where they have the character of two things being true or there’s like, you know, real tension on both sides and in particular, in the context of those kinds of problems, roles in which, like, the whole job is actually just sitting with that tension, not necessarily, like, resolving it and expecting that you’re done.&lt;/p&gt;



&lt;p&gt;And I think, really, there are two reasons why tech is so, kind of, representative of that kind of challenge that I’ve always found fascinating. You know, one is that, of course, tech is, sort of, ubiquitous. It’s really impacting so many people’s lives. But also, you know, because, as I think has become part of our vernacular now, but, you know, is not necessarily immediately intuitive, is like the fact that technology is both a tool and a weapon. And so that’s just, like, another reason why, you know, we have to continuously work through that tension and, sort of, like, sit with it, right, and even as tech evolves over time.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; You bring up such great points, and this field is not black and white. I think that even underscores, you know, this notion that you highlighted that it’s impacting everyone. And, you know, to set the stage for our listeners, last year, we pulled in a bunch of experts from cybersecurity, biotech, finance, and we ran this large workshop to study how they’re thinking about governance in those playbooks. And so I’d love to understand a little bit more about what sparked that effort—and, you know, there’s a piece of this which is really centered around testing—and to hear from you why the focus on testing is so important.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; If I could rewind a little bit and give you a bit of history of how we even arrived at bringing these experts together, you know, we actually started on this journey in 2023. At that time, there were, like, a lot of these big questions swirling around about, you know, what did we need in terms of governance for AI? Of course, this was in the immediate aftermath of the ChatGPT sort of wave and everyone recognizing that, like, the technology was going to have a different level of impact in the near term. And so, you know, what do we need from governance? What do we need at the global level, in particular, of governance?&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And so at the time, in early 2023 especially, there were a lot of attempts to sort of draw analogies to other global governance institutions in other domains. So we actually in 2023 brought together a different workshop than the one that you’re referring to specifically focused on testing last year. And we, kind of, had two big takeaways from that conversation.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;One was, what are the actual functions of these institutions and how do they apply to AI? And, actually, one of the takeaways was they all sort of apply. [LAUGHS] There’s, like, a role for, you know, any of the functions, whether it be sort of driving consensus on research or building industry standards or managing, kind of, frontier risks, for thinking about how those might be needed in the AI context.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And one of the other big takeaways was that, you know, there are also limitations in these analogies. You know, each of the institutions grew up in its own, sort of, unique historical moment, like the one that we sit in with AI right now. And in each of those circumstances, they don’t exactly translate to this moment. And so, yeah, there was like this kind of, OK, we want to draw what we can from this conversation and then we also want to understand, what is also very important that’s just different for AI right now?&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;We published a book with the lessons from that conversation in 2023&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. And then we actually went on a bit of a tour [LAUGHS] with that content where we had a number of roundtables actually all over the world where we gathered feedback on how those analogies were landing, how our takeaways were landing. And one of the things that we took from them was a gap that some of the participants saw in the analogies that we chose to focus on. So across multiple conversations, other domains kept being raised, like, why did you not also study pharmaceuticals? Why did you also not study cybersecurity, for example? And so that, you know, naturally got us thinking about what further lessons we could draw from &lt;em&gt;those&lt;/em&gt; domains.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;At the same time, though, we also saw a need to, again, go deeper than what we went and really, like, focus on a narrower problem. So that’s really what led us to trying to think about a more specific problem where we could think across levels of governance and bring in some of these other domains. And, you know, testing was top of mind. Continues to be a really important topic in the AI policy conversation right now, I think, for really good reason. A lot of policymakers are focused on, you know, what we need to do to,&amp;nbsp;kind of, have there be sufficient trust, and testing is going to be a part of that—really better understand risk, enable everyone to be able to make more, kind of, risk-informed decisions, right. Testing is an important component for governance and AI and, of course, in all of these other domains, as well.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So I’ll just add the other, kind of, input into the process for this second round was exploring other analogies beyond those that we, kind of, got feedback on. And one of the early, kind of, examples of another domain that would be really worthwhile to study that came to mind from, sort of, just studying the literature was genome editing. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;You know, genome editing was really interesting through the process of thinking about other kind of general-purpose technologies. We also arrived at nanoscience and brought those into the conversation.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; That’s great. I mean, actually, if you could double-click,&amp;nbsp;I mean, you just named a number of industries. I’d love to just understand which of those worlds maybe feels the closest to what we’re wrestling with, with AI and maybe which is kind of the farthest off, and what makes them stand out to you?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Oh, such a good question. For this second round, we actually brought together eight different domains, right. And I think we actually thought we would come out of this conversation with some bit of clarity around, &lt;em&gt;Oh, if we just, sort of, take this approach for this domain or that domain, we’ll sort of have—at least for now—really solved part of the puzzle.&lt;/em&gt; [LAUGHS] And, you know, our public policy team the day after the workshop, we had a, sort of, follow-on discussion, and the very first thing that we started with in that conversation was like, &lt;em&gt;OK, so which of these domains?&lt;/em&gt; And fascinatingly, like, everyone was sort of like, &lt;em&gt;Ahh! &lt;/em&gt;[LAUGHS] &lt;em&gt;None of them are applying perfectly&lt;/em&gt;. I mean, this is also speaking to the limitations of analogies that we already acknowledged.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And also, you know, all of the experts from across these domains gave us really interesting insights into, sort of, the tradeoffs and the limitations and how they were working. None are really applying perfectly for us. But all of them do offer a thread of insight that is really useful for thinking about testing in AI, and there are some different dimensions that I think are really useful as framing for that.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I mean, one is just this horizontal-versus-vertical,&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;kind of, difference in domains and, you know, the horizontal technology like genome editing or nanoscience&amp;nbsp;just being inherently different and seemingly very similar to AI in that you want to be able to understand risks in the technology itself &lt;em&gt;and&lt;/em&gt; there is just so much contextual, sort of, factor that matters in the application of those technologies for how the risk manifests that you really need to, kind of, do those two things at once—of understanding the technology but then really thinking about risk and governance in the context of application versus, you know, a context like or a domain like civil aviation or nuclear technology, for example.&lt;/p&gt;



&lt;p&gt;You know, even in the workshop itself that we hosted late last year, where we brought together this second round of experts, it was really interesting. We actually started the conversation by trying to understand how those different domains defined risks, where they were able to set risk thresholds. That’s been such a part of the AI policy conversation in the last year. And, you know, it was really instructive that the more vertical domains were able to, sort of, snap to clearer answers much more quickly. [LAUGHS] But, like, the horizontal nanoscience and genome editing were not because it just depends, right. So anyway, the horizontal-vertical dimension seems like a really important one to draw from and apply to AI. &lt;/p&gt;



&lt;p&gt;The couple of others that I would offer is just, you know, thinking about the different kinds of technologies. You know, obviously, there’s some of the domains that we studied that they’re just inherently, sort of, like, physical technologies … a mix of physical and digital or virtual in a lot of cases because all of these are, of course, applying digital technology. But like, you know, there is just a difference between something like an &lt;em&gt;airplane&lt;/em&gt; or a &lt;em&gt;medical device&lt;/em&gt; or, you know, the more kind of virtual or intangible sort of technologies even, you know, of course, AI and some of the other like cyber and genome editing but also like, you know, financial services having some of that quality. And again, I think the thing that’s interesting to us about AI is to think about AI and risk evaluation of AI as being, you know, having a large component of that being about the kind of virtual or intangible technology. &lt;em&gt;And also&lt;/em&gt;, you know, there is a future of robotics where we might need to think about the, kind of, physical risk evaluation kind of work, as well.&lt;/p&gt;



&lt;p&gt;And then the final thing I’d maybe say in terms of thinking about which domains have the lessons for AI that are most applicable is just how they’ve grappled with these different kind of governance questions. Things like how to turn the dial in terms of being more or less prescriptive on risk evaluation approaches, how they think about the balance of, kind of, pre-market versus post-market risk evaluation in testing, and what the tradeoffs have been there across domains has been really interesting to kind of tease out. And then also thinking about, sort of, who does what?&lt;/p&gt;



&lt;p&gt;So, you know, in each of these different domains, it was interesting to hear about, like, you know, the role of industry, the role of governments, the role of third-party experts in designing evaluations and developing standards and actually doing the work, and, kind of, having the pull through of what it means for risk and governance decisions. There were, again, there was a variety of, sort of, approaches across these domains that I think were interesting for AI.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; You mentioned that there’s a number of different stakeholders to be considering across the board as we’re thinking about policy, as we’re thinking about regulation. Where can we collaborate more across industry? Is it academia? Regulators? Just, how can we move the needle faster?&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; I think all of the above [LAUGHTER] is needed. But it’s also really important to have all of that, kind of, expertise brought together, you know, and I think, you know, one of the things that we certainly heard from multiple of the domains, if not all of them, was that same actual interest and need and the same sort of ongoing work to try to figure that out.&lt;/p&gt;



&lt;p&gt;You know, even where there had been progress in some of the other domains with bringing together, you know, some industry stakeholders or, you know, industry and government, there was still a desire to actually do more there. Like, if there was some progress in industry and government, the need was, &lt;em&gt;And more kind of cross-jurisdiction government conversation&lt;/em&gt;, for example. Or some progress on, you know, within the industry but needing to, like, strengthen the partnership with academia, for example. So, you know, I think it speaks to, like, the quality of your question, to be honest, that, you know, all of these domains are actually still grappling with this and still seeing the need to grow in that direction more.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;What I’d say about AI today is that we have made good progress with, you know, starting to build some industry partnerships. You know, we were a founding member of the Frontier Model Forum, or FMF&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, which has been a very useful place for us to work with some peers on really trying to bring forward some best practices that apply across our organizations. You know, there are other forums as well, like MLCommons&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, where we’re working with others in industry and broader, sort of, academic and civil society communities. Partnership on AI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; is another one I think about that, kind of, fits that mold, as well, in a really positive way. And, like, there are a lot of different, sort of, governance needs to think through and where, you know, we can really think about bringing that expertise together is going to be so important.&lt;/p&gt;



&lt;p&gt;I think about almost, like, in the near to mid-term, like three issues that we need to address in the AI, kind of, policy and testing context. One is just building kind of, like, a flexible framework that allows us to really build trust while we continue to advance the science and the standards. You know, we are going to need to do both at once. And so we need a flexible framework that enables that kind of agility, and advancing the science and the standards, that &lt;em&gt;is &lt;/em&gt;going to be something that really demands that kind of cross-discipline or cross kind of expertise group coming together to work on that—researchers, academics, civil society, governments and, of course, industry.&lt;/p&gt;



&lt;p&gt;And so I think that is, actually, the second problem is, like, how do we actually build the kind of forums and ways of working together, the public-private partnership kind of efforts that allow all of that expertise to come together and fit together over time, right. Because when these are really big, broad challenges, you kind of have to break them down incrementally, make progress on them, and then bring them back together. &lt;/p&gt;



&lt;p&gt;And so I think about, like, one example that I, you know, really have been reflecting on lately is, you know, in the context of building standards, like, how do you do that, right? Again, standards are going to benefit from that whole community of expertise. And, you know, there are lots of different kinds of quote-unquote standards, though, right. You kind of have the “small &lt;em&gt;s&lt;/em&gt;” industry standards. You have the kind of “big &lt;em&gt;S&lt;/em&gt;” international standards, for example. And how do you, kind of, leverage one to accelerate the other, I think, is part of, like, how we need to work together within this ecosystem. And, like, I think what we and others have done in an organization like C2PA [Coalition for Content Provenance and Authenticity]&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, for example, where we’ve really built an industry specification but then built on that towards an international standard effort is one example that is interesting, right, to point to.&lt;/p&gt;



&lt;p&gt;And then, you know, I actually think that bridges to the third thing that we need to do together within this whole community, which is, you know, really think again about how we manage the breadth of this challenge and opportunity of AI by thinking about this horizontal-vertical problem. And, you know, I think that’s where it’s not just the sort of tech industry, for example. It’s broader industry that’s going to be really applying this technology that needs to get involved in the conversation about not just, sort of, testing AI models, for example, but also testing how AI systems or applications are working in context. And so, yes, so much fun opportunity!&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; Amanda, this was just fantastic. You’ve really set the stage for this podcast. And thank you so much for sharing your time and wisdom with us.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Thank you.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; And to our listeners, we’re so glad you joined us for this conversation. An exciting lineup of episodes are on the way, and we can’t wait to have you back for the next one.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&amp;nbsp;&lt;/p&gt;

				&lt;/span&gt;
			&lt;/div&gt;
			&lt;button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button"&gt;
				Show more			&lt;/button&gt;
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/podcast/ai-testing-and-evaluation-learnings-from-science-and-industry/</guid><pubDate>Mon, 23 Jun 2025 16:38:09 +0000</pubDate></item><item><title>[NEW] Google brings new Gemini features to Chromebooks, debuts first on-device AI (AI – Ars Technica)</title><link>https://arstechnica.com/google/2025/06/google-brings-new-gemini-features-to-chromebooks-debuts-first-on-device-ai/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google is bringing its AI obsession to Chrome OS.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Lenovo Chromebook Plus 14" class="absolute inset-0 w-full h-full object-cover hidden" height="405" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/9_Lenovo-Chromebook-Plus-14-copy-640x405.jpg" width="640" /&gt;
                  &lt;img alt="Lenovo Chromebook Plus 14" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/9_Lenovo-Chromebook-Plus-14-copy-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The new Lenovo Chromebook Plus 14 is the first Google-powered laptop to have on-device AI features. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google hasn't been talking about Chromebooks as much since AI became its all-consuming focus, but that's changing today with a bounty of new AI features for Google-powered laptops. Newer, more powerful Chromebooks will soon have image generation, text summarization, and more built into the OS. There's also a new Lenovo Chromebook with a few exclusive AI goodies that only work thanks to its overpowered hardware.&lt;/p&gt;
&lt;p&gt;If you have a Chromebook Plus device, which requires a modern CPU and at least 8GB of RAM, your machine will soon get a collection of features you may recognize from other Google products. For example, Lens is expanding on Chrome OS, allowing you to long-press the launcher icon to select any area of the screen to perform a visual search. Lens also includes text capture and integration with Google Calendar and Docs.&lt;/p&gt;
&lt;p&gt;Gemini models are also playing a role here, according to Google. The Quick Insert key, which debuted last year, is gaining a new visual element. It could already insert photos or emoji with ease, but it can now also help you generate a new image on demand with AI.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Google's new Chromebook AI features.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;Even though Google's AI features are running in the cloud, the AI additions are limited to this more powerful class of Google-powered laptops. The Help Me Read feature leverages Gemini to summarize long documents and webpages, and it can now distill that data into a more basic form. The new Summarize option can turn dense, technical text into something more readable in a few clicks.&lt;/p&gt;
&lt;p&gt;Google has also rolled out a new AI trial for Chromebook Plus devices. If you buy one of these premium Chromebooks, you'll get a 12-month free trial of the Google AI Pro plan, which gives you 2TB of cloud storage, expanded access to Google's Gemini Pro model, and NotebookLM Pro. NotebookLM is also getting a place in the Chrome OS shelf.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;The dawn of on-device Chromebook AI&lt;/h2&gt;
&lt;p&gt;Along with the new features for all Chromebook Plus devices, Google and Lenovo have teamed up to add some extra AI enhancements to the new Chromebook Plus 14. This machine runs on the MediaTek Kompanio Ultra processor, which Google calls the "strongest ever ARM chip in a Chromebook." The Kompanio Ultra has an NPU capable of 50 TOPS of AI throughput, which is enough to run useful AI models on the machine itself. It's a bit like a Windows-based Copilot+ laptop in that sense.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2102271 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="Lenovo Chromebook specs" class="fullwidth full" height="1070" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/lenovo-14-plus.png" width="1919" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Most of the AI models we encounter in consumer technology are running on giant servers in a data center somewhere. You may not want to send all your data to someone else's server, which is why companies are also working on more focused on-device AI. For example, Microsoft made Recall slightly less alarming by running all the image analysis on local machines, and Google uses Gemini Nano on Pixel phones to parse sensitive data like voice recordings and text messages.&lt;/p&gt;
&lt;p&gt;For the Lenovo Chromebook Plus 14, Google has implemented two local AI features. Smart Grouping will organize open tabs and documents into "logical groups" with the help of AI. The Chrome OS Gallery app on this machine will gain the ability to edit photos with AI as well. Users will be able to remove backgrounds in one step and create stickers, all without sending data to the cloud.&lt;/p&gt;
&lt;p&gt;As more Chromebooks launch with powerful NPUs, we could see these features expand. However, they are exclusive to the Lenovo Chromebook Plus 14 for now. This laptop is available today at Best Buy for $749.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google is bringing its AI obsession to Chrome OS.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Lenovo Chromebook Plus 14" class="absolute inset-0 w-full h-full object-cover hidden" height="405" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/9_Lenovo-Chromebook-Plus-14-copy-640x405.jpg" width="640" /&gt;
                  &lt;img alt="Lenovo Chromebook Plus 14" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/9_Lenovo-Chromebook-Plus-14-copy-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The new Lenovo Chromebook Plus 14 is the first Google-powered laptop to have on-device AI features. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google hasn't been talking about Chromebooks as much since AI became its all-consuming focus, but that's changing today with a bounty of new AI features for Google-powered laptops. Newer, more powerful Chromebooks will soon have image generation, text summarization, and more built into the OS. There's also a new Lenovo Chromebook with a few exclusive AI goodies that only work thanks to its overpowered hardware.&lt;/p&gt;
&lt;p&gt;If you have a Chromebook Plus device, which requires a modern CPU and at least 8GB of RAM, your machine will soon get a collection of features you may recognize from other Google products. For example, Lens is expanding on Chrome OS, allowing you to long-press the launcher icon to select any area of the screen to perform a visual search. Lens also includes text capture and integration with Google Calendar and Docs.&lt;/p&gt;
&lt;p&gt;Gemini models are also playing a role here, according to Google. The Quick Insert key, which debuted last year, is gaining a new visual element. It could already insert photos or emoji with ease, but it can now also help you generate a new image on demand with AI.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Google's new Chromebook AI features.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;Even though Google's AI features are running in the cloud, the AI additions are limited to this more powerful class of Google-powered laptops. The Help Me Read feature leverages Gemini to summarize long documents and webpages, and it can now distill that data into a more basic form. The new Summarize option can turn dense, technical text into something more readable in a few clicks.&lt;/p&gt;
&lt;p&gt;Google has also rolled out a new AI trial for Chromebook Plus devices. If you buy one of these premium Chromebooks, you'll get a 12-month free trial of the Google AI Pro plan, which gives you 2TB of cloud storage, expanded access to Google's Gemini Pro model, and NotebookLM Pro. NotebookLM is also getting a place in the Chrome OS shelf.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;The dawn of on-device Chromebook AI&lt;/h2&gt;
&lt;p&gt;Along with the new features for all Chromebook Plus devices, Google and Lenovo have teamed up to add some extra AI enhancements to the new Chromebook Plus 14. This machine runs on the MediaTek Kompanio Ultra processor, which Google calls the "strongest ever ARM chip in a Chromebook." The Kompanio Ultra has an NPU capable of 50 TOPS of AI throughput, which is enough to run useful AI models on the machine itself. It's a bit like a Windows-based Copilot+ laptop in that sense.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2102271 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="Lenovo Chromebook specs" class="fullwidth full" height="1070" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/lenovo-14-plus.png" width="1919" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Most of the AI models we encounter in consumer technology are running on giant servers in a data center somewhere. You may not want to send all your data to someone else's server, which is why companies are also working on more focused on-device AI. For example, Microsoft made Recall slightly less alarming by running all the image analysis on local machines, and Google uses Gemini Nano on Pixel phones to parse sensitive data like voice recordings and text messages.&lt;/p&gt;
&lt;p&gt;For the Lenovo Chromebook Plus 14, Google has implemented two local AI features. Smart Grouping will organize open tabs and documents into "logical groups" with the help of AI. The Chrome OS Gallery app on this machine will gain the ability to edit photos with AI as well. Users will be able to remove backgrounds in one step and create stickers, all without sending data to the cloud.&lt;/p&gt;
&lt;p&gt;As more Chromebooks launch with powerful NPUs, we could see these features expand. However, they are exclusive to the Lenovo Chromebook Plus 14 for now. This laptop is available today at Best Buy for $749.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2025/06/google-brings-new-gemini-features-to-chromebooks-debuts-first-on-device-ai/</guid><pubDate>Mon, 23 Jun 2025 16:56:24 +0000</pubDate></item><item><title>[NEW] Judge denies creating “mass surveillance program” harming all ChatGPT users (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/06/judge-rejects-claim-that-forcing-openai-to-keep-chatgpt-logs-is-mass-surveillance/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI will fight order to keep all ChatGPT logs after users fail to sway court.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GettyImages-2185905668-640x360.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GettyImages-2185905668-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Yurii Karvatskyi | iStock / Getty Images Plus

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;After a court ordered OpenAI to "indefinitely" retain all ChatGPT logs, including deleted chats, of millions of users, two panicked users tried and failed to intervene. The order sought to preserve potential evidence in a copyright infringement lawsuit raised by news organizations.&lt;/p&gt;
&lt;p&gt;In May, Judge Ona Wang, who drafted the order, rejected the first user's request on behalf of his company simply because the company should have hired a lawyer to draft the filing. But more recently, Wang rejected a second claim from another ChatGPT user, and that order went into greater detail, revealing how the judge is considering opposition to the order ahead of oral arguments this week, which were urgently requested by OpenAI.&lt;/p&gt;
&lt;p&gt;The second request to intervene came from a ChatGPT user named Aidan Hunt, who said that he uses ChatGPT "from time to time," occasionally sending OpenAI "highly sensitive personal and commercial information in the course of using the service."&lt;/p&gt;
&lt;p&gt;In his filing, Hunt alleged that Wang's preservation order created a "nationwide mass surveillance program" affecting and potentially harming "all ChatGPT users," who received no warning that their deleted and anonymous chats were suddenly being retained. He warned that the order limiting retention to just ChatGPT outputs carried the same risks as including user inputs, since outputs "inherently reveal, and often explicitly restate, the input questions or topics input."&lt;/p&gt;
&lt;p&gt;Hunt claimed that he only learned that ChatGPT was retaining this information—despite policies specifying they would not—by stumbling upon the news in an online forum. Feeling that his Fourth Amendment and due process rights were being infringed, Hunt sought to influence the court's decision and proposed a motion to vacate the order that said Wang's "order effectively requires Defendants to implement a mass surveillance program affecting all ChatGPT users."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Hunt's fears are not unfounded, Corynne McSherry, legal director for the digital rights group the Electronic Frontier Foundation, told Ars.&lt;/p&gt;
&lt;p&gt;"The discovery order poses genuine risks to user privacy in itself and as a precedent for the many other lawsuits around the country," McSherry said. "And it is emblematic of a broader problem: AI chatbots are opening another vector for corporate surveillance, especially if users don't have meaningful control over what happens to their chat histories and records."&lt;/p&gt;
&lt;p&gt;According to Hunt, Wang failed to "consider exempting 'Anonymous Chats,' which are reasonably expected to contain the most sensitive and potentially damaging information of users, from retention and disclosure in this case," claiming that it "constitutes an overly broad and unreasonable action."&lt;/p&gt;
&lt;p&gt;He urged the judge to revise the order to include this exemption, as well as exemptions for any chats "discussing medical, financial, legal, and personal topics that contain deeply private information of users and bear no relevance whatsoever" to the plaintiff news organizations' claimed interests.&lt;/p&gt;
&lt;p&gt;For Hunt and many other users blindsided by the order, the stakes appear high. He suggested that Wang should have allowed him to intervene "because this case involves important, novel constitutional questions about the privacy rights incident to artificial intelligence usage—a rapidly developing area of law—and the ability of a magistrate to institute a nationwide mass surveillance program by means of a discovery order in a civil case."&lt;/p&gt;
&lt;p&gt;But Wang disagreed with Hunt that she exceeded her authority in enforcing the order, emphasizing in a footnote that her order cannot be construed as enabling mass surveillance.&lt;/p&gt;
&lt;p&gt;"Proposed Intervenor does not explain how a court’s document retention order that directs the preservation, segregation, and retention of certain privately held data by a private company for the limited purposes of litigation is, or could be, a 'nationwide mass surveillance program,'" Wang wrote. "It is not. The judiciary is not a law enforcement agency."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;However, McSherry warned that "it's only a matter of time before law enforcement and private litigants start going to OpenAI to try to get chat histories/records about users for all sorts of purposes, just as they do already for search histories, social media posts, etc." Wang's order could become a gateway to that future, she said.&lt;/p&gt;
&lt;p&gt;Wang rejected Hunt's motion primarily because "whether the temporary preservation of certain chat output log data that was routinely being deleted by OpenAI throughout the course of this litigation may infringe on purported constitutional and contractual privacy rights of individual consumers that use ChatGPT" was deemed a "collateral issue" that does not directly pertain to the central question of copyright infringement.&lt;/p&gt;
&lt;p&gt;Finding that Hunt's intervention would not contribute "in any way" to "the development of the underlying factual issues in this case," Wang ruled that Hunt ultimately had no right to intervene.&lt;/p&gt;
&lt;p&gt;"None of Proposed Intervenor’s purported 'novel' questions are at issue in this copyright infringement action," Wang wrote. "Even if the Court were to entertain such questions, they would only work to unduly delay the resolution of the legal questions actually at issue."&lt;/p&gt;
&lt;h2&gt;User questions how hard OpenAI will fight&lt;/h2&gt;
&lt;p&gt;OpenAI will have a chance to defend panicked users on June 26, when Wang hears oral arguments over the ChatGPT maker's concerns about the preservation order.&lt;/p&gt;
&lt;p&gt;In his filing, Hunt explained that among his worst fears is that the order will not be blocked and that chat data will be disclosed to news plaintiffs who may be motivated to publicly disseminate the deleted chats.&lt;/p&gt;
&lt;p&gt;That could happen if news organizations find evidence of deleted chats they say are likely to contain user attempts to generate full news articles. Ars could not immediately reach a spokesperson for the lead plaintiff in the copyright lawsuit, The New York Times, for comment on this alleged privacy risk for ChatGPT users.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Wang suggested that there is no risk at this time since no chat data has yet been disclosed to the news organizations. That could mean that ChatGPT users may have better luck intervening after chat data is shared, should OpenAI's fight to block the order this week fail. But that's likely no comfort to users like Hunt, who worry that OpenAI merely retaining the data—even if it's never shared with news organizations—could cause severe and irreparable harms.&lt;/p&gt;
&lt;p&gt;Some users appear to be questioning how hard OpenAI will fight. In particular, Hunt is worried that OpenAI may not prioritize defending users' privacy if other concerns—like "financial costs of the case, desire for a quick resolution, and avoiding reputational damage"—are deemed more important, his filing said.&lt;/p&gt;
&lt;p&gt;OpenAI did not immediately respond to Ars' request to comment. The company previously provided a breakdown of affected users and vowed to fight the order.&lt;/p&gt;
&lt;p&gt;For now, ChatGPT users must wait to see the fate of their most sensitive chat logs. Intervening ChatGPT users had tried to argue that, at minimum, OpenAI should have been required to directly notify users that their deleted and anonymous chats were being retained. Hunt suggested that it would have stopped him from inputting sensitive data sooner. McSherry told Ars that more transparency will be needed as courts continue tangling with cases impacting chatbot users.&lt;/p&gt;
&lt;p&gt;"All AI chat apps should be taking steps not only to ensure that users can delete their records and be sure they are actually erased but also to ensure that users get timely notice of demands for their information," McSherry said. "If they aren't already doing so, they should also commit to regular transparency reporting about demands for user data."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI will fight order to keep all ChatGPT logs after users fail to sway court.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GettyImages-2185905668-640x360.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GettyImages-2185905668-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Yurii Karvatskyi | iStock / Getty Images Plus

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;After a court ordered OpenAI to "indefinitely" retain all ChatGPT logs, including deleted chats, of millions of users, two panicked users tried and failed to intervene. The order sought to preserve potential evidence in a copyright infringement lawsuit raised by news organizations.&lt;/p&gt;
&lt;p&gt;In May, Judge Ona Wang, who drafted the order, rejected the first user's request on behalf of his company simply because the company should have hired a lawyer to draft the filing. But more recently, Wang rejected a second claim from another ChatGPT user, and that order went into greater detail, revealing how the judge is considering opposition to the order ahead of oral arguments this week, which were urgently requested by OpenAI.&lt;/p&gt;
&lt;p&gt;The second request to intervene came from a ChatGPT user named Aidan Hunt, who said that he uses ChatGPT "from time to time," occasionally sending OpenAI "highly sensitive personal and commercial information in the course of using the service."&lt;/p&gt;
&lt;p&gt;In his filing, Hunt alleged that Wang's preservation order created a "nationwide mass surveillance program" affecting and potentially harming "all ChatGPT users," who received no warning that their deleted and anonymous chats were suddenly being retained. He warned that the order limiting retention to just ChatGPT outputs carried the same risks as including user inputs, since outputs "inherently reveal, and often explicitly restate, the input questions or topics input."&lt;/p&gt;
&lt;p&gt;Hunt claimed that he only learned that ChatGPT was retaining this information—despite policies specifying they would not—by stumbling upon the news in an online forum. Feeling that his Fourth Amendment and due process rights were being infringed, Hunt sought to influence the court's decision and proposed a motion to vacate the order that said Wang's "order effectively requires Defendants to implement a mass surveillance program affecting all ChatGPT users."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Hunt's fears are not unfounded, Corynne McSherry, legal director for the digital rights group the Electronic Frontier Foundation, told Ars.&lt;/p&gt;
&lt;p&gt;"The discovery order poses genuine risks to user privacy in itself and as a precedent for the many other lawsuits around the country," McSherry said. "And it is emblematic of a broader problem: AI chatbots are opening another vector for corporate surveillance, especially if users don't have meaningful control over what happens to their chat histories and records."&lt;/p&gt;
&lt;p&gt;According to Hunt, Wang failed to "consider exempting 'Anonymous Chats,' which are reasonably expected to contain the most sensitive and potentially damaging information of users, from retention and disclosure in this case," claiming that it "constitutes an overly broad and unreasonable action."&lt;/p&gt;
&lt;p&gt;He urged the judge to revise the order to include this exemption, as well as exemptions for any chats "discussing medical, financial, legal, and personal topics that contain deeply private information of users and bear no relevance whatsoever" to the plaintiff news organizations' claimed interests.&lt;/p&gt;
&lt;p&gt;For Hunt and many other users blindsided by the order, the stakes appear high. He suggested that Wang should have allowed him to intervene "because this case involves important, novel constitutional questions about the privacy rights incident to artificial intelligence usage—a rapidly developing area of law—and the ability of a magistrate to institute a nationwide mass surveillance program by means of a discovery order in a civil case."&lt;/p&gt;
&lt;p&gt;But Wang disagreed with Hunt that she exceeded her authority in enforcing the order, emphasizing in a footnote that her order cannot be construed as enabling mass surveillance.&lt;/p&gt;
&lt;p&gt;"Proposed Intervenor does not explain how a court’s document retention order that directs the preservation, segregation, and retention of certain privately held data by a private company for the limited purposes of litigation is, or could be, a 'nationwide mass surveillance program,'" Wang wrote. "It is not. The judiciary is not a law enforcement agency."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;However, McSherry warned that "it's only a matter of time before law enforcement and private litigants start going to OpenAI to try to get chat histories/records about users for all sorts of purposes, just as they do already for search histories, social media posts, etc." Wang's order could become a gateway to that future, she said.&lt;/p&gt;
&lt;p&gt;Wang rejected Hunt's motion primarily because "whether the temporary preservation of certain chat output log data that was routinely being deleted by OpenAI throughout the course of this litigation may infringe on purported constitutional and contractual privacy rights of individual consumers that use ChatGPT" was deemed a "collateral issue" that does not directly pertain to the central question of copyright infringement.&lt;/p&gt;
&lt;p&gt;Finding that Hunt's intervention would not contribute "in any way" to "the development of the underlying factual issues in this case," Wang ruled that Hunt ultimately had no right to intervene.&lt;/p&gt;
&lt;p&gt;"None of Proposed Intervenor’s purported 'novel' questions are at issue in this copyright infringement action," Wang wrote. "Even if the Court were to entertain such questions, they would only work to unduly delay the resolution of the legal questions actually at issue."&lt;/p&gt;
&lt;h2&gt;User questions how hard OpenAI will fight&lt;/h2&gt;
&lt;p&gt;OpenAI will have a chance to defend panicked users on June 26, when Wang hears oral arguments over the ChatGPT maker's concerns about the preservation order.&lt;/p&gt;
&lt;p&gt;In his filing, Hunt explained that among his worst fears is that the order will not be blocked and that chat data will be disclosed to news plaintiffs who may be motivated to publicly disseminate the deleted chats.&lt;/p&gt;
&lt;p&gt;That could happen if news organizations find evidence of deleted chats they say are likely to contain user attempts to generate full news articles. Ars could not immediately reach a spokesperson for the lead plaintiff in the copyright lawsuit, The New York Times, for comment on this alleged privacy risk for ChatGPT users.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Wang suggested that there is no risk at this time since no chat data has yet been disclosed to the news organizations. That could mean that ChatGPT users may have better luck intervening after chat data is shared, should OpenAI's fight to block the order this week fail. But that's likely no comfort to users like Hunt, who worry that OpenAI merely retaining the data—even if it's never shared with news organizations—could cause severe and irreparable harms.&lt;/p&gt;
&lt;p&gt;Some users appear to be questioning how hard OpenAI will fight. In particular, Hunt is worried that OpenAI may not prioritize defending users' privacy if other concerns—like "financial costs of the case, desire for a quick resolution, and avoiding reputational damage"—are deemed more important, his filing said.&lt;/p&gt;
&lt;p&gt;OpenAI did not immediately respond to Ars' request to comment. The company previously provided a breakdown of affected users and vowed to fight the order.&lt;/p&gt;
&lt;p&gt;For now, ChatGPT users must wait to see the fate of their most sensitive chat logs. Intervening ChatGPT users had tried to argue that, at minimum, OpenAI should have been required to directly notify users that their deleted and anonymous chats were being retained. Hunt suggested that it would have stopped him from inputting sensitive data sooner. McSherry told Ars that more transparency will be needed as courts continue tangling with cases impacting chatbot users.&lt;/p&gt;
&lt;p&gt;"All AI chat apps should be taking steps not only to ensure that users can delete their records and be sure they are actually erased but also to ensure that users get timely notice of demands for their information," McSherry said. "If they aren't already doing so, they should also commit to regular transparency reporting about demands for user data."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/06/judge-rejects-claim-that-forcing-openai-to-keep-chatgpt-logs-is-mass-surveillance/</guid><pubDate>Mon, 23 Jun 2025 17:33:11 +0000</pubDate></item><item><title>[NEW] Over a million people now have access to the gen-AI powered Alexa+ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/23/over-a-million-people-now-have-access-to-the-gen-ai-powered-alexa/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;More invites to Amazon’s upgraded digital assistant, Alexa+, powered by generative AI, have been steadily rolling out. The service, first announced in February, now reaches over a million users, Amazon confirmed to TechCrunch on Monday. However, Alexa+ is not yet publicly available. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead, Amazon has been working through its waitlist, sending out invites to those customers who originally signed up to test the service when it became available. Over the past several weeks, many people have shared on social media that they’ve received an invite to try Alexa+, whose service offers more natural and personalized interactions, smart home integration, and expanded capabilities thanks to AI.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Alexa+ is available for free during Early Access and will later be free for Prime customers. Non-Prime users will be able to use the service for $19.99 per month after it publicly launches.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon earlier this year noted that invites to try out the new system would roll out in waves in the months ahead. As of May 2025, Amazon CEO Andy Jassy said that Alexa+ had so far reached over 100,000 users, representing only a tiny fraction of the 600 million Alexa devices that had been sold. That number has grown significantly in the weeks since.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3021198" height="383" src="https://techcrunch.com/wp-content/uploads/2025/06/download1.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Alexa+ represents a serious attempt by Amazon to create a generative AI experience for consumers that it can eventually monetize.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Though Amazon created a market for smart home-connected, voice-based assistants through its Alexa-powered Echo devices, it wasn’t able to turn that traction into a revenue-generating business. Meanwhile, Alexa lost its shine in more recent years as generative AI services, like ChatGPT, took off. Compared with modern-day AI, Alexa began to feel clunky, constrained, and underpowered. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alexa+ aims to bring the digital assistant new capabilities. The service allows users to chat with the digital assistant using more natural language, where you can phrase requests your own way. For instance, you could tell Alexa, “It’s too cold in here,” to have Alexa adjust their smart thermostat. You’ll also more easily be able to create routines, search across your Ring camera footage, interrupt or pivot the conversation with the assistant, and more.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;The experience is more personalized, too, as it saves your preferences and remembers what you like, from favorite songs to recipes and beyond.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3021199" height="383" src="https://techcrunch.com/wp-content/uploads/2025/06/download.gif?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;With its generative AI component, Alexa can do things like summarize long emails you share with the service, create unique bedtime stories, generate quizzes from study guides, make travel itineraries, provide summaries of your smart home activity, and answer other questions, similar to how an AI chatbot might respond.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, the assistant will be able to help you take certain actions — like buying concert tickets, booking a dinner reservation, and notifying you when something you’ve been watching goes on sale, among other things. Initial partners on this feature include OpenTable, Ticketmaster, Uber Eats, Tripadvisor, Grubhub, Yelp, Priceline, Viator, Thumbtack, Atom, Fodor’s, and others.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While in early access, Alexa+ will initially be available on Echo Show 8, 10, 15, or 21 devices in the U.S. Over time, it will expand to more Echo customers, Fire TV users, and Fire tablet users.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Customers who have been invited to try the product are reporting mixed results, with some praising the service, noting it’s more advanced than Siri, while others said it’s still rough around the edges. (Some early reviews agree with the latter.) &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s worth pointing out that Alexa+ is not currently fully launched, but it’s getting close. Amazon says nearly 90% of the features it previously announced have since shipped. &lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;More invites to Amazon’s upgraded digital assistant, Alexa+, powered by generative AI, have been steadily rolling out. The service, first announced in February, now reaches over a million users, Amazon confirmed to TechCrunch on Monday. However, Alexa+ is not yet publicly available. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead, Amazon has been working through its waitlist, sending out invites to those customers who originally signed up to test the service when it became available. Over the past several weeks, many people have shared on social media that they’ve received an invite to try Alexa+, whose service offers more natural and personalized interactions, smart home integration, and expanded capabilities thanks to AI.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Alexa+ is available for free during Early Access and will later be free for Prime customers. Non-Prime users will be able to use the service for $19.99 per month after it publicly launches.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon earlier this year noted that invites to try out the new system would roll out in waves in the months ahead. As of May 2025, Amazon CEO Andy Jassy said that Alexa+ had so far reached over 100,000 users, representing only a tiny fraction of the 600 million Alexa devices that had been sold. That number has grown significantly in the weeks since.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3021198" height="383" src="https://techcrunch.com/wp-content/uploads/2025/06/download1.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Alexa+ represents a serious attempt by Amazon to create a generative AI experience for consumers that it can eventually monetize.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Though Amazon created a market for smart home-connected, voice-based assistants through its Alexa-powered Echo devices, it wasn’t able to turn that traction into a revenue-generating business. Meanwhile, Alexa lost its shine in more recent years as generative AI services, like ChatGPT, took off. Compared with modern-day AI, Alexa began to feel clunky, constrained, and underpowered. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alexa+ aims to bring the digital assistant new capabilities. The service allows users to chat with the digital assistant using more natural language, where you can phrase requests your own way. For instance, you could tell Alexa, “It’s too cold in here,” to have Alexa adjust their smart thermostat. You’ll also more easily be able to create routines, search across your Ring camera footage, interrupt or pivot the conversation with the assistant, and more.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;The experience is more personalized, too, as it saves your preferences and remembers what you like, from favorite songs to recipes and beyond.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3021199" height="383" src="https://techcrunch.com/wp-content/uploads/2025/06/download.gif?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;With its generative AI component, Alexa can do things like summarize long emails you share with the service, create unique bedtime stories, generate quizzes from study guides, make travel itineraries, provide summaries of your smart home activity, and answer other questions, similar to how an AI chatbot might respond.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, the assistant will be able to help you take certain actions — like buying concert tickets, booking a dinner reservation, and notifying you when something you’ve been watching goes on sale, among other things. Initial partners on this feature include OpenTable, Ticketmaster, Uber Eats, Tripadvisor, Grubhub, Yelp, Priceline, Viator, Thumbtack, Atom, Fodor’s, and others.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While in early access, Alexa+ will initially be available on Echo Show 8, 10, 15, or 21 devices in the U.S. Over time, it will expand to more Echo customers, Fire TV users, and Fire tablet users.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Customers who have been invited to try the product are reporting mixed results, with some praising the service, noting it’s more advanced than Siri, while others said it’s still rough around the edges. (Some early reviews agree with the latter.) &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s worth pointing out that Alexa+ is not currently fully launched, but it’s getting close. Amazon says nearly 90% of the features it previously announced have since shipped. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/23/over-a-million-people-now-have-access-to-the-gen-ai-powered-alexa/</guid><pubDate>Mon, 23 Jun 2025 17:38:40 +0000</pubDate></item><item><title>[NEW] Four months after a $3B valuation, Harvey AI grows to $5B (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/23/four-months-after-a-3b-valuation-harvey-ai-grows-to-5b/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/09/GettyImages-1474442258.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Harvey AI, a startup that provides automation for legal work, has raised $300 million in Series E funding at a $5 billion valuation, the company told Fortune.&amp;nbsp;The round was co-led by Kleiner Perkins and Coatue, with participation from existing investors, including Conviction, Elad Gil, OpenAI Startup Fund, and Sequoia.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The financing comes just four months after Harvey announced that Sequoia led a $300 million Series D round at a $3 billion valuation.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While many AI companies aim to keep headcount low, Harvey is rapidly expanding its staff. The three-year-old startup already employs 340 people and plans to double that number with its fresh funds, Fortune reported. Some of the new staff will be hired to help Harvey build AI products for professional services beyond legal, including tax accounting. The company’s AI solutions, which assist lawyers in reviewing documents and drafting contracts, are used by 337 legal clients.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Harvey has been growing at a rapid clip, reaching an annualized run-rate revenue of $75 million in April, up from $50 million earlier in the year, Reuters reported last month.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some of Harvey’s competitors include older legal startups, such as 10-year-old Ironclad and 17-year-old Clio, which raised a $300 million round at a $3 billion valuation last year.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/09/GettyImages-1474442258.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Harvey AI, a startup that provides automation for legal work, has raised $300 million in Series E funding at a $5 billion valuation, the company told Fortune.&amp;nbsp;The round was co-led by Kleiner Perkins and Coatue, with participation from existing investors, including Conviction, Elad Gil, OpenAI Startup Fund, and Sequoia.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The financing comes just four months after Harvey announced that Sequoia led a $300 million Series D round at a $3 billion valuation.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While many AI companies aim to keep headcount low, Harvey is rapidly expanding its staff. The three-year-old startup already employs 340 people and plans to double that number with its fresh funds, Fortune reported. Some of the new staff will be hired to help Harvey build AI products for professional services beyond legal, including tax accounting. The company’s AI solutions, which assist lawyers in reviewing documents and drafting contracts, are used by 337 legal clients.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Harvey has been growing at a rapid clip, reaching an annualized run-rate revenue of $75 million in April, up from $50 million earlier in the year, Reuters reported last month.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some of Harvey’s competitors include older legal startups, such as 10-year-old Ironclad and 17-year-old Clio, which raised a $300 million round at a $3 billion valuation last year.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/23/four-months-after-a-3b-valuation-harvey-ai-grows-to-5b/</guid><pubDate>Mon, 23 Jun 2025 18:03:00 +0000</pubDate></item><item><title>[NEW] Leak reveals Grok might soon edit your spreadsheets (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/23/leak-reveals-grok-might-soon-edit-your-spreadsheets/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/11/xAI-Grok-GettyImages-1765893916.jpeg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Leaked code suggests xAI is developing an advanced file editor for Grok with spreadsheet support, signaling the company’s push to compete with OpenAI, Google, and Microsoft by embedding AI copilots into productivity tools.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You can talk to Grok and ask it to assist you at the same time you’re editing the files!” writes reverse engineer Nima Owji, who leaked the finding.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed aligncenter is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;BREAKING: xAI is working on an advanced FILE EDITOR for GROK!&lt;/p&gt;&lt;p&gt;It even supports SPREADSHEETS!&lt;/p&gt;&lt;p&gt;You can talk to Grok and ask it to assist you at the same time you're editing the files! pic.twitter.com/9vIKRZj6Wn&lt;/p&gt;— Nima Owji (@nima_owji) June 22, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to xAI to confirm the findings and learn more.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;xAI hasn’t explicitly detailed its strategy for pursuing interactive, multimodal AI workspaces, but it has dropped a series of announcements that point to how the company is thinking about these tools. In April 2025, xAI launched Grok Studio, a split-screen workspace that lets users collaborate with Grok on generating documents, code, reports, and browser games. It also launched the ability to create Workspaces that let you organize files and conversations in a single place.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While OpenAI and Microsoft have similar tools, Google’s Gemini Workspace for Sheets, Docs, and Gmail appears to be the most similar to what xAI is reportedly building. Google’s tools can edit Docs and Sheets and allow you to chat with Gemini while looking at or editing documents. The difference is that Gemini Workspace only works within Google’s own ecosystem.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s not clear what types of files xAI’s editor might support aside from spreadsheets, or whether xAI plans to build a full productivity suite that could compete with Google Workspace or Microsoft 365.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If Owji’s findings are true, the advanced editor would be a step towards Elon Musk’s ambitions to turn X into an “everything app” that includes docs, chat, payments, and social media.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/11/xAI-Grok-GettyImages-1765893916.jpeg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Leaked code suggests xAI is developing an advanced file editor for Grok with spreadsheet support, signaling the company’s push to compete with OpenAI, Google, and Microsoft by embedding AI copilots into productivity tools.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You can talk to Grok and ask it to assist you at the same time you’re editing the files!” writes reverse engineer Nima Owji, who leaked the finding.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed aligncenter is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;BREAKING: xAI is working on an advanced FILE EDITOR for GROK!&lt;/p&gt;&lt;p&gt;It even supports SPREADSHEETS!&lt;/p&gt;&lt;p&gt;You can talk to Grok and ask it to assist you at the same time you're editing the files! pic.twitter.com/9vIKRZj6Wn&lt;/p&gt;— Nima Owji (@nima_owji) June 22, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to xAI to confirm the findings and learn more.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;xAI hasn’t explicitly detailed its strategy for pursuing interactive, multimodal AI workspaces, but it has dropped a series of announcements that point to how the company is thinking about these tools. In April 2025, xAI launched Grok Studio, a split-screen workspace that lets users collaborate with Grok on generating documents, code, reports, and browser games. It also launched the ability to create Workspaces that let you organize files and conversations in a single place.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While OpenAI and Microsoft have similar tools, Google’s Gemini Workspace for Sheets, Docs, and Gmail appears to be the most similar to what xAI is reportedly building. Google’s tools can edit Docs and Sheets and allow you to chat with Gemini while looking at or editing documents. The difference is that Gemini Workspace only works within Google’s own ecosystem.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s not clear what types of files xAI’s editor might support aside from spreadsheets, or whether xAI plans to build a full productivity suite that could compete with Google Workspace or Microsoft 365.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If Owji’s findings are true, the advanced editor would be a step towards Elon Musk’s ambitions to turn X into an “everything app” that includes docs, chat, payments, and social media.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/23/leak-reveals-grok-might-soon-edit-your-spreadsheets/</guid><pubDate>Mon, 23 Jun 2025 18:16:57 +0000</pubDate></item></channel></rss>