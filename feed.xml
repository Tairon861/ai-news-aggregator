<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 05 Sep 2025 01:38:33 +0000</lastBuildDate><item><title> ()</title><link>https://venturebeat.com/category/ai/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://venturebeat.com/category/ai/feed/</guid></item><item><title>Only 2 days left to claim your exhibitor spot at TechCrunch Disrupt 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/04/only-2-days-left-to-claim-your-exhibitor-spot-at-techcrunch-disrupt-2025/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;We’re in the final stretch — with just two days left and only 10 tables remaining, now is the time to act if you want to shine the light on your brand at one of the most anticipated tech conferences of the year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; is coming to Moscone West in San Francisco this October 27–29. Exhibiting puts your startup in the heart of the action — where 10,000+ founders, investors, press, and decision-makers come to discover what’s next.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Your brand missing the spotlight at this tech epicenter means missed ROI. Disrupt is &lt;em&gt;the&lt;/em&gt; launchpad for scaling startups — &lt;strong&gt;claim your exhibit table before your competitor does&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2758507" height="453" src="https://techcrunch.com/wp-content/uploads/2024/05/expo_startup_fun.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-what-you-get-with-your-table"&gt;What you get with your table&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Enjoy three full days of brand spotlighting in the heart of Disrupt — where investors are scouting their next deal, your target customers are ready to buy, and the tech press is searching for the next breakthrough, plus:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;A 6’ x 30″ table with linen and chairs, complete with an 11” x 14” branded tabletop sign for all-day visibility.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;A Silver Tier sponsorship with branding across the venue, site, and app.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;10 free passes for your team to experience all the activations at Disrupt.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;High-quality lead generation.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Access to the Disrupt press list.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;And more. Check out the exhibit page to learn all the perks of exhibiting.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;Exhibitor tables are disappearing fast, and once they’re gone, they’re gone. Don’t let your competitor own the spotlight while you watch from the sidelines. &lt;strong&gt;Book now before tomorrow’s deadline, or before it sells out&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 no anniversary" class="wp-image-3040972" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/TC25_Disrupt_General_Article_No-Anniversary-at-all_Headers_1920x1080.png?w=680" width="680" /&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;We’re in the final stretch — with just two days left and only 10 tables remaining, now is the time to act if you want to shine the light on your brand at one of the most anticipated tech conferences of the year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; is coming to Moscone West in San Francisco this October 27–29. Exhibiting puts your startup in the heart of the action — where 10,000+ founders, investors, press, and decision-makers come to discover what’s next.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Your brand missing the spotlight at this tech epicenter means missed ROI. Disrupt is &lt;em&gt;the&lt;/em&gt; launchpad for scaling startups — &lt;strong&gt;claim your exhibit table before your competitor does&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2758507" height="453" src="https://techcrunch.com/wp-content/uploads/2024/05/expo_startup_fun.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-what-you-get-with-your-table"&gt;What you get with your table&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Enjoy three full days of brand spotlighting in the heart of Disrupt — where investors are scouting their next deal, your target customers are ready to buy, and the tech press is searching for the next breakthrough, plus:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;A 6’ x 30″ table with linen and chairs, complete with an 11” x 14” branded tabletop sign for all-day visibility.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;A Silver Tier sponsorship with branding across the venue, site, and app.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;10 free passes for your team to experience all the activations at Disrupt.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;High-quality lead generation.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Access to the Disrupt press list.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;And more. Check out the exhibit page to learn all the perks of exhibiting.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;Exhibitor tables are disappearing fast, and once they’re gone, they’re gone. Don’t let your competitor own the spotlight while you watch from the sidelines. &lt;strong&gt;Book now before tomorrow’s deadline, or before it sells out&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 no anniversary" class="wp-image-3040972" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/TC25_Disrupt_General_Article_No-Anniversary-at-all_Headers_1920x1080.png?w=680" width="680" /&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/04/only-2-days-left-to-claim-your-exhibitor-spot-at-techcrunch-disrupt-2025/</guid><pubDate>Thu, 04 Sep 2025 14:00:00 +0000</pubDate></item><item><title>Captions rebrands as Mirage, expands beyond creator tools to AI video research (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/04/captions-rebrands-as-mirage-expands-beyond-creator-tools-to-ai-video-research/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Captions, an AI-powered video creation and editing app for content creators that has secured over $100 million in venture capital to date at a valuation of $500 million, is rebranding to Mirage, the company announced on Thursday.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new name reflects the company’s broader ambitions to become an AI research lab focused on multimodal foundational models specifically designed for short-form video content for platforms like TikTok, Reels, and Shorts. The company believes this approach will distinguish it from traditional AI models and competitors such as D-ID, Synthesia, and Hour One.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The rebranding will also unify the company’s offerings under one umbrella, bringing together the flagship creator-focused AI video platform, Captions, and the recently launched Mirage Studio, which caters to brands and ad production.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The way we see it, the real race for AI video hasn’t begun. Our new identity, Mirage, reflects our expanded vision and commitment to redefining the video category, starting with short-form video, through frontier AI research and models,” CEO Gaurav Misra told TechCrunch.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3042357" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/Mirage-_-Product-Image.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Mirage&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The sales pitch behind Mirage Studio, which launched in June, focuses on enabling brands to create short advertisements without relying on human talent or large budgets. By simply submitting an audio file, the AI generates video content from scratch, with an AI-generated background and custom AI avatars.&amp;nbsp;Users can also upload selfies to create an avatar using their likeness. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What sets the platform apart, according to the company, is its ability to produce AI avatars that have natural-looking speech, movements, and facial expressions. Additionally, Mirage says it doesn’t rely on existing stock footage, voice cloning, or lip-syncing.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mirage Studio is available under the business plan, which costs $399 per month for 8,000 credits. New users receive 50% off the first month.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;[embedded content]&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While these tools will likely benefit brands wanting to streamline video production and save some money, they also spark concerns around the potential impact on the creative workforce. The growing use of AI in advertisements has prompted backlash, as seen in a recent Guess ad in Vogue’s July print edition that featured an AI-generated model.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, as this technology becomes more advanced, distinguishing between real and deepfake videos becomes increasingly difficult. It’s a difficult pill to swallow for many people, especially given how quickly misinformation can spread these days.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Mirage recently addressed its role in deepfake technology in a blog post. The company acknowledged the genuine risks of misinformation while also expressing optimism about the positive potential of AI video. It mentioned that it has put moderation measures in place to limit misuse, such as preventing impersonation and requiring consent for likeness use.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, the company emphasized that “design isn’t a catch-all” and that the real solution lies in fostering a “new kind of media literacy” where people approach video content with the same critical eye as they do news headlines.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Captions, an AI-powered video creation and editing app for content creators that has secured over $100 million in venture capital to date at a valuation of $500 million, is rebranding to Mirage, the company announced on Thursday.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new name reflects the company’s broader ambitions to become an AI research lab focused on multimodal foundational models specifically designed for short-form video content for platforms like TikTok, Reels, and Shorts. The company believes this approach will distinguish it from traditional AI models and competitors such as D-ID, Synthesia, and Hour One.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The rebranding will also unify the company’s offerings under one umbrella, bringing together the flagship creator-focused AI video platform, Captions, and the recently launched Mirage Studio, which caters to brands and ad production.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The way we see it, the real race for AI video hasn’t begun. Our new identity, Mirage, reflects our expanded vision and commitment to redefining the video category, starting with short-form video, through frontier AI research and models,” CEO Gaurav Misra told TechCrunch.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3042357" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/Mirage-_-Product-Image.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Mirage&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The sales pitch behind Mirage Studio, which launched in June, focuses on enabling brands to create short advertisements without relying on human talent or large budgets. By simply submitting an audio file, the AI generates video content from scratch, with an AI-generated background and custom AI avatars.&amp;nbsp;Users can also upload selfies to create an avatar using their likeness. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What sets the platform apart, according to the company, is its ability to produce AI avatars that have natural-looking speech, movements, and facial expressions. Additionally, Mirage says it doesn’t rely on existing stock footage, voice cloning, or lip-syncing.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mirage Studio is available under the business plan, which costs $399 per month for 8,000 credits. New users receive 50% off the first month.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;[embedded content]&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While these tools will likely benefit brands wanting to streamline video production and save some money, they also spark concerns around the potential impact on the creative workforce. The growing use of AI in advertisements has prompted backlash, as seen in a recent Guess ad in Vogue’s July print edition that featured an AI-generated model.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, as this technology becomes more advanced, distinguishing between real and deepfake videos becomes increasingly difficult. It’s a difficult pill to swallow for many people, especially given how quickly misinformation can spread these days.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Mirage recently addressed its role in deepfake technology in a blog post. The company acknowledged the genuine risks of misinformation while also expressing optimism about the positive potential of AI video. It mentioned that it has put moderation measures in place to limit misuse, such as preventing impersonation and requiring consent for likeness use.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, the company emphasized that “design isn’t a catch-all” and that the real solution lies in fostering a “new kind of media literacy” where people approach video content with the same critical eye as they do news headlines.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/04/captions-rebrands-as-mirage-expands-beyond-creator-tools-to-ai-video-research/</guid><pubDate>Thu, 04 Sep 2025 15:39:15 +0000</pubDate></item><item><title>Cloud provider Lambda may be gearing up for an IPO (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/04/cloud-provider-lambda-may-be-gearing-up-for-an-ipo/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/cloud-computing-getty.jpg?resize=1200,750" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Cloud provider Lambda might be following rival CoreWeave to the public markets.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lambda, an AI infrastructure company offering on-demand GPUs, has hired banks for an upcoming IPO, according to reporting from The Information. Lambda has reportedly hired Morgan Stanley, J.P. Morgan, and Citi for a public listing that could happen as early as the first half of 2026.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Lambda did not respond to a request for comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has raised more than $1.7 billion in funding, according to Crunchbase data, from investors including Nvidia, Alumni Ventures, and Andra Capital, among others. It most recently raised $480 million in a Series D round in February.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CoreWeave, Lambda’s biggest rival, went public in March of this year.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/cloud-computing-getty.jpg?resize=1200,750" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Cloud provider Lambda might be following rival CoreWeave to the public markets.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lambda, an AI infrastructure company offering on-demand GPUs, has hired banks for an upcoming IPO, according to reporting from The Information. Lambda has reportedly hired Morgan Stanley, J.P. Morgan, and Citi for a public listing that could happen as early as the first half of 2026.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Lambda did not respond to a request for comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has raised more than $1.7 billion in funding, according to Crunchbase data, from investors including Nvidia, Alumni Ventures, and Andra Capital, among others. It most recently raised $480 million in a Series D round in February.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CoreWeave, Lambda’s biggest rival, went public in March of this year.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/04/cloud-provider-lambda-may-be-gearing-up-for-an-ipo/</guid><pubDate>Thu, 04 Sep 2025 15:43:06 +0000</pubDate></item><item><title>Google Photos upgrades its image-to-video feature with Veo 3 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/04/google-photos-adds-ai-video-generation-with-veo-3/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google’s latest video-generation model, Veo 3, is coming to Google Photos. The new model, available on the mobile app’s Create tab, will allow users in the U.S. to turn their still images into video clips.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google Photos already offers video generation through its recently added “Photo to video” feature, but the company says the addition of Veo 3 enhances that functionality with higher-quality video. The launch also represents how the company is working to bring its latest AI tech to consumers through its products — Google Photos, for instance, had over 1.5 billion monthly active users as of May 2025.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3042346" height="680" src="https://techcrunch.com/wp-content/uploads/2025/09/google-photos-veo-3.jpg?w=391" width="391" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Introduced in May at its I/O developer conference, Google brought Veo 3, which added image-to-video generation, to its Gemini app in July, making it available on its AI Ultra and AI Pro subscription plans. On those plans, users could generate three videos per day, which would carry both visible and invisible watermarks to identify the videos as being AI generated.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In Google Photos, the company imagines users bringing memories to life, or even animating older photos. The existing image-to-video feature is powered by Veo 2 and lets users select a photo from their gallery and choose one of two prompts for either “subtle movements” or a surprise animation by tapping an “I’m feeling lucky” button. The model then generates a six-second clip you could share with others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With Veo 3, the feature will remain free with a limited number of generations available, says Google. AI Pro and AI Ultra subscribers will have access to more generations. However, it won’t support audio and its videos will be four seconds long.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new photo-to-video feature is available within the Create hub, a new section in the Google Photos app where users can explore creative tools and features powered by AI. In addition to Veo 3, these include a remix option to change a photo’s style; make a collage; put together montages from your galleries; create moving, 3D photos called “cinematic” photos; and a tool to make GIFs from pics.&lt;/p&gt;


&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google’s latest video-generation model, Veo 3, is coming to Google Photos. The new model, available on the mobile app’s Create tab, will allow users in the U.S. to turn their still images into video clips.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google Photos already offers video generation through its recently added “Photo to video” feature, but the company says the addition of Veo 3 enhances that functionality with higher-quality video. The launch also represents how the company is working to bring its latest AI tech to consumers through its products — Google Photos, for instance, had over 1.5 billion monthly active users as of May 2025.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3042346" height="680" src="https://techcrunch.com/wp-content/uploads/2025/09/google-photos-veo-3.jpg?w=391" width="391" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Introduced in May at its I/O developer conference, Google brought Veo 3, which added image-to-video generation, to its Gemini app in July, making it available on its AI Ultra and AI Pro subscription plans. On those plans, users could generate three videos per day, which would carry both visible and invisible watermarks to identify the videos as being AI generated.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In Google Photos, the company imagines users bringing memories to life, or even animating older photos. The existing image-to-video feature is powered by Veo 2 and lets users select a photo from their gallery and choose one of two prompts for either “subtle movements” or a surprise animation by tapping an “I’m feeling lucky” button. The model then generates a six-second clip you could share with others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With Veo 3, the feature will remain free with a limited number of generations available, says Google. AI Pro and AI Ultra subscribers will have access to more generations. However, it won’t support audio and its videos will be four seconds long.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new photo-to-video feature is available within the Create hub, a new section in the Google Photos app where users can explore creative tools and features powered by AI. In addition to Veo 3, these include a remix option to change a photo’s style; make a collage; put together montages from your galleries; create moving, 3D photos called “cinematic” photos; and a tool to make GIFs from pics.&lt;/p&gt;


&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/04/google-photos-adds-ai-video-generation-with-veo-3/</guid><pubDate>Thu, 04 Sep 2025 16:00:00 +0000</pubDate></item><item><title>AI On: 6 Ways AI Agents Are Raising Team Performance — and How to Measure It (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/ways-ai-agents-are-raising-team-performance/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This post is part of the &lt;/i&gt;&lt;i&gt;AI On&lt;/i&gt;&lt;i&gt; blog series, which explores the latest techniques and real-world applications of agentic AI, chatbots and copilots. The series also highlights the NVIDIA software and hardware powering advanced AI agents, which form the foundation of AI query engines that gather insights and perform tasks to transform everyday experiences and reshape industries.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;AI agents are expected to be involved in most business tasks within three years, with effective human-agent collaboration projected to increase human engagement in high-value tasks by 65%.&lt;/p&gt;
&lt;p&gt;AI agents can help achieve and exceed efficiency goals as they learn, reason and adjust based on context and outcomes. As they become increasingly central to business strategies, understanding where they deliver impact and justify investment is essential for leaders.&lt;/p&gt;
&lt;p&gt;Here are six ways agentic AI boosts team performance — and practical tips for measuring its impact.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;1. Accelerating Software Development With AI Agents&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;AI agents can act as intelligent copilots, helping automate code generation, testing and deployment.&lt;/p&gt;
&lt;p&gt;They can pinpoint errors early, resulting in higher-quality, faster releases, and speed onboarding of new engineers by providing AI-curated information and context on documentation.&lt;/p&gt;
&lt;p&gt;For example, NVIDIA ChipNeMo — a team of specialized agents built on custom large language models (LLMs) and trained on NVIDIA’s internal chip design data — helped 5,000 NVIDIA engineers in design, verification and documentation save 4,000 engineering days in just one year.&lt;/p&gt;
&lt;p&gt;Since deployment, ChipNeMo has:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Infographic that conveys NVIDIA ChipNeMo has: Demonstrated 85%+ response accuracy, reflecting its reliability in real-world applications. Cut time spent sourcing technical answers from hours to seconds, streamlining development and troubleshooting. Accelerated verification cycles by identifying test gaps and diagnosing failures, addressing workflows that can take 30-50% of typical development schedules." class="aligncenter wp-image-84456 size-large" height="672" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/ai-on-chipnemo-infographic-1680x672.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;Learn about building agents with NVIDIA Nemotron and improving AI code generation using NVIDIA NeMo Agent Toolkit.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;2. Driving Data-Backed Decision-Making&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Agents can help businesses across industries easily glean insights from complex, time-sensitive data for critical decision-making, such as on investments or business strategy.&lt;/p&gt;
&lt;p&gt;BlackRock’s Aladdin Copilot — an embedded AI assistant serving thousands of users across hundreds of financial institutions — lets teams garner portfolio insights, assess investment research and monitor available cash balances through simple text prompts. It’s helped reduce research time from minutes to seconds while enhancing data-driven investment decisions.&lt;/p&gt;
&lt;p&gt;VAST Data uses agents to rapidly gather and synthesize information from internal and external sources. For its sales teams, this means faster access to useful, up-to-date insights on client accounts.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;3. Optimizing IT Operations&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Agents excel at maintaining IT operations, including by proactively monitoring infrastructure and automating decision-making.&lt;/p&gt;
&lt;p&gt;AI agents in IT operations offer:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Infographic that conveys that AI agents in IT operations offer — Faster issue resolution: Self-service IT support agents can quickly resolve tickets and automate routine tasks, improving user experiences. Security automation: AI agents facilitate investigation and triage in security operations, helping teams respond to threats swiftly and with greater accuracy. Enterprise search: Agents power advanced search across organizational data, surfacing insights and maintaining institutional knowledge." class="aligncenter size-large wp-image-84459" height="672" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/ai-on-agents-in-it-infographic-1680x672.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;In fast-paced telco environments, agents can help manage networks by analyzing real-time performance indicators and predicting service failures. For example, Telenor Group integrated the NVIDIA Blueprint for telco network configuration to deploy intelligent, autonomous networks that meet the performance demands of 5G and beyond.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;4. Streamlining Industrial and Manufacturing Operations&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Able to interact with the physical world, video analytics AI agents can monitor assembly lines for quality checks and anomaly detection.&lt;/p&gt;
&lt;p&gt;Pegatron developed the PEGA AI Factory platform to accelerate the development of AI agents across the company by 400% in the last four years. In addition, the company’s digital twin platform PEGAVERSE was built on the NVIDIA Omniverse platform and lets engineers virtually simulate, test and optimize production lines before they’re built, cutting factory construction time by 40%.&lt;/p&gt;
&lt;p&gt;Pegatron also augmented its assembly process using video analytics AI agents, powered by NVIDIA AI Blueprint for video search and summarization, and saw a 7% reduction in labor costs per assembly line and a 67% decrease in defect rates.&lt;/p&gt;
&lt;p&gt;Siemens is bringing generative AI into their solutions with the Industrial Copilot to tap real-time factory data to guide maintenance technicians and shopfloor operators. Interviews with maintenance engineers indicate that this could save on average 25% reactive maintenance time.&lt;/p&gt;
&lt;p&gt;Foxconn uses digital twins and AI agents to optimize its production lines, reducing deployment time by 50%, as well as to simulate robots and monitor quality and safety in real time.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;5. Enhancing Customer Service&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Agents excel at handling customer service at scale, reducing customer wait times by handling thousands of inquiries simultaneously.&lt;/p&gt;
&lt;p&gt;AT&amp;amp;T employees and contractors use a generative AI solution called “Ask AT&amp;amp;T,” which has over 100 solutions and agents in production. Built with LLMs served by NVIDIA NeMo and NIM microservices, Ask AT&amp;amp;T helps fetch relevant documentation and autonomously resolve routine inquiries.&lt;/p&gt;
&lt;p&gt;Offering 24/7 personalized support, Ask AT&amp;amp;T shares context-relevant suggestions by recalling organizational information from emails, meetings and past transactions. And to continuously improve agent performance, real-time feedback loops are built into the system using a data flywheel.&lt;/p&gt;
&lt;p&gt;These automated services resulted in 84% lower call center transcript analytics costs.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;6. Delivering Personalized Education&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;AI agents are making individualized learning support more accessible, scalable and effective while freeing up instructors for more in-depth teaching.&lt;/p&gt;
&lt;p&gt;Faced with surging class sizes and a shortage of teaching assistants, Clemson University developed an AI-powered TA — built with the NVIDIA Blueprint for retrieval-augmented generation — to guide students through challenging concepts.&lt;/p&gt;
&lt;p&gt;Rather than simply providing answers, the virtual TA walks students through problems step by step, encouraging active problem-solving and critical thinking to promote deeper understanding and academic integrity.&lt;/p&gt;
&lt;p&gt;The assistant also personalizes feedback and hints in alignment with course content, assignment deadlines and student submissions. It operates 24/7, giving every student timely, tailored support regardless of enrollment size.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;How Can the Success of AI Agents Be Measured?&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Measuring the impact of AI agents isn’t just a box to check — it’s essential to maximizing investment. The way users define success will directly shape how well these systems deliver value. Too often, businesses deploy agents without a clear measurement framework, making it difficult to prove return on investment or identify areas for improvement.&lt;/p&gt;
&lt;p&gt;When setting up an evaluation strategy, users should consider which metrics matter most for their goals. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Adoption and engagement: &lt;/b&gt;Track whether the technology is being embraced. Metrics include how many eligible users interact with the agent — and how frequently — along with how long the sessions last. High engagement means the agent is routinely providing effective support.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Task completion: &lt;/b&gt;Look beyond usage to outcomes. Measure how many tasks or requests the agent handles and what portions are fulfilled without human intervention. In software development, users can measure the automated code generation rate to see how much of the software is being developed by an agent. A high automated task completion rate means employees are freed up for higher-value work.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Productivity and efficiency gains: &lt;/b&gt;Quantify time saved. Metrics like time to resolve IT issues, report generation time for decision-making and average handling time for customer service interactions help demonstrate clear efficiency improvements.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Business outcomes: &lt;/b&gt;Connect agent performance to bottom-line results. This could mean cost per interaction in support, time to market in software development or unplanned downtime reduction in IT operations.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;High-quality user experience: &lt;/b&gt;Ensure the system is both trusted and effective. Consider a code quality score for developers, prediction accuracy in data-backed decision-making or customer satisfaction scores in service scenarios.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The key takeaway: measuring AI agent success goes far beyond a single number. Adoption, efficiency, accuracy and business impact all matter. By choosing the right mix of metrics upfront, businesses can validate success while continually refining and improving how agents deliver value.&lt;/p&gt;
&lt;p&gt;Read more stories on how customers are adopting AI applications to reshape their daily operations and increase their return on investment.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Stay up to date on agentic AI, NVIDIA Nemotron and more by subscribing to &lt;/i&gt;&lt;i&gt;NVIDIA news&lt;/i&gt;&lt;i&gt;,&lt;/i&gt;&lt;i&gt; joining the community&lt;/i&gt;&lt;i&gt; and following NVIDIA AI on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;. Plus, explore &lt;/i&gt;&lt;i&gt;self-paced video tutorials and livestreams&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This post is part of the &lt;/i&gt;&lt;i&gt;AI On&lt;/i&gt;&lt;i&gt; blog series, which explores the latest techniques and real-world applications of agentic AI, chatbots and copilots. The series also highlights the NVIDIA software and hardware powering advanced AI agents, which form the foundation of AI query engines that gather insights and perform tasks to transform everyday experiences and reshape industries.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;AI agents are expected to be involved in most business tasks within three years, with effective human-agent collaboration projected to increase human engagement in high-value tasks by 65%.&lt;/p&gt;
&lt;p&gt;AI agents can help achieve and exceed efficiency goals as they learn, reason and adjust based on context and outcomes. As they become increasingly central to business strategies, understanding where they deliver impact and justify investment is essential for leaders.&lt;/p&gt;
&lt;p&gt;Here are six ways agentic AI boosts team performance — and practical tips for measuring its impact.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;1. Accelerating Software Development With AI Agents&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;AI agents can act as intelligent copilots, helping automate code generation, testing and deployment.&lt;/p&gt;
&lt;p&gt;They can pinpoint errors early, resulting in higher-quality, faster releases, and speed onboarding of new engineers by providing AI-curated information and context on documentation.&lt;/p&gt;
&lt;p&gt;For example, NVIDIA ChipNeMo — a team of specialized agents built on custom large language models (LLMs) and trained on NVIDIA’s internal chip design data — helped 5,000 NVIDIA engineers in design, verification and documentation save 4,000 engineering days in just one year.&lt;/p&gt;
&lt;p&gt;Since deployment, ChipNeMo has:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Infographic that conveys NVIDIA ChipNeMo has: Demonstrated 85%+ response accuracy, reflecting its reliability in real-world applications. Cut time spent sourcing technical answers from hours to seconds, streamlining development and troubleshooting. Accelerated verification cycles by identifying test gaps and diagnosing failures, addressing workflows that can take 30-50% of typical development schedules." class="aligncenter wp-image-84456 size-large" height="672" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/ai-on-chipnemo-infographic-1680x672.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;Learn about building agents with NVIDIA Nemotron and improving AI code generation using NVIDIA NeMo Agent Toolkit.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;2. Driving Data-Backed Decision-Making&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Agents can help businesses across industries easily glean insights from complex, time-sensitive data for critical decision-making, such as on investments or business strategy.&lt;/p&gt;
&lt;p&gt;BlackRock’s Aladdin Copilot — an embedded AI assistant serving thousands of users across hundreds of financial institutions — lets teams garner portfolio insights, assess investment research and monitor available cash balances through simple text prompts. It’s helped reduce research time from minutes to seconds while enhancing data-driven investment decisions.&lt;/p&gt;
&lt;p&gt;VAST Data uses agents to rapidly gather and synthesize information from internal and external sources. For its sales teams, this means faster access to useful, up-to-date insights on client accounts.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;3. Optimizing IT Operations&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Agents excel at maintaining IT operations, including by proactively monitoring infrastructure and automating decision-making.&lt;/p&gt;
&lt;p&gt;AI agents in IT operations offer:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Infographic that conveys that AI agents in IT operations offer — Faster issue resolution: Self-service IT support agents can quickly resolve tickets and automate routine tasks, improving user experiences. Security automation: AI agents facilitate investigation and triage in security operations, helping teams respond to threats swiftly and with greater accuracy. Enterprise search: Agents power advanced search across organizational data, surfacing insights and maintaining institutional knowledge." class="aligncenter size-large wp-image-84459" height="672" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/ai-on-agents-in-it-infographic-1680x672.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;In fast-paced telco environments, agents can help manage networks by analyzing real-time performance indicators and predicting service failures. For example, Telenor Group integrated the NVIDIA Blueprint for telco network configuration to deploy intelligent, autonomous networks that meet the performance demands of 5G and beyond.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;4. Streamlining Industrial and Manufacturing Operations&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Able to interact with the physical world, video analytics AI agents can monitor assembly lines for quality checks and anomaly detection.&lt;/p&gt;
&lt;p&gt;Pegatron developed the PEGA AI Factory platform to accelerate the development of AI agents across the company by 400% in the last four years. In addition, the company’s digital twin platform PEGAVERSE was built on the NVIDIA Omniverse platform and lets engineers virtually simulate, test and optimize production lines before they’re built, cutting factory construction time by 40%.&lt;/p&gt;
&lt;p&gt;Pegatron also augmented its assembly process using video analytics AI agents, powered by NVIDIA AI Blueprint for video search and summarization, and saw a 7% reduction in labor costs per assembly line and a 67% decrease in defect rates.&lt;/p&gt;
&lt;p&gt;Siemens is bringing generative AI into their solutions with the Industrial Copilot to tap real-time factory data to guide maintenance technicians and shopfloor operators. Interviews with maintenance engineers indicate that this could save on average 25% reactive maintenance time.&lt;/p&gt;
&lt;p&gt;Foxconn uses digital twins and AI agents to optimize its production lines, reducing deployment time by 50%, as well as to simulate robots and monitor quality and safety in real time.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;5. Enhancing Customer Service&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Agents excel at handling customer service at scale, reducing customer wait times by handling thousands of inquiries simultaneously.&lt;/p&gt;
&lt;p&gt;AT&amp;amp;T employees and contractors use a generative AI solution called “Ask AT&amp;amp;T,” which has over 100 solutions and agents in production. Built with LLMs served by NVIDIA NeMo and NIM microservices, Ask AT&amp;amp;T helps fetch relevant documentation and autonomously resolve routine inquiries.&lt;/p&gt;
&lt;p&gt;Offering 24/7 personalized support, Ask AT&amp;amp;T shares context-relevant suggestions by recalling organizational information from emails, meetings and past transactions. And to continuously improve agent performance, real-time feedback loops are built into the system using a data flywheel.&lt;/p&gt;
&lt;p&gt;These automated services resulted in 84% lower call center transcript analytics costs.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;6. Delivering Personalized Education&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;AI agents are making individualized learning support more accessible, scalable and effective while freeing up instructors for more in-depth teaching.&lt;/p&gt;
&lt;p&gt;Faced with surging class sizes and a shortage of teaching assistants, Clemson University developed an AI-powered TA — built with the NVIDIA Blueprint for retrieval-augmented generation — to guide students through challenging concepts.&lt;/p&gt;
&lt;p&gt;Rather than simply providing answers, the virtual TA walks students through problems step by step, encouraging active problem-solving and critical thinking to promote deeper understanding and academic integrity.&lt;/p&gt;
&lt;p&gt;The assistant also personalizes feedback and hints in alignment with course content, assignment deadlines and student submissions. It operates 24/7, giving every student timely, tailored support regardless of enrollment size.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;How Can the Success of AI Agents Be Measured?&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Measuring the impact of AI agents isn’t just a box to check — it’s essential to maximizing investment. The way users define success will directly shape how well these systems deliver value. Too often, businesses deploy agents without a clear measurement framework, making it difficult to prove return on investment or identify areas for improvement.&lt;/p&gt;
&lt;p&gt;When setting up an evaluation strategy, users should consider which metrics matter most for their goals. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Adoption and engagement: &lt;/b&gt;Track whether the technology is being embraced. Metrics include how many eligible users interact with the agent — and how frequently — along with how long the sessions last. High engagement means the agent is routinely providing effective support.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Task completion: &lt;/b&gt;Look beyond usage to outcomes. Measure how many tasks or requests the agent handles and what portions are fulfilled without human intervention. In software development, users can measure the automated code generation rate to see how much of the software is being developed by an agent. A high automated task completion rate means employees are freed up for higher-value work.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Productivity and efficiency gains: &lt;/b&gt;Quantify time saved. Metrics like time to resolve IT issues, report generation time for decision-making and average handling time for customer service interactions help demonstrate clear efficiency improvements.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Business outcomes: &lt;/b&gt;Connect agent performance to bottom-line results. This could mean cost per interaction in support, time to market in software development or unplanned downtime reduction in IT operations.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;High-quality user experience: &lt;/b&gt;Ensure the system is both trusted and effective. Consider a code quality score for developers, prediction accuracy in data-backed decision-making or customer satisfaction scores in service scenarios.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The key takeaway: measuring AI agent success goes far beyond a single number. Adoption, efficiency, accuracy and business impact all matter. By choosing the right mix of metrics upfront, businesses can validate success while continually refining and improving how agents deliver value.&lt;/p&gt;
&lt;p&gt;Read more stories on how customers are adopting AI applications to reshape their daily operations and increase their return on investment.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Stay up to date on agentic AI, NVIDIA Nemotron and more by subscribing to &lt;/i&gt;&lt;i&gt;NVIDIA news&lt;/i&gt;&lt;i&gt;,&lt;/i&gt;&lt;i&gt; joining the community&lt;/i&gt;&lt;i&gt; and following NVIDIA AI on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;. Plus, explore &lt;/i&gt;&lt;i&gt;self-paced video tutorials and livestreams&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/ways-ai-agents-are-raising-team-performance/</guid><pubDate>Thu, 04 Sep 2025 16:00:59 +0000</pubDate></item><item><title>Imagining the future of banking with agentic AI (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/09/04/1123023/imagining-the-future-of-banking-with-agentic-ai/</link><description>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In association with&lt;/span&gt;EY&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Agentic AI is coming of age. And with it comes new opportunities in the financial services sector. Banks are increasingly employing agentic AI to optimize processes, navigate complex systems, and sift through vast quantities of unstructured data to make decisions and take actions—with or without human involvement. “With the maturing of agentic AI, it is becoming a lot more technologically possible for large-scale process automation that was not possible with rules-based approaches like robotic process automation before,” says Sameer Gupta, Americas financial services AI leader at EY. “That moves the needle in terms of cost, efficiency, and customer experience impact.”&lt;/p&gt;  &lt;figure class="wp-block-image alignright size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-1123027" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/MIT_EY_V10_Aug1325_cover_6ee359.png?w=1555" width="1555" /&gt;&lt;/figure&gt;  &lt;p&gt;From responding to customer services requests, to automating loan approvals, adjusting bill payments to align with regular paychecks, or extracting key terms and conditions from financial agreements, agentic AI has the potential to transform the customer experience—and how financial institutions operate too. &lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;  &lt;p&gt;Adapting to new and emerging technologies like agentic AI is essential for an organization’s survival, says Murli Buluswar, head of US personal banking analytics at Citi. “A company’s ability to adopt new technical capabilities and rearchitect how their firm operates is going to make the difference between the firms that succeed and those that get left behind,” says Buluswar. “Your people and your firm must recognize that how they go about their work is going to be meaningfully different.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;The emerging landscape&lt;/h3&gt;  &lt;p&gt;Agentic AI is already being rapidly adopted in the banking sector. A 2025 survey of 250 banking executives by MIT Technology Review Insights found that 70% of leaders say their firm uses agentic AI to some degree, either through existing deployments (16%) or pilot projects (52%). And it is already proving effective in a range of different functions. More than half of executives say agentic AI systems are highly capable of improving fraud detection (56%) and security (51%). Other strong use cases include reducing cost and increasing efficiency (41%) and improving the customer experience (41%).&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Download the report.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. It was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In association with&lt;/span&gt;EY&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Agentic AI is coming of age. And with it comes new opportunities in the financial services sector. Banks are increasingly employing agentic AI to optimize processes, navigate complex systems, and sift through vast quantities of unstructured data to make decisions and take actions—with or without human involvement. “With the maturing of agentic AI, it is becoming a lot more technologically possible for large-scale process automation that was not possible with rules-based approaches like robotic process automation before,” says Sameer Gupta, Americas financial services AI leader at EY. “That moves the needle in terms of cost, efficiency, and customer experience impact.”&lt;/p&gt;  &lt;figure class="wp-block-image alignright size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-1123027" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/MIT_EY_V10_Aug1325_cover_6ee359.png?w=1555" width="1555" /&gt;&lt;/figure&gt;  &lt;p&gt;From responding to customer services requests, to automating loan approvals, adjusting bill payments to align with regular paychecks, or extracting key terms and conditions from financial agreements, agentic AI has the potential to transform the customer experience—and how financial institutions operate too. &lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;  &lt;p&gt;Adapting to new and emerging technologies like agentic AI is essential for an organization’s survival, says Murli Buluswar, head of US personal banking analytics at Citi. “A company’s ability to adopt new technical capabilities and rearchitect how their firm operates is going to make the difference between the firms that succeed and those that get left behind,” says Buluswar. “Your people and your firm must recognize that how they go about their work is going to be meaningfully different.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;The emerging landscape&lt;/h3&gt;  &lt;p&gt;Agentic AI is already being rapidly adopted in the banking sector. A 2025 survey of 250 banking executives by MIT Technology Review Insights found that 70% of leaders say their firm uses agentic AI to some degree, either through existing deployments (16%) or pilot projects (52%). And it is already proving effective in a range of different functions. More than half of executives say agentic AI systems are highly capable of improving fraud detection (56%) and security (51%). Other strong use cases include reducing cost and increasing efficiency (41%) and improving the customer experience (41%).&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Download the report.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. It was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/09/04/1123023/imagining-the-future-of-banking-with-agentic-ai/</guid><pubDate>Thu, 04 Sep 2025 16:21:23 +0000</pubDate></item><item><title>Using AI to perceive the universe in greater depth (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/using-ai-to-perceive-the-universe-in-greater-depth/</link><description>&lt;div class="article-cover article-cover--centered"&gt;
    &lt;div class="article-cover__header"&gt;
      &lt;p class="article-cover__eyebrow glue-label"&gt;Science&lt;/p&gt;
      

      
    &lt;dl class="article-cover__meta"&gt;
      
        &lt;dt class="glue-visually-hidden"&gt;Published&lt;/dt&gt;
        &lt;dd class="article-cover__date glue-label"&gt;&lt;time datetime="2025-09-04"&gt;4 September 2025&lt;/time&gt;&lt;/dd&gt;
      
      
        &lt;dt class="glue-visually-hidden"&gt;Authors&lt;/dt&gt;
        &lt;dd class="article-cover__authors"&gt;&lt;p&gt;Brendan Tracey, Jonas Buchli&lt;/p&gt;&lt;/dd&gt;
      
    &lt;/dl&gt;
  

      
    &lt;/div&gt;

    
      
    
    
    
      &lt;source height="603" media="(min-width: 1024px)" type="image/webp" width="1072" /&gt;&lt;source height="522" media="(min-width: 600px)" type="image/webp" width="928" /&gt;&lt;source height="297" type="image/webp" width="528" /&gt;
      &lt;img alt="An artist's illustration of how gravitational wave observatories are used to peer into the universe. In the background, two orbiting black holes distort the fabric of spacetime, sending out gravitational waves. In the foreground, a conceptual detector, much like LIGO, uses a laser beam between two mirrors to measure these infinitesimal disturbances, unveiling the secrets of cosmic collisions." class="picture__image" height="603" src="https://lh3.googleusercontent.com/eXXHIbqGsexZTTs915Rx6INAnpTuWyLcPcvqqK0XQBpZ6XDycXiIAfWM5Bf5N6KAg_sG8b67sSof5fjEbtKn3XX0kdK4YUiqqOIRKJyXdT2D_nq8PPA=w1072-h603-n-nu" width="1072" /&gt;
    
    
  
    
  &lt;/div&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p class="gdm-rich-text__subtitle"&gt;Our novel Deep Loop Shaping method improves control of gravitational wave observatories, helping astronomers better understand the dynamics and formation of the universe.&lt;/p&gt;&lt;p&gt;To help astronomers study the universe’s most powerful processes, our teams have been using AI to stabilize one of the most sensitive observation instruments ever built.&lt;/p&gt;&lt;p&gt;In a paper published today in Science, we introduce Deep Loop Shaping, a novel AI method that will unlock next-generation gravitational-wave science. Deep Loop Shaping reduces noise and improves control in an observatory’s feedback system, helping stabilize components used for measuring gravitational waves — the tiny ripples in the fabric of space and time.&lt;/p&gt;&lt;p&gt;These waves are generated by events like neutron star collisions and black hole mergers. Our method will help astronomers gather data critical to understanding the dynamics and formation of the universe, and better test fundamental theories of physics and cosmology.&lt;/p&gt;&lt;p&gt;We developed Deep Loop Shaping in collaboration with LIGO (Laser Interferometer Gravitational-Wave Observatory) operated by Caltech, and GSSI (Gran Sasso Science Institute), and proved our method at the observatory in Livingston, Louisiana.&lt;/p&gt;&lt;p&gt;LIGO measures the properties and origins of gravitational waves with incredible accuracy. But the slightest vibration can disrupt its measurements, even from waves crashing 100 miles away on the Gulf coast. To function, LIGO relies on thousands of control systems keeping every part in near-perfect alignment, and adapts to environmental disturbances with continuous feedback.&lt;/p&gt;&lt;p&gt;Deep Loop Shaping reduces the noise level in the most unstable and difficult feedback loop at LIGO by 30 to 100 times, improving the stability of its highly-sensitive interferometer mirrors. Applying our method to all of LIGO’s mirror control loops could help astronomers detect and gather data about hundreds of more events per year, in far greater detail.&lt;/p&gt;&lt;p&gt;In the future, Deep Loop Shaping could also be applied to many other engineering problems involving vibration suppression, noise cancellation and highly dynamic or unstable systems important in aerospace, robotics, and structural engineering.&lt;/p&gt;&lt;h2&gt;Measuring across the universe&lt;/h2&gt;&lt;p&gt;LIGO uses the interference of laser light to measure the properties of gravitational waves. By studying these properties, scientists can figure out what caused them and where they came from. The observatory’s lasers reflect off mirrors positioned 4 kilometers apart, housed in the world’s largest vacuum chambers.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--large"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-54ad3332-75de-4382-a4ab-520d70420df7"&gt;
    &lt;p&gt;Aerial view of LIGO (Laser Interferometer Gravitational-Wave Observatory) in Livingston, Louisiana, USA. The observatory’s lasers reflect off mirrors positioned 4 kilometers apart. Photo credit of Caltech/MIT/LIGO Lab.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;Since first detecting gravitational waves produced by a pair of colliding black holes, in 2015, verifying the predictions of Albert Einstein’s general theory of relativity, LIGO’s measurements have deeply changed our understanding of the universe.&lt;/p&gt;&lt;p&gt;With this observatory, astronomers have detected hundreds of black hole and neutron star collisions, proven the existence of binary black hole systems, seen new black holes formed in neutron star collisions, studied the creation of heavy elements like gold and more.&lt;/p&gt;&lt;p&gt;Astronomers already know a lot about the largest and smallest black holes, but we only have limited data on intermediate-mass black holes — considered the “missing link” to understanding galaxy evolution.&lt;/p&gt;&lt;p&gt;Until now, LIGO has only been capable of observing very few of these systems. To help astronomers capture more detail and data of this phenomena, we worked to improve the most difficult part of the control system and expand how far away we can see these events.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  &lt;figure class="quote quote--inline"&gt;
  &lt;blockquote class="quote__text"&gt;
    &lt;p&gt;“&lt;/p&gt;
    &lt;p&gt;Studying the universe using gravity instead of light, is like listening instead of looking. This work allows us to tune in to the bass.&lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;figcaption class="quote__author"&gt;&lt;p&gt;Rana Adhikari, Professor of Physics at the Caltech, 2025&lt;/p&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;h2&gt;Reducing noise and stabilizing the system&lt;/h2&gt;&lt;p&gt;As gravitational waves pass through LIGO’s two 4 kilometer arms, they warp the space between them, changing the distance between the mirrors at either end. These tiny differences in length are measured using light interference to an accuracy of 10^-19 meters, which is 1/10’000 the size of a proton. With measurements this small, LIGO’s detector mirrors must be kept extremely still, isolated from environmental disturbance.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--inline"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-510b7ec7-f5d1-4d94-99a1-2e5ae657ea7d"&gt;
    &lt;p&gt;Closeup photograph of LIGO, which uses strong lasers and mirrors to detect gravitational waves in the universe, generated by events like collisions and mergers of black holes. Photo credit of Caltech/MIT/LIGO Lab.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;This requires one system for passive mechanical isolation and another control system for actively suppressing vibrations. Too little control causes the mirrors to swing, making it impossible to measure anything. But too much control actually amplifies vibrations in the system, instead of suppressing them, drowning out the signal in certain frequency ranges.&lt;/p&gt;&lt;p&gt;These vibrations, known as “control noise”, are a critical blocker to improving LIGO’s ability to peer into the universe. Our team designed Deep Loop Shaping to move beyond traditional methods, such as the linear control design methods currently in operation, to remove the controller as a meaningful cause of noise.&lt;/p&gt;&lt;h2&gt;A more effective control system&lt;/h2&gt;&lt;p&gt;Deep Loop Shaping leverages a reinforcement learning method using frequency domain rewards and surpasses state-of-the-art feedback control performance.&lt;/p&gt;&lt;p&gt;In a simulated LIGO environment, we trained a controller that tries to avoid amplifying noise in the observation band used for measuring gravitational waves — the band where we need the mirror to be still to see events like black hole mergers of up to a few hundred solar masses.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--large"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-ce80eee9-798a-4bbc-ae9b-50bac73aaad1"&gt;
    &lt;p&gt;Diagram showing LIGO’s intricate systems of lasers and mirrors. A distributed control system actively adjusts the mirrors, counteracting the laser radiation pressure and vibrations from external sources.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;Through repeated interaction, guided by frequency domain rewards, the controller learns to suppress the control noise in the observation band. In other words, our controllers learn to stabilize the mirrors without adding harmful control noise, bringing noise levels down by a factor of ten or more, below the amount of vibrations caused by quantum fluctuations in the radiation pressure of light reflecting off the mirrors.&lt;/p&gt;&lt;h2&gt;Strong performance across simulation and hardware&lt;/h2&gt;&lt;p&gt;We tested our controllers on the real LIGO system in Livingston, Louisiana, USA — finding that they worked as well on hardware as in simulation.&lt;/p&gt;&lt;p&gt;Our results show that Deep Loop Shaping controls noise up to 30-100 times better than existing controllers, and it eliminated the most unstable and difficult feedback loop as a meaningful source of noise on LIGO for the first time.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--large"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-eec85cdb-e5c7-488c-b227-d6bb2ddeda70"&gt;
    &lt;p&gt;Line chart showing the resulting control noise spectrum using our Deep Loop Shaping method. There is an improvement of 30-100 times in the injected control noise levels in the most unstable and difficult feedback control loop.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;In repeated experiments, we confirmed that our controller keeps the observatory’s system stable over prolonged periods.&lt;/p&gt;&lt;h2&gt;Better understanding the nature of the universe&lt;/h2&gt;&lt;p&gt;Deep Loop Shaping pushes the boundaries of what’s currently possible in astrophysics by solving a critical blocker to studying gravitational waves.&lt;/p&gt;&lt;p&gt;Applying Deep Loop Shaping to LIGO’s entire mirror control system has the potential to eliminate noise from the control system itself, paving the way for expanding its cosmological reach.&lt;/p&gt;&lt;p&gt;Beyond significantly improving how existing gravitational wave observatories measure further and dimmer sources, we expect our work to influence the design of future observatories, both on Earth and in space — and ultimately help connect missing links throughout the universe for the first time.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  

&lt;section class="button-group button-group--stacked"&gt;
  
    &lt;h2 class="glue-headline glue-headline--headline-6 button-group__title"&gt;Learn more about our work&lt;/h2&gt;
  

  
&lt;/section&gt;
                
              
                
                
                  
                  &lt;section class="notes"&gt;
  &lt;div class="glue-page"&gt;
    &lt;div class="gdm-rich-text notes__inner"&gt;
      &lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;This research was done by Jonas Buchli, Brendan Tracey, Tomislav Andric, Christopher Wipf, Yu Him Justin Chiu, Matthias Lochbrunner, Craig Donner, Rana X Adhikari, Jan Harms, Iain Barr, Roland Hafner, Andrea Huber, Abbas Abdolmaleki, Charlie Beattie, Joseph Betzwieser, Serkan Cabi, Jonas Degrave, Yuzhu Dong, Leslie Fritz, Anchal Gupta, Oliver Groth, Sandy Huang, Tamara Norman, Hannah Openshaw, Jameson Rollins, Greg Thornton, George van den Driessche, Markus Wulfmeier, Pushmeet Kohli, Martin Riedmiller and is a collaboration of LIGO, Caltech, GSSI and GDM.&lt;/p&gt;&lt;p&gt;We’d like to thank the fantastic LIGO instrument team for their tireless work on keeping the observatories up and running and supporting our experiments.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/section&gt;</description><content:encoded>&lt;div class="article-cover article-cover--centered"&gt;
    &lt;div class="article-cover__header"&gt;
      &lt;p class="article-cover__eyebrow glue-label"&gt;Science&lt;/p&gt;
      

      
    &lt;dl class="article-cover__meta"&gt;
      
        &lt;dt class="glue-visually-hidden"&gt;Published&lt;/dt&gt;
        &lt;dd class="article-cover__date glue-label"&gt;&lt;time datetime="2025-09-04"&gt;4 September 2025&lt;/time&gt;&lt;/dd&gt;
      
      
        &lt;dt class="glue-visually-hidden"&gt;Authors&lt;/dt&gt;
        &lt;dd class="article-cover__authors"&gt;&lt;p&gt;Brendan Tracey, Jonas Buchli&lt;/p&gt;&lt;/dd&gt;
      
    &lt;/dl&gt;
  

      
    &lt;/div&gt;

    
      
    
    
    
      &lt;source height="603" media="(min-width: 1024px)" type="image/webp" width="1072" /&gt;&lt;source height="522" media="(min-width: 600px)" type="image/webp" width="928" /&gt;&lt;source height="297" type="image/webp" width="528" /&gt;
      &lt;img alt="An artist's illustration of how gravitational wave observatories are used to peer into the universe. In the background, two orbiting black holes distort the fabric of spacetime, sending out gravitational waves. In the foreground, a conceptual detector, much like LIGO, uses a laser beam between two mirrors to measure these infinitesimal disturbances, unveiling the secrets of cosmic collisions." class="picture__image" height="603" src="https://lh3.googleusercontent.com/eXXHIbqGsexZTTs915Rx6INAnpTuWyLcPcvqqK0XQBpZ6XDycXiIAfWM5Bf5N6KAg_sG8b67sSof5fjEbtKn3XX0kdK4YUiqqOIRKJyXdT2D_nq8PPA=w1072-h603-n-nu" width="1072" /&gt;
    
    
  
    
  &lt;/div&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p class="gdm-rich-text__subtitle"&gt;Our novel Deep Loop Shaping method improves control of gravitational wave observatories, helping astronomers better understand the dynamics and formation of the universe.&lt;/p&gt;&lt;p&gt;To help astronomers study the universe’s most powerful processes, our teams have been using AI to stabilize one of the most sensitive observation instruments ever built.&lt;/p&gt;&lt;p&gt;In a paper published today in Science, we introduce Deep Loop Shaping, a novel AI method that will unlock next-generation gravitational-wave science. Deep Loop Shaping reduces noise and improves control in an observatory’s feedback system, helping stabilize components used for measuring gravitational waves — the tiny ripples in the fabric of space and time.&lt;/p&gt;&lt;p&gt;These waves are generated by events like neutron star collisions and black hole mergers. Our method will help astronomers gather data critical to understanding the dynamics and formation of the universe, and better test fundamental theories of physics and cosmology.&lt;/p&gt;&lt;p&gt;We developed Deep Loop Shaping in collaboration with LIGO (Laser Interferometer Gravitational-Wave Observatory) operated by Caltech, and GSSI (Gran Sasso Science Institute), and proved our method at the observatory in Livingston, Louisiana.&lt;/p&gt;&lt;p&gt;LIGO measures the properties and origins of gravitational waves with incredible accuracy. But the slightest vibration can disrupt its measurements, even from waves crashing 100 miles away on the Gulf coast. To function, LIGO relies on thousands of control systems keeping every part in near-perfect alignment, and adapts to environmental disturbances with continuous feedback.&lt;/p&gt;&lt;p&gt;Deep Loop Shaping reduces the noise level in the most unstable and difficult feedback loop at LIGO by 30 to 100 times, improving the stability of its highly-sensitive interferometer mirrors. Applying our method to all of LIGO’s mirror control loops could help astronomers detect and gather data about hundreds of more events per year, in far greater detail.&lt;/p&gt;&lt;p&gt;In the future, Deep Loop Shaping could also be applied to many other engineering problems involving vibration suppression, noise cancellation and highly dynamic or unstable systems important in aerospace, robotics, and structural engineering.&lt;/p&gt;&lt;h2&gt;Measuring across the universe&lt;/h2&gt;&lt;p&gt;LIGO uses the interference of laser light to measure the properties of gravitational waves. By studying these properties, scientists can figure out what caused them and where they came from. The observatory’s lasers reflect off mirrors positioned 4 kilometers apart, housed in the world’s largest vacuum chambers.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--large"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-54ad3332-75de-4382-a4ab-520d70420df7"&gt;
    &lt;p&gt;Aerial view of LIGO (Laser Interferometer Gravitational-Wave Observatory) in Livingston, Louisiana, USA. The observatory’s lasers reflect off mirrors positioned 4 kilometers apart. Photo credit of Caltech/MIT/LIGO Lab.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;Since first detecting gravitational waves produced by a pair of colliding black holes, in 2015, verifying the predictions of Albert Einstein’s general theory of relativity, LIGO’s measurements have deeply changed our understanding of the universe.&lt;/p&gt;&lt;p&gt;With this observatory, astronomers have detected hundreds of black hole and neutron star collisions, proven the existence of binary black hole systems, seen new black holes formed in neutron star collisions, studied the creation of heavy elements like gold and more.&lt;/p&gt;&lt;p&gt;Astronomers already know a lot about the largest and smallest black holes, but we only have limited data on intermediate-mass black holes — considered the “missing link” to understanding galaxy evolution.&lt;/p&gt;&lt;p&gt;Until now, LIGO has only been capable of observing very few of these systems. To help astronomers capture more detail and data of this phenomena, we worked to improve the most difficult part of the control system and expand how far away we can see these events.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  &lt;figure class="quote quote--inline"&gt;
  &lt;blockquote class="quote__text"&gt;
    &lt;p&gt;“&lt;/p&gt;
    &lt;p&gt;Studying the universe using gravity instead of light, is like listening instead of looking. This work allows us to tune in to the bass.&lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;figcaption class="quote__author"&gt;&lt;p&gt;Rana Adhikari, Professor of Physics at the Caltech, 2025&lt;/p&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;h2&gt;Reducing noise and stabilizing the system&lt;/h2&gt;&lt;p&gt;As gravitational waves pass through LIGO’s two 4 kilometer arms, they warp the space between them, changing the distance between the mirrors at either end. These tiny differences in length are measured using light interference to an accuracy of 10^-19 meters, which is 1/10’000 the size of a proton. With measurements this small, LIGO’s detector mirrors must be kept extremely still, isolated from environmental disturbance.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--inline"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-510b7ec7-f5d1-4d94-99a1-2e5ae657ea7d"&gt;
    &lt;p&gt;Closeup photograph of LIGO, which uses strong lasers and mirrors to detect gravitational waves in the universe, generated by events like collisions and mergers of black holes. Photo credit of Caltech/MIT/LIGO Lab.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;This requires one system for passive mechanical isolation and another control system for actively suppressing vibrations. Too little control causes the mirrors to swing, making it impossible to measure anything. But too much control actually amplifies vibrations in the system, instead of suppressing them, drowning out the signal in certain frequency ranges.&lt;/p&gt;&lt;p&gt;These vibrations, known as “control noise”, are a critical blocker to improving LIGO’s ability to peer into the universe. Our team designed Deep Loop Shaping to move beyond traditional methods, such as the linear control design methods currently in operation, to remove the controller as a meaningful cause of noise.&lt;/p&gt;&lt;h2&gt;A more effective control system&lt;/h2&gt;&lt;p&gt;Deep Loop Shaping leverages a reinforcement learning method using frequency domain rewards and surpasses state-of-the-art feedback control performance.&lt;/p&gt;&lt;p&gt;In a simulated LIGO environment, we trained a controller that tries to avoid amplifying noise in the observation band used for measuring gravitational waves — the band where we need the mirror to be still to see events like black hole mergers of up to a few hundred solar masses.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--large"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-ce80eee9-798a-4bbc-ae9b-50bac73aaad1"&gt;
    &lt;p&gt;Diagram showing LIGO’s intricate systems of lasers and mirrors. A distributed control system actively adjusts the mirrors, counteracting the laser radiation pressure and vibrations from external sources.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;Through repeated interaction, guided by frequency domain rewards, the controller learns to suppress the control noise in the observation band. In other words, our controllers learn to stabilize the mirrors without adding harmful control noise, bringing noise levels down by a factor of ten or more, below the amount of vibrations caused by quantum fluctuations in the radiation pressure of light reflecting off the mirrors.&lt;/p&gt;&lt;h2&gt;Strong performance across simulation and hardware&lt;/h2&gt;&lt;p&gt;We tested our controllers on the real LIGO system in Livingston, Louisiana, USA — finding that they worked as well on hardware as in simulation.&lt;/p&gt;&lt;p&gt;Our results show that Deep Loop Shaping controls noise up to 30-100 times better than existing controllers, and it eliminated the most unstable and difficult feedback loop as a meaningful source of noise on LIGO for the first time.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--large"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-eec85cdb-e5c7-488c-b227-d6bb2ddeda70"&gt;
    &lt;p&gt;Line chart showing the resulting control noise spectrum using our Deep Loop Shaping method. There is an improvement of 30-100 times in the injected control noise levels in the most unstable and difficult feedback control loop.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;In repeated experiments, we confirmed that our controller keeps the observatory’s system stable over prolonged periods.&lt;/p&gt;&lt;h2&gt;Better understanding the nature of the universe&lt;/h2&gt;&lt;p&gt;Deep Loop Shaping pushes the boundaries of what’s currently possible in astrophysics by solving a critical blocker to studying gravitational waves.&lt;/p&gt;&lt;p&gt;Applying Deep Loop Shaping to LIGO’s entire mirror control system has the potential to eliminate noise from the control system itself, paving the way for expanding its cosmological reach.&lt;/p&gt;&lt;p&gt;Beyond significantly improving how existing gravitational wave observatories measure further and dimmer sources, we expect our work to influence the design of future observatories, both on Earth and in space — and ultimately help connect missing links throughout the universe for the first time.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  

&lt;section class="button-group button-group--stacked"&gt;
  
    &lt;h2 class="glue-headline glue-headline--headline-6 button-group__title"&gt;Learn more about our work&lt;/h2&gt;
  

  
&lt;/section&gt;
                
              
                
                
                  
                  &lt;section class="notes"&gt;
  &lt;div class="glue-page"&gt;
    &lt;div class="gdm-rich-text notes__inner"&gt;
      &lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;This research was done by Jonas Buchli, Brendan Tracey, Tomislav Andric, Christopher Wipf, Yu Him Justin Chiu, Matthias Lochbrunner, Craig Donner, Rana X Adhikari, Jan Harms, Iain Barr, Roland Hafner, Andrea Huber, Abbas Abdolmaleki, Charlie Beattie, Joseph Betzwieser, Serkan Cabi, Jonas Degrave, Yuzhu Dong, Leslie Fritz, Anchal Gupta, Oliver Groth, Sandy Huang, Tamara Norman, Hannah Openshaw, Jameson Rollins, Greg Thornton, George van den Driessche, Markus Wulfmeier, Pushmeet Kohli, Martin Riedmiller and is a collaboration of LIGO, Caltech, GSSI and GDM.&lt;/p&gt;&lt;p&gt;We’d like to thank the fantastic LIGO instrument team for their tireless work on keeping the observatories up and running and supporting our experiments.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/section&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/using-ai-to-perceive-the-universe-in-greater-depth/</guid><pubDate>Thu, 04 Sep 2025 18:00:00 +0000</pubDate></item><item><title>[NEW] AI logistics startup Augment, from Deliverr’s founder, raises massive $85M Series A (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/04/ai-logistics-startup-augment-from-deliverrs-founder-raises-massive-85m-series-a/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/Harish-Abbott-2.png?resize=1200,670" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Having built and sold e-commerce shipping startup Deliverr to Shopify for $2.1 billion in 2022, co-founder and CEO Harish Abbott knows the logistics industry well.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Abbott felt that many manual tasks in logistics could be automated using AI. That’s why last year he launched Augment, which offers an AI assistant called “Augie” that can take over tedious and repetitive work performed by freight shippers, carriers, and brokers.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;On Thursday, Augment announced that it raised an $85 million Series A led by Redpoint, with participation from 8VC, Autotech Ventures, and others. The massive round comes just five months after the startup launched out of stealth with a hefty $25 million seed round.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Freight and logistics is a very large industry that employs lots of people who are busy chasing emails, documents, phone calls, text messages all day long,” Abbott told TechCrunch. “Augie can take care of all that like their own personal assistant, so they can focus on relationships and negotiations.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Today, Augie can perform seven key tasks in the logistics process, from gathering and reviewing pricing bids from trucking companies and tracking packages en route, to building a load — the method of combining multiple shipments to maximize truck space — and collecting invoicing documents to ensure timely billing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;All of these processes typically involve numerous phone calls, emails, or texts exchanged between various participants in the logistics process. Augie can help humans streamline those communications by operating across multiple channels like voice, email, Slack, SMS, and Telegram.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Augment is not disclosing its revenue, Abbott says the company has more than doubled the number of customers it serves since raising its seed round.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Although many customers are still trialing the product, fully onboarded clients like Armstrong Transport Group are already achieving significant productivity gains with Augment, such as a 40% reduction in invoice delays.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jacob Effron, a managing director at Redpoint, said he invested in Augment after speaking with a number of Augment’s clients. “The customer feedback is honestly amazing. People really love the product. I think they use it in quite a ubiquitous way.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The large funding round will go toward hiring 50 engineers and then working on adding more features. “It is a large, fragmented market. It’s complex, it’s messy. The systems are a bit archaic and siloed,” Abbott said. “We have to have many engineers because there’s lots of software systems these companies use.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company is currently serving the trucking industry, but its long-term vision is to expand into international shipping and other aspects of logistics.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Abbott is not alone in pursuing AI for logistics. Other AI assistants for freight management include Vooma and FleetWorks.&amp;nbsp;Meanwhile, shipping giants FedEx and UPS have signaled that they are investing in proprietary AI technology.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But all the competition doesn’t phase Abbott. “We have tremendous adoption,” he said. “Augie does really cool stuff. Augie is really thinking ahead and reasoning like a human and acting on it, so it saves everybody a lot of time.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/Harish-Abbott-2.png?resize=1200,670" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Having built and sold e-commerce shipping startup Deliverr to Shopify for $2.1 billion in 2022, co-founder and CEO Harish Abbott knows the logistics industry well.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Abbott felt that many manual tasks in logistics could be automated using AI. That’s why last year he launched Augment, which offers an AI assistant called “Augie” that can take over tedious and repetitive work performed by freight shippers, carriers, and brokers.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;On Thursday, Augment announced that it raised an $85 million Series A led by Redpoint, with participation from 8VC, Autotech Ventures, and others. The massive round comes just five months after the startup launched out of stealth with a hefty $25 million seed round.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Freight and logistics is a very large industry that employs lots of people who are busy chasing emails, documents, phone calls, text messages all day long,” Abbott told TechCrunch. “Augie can take care of all that like their own personal assistant, so they can focus on relationships and negotiations.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Today, Augie can perform seven key tasks in the logistics process, from gathering and reviewing pricing bids from trucking companies and tracking packages en route, to building a load — the method of combining multiple shipments to maximize truck space — and collecting invoicing documents to ensure timely billing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;All of these processes typically involve numerous phone calls, emails, or texts exchanged between various participants in the logistics process. Augie can help humans streamline those communications by operating across multiple channels like voice, email, Slack, SMS, and Telegram.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Augment is not disclosing its revenue, Abbott says the company has more than doubled the number of customers it serves since raising its seed round.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Although many customers are still trialing the product, fully onboarded clients like Armstrong Transport Group are already achieving significant productivity gains with Augment, such as a 40% reduction in invoice delays.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jacob Effron, a managing director at Redpoint, said he invested in Augment after speaking with a number of Augment’s clients. “The customer feedback is honestly amazing. People really love the product. I think they use it in quite a ubiquitous way.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The large funding round will go toward hiring 50 engineers and then working on adding more features. “It is a large, fragmented market. It’s complex, it’s messy. The systems are a bit archaic and siloed,” Abbott said. “We have to have many engineers because there’s lots of software systems these companies use.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company is currently serving the trucking industry, but its long-term vision is to expand into international shipping and other aspects of logistics.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Abbott is not alone in pursuing AI for logistics. Other AI assistants for freight management include Vooma and FleetWorks.&amp;nbsp;Meanwhile, shipping giants FedEx and UPS have signaled that they are investing in proprietary AI technology.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But all the competition doesn’t phase Abbott. “We have tremendous adoption,” he said. “Augie does really cool stuff. Augie is really thinking ahead and reasoning like a human and acting on it, so it saves everybody a lot of time.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/04/ai-logistics-startup-augment-from-deliverrs-founder-raises-massive-85m-series-a/</guid><pubDate>Thu, 04 Sep 2025 19:00:00 +0000</pubDate></item><item><title>[NEW] OpenAI announces AI-powered hiring platform to take on LinkedIn (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/04/openai-announces-ai-powered-hiring-platform-to-take-on-linkedin/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2198379368.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI says it’s developing an AI-powered hiring platform to connect businesses and employees, a service that would put the outfit in close competition with LinkedIn. The product is called the OpenAI Jobs Platform, and the company expects to launch the service by mid-2026, an OpenAI spokesperson told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI CEO of Applications Fidji Simo announced the new endeavor in a blog post Thursday, saying the company will “use AI to help find the perfect matches between what companies need and what workers can offer.” Simo said the service would offer a dedicated track for small businesses and local governments to access top AI talent.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI is interested in expanding into several new markets beyond its core consumer offering, ChatGPT. At a recent dinner with reporters, OpenAI CEO Sam Altman said that Simo would oversee several applications beyond the chatbot. This will apparently include the OpenAI Jobs Platform and potentially other offerings OpenAI is reportedly working on, such as a browser and a social media app.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, OpenAI’s hiring platform could put the company in direct competition with LinkedIn, which was co-founded by Reid Hoffman, one of OpenAI’s earliest investors. LinkedIn is also owned by Microsoft, OpenAI’s largest financial backer. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the last year, LinkedIn has worked to infuse its platform with AI features to help match job candidates with businesses.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI also says it will start offering certifications for people with different levels of “AI fluency” through its OpenAI Academy, an online program the company launched last year. An OpenAI spokesperson says the company plans to launch a pilot of OpenAI Certifications in late 2025.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Many tech executives have raised concerns that AI will disrupt numerous traditional jobs. Anthropic CEO Dario Amodei has said that AI could eliminate up to 50% of entry-level white-collar jobs before 2030. In her blog post, Simo acknowledged that risk, saying OpenAI can’t prevent that disruption. However, she says the company can do its part by helping people become fluent in AI and connecting them with companies that need their skills.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The ChatGPT maker says it’s working with Walmart, one of the biggest private employers in the world, on its certification program and aims to certify 10 million Americans by 2030.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI says it’s launching these programs as part of its commitment to the White House’s initiative to expand AI literacy. Altman and other Big Tech executives are meeting with President Donald Trump at the White House on Thursday to discuss AI.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2198379368.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI says it’s developing an AI-powered hiring platform to connect businesses and employees, a service that would put the outfit in close competition with LinkedIn. The product is called the OpenAI Jobs Platform, and the company expects to launch the service by mid-2026, an OpenAI spokesperson told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI CEO of Applications Fidji Simo announced the new endeavor in a blog post Thursday, saying the company will “use AI to help find the perfect matches between what companies need and what workers can offer.” Simo said the service would offer a dedicated track for small businesses and local governments to access top AI talent.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI is interested in expanding into several new markets beyond its core consumer offering, ChatGPT. At a recent dinner with reporters, OpenAI CEO Sam Altman said that Simo would oversee several applications beyond the chatbot. This will apparently include the OpenAI Jobs Platform and potentially other offerings OpenAI is reportedly working on, such as a browser and a social media app.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, OpenAI’s hiring platform could put the company in direct competition with LinkedIn, which was co-founded by Reid Hoffman, one of OpenAI’s earliest investors. LinkedIn is also owned by Microsoft, OpenAI’s largest financial backer. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the last year, LinkedIn has worked to infuse its platform with AI features to help match job candidates with businesses.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI also says it will start offering certifications for people with different levels of “AI fluency” through its OpenAI Academy, an online program the company launched last year. An OpenAI spokesperson says the company plans to launch a pilot of OpenAI Certifications in late 2025.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Many tech executives have raised concerns that AI will disrupt numerous traditional jobs. Anthropic CEO Dario Amodei has said that AI could eliminate up to 50% of entry-level white-collar jobs before 2030. In her blog post, Simo acknowledged that risk, saying OpenAI can’t prevent that disruption. However, she says the company can do its part by helping people become fluent in AI and connecting them with companies that need their skills.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The ChatGPT maker says it’s working with Walmart, one of the biggest private employers in the world, on its certification program and aims to certify 10 million Americans by 2030.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI says it’s launching these programs as part of its commitment to the White House’s initiative to expand AI literacy. Altman and other Big Tech executives are meeting with President Donald Trump at the White House on Thursday to discuss AI.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/04/openai-announces-ai-powered-hiring-platform-to-take-on-linkedin/</guid><pubDate>Thu, 04 Sep 2025 19:19:46 +0000</pubDate></item><item><title>[NEW] Fashion retailers partner to offer personalized AI styling tool ‘Ella’ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/04/fashion-retailers-partner-to-offer-personalized-ai-styling-tool-ella/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The luxury membership platform Vivrelle, which allows customers to rent high-end goods, announced Thursday the launch of an AI personal styling tool called Ella as part of its partnership with fashion retailers Revolve and FWRD.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The launch is an example of how the fashion industry is leveraging AI technology to enhance customer experiences and is one of the first partnerships to see three retailers come together to offer a personalized AI experience. Revolve and FWRD let users shop designer clothing, while Revolve also has an option to shop pre-owned. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The tool, Ella, provides recommendations to customers across the three retailers on what to purchase or rent to make an outfit come to life.&amp;nbsp;For example, users can ask for “a bachelorette weekend outfit,” or “what to pack for a trip,” and the technology will search across the Vivrelle, FWRD, and Revolve shopping platforms to create outfit suggestions. Users can then check out in one cart on Vivrelle. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In theory, the more one uses Ella, the better its suggestions become. It’s the fashion equivalent of asking ChatGPT what to wear in Miami for a girl’s weekend.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3042438" height="525" src="https://techcrunch.com/wp-content/uploads/2025/09/ELLA-AI-.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Vivrelle&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Blake Geffen, the CEO and co-founder of Vivrelle (which announced a $62 million Series C earlier this year), told TechCrunch that she hopes Ella can take the “stress out of packing for a vacation or everyday dressing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Ella has been in the works for quite some time,” she told TechCrunch, adding that it took about a year to build and release the product.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is actually the second AI tool from the three companies. The Vivrelle, Revolve, and FWRD partnership  earlier this year also launched Complete the Look, which offers last-minute fashion suggestions to complement what’s in a customer’s cart at checkout. Their latest tool, Ella, however, takes the fashion recommendation game to another level.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Fashion has been obsessed with trying to make personalized shopping happen for decades now. Even the 90s movie “Clueless” showed Cher picking outfits from her digitized wardrobe.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This current AI boom has led to rapid innovation and democratized access to AI technology, allowing many fashion companies to launch personalized AI fashion platforms and raise millions while doing so.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“With Ella, we’re giving our members as much flexibility and options as possible to shop or borrow with ease, through seamless conversations that allow you to share as little or as much as you want, just like talking to a live stylist,” Geffen said. “We’re excited to be the first brand to integrate rental, resale, and retail into one streamlined omnichannel experience.”&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The luxury membership platform Vivrelle, which allows customers to rent high-end goods, announced Thursday the launch of an AI personal styling tool called Ella as part of its partnership with fashion retailers Revolve and FWRD.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The launch is an example of how the fashion industry is leveraging AI technology to enhance customer experiences and is one of the first partnerships to see three retailers come together to offer a personalized AI experience. Revolve and FWRD let users shop designer clothing, while Revolve also has an option to shop pre-owned. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The tool, Ella, provides recommendations to customers across the three retailers on what to purchase or rent to make an outfit come to life.&amp;nbsp;For example, users can ask for “a bachelorette weekend outfit,” or “what to pack for a trip,” and the technology will search across the Vivrelle, FWRD, and Revolve shopping platforms to create outfit suggestions. Users can then check out in one cart on Vivrelle. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In theory, the more one uses Ella, the better its suggestions become. It’s the fashion equivalent of asking ChatGPT what to wear in Miami for a girl’s weekend.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3042438" height="525" src="https://techcrunch.com/wp-content/uploads/2025/09/ELLA-AI-.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Vivrelle&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Blake Geffen, the CEO and co-founder of Vivrelle (which announced a $62 million Series C earlier this year), told TechCrunch that she hopes Ella can take the “stress out of packing for a vacation or everyday dressing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Ella has been in the works for quite some time,” she told TechCrunch, adding that it took about a year to build and release the product.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is actually the second AI tool from the three companies. The Vivrelle, Revolve, and FWRD partnership  earlier this year also launched Complete the Look, which offers last-minute fashion suggestions to complement what’s in a customer’s cart at checkout. Their latest tool, Ella, however, takes the fashion recommendation game to another level.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Fashion has been obsessed with trying to make personalized shopping happen for decades now. Even the 90s movie “Clueless” showed Cher picking outfits from her digitized wardrobe.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This current AI boom has led to rapid innovation and democratized access to AI technology, allowing many fashion companies to launch personalized AI fashion platforms and raise millions while doing so.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“With Ella, we’re giving our members as much flexibility and options as possible to shop or borrow with ease, through seamless conversations that allow you to share as little or as much as you want, just like talking to a live stylist,” Geffen said. “We’re excited to be the first brand to integrate rental, resale, and retail into one streamlined omnichannel experience.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/04/fashion-retailers-partner-to-offer-personalized-ai-styling-tool-ella/</guid><pubDate>Thu, 04 Sep 2025 19:45:00 +0000</pubDate></item><item><title>[NEW] NVIDIA Pledges AI Education Funding for K-12 Programs (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/ai-education-k-12/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/nvidiaheadquarters.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA today announced new AI education support for K-12 programs at a White House event to celebrate public-private partnerships that advance artificial intelligence education for America’s youth.&lt;/p&gt;
&lt;p&gt;The commitment comes after recent NVIDIA announcements to support AI education and academic research, including a $30 million contribution to the National AI Intelligence Research pilot and a U.S. National Science Foundation partnership in support of academic research. Over the last five years, NVIDIA has invested $50 million in higher education and academic research in the United States.&lt;/p&gt;
&lt;p&gt;Pledging $25 million in support with AI education programs, NVIDIA is partnering with Study Fetch and CK-12 — two leading K-12 learning platforms — to tailor the NVIDIA Deep Learning Institute (DLI) and NVIDIA Academy content offerings to meet the instructional needs of U.S. K-12 classrooms.&lt;/p&gt;
&lt;p&gt;The NVIDIA effort aligns with the White House executive order Advancing Artificial Intelligence Education for American Youth, announced in April. Additionally, in support of the executive order, NVIDIA signed the White House’s Pledge to America’s Youth: Investing in AI Education, committing to delivering AI literacy, credentialing and educator enablement.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Supporting America’s Educators in Driving AI Literacy&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;In the first year, NVIDIA will support curriculum adaptation, platform integration, educator training, institutional engagement and ecosystem-wide outreach.&lt;/p&gt;
&lt;p&gt;The NVIDIA DLI program integrations with Study Fetch and CK-12 will make available NVIDIA’s industry-leading training materials to help empower high school educators in applying DLI Teaching Kits.&lt;/p&gt;
&lt;p&gt;NVIDIA DLI courses are geared toward teaching professional skills to developers. NVIDIA is partnering with Study Fetch and CK-12, which will curate the course material content for high school students to get hands-on experience with AI, aiming to spark curiosity, build practical skills, and prepare the next generation of job seekers to thrive in the AI-driven economy.&lt;/p&gt;
&lt;p&gt;The NVIDIA partnership aims to reach 1 million K-12 students within three years.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Preparing the Next Generation for AI Leadership&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The White House initiative and NVIDIA commitments are united on a central mission to drive American leadership in AI.&lt;/p&gt;
&lt;p&gt;Winning the AI Race: America’s AI Action Plan was announced in July by the White House, supported with executive orders to accelerate federal permitting of data center infrastructure and promote exportation of the American AI technology stack.&lt;/p&gt;
&lt;p&gt;Aligned with the White House AI Action Plan, NVIDIA and the U.S. National Science Foundation recently committed $152 million in support to Ai2 for the development of open AI models to drive U.S. academic and nonprofit scientific leadership.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/nvidiaheadquarters.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA today announced new AI education support for K-12 programs at a White House event to celebrate public-private partnerships that advance artificial intelligence education for America’s youth.&lt;/p&gt;
&lt;p&gt;The commitment comes after recent NVIDIA announcements to support AI education and academic research, including a $30 million contribution to the National AI Intelligence Research pilot and a U.S. National Science Foundation partnership in support of academic research. Over the last five years, NVIDIA has invested $50 million in higher education and academic research in the United States.&lt;/p&gt;
&lt;p&gt;Pledging $25 million in support with AI education programs, NVIDIA is partnering with Study Fetch and CK-12 — two leading K-12 learning platforms — to tailor the NVIDIA Deep Learning Institute (DLI) and NVIDIA Academy content offerings to meet the instructional needs of U.S. K-12 classrooms.&lt;/p&gt;
&lt;p&gt;The NVIDIA effort aligns with the White House executive order Advancing Artificial Intelligence Education for American Youth, announced in April. Additionally, in support of the executive order, NVIDIA signed the White House’s Pledge to America’s Youth: Investing in AI Education, committing to delivering AI literacy, credentialing and educator enablement.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Supporting America’s Educators in Driving AI Literacy&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;In the first year, NVIDIA will support curriculum adaptation, platform integration, educator training, institutional engagement and ecosystem-wide outreach.&lt;/p&gt;
&lt;p&gt;The NVIDIA DLI program integrations with Study Fetch and CK-12 will make available NVIDIA’s industry-leading training materials to help empower high school educators in applying DLI Teaching Kits.&lt;/p&gt;
&lt;p&gt;NVIDIA DLI courses are geared toward teaching professional skills to developers. NVIDIA is partnering with Study Fetch and CK-12, which will curate the course material content for high school students to get hands-on experience with AI, aiming to spark curiosity, build practical skills, and prepare the next generation of job seekers to thrive in the AI-driven economy.&lt;/p&gt;
&lt;p&gt;The NVIDIA partnership aims to reach 1 million K-12 students within three years.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Preparing the Next Generation for AI Leadership&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The White House initiative and NVIDIA commitments are united on a central mission to drive American leadership in AI.&lt;/p&gt;
&lt;p&gt;Winning the AI Race: America’s AI Action Plan was announced in July by the White House, supported with executive orders to accelerate federal permitting of data center infrastructure and promote exportation of the American AI technology stack.&lt;/p&gt;
&lt;p&gt;Aligned with the White House AI Action Plan, NVIDIA and the U.S. National Science Foundation recently committed $152 million in support to Ai2 for the development of open AI models to drive U.S. academic and nonprofit scientific leadership.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/ai-education-k-12/</guid><pubDate>Thu, 04 Sep 2025 19:46:50 +0000</pubDate></item><item><title>[NEW] A greener way to 3D print stronger stuff (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/greener-way-3d-print-stronger-stuff-0904</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202509/mit-csail-SustainaPrint.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr" id="docs-internal-guid-8a16a100-7fff-866e-17d3-0c98f0ab7768"&gt;3D printing has come a long way since its invention in 1983 by Chuck Hull, who pioneered stereolithography, a technique that solidifies liquid resin into solid objects using ultraviolet lasers. Over the decades, 3D printers have evolved from experimental curiosities into tools capable of producing everything from custom prosthetics to complex food designs, architectural models, and even functioning human organs.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;But as the technology matures, its environmental footprint has become increasingly difficult to set aside. The vast majority of consumer and industrial 3D printing still relies on petroleum-based plastic filament. And while “greener” alternatives made from biodegradable or recycled materials exist, they come with a serious trade-off: they’re often not as strong. These eco-friendly filaments tend to become brittle under stress, making them ill-suited for structural applications or load-bearing parts — exactly where strength matters most.&lt;/p&gt;&lt;p dir="ltr"&gt;This trade-off between sustainability and mechanical performance prompted researchers at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Hasso Plattner Institute to ask: Is it possible to build objects that are mostly eco-friendly, but still strong where it counts?&lt;/p&gt;&lt;p dir="ltr"&gt;Their answer is SustainaPrint, a new software and hardware toolkit designed to help users strategically combine strong and weak filaments to get the best of both worlds. Instead of printing an entire object with high-performance plastic, the system analyzes a model through finite element analysis simulations, predicts where the object is most likely to experience stress, and then reinforces just those zones with stronger material. The rest of the part can be printed using greener, weaker filament, reducing plastic use while preserving structural integrity.&lt;/p&gt;&lt;p dir="ltr"&gt;“Our hope is that SustainaPrint can be used in industrial and distributed manufacturing settings one day, where local material stocks may vary in quality and composition,” says MIT PhD student and CSAIL researcher Maxine Perroni-Scharf, who is a lead author on a paper presenting the project. “In these contexts, the testing toolkit could help ensure the reliability of available filaments, while the software’s reinforcement strategy could reduce overall material consumption without sacrificing function.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;For their experiments, the team used Polymaker’s PolyTerra PLA as the eco-friendly filament, and standard or Tough PLA from Ultimaker for reinforcement. They used a 20 percent reinforcement threshold to show that even a small amount of strong plastic goes a long way. Using this ratio, SustainaPrint was able to recover up to 70 percent of the strength of an object printed entirely with high-performance plastic.&lt;/p&gt;&lt;p dir="ltr"&gt;They printed dozens of objects, from simple mechanical shapes like rings and beams to more functional household items such as headphone stands, wall hooks, and plant pots. Each object was printed three ways: once using only eco-friendly filament, once using only strong PLA, and once with the hybrid SustainaPrint configuration. The printed parts were then mechanically tested by pulling, bending, or otherwise breaking them to measure how much force each configuration could withstand.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;In many cases, the hybrid prints held up nearly as well as the full-strength versions. For example, in one test involving a dome-like shape, the hybrid version outperformed the version printed entirely in Tough PLA. The team believes this may be due to the reinforced version’s ability to distribute stress more evenly, avoiding the brittle failure sometimes caused by excessive stiffness.&lt;/p&gt;&lt;p dir="ltr"&gt;“This indicates that in certain geometries and loading conditions, mixing materials strategically may actually outperform a single homogenous material,” says Perroni-Scharf. “It’s a reminder that real-world mechanical behavior is full of complexity, especially in 3D printing, where interlayer adhesion and tool path decisions can affect performance in unexpected ways.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;A lean, green, eco-friendly printing machine&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;SustainaPrint starts off by letting a user upload their 3D model into a custom interface. By selecting fixed regions and areas where forces will be applied, the software then uses an approach called “Finite Element Analysis” to simulate how the object will deform under stress. It then creates a map showing pressure distribution inside the structure, highlighting areas under compression or tension, and applies heuristics to segment the object into two categories: those that need reinforcement, and those that don’t.&lt;/p&gt;&lt;p dir="ltr"&gt;Recognizing the need for accessible and low-cost testing, the team also developed a DIY testing toolkit to help users assess strength before printing. The kit has a 3D-printable device with modules for measuring both tensile and flexural strength. Users can pair the device with common items like pull-up bars or digital scales to get rough, but reliable performance metrics. The team benchmarked their results against manufacturer data and found that their measurements consistently fell within one standard deviation, even for filaments that had undergone multiple recycling cycles.&lt;/p&gt;&lt;p dir="ltr"&gt;Although the current system is designed for dual-extrusion printers, the researchers believe that with some manual filament swapping and calibration, it could be adapted for single-extruder setups, too. In current form, the system simplifies the modeling process by allowing just one force and one fixed boundary per simulation. While this covers a wide range of common use cases, the team sees future work expanding the software to support more complex and dynamic loading conditions. The team also sees potential in using AI to infer the object’s intended use based on its geometry, which could allow for fully automated stress modeling without manual input of forces or boundaries.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;3D for free&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers plan to release SustainaPrint open-source, making both the software and testing toolkit available for public use and modification. Another initiative they aspire to bring to life in the future: education. “In a classroom, SustainaPrint isn’t just a tool, it’s a way to teach students about material science, structural engineering, and sustainable design, all in one project,” says Perroni-Scharf. “It turns these abstract concepts into something tangible.”&lt;/p&gt;&lt;p dir="ltr"&gt;As 3D printing becomes more embedded in how we manufacture and prototype everything from consumer goods to emergency equipment, sustainability concerns will only grow. With tools like SustainaPrint, those concerns no longer need to come at the expense of performance. Instead, they can become part of the design process: built into the very geometry of the things we make.&lt;/p&gt;&lt;p dir="ltr"&gt;Co-author Patrick Baudisch, who is a professor at the Hasso Plattner Institute, adds that “the project addresses a key question: What is the point of collecting material for the purpose of recycling, when there is no plan to actually ever use that material? Maxine presents the missing link between the theoretical/abstract idea of 3D printing material recycling and what it actually takes to make this idea relevant.”&lt;/p&gt;&lt;p dir="ltr"&gt;Perroni-Scharf and Baudisch wrote the paper with CSAIL research assistant Jennifer Xiao; MIT Department of Electrical Engineering and Computer Science master’s student Cole Paulin ’24; master’s student Ray Wang SM ’25 and PhD student Ticha Sethapakdi SM ’19 (both CSAIL members); Hasso Plattner Institute PhD student Muhammad Abdullah; and Associate Professor Stefanie Mueller, lead of the Human-Computer Interaction Engineering Group at CSAIL.&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers’ work was supported by a Designing for Sustainability Grant from the Designing for Sustainability MIT-HPI Research Program. Their work will be presented at the ACM Symposium on User Interface Software and Technology in September.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202509/mit-csail-SustainaPrint.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr" id="docs-internal-guid-8a16a100-7fff-866e-17d3-0c98f0ab7768"&gt;3D printing has come a long way since its invention in 1983 by Chuck Hull, who pioneered stereolithography, a technique that solidifies liquid resin into solid objects using ultraviolet lasers. Over the decades, 3D printers have evolved from experimental curiosities into tools capable of producing everything from custom prosthetics to complex food designs, architectural models, and even functioning human organs.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;But as the technology matures, its environmental footprint has become increasingly difficult to set aside. The vast majority of consumer and industrial 3D printing still relies on petroleum-based plastic filament. And while “greener” alternatives made from biodegradable or recycled materials exist, they come with a serious trade-off: they’re often not as strong. These eco-friendly filaments tend to become brittle under stress, making them ill-suited for structural applications or load-bearing parts — exactly where strength matters most.&lt;/p&gt;&lt;p dir="ltr"&gt;This trade-off between sustainability and mechanical performance prompted researchers at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Hasso Plattner Institute to ask: Is it possible to build objects that are mostly eco-friendly, but still strong where it counts?&lt;/p&gt;&lt;p dir="ltr"&gt;Their answer is SustainaPrint, a new software and hardware toolkit designed to help users strategically combine strong and weak filaments to get the best of both worlds. Instead of printing an entire object with high-performance plastic, the system analyzes a model through finite element analysis simulations, predicts where the object is most likely to experience stress, and then reinforces just those zones with stronger material. The rest of the part can be printed using greener, weaker filament, reducing plastic use while preserving structural integrity.&lt;/p&gt;&lt;p dir="ltr"&gt;“Our hope is that SustainaPrint can be used in industrial and distributed manufacturing settings one day, where local material stocks may vary in quality and composition,” says MIT PhD student and CSAIL researcher Maxine Perroni-Scharf, who is a lead author on a paper presenting the project. “In these contexts, the testing toolkit could help ensure the reliability of available filaments, while the software’s reinforcement strategy could reduce overall material consumption without sacrificing function.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;For their experiments, the team used Polymaker’s PolyTerra PLA as the eco-friendly filament, and standard or Tough PLA from Ultimaker for reinforcement. They used a 20 percent reinforcement threshold to show that even a small amount of strong plastic goes a long way. Using this ratio, SustainaPrint was able to recover up to 70 percent of the strength of an object printed entirely with high-performance plastic.&lt;/p&gt;&lt;p dir="ltr"&gt;They printed dozens of objects, from simple mechanical shapes like rings and beams to more functional household items such as headphone stands, wall hooks, and plant pots. Each object was printed three ways: once using only eco-friendly filament, once using only strong PLA, and once with the hybrid SustainaPrint configuration. The printed parts were then mechanically tested by pulling, bending, or otherwise breaking them to measure how much force each configuration could withstand.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;In many cases, the hybrid prints held up nearly as well as the full-strength versions. For example, in one test involving a dome-like shape, the hybrid version outperformed the version printed entirely in Tough PLA. The team believes this may be due to the reinforced version’s ability to distribute stress more evenly, avoiding the brittle failure sometimes caused by excessive stiffness.&lt;/p&gt;&lt;p dir="ltr"&gt;“This indicates that in certain geometries and loading conditions, mixing materials strategically may actually outperform a single homogenous material,” says Perroni-Scharf. “It’s a reminder that real-world mechanical behavior is full of complexity, especially in 3D printing, where interlayer adhesion and tool path decisions can affect performance in unexpected ways.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;A lean, green, eco-friendly printing machine&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;SustainaPrint starts off by letting a user upload their 3D model into a custom interface. By selecting fixed regions and areas where forces will be applied, the software then uses an approach called “Finite Element Analysis” to simulate how the object will deform under stress. It then creates a map showing pressure distribution inside the structure, highlighting areas under compression or tension, and applies heuristics to segment the object into two categories: those that need reinforcement, and those that don’t.&lt;/p&gt;&lt;p dir="ltr"&gt;Recognizing the need for accessible and low-cost testing, the team also developed a DIY testing toolkit to help users assess strength before printing. The kit has a 3D-printable device with modules for measuring both tensile and flexural strength. Users can pair the device with common items like pull-up bars or digital scales to get rough, but reliable performance metrics. The team benchmarked their results against manufacturer data and found that their measurements consistently fell within one standard deviation, even for filaments that had undergone multiple recycling cycles.&lt;/p&gt;&lt;p dir="ltr"&gt;Although the current system is designed for dual-extrusion printers, the researchers believe that with some manual filament swapping and calibration, it could be adapted for single-extruder setups, too. In current form, the system simplifies the modeling process by allowing just one force and one fixed boundary per simulation. While this covers a wide range of common use cases, the team sees future work expanding the software to support more complex and dynamic loading conditions. The team also sees potential in using AI to infer the object’s intended use based on its geometry, which could allow for fully automated stress modeling without manual input of forces or boundaries.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;3D for free&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers plan to release SustainaPrint open-source, making both the software and testing toolkit available for public use and modification. Another initiative they aspire to bring to life in the future: education. “In a classroom, SustainaPrint isn’t just a tool, it’s a way to teach students about material science, structural engineering, and sustainable design, all in one project,” says Perroni-Scharf. “It turns these abstract concepts into something tangible.”&lt;/p&gt;&lt;p dir="ltr"&gt;As 3D printing becomes more embedded in how we manufacture and prototype everything from consumer goods to emergency equipment, sustainability concerns will only grow. With tools like SustainaPrint, those concerns no longer need to come at the expense of performance. Instead, they can become part of the design process: built into the very geometry of the things we make.&lt;/p&gt;&lt;p dir="ltr"&gt;Co-author Patrick Baudisch, who is a professor at the Hasso Plattner Institute, adds that “the project addresses a key question: What is the point of collecting material for the purpose of recycling, when there is no plan to actually ever use that material? Maxine presents the missing link between the theoretical/abstract idea of 3D printing material recycling and what it actually takes to make this idea relevant.”&lt;/p&gt;&lt;p dir="ltr"&gt;Perroni-Scharf and Baudisch wrote the paper with CSAIL research assistant Jennifer Xiao; MIT Department of Electrical Engineering and Computer Science master’s student Cole Paulin ’24; master’s student Ray Wang SM ’25 and PhD student Ticha Sethapakdi SM ’19 (both CSAIL members); Hasso Plattner Institute PhD student Muhammad Abdullah; and Associate Professor Stefanie Mueller, lead of the Human-Computer Interaction Engineering Group at CSAIL.&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers’ work was supported by a Designing for Sustainability Grant from the Designing for Sustainability MIT-HPI Research Program. Their work will be presented at the ACM Symposium on User Interface Software and Technology in September.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/greener-way-3d-print-stronger-stuff-0904</guid><pubDate>Thu, 04 Sep 2025 20:30:00 +0000</pubDate></item><item><title>[NEW] Bret Taylor’s Sierra raises $350M at a $10B valuation (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/04/bret-taylors-sierra-raises-350m-at-a-10b-valuation/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2022/10/AP22166762816752.jpg?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Investors are clearly bullish about former Salesforce co-CEO Bret Taylor’s AI agent startup Sierra.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sierra, which helps enterprises build customer service AI agents, announced it raised a $350 million funding round on Thursday. The round, led by earlier investor Greenoaks Capital, values the startup at $10 billion, according to a company blog post that confirmed an earlier report from Axios on Wednesday.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Sierra was founded in early 2024 by Taylor and longtime Google alum Clay Bavor. The company claims to have landed hundreds of customers, including SoFi, Ramp, and Brex, among others, in its 18 months of operation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sierra has now raised $635 million altogether, including $110 million that closed in February of last year led by Sequoia and Benchmark, and a $175 million round that closed in October of last year led by Greenoaks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Others of its investors include ICONIQ and Thrive Capital.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As TechCrunch has previously reported, Taylor and Bavor have a long history in customer service tech. Taylor spent nearly a decade at Salesforce and years ago founded Quip, which Salesforce bought for $750 million in the summer of 2016. Bavor managed Gmail and Google Drive at Google, among other consumer-facing products.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Taylor met Bavor while at Google, where he worked before serving as Facebook’s CTO for several years. At Google, Taylor is widely credited with helping to launch Google Maps. Years later, he’d oversee the Twitter board throughout Elon Musk’s takeover of the social media site.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this week, in fact, Taylor announced that Sierra is launching its second year of its so-called APX program, a rotational opportunity for recent technical graduates that directly mirrors the Google program that launched both Taylor’s and co-founder Clay Bavor’s careers two decades ago. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The hiring announcement stands out in what has become an increasingly tough job market, particularly as companies assess the power of AI technologies like those Sierra is selling and their potential impact on workforce needs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The program targets computer science graduates and says it offers experience in both agent engineering and product management. Taylor described the roles as providing what he calls “an irresponsible amount of responsibility” — similar to the freedom to build and launch products that he and Bavor had at Google — with new graduates expected to work on multiple product launches during their first year.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2022/10/AP22166762816752.jpg?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Investors are clearly bullish about former Salesforce co-CEO Bret Taylor’s AI agent startup Sierra.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sierra, which helps enterprises build customer service AI agents, announced it raised a $350 million funding round on Thursday. The round, led by earlier investor Greenoaks Capital, values the startup at $10 billion, according to a company blog post that confirmed an earlier report from Axios on Wednesday.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Sierra was founded in early 2024 by Taylor and longtime Google alum Clay Bavor. The company claims to have landed hundreds of customers, including SoFi, Ramp, and Brex, among others, in its 18 months of operation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sierra has now raised $635 million altogether, including $110 million that closed in February of last year led by Sequoia and Benchmark, and a $175 million round that closed in October of last year led by Greenoaks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Others of its investors include ICONIQ and Thrive Capital.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As TechCrunch has previously reported, Taylor and Bavor have a long history in customer service tech. Taylor spent nearly a decade at Salesforce and years ago founded Quip, which Salesforce bought for $750 million in the summer of 2016. Bavor managed Gmail and Google Drive at Google, among other consumer-facing products.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Taylor met Bavor while at Google, where he worked before serving as Facebook’s CTO for several years. At Google, Taylor is widely credited with helping to launch Google Maps. Years later, he’d oversee the Twitter board throughout Elon Musk’s takeover of the social media site.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this week, in fact, Taylor announced that Sierra is launching its second year of its so-called APX program, a rotational opportunity for recent technical graduates that directly mirrors the Google program that launched both Taylor’s and co-founder Clay Bavor’s careers two decades ago. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The hiring announcement stands out in what has become an increasingly tough job market, particularly as companies assess the power of AI technologies like those Sierra is selling and their potential impact on workforce needs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The program targets computer science graduates and says it offers experience in both agent engineering and product management. Taylor described the roles as providing what he calls “an irresponsible amount of responsibility” — similar to the freedom to build and launch products that he and Bavor had at Google — with new graduates expected to work on multiple product launches during their first year.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/04/bret-taylors-sierra-raises-350m-at-a-10b-valuation/</guid><pubDate>Thu, 04 Sep 2025 23:33:40 +0000</pubDate></item></channel></rss>