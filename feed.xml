<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 06 Jan 2026 06:38:44 +0000</lastBuildDate><item><title>[NEW] Amazon Alexa+ released to the general public via an early access website (AI - Ars Technica)</title><link>https://arstechnica.com/gadgets/2026/01/amazon-alexa-released-to-the-general-public-via-an-early-access-website/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Amazon brings back browser-based Alexa but will eventually add a paywall.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Alexa+ logo with a person standing in front of it" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-2201505743-640x427.jpg" width="640" /&gt;
                  &lt;img alt="Alexa+ logo with a person standing in front of it" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-2201505743-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Alexa+ signage during an unveiling event in New York, US, on Wednesday, Feb. 26, 2025. Amazon has rebooted Alexa with artificial intelligence, marking the biggest overhaul of the voice-activated assistant since its introduction over a decade ago. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Michael Nagle/Bloomberg via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Anyone can now try Alexa+, Amazon’s generative AI assistant, through a free early access program at Alexa.com. The website frees the AI, which Amazon released via early access in February, from hardware and makes it as easily accessible as more established chatbots, like OpenAI’s ChatGPT and Google’s Gemini.&lt;/p&gt;
&lt;p&gt;Until today, you needed a supporting device to access Alexa+. Amazon hasn’t said when the early access period will end, but when it does, Alexa+ will be included with Amazon Prime memberships, which start at $15 per month, or cost $20 per month on its own.&lt;/p&gt;
&lt;p&gt;The above pricing suggests that Amazon wants Alexa+ to drive people toward Prime subscriptions. By being interwoven with Amazon’s shopping ecosystem, including Amazon’s e-commerce platform, grocery delivery business, and Whole Foods, Alexa+ can make more money for Amazon.&lt;/p&gt;
&lt;p&gt;Just like it has with Alexa+ on devices, Amazon is pushing Alexa.com as a tool for people to organize and manage their household. Amazon’s announcement of Alexa.com today emphasizes Alexa+’s features for planning trips and meals, to-do lists, calendars, and smart homes. Alexa.com “also provides persistent context and continuity, allowing you to access Alexa on whichever device or interface best serves the task at hand, with all previous chats, preferences, and personalization” carrying over, Amazon said.&lt;/p&gt;
&lt;div class="mceTemp"&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="Alexa.com website screenshot" class="ars-gallery-image" height="522" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Alex-1024x522.jpg" width="1024" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;The Alexa+ website’s homepage. &lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      Scharon Harding/Amazon
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="An example of someone using the Alexa+ website for shopping lists." class="ars-gallery-image" height="640" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Alexa-1024x640.jpg" width="1024" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Amazon provided this example of someone using the Alexa+ website for organizing lists. &lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      Amazon
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;The Alexa+ website’s homepage. &lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      Scharon Harding/Amazon
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Amazon provided this example of someone using the Alexa+ website for organizing lists. &lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      Amazon
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;Amazon already knew a browser-based version of Alexa would be helpful. Alexa was available via Alexa.Amazon.com until around the time Amazon started publicly discussing a generative AI version of Alexa in 2023. Alexa+ is now accessible through Alexa.Amazon.com (in addition to Alexa.com).&lt;/p&gt;
&lt;p&gt;“This is a new interaction model and adds a powerful way to use and collaborate with Alexa+,” Amazon said today. “Combined with the redesigned Alexa mobile app, which will feature an agent-forward design, Alexa+ will be accessible across every surface—whether you’re at your desk, on the go, or at home.”&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2133912 align-none"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="An example of someone using the Alexa+ website to manage smart home devices." class="none medium" height="400" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/smart-home-dark-640x400.jpg" width="640" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Amazon provided this example of someone using the Alexa+ website to manage smart home devices.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Amazon

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Alexa has largely been reported to cost Amazon billions of dollars, despite Amazon’s claim that 600 million Alexa-powered devices have been sold. By incorporating more powerful and generative AI-based features and a subscription fee, Amazon hopes people will use Alexa+ more frequently and for more advanced and essential tasks, resulting in the financial success that has eluded the original Alexa. Amazon is also considering injecting ads into Alexa+ conversations.&lt;/p&gt;
&lt;p&gt;Notably, ahead of its final release and while still in early access, Alexa+ has been reported to be slower than expected and struggle with inaccuracies at times. It also lacks some features that Amazon executives have previously touted, like the ability to order takeout.&lt;/p&gt;
&lt;/div&gt;


          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Amazon brings back browser-based Alexa but will eventually add a paywall.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Alexa+ logo with a person standing in front of it" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-2201505743-640x427.jpg" width="640" /&gt;
                  &lt;img alt="Alexa+ logo with a person standing in front of it" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-2201505743-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Alexa+ signage during an unveiling event in New York, US, on Wednesday, Feb. 26, 2025. Amazon has rebooted Alexa with artificial intelligence, marking the biggest overhaul of the voice-activated assistant since its introduction over a decade ago. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Michael Nagle/Bloomberg via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Anyone can now try Alexa+, Amazon’s generative AI assistant, through a free early access program at Alexa.com. The website frees the AI, which Amazon released via early access in February, from hardware and makes it as easily accessible as more established chatbots, like OpenAI’s ChatGPT and Google’s Gemini.&lt;/p&gt;
&lt;p&gt;Until today, you needed a supporting device to access Alexa+. Amazon hasn’t said when the early access period will end, but when it does, Alexa+ will be included with Amazon Prime memberships, which start at $15 per month, or cost $20 per month on its own.&lt;/p&gt;
&lt;p&gt;The above pricing suggests that Amazon wants Alexa+ to drive people toward Prime subscriptions. By being interwoven with Amazon’s shopping ecosystem, including Amazon’s e-commerce platform, grocery delivery business, and Whole Foods, Alexa+ can make more money for Amazon.&lt;/p&gt;
&lt;p&gt;Just like it has with Alexa+ on devices, Amazon is pushing Alexa.com as a tool for people to organize and manage their household. Amazon’s announcement of Alexa.com today emphasizes Alexa+’s features for planning trips and meals, to-do lists, calendars, and smart homes. Alexa.com “also provides persistent context and continuity, allowing you to access Alexa on whichever device or interface best serves the task at hand, with all previous chats, preferences, and personalization” carrying over, Amazon said.&lt;/p&gt;
&lt;div class="mceTemp"&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="Alexa.com website screenshot" class="ars-gallery-image" height="522" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Alex-1024x522.jpg" width="1024" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;The Alexa+ website’s homepage. &lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      Scharon Harding/Amazon
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="An example of someone using the Alexa+ website for shopping lists." class="ars-gallery-image" height="640" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Alexa-1024x640.jpg" width="1024" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Amazon provided this example of someone using the Alexa+ website for organizing lists. &lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      Amazon
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;The Alexa+ website’s homepage. &lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      Scharon Harding/Amazon
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Amazon provided this example of someone using the Alexa+ website for organizing lists. &lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      Amazon
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;Amazon already knew a browser-based version of Alexa would be helpful. Alexa was available via Alexa.Amazon.com until around the time Amazon started publicly discussing a generative AI version of Alexa in 2023. Alexa+ is now accessible through Alexa.Amazon.com (in addition to Alexa.com).&lt;/p&gt;
&lt;p&gt;“This is a new interaction model and adds a powerful way to use and collaborate with Alexa+,” Amazon said today. “Combined with the redesigned Alexa mobile app, which will feature an agent-forward design, Alexa+ will be accessible across every surface—whether you’re at your desk, on the go, or at home.”&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2133912 align-none"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="An example of someone using the Alexa+ website to manage smart home devices." class="none medium" height="400" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/smart-home-dark-640x400.jpg" width="640" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Amazon provided this example of someone using the Alexa+ website to manage smart home devices.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Amazon

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Alexa has largely been reported to cost Amazon billions of dollars, despite Amazon’s claim that 600 million Alexa-powered devices have been sold. By incorporating more powerful and generative AI-based features and a subscription fee, Amazon hopes people will use Alexa+ more frequently and for more advanced and essential tasks, resulting in the financial success that has eluded the original Alexa. Amazon is also considering injecting ads into Alexa+ conversations.&lt;/p&gt;
&lt;p&gt;Notably, ahead of its final release and while still in early access, Alexa+ has been reported to be slower than expected and struggle with inaccuracies at times. It also lacks some features that Amazon executives have previously touted, like the ability to order takeout.&lt;/p&gt;
&lt;/div&gt;


          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/gadgets/2026/01/amazon-alexa-released-to-the-general-public-via-an-early-access-website/</guid><pubDate>Mon, 05 Jan 2026 20:01:12 +0000</pubDate></item><item><title>[NEW] Using design to interpret the past and envision the future (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2026/using-design-interpret-past-envision-future-c-jacob-payne-0105</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/mit-architecture-Jacob-Payne.JPG" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Some of designer C Jacob Payne’s projects present new, futuristic products — such as zero-gravity footwear for astronauts, and electronic-embedded ceramics — using technological tools and processes of digital fabrication, material innovation, and interactive interfaces. Other projects travel back in time to past centuries, considering the challenge of preserving and reconstructing Black architectural heritage.&lt;/p&gt;&lt;p&gt;Payne graduated from Yale University with a bachelor’s degree in architecture and environmental studies, and then worked briefly at architecture firms in New York and Los Angeles. He decided to pursue a professional degree in order to become a licensed architect and to try out different types of design. He began the&amp;nbsp;MIT Master of Architecture (MArch) program in 2023, and is aiming to graduate in January 2027.&lt;/p&gt;&lt;p&gt;“I have especially valued the academic freedom to make my own path,” says Payne. “Although the MArch program requires certain classes each semester, I’ve been able to find a way to tailor the degree in a way that really reflects my interests.”&lt;/p&gt;&lt;p&gt;Payne says he appreciates how his experiences in the program have allowed him to work on design projects at a variety of scales — from the smaller scale in industrial and product design classes, to the larger scale in classes in the&amp;nbsp;Department of Urban Studies and Planning. He is a collaborator at the&amp;nbsp;Design Intelligence Lab and has served as a teaching assistant in MIT’s architecture wood shop, helping students to bring together digital design techniques with hands-on fabrication. Payne says he values the off-campus opportunities he has had, including working at a furniture and product design company in Barcelona through&amp;nbsp;MISTI and spending a summer working at the experience design firm 2x4 in New York.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Rediscovering the architecture of the past&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Through his graduate classes, Payne became especially interested in research into different types of vernacular architecture in America, especially in the American South. During his second semester, he took the class 4.182 (Brick x Brick: Drawing a Particular Survey), taught by Assistant Professor Carrie Norman, director of the architecture department’s undergraduate major and minor programs. As part of the curriculum, the class traveled to Tuskegee University to research the history and works of Robert R. Taylor, the first Black graduate of MIT (in 1892) and also the first licensed Black architect in America.&lt;/p&gt;&lt;p&gt;Following the class, Payne continued working on models and drawings reconstructing some important Tuskegee architecture. He created&amp;nbsp;models of Taylor’s original 1896 Tuskegee University Chapel, lost to fire in 1957, and the subsequent chapel built in its place in 1969, designed by Paul Rudolph in collaboration with Tuskegee University. He also produced a set of speculative drawings reconstructing Taylor’s 1896 chapel, using the very sparse remaining archival materials (including a few photographs and one drawing), the standards of the Historic American Buildings Survey, and inferred details.&lt;/p&gt;&lt;p&gt;“A lot of the work was figuring out how we can better understand and reconstruct historic spaces with very limited information,” says Payne. “I think it’s important to not treat the past as something static or fixed — because there’s so much that we don’t know, that has been unexplored.”&lt;/p&gt;&lt;p&gt;Payne received the 2025-26&amp;nbsp;L. Dennis Shapiro (1955) Graduate Fellowship in the History of African American Experience of Technology. He is currently looking into different typologies of architecture that were in the American South, with a particular focus on “juke joints,” structures that came about during the Jim Crow era. These were intended as secret social spaces for Black people to congregate, dance, sing, and play blues music — at a time when they were often barred from many establishments. Since there is very little documentation still remaining to use in this research, Payne says, the challenge is identifying which current techniques of architecture and design can be used to better understand and visualize these spaces.&lt;/p&gt;&lt;p&gt;“As his advisor, I have watched Jacob develop a body of work that treats architectural representation as both record and repair, recovering lost and overlooked Black-built traditions as vital expressions of Black spatial agency,” says Norman. “Through drawings, models, and speculative reconstructions, he expands the tools of the discipline to engage histories of cultural identity and heritage."&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Incorporating AI to design for the future&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;While much of Payne’s research is rooted in the past, he is also interested in artificial intelligence and its implications for future innovations. Last spring, he took the class 4.154 (Space Architecture) and learned how to design for the particular challenges of working in space. Along with his team,&amp;nbsp;he designed a footwear system for astronauts that could anchor to spacecraft structures with a mechanical, rotating sole, and inflatable bladders around the ankle for support.&lt;/p&gt;&lt;p&gt;In addition, Payne took a class about large language objects taught by associate professor of the practice Marcelo Coelho, director of the Design Intelligence Lab. “Designing products that integrate large language models involves thinking about how people can interact with AI in the physical world,” says Payne. “We are able create new experiences that challenge the ways that people think about how AI will look in the future.”&lt;/p&gt;&lt;p&gt;For the class, Payne and his team worked on a project using AI in the kitchen, developing a countertop device called the&amp;nbsp;Kitchen Cosmo. A camera at the top scans the ingredients placed in front of it. The user can input information such as how many people will be eating the meal and how much time is available to prepare the meal, and the device prints out a recipe.&lt;/p&gt;&lt;p&gt;Payne also worked on a project with Coelho for the Venice Biennale: a lamp that used geopolymers — a more sustainable alternative to concrete or other castable materials. Because this ceramic material doesn’t need to be fired in a kiln to harden, it can have electronics embedded within it. Payne now continues to work on AI research and product design in the Design Intelligence Lab.&lt;/p&gt;&lt;p&gt;“Jacob is an exceptional designer who deeply embodies MIT’s ‘mens et manus’ [‘mind and hand’] ethos by approaching product and interaction design with an exciting combination of intellectual rigor and high-quality, hands-on making,” says Coelho. “He is equally comfortable thinking conceptually about the cultural implications of artificial intelligence and working on the technical and craft detailing needed to bring his ideas to life.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/mit-architecture-Jacob-Payne.JPG" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Some of designer C Jacob Payne’s projects present new, futuristic products — such as zero-gravity footwear for astronauts, and electronic-embedded ceramics — using technological tools and processes of digital fabrication, material innovation, and interactive interfaces. Other projects travel back in time to past centuries, considering the challenge of preserving and reconstructing Black architectural heritage.&lt;/p&gt;&lt;p&gt;Payne graduated from Yale University with a bachelor’s degree in architecture and environmental studies, and then worked briefly at architecture firms in New York and Los Angeles. He decided to pursue a professional degree in order to become a licensed architect and to try out different types of design. He began the&amp;nbsp;MIT Master of Architecture (MArch) program in 2023, and is aiming to graduate in January 2027.&lt;/p&gt;&lt;p&gt;“I have especially valued the academic freedom to make my own path,” says Payne. “Although the MArch program requires certain classes each semester, I’ve been able to find a way to tailor the degree in a way that really reflects my interests.”&lt;/p&gt;&lt;p&gt;Payne says he appreciates how his experiences in the program have allowed him to work on design projects at a variety of scales — from the smaller scale in industrial and product design classes, to the larger scale in classes in the&amp;nbsp;Department of Urban Studies and Planning. He is a collaborator at the&amp;nbsp;Design Intelligence Lab and has served as a teaching assistant in MIT’s architecture wood shop, helping students to bring together digital design techniques with hands-on fabrication. Payne says he values the off-campus opportunities he has had, including working at a furniture and product design company in Barcelona through&amp;nbsp;MISTI and spending a summer working at the experience design firm 2x4 in New York.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Rediscovering the architecture of the past&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Through his graduate classes, Payne became especially interested in research into different types of vernacular architecture in America, especially in the American South. During his second semester, he took the class 4.182 (Brick x Brick: Drawing a Particular Survey), taught by Assistant Professor Carrie Norman, director of the architecture department’s undergraduate major and minor programs. As part of the curriculum, the class traveled to Tuskegee University to research the history and works of Robert R. Taylor, the first Black graduate of MIT (in 1892) and also the first licensed Black architect in America.&lt;/p&gt;&lt;p&gt;Following the class, Payne continued working on models and drawings reconstructing some important Tuskegee architecture. He created&amp;nbsp;models of Taylor’s original 1896 Tuskegee University Chapel, lost to fire in 1957, and the subsequent chapel built in its place in 1969, designed by Paul Rudolph in collaboration with Tuskegee University. He also produced a set of speculative drawings reconstructing Taylor’s 1896 chapel, using the very sparse remaining archival materials (including a few photographs and one drawing), the standards of the Historic American Buildings Survey, and inferred details.&lt;/p&gt;&lt;p&gt;“A lot of the work was figuring out how we can better understand and reconstruct historic spaces with very limited information,” says Payne. “I think it’s important to not treat the past as something static or fixed — because there’s so much that we don’t know, that has been unexplored.”&lt;/p&gt;&lt;p&gt;Payne received the 2025-26&amp;nbsp;L. Dennis Shapiro (1955) Graduate Fellowship in the History of African American Experience of Technology. He is currently looking into different typologies of architecture that were in the American South, with a particular focus on “juke joints,” structures that came about during the Jim Crow era. These were intended as secret social spaces for Black people to congregate, dance, sing, and play blues music — at a time when they were often barred from many establishments. Since there is very little documentation still remaining to use in this research, Payne says, the challenge is identifying which current techniques of architecture and design can be used to better understand and visualize these spaces.&lt;/p&gt;&lt;p&gt;“As his advisor, I have watched Jacob develop a body of work that treats architectural representation as both record and repair, recovering lost and overlooked Black-built traditions as vital expressions of Black spatial agency,” says Norman. “Through drawings, models, and speculative reconstructions, he expands the tools of the discipline to engage histories of cultural identity and heritage."&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Incorporating AI to design for the future&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;While much of Payne’s research is rooted in the past, he is also interested in artificial intelligence and its implications for future innovations. Last spring, he took the class 4.154 (Space Architecture) and learned how to design for the particular challenges of working in space. Along with his team,&amp;nbsp;he designed a footwear system for astronauts that could anchor to spacecraft structures with a mechanical, rotating sole, and inflatable bladders around the ankle for support.&lt;/p&gt;&lt;p&gt;In addition, Payne took a class about large language objects taught by associate professor of the practice Marcelo Coelho, director of the Design Intelligence Lab. “Designing products that integrate large language models involves thinking about how people can interact with AI in the physical world,” says Payne. “We are able create new experiences that challenge the ways that people think about how AI will look in the future.”&lt;/p&gt;&lt;p&gt;For the class, Payne and his team worked on a project using AI in the kitchen, developing a countertop device called the&amp;nbsp;Kitchen Cosmo. A camera at the top scans the ingredients placed in front of it. The user can input information such as how many people will be eating the meal and how much time is available to prepare the meal, and the device prints out a recipe.&lt;/p&gt;&lt;p&gt;Payne also worked on a project with Coelho for the Venice Biennale: a lamp that used geopolymers — a more sustainable alternative to concrete or other castable materials. Because this ceramic material doesn’t need to be fired in a kiln to harden, it can have electronics embedded within it. Payne now continues to work on AI research and product design in the Design Intelligence Lab.&lt;/p&gt;&lt;p&gt;“Jacob is an exceptional designer who deeply embodies MIT’s ‘mens et manus’ [‘mind and hand’] ethos by approaching product and interaction design with an exciting combination of intellectual rigor and high-quality, hands-on making,” says Coelho. “He is equally comfortable thinking conceptually about the cultural implications of artificial intelligence and working on the technical and craft detailing needed to bring his ideas to life.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2026/using-design-interpret-past-envision-future-c-jacob-payne-0105</guid><pubDate>Mon, 05 Jan 2026 20:25:00 +0000</pubDate></item><item><title>[NEW] Nvidia launches Alpamayo, open AI models that allow autonomous vehicles to ‘think like a human’ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/05/nvidia-launches-alpamayo-open-ai-models-that-allow-autonomous-vehicles-to-think-like-a-human/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/Alpamayo-Image.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;At CES 2026, Nvidia launched Alpamayo, a new family of open source AI models, simulation tools, and datasets for training physical robots and vehicles that are designed to help autonomous vehicles reason through complex driving situations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The ChatGPT moment for physical AI is here – when machines begin to understand, reason, and act in the real world,” Nvidia CEO Jensen Huang said in a statement. “Alpamayo brings reasoning to autonomous vehicles, allowing them to think through rare scenarios, drive safely in complex environments, and explain their driving decisions.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;At the core of Nvidia’s new family is Alpamayo 1, a 10 billion-parameter chain-of-thought, reason-based vision language action (VLA) model that allows an AV to think more like a human so it can solve complex edge cases — like how to navigate a traffic light outage at a busy intersection — without previous experience.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It does this by breaking down problems into steps, reasoning through every possibility, and then selecting the safest path,” Ali Kani, Nvidia’s vice president of automotive, said Monday during a press briefing.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Or as Huang put it during his keynote on Monday: “Not only does [Alpamayo] take sensor input and activate steering wheel, brakes, and acceleration, it also reasons about what action it’s about to take. It tells you what action it’s going to take, the reasons by which it came about that action. And then, of course, the trajectory.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alpamayo 1’s underlying code is available on Hugging Face. Developers can fine-tune Alpamayo into smaller, faster versions for vehicle development, use it to train simpler driving systems, or build tools on top of it like auto-labeling systems that automatically tag video data or evaluators that check if a car made a smart decision.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“They can also use Cosmos to generate synthetic data and then train and test their Alpamayo-based AV application on the combination of the real and synthetic dataset,” Kani said. Cosmos is Nvidia’s brand of generative world models, AI systems that create a representation of a physical environment so they can make predictions and take actions.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;As part of the Alpamayo rollout, Nvidia is also releasing an open dataset with more than 1,700 hours of driving data collected across a range of geographies and conditions, covering rare and complex real-world scenarios. The company is additionally launching AlpaSim, an open source simulation framework for validating autonomous driving systems. Available on GitHub, AlpaSim is designed to recreate real-world driving conditions, from sensors to traffic, so developers can safely test systems at scale.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/Alpamayo-Image.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;At CES 2026, Nvidia launched Alpamayo, a new family of open source AI models, simulation tools, and datasets for training physical robots and vehicles that are designed to help autonomous vehicles reason through complex driving situations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The ChatGPT moment for physical AI is here – when machines begin to understand, reason, and act in the real world,” Nvidia CEO Jensen Huang said in a statement. “Alpamayo brings reasoning to autonomous vehicles, allowing them to think through rare scenarios, drive safely in complex environments, and explain their driving decisions.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;At the core of Nvidia’s new family is Alpamayo 1, a 10 billion-parameter chain-of-thought, reason-based vision language action (VLA) model that allows an AV to think more like a human so it can solve complex edge cases — like how to navigate a traffic light outage at a busy intersection — without previous experience.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It does this by breaking down problems into steps, reasoning through every possibility, and then selecting the safest path,” Ali Kani, Nvidia’s vice president of automotive, said Monday during a press briefing.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Or as Huang put it during his keynote on Monday: “Not only does [Alpamayo] take sensor input and activate steering wheel, brakes, and acceleration, it also reasons about what action it’s about to take. It tells you what action it’s going to take, the reasons by which it came about that action. And then, of course, the trajectory.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alpamayo 1’s underlying code is available on Hugging Face. Developers can fine-tune Alpamayo into smaller, faster versions for vehicle development, use it to train simpler driving systems, or build tools on top of it like auto-labeling systems that automatically tag video data or evaluators that check if a car made a smart decision.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“They can also use Cosmos to generate synthetic data and then train and test their Alpamayo-based AV application on the combination of the real and synthetic dataset,” Kani said. Cosmos is Nvidia’s brand of generative world models, AI systems that create a representation of a physical environment so they can make predictions and take actions.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;As part of the Alpamayo rollout, Nvidia is also releasing an open dataset with more than 1,700 hours of driving data collected across a range of geographies and conditions, covering rare and complex real-world scenarios. The company is additionally launching AlpaSim, an open source simulation framework for validating autonomous driving systems. Available on GitHub, AlpaSim is designed to recreate real-world driving conditions, from sensors to traffic, so developers can safely test systems at scale.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/05/nvidia-launches-alpamayo-open-ai-models-that-allow-autonomous-vehicles-to-think-like-a-human/</guid><pubDate>Mon, 05 Jan 2026 21:52:22 +0000</pubDate></item><item><title>[NEW] MIT scientists investigate memorization risk in the age of clinical AI (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2026/mit-scientists-investigate-memorization-risk-clinical-ai-0105</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/mit-j-clinic-EHR-Memorization.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;What is patient privacy for? The Hippocratic Oath, thought to be one of the earliest and most widely known medical ethics texts in the world, reads: “Whatever I see or hear in the lives of my patients, whether in connection with my professional practice or not, which ought not to be spoken of outside, I will keep secret, as considering all such things to be private.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;As privacy becomes increasingly scarce in the age of data-hungry algorithms and cyberattacks, medicine is one of the few remaining domains where confidentiality remains central to practice, enabling patients to trust their physicians with sensitive information.&lt;/p&gt;&lt;p dir="ltr"&gt;But&amp;nbsp;a paper co-authored by MIT researchers investigates how artificial intelligence models trained on de-identified electronic health records (EHRs) can memorize patient-specific information. The work, which was recently presented at the 2025 Conference on Neural Information Processing Systems (NeurIPS), recommends a rigorous testing setup to ensure targeted prompts cannot reveal information, emphasizing that leakage must be evaluated in a health care context to determine whether it meaningfully compromises patient privacy.&lt;/p&gt;&lt;p dir="ltr"&gt;Foundation models trained on EHRs should normally generalize knowledge to make better predictions, drawing upon many patient records. But in “memorization,” the model draws upon a singular patient record to deliver its output, potentially violating patient privacy. Notably, foundation models are already known to be&amp;nbsp;prone to data leakage.&lt;/p&gt;&lt;p dir="ltr"&gt;“Knowledge in these high-capacity models can be a resource for many communities, but adversarial attackers can prompt a model to extract information on training data,” says Sana Tonekaboni, a postdoc at the Eric and Wendy Schmidt Center at the Broad Institute of MIT and Harvard and first author of the paper. Given the risk that foundation models could also memorize private data, she notes, “this work is a step towards ensuring there are practical evaluation steps our community can take before releasing models.”&lt;/p&gt;&lt;p dir="ltr"&gt;To conduct research on the potential risk EHR foundation models could pose in medicine, Tonekaboni approached MIT Associate Professor&amp;nbsp;Marzyeh Ghassemi, who is a principal investigator at the Abdul Latif Jameel Clinic for Machine Learning in Health (Jameel Clinic), a member of the Computer Science and Artificial Intelligence Lab. Ghassemi, a faculty member in the MIT Department of Electrical Engineering and Computer Science and Institute for Medical Engineering and Science, runs the&amp;nbsp;Healthy ML group, which focuses on robust machine learning in health.&lt;/p&gt;&lt;p dir="ltr"&gt;Just how much information does a bad actor need to expose sensitive data, and what are the risks associated with the leaked information? To assess this, the research team developed a series of tests that they hope will lay the groundwork for future privacy evaluations. These tests are designed to measure various types of uncertainty, and assess their practical risk to patients by measuring various tiers of attack possibility.&amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“We really tried to emphasize practicality here; if an attacker has to know the date and value of a dozen laboratory tests from your record in order to extract information, there is very little risk of harm. If I already have access to that level of protected source data, why would I need to attack a large foundation model for more?” says Ghassemi.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;With the inevitable digitization of medical records, data breaches have become more commonplace. In the past 24 months, the&amp;nbsp;U.S. Department of Health and Human Services has recorded 747 data breaches of health information affecting more than 500 individuals, with the majority categorized as hacking/IT incidents.&lt;/p&gt;&lt;p dir="ltr"&gt;Patients with unique conditions are especially vulnerable, given how easy it is to pick them out. “Even with de-identified data, it depends on what sort of information you leak about the individual,” Tonekaboni says. “Once you identify them, you know a lot more.”&lt;/p&gt;&lt;p dir="ltr"&gt;In their structured tests, the researchers found that the more information the attacker has about a particular patient, the more likely the model is to leak information. They demonstrated how to distinguish model generalization cases from patient-level memorization, to properly assess privacy risk.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;The paper also emphasized that some leaks are more harmful than others. For instance, a model revealing a patient’s age or demographics could be characterized as a more benign leakage than the model revealing more sensitive information, like an HIV diagnosis or alcohol abuse.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers note that patients with unique conditions are especially vulnerable given how easy it is to pick them out, which may require higher levels of protection. “Even with de-identified data, it really depends on what sort of information you leak about the individual,” Tonekaboni says. The researchers plan to expand the work to become more interdisciplinary, adding clinicians and privacy experts as well as legal experts.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“There’s a reason our health data is private,” Tonekaboni says. “There’s no reason for others to know about it.”&lt;/p&gt;&lt;p dir="ltr"&gt;This work supported by the Eric and Wendy Schmidt Center at the Broad Institute of MIT and Harvard, Wallenberg AI, the Knut and Alice Wallenberg Foundation, the U.S. National Science Foundation (NSF), a Gordon and Betty Moore Foundation award, a Google Research Scholar award, and the AI2050 Program at Schmidt Sciences. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/mit-j-clinic-EHR-Memorization.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;What is patient privacy for? The Hippocratic Oath, thought to be one of the earliest and most widely known medical ethics texts in the world, reads: “Whatever I see or hear in the lives of my patients, whether in connection with my professional practice or not, which ought not to be spoken of outside, I will keep secret, as considering all such things to be private.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;As privacy becomes increasingly scarce in the age of data-hungry algorithms and cyberattacks, medicine is one of the few remaining domains where confidentiality remains central to practice, enabling patients to trust their physicians with sensitive information.&lt;/p&gt;&lt;p dir="ltr"&gt;But&amp;nbsp;a paper co-authored by MIT researchers investigates how artificial intelligence models trained on de-identified electronic health records (EHRs) can memorize patient-specific information. The work, which was recently presented at the 2025 Conference on Neural Information Processing Systems (NeurIPS), recommends a rigorous testing setup to ensure targeted prompts cannot reveal information, emphasizing that leakage must be evaluated in a health care context to determine whether it meaningfully compromises patient privacy.&lt;/p&gt;&lt;p dir="ltr"&gt;Foundation models trained on EHRs should normally generalize knowledge to make better predictions, drawing upon many patient records. But in “memorization,” the model draws upon a singular patient record to deliver its output, potentially violating patient privacy. Notably, foundation models are already known to be&amp;nbsp;prone to data leakage.&lt;/p&gt;&lt;p dir="ltr"&gt;“Knowledge in these high-capacity models can be a resource for many communities, but adversarial attackers can prompt a model to extract information on training data,” says Sana Tonekaboni, a postdoc at the Eric and Wendy Schmidt Center at the Broad Institute of MIT and Harvard and first author of the paper. Given the risk that foundation models could also memorize private data, she notes, “this work is a step towards ensuring there are practical evaluation steps our community can take before releasing models.”&lt;/p&gt;&lt;p dir="ltr"&gt;To conduct research on the potential risk EHR foundation models could pose in medicine, Tonekaboni approached MIT Associate Professor&amp;nbsp;Marzyeh Ghassemi, who is a principal investigator at the Abdul Latif Jameel Clinic for Machine Learning in Health (Jameel Clinic), a member of the Computer Science and Artificial Intelligence Lab. Ghassemi, a faculty member in the MIT Department of Electrical Engineering and Computer Science and Institute for Medical Engineering and Science, runs the&amp;nbsp;Healthy ML group, which focuses on robust machine learning in health.&lt;/p&gt;&lt;p dir="ltr"&gt;Just how much information does a bad actor need to expose sensitive data, and what are the risks associated with the leaked information? To assess this, the research team developed a series of tests that they hope will lay the groundwork for future privacy evaluations. These tests are designed to measure various types of uncertainty, and assess their practical risk to patients by measuring various tiers of attack possibility.&amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“We really tried to emphasize practicality here; if an attacker has to know the date and value of a dozen laboratory tests from your record in order to extract information, there is very little risk of harm. If I already have access to that level of protected source data, why would I need to attack a large foundation model for more?” says Ghassemi.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;With the inevitable digitization of medical records, data breaches have become more commonplace. In the past 24 months, the&amp;nbsp;U.S. Department of Health and Human Services has recorded 747 data breaches of health information affecting more than 500 individuals, with the majority categorized as hacking/IT incidents.&lt;/p&gt;&lt;p dir="ltr"&gt;Patients with unique conditions are especially vulnerable, given how easy it is to pick them out. “Even with de-identified data, it depends on what sort of information you leak about the individual,” Tonekaboni says. “Once you identify them, you know a lot more.”&lt;/p&gt;&lt;p dir="ltr"&gt;In their structured tests, the researchers found that the more information the attacker has about a particular patient, the more likely the model is to leak information. They demonstrated how to distinguish model generalization cases from patient-level memorization, to properly assess privacy risk.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;The paper also emphasized that some leaks are more harmful than others. For instance, a model revealing a patient’s age or demographics could be characterized as a more benign leakage than the model revealing more sensitive information, like an HIV diagnosis or alcohol abuse.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers note that patients with unique conditions are especially vulnerable given how easy it is to pick them out, which may require higher levels of protection. “Even with de-identified data, it really depends on what sort of information you leak about the individual,” Tonekaboni says. The researchers plan to expand the work to become more interdisciplinary, adding clinicians and privacy experts as well as legal experts.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“There’s a reason our health data is private,” Tonekaboni says. “There’s no reason for others to know about it.”&lt;/p&gt;&lt;p dir="ltr"&gt;This work supported by the Eric and Wendy Schmidt Center at the Broad Institute of MIT and Harvard, Wallenberg AI, the Knut and Alice Wallenberg Foundation, the U.S. National Science Foundation (NSF), a Gordon and Betty Moore Foundation award, a Google Research Scholar award, and the AI2050 Program at Schmidt Sciences. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2026/mit-scientists-investigate-memorization-risk-clinical-ai-0105</guid><pubDate>Mon, 05 Jan 2026 21:55:00 +0000</pubDate></item><item><title>[NEW] Nvidia launches powerful new Rubin chip architecture (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/05/nvidia-launches-powerful-new-rubin-chip-architecture/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2219673294.jpg?resize=1200,750" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Today at the Consumer Electronics Show, Nvidia CEO Jensen Huang officially launched the company’s new Rubin computing architecture, which he described as the state of the art in AI hardware. The new architecture is currently in production and is expected to ramp up further in the second half of the year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Vera Rubin is designed to address this fundamental challenge that we have: The amount of computation necessary for AI is skyrocketing.” Huang told the audience. “Today, I can tell you that Vera Rubin is in full production.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Rubin architecture, which was first announced in 2024, is the latest result of Nvidia’s relentless hardware development cycle, which has transformed Nvidia into the most valuable corporation in the world. The Rubin architecture will replace the Blackwell architecture, which in turn, replaced the Hopper and Lovelace architectures.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-4-3 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Rubin chips are already slated for use by nearly every major cloud provider, including high-profile Nvidia partnerships with Anthropic, OpenAI, and Amazon Web Services. Rubin systems will also be used in HPE’s Blue Lion supercomputer and the upcoming Doudna supercomputer at Lawrence Berkeley National Lab.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Named for the astronomer Vera Florence Cooper Rubin, the Rubin architecture consists of six separate chips designed to be used in concert. The Rubin GPU stands at the center, but the architecture also addresses growing bottlenecks in storage and interconnection with new improvements in the Bluefield and NVLink, systems respectively. The architecture also includes a new Vera CPU, designed for agentic reasoning.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Explaining the benefits of the new storage, Nvidia’s senior director of AI infrastructure solutions Dion Harris pointed to the growing cache-related memory demands of modern AI systems. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As you start to enable new types of workflows, like agentic AI or long-term tasks, that puts a lot of stress and requirements on your KV cache,” Harris told reporters on a call, referring to a memory system used by AI models to condense inputs. “So we’ve introduced a new tier of storage that connects externally to the compute device, which allows you to scale your storage pool much more efficiently.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;As expected, the new architecture also represents a significant advance in speed and power efficiency. According to Nvidia’s tests, the Rubin architecture will operate three and a half times faster than the previous Blackwell architecture on model-training tasks and five times faster on inference tasks, reaching as high as 50 petaflops. The new platform will also support eight times more inference compute per watt.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rubin’s new capabilities come amid intense competition to build AI infrastructure, which has seen both AI labs and cloud providers scramble for Nvidia chips as well as the facilities necessary to power them. On an earnings call in October 2025, Huang estimated that between $3 trillion and $4 trillion will be spent on AI infrastructure over the next five years.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Follow along with all of TechCrunch’s coverage of the annual CES conference here.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Watch Nvidia CEO Jensen Huang reveal what he described as the state of the art in AI hardware: the new Rubin computing architecture.&lt;/p&gt;&lt;p&gt;“Vera Rubin is designed to address this fundamental challenge that we have: The amount of computation necessary for AI is skyrocketing.” Huang… pic.twitter.com/MhGVqytX04&lt;/p&gt;— TechCrunch (@TechCrunch) January 5, 2026&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2219673294.jpg?resize=1200,750" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Today at the Consumer Electronics Show, Nvidia CEO Jensen Huang officially launched the company’s new Rubin computing architecture, which he described as the state of the art in AI hardware. The new architecture is currently in production and is expected to ramp up further in the second half of the year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Vera Rubin is designed to address this fundamental challenge that we have: The amount of computation necessary for AI is skyrocketing.” Huang told the audience. “Today, I can tell you that Vera Rubin is in full production.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Rubin architecture, which was first announced in 2024, is the latest result of Nvidia’s relentless hardware development cycle, which has transformed Nvidia into the most valuable corporation in the world. The Rubin architecture will replace the Blackwell architecture, which in turn, replaced the Hopper and Lovelace architectures.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-4-3 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Rubin chips are already slated for use by nearly every major cloud provider, including high-profile Nvidia partnerships with Anthropic, OpenAI, and Amazon Web Services. Rubin systems will also be used in HPE’s Blue Lion supercomputer and the upcoming Doudna supercomputer at Lawrence Berkeley National Lab.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Named for the astronomer Vera Florence Cooper Rubin, the Rubin architecture consists of six separate chips designed to be used in concert. The Rubin GPU stands at the center, but the architecture also addresses growing bottlenecks in storage and interconnection with new improvements in the Bluefield and NVLink, systems respectively. The architecture also includes a new Vera CPU, designed for agentic reasoning.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Explaining the benefits of the new storage, Nvidia’s senior director of AI infrastructure solutions Dion Harris pointed to the growing cache-related memory demands of modern AI systems. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As you start to enable new types of workflows, like agentic AI or long-term tasks, that puts a lot of stress and requirements on your KV cache,” Harris told reporters on a call, referring to a memory system used by AI models to condense inputs. “So we’ve introduced a new tier of storage that connects externally to the compute device, which allows you to scale your storage pool much more efficiently.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;As expected, the new architecture also represents a significant advance in speed and power efficiency. According to Nvidia’s tests, the Rubin architecture will operate three and a half times faster than the previous Blackwell architecture on model-training tasks and five times faster on inference tasks, reaching as high as 50 petaflops. The new platform will also support eight times more inference compute per watt.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rubin’s new capabilities come amid intense competition to build AI infrastructure, which has seen both AI labs and cloud providers scramble for Nvidia chips as well as the facilities necessary to power them. On an earnings call in October 2025, Huang estimated that between $3 trillion and $4 trillion will be spent on AI infrastructure over the next five years.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Follow along with all of TechCrunch’s coverage of the annual CES conference here.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Watch Nvidia CEO Jensen Huang reveal what he described as the state of the art in AI hardware: the new Rubin computing architecture.&lt;/p&gt;&lt;p&gt;“Vera Rubin is designed to address this fundamental challenge that we have: The amount of computation necessary for AI is skyrocketing.” Huang… pic.twitter.com/MhGVqytX04&lt;/p&gt;— TechCrunch (@TechCrunch) January 5, 2026&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/05/nvidia-launches-powerful-new-rubin-chip-architecture/</guid><pubDate>Mon, 05 Jan 2026 22:16:58 +0000</pubDate></item><item><title>[NEW] NVIDIA Cosmos Reason 2 Brings Advanced Reasoning To Physical AI (Hugging Face - Blog)</title><link>https://huggingface.co/blog/nvidia/nvidia-cosmos-reason-2-brings-advanced-reasoning</link><description>&lt;!-- HTML_TAG_START --&gt;
&lt;video class="max-w-full!" controls="controls" src="https://cdn-uploads.huggingface.co/production/uploads/63b738acbd2d1535227daa4c/lX5SpKaxbeKnnd5pxAh0s.mp4"&gt;&lt;/video&gt;

&lt;p&gt;NVIDIA today released Cosmos Reason 2, the latest advancement in open, reasoning vision language models for physical AI. Cosmos Reason 2 surpasses its previous version in accuracy and tops the Physical AI Bench and Physical Reasoning leaderboards as the #1 open model for visual understanding.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		NVIDIA Cosmos Reason 2: Reasoning Vision Language Model for Physical AI
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Since their introduction, vision-language models have rapidly improved at tasks like object and pattern recognition in images. But they still struggle with tasks humans find natural, like planning several steps ahead, dealing with uncertainty or adapting to new situations. Cosmos Reason is designed to close this gap by giving robots and AI agents stronger common sense and reasoning to solve complex problems step by step.&lt;/p&gt;
&lt;p&gt;Cosmos Reason 2 is a state-of-the-art, open reasoning vision-language model (VLM) that enables robots and AI agents to see, understand, plan, and act in the physical world like humans. It uses common sense, physics, and prior knowledge to recognize how objects move across space and time to handle complex tasks, adapt to new situations, and figure out how to solve problems step by step. &lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		✨ Key Highlights
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Improved spatio-temporal understanding and timestamp precision.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Optimized performance with flexible deployment options from edge to cloud with 2B and 8B parameters model sizes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Support for expanded set of spatial understanding and visual perception capabilities — 2D/3D point localization, bounding box coordinates, trajectory data, and OCR support. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Improved long-context understanding with 256K input tokens, up from 16K with Cosmos Reason 1.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Adaptable to multiple use cases with easy-to-use Cosmos Cookbook recipes.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		🤖 Popular Use Cases
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Video analytics AI agents&lt;/strong&gt; — These agents can extract valuable insights from massive volumes of video data to optimize processes. Cosmos Reason 2 builds on the capabilities of Cosmos Reason 1 and now provides OCR support, as well as 2D/3D point localization and a set of mark understanding.  &lt;/p&gt;
&lt;figure&gt;
  &lt;video class="max-w-full!" controls="controls" src="https://cdn-uploads.huggingface.co/production/uploads/63b738acbd2d1535227daa4c/aDGQNFfUigMTZwUkImNcf.mp4"&gt;&lt;/video&gt;
  &lt;figcaption align="center"&gt;Example of how Cosmos Reason can understand text embedded within a video to determine the condition of the road during a rainstorm. &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Developers can jumpstart development of video analytics AI agents by using the NVIDIA blueprint for video search and summarization (VSS) with Cosmos Reason as the VLM.&lt;/p&gt;
&lt;p&gt;Salesforce is transforming workplace safety and compliance by analyzing video footage captured by Cobalt robots with Agentforce and VSS blueprint with Cosmos Reason as the VLM.  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Data annotation and critique&lt;/strong&gt; — Enable developers to automate high-quality annotation and critique of massive, diverse training datasets. Cosmos Reason provides time stamps and detailed descriptions for real or synthetically generated training videos. &lt;/p&gt;
&lt;figure&gt;
  &lt;img alt="Data annotation and critique example" src="https://cdn-uploads.huggingface.co/production/uploads/63b738acbd2d1535227daa4c/N1iod6-BikvBPFe9LUoi5.png" /&gt;
  &lt;figcaption align="center"&gt;Example of a sample prompt to generate detailed, time-stamped captions for a race car video.&lt;/figcaption&gt;
&lt;/figure&gt;


&lt;p&gt;Uber is exploring Cosmos Reason 2 to deliver accurate, searchable video captions for autonomous vehicle (AV) training data, enabling efficient identification of critical driving scenarios. This co-authored Reason 2 for AV Video Captioning and VQA recipe demonstrates how to fine-tune and evaluate Cosmos Reason 2-8B on annotated AV videos. Across multiple evaluation metrics, measurable improvements were achieved: BLEU scores improved 10.6% (0.113 → 0.125), MCQ-based VQA gained 0.67 percentage points (80.18% → 80.85%), and LingoQA increased 13.8% (63.2% → 77.0%). These gains demonstrate effective domain adaptation for AV applications.  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Robot planning and reasoning&lt;/strong&gt; — Act as the brain for deliberate, methodical decision-making in a robot vision language action (VLA) model. Cosmos Reason 2 now provides trajectory coordinates in addition to determining next steps. &lt;/p&gt;
  &lt;figure&gt;
    &lt;video class="max-w-full!" controls="controls" src="https://cdn-uploads.huggingface.co/production/uploads/63b738acbd2d1535227daa4c/1J--ZPp_uSZ8ViKrqwntU.mp4"&gt;&lt;/video&gt;
    &lt;figcaption align="center"&gt;Example of the prompt and JSON output from Cosmos Reason 2 to provide the steps and trajectory the robot gripper needs to take to move the painter’s tape into the basket.&lt;/figcaption&gt;
  &lt;/figure&gt;

&lt;p&gt;Encord provides native support for Cosmos Reason 2 in its Data Agent library and AI data platform, enabling developers to leverage Cosmos Reason 2 as a VLA for robotics and other physical AI use cases.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Companies like Hitachi, Milestone and VAST Data are using Cosmos Reason to advance robotics, autonomous driving, and video analytics AI agents for traffic and workplace safety.&lt;/p&gt;
&lt;p&gt;Try Cosmos Reason 2 on build.nvidia.com and experience the latest features with sample prompts for generating bounding boxes and robot trajectories. Upload your own videos and images for further analysis. &lt;/p&gt;
&lt;p&gt;Download Cosmos Reason 2 models (2B and 8B) on Hugging Face or use Cosmos Reason 2 in the cloud. The model will be available soon on Amazon Web Services, Google Cloud and Microsoft Azure. To get started, check out Cosmos Reason 2 documentation and the Cosmos Cookbook.  &lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Other Models From The Cosmos Family:
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;🔮 Cosmos Predict 2.5&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Cosmos Predict is a generative AI model that predicts future states of the physical world as video, based on text, image, or video inputs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Physical AI Bench leader for quality, accuracy and overall consistency.  &lt;/li&gt;
&lt;li&gt;Up to 30 seconds of physically and temporally consistent clip per generation.   &lt;/li&gt;
&lt;li&gt;Supports multiple framerates and resolution.  &lt;/li&gt;
&lt;li&gt;Pre-trained on 200 million clips.   &lt;/li&gt;
&lt;li&gt;Available as 2B and 14B pre-trained models and various 2B post-trained models for multiview, action conditioning  and autonomous vehicle training.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Check out model card&amp;gt;&amp;gt;&lt;/p&gt;
&lt;p&gt; &lt;strong&gt;🔁 Cosmos Transfer 2.5&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Cosmos Transfer is our lightest multicontrol model built for video to world style transfer. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Scale a single simulation or spatial video across various environments and lighting conditions.  &lt;/li&gt;
&lt;li&gt;Improved prompt adherence and physics alignment.  &lt;/li&gt;
&lt;li&gt;Use with NVIDIA Isaac Sim™ or NVIDIA Omniverse NuRec for simulation to real transformation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Check out model card&amp;gt;&amp;gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;🤖 NVIDIA GR00T N1.6&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NVIDIA GR00T N1.6 is an open reasoning vision language action (VLA) model, purpose-built for humanoid robots, that unlocks full body control and uses NVIDIA Cosmos Reason for better reasoning and contextual understanding.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Resources
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;🧑🏻‍🍳 Read the Cosmos Cookbook → https://nvda.ws/4qevli8&lt;/p&gt;
&lt;p&gt;📚 Explore Models &amp;amp; Datasets → https://github.com/nvidia-cosmos&lt;/p&gt;
&lt;p&gt;⬇️ Try Cosmos Models in our Hosted Catalog → https://nvda.ws/3Yg0Dcx&lt;/p&gt;
&lt;p&gt;💻 Join the Cosmos Community → https://discord.gg/u23rXTHSC9&lt;/p&gt;
&lt;p&gt;🗳️ Contribute to the Cosmos Cookbook → https://nvda.ws/4aQcBkk&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;!-- HTML_TAG_START --&gt;
&lt;video class="max-w-full!" controls="controls" src="https://cdn-uploads.huggingface.co/production/uploads/63b738acbd2d1535227daa4c/lX5SpKaxbeKnnd5pxAh0s.mp4"&gt;&lt;/video&gt;

&lt;p&gt;NVIDIA today released Cosmos Reason 2, the latest advancement in open, reasoning vision language models for physical AI. Cosmos Reason 2 surpasses its previous version in accuracy and tops the Physical AI Bench and Physical Reasoning leaderboards as the #1 open model for visual understanding.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		NVIDIA Cosmos Reason 2: Reasoning Vision Language Model for Physical AI
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Since their introduction, vision-language models have rapidly improved at tasks like object and pattern recognition in images. But they still struggle with tasks humans find natural, like planning several steps ahead, dealing with uncertainty or adapting to new situations. Cosmos Reason is designed to close this gap by giving robots and AI agents stronger common sense and reasoning to solve complex problems step by step.&lt;/p&gt;
&lt;p&gt;Cosmos Reason 2 is a state-of-the-art, open reasoning vision-language model (VLM) that enables robots and AI agents to see, understand, plan, and act in the physical world like humans. It uses common sense, physics, and prior knowledge to recognize how objects move across space and time to handle complex tasks, adapt to new situations, and figure out how to solve problems step by step. &lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		✨ Key Highlights
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Improved spatio-temporal understanding and timestamp precision.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Optimized performance with flexible deployment options from edge to cloud with 2B and 8B parameters model sizes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Support for expanded set of spatial understanding and visual perception capabilities — 2D/3D point localization, bounding box coordinates, trajectory data, and OCR support. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Improved long-context understanding with 256K input tokens, up from 16K with Cosmos Reason 1.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Adaptable to multiple use cases with easy-to-use Cosmos Cookbook recipes.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		🤖 Popular Use Cases
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Video analytics AI agents&lt;/strong&gt; — These agents can extract valuable insights from massive volumes of video data to optimize processes. Cosmos Reason 2 builds on the capabilities of Cosmos Reason 1 and now provides OCR support, as well as 2D/3D point localization and a set of mark understanding.  &lt;/p&gt;
&lt;figure&gt;
  &lt;video class="max-w-full!" controls="controls" src="https://cdn-uploads.huggingface.co/production/uploads/63b738acbd2d1535227daa4c/aDGQNFfUigMTZwUkImNcf.mp4"&gt;&lt;/video&gt;
  &lt;figcaption align="center"&gt;Example of how Cosmos Reason can understand text embedded within a video to determine the condition of the road during a rainstorm. &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Developers can jumpstart development of video analytics AI agents by using the NVIDIA blueprint for video search and summarization (VSS) with Cosmos Reason as the VLM.&lt;/p&gt;
&lt;p&gt;Salesforce is transforming workplace safety and compliance by analyzing video footage captured by Cobalt robots with Agentforce and VSS blueprint with Cosmos Reason as the VLM.  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Data annotation and critique&lt;/strong&gt; — Enable developers to automate high-quality annotation and critique of massive, diverse training datasets. Cosmos Reason provides time stamps and detailed descriptions for real or synthetically generated training videos. &lt;/p&gt;
&lt;figure&gt;
  &lt;img alt="Data annotation and critique example" src="https://cdn-uploads.huggingface.co/production/uploads/63b738acbd2d1535227daa4c/N1iod6-BikvBPFe9LUoi5.png" /&gt;
  &lt;figcaption align="center"&gt;Example of a sample prompt to generate detailed, time-stamped captions for a race car video.&lt;/figcaption&gt;
&lt;/figure&gt;


&lt;p&gt;Uber is exploring Cosmos Reason 2 to deliver accurate, searchable video captions for autonomous vehicle (AV) training data, enabling efficient identification of critical driving scenarios. This co-authored Reason 2 for AV Video Captioning and VQA recipe demonstrates how to fine-tune and evaluate Cosmos Reason 2-8B on annotated AV videos. Across multiple evaluation metrics, measurable improvements were achieved: BLEU scores improved 10.6% (0.113 → 0.125), MCQ-based VQA gained 0.67 percentage points (80.18% → 80.85%), and LingoQA increased 13.8% (63.2% → 77.0%). These gains demonstrate effective domain adaptation for AV applications.  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Robot planning and reasoning&lt;/strong&gt; — Act as the brain for deliberate, methodical decision-making in a robot vision language action (VLA) model. Cosmos Reason 2 now provides trajectory coordinates in addition to determining next steps. &lt;/p&gt;
  &lt;figure&gt;
    &lt;video class="max-w-full!" controls="controls" src="https://cdn-uploads.huggingface.co/production/uploads/63b738acbd2d1535227daa4c/1J--ZPp_uSZ8ViKrqwntU.mp4"&gt;&lt;/video&gt;
    &lt;figcaption align="center"&gt;Example of the prompt and JSON output from Cosmos Reason 2 to provide the steps and trajectory the robot gripper needs to take to move the painter’s tape into the basket.&lt;/figcaption&gt;
  &lt;/figure&gt;

&lt;p&gt;Encord provides native support for Cosmos Reason 2 in its Data Agent library and AI data platform, enabling developers to leverage Cosmos Reason 2 as a VLA for robotics and other physical AI use cases.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Companies like Hitachi, Milestone and VAST Data are using Cosmos Reason to advance robotics, autonomous driving, and video analytics AI agents for traffic and workplace safety.&lt;/p&gt;
&lt;p&gt;Try Cosmos Reason 2 on build.nvidia.com and experience the latest features with sample prompts for generating bounding boxes and robot trajectories. Upload your own videos and images for further analysis. &lt;/p&gt;
&lt;p&gt;Download Cosmos Reason 2 models (2B and 8B) on Hugging Face or use Cosmos Reason 2 in the cloud. The model will be available soon on Amazon Web Services, Google Cloud and Microsoft Azure. To get started, check out Cosmos Reason 2 documentation and the Cosmos Cookbook.  &lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Other Models From The Cosmos Family:
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;🔮 Cosmos Predict 2.5&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Cosmos Predict is a generative AI model that predicts future states of the physical world as video, based on text, image, or video inputs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Physical AI Bench leader for quality, accuracy and overall consistency.  &lt;/li&gt;
&lt;li&gt;Up to 30 seconds of physically and temporally consistent clip per generation.   &lt;/li&gt;
&lt;li&gt;Supports multiple framerates and resolution.  &lt;/li&gt;
&lt;li&gt;Pre-trained on 200 million clips.   &lt;/li&gt;
&lt;li&gt;Available as 2B and 14B pre-trained models and various 2B post-trained models for multiview, action conditioning  and autonomous vehicle training.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Check out model card&amp;gt;&amp;gt;&lt;/p&gt;
&lt;p&gt; &lt;strong&gt;🔁 Cosmos Transfer 2.5&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Cosmos Transfer is our lightest multicontrol model built for video to world style transfer. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Scale a single simulation or spatial video across various environments and lighting conditions.  &lt;/li&gt;
&lt;li&gt;Improved prompt adherence and physics alignment.  &lt;/li&gt;
&lt;li&gt;Use with NVIDIA Isaac Sim™ or NVIDIA Omniverse NuRec for simulation to real transformation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Check out model card&amp;gt;&amp;gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;🤖 NVIDIA GR00T N1.6&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NVIDIA GR00T N1.6 is an open reasoning vision language action (VLA) model, purpose-built for humanoid robots, that unlocks full body control and uses NVIDIA Cosmos Reason for better reasoning and contextual understanding.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Resources
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;🧑🏻‍🍳 Read the Cosmos Cookbook → https://nvda.ws/4qevli8&lt;/p&gt;
&lt;p&gt;📚 Explore Models &amp;amp; Datasets → https://github.com/nvidia-cosmos&lt;/p&gt;
&lt;p&gt;⬇️ Try Cosmos Models in our Hosted Catalog → https://nvda.ws/3Yg0Dcx&lt;/p&gt;
&lt;p&gt;💻 Join the Cosmos Community → https://discord.gg/u23rXTHSC9&lt;/p&gt;
&lt;p&gt;🗳️ Contribute to the Cosmos Cookbook → https://nvda.ws/4aQcBkk&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/nvidia/nvidia-cosmos-reason-2-brings-advanced-reasoning</guid><pubDate>Mon, 05 Jan 2026 22:56:51 +0000</pubDate></item><item><title>[NEW] Nvidia wants to be the Android of generalist robotics (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/05/nvidia-wants-to-be-the-android-of-generalist-robotics/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/Screenshot-2026-01-05-at-5.03.42-PM.png?resize=1200,669" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nvidia released a new stack of robot foundation models, simulation tools, and edge hardware at CES 2026, moves that signal the company’s ambition to become the default platform for generalist robotics, much as Android became the operating system for smartphones.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia’s move into robotics reflects a broader industry shift as AI moves off the cloud and into machines that can learn how to think in the physical world, enabled by cheaper sensors, advanced simulation, and AI models that increasingly can generalize across tasks.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Nvidia revealed details on Monday about its full-stack ecosystem for physical AI, including new open foundation models that allow robots to reason, plan, and adapt across many tasks and diverse environments, moving beyond narrow task-specific bots, all of which are available on Hugging Face.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Those models include: Cosmos Transfer 2.5 and Cosmos Predict 2.5, two world models for synthetic data generation and robot policy evaluation in simulation; Cosmos Reason 2, a reasoning vision language model (VLM) that allows AI systems to see, understand, and act in the physical world; and Isaac GR00T N1.6, its next-gen vision language action (VLA) model purpose-built for humanoid robots. GR00T relies on Cosmos Reason as its brain, and it unlocks whole-body control for humanoids so they can move and handle objects simultaneously.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia also introduced Isaac Lab-Arena at CES, an open source simulation framework hosted on GitHub that serves as another component of the company’s physical AI platform, enabling safe virtual testing of robotic capabilities.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The platform promises to address a critical industry challenge: As robots learn increasingly complex tasks, from precise object handling to cable installation, validating these abilities in physical environments can be costly, slow, and risky. Isaac Lab-Arena tackles this by consolidating resources, task scenarios, training tools, and established benchmarks like Libero, RoboCasa, and RoboTwin, creating a unified standard where the industry previously lacked one.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Supporting the ecosystem is Nvidia OSMO, an open source command center that serves as connective infrastructure that integrates the entire workflow from data generation through training across both desktop and cloud environments.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;And to help power it all, there’s the new Blackwell-powered Jetson T4000 graphics card, the newest member of the Thor family. Nvidia is pitching it as a cost-effective on-device compute upgrade that delivers 1200 teraflops of AI compute and 64 gigabytes of memory while running efficiently at 40 to 70 watts.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia is also deepening its partnership with Hugging Face to let more people experiment with robot training without needing expensive hardware or specialized knowledge. The collaboration integrates Nvidia’s Isaac and GR00T technologies into Hugging Face’s LeRobot framework, connecting Nvidia’s 2 million robotics developers with Hugging Face’s 13 million AI builders. The developer platform’s open source Reachy 2 humanoid now works directly with Nvidia’s Jetson Thor chip, letting developers experiment with different AI models without being locked into proprietary systems.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The bigger picture here is that Nvidia is trying to make robotics development more accessible, and it wants to be the underlying hardware and software vendor powering it, much like Android is the default for smartphone makers.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;There are early signs that Nvidia’s strategy is working. Robotics is the fastest growing category on Hugging Face, with Nvidia’s models leading downloads. Meanwhile robotics companies, from Boston Dynamics and Caterpillar to Franka Robots and NEURA Robotics, are already using Nvidia’s tech.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Follow along with all of TechCrunch’s coverage of the annual CES conference here.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/Screenshot-2026-01-05-at-5.03.42-PM.png?resize=1200,669" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nvidia released a new stack of robot foundation models, simulation tools, and edge hardware at CES 2026, moves that signal the company’s ambition to become the default platform for generalist robotics, much as Android became the operating system for smartphones.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia’s move into robotics reflects a broader industry shift as AI moves off the cloud and into machines that can learn how to think in the physical world, enabled by cheaper sensors, advanced simulation, and AI models that increasingly can generalize across tasks.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Nvidia revealed details on Monday about its full-stack ecosystem for physical AI, including new open foundation models that allow robots to reason, plan, and adapt across many tasks and diverse environments, moving beyond narrow task-specific bots, all of which are available on Hugging Face.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Those models include: Cosmos Transfer 2.5 and Cosmos Predict 2.5, two world models for synthetic data generation and robot policy evaluation in simulation; Cosmos Reason 2, a reasoning vision language model (VLM) that allows AI systems to see, understand, and act in the physical world; and Isaac GR00T N1.6, its next-gen vision language action (VLA) model purpose-built for humanoid robots. GR00T relies on Cosmos Reason as its brain, and it unlocks whole-body control for humanoids so they can move and handle objects simultaneously.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia also introduced Isaac Lab-Arena at CES, an open source simulation framework hosted on GitHub that serves as another component of the company’s physical AI platform, enabling safe virtual testing of robotic capabilities.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The platform promises to address a critical industry challenge: As robots learn increasingly complex tasks, from precise object handling to cable installation, validating these abilities in physical environments can be costly, slow, and risky. Isaac Lab-Arena tackles this by consolidating resources, task scenarios, training tools, and established benchmarks like Libero, RoboCasa, and RoboTwin, creating a unified standard where the industry previously lacked one.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Supporting the ecosystem is Nvidia OSMO, an open source command center that serves as connective infrastructure that integrates the entire workflow from data generation through training across both desktop and cloud environments.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;And to help power it all, there’s the new Blackwell-powered Jetson T4000 graphics card, the newest member of the Thor family. Nvidia is pitching it as a cost-effective on-device compute upgrade that delivers 1200 teraflops of AI compute and 64 gigabytes of memory while running efficiently at 40 to 70 watts.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia is also deepening its partnership with Hugging Face to let more people experiment with robot training without needing expensive hardware or specialized knowledge. The collaboration integrates Nvidia’s Isaac and GR00T technologies into Hugging Face’s LeRobot framework, connecting Nvidia’s 2 million robotics developers with Hugging Face’s 13 million AI builders. The developer platform’s open source Reachy 2 humanoid now works directly with Nvidia’s Jetson Thor chip, letting developers experiment with different AI models without being locked into proprietary systems.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The bigger picture here is that Nvidia is trying to make robotics development more accessible, and it wants to be the underlying hardware and software vendor powering it, much like Android is the default for smartphone makers.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;There are early signs that Nvidia’s strategy is working. Robotics is the fastest growing category on Hugging Face, with Nvidia’s models leading downloads. Meanwhile robotics companies, from Boston Dynamics and Caterpillar to Franka Robots and NEURA Robotics, are already using Nvidia’s tech.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Follow along with all of TechCrunch’s coverage of the annual CES conference here.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/05/nvidia-wants-to-be-the-android-of-generalist-robotics/</guid><pubDate>Mon, 05 Jan 2026 23:00:00 +0000</pubDate></item><item><title>[NEW] Microsoft’s Nadella wants us to stop thinking of AI as ‘slop’ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/05/microsofts-nadella-wants-us-to-stop-thinking-of-ai-as-slop/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/11/GettyImages-1778706501.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A couple of weeks after Merriam-Webster named “slop” as its word of the year, Microsoft CEO Satya Nadella weighed in on what to expect from AI in 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In his classic, intellectual style, Nadella wrote on his personal blog that he wants us to stop thinking of AI as “slop” and start thinking of it as “bicycles for the mind.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;He wrote, “A new concept that evolves ‘bicycles for the mind’ such that we always think of AI as a scaffolding for human potential vs a substitute.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He continued: “We need to get beyond the arguments of slop vs sophistication and develop a new equilibrium in terms of our ‘theory of the mind’ that accounts for humans being equipped with these new cognitive amplifier tools as we relate to each other.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If you parse through those syllables, you may see that he’s not only urging everyone to stop thinking of AI-generated content as slop, but also wants the tech industry to stop talking about AI as a replacement for humans. He hopes the industry will start talking about it as a human-helper productivity tool instead.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Here’s the problem with that framing, though: Much of AI agent marketing uses the idea of replacing human labor as a way to price it, and justify its expense.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, some of the biggest names in AI have been sounding the alarm that the tech will soon cause very high levels of human unemployment. For instance, in May Anthropic CEO Dario Amodei warned that AI could take away half of all entry-level white-collar jobs, raising unemployment to 10-20% over the next five years, and he doubled down on that last month in an interview on 60 Minutes.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Yet we currently don’t know how true such doomsday stats are. As Nadella implies, most AI tools today don’t replace workers, they are used by them (as long as the human doesn’t mind checking the AI’s work for accuracy).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One oft-cited research study is MIT’s ongoing Project Iceberg, which seeks to measure the economic impact on jobs as AI enters the workforce. Project Iceberg estimates that AI is currently capable of performing about 11.7% of human paid labor.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While this has been widely reported as AI being capable of replacing nearly 12% of jobs, the Project says what it’s actually estimating is how much of a job can be offloaded to AI. It then calculates wages attached to that offloaded work. Interestingly, the tasks it cites as examples include automated paperwork for nurses and AI-written computer code.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That’s not to say there are no jobs being heavily impacted by AI. Corporate graphic artists and marketing bloggers are two examples, according to a Substack called Blood in the Machine. Then there are the high unemployment rates among new-grad junior coders.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But it’s also true that highly skilled artists, writers, and programmers produce better work with AI tools than those without the skills. AI can’t replace human creativity, yet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So it’s perhaps no surprise that as we slide into 2026, some data is emerging that shows the jobs where AI has made the most progress are actually flourishing. Vanguard’s 2026 economic forecast report found that “the approximately 100 occupations most exposed to AI automation are actually outperforming the rest of the labor market in terms of job growth and real wage increases.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Vanguard report concludes that those who are masterfully using AI are making themselves more valuable, not replaceable. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The irony is that Microsoft’s own actions last year helped give rise to the AI-is-coming-for-our-jobs narrative. The company laid off over 15,000 people in 2025, even as it recorded record revenues and profits for its last fiscal year, which closed in June — citing success with AI as a reason. Nadella even wrote a public memo about the layoffs after these results.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, he didn’t say that internal AI efficiency led to cuts. But he did say that Microsoft had to “reimagine our mission for a new era” and named “AI transformation” as one of the company’s three business objectives in this era (the other two being security and quality).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The truth about job loss attributed to AI during 2025 is more nuanced. As the Vanguard report points out, this had less to do with internal AI efficiency and more to do with ordinary business practices that are less exciting to investors, like ending investment in slowing areas to pile in to growing ones.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To be fair, Microsoft wasn’t alone in laying off workers while pursuing AI. The technology was said to be responsible for almost 55,000 layoffs in the U.S. in 2025, according to research from firm Challenger, Gray &amp;amp; Christmas, CNBC reported. That report cited the large cuts last year at Amazon, Salesforce, Microsoft, and other tech companies chasing AI.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;And to be fair to slop, those of us who spend more time than we should on social media laughing at memes and AI-generated short-form videos might argue that slop is one of AI’s most entertaining (if not best) uses, too.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/11/GettyImages-1778706501.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A couple of weeks after Merriam-Webster named “slop” as its word of the year, Microsoft CEO Satya Nadella weighed in on what to expect from AI in 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In his classic, intellectual style, Nadella wrote on his personal blog that he wants us to stop thinking of AI as “slop” and start thinking of it as “bicycles for the mind.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;He wrote, “A new concept that evolves ‘bicycles for the mind’ such that we always think of AI as a scaffolding for human potential vs a substitute.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He continued: “We need to get beyond the arguments of slop vs sophistication and develop a new equilibrium in terms of our ‘theory of the mind’ that accounts for humans being equipped with these new cognitive amplifier tools as we relate to each other.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If you parse through those syllables, you may see that he’s not only urging everyone to stop thinking of AI-generated content as slop, but also wants the tech industry to stop talking about AI as a replacement for humans. He hopes the industry will start talking about it as a human-helper productivity tool instead.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Here’s the problem with that framing, though: Much of AI agent marketing uses the idea of replacing human labor as a way to price it, and justify its expense.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, some of the biggest names in AI have been sounding the alarm that the tech will soon cause very high levels of human unemployment. For instance, in May Anthropic CEO Dario Amodei warned that AI could take away half of all entry-level white-collar jobs, raising unemployment to 10-20% over the next five years, and he doubled down on that last month in an interview on 60 Minutes.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Yet we currently don’t know how true such doomsday stats are. As Nadella implies, most AI tools today don’t replace workers, they are used by them (as long as the human doesn’t mind checking the AI’s work for accuracy).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One oft-cited research study is MIT’s ongoing Project Iceberg, which seeks to measure the economic impact on jobs as AI enters the workforce. Project Iceberg estimates that AI is currently capable of performing about 11.7% of human paid labor.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While this has been widely reported as AI being capable of replacing nearly 12% of jobs, the Project says what it’s actually estimating is how much of a job can be offloaded to AI. It then calculates wages attached to that offloaded work. Interestingly, the tasks it cites as examples include automated paperwork for nurses and AI-written computer code.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That’s not to say there are no jobs being heavily impacted by AI. Corporate graphic artists and marketing bloggers are two examples, according to a Substack called Blood in the Machine. Then there are the high unemployment rates among new-grad junior coders.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But it’s also true that highly skilled artists, writers, and programmers produce better work with AI tools than those without the skills. AI can’t replace human creativity, yet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So it’s perhaps no surprise that as we slide into 2026, some data is emerging that shows the jobs where AI has made the most progress are actually flourishing. Vanguard’s 2026 economic forecast report found that “the approximately 100 occupations most exposed to AI automation are actually outperforming the rest of the labor market in terms of job growth and real wage increases.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Vanguard report concludes that those who are masterfully using AI are making themselves more valuable, not replaceable. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The irony is that Microsoft’s own actions last year helped give rise to the AI-is-coming-for-our-jobs narrative. The company laid off over 15,000 people in 2025, even as it recorded record revenues and profits for its last fiscal year, which closed in June — citing success with AI as a reason. Nadella even wrote a public memo about the layoffs after these results.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, he didn’t say that internal AI efficiency led to cuts. But he did say that Microsoft had to “reimagine our mission for a new era” and named “AI transformation” as one of the company’s three business objectives in this era (the other two being security and quality).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The truth about job loss attributed to AI during 2025 is more nuanced. As the Vanguard report points out, this had less to do with internal AI efficiency and more to do with ordinary business practices that are less exciting to investors, like ending investment in slowing areas to pile in to growing ones.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To be fair, Microsoft wasn’t alone in laying off workers while pursuing AI. The technology was said to be responsible for almost 55,000 layoffs in the U.S. in 2025, according to research from firm Challenger, Gray &amp;amp; Christmas, CNBC reported. That report cited the large cuts last year at Amazon, Salesforce, Microsoft, and other tech companies chasing AI.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;And to be fair to slop, those of us who spend more time than we should on social media laughing at memes and AI-generated short-form videos might argue that slop is one of AI’s most entertaining (if not best) uses, too.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/05/microsofts-nadella-wants-us-to-stop-thinking-of-ai-as-slop/</guid><pubDate>Mon, 05 Jan 2026 23:09:56 +0000</pubDate></item><item><title>[NEW] AMD unveils new AI PC processors for general use and gaming at CES (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/05/amd-unveils-new-ai-pc-processors-for-general-use-and-gaming-at-ces/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/Ryzen-AI-Blog-1200x675-1.jpg?w=960" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AMD Chair and CEO Lisa Su kicked off her keynote at CES 2026 with a message about what compute could deliver: AI for everyone. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As part of that promise, AMD announced a new line of AI processors as the company thinks AI-powered personal computers are the way of the future.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The semiconductor giant revealed AMD Ryzen AI 400 Series processor, its latest version of its AI-powered PC chips, at the yearly CES conference on Monday. The company says the latest version of its Ryzen processor series allows for 1.3x faster multitasking than its competitors and are 1.7x times faster at content creation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These new chips feature 12 CPU Cores, individual processing units inside a core processor, and 24 threads, independent streams of instruction&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is an upgrade to the Ryzen AI 300 Series processor that was announced in 2024. AMD started producing the Ryzen processor series in 2017.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rahul Tikoo, senior vice president and general manager of AMD’s client business, said AMD has expanded to over 250 AI PC platforms on the company’s recent press briefing. That represents a growth 2x over the last year, he added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In the years ahead, AI is going to be a multi-layered fabric that gets woven into every level of computing at the personal layer,” Tikoo said. “Our AI PCs and devices will transform how we work, how we play, how we create and how we connect with each other.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;AMD also announced the release of the AMD Ryzen 7 9850X3D, the latest version of its gaming-focused processor.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“No matter who you are and how you use technology on a daily basis, AI is reshaping everyday computing,” Tikoo said. “You have thousands of interactions with your PC every day. AI is able to understand, learn context, bring automation, provide deep reasoning and personal customization to every individual.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;PCs that include either the Ryzen AI 300 Series processor or the AMD Ryzen 7 9850X3D processor become available in the first quarter of 2026.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company also announced the latest version of its Redstone ray tracing technology, which simulates physical behavior of light, which allows for better video game graphics without a performance or speed lag.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Follow along with all of TechCrunch’s coverage of the annual CES conference here. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/Ryzen-AI-Blog-1200x675-1.jpg?w=960" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AMD Chair and CEO Lisa Su kicked off her keynote at CES 2026 with a message about what compute could deliver: AI for everyone. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As part of that promise, AMD announced a new line of AI processors as the company thinks AI-powered personal computers are the way of the future.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The semiconductor giant revealed AMD Ryzen AI 400 Series processor, its latest version of its AI-powered PC chips, at the yearly CES conference on Monday. The company says the latest version of its Ryzen processor series allows for 1.3x faster multitasking than its competitors and are 1.7x times faster at content creation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These new chips feature 12 CPU Cores, individual processing units inside a core processor, and 24 threads, independent streams of instruction&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is an upgrade to the Ryzen AI 300 Series processor that was announced in 2024. AMD started producing the Ryzen processor series in 2017.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rahul Tikoo, senior vice president and general manager of AMD’s client business, said AMD has expanded to over 250 AI PC platforms on the company’s recent press briefing. That represents a growth 2x over the last year, he added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In the years ahead, AI is going to be a multi-layered fabric that gets woven into every level of computing at the personal layer,” Tikoo said. “Our AI PCs and devices will transform how we work, how we play, how we create and how we connect with each other.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;AMD also announced the release of the AMD Ryzen 7 9850X3D, the latest version of its gaming-focused processor.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“No matter who you are and how you use technology on a daily basis, AI is reshaping everyday computing,” Tikoo said. “You have thousands of interactions with your PC every day. AI is able to understand, learn context, bring automation, provide deep reasoning and personal customization to every individual.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;PCs that include either the Ryzen AI 300 Series processor or the AMD Ryzen 7 9850X3D processor become available in the first quarter of 2026.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company also announced the latest version of its Redstone ray tracing technology, which simulates physical behavior of light, which allows for better video game graphics without a performance or speed lag.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Follow along with all of TechCrunch’s coverage of the annual CES conference here. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/05/amd-unveils-new-ai-pc-processors-for-general-use-and-gaming-at-ces/</guid><pubDate>Tue, 06 Jan 2026 03:30:00 +0000</pubDate></item></channel></rss>