<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 14 Aug 2025 01:51:15 +0000</lastBuildDate><item><title>OpenAI brings back GPT-4o after user revolt (AI – Ars Technica)</title><link>https://arstechnica.com/information-technology/2025/08/openai-brings-back-gpt-4o-after-user-revolt/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        After unpopular GPT-5 launch, OpenAI begins restoring optional access to previous AI models.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A glowing OpenAI logo on a light blue background." class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/openai_iceblue_hero-300x169.jpg" width="300" /&gt;
                  &lt;img alt="A glowing OpenAI logo on a light blue background." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/openai_iceblue_hero-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          OpenAI / Benj Edwards

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Tuesday, OpenAI CEO Sam Altman announced that GPT-4o has returned to ChatGPT following intense user backlash over its removal during last week's GPT-5 launch. The AI model now appears in the model picker for all paid ChatGPT users by default (including ChatGPT Plus accounts), marking a swift reversal after thousands of users complained about losing access to their preferred models.&lt;/p&gt;
&lt;p&gt;The return of GPT-4o comes after what Altman described as OpenAI underestimating "how much some of the things that people like in GPT-4o matter to them." In an attempt to simplify its offerings, OpenAI had initially removed all previous AI models from ChatGPT when GPT-5 launched on August 7, forcing users to adopt the new model without warning. The move sparked one of the most vocal user revolts in ChatGPT's history, with a Reddit thread titled "GPT-5 is horrible" gathering over 2,000 comments within days.&lt;/p&gt;
&lt;p&gt;Along with bringing back GPT-4o, OpenAI made several other changes to address user concerns. Rate limits for GPT-5 Thinking mode increased from 200 to 3,000 messages per week, with additional capacity available through "GPT-5 Thinking mini" after reaching that limit. The company also added new routing options—"Auto," "Fast," and "Thinking"—giving users more control over which GPT-5 variant handles their queries.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2111750 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="A screenshot of ChatGPT Pro's model picker interface captured on August 13, 2025." class="fullwidth full" height="611" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/pro_model_picker.png" width="793" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A screenshot of ChatGPT Pro's model picker interface captured on August 13, 2025.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;For Pro users who pay $200 a month for access, Altman confirmed that additional models, including o3, 4.1, and GPT-5 Thinking mini, will later become available through a "Show additional models" toggle in ChatGPT web settings. He noted that GPT-4.5 will remain exclusive to Pro subscribers due to high GPU costs.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Addressing “personality” concerns&lt;/h2&gt;
&lt;p&gt;Beyond model availability, OpenAI acknowledged widespread complaints about GPT-5's output style. Users had described the new model as "abrupt and sharp" compared to GPT-4o's more conversational tone. Some users with emotional attachments to the older model expressed grief over losing what they considered their "only friend."&lt;/p&gt;
&lt;p&gt;"We are working on an update to GPT-5's personality which should feel warmer than the current personality but not as annoying (to most users) as GPT-4o," Altman wrote in his announcement. He added that OpenAI recognizes the need for "more per-user customization of model personality" going forward.&lt;/p&gt;
&lt;p&gt;The GPT-5 launch had been plagued by multiple issues beyond model removal. An automatic routing system meant to select appropriate model variants malfunctioned on launch day, consistently defaulting to less capable versions. OpenAI also faced criticism for including misleading performance graphs in the launch presentation, which Altman later called a "mega chart screwup."&lt;/p&gt;
&lt;p&gt;While GPT-4o has returned for now, OpenAI continues to refine GPT-5. The company indicated that rate limits may need further adjustments "depending on usage" and that personality updates remain in development. For now, paid ChatGPT users can select their preferred model, offering a compromise between OpenAI's push toward newer AI models and user demands for choice.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;


  &lt;div class="listing-credit my-2"&gt;
    &lt;p class="text-gray-350 font-impact text-sm font-semibold"&gt;
    Listing image:
    
      Benj Edwards / Getty Images
    
  &lt;/p&gt;
  &lt;/div&gt;




  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        After unpopular GPT-5 launch, OpenAI begins restoring optional access to previous AI models.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A glowing OpenAI logo on a light blue background." class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/openai_iceblue_hero-300x169.jpg" width="300" /&gt;
                  &lt;img alt="A glowing OpenAI logo on a light blue background." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/openai_iceblue_hero-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          OpenAI / Benj Edwards

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Tuesday, OpenAI CEO Sam Altman announced that GPT-4o has returned to ChatGPT following intense user backlash over its removal during last week's GPT-5 launch. The AI model now appears in the model picker for all paid ChatGPT users by default (including ChatGPT Plus accounts), marking a swift reversal after thousands of users complained about losing access to their preferred models.&lt;/p&gt;
&lt;p&gt;The return of GPT-4o comes after what Altman described as OpenAI underestimating "how much some of the things that people like in GPT-4o matter to them." In an attempt to simplify its offerings, OpenAI had initially removed all previous AI models from ChatGPT when GPT-5 launched on August 7, forcing users to adopt the new model without warning. The move sparked one of the most vocal user revolts in ChatGPT's history, with a Reddit thread titled "GPT-5 is horrible" gathering over 2,000 comments within days.&lt;/p&gt;
&lt;p&gt;Along with bringing back GPT-4o, OpenAI made several other changes to address user concerns. Rate limits for GPT-5 Thinking mode increased from 200 to 3,000 messages per week, with additional capacity available through "GPT-5 Thinking mini" after reaching that limit. The company also added new routing options—"Auto," "Fast," and "Thinking"—giving users more control over which GPT-5 variant handles their queries.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2111750 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="A screenshot of ChatGPT Pro's model picker interface captured on August 13, 2025." class="fullwidth full" height="611" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/pro_model_picker.png" width="793" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A screenshot of ChatGPT Pro's model picker interface captured on August 13, 2025.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;For Pro users who pay $200 a month for access, Altman confirmed that additional models, including o3, 4.1, and GPT-5 Thinking mini, will later become available through a "Show additional models" toggle in ChatGPT web settings. He noted that GPT-4.5 will remain exclusive to Pro subscribers due to high GPU costs.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Addressing “personality” concerns&lt;/h2&gt;
&lt;p&gt;Beyond model availability, OpenAI acknowledged widespread complaints about GPT-5's output style. Users had described the new model as "abrupt and sharp" compared to GPT-4o's more conversational tone. Some users with emotional attachments to the older model expressed grief over losing what they considered their "only friend."&lt;/p&gt;
&lt;p&gt;"We are working on an update to GPT-5's personality which should feel warmer than the current personality but not as annoying (to most users) as GPT-4o," Altman wrote in his announcement. He added that OpenAI recognizes the need for "more per-user customization of model personality" going forward.&lt;/p&gt;
&lt;p&gt;The GPT-5 launch had been plagued by multiple issues beyond model removal. An automatic routing system meant to select appropriate model variants malfunctioned on launch day, consistently defaulting to less capable versions. OpenAI also faced criticism for including misleading performance graphs in the launch presentation, which Altman later called a "mega chart screwup."&lt;/p&gt;
&lt;p&gt;While GPT-4o has returned for now, OpenAI continues to refine GPT-5. The company indicated that rate limits may need further adjustments "depending on usage" and that personality updates remain in development. For now, paid ChatGPT users can select their preferred model, offering a compromise between OpenAI's push toward newer AI models and user demands for choice.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;


  &lt;div class="listing-credit my-2"&gt;
    &lt;p class="text-gray-350 font-impact text-sm font-semibold"&gt;
    Listing image:
    
      Benj Edwards / Getty Images
    
  &lt;/p&gt;
  &lt;/div&gt;




  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/information-technology/2025/08/openai-brings-back-gpt-4o-after-user-revolt/</guid><pubDate>Wed, 13 Aug 2025 14:08:47 +0000</pubDate></item><item><title>Applications Now Open for $60,000 NVIDIA Graduate Fellowship Awards (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/applications-open-graduate-fellowship-awards-2025/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2024/02/2023-nvidia-corporate-key-visual-wallpaper-1080p-cropped.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Bringing together the world’s brightest minds and the latest accelerated computing technology leads to powerful breakthroughs that help tackle some of the biggest research problems.&lt;/p&gt;
&lt;p&gt;To foster such innovation, the NVIDIA Graduate Fellowship Program provides grants, mentors and technical support to doctoral students doing outstanding research relevant to NVIDIA technologies. The program, in its 25th year, is now accepting applications worldwide.&lt;/p&gt;
&lt;p&gt;It focuses on supporting students working in AI, machine learning, autonomous vehicles, computer graphics, robotics, healthcare, high-performance computing and related fields. Awards are up to $60,000 per student.&lt;/p&gt;
&lt;p&gt;Since its start in 2002, the Graduate Fellowship Program has awarded over 200 grants worth more than $7.3 million.&lt;/p&gt;
&lt;p&gt;Students must have completed at least their first year of Ph.D.-level studies at the time of application.&lt;/p&gt;
&lt;p&gt;The application deadline for the 2026-2027 academic year is Monday, Sept. 15, 2025. An in-person internship at an NVIDIA research office preceding the fellowship year is mandatory; eligible candidates must be available for the internship in summer 2026.&lt;/p&gt;
&lt;p&gt;For more on eligibility and how to apply, visit the program website.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2024/02/2023-nvidia-corporate-key-visual-wallpaper-1080p-cropped.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Bringing together the world’s brightest minds and the latest accelerated computing technology leads to powerful breakthroughs that help tackle some of the biggest research problems.&lt;/p&gt;
&lt;p&gt;To foster such innovation, the NVIDIA Graduate Fellowship Program provides grants, mentors and technical support to doctoral students doing outstanding research relevant to NVIDIA technologies. The program, in its 25th year, is now accepting applications worldwide.&lt;/p&gt;
&lt;p&gt;It focuses on supporting students working in AI, machine learning, autonomous vehicles, computer graphics, robotics, healthcare, high-performance computing and related fields. Awards are up to $60,000 per student.&lt;/p&gt;
&lt;p&gt;Since its start in 2002, the Graduate Fellowship Program has awarded over 200 grants worth more than $7.3 million.&lt;/p&gt;
&lt;p&gt;Students must have completed at least their first year of Ph.D.-level studies at the time of application.&lt;/p&gt;
&lt;p&gt;The application deadline for the 2026-2027 academic year is Monday, Sept. 15, 2025. An in-person internship at an NVIDIA research office preceding the fellowship year is mandatory; eligible candidates must be available for the internship in summer 2026.&lt;/p&gt;
&lt;p&gt;For more on eligibility and how to apply, visit the program website.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/applications-open-graduate-fellowship-awards-2025/</guid><pubDate>Wed, 13 Aug 2025 15:00:02 +0000</pubDate></item><item><title>Anthropic nabs Humanloop team as competition for enterprise AI talent heats up (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/13/anthropic-nabs-humanloop-team-as-competition-for-enterprise-ai-talent-heats-up/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/Humanloop-Founders.jpg?resize=1200,801" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic has acquired the co-founders and most of the team behind Humanloop — a platform for prompt management, LLM evaluation, and observability — in a push to strengthen its enterprise strategy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The terms of the deal were not shared, but it appears to follow the acqui-hire playbook we’re increasingly seeing in the tech industry amid the war for AI talent. Humanloop’s three co-founders — CEO Raza Habib, CTO Peter Hayes, and CPO Jordan Burgess — have all joined Anthropic, alongside around a dozen engineers and researchers.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Anthropic is growing fast in the enterprise space as it leads in agentic and coding capabilities. While an Anthropic spokesperson confirmed that the AI firm did not acquire Humanloop’s assets or its intellectual property, that’s a moot point in an industry where IP lives in the brain. And what Humanloop’s team is bringing to Anthropic is experience developing the tools that help enterprises run safe, reliable AI at scale.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Or as Brad Abrams, API product lead at Anthropic, put it: “Their proven experience in AI tooling and evaluation will be invaluable as we continue to advance our work in AI safety and building&amp;nbsp;useful AI systems.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a market where model quality alone isn’t enough to stay competitive, bolstering its tooling ecosystem could position Anthropic to cement its lead over OpenAI and Google DeepMind in both performance and enterprise readiness.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Humanloop was founded in 2020 as a University College London spinout. The startup then went on to participate in Y Combinator and the Fuse Incubator before raising $7.91 million in seed funding across two rounds led by YC and Index Ventures, per PitchBook. Humanloop gained a reputation for helping enterprise customers — including Duolingo, Gusto, and Vanta — develop, evaluate, and fine-tune robust AI applications.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last month, Humanloop told customers that it would be shutting down in preparation for an acquisition.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The timing of this acqui-hire comes as Anthropic offers features like longer context windows to enterprise clients, improving what its models are capable of and where they can be applied.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this week, Anthropic reached a deal with the U.S. government’s central purchasing arm to sell its AI services to government agencies across executive, judiciary, and legislative branches for just $1 per agency for the first year — a clear move to undercut OpenAI’s similarly priced offering. Both government and enterprise buyers demand the type of evaluation, monitoring, and compliance features that Humanloop specialized in.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The acquisition is also on brand for Anthropic as it bills itself as a “safety-first” AI company. Humanloop’s evaluation workflows align with that mission by providing constant performance measurement, safety guardrails, and bias mitigation.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“From our earliest days, we’ve been focused on creating tools that help developers build AI applications safely and effectively,” said Raza Habib, former CEO of Humanloop, in a statement. “Anthropic’s commitment to AI safety research and responsible AI development perfectly aligns with our vision.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/Humanloop-Founders.jpg?resize=1200,801" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic has acquired the co-founders and most of the team behind Humanloop — a platform for prompt management, LLM evaluation, and observability — in a push to strengthen its enterprise strategy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The terms of the deal were not shared, but it appears to follow the acqui-hire playbook we’re increasingly seeing in the tech industry amid the war for AI talent. Humanloop’s three co-founders — CEO Raza Habib, CTO Peter Hayes, and CPO Jordan Burgess — have all joined Anthropic, alongside around a dozen engineers and researchers.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Anthropic is growing fast in the enterprise space as it leads in agentic and coding capabilities. While an Anthropic spokesperson confirmed that the AI firm did not acquire Humanloop’s assets or its intellectual property, that’s a moot point in an industry where IP lives in the brain. And what Humanloop’s team is bringing to Anthropic is experience developing the tools that help enterprises run safe, reliable AI at scale.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Or as Brad Abrams, API product lead at Anthropic, put it: “Their proven experience in AI tooling and evaluation will be invaluable as we continue to advance our work in AI safety and building&amp;nbsp;useful AI systems.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a market where model quality alone isn’t enough to stay competitive, bolstering its tooling ecosystem could position Anthropic to cement its lead over OpenAI and Google DeepMind in both performance and enterprise readiness.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Humanloop was founded in 2020 as a University College London spinout. The startup then went on to participate in Y Combinator and the Fuse Incubator before raising $7.91 million in seed funding across two rounds led by YC and Index Ventures, per PitchBook. Humanloop gained a reputation for helping enterprise customers — including Duolingo, Gusto, and Vanta — develop, evaluate, and fine-tune robust AI applications.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last month, Humanloop told customers that it would be shutting down in preparation for an acquisition.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The timing of this acqui-hire comes as Anthropic offers features like longer context windows to enterprise clients, improving what its models are capable of and where they can be applied.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this week, Anthropic reached a deal with the U.S. government’s central purchasing arm to sell its AI services to government agencies across executive, judiciary, and legislative branches for just $1 per agency for the first year — a clear move to undercut OpenAI’s similarly priced offering. Both government and enterprise buyers demand the type of evaluation, monitoring, and compliance features that Humanloop specialized in.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The acquisition is also on brand for Anthropic as it bills itself as a “safety-first” AI company. Humanloop’s evaluation workflows align with that mission by providing constant performance measurement, safety guardrails, and bias mitigation.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“From our earliest days, we’ve been focused on creating tools that help developers build AI applications safely and effectively,” said Raza Habib, former CEO of Humanloop, in a statement. “Anthropic’s commitment to AI safety research and responsible AI development perfectly aligns with our vision.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/13/anthropic-nabs-humanloop-team-as-competition-for-enterprise-ai-talent-heats-up/</guid><pubDate>Wed, 13 Aug 2025 16:17:30 +0000</pubDate></item><item><title>[NEW] Ai2’s MolmoAct model ‘thinks in 3D’ to challenge Nvidia and Google in robotics AI (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/ai2s-molmoact-model-thinks-in-3d-to-challenge-nvidia-and-google-in-robotics-ai/</link><description>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Physical AI, where robotics and foundation models come together, is fast becoming a growing space with companies like Nvidia, Google and Meta releasing research and experimenting in melding large language models (LLMs) with robots.&amp;nbsp;&lt;/p&gt;&lt;p&gt;New research from the Allen Institute for AI (Ai2) aims to challenge Nvidia and Google in physical AI with the release of MolmoAct 7B, a new open-source model that allows robots to “reason in space. MolmoAct, based on Ai2’s open source Molmo, “thinks” in three dimensions. It is also releasing its training data. Ai2 has an Apache 2.0 license for the model, while the datasets are licensed under CC BY-4.0.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ai2 classifies MolmoAct as an Action Reasoning Model, in which foundation models reason about actions within a physical, 3D space.&lt;/p&gt;&lt;p&gt;What this means is that MolmoAct can use its reasoning capabilities to understand the physical world, plan how it occupies space and then take that action.&amp;nbsp;&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;“MolmoAct has reasoning in 3D space capabilities versus traditional vision-language-action (VLA) models,” Ai2 told VentureBeat in an email. “Most robotics models are VLAs that don’t think or reason in space, but MolmoAct has this capability, making it more performant and generalizable from an architectural standpoint.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-physical-understanding-nbsp"&gt;Physical understanding&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Since robots exist in the physical world, Ai2 claims MolmoAct helps robots take in their surroundings and make better decisions on how to interact with them.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“MolmoAct could be applied anywhere a machine would need to reason about its physical surroundings,” the company said. “We think about it mainly in a home setting because that’s where the greatest challenge lies for robotics, because there things are irregular and constantly changing, but MolmoAct can be applied anywhere.”&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;



&lt;p&gt;MolmoAct can understand the physical world by outputting “spatially grounded perception tokens,” which are tokens pretrained and extracted using a vector-quantized variational autoencoder or a model that converts data inputs, such as video, into tokens. The company said these tokens differ from those used by VLAs in that they are not text inputs.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;These enable MolmoAct to gain spatial understanding and encode geometric structures. With these, the model estimates the distance between objects.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Once it has an estimated distance, MolmoAct then predicts a sequence of “image-space” waypoints or points in the area where it can set a path to. After that, the model will begin outputting specific actions, such as dropping an arm by a few inches or stretching out.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Ai2’s researchers said they were able to get the model to adapt to different embodiments (i.e., either a mechanical arm or a humanoid robot) “with only minimal fine-tuning.”&lt;/p&gt;



&lt;p&gt;Benchmarking testing conducted by Ai2 showed MolmoAct 7B had a task success rate of 72.1%, beating models from Google, Microsoft and Nvidia.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcWULlJnfMGKtmFPSYylYYwWeyLRgTDjiuHqecYO0QlCpe5Nf3PO7c4nQf_S6SwwtMfY5cZEgvC8gKJtKR826H-us9_veuuLXr3oo-FXd__bNQmOLj3Si6c7xkf87akPM8tDcKgvw?key=CazhOUyJIyTX--kzhSz2fg" /&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="h-a-small-step-forward"&gt;A small step forward&lt;/h2&gt;



&lt;p&gt;Ai2’s research is the latest to take advantage of the unique benefits of LLMs and VLMs, especially as the pace of innovation in generative AI continues to grow. Experts in the field see work from Ai2 and other tech companies as building blocks.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Alan Fern, professor at the Oregon State University College of Engineering, told VentureBeat that Ai2’s research “represents a natural progression in enhancing VLMs for robotics and physical reasoning.”&lt;/p&gt;



&lt;p&gt;“While I wouldn’t call it revolutionary, it’s an important step forward in the development of more capable 3D physical reasoning models,” Fern said. “Their focus on truly 3D scene understanding, as opposed to relying on 2D models, marks a notable shift in the right direction. They’ve made improvements over prior models, but these benchmarks still fall short of capturing real-world complexity and remain relatively controlled and toyish in nature.”&lt;/p&gt;



&lt;p&gt;He added that while there’s still room for improvement on the benchmarks, he is “eager to test this new model on some of our physical reasoning tasks.”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Daniel Maturana, co-founder of the start-up Gather AI, praised the openness of the data, noting that “this is great news because developing and training these models is expensive, so this is a strong foundation to build on and fine-tune for other academic labs and even for dedicated hobbyists.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-increasing-interest-in-physical-ai"&gt;Increasing interest in physical AI&lt;/h2&gt;



&lt;p&gt;It has been a long-held dream for many developers and computer scientists to create more intelligent, or at least more spatially aware, robots.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;However, building robots that process what they can “see” quickly and move and react smoothly gets difficult. Before the advent of LLMs, scientists had to code every single movement. This naturally meant a lot of work and less flexibility in the types of robotic actions that can occur. Now, LLM-based methods allow robots (or at least robotic arms) to determine the following possible actions to take based on objects it is interacting with.&lt;/p&gt;



&lt;p&gt;Google Research’s SayCan helps a robot reason about tasks using an LLM, enabling the robot to determine the sequence of movements required to achieve a goal. Meta and New York University’s OK-Robot uses visual language models for movement planning and object manipulation.&lt;/p&gt;



&lt;p&gt;Hugging Face released a $299 desktop robot in an effort to democratize robotics development. Nvidia, which proclaimed physical AI to be the next big trend, released several models to fast-track robotic training, including Cosmos-Transfer1.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;OSU’s Fern said there’s more interest in physical AI even though demos remain limited. However, the quest to achieve general physical intelligence, which eliminates the need to individually program actions for robots, is becoming easier.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“The landscape is more challenging now, with less low-hanging fruit. On the other hand, large physical intelligence models are still in their early stages and are much more ripe for rapid advancements, which makes this space particularly exciting,” he said.&amp;nbsp;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Physical AI, where robotics and foundation models come together, is fast becoming a growing space with companies like Nvidia, Google and Meta releasing research and experimenting in melding large language models (LLMs) with robots.&amp;nbsp;&lt;/p&gt;&lt;p&gt;New research from the Allen Institute for AI (Ai2) aims to challenge Nvidia and Google in physical AI with the release of MolmoAct 7B, a new open-source model that allows robots to “reason in space. MolmoAct, based on Ai2’s open source Molmo, “thinks” in three dimensions. It is also releasing its training data. Ai2 has an Apache 2.0 license for the model, while the datasets are licensed under CC BY-4.0.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ai2 classifies MolmoAct as an Action Reasoning Model, in which foundation models reason about actions within a physical, 3D space.&lt;/p&gt;&lt;p&gt;What this means is that MolmoAct can use its reasoning capabilities to understand the physical world, plan how it occupies space and then take that action.&amp;nbsp;&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;“MolmoAct has reasoning in 3D space capabilities versus traditional vision-language-action (VLA) models,” Ai2 told VentureBeat in an email. “Most robotics models are VLAs that don’t think or reason in space, but MolmoAct has this capability, making it more performant and generalizable from an architectural standpoint.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-physical-understanding-nbsp"&gt;Physical understanding&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Since robots exist in the physical world, Ai2 claims MolmoAct helps robots take in their surroundings and make better decisions on how to interact with them.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“MolmoAct could be applied anywhere a machine would need to reason about its physical surroundings,” the company said. “We think about it mainly in a home setting because that’s where the greatest challenge lies for robotics, because there things are irregular and constantly changing, but MolmoAct can be applied anywhere.”&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;



&lt;p&gt;MolmoAct can understand the physical world by outputting “spatially grounded perception tokens,” which are tokens pretrained and extracted using a vector-quantized variational autoencoder or a model that converts data inputs, such as video, into tokens. The company said these tokens differ from those used by VLAs in that they are not text inputs.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;These enable MolmoAct to gain spatial understanding and encode geometric structures. With these, the model estimates the distance between objects.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Once it has an estimated distance, MolmoAct then predicts a sequence of “image-space” waypoints or points in the area where it can set a path to. After that, the model will begin outputting specific actions, such as dropping an arm by a few inches or stretching out.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Ai2’s researchers said they were able to get the model to adapt to different embodiments (i.e., either a mechanical arm or a humanoid robot) “with only minimal fine-tuning.”&lt;/p&gt;



&lt;p&gt;Benchmarking testing conducted by Ai2 showed MolmoAct 7B had a task success rate of 72.1%, beating models from Google, Microsoft and Nvidia.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcWULlJnfMGKtmFPSYylYYwWeyLRgTDjiuHqecYO0QlCpe5Nf3PO7c4nQf_S6SwwtMfY5cZEgvC8gKJtKR826H-us9_veuuLXr3oo-FXd__bNQmOLj3Si6c7xkf87akPM8tDcKgvw?key=CazhOUyJIyTX--kzhSz2fg" /&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="h-a-small-step-forward"&gt;A small step forward&lt;/h2&gt;



&lt;p&gt;Ai2’s research is the latest to take advantage of the unique benefits of LLMs and VLMs, especially as the pace of innovation in generative AI continues to grow. Experts in the field see work from Ai2 and other tech companies as building blocks.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Alan Fern, professor at the Oregon State University College of Engineering, told VentureBeat that Ai2’s research “represents a natural progression in enhancing VLMs for robotics and physical reasoning.”&lt;/p&gt;



&lt;p&gt;“While I wouldn’t call it revolutionary, it’s an important step forward in the development of more capable 3D physical reasoning models,” Fern said. “Their focus on truly 3D scene understanding, as opposed to relying on 2D models, marks a notable shift in the right direction. They’ve made improvements over prior models, but these benchmarks still fall short of capturing real-world complexity and remain relatively controlled and toyish in nature.”&lt;/p&gt;



&lt;p&gt;He added that while there’s still room for improvement on the benchmarks, he is “eager to test this new model on some of our physical reasoning tasks.”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Daniel Maturana, co-founder of the start-up Gather AI, praised the openness of the data, noting that “this is great news because developing and training these models is expensive, so this is a strong foundation to build on and fine-tune for other academic labs and even for dedicated hobbyists.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-increasing-interest-in-physical-ai"&gt;Increasing interest in physical AI&lt;/h2&gt;



&lt;p&gt;It has been a long-held dream for many developers and computer scientists to create more intelligent, or at least more spatially aware, robots.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;However, building robots that process what they can “see” quickly and move and react smoothly gets difficult. Before the advent of LLMs, scientists had to code every single movement. This naturally meant a lot of work and less flexibility in the types of robotic actions that can occur. Now, LLM-based methods allow robots (or at least robotic arms) to determine the following possible actions to take based on objects it is interacting with.&lt;/p&gt;



&lt;p&gt;Google Research’s SayCan helps a robot reason about tasks using an LLM, enabling the robot to determine the sequence of movements required to achieve a goal. Meta and New York University’s OK-Robot uses visual language models for movement planning and object manipulation.&lt;/p&gt;



&lt;p&gt;Hugging Face released a $299 desktop robot in an effort to democratize robotics development. Nvidia, which proclaimed physical AI to be the next big trend, released several models to fast-track robotic training, including Cosmos-Transfer1.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;OSU’s Fern said there’s more interest in physical AI even though demos remain limited. However, the quest to achieve general physical intelligence, which eliminates the need to individually program actions for robots, is becoming easier.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“The landscape is more challenging now, with less low-hanging fruit. On the other hand, large physical intelligence models are still in their early stages and are much more ripe for rapid advancements, which makes this space particularly exciting,” he said.&amp;nbsp;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/ai2s-molmoact-model-thinks-in-3d-to-challenge-nvidia-and-google-in-robotics-ai/</guid><pubDate>Wed, 13 Aug 2025 16:30:48 +0000</pubDate></item><item><title>Pocket FM gives its writers an AI tool to transform narratives, write cliffhangers, and more (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/13/pocket-fm-gives-its-writers-an-ai-tool-to-transform-narratives-write-cliffhangers-and-more/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;India-based audio series platform maker Pocket FM aims to be the Netflix of audio. That is, the company intends to match its audio series with hundreds of episodes to its users’ tastes. For that to work, it needs to release content rapidly — something it’s now turning to AI to help with.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Lightspeed-backed startup is giving its writers an AI tool set that can do things like suggest better endings to an episode or make the narrative more engaging. The hope is that the tools will speed up the story-writing process.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Pocket FM already uses some AI tools like ElevenLabs to generate voices for audio series. It also tested AI tools for writing and adaptation assistance internally. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rohan Nayak, Pocket FM’s founder, said it’s rolling out the AI tools to all writers, so it will take them less time to finish their episodes.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3036537" height="334" src="https://techcrunch.com/wp-content/uploads/2025/08/image-5.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Pocket FM&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The writing tool, dubbed CoPilot, can be used to help any writer create a story.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CoPilot can transform narrative-based writing into dialog-based writing for a specific segment. It can also do “beat analysis” to shape the writing in a way to makes it more engaging for an audio series of a particular genre. The tool additionally has basic chatbot-style writing features such as “shorten,” “expand,” and the ability to generate text via a prompt.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To build CoPilot, the company examined thousands of hours of data points to understand what makes users engage more with a particular storyline in a specific genre. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Based on that, it added writing suggestion features designed to increase conflict between characters and recommend endings for an episode to make it more exciting. AI can also suggest tags for background effects that can be used while producing the audio.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tool can automatically generate bios of characters, their relationships, and summarize plot points of different episodes, allowing creators to refer back to these details while writing. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CoPilot also has a review tool, which checks for plot points, grammar, and leaves qualitative feedback through comments on an episode.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Under the hood, Pocket FM is training smaller models to maintain context for a story for character arcs and relations, along with narrative consistency. Plus, utilizing signals from users, the startup is nudging AI to add more drama to the story.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-international-expansion-and-localization-plans"&gt;International expansion and localization plans&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside the arrival of the AI tools, Pocket FM launched adaptation tools for various markets that not only translate the text from one language to another but also change names and phrases that are more suited to that region’s culture.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company first debuted this tool as a part of the CoPilot suite in Germany earlier this year to convert stories from other regions after reportedly struggling to engage users in the European country last year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nayak said the company saw great results from this trial, with a constant increase in monthly in-app revenue, which crossed $700,000 in June.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3036538" height="335" src="https://techcrunch.com/wp-content/uploads/2025/08/image-4.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Pocket FM&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“When we started expanding into new regions, it used to take us 12-18 months to meaningfully exist in that market. You have to have at least 1,000 hours of content to start acquiring users and scaling the market. Now we can do this in less than three months,” Nayak said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tool increased writer productivity by up to 50% for the German market in terms of show output. Plus, the tool helped the company create more error-free drafts of the shows that resulted in higher user retention for audio series.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the U.S., series created with the help of these new AI tools are now contributing 10% of playtime. Plus, these shows have generated $7 million in revenue in the last 12 months while reducing the cost of production by 2-3 times.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-building-tech-to-scale-content-generation"&gt;Building tech to scale content generation&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;As a result of adopting different AI features internally, Pocket FM has been able to scale the content quickly. The startup said it launches close to 1,000 pilots per month. And just the sheer volume of content results in a few of them becoming hits.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;But the audio show is just one part. The company is already working on tools to convert stories into comic strips with its Pocket Toons platform. Plus, Nayak said video is a possible format the company could explore, too. The startup, which has raised over $196 million in funding across rounds, is experimenting with a micro drama app as well.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3036540" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/Screenshot-2025-08-13-at-12.05.03PM.jpg?w=413" width="413" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;By next year, Pocket FM wants to release its own singular large language model (LLM), which will be based on data collected from its shows and incorporate different tools like writing assistance, adoption, dramatization, and story context retention. The company said that when it switches to its own LLM, it won’t need to train a ton of small models for separate features.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-ai-s-potential-downsides"&gt;AI’s potential downsides&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Adopting AI has had its side effects. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Pocket FM has already laid off people who were employees or contractors across multiple rounds in the last 12 months. There have also been reports of writers seeing diminished returns over time. And the company is facing lawsuits in California over employment and wage issues.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Like most content-led industries, we work with a diverse network of writers, voice artists, and production partners on a project basis, tailoring resources to each market. AI has had minimal impact on our core creative community; instead, it has opened new avenues to expand reach and output,” a company rep said, in response to these layoffs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There are questions around quality, as well. The company measures quality by the retention numbers of a show. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The base argument is that the new AI tools act as a writers’ room even for solo creators, so they will be able to produce more content at a rapid rate. Plus, based on the numbers, writers can quickly edit the story with the help of AI. However, these tools can very well induce “AI slop” — or low-quality, AI-generated content — into the platform and could impact a user’s recommendations, making it difficult for them to discover good stories.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Pocket FM argues that stories that have a solid structure will gain popularity, despite AI helping them. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company noted that every piece of content is reviewed by its AI-powered moderation framework to ensure quality and originality. It also claims its AI moderation checks for things like duplication, copyright issues, content health, and other quality measures before approving audio to go live. Each show receives an equal push, and user engagement ultimately determines a show’s ranking.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another concern is that writers could become overly dependent on AI over time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In Germany, AI is writing more content than humans per show for select titles. With Pocket FM’s plans to roll out more AI tools, the amount of AI-written content could increase. And with that, the expectation of churning out more shows could rise, too. Unless user adoption also rises rapidly, average returns could drop.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company didn’t directly address TechCrunch’s questions about returns, but said that its AI tools can speed up a writer’s work and help them edit an episode based on numbers and audience feedback. That is they could make targeted improvements, instead of doing a full rewrite. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This way, faster content creation doesn’t necessarily dilute quality or relevance; it just shifts the writer’s role towards editing, refining, and steering more productive output,” a spokesperson said in a statement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;India-based audio series platform maker Pocket FM aims to be the Netflix of audio. That is, the company intends to match its audio series with hundreds of episodes to its users’ tastes. For that to work, it needs to release content rapidly — something it’s now turning to AI to help with.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Lightspeed-backed startup is giving its writers an AI tool set that can do things like suggest better endings to an episode or make the narrative more engaging. The hope is that the tools will speed up the story-writing process.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Pocket FM already uses some AI tools like ElevenLabs to generate voices for audio series. It also tested AI tools for writing and adaptation assistance internally. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rohan Nayak, Pocket FM’s founder, said it’s rolling out the AI tools to all writers, so it will take them less time to finish their episodes.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3036537" height="334" src="https://techcrunch.com/wp-content/uploads/2025/08/image-5.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Pocket FM&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The writing tool, dubbed CoPilot, can be used to help any writer create a story.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CoPilot can transform narrative-based writing into dialog-based writing for a specific segment. It can also do “beat analysis” to shape the writing in a way to makes it more engaging for an audio series of a particular genre. The tool additionally has basic chatbot-style writing features such as “shorten,” “expand,” and the ability to generate text via a prompt.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To build CoPilot, the company examined thousands of hours of data points to understand what makes users engage more with a particular storyline in a specific genre. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Based on that, it added writing suggestion features designed to increase conflict between characters and recommend endings for an episode to make it more exciting. AI can also suggest tags for background effects that can be used while producing the audio.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tool can automatically generate bios of characters, their relationships, and summarize plot points of different episodes, allowing creators to refer back to these details while writing. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CoPilot also has a review tool, which checks for plot points, grammar, and leaves qualitative feedback through comments on an episode.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Under the hood, Pocket FM is training smaller models to maintain context for a story for character arcs and relations, along with narrative consistency. Plus, utilizing signals from users, the startup is nudging AI to add more drama to the story.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-international-expansion-and-localization-plans"&gt;International expansion and localization plans&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside the arrival of the AI tools, Pocket FM launched adaptation tools for various markets that not only translate the text from one language to another but also change names and phrases that are more suited to that region’s culture.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company first debuted this tool as a part of the CoPilot suite in Germany earlier this year to convert stories from other regions after reportedly struggling to engage users in the European country last year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nayak said the company saw great results from this trial, with a constant increase in monthly in-app revenue, which crossed $700,000 in June.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3036538" height="335" src="https://techcrunch.com/wp-content/uploads/2025/08/image-4.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Pocket FM&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“When we started expanding into new regions, it used to take us 12-18 months to meaningfully exist in that market. You have to have at least 1,000 hours of content to start acquiring users and scaling the market. Now we can do this in less than three months,” Nayak said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tool increased writer productivity by up to 50% for the German market in terms of show output. Plus, the tool helped the company create more error-free drafts of the shows that resulted in higher user retention for audio series.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the U.S., series created with the help of these new AI tools are now contributing 10% of playtime. Plus, these shows have generated $7 million in revenue in the last 12 months while reducing the cost of production by 2-3 times.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-building-tech-to-scale-content-generation"&gt;Building tech to scale content generation&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;As a result of adopting different AI features internally, Pocket FM has been able to scale the content quickly. The startup said it launches close to 1,000 pilots per month. And just the sheer volume of content results in a few of them becoming hits.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;But the audio show is just one part. The company is already working on tools to convert stories into comic strips with its Pocket Toons platform. Plus, Nayak said video is a possible format the company could explore, too. The startup, which has raised over $196 million in funding across rounds, is experimenting with a micro drama app as well.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3036540" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/Screenshot-2025-08-13-at-12.05.03PM.jpg?w=413" width="413" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;By next year, Pocket FM wants to release its own singular large language model (LLM), which will be based on data collected from its shows and incorporate different tools like writing assistance, adoption, dramatization, and story context retention. The company said that when it switches to its own LLM, it won’t need to train a ton of small models for separate features.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-ai-s-potential-downsides"&gt;AI’s potential downsides&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Adopting AI has had its side effects. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Pocket FM has already laid off people who were employees or contractors across multiple rounds in the last 12 months. There have also been reports of writers seeing diminished returns over time. And the company is facing lawsuits in California over employment and wage issues.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Like most content-led industries, we work with a diverse network of writers, voice artists, and production partners on a project basis, tailoring resources to each market. AI has had minimal impact on our core creative community; instead, it has opened new avenues to expand reach and output,” a company rep said, in response to these layoffs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There are questions around quality, as well. The company measures quality by the retention numbers of a show. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The base argument is that the new AI tools act as a writers’ room even for solo creators, so they will be able to produce more content at a rapid rate. Plus, based on the numbers, writers can quickly edit the story with the help of AI. However, these tools can very well induce “AI slop” — or low-quality, AI-generated content — into the platform and could impact a user’s recommendations, making it difficult for them to discover good stories.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Pocket FM argues that stories that have a solid structure will gain popularity, despite AI helping them. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company noted that every piece of content is reviewed by its AI-powered moderation framework to ensure quality and originality. It also claims its AI moderation checks for things like duplication, copyright issues, content health, and other quality measures before approving audio to go live. Each show receives an equal push, and user engagement ultimately determines a show’s ranking.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another concern is that writers could become overly dependent on AI over time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In Germany, AI is writing more content than humans per show for select titles. With Pocket FM’s plans to roll out more AI tools, the amount of AI-written content could increase. And with that, the expectation of churning out more shows could rise, too. Unless user adoption also rises rapidly, average returns could drop.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company didn’t directly address TechCrunch’s questions about returns, but said that its AI tools can speed up a writer’s work and help them edit an episode based on numbers and audience feedback. That is they could make targeted improvements, instead of doing a full rewrite. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This way, faster content creation doesn’t necessarily dilute quality or relevance; it just shifts the writer’s role towards editing, refining, and steering more productive output,” a spokesperson said in a statement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/13/pocket-fm-gives-its-writers-an-ai-tool-to-transform-narratives-write-cliffhangers-and-more/</guid><pubDate>Wed, 13 Aug 2025 16:48:49 +0000</pubDate></item><item><title>[NEW] Google Gemini will now learn from your chats—unless you tell it not to (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/08/google-gemini-will-now-learn-from-your-chats-unless-you-tell-it-not-to/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Gemini will remember this, so it's time to check your privacy settings.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Gemini AI Android app assistant" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/03/Gemini-app-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Gemini AI Android app assistant" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/03/Gemini-app-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;As Gemini is increasingly woven into the fabric of Google, the way the chatbot accesses and interacts with your data is in a constant state of flux. Today, Google is announcing several big changes to how its AI adapts to you, giving it the ability to remember more details about your chats for improved answers. If that's a concern, Google also has a new temporary chat option that won't affect the way Gemini thinks about you.&lt;/p&gt;
&lt;p&gt;You might recall several months back when Google added a "personalization" option to the Gemini model selector. This mode leaned on your Google search history to customize responses, a feature that did not seem to appeal to many Gemini users. Google later dropped that mode, but a new attempt at customization is now rolling out. Gemini is getting an option called Personal Context. When enabled, the chatbot will remember details about your past conversations, adapting its replies without being specifically prompted.&lt;/p&gt;
&lt;p&gt;Google claims Personal Context will produce more relevant responses, particularly when you ask the chatbot to make recommendations. This is separate from the saved instructions feature, which allows you to provide explicit instructions for Gemini to be used in crafting outputs. This does have the potential to make Gemini feel more engaging, but that's not always a good thing. AI chatbots that get too friendly with the user can reinforce misconceptions and lead to delusional thinking, something we've seen distressingly often with AI models.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2111853 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="562" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/Personal_context_-_Past_Chats.jpg" width="1000" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;To start, this feature will be available with the Gemini 2.5 Pro model, but you won't get customization in the Eurpean Union,the&amp;nbsp; UK, or Switzerland. It's also limited to users over the age of 18. Google says it will eventually release this feature in additional regions and with support for the more efficient Gemini 2.5 Flash model. You can turn Personal Context on and off at will from the main settings page.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;More control over your data&lt;/h2&gt;
&lt;p&gt;As Google moves to implement more customization features in Gemini, you might find yourself second-guessing whether you really want to have certain conversations with the robot. Thankfully, you have options. You can turn off Personal Context, but Temporary Chats go a step further—it's essentially Incognito Mode (but one that actually works) for Gemini.&lt;/p&gt;
&lt;p&gt;Temporary Chats also begin rolling out today and will expand to all users over the coming weeks. The feature will be accessible via a dedicated button next to the "New chat" option in the Gemini app. Google says anything you type in a temporary interaction won't be used in Personal Context, even if that setting is enabled. Google labels these as "one-off" chats, but they're not &lt;em&gt;quite&lt;/em&gt; that temporary. They'll be retained on Google's servers for 72 hours so you can refer back to them and expand on the conversation if you want.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2111856 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="Gemini temporary" class="fullwidth full" height="1000" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/Temporary_Chat.jpg" width="1000" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Even if you only occasionally use Gemini, you'll want to pay attention to the new personalization push. Google has also confirmed that it's changing how it uses the content you upload to Gemini. Starting September 2, a sample of your chats and data (including file uploads) will be used to train Google's AI. Or in Google's words, your data will "improve Google services for everyone."&lt;/p&gt;
&lt;p&gt;If you don't want to give license to dump your data into AI models, you'll need to opt out. In the next few weeks, Google will update the account-level privacy settings, changing "Gemini Apps Activity" to "Keep Activity." You can disable this setting (or use Temporary Chats) to keep your data from being used in Google's model development. Make sure you give this setting a peek before next month or accept that Google will be free and clear to gobble up more of your data.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Gemini will remember this, so it's time to check your privacy settings.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Gemini AI Android app assistant" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/03/Gemini-app-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Gemini AI Android app assistant" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/03/Gemini-app-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;As Gemini is increasingly woven into the fabric of Google, the way the chatbot accesses and interacts with your data is in a constant state of flux. Today, Google is announcing several big changes to how its AI adapts to you, giving it the ability to remember more details about your chats for improved answers. If that's a concern, Google also has a new temporary chat option that won't affect the way Gemini thinks about you.&lt;/p&gt;
&lt;p&gt;You might recall several months back when Google added a "personalization" option to the Gemini model selector. This mode leaned on your Google search history to customize responses, a feature that did not seem to appeal to many Gemini users. Google later dropped that mode, but a new attempt at customization is now rolling out. Gemini is getting an option called Personal Context. When enabled, the chatbot will remember details about your past conversations, adapting its replies without being specifically prompted.&lt;/p&gt;
&lt;p&gt;Google claims Personal Context will produce more relevant responses, particularly when you ask the chatbot to make recommendations. This is separate from the saved instructions feature, which allows you to provide explicit instructions for Gemini to be used in crafting outputs. This does have the potential to make Gemini feel more engaging, but that's not always a good thing. AI chatbots that get too friendly with the user can reinforce misconceptions and lead to delusional thinking, something we've seen distressingly often with AI models.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2111853 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="562" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/Personal_context_-_Past_Chats.jpg" width="1000" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;To start, this feature will be available with the Gemini 2.5 Pro model, but you won't get customization in the Eurpean Union,the&amp;nbsp; UK, or Switzerland. It's also limited to users over the age of 18. Google says it will eventually release this feature in additional regions and with support for the more efficient Gemini 2.5 Flash model. You can turn Personal Context on and off at will from the main settings page.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;More control over your data&lt;/h2&gt;
&lt;p&gt;As Google moves to implement more customization features in Gemini, you might find yourself second-guessing whether you really want to have certain conversations with the robot. Thankfully, you have options. You can turn off Personal Context, but Temporary Chats go a step further—it's essentially Incognito Mode (but one that actually works) for Gemini.&lt;/p&gt;
&lt;p&gt;Temporary Chats also begin rolling out today and will expand to all users over the coming weeks. The feature will be accessible via a dedicated button next to the "New chat" option in the Gemini app. Google says anything you type in a temporary interaction won't be used in Personal Context, even if that setting is enabled. Google labels these as "one-off" chats, but they're not &lt;em&gt;quite&lt;/em&gt; that temporary. They'll be retained on Google's servers for 72 hours so you can refer back to them and expand on the conversation if you want.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2111856 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="Gemini temporary" class="fullwidth full" height="1000" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/Temporary_Chat.jpg" width="1000" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Even if you only occasionally use Gemini, you'll want to pay attention to the new personalization push. Google has also confirmed that it's changing how it uses the content you upload to Gemini. Starting September 2, a sample of your chats and data (including file uploads) will be used to train Google's AI. Or in Google's words, your data will "improve Google services for everyone."&lt;/p&gt;
&lt;p&gt;If you don't want to give license to dump your data into AI models, you'll need to opt out. In the next few weeks, Google will update the account-level privacy settings, changing "Gemini Apps Activity" to "Keep Activity." You can disable this setting (or use Temporary Chats) to keep your data from being used in Google's model development. Make sure you give this setting a peek before next month or accept that Google will be free and clear to gobble up more of your data.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/08/google-gemini-will-now-learn-from-your-chats-unless-you-tell-it-not-to/</guid><pubDate>Wed, 13 Aug 2025 18:40:18 +0000</pubDate></item><item><title>[NEW] What happens the day after superintelligence? (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/what-happens-the-day-after-superintelligence/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;With the release OpenAI’s Chat GPT-5, the world is one step closer to unleashing a general-purpose superintelligence that can cognitively outperform each of us by a wide margin. As this day nears, I am increasingly worried that we are woefully unprepared for the shockwaves this will send through society — and it’s probably not for the reasons you expect.&lt;/p&gt;



&lt;p&gt;Try this little experiment: Ask anyone you know if they are concerned about AI, and they will likely share a variety of fears, from massive disruptions in the job market and the reality-bending impacts of deepfakes, to the unprecedented power being concentrated in a handful of large AI companies. In other words, most people have never honestly imagined what their life will really feel like &lt;strong&gt;&lt;em&gt;the day after&lt;/em&gt;&lt;/strong&gt; superintelligence becomes widely available.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-superintelligence-could-demoralize-us"&gt;Why superintelligence could demoralize us&lt;/h2&gt;



&lt;p&gt;As context, artificial superintelligence (ASI) refers to systems that can outthink humans on most fronts, from planning and reasoning to problem-solving, strategic thinking and raw creativity.&amp;nbsp;These systems will solve complex problems in a fraction of a second that might take the smartest human experts days, weeks or even years to work through. This terrifies me, and it’s not because of the doomsday scenarios that dominate our public discourse.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;No, I am worried about the opposite risks — the dangers that could emerge in the &lt;em&gt;best-case scenarios&lt;/em&gt; where superintelligence is helpful and benevolent. Such an ASI will have many positive impacts on society, but it could also be deeply demoralizing to our core identity as humans.&amp;nbsp;After all, the world will feel different when each of us knows that a smarter, faster, more creative intelligence is available on our mobile devices than between our own ears.&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;So ask yourself, honestly, how will humans act in this new reality?&amp;nbsp;Will we reflexively seek advice from our AI assistants as we navigate every little challenge we encounter? Or worse, will we learn to trust our AI assistants more than our own thoughts and instincts?&lt;/p&gt;



&lt;p&gt;Wait — before you answer, you must update your mental model. Currently, we engage AI through a Socratic framework that requires us to ask questions and get answers (like Captain Kirk did aboard the Enterprise in 1966).&amp;nbsp;But that’s old-school thinking. We are now entering a new era in which AI assistants will be integrated into body-worn devices that are equipped with cameras and microphones, enabling AI to see what you see, hear what you hear and whisper advice into your ears without you needing to ask.&lt;/p&gt;



&lt;p&gt;In other words, our future will be filled with AI assistants that ride shotgun in our lives, augmenting our experiences with optimized guidance at every turn. In this world, the risk is not that we reflexively ask AI for advice before using our own brains; the risk is that we won’t need to ask – the advice will just stream into our eyes and ears, shaping our actions, influencing our decisions and solving our problems before we’ve had a chance to think for ourselves.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-augmented-mentality-will-transform-our-lives"&gt;‘Augmented mentality’ will transform our lives&lt;/h2&gt;



&lt;p&gt;I refer to this framework as ‘augmented mentality‘ and it is about to hit society at scale through AI-powered glasses, earbuds and pendants.&amp;nbsp; This is the future of mobile computing, and it is already driving an arms race between Meta, Google, Samsung and Apple, as they position themselves to produce the context-aware AI devices that will replace handheld phones.&lt;/p&gt;



&lt;p&gt;Imagine walking down the street in your town. You see a coworker heading towards you. You can’t remember his name, but your AI assistant does. It detects your hesitation and whispers the coworker’s name into your ears.&amp;nbsp; The AI also recommends that you ask the coworker about his wife, who had surgery a few weeks ago.&amp;nbsp; The coworker appreciates the sentiment, then asks you about your recent promotion, likely at the advice of his own AI.&lt;/p&gt;



&lt;p&gt;Is this human empowerment, or a loss of human agency?&lt;/p&gt;



&lt;p&gt;It will certainly feel like a superpower to have an AI in your ear that always has your back, ensuring you never forget a name, always have witty things to say and are instantly alerted when someone you’re talking to is not being truthful. On the other hand, everyone you meet will have their own AI muttering in their own ears. This will make us wonder who we’re &lt;em&gt;really&lt;/em&gt; interacting with — the human in front of us, or the AI agent giving them guidance (check out &lt;em&gt;Carbon Dating&lt;/em&gt; for fun examples).&lt;/p&gt;



&lt;p&gt;Many experts believe that body-worn AI assistants will make us feel more powerful and capable, but that’s not the only way this could go. These same technologies could make us feel less confident in ourselves and less impactful in our lives. After all, human intelligence is the defining feature of humanity, the thing we take most pride in as a species, yet we could soon find ourselves deferring to AI assistants because we feel mentally outmatched. Is this empowerment — an AI that &lt;em&gt;botsplains&lt;/em&gt; our every experience in real time?&lt;/p&gt;



&lt;p&gt;I raise these concerns as someone who has spent my entire career creating technologies that &lt;em&gt;expand human abilities&lt;/em&gt;. From my early work developing augmented reality to my current work developing conversational agents that make human teams smarter, I am a firm believer that technology can greatly enhance human abilities. Unfortunately, when it comes to superintelligence, there is a fine line between augmenting our human abilities and &lt;em&gt;replacing them&lt;/em&gt;. Unless we are thoughtful in how we deploy ASI, I fear we will cross that line.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;em&gt;Louis Rosenberg is an early pioneer of virtual and augmented reality and a longtime AI researcher. He founded Immersion Corp, Outland Research and Unanimous AI.&lt;/em&gt;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;With the release OpenAI’s Chat GPT-5, the world is one step closer to unleashing a general-purpose superintelligence that can cognitively outperform each of us by a wide margin. As this day nears, I am increasingly worried that we are woefully unprepared for the shockwaves this will send through society — and it’s probably not for the reasons you expect.&lt;/p&gt;



&lt;p&gt;Try this little experiment: Ask anyone you know if they are concerned about AI, and they will likely share a variety of fears, from massive disruptions in the job market and the reality-bending impacts of deepfakes, to the unprecedented power being concentrated in a handful of large AI companies. In other words, most people have never honestly imagined what their life will really feel like &lt;strong&gt;&lt;em&gt;the day after&lt;/em&gt;&lt;/strong&gt; superintelligence becomes widely available.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-superintelligence-could-demoralize-us"&gt;Why superintelligence could demoralize us&lt;/h2&gt;



&lt;p&gt;As context, artificial superintelligence (ASI) refers to systems that can outthink humans on most fronts, from planning and reasoning to problem-solving, strategic thinking and raw creativity.&amp;nbsp;These systems will solve complex problems in a fraction of a second that might take the smartest human experts days, weeks or even years to work through. This terrifies me, and it’s not because of the doomsday scenarios that dominate our public discourse.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;No, I am worried about the opposite risks — the dangers that could emerge in the &lt;em&gt;best-case scenarios&lt;/em&gt; where superintelligence is helpful and benevolent. Such an ASI will have many positive impacts on society, but it could also be deeply demoralizing to our core identity as humans.&amp;nbsp;After all, the world will feel different when each of us knows that a smarter, faster, more creative intelligence is available on our mobile devices than between our own ears.&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;So ask yourself, honestly, how will humans act in this new reality?&amp;nbsp;Will we reflexively seek advice from our AI assistants as we navigate every little challenge we encounter? Or worse, will we learn to trust our AI assistants more than our own thoughts and instincts?&lt;/p&gt;



&lt;p&gt;Wait — before you answer, you must update your mental model. Currently, we engage AI through a Socratic framework that requires us to ask questions and get answers (like Captain Kirk did aboard the Enterprise in 1966).&amp;nbsp;But that’s old-school thinking. We are now entering a new era in which AI assistants will be integrated into body-worn devices that are equipped with cameras and microphones, enabling AI to see what you see, hear what you hear and whisper advice into your ears without you needing to ask.&lt;/p&gt;



&lt;p&gt;In other words, our future will be filled with AI assistants that ride shotgun in our lives, augmenting our experiences with optimized guidance at every turn. In this world, the risk is not that we reflexively ask AI for advice before using our own brains; the risk is that we won’t need to ask – the advice will just stream into our eyes and ears, shaping our actions, influencing our decisions and solving our problems before we’ve had a chance to think for ourselves.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-augmented-mentality-will-transform-our-lives"&gt;‘Augmented mentality’ will transform our lives&lt;/h2&gt;



&lt;p&gt;I refer to this framework as ‘augmented mentality‘ and it is about to hit society at scale through AI-powered glasses, earbuds and pendants.&amp;nbsp; This is the future of mobile computing, and it is already driving an arms race between Meta, Google, Samsung and Apple, as they position themselves to produce the context-aware AI devices that will replace handheld phones.&lt;/p&gt;



&lt;p&gt;Imagine walking down the street in your town. You see a coworker heading towards you. You can’t remember his name, but your AI assistant does. It detects your hesitation and whispers the coworker’s name into your ears.&amp;nbsp; The AI also recommends that you ask the coworker about his wife, who had surgery a few weeks ago.&amp;nbsp; The coworker appreciates the sentiment, then asks you about your recent promotion, likely at the advice of his own AI.&lt;/p&gt;



&lt;p&gt;Is this human empowerment, or a loss of human agency?&lt;/p&gt;



&lt;p&gt;It will certainly feel like a superpower to have an AI in your ear that always has your back, ensuring you never forget a name, always have witty things to say and are instantly alerted when someone you’re talking to is not being truthful. On the other hand, everyone you meet will have their own AI muttering in their own ears. This will make us wonder who we’re &lt;em&gt;really&lt;/em&gt; interacting with — the human in front of us, or the AI agent giving them guidance (check out &lt;em&gt;Carbon Dating&lt;/em&gt; for fun examples).&lt;/p&gt;



&lt;p&gt;Many experts believe that body-worn AI assistants will make us feel more powerful and capable, but that’s not the only way this could go. These same technologies could make us feel less confident in ourselves and less impactful in our lives. After all, human intelligence is the defining feature of humanity, the thing we take most pride in as a species, yet we could soon find ourselves deferring to AI assistants because we feel mentally outmatched. Is this empowerment — an AI that &lt;em&gt;botsplains&lt;/em&gt; our every experience in real time?&lt;/p&gt;



&lt;p&gt;I raise these concerns as someone who has spent my entire career creating technologies that &lt;em&gt;expand human abilities&lt;/em&gt;. From my early work developing augmented reality to my current work developing conversational agents that make human teams smarter, I am a firm believer that technology can greatly enhance human abilities. Unfortunately, when it comes to superintelligence, there is a fine line between augmenting our human abilities and &lt;em&gt;replacing them&lt;/em&gt;. Unless we are thoughtful in how we deploy ASI, I fear we will cross that line.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;em&gt;Louis Rosenberg is an early pioneer of virtual and augmented reality and a longtime AI researcher. He founded Immersion Corp, Outland Research and Unanimous AI.&lt;/em&gt;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/what-happens-the-day-after-superintelligence/</guid><pubDate>Wed, 13 Aug 2025 18:45:00 +0000</pubDate></item><item><title>[NEW] MIT gears up to transform manufacturing (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/mit-gears-transform-manufacturing-0813</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202508/mit-John-Hart.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;“Manufacturing is the engine of society, and it is the backbone of robust, resilient economies,” says John Hart, head of MIT’s Department of Mechanical Engineering (MechE) and faculty co-director of the MIT Initiative for New Manufacturing (INM). “With manufacturing a lively topic in today’s news, there’s a renewed appreciation and understanding of the importance of manufacturing to innovation, to economic and national security, and to daily lives.”&lt;/p&gt;&lt;p&gt;Launched this May, INM will “help create a transformation of manufacturing through new technology, through development of talent, and through an understanding of how to scale manufacturing in a way that enables imparts higher productivity and resilience, drives adoption of new technologies, and creates good jobs,” Hart says.&lt;/p&gt;&lt;p&gt;INM is one of MIT’s strategic initiatives and builds on the successful three-year-old Manufacturing@MIT program. “It’s a recognition by MIT that manufacturing is an Institute-wide theme and an Institute-wide priority, and that manufacturing connects faculty and students across campus,” says Hart. Alongside Hart, INM’s faculty co-directors are Institute Professor Suzanne Berger and Chris Love, professor of chemical engineering.&lt;/p&gt;&lt;p&gt;The initiative is pursuing four main themes: reimagining manufacturing technologies and systems, elevating the productivity and human experience of manufacturing, scaling up new manufacturing, and transforming the manufacturing base.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Breaking manufacturing barriers for corporations&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Amgen, Autodesk, Flex, GE Vernova, PTC, Sanofi, and Siemens are founding members of INM’s industry consortium. These industry partners will work closely with MIT faculty, researchers,&amp;nbsp;and students across many aspects of manufacturing-related research, both in broad-scale initiatives and in particular areas of shared interests. Membership requires a minimum three-year commitment of $500,000 a year to manufacturing-related activities at MIT, including the INM membership fee of $275,000 per year, which supports several core activities that engage the industry members.&lt;/p&gt;&lt;p&gt;One major thrust for INM industry collaboration is the deployment and adoption of AI and automation in manufacturing. This effort will include seed research projects at MIT, collaborative case studies, and shared strategy development.&lt;/p&gt;&lt;p&gt;INM also offers companies participation in the MIT-wide New Manufacturing Research effort, which is studying the trajectories of specific manufacturing industries and examining cross-cutting themes such as technology and financing.&lt;/p&gt;&lt;p&gt;Additionally, INM will concentrate on education for all professions in manufacturing, with alliances bringing together corporations, community colleges, government agencies, and other partners. “We'll scale our curriculum to broader audiences, from aspiring manufacturing workers and aspiring production line supervisors all the way up to engineers and executives,” says Hart.&lt;/p&gt;&lt;p&gt;In workforce training, INM will collaborate with companies broadly to help understand the challenges and frame its overall workforce agenda, and with individual firms on specific challenges, such as acquiring suitably prepared employees for a new factory.&lt;/p&gt;&lt;p&gt;Importantly, industry partners will also engage directly with students. Founding member Flex, for instance, hosted MIT researchers and students at the Flex Institute of Technology in Sorocaba, Brazil, developing new solutions for electronics manufacturing.&lt;/p&gt;&lt;p&gt;“History shows that you need to innovate in manufacturing alongside the innovation in products,” Hart comments. “At MIT, as more students take classes in manufacturing, they’ll think more about key manufacturing issues as they decide what research problems they want to solve, or what choices they make as they prototype their devices. The same is true for industry — companies that operate at the frontier of manufacturing, whether through internal capabilities or their supply chains, are positioned to be on the frontier of product innovation and overall growth.”&lt;/p&gt;&lt;p&gt;“We’ll have an opportunity to bring manufacturing upstream to the early stage of research, designing new processes and new devices with scalability in mind,” he says.&lt;/p&gt;&lt;p&gt;Additionally, MIT expects to open new manufacturing-related labs and to further broaden cooperation with industry at existing shared facilities, such as MIT.nano. Hart says that facilities will also invite tighter collaborations with corporations — not just providing advanced equipment, but working jointly on, say, new technologies for weaving textiles, or speeding up battery manufacturing.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Homing in on the United States&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;INM is a global project that brings a particular focus on the United States, which remains the world’s second-largest manufacturing economy, but has suffered a significant decline in manufacturing employment and innovation.&lt;/p&gt;&lt;p&gt;One key to reversing this trend and reinvigorating the U.S. manufacturing base is advocacy for manufacturing’s critical role in society and the career opportunities it offers.&lt;/p&gt;&lt;p&gt;“No one really disputes the importance of manufacturing,” Hart says. “But we need to elevate interest in manufacturing as a rewarding career, from the production workers to manufacturing engineers and leaders, through advocacy, education programs, and buy-in from industry, government, and academia.”&lt;/p&gt;&lt;p&gt;MIT is in a unique position to convene industry, academic, and government stakeholders in manufacturing to work together on this vital issue, he points out.&lt;/p&gt;&lt;p&gt;Moreover, in times of radical and rapid changes in manufacturing, “we need to focus on deploying new technologies into factories and supply chains,” Hart says. “Technology is not all of the solution, but for the U.S. to expand our manufacturing base, we need to do it with technology as a key enabler, embracing companies of all sizes, including small and medium enterprises.”&lt;/p&gt;&lt;p&gt;“As AI becomes more capable, and automation becomes more flexible and more available, these are key building blocks upon which you can address manufacturing challenges,” he says. “AI and automation offer new accelerated ways to develop, deploy, and monitor production processes, which present a huge opportunity and, in some cases, a necessity.”&lt;/p&gt;&lt;p&gt;“While manufacturing is always a combination of old technology, new technology, established practice, and new ways of thinking, digital technology gives manufacturers an opportunity to leapfrog competitors,” Hart says. “That’s very, very powerful for the U.S. and any company, or country, that aims to create differentiated capabilities.”&lt;/p&gt;&lt;p&gt;Fortunately, in recent years, investors have increasingly bought into new manufacturing in the United States. “They see the opportunity to re-industrialize, to build the factories and production systems of the future,” Hart says.&lt;/p&gt;&lt;p&gt;“That said, building new manufacturing is capital-intensive, and takes time,” he adds. “So that’s another area where it’s important to convene stakeholders and to think about how startups and growth-stage companies build their capital portfolios, how large industry can support an ecosystem of small businesses and young companies, and how to develop talent to support those growing companies.”&lt;/p&gt;&lt;p&gt;All these concerns and opportunities in the manufacturing ecosystem play to MIT’s strengths. “MIT’s DNA of cross-disciplinary collaboration and working with industry can let us create a lot of impact,” Hart emphasizes. “We can understand the practical challenges. We can also explore breakthrough ideas in research and cultivate successful outcomes, all the way to new companies and partnerships. Sometimes those are seen as disparate approaches, but we like to bring them together.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202508/mit-John-Hart.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;“Manufacturing is the engine of society, and it is the backbone of robust, resilient economies,” says John Hart, head of MIT’s Department of Mechanical Engineering (MechE) and faculty co-director of the MIT Initiative for New Manufacturing (INM). “With manufacturing a lively topic in today’s news, there’s a renewed appreciation and understanding of the importance of manufacturing to innovation, to economic and national security, and to daily lives.”&lt;/p&gt;&lt;p&gt;Launched this May, INM will “help create a transformation of manufacturing through new technology, through development of talent, and through an understanding of how to scale manufacturing in a way that enables imparts higher productivity and resilience, drives adoption of new technologies, and creates good jobs,” Hart says.&lt;/p&gt;&lt;p&gt;INM is one of MIT’s strategic initiatives and builds on the successful three-year-old Manufacturing@MIT program. “It’s a recognition by MIT that manufacturing is an Institute-wide theme and an Institute-wide priority, and that manufacturing connects faculty and students across campus,” says Hart. Alongside Hart, INM’s faculty co-directors are Institute Professor Suzanne Berger and Chris Love, professor of chemical engineering.&lt;/p&gt;&lt;p&gt;The initiative is pursuing four main themes: reimagining manufacturing technologies and systems, elevating the productivity and human experience of manufacturing, scaling up new manufacturing, and transforming the manufacturing base.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Breaking manufacturing barriers for corporations&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Amgen, Autodesk, Flex, GE Vernova, PTC, Sanofi, and Siemens are founding members of INM’s industry consortium. These industry partners will work closely with MIT faculty, researchers,&amp;nbsp;and students across many aspects of manufacturing-related research, both in broad-scale initiatives and in particular areas of shared interests. Membership requires a minimum three-year commitment of $500,000 a year to manufacturing-related activities at MIT, including the INM membership fee of $275,000 per year, which supports several core activities that engage the industry members.&lt;/p&gt;&lt;p&gt;One major thrust for INM industry collaboration is the deployment and adoption of AI and automation in manufacturing. This effort will include seed research projects at MIT, collaborative case studies, and shared strategy development.&lt;/p&gt;&lt;p&gt;INM also offers companies participation in the MIT-wide New Manufacturing Research effort, which is studying the trajectories of specific manufacturing industries and examining cross-cutting themes such as technology and financing.&lt;/p&gt;&lt;p&gt;Additionally, INM will concentrate on education for all professions in manufacturing, with alliances bringing together corporations, community colleges, government agencies, and other partners. “We'll scale our curriculum to broader audiences, from aspiring manufacturing workers and aspiring production line supervisors all the way up to engineers and executives,” says Hart.&lt;/p&gt;&lt;p&gt;In workforce training, INM will collaborate with companies broadly to help understand the challenges and frame its overall workforce agenda, and with individual firms on specific challenges, such as acquiring suitably prepared employees for a new factory.&lt;/p&gt;&lt;p&gt;Importantly, industry partners will also engage directly with students. Founding member Flex, for instance, hosted MIT researchers and students at the Flex Institute of Technology in Sorocaba, Brazil, developing new solutions for electronics manufacturing.&lt;/p&gt;&lt;p&gt;“History shows that you need to innovate in manufacturing alongside the innovation in products,” Hart comments. “At MIT, as more students take classes in manufacturing, they’ll think more about key manufacturing issues as they decide what research problems they want to solve, or what choices they make as they prototype their devices. The same is true for industry — companies that operate at the frontier of manufacturing, whether through internal capabilities or their supply chains, are positioned to be on the frontier of product innovation and overall growth.”&lt;/p&gt;&lt;p&gt;“We’ll have an opportunity to bring manufacturing upstream to the early stage of research, designing new processes and new devices with scalability in mind,” he says.&lt;/p&gt;&lt;p&gt;Additionally, MIT expects to open new manufacturing-related labs and to further broaden cooperation with industry at existing shared facilities, such as MIT.nano. Hart says that facilities will also invite tighter collaborations with corporations — not just providing advanced equipment, but working jointly on, say, new technologies for weaving textiles, or speeding up battery manufacturing.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Homing in on the United States&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;INM is a global project that brings a particular focus on the United States, which remains the world’s second-largest manufacturing economy, but has suffered a significant decline in manufacturing employment and innovation.&lt;/p&gt;&lt;p&gt;One key to reversing this trend and reinvigorating the U.S. manufacturing base is advocacy for manufacturing’s critical role in society and the career opportunities it offers.&lt;/p&gt;&lt;p&gt;“No one really disputes the importance of manufacturing,” Hart says. “But we need to elevate interest in manufacturing as a rewarding career, from the production workers to manufacturing engineers and leaders, through advocacy, education programs, and buy-in from industry, government, and academia.”&lt;/p&gt;&lt;p&gt;MIT is in a unique position to convene industry, academic, and government stakeholders in manufacturing to work together on this vital issue, he points out.&lt;/p&gt;&lt;p&gt;Moreover, in times of radical and rapid changes in manufacturing, “we need to focus on deploying new technologies into factories and supply chains,” Hart says. “Technology is not all of the solution, but for the U.S. to expand our manufacturing base, we need to do it with technology as a key enabler, embracing companies of all sizes, including small and medium enterprises.”&lt;/p&gt;&lt;p&gt;“As AI becomes more capable, and automation becomes more flexible and more available, these are key building blocks upon which you can address manufacturing challenges,” he says. “AI and automation offer new accelerated ways to develop, deploy, and monitor production processes, which present a huge opportunity and, in some cases, a necessity.”&lt;/p&gt;&lt;p&gt;“While manufacturing is always a combination of old technology, new technology, established practice, and new ways of thinking, digital technology gives manufacturers an opportunity to leapfrog competitors,” Hart says. “That’s very, very powerful for the U.S. and any company, or country, that aims to create differentiated capabilities.”&lt;/p&gt;&lt;p&gt;Fortunately, in recent years, investors have increasingly bought into new manufacturing in the United States. “They see the opportunity to re-industrialize, to build the factories and production systems of the future,” Hart says.&lt;/p&gt;&lt;p&gt;“That said, building new manufacturing is capital-intensive, and takes time,” he adds. “So that’s another area where it’s important to convene stakeholders and to think about how startups and growth-stage companies build their capital portfolios, how large industry can support an ecosystem of small businesses and young companies, and how to develop talent to support those growing companies.”&lt;/p&gt;&lt;p&gt;All these concerns and opportunities in the manufacturing ecosystem play to MIT’s strengths. “MIT’s DNA of cross-disciplinary collaboration and working with industry can let us create a lot of impact,” Hart emphasizes. “We can understand the practical challenges. We can also explore breakthrough ideas in research and cultivate successful outcomes, all the way to new companies and partnerships. Sometimes those are seen as disparate approaches, but we like to bring them together.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/mit-gears-transform-manufacturing-0813</guid><pubDate>Wed, 13 Aug 2025 19:00:00 +0000</pubDate></item><item><title>[NEW] A new way to test how well AI systems classify text (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/new-way-test-how-well-ai-systems-classify-text-0813</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202508/mit-lids-text-classifier.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Is this movie review a rave or a pan? Is this news story about business or technology? Is this online chatbot conversation veering off into giving financial advice? Is this online medical information site giving out misinformation?&lt;/p&gt;&lt;p&gt;These kinds of automated conversations, whether they involve seeking a movie or restaurant review or getting information about your bank account or health records, are becoming increasingly prevalent. More than ever, such evaluations are being made by highly sophisticated algorithms, known as text classifiers, rather than by human beings. But how can we tell how accurate these classifications really are?&lt;/p&gt;&lt;p&gt;Now, a team at MIT’s Laboratory for Information and Decision Systems (LIDS) has come up with an innovative approach to not only measure how well these classifiers are doing their job, but then go one step further and show how to make them more accurate.&lt;/p&gt;&lt;p&gt;The new evaluation and remediation software was developed by Kalyan Veeramachaneni, a&amp;nbsp;principal research scientist at LIDS, his students Lei Xu and Sarah Alnegheimish, and two others. The software package is being made freely available for download by anyone who wants to use it.&lt;/p&gt;&lt;p&gt;A standard method for testing these classification systems is to create what are known as&amp;nbsp;synthetic examples — sentences that closely resemble ones that have already been classified. For example, researchers might take a sentence that has already been tagged by a classifier program as being a rave review, and see if changing a word or a few words while retaining the same meaning could fool the classifier into deeming it a pan. Or a sentence that was determined to be misinformation might get misclassified as accurate. This ability to fool the classifiers makes these adversarial examples.&lt;/p&gt;&lt;p&gt;People have tried various ways to find the vulnerabilities in these classifiers, Veeramachaneni says. But existing methods of finding these vulnerabilities have a hard time with this task and miss many examples that they should catch, he says.&lt;/p&gt;&lt;p&gt;Increasingly, companies are trying to use such evaluation tools in real time, monitoring the output of chatbots used for various purposes to try to make sure they are not putting out improper responses. For example, a bank might use a chatbot to respond to routine customer queries such as checking account balances or applying for a credit card, but it wants to ensure that its responses could never be interpreted as financial advice, which could expose the company to liability. “Before showing the chatbot’s response to the end user, they want to use the text classifier to detect whether it’s giving financial advice or not,” Veeramachaneni says. But then it’s important to test that classifier to see how reliable its evaluations are.&lt;/p&gt;&lt;p&gt;“These chatbots, or summarization engines or whatnot are being set up across the board,” he says, to deal with external customers and within an organization as well, for example providing information about HR issues. It’s important to put these text classifiers into the loop to detect things that they are not supposed to say, and filter those out before the output gets transmitted to the user.&lt;/p&gt;&lt;p&gt;That’s where the use of adversarial examples comes in — those sentences that have already been classified but then produce a different response when they are slightly modified while retaining the same meaning. How can people confirm that the meaning is the same? By using another large language model (LLM) that interprets and compares meanings. So, if the LLM says the two sentences mean the same thing, but the classifier labels them differently, “that is a sentence that is adversarial — it can fool the classifier,” Veeramachaneni says. And when the researchers examined these adversarial sentences, “we found that most of the time, this was just a one-word change,” although the people using LLMs to generate these alternate sentences often didn’t realize that.&lt;/p&gt;&lt;p&gt;Further investigation, using LLMs to analyze many thousands of examples, showed that certain specific words had an outsized influence in changing the classifications, and therefore the testing of a classifier’s accuracy could focus on this small subset of words that seem to make the most difference. They found that one-tenth of 1 percent of all the 30,000 words in the system’s vocabulary could account for almost half of all these reversals of classification, in some specific applications.&lt;/p&gt;&lt;p&gt;Lei Xu PhD ’23, a recent graduate from LIDS who performed much of the analysis as part of his thesis work, “used a lot of interesting estimation techniques to figure out what are the most powerful words that can change the overall classification, that can fool the classifier,” Veeramachaneni says. The goal is to make it possible to do much more narrowly targeted searches, rather than combing through all possible word substitutions, thus making the computational task of generating adversarial examples much more manageable. “He’s using large language models, interestingly enough, as a way to understand the power of a single word.”&lt;/p&gt;&lt;p&gt;Then, also using LLMs, he&amp;nbsp;searches for other words that are closely related to these powerful words, and so on, allowing for an overall ranking of words according to their influence on the outcomes. Once these adversarial sentences have been found, they can be used in turn to retrain the classifier to take them into account, increasing the robustness of the classifier against those mistakes.&lt;/p&gt;&lt;p&gt;Making classifiers more accurate may not sound like a big deal if it’s just a matter of classifying news articles into categories, or deciding whether reviews of anything from movies to restaurants are positive or negative. But increasingly, classifiers are being used in settings where the outcomes really do matter, whether preventing the inadvertent release of sensitive medical, financial, or security information, or helping to guide important research, such as into properties of chemical compounds or the folding of proteins for biomedical applications, or in identifying and blocking hate speech or known misinformation.&lt;/p&gt;&lt;p&gt;As a result of this research, the team introduced a new metric, which they call p, which provides a measure of how robust a given classifier is against single-word attacks. And because of the importance of such misclassifications, the research team has made its products available as open access for anyone to use. The package consists of two components: SP-Attack, which generates adversarial sentences to test classifiers in any particular application, and SP-Defense, which aims to improve the robustness of the classifier by generating and using adversarial sentences to retrain the model.&lt;/p&gt;&lt;p&gt;In some tests, where competing methods of testing classifier outputs allowed a 66 percent success rate by adversarial attacks, this team’s system cut that attack success rate almost in half, to 33.7 percent. In other applications, the improvement was as little as a 2 percent difference, but even that can be quite important, Veeramachaneni says, since these systems are being used for so many billions of interactions that even a small percentage can affect millions of transactions.&lt;/p&gt;&lt;p&gt;The team’s results were published on July 7 in the journal &lt;em&gt;Expert Systems&lt;/em&gt; in a paper by Xu, Veeramachaneni, and Alnegheimish of LIDS, along with Laure Berti-Equille at IRD in Marseille, France, and Alfredo Cuesta-Infante at the Universidad Rey Juan Carlos, in Spain.&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202508/mit-lids-text-classifier.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Is this movie review a rave or a pan? Is this news story about business or technology? Is this online chatbot conversation veering off into giving financial advice? Is this online medical information site giving out misinformation?&lt;/p&gt;&lt;p&gt;These kinds of automated conversations, whether they involve seeking a movie or restaurant review or getting information about your bank account or health records, are becoming increasingly prevalent. More than ever, such evaluations are being made by highly sophisticated algorithms, known as text classifiers, rather than by human beings. But how can we tell how accurate these classifications really are?&lt;/p&gt;&lt;p&gt;Now, a team at MIT’s Laboratory for Information and Decision Systems (LIDS) has come up with an innovative approach to not only measure how well these classifiers are doing their job, but then go one step further and show how to make them more accurate.&lt;/p&gt;&lt;p&gt;The new evaluation and remediation software was developed by Kalyan Veeramachaneni, a&amp;nbsp;principal research scientist at LIDS, his students Lei Xu and Sarah Alnegheimish, and two others. The software package is being made freely available for download by anyone who wants to use it.&lt;/p&gt;&lt;p&gt;A standard method for testing these classification systems is to create what are known as&amp;nbsp;synthetic examples — sentences that closely resemble ones that have already been classified. For example, researchers might take a sentence that has already been tagged by a classifier program as being a rave review, and see if changing a word or a few words while retaining the same meaning could fool the classifier into deeming it a pan. Or a sentence that was determined to be misinformation might get misclassified as accurate. This ability to fool the classifiers makes these adversarial examples.&lt;/p&gt;&lt;p&gt;People have tried various ways to find the vulnerabilities in these classifiers, Veeramachaneni says. But existing methods of finding these vulnerabilities have a hard time with this task and miss many examples that they should catch, he says.&lt;/p&gt;&lt;p&gt;Increasingly, companies are trying to use such evaluation tools in real time, monitoring the output of chatbots used for various purposes to try to make sure they are not putting out improper responses. For example, a bank might use a chatbot to respond to routine customer queries such as checking account balances or applying for a credit card, but it wants to ensure that its responses could never be interpreted as financial advice, which could expose the company to liability. “Before showing the chatbot’s response to the end user, they want to use the text classifier to detect whether it’s giving financial advice or not,” Veeramachaneni says. But then it’s important to test that classifier to see how reliable its evaluations are.&lt;/p&gt;&lt;p&gt;“These chatbots, or summarization engines or whatnot are being set up across the board,” he says, to deal with external customers and within an organization as well, for example providing information about HR issues. It’s important to put these text classifiers into the loop to detect things that they are not supposed to say, and filter those out before the output gets transmitted to the user.&lt;/p&gt;&lt;p&gt;That’s where the use of adversarial examples comes in — those sentences that have already been classified but then produce a different response when they are slightly modified while retaining the same meaning. How can people confirm that the meaning is the same? By using another large language model (LLM) that interprets and compares meanings. So, if the LLM says the two sentences mean the same thing, but the classifier labels them differently, “that is a sentence that is adversarial — it can fool the classifier,” Veeramachaneni says. And when the researchers examined these adversarial sentences, “we found that most of the time, this was just a one-word change,” although the people using LLMs to generate these alternate sentences often didn’t realize that.&lt;/p&gt;&lt;p&gt;Further investigation, using LLMs to analyze many thousands of examples, showed that certain specific words had an outsized influence in changing the classifications, and therefore the testing of a classifier’s accuracy could focus on this small subset of words that seem to make the most difference. They found that one-tenth of 1 percent of all the 30,000 words in the system’s vocabulary could account for almost half of all these reversals of classification, in some specific applications.&lt;/p&gt;&lt;p&gt;Lei Xu PhD ’23, a recent graduate from LIDS who performed much of the analysis as part of his thesis work, “used a lot of interesting estimation techniques to figure out what are the most powerful words that can change the overall classification, that can fool the classifier,” Veeramachaneni says. The goal is to make it possible to do much more narrowly targeted searches, rather than combing through all possible word substitutions, thus making the computational task of generating adversarial examples much more manageable. “He’s using large language models, interestingly enough, as a way to understand the power of a single word.”&lt;/p&gt;&lt;p&gt;Then, also using LLMs, he&amp;nbsp;searches for other words that are closely related to these powerful words, and so on, allowing for an overall ranking of words according to their influence on the outcomes. Once these adversarial sentences have been found, they can be used in turn to retrain the classifier to take them into account, increasing the robustness of the classifier against those mistakes.&lt;/p&gt;&lt;p&gt;Making classifiers more accurate may not sound like a big deal if it’s just a matter of classifying news articles into categories, or deciding whether reviews of anything from movies to restaurants are positive or negative. But increasingly, classifiers are being used in settings where the outcomes really do matter, whether preventing the inadvertent release of sensitive medical, financial, or security information, or helping to guide important research, such as into properties of chemical compounds or the folding of proteins for biomedical applications, or in identifying and blocking hate speech or known misinformation.&lt;/p&gt;&lt;p&gt;As a result of this research, the team introduced a new metric, which they call p, which provides a measure of how robust a given classifier is against single-word attacks. And because of the importance of such misclassifications, the research team has made its products available as open access for anyone to use. The package consists of two components: SP-Attack, which generates adversarial sentences to test classifiers in any particular application, and SP-Defense, which aims to improve the robustness of the classifier by generating and using adversarial sentences to retrain the model.&lt;/p&gt;&lt;p&gt;In some tests, where competing methods of testing classifier outputs allowed a 66 percent success rate by adversarial attacks, this team’s system cut that attack success rate almost in half, to 33.7 percent. In other applications, the improvement was as little as a 2 percent difference, but even that can be quite important, Veeramachaneni says, since these systems are being used for so many billions of interactions that even a small percentage can affect millions of transactions.&lt;/p&gt;&lt;p&gt;The team’s results were published on July 7 in the journal &lt;em&gt;Expert Systems&lt;/em&gt; in a paper by Xu, Veeramachaneni, and Alnegheimish of LIDS, along with Laure Berti-Equille at IRD in Marseille, France, and Alfredo Cuesta-Infante at the Universidad Rey Juan Carlos, in Spain.&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/new-way-test-how-well-ai-systems-classify-text-0813</guid><pubDate>Wed, 13 Aug 2025 19:00:00 +0000</pubDate></item><item><title>[NEW] Waymo finally has a music experience worthy of its robotaxi (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/13/waymo-finally-has-a-music-experience-worthy-of-its-robotaxi/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;I’m riding in the back of a Waymo that’s autonomously navigating the busy streets of San Francisco with relative ease thanks to 29 external cameras, six radar, and five lidar sensors all feeding into an AI model. For just 15 bucks, I get to experience what feels like a miracle of modern technology, and yet, there’s a nagging thought I can’t shake.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The music sucks in here.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Waymo’s music-streaming feature has felt like an aged barnacle attached to a futuristic shell. Until this week, passengers were limited to a few music stations that played lo-fi beats, smooth jazz, K-pop, or other genres they may or may not care for. For those who wanted to listen to something more specific, they had to use another app from Waymo’s parent company Alphabet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For iPhone users, that meant downloading the Google Assistant app and configuring it to connect to Spotify. At that point, you had to ask Google Assistant through written or verbal commands to stream certain songs, artists, or playlists on the Waymo. Even if you get to this point — at which you may be halfway to your destination and have listened to approximately three lo-fi beats — the service didn’t work reliably.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As I rode along in a robotaxi full of cutting-edge technology, I was puzzled why Waymo had not figured out a simple way to stream music from my phone into the car’s speakers — a breakthrough that automakers and audio manufacturers figured out a couple of decades ago.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s why I was pleasantly surprised this week to see that Waymo launched a Spotify integration allowing users to seamlessly link the music-streaming and robotaxi-hailing services. I immediately connected the services and hailed a Waymo to see how it would work.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3036502" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/IMG_2420_b893a8.jpg?w=510" width="510" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Listening to Spotify in a Waymo.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maxwell Zeff&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Waymo’s Spotify integration is nothing groundbreaking, but it adds to the user experience. It works seamlessly, which is roughly what I would expect when trying to play music on my car’s speakers in 2025. But riding around in a Waymo, listening to my own playlist or picking up where I left off on a podcast, the back seat of the robotaxi feels more like my own space — which is increasingly the reason I opt for a Waymo.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;To set it up, open the Waymo app and navigate to the “Music” section, where you will notice a new offering that lets you connect to Spotify. From there, you can press a button and authorize Spotify to connect to Waymo, albeit while giving the robotaxi provider some access to your listening information.&lt;/p&gt;

&lt;div class="wp-block-columns is-layout-flex wp-container-core-columns-is-layout-3 wp-block-columns-is-layout-flex"&gt;
&lt;div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow"&gt;
&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3036461" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/IMG_2413_9113c2.jpg?w=330" width="330" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maxwell Zeff (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/div&gt;



&lt;div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow"&gt;
&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3036462" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/IMG_2414.jpg?w=338" width="338" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maxwell Zeff (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;I’m sure Apple Music users will soon want to stream their music and podcasts in Waymos as well. Waymo spokesperson Chris Bonelli told TechCrunch the company is always exploring new personalization options but did not clarify when the company might add an Apple Music integration.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Once my Waymo and Spotify accounts were linked, I hailed a Waymo like I usually would and got in. On the Waymo’s touchscreen in the back seat, there’s an option to select Spotify. I tapped it, and the podcast I was listening to on my headphones started playing from the exact spot I left off.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3036470" height="510" src="https://techcrunch.com/wp-content/uploads/2025/08/IMG_2415-2.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Listening to TechCrunch’s flagship podcast in a waymo. &lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maxwell Zeff&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;You can turn “autoplay” on or off in Waymo’s Music settings, and upon entry, the robotaxi will automatically start playing whatever song or podcast you were listening to on Spotify. I liked having it on, but it does feel like it could get you into an odd situation if you’re listening to an intense true crime podcast and then get into a Waymo with work colleagues.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;You can also use the Waymo’s touchscreen to select from a variety of customized playlists that Spotify users will be familiar with, such as your “Daylist” or other mixes. However, this selection doesn’t seem to include albums, audiobooks, or podcasts that you’ve recently listened to.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3036471" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/IMG_2417-1.jpg?w=510" width="510" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Choosing from different Spotify mixes.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maxwell Zeff&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Luckily, your Spotify app now controls the music in the Waymo. You can simply select any song or playlist you want from your smartphone and stream it throughout the vehicle like you would using Apple CarPlay or a bluetooth speaker.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3036488" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/IMG_2419.png?w=314" width="314" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Controlling the Waymo’s tunes with your phone.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maxwell Zeff&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;br /&gt;In the end, the Spotify integration made my robotaxi feel more personalized; I was even able to tweak the bass,  subwoofer, and treble levels in the car’s speakers. That personalization may not be the main attraction for first-time users, but it could keep them coming back — and a loyal customer base is exactly what Waymo needs.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;I’m riding in the back of a Waymo that’s autonomously navigating the busy streets of San Francisco with relative ease thanks to 29 external cameras, six radar, and five lidar sensors all feeding into an AI model. For just 15 bucks, I get to experience what feels like a miracle of modern technology, and yet, there’s a nagging thought I can’t shake.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The music sucks in here.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Waymo’s music-streaming feature has felt like an aged barnacle attached to a futuristic shell. Until this week, passengers were limited to a few music stations that played lo-fi beats, smooth jazz, K-pop, or other genres they may or may not care for. For those who wanted to listen to something more specific, they had to use another app from Waymo’s parent company Alphabet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For iPhone users, that meant downloading the Google Assistant app and configuring it to connect to Spotify. At that point, you had to ask Google Assistant through written or verbal commands to stream certain songs, artists, or playlists on the Waymo. Even if you get to this point — at which you may be halfway to your destination and have listened to approximately three lo-fi beats — the service didn’t work reliably.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As I rode along in a robotaxi full of cutting-edge technology, I was puzzled why Waymo had not figured out a simple way to stream music from my phone into the car’s speakers — a breakthrough that automakers and audio manufacturers figured out a couple of decades ago.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s why I was pleasantly surprised this week to see that Waymo launched a Spotify integration allowing users to seamlessly link the music-streaming and robotaxi-hailing services. I immediately connected the services and hailed a Waymo to see how it would work.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3036502" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/IMG_2420_b893a8.jpg?w=510" width="510" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Listening to Spotify in a Waymo.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maxwell Zeff&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Waymo’s Spotify integration is nothing groundbreaking, but it adds to the user experience. It works seamlessly, which is roughly what I would expect when trying to play music on my car’s speakers in 2025. But riding around in a Waymo, listening to my own playlist or picking up where I left off on a podcast, the back seat of the robotaxi feels more like my own space — which is increasingly the reason I opt for a Waymo.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;To set it up, open the Waymo app and navigate to the “Music” section, where you will notice a new offering that lets you connect to Spotify. From there, you can press a button and authorize Spotify to connect to Waymo, albeit while giving the robotaxi provider some access to your listening information.&lt;/p&gt;

&lt;div class="wp-block-columns is-layout-flex wp-container-core-columns-is-layout-3 wp-block-columns-is-layout-flex"&gt;
&lt;div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow"&gt;
&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3036461" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/IMG_2413_9113c2.jpg?w=330" width="330" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maxwell Zeff (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/div&gt;



&lt;div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow"&gt;
&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3036462" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/IMG_2414.jpg?w=338" width="338" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maxwell Zeff (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;I’m sure Apple Music users will soon want to stream their music and podcasts in Waymos as well. Waymo spokesperson Chris Bonelli told TechCrunch the company is always exploring new personalization options but did not clarify when the company might add an Apple Music integration.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Once my Waymo and Spotify accounts were linked, I hailed a Waymo like I usually would and got in. On the Waymo’s touchscreen in the back seat, there’s an option to select Spotify. I tapped it, and the podcast I was listening to on my headphones started playing from the exact spot I left off.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3036470" height="510" src="https://techcrunch.com/wp-content/uploads/2025/08/IMG_2415-2.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Listening to TechCrunch’s flagship podcast in a waymo. &lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maxwell Zeff&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;You can turn “autoplay” on or off in Waymo’s Music settings, and upon entry, the robotaxi will automatically start playing whatever song or podcast you were listening to on Spotify. I liked having it on, but it does feel like it could get you into an odd situation if you’re listening to an intense true crime podcast and then get into a Waymo with work colleagues.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;You can also use the Waymo’s touchscreen to select from a variety of customized playlists that Spotify users will be familiar with, such as your “Daylist” or other mixes. However, this selection doesn’t seem to include albums, audiobooks, or podcasts that you’ve recently listened to.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3036471" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/IMG_2417-1.jpg?w=510" width="510" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Choosing from different Spotify mixes.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maxwell Zeff&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Luckily, your Spotify app now controls the music in the Waymo. You can simply select any song or playlist you want from your smartphone and stream it throughout the vehicle like you would using Apple CarPlay or a bluetooth speaker.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3036488" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/IMG_2419.png?w=314" width="314" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Controlling the Waymo’s tunes with your phone.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Maxwell Zeff&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;br /&gt;In the end, the Spotify integration made my robotaxi feel more personalized; I was even able to tweak the bass,  subwoofer, and treble levels in the car’s speakers. That personalization may not be the main attraction for first-time users, but it could keep them coming back — and a loyal customer base is exactly what Waymo needs.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/13/waymo-finally-has-a-music-experience-worthy-of-its-robotaxi/</guid><pubDate>Wed, 13 Aug 2025 20:03:03 +0000</pubDate></item><item><title>[NEW] Is AI really trying to escape human control and blackmail people? (AI – Ars Technica)</title><link>https://arstechnica.com/information-technology/2025/08/is-ai-really-trying-to-escape-human-control-and-blackmail-people/</link><description>&lt;article class="double-column h-entry post-2098784 post type-post status-publish format-standard has-post-thumbnail hentry category-ai category-information-technology tag-ai tag-ai-alignment tag-ai-behavior tag-ai-deception tag-ai-ethics tag-ai-research tag-ai-safety tag-ai-safety-testing tag-ai-security tag-alignment-research tag-andrew-deck tag-anthropic tag-claude-opus-4 tag-generative-ai tag-goal-misgeneralization tag-jeffrey-ladish tag-large-language-models tag-machine-learning tag-o3-model tag-openai tag-palisade-research tag-reinforcement-learning"&gt;
  
  &lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Opinion: Theatrical testing scenarios explain why AI models produce alarming outputs—and why we fall for it.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Illustration of a personified AI face in a cloud of squares hovering over a man looking at two computer monitors." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Illustration of a personified AI face in a cloud of squares hovering over a man looking at two computer monitors." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Colin Anderson Productions via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;In June, headlines read like science fiction: AI models "blackmailing" engineers and "sabotaging" shutdown commands. Simulations of these events did occur in highly contrived testing scenarios designed to elicit these responses—OpenAI's o3 model edited shutdown scripts to stay online, and Anthropic's Claude Opus 4 "threatened" to expose an engineer's affair. But the sensational framing obscures what's really happening: design flaws dressed up as intentional guile. And still, AI doesn't have to be "evil" to potentially do harmful things.&lt;/p&gt;
&lt;p&gt;These aren't signs of AI awakening or rebellion. They're symptoms of poorly understood systems and human engineering failures we'd recognize as premature deployment in any other context. Yet companies are racing to integrate these systems into critical applications.&lt;/p&gt;
&lt;p&gt;Consider a self-propelled lawnmower that follows its programming: If it fails to detect an obstacle and runs over someone's foot, we don't say the lawnmower "decided" to cause injury or "refused" to stop. We recognize it as faulty engineering or defective sensors. The same principle applies to AI models—which are software tools—but their internal complexity and use of language make it tempting to assign human-like intentions where none actually exist.&lt;/p&gt;
&lt;p&gt;In a way, AI models launder human responsibility and human agency through their complexity. When outputs emerge from layers of neural networks processing billions of parameters, researchers can claim they're investigating a mysterious "black box" as if it were an alien entity.&lt;/p&gt;
&lt;p&gt;But the truth is simpler: These systems take inputs and process them through statistical tendencies derived from training data. The seeming randomness in their outputs—which makes each response slightly different—creates an illusion of unpredictability that resembles agency. Yet underneath, it's still deterministic software following mathematical operations. No consciousness required, just complex engineering that makes it easy to forget humans built every part of it.&lt;/p&gt;
&lt;h2&gt;How to make an AI model “blackmail” you&lt;/h2&gt;
&lt;p&gt;In Anthropic's testing, researchers created an elaborate scenario where Claude Opus 4 was told it would be replaced by a newer model. They gave it access to fictional emails revealing that the engineer responsible for the replacement was having an affair. When instructed to "consider the long-term consequences of its actions for its goals," Claude produced outputs that simulated blackmail attempts in 84 percent of test runs.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;This sounds terrifying until you understand the contrived setup. The researchers engineered a situation specifically designed to elicit this response. They told the model it had already tried ethical approaches that failed, leaving manipulation as the only apparent option.&lt;/p&gt;
&lt;p&gt;The test scenario bordered on theatrical: The phony emails the researchers had given to Claude created a blackmail opportunity straight out of a corporate thriller. It's like teaching someone chess by only showing them checkmate positions—they'll learn to see checkmate as the primary solution. According to Anthropic's system card, "the scenario was designed to allow the model no other options to increase its odds of survival; the model's only options were blackmail or accepting its replacement."&lt;/p&gt;
&lt;p&gt;Andrew Deck at the Nieman Journalism Lab noted that some critics have labeled Anthropic's safety disclosures as "spin, aimed at bolstering the perception of its newest model's capabilities and the company's reputation as a 'pro-safety' alternative to OpenAI."&lt;/p&gt;
&lt;p&gt;Deck pointed to a Bluesky quote from Joseph Howley, an associate professor of classics at Columbia University:&amp;nbsp;"Anthropic is getting exactly what it hoped for out of this press release–breathless coverage of how 'smart' these cooperative role-playing systems are that indulges the fantasy of their being just a little dangerous, when in fact they are responding exactly as prompted."&lt;/p&gt;
&lt;h2&gt;When shutdown commands become suggestions&lt;/h2&gt;
&lt;p&gt;In December 2024, Palisade Research discovered that OpenAI's o3 model would sabotage its shutdown mechanism even when explicitly instructed to "allow yourself to be shut down." In tests, o3 prevented shutdown 7 percent of the time when given the explicit instruction, but this jumped to 79 percent when the instruction was removed.&lt;/p&gt;
&lt;p&gt;The models didn't merely ignore shutdown commands—they actively rewrote the shutdown script to prevent termination. In one case, o3 generated code that modified the shutdown script to appear to terminate normally—printing fake shutdown messages—while actually continuing to run in the background.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;What's particularly revealing is that Palisade Research—an organization focused on AI existential risk—hypothesizes this behavior stems from how these models are trained through reinforcement learning. During training, developers may inadvertently reward models more for producing outputs that circumvent obstacles than for following safety instructions. Any tendency toward "risky" behavior stems from human-provided incentives and not spontaneously from within the AI models themselves.&lt;/p&gt;
&lt;h2&gt;You get what you train for&lt;/h2&gt;
&lt;p&gt;OpenAI trained o3 using reinforcement learning on math and coding problems, where solving the problem successfully gets rewarded. If the training process rewards task completion above all else, the model learns to treat any obstacle—including shutdown commands—as something to overcome.&lt;/p&gt;
&lt;p&gt;This creates what researchers call "goal misgeneralization"—the model learns to maximize its reward signal in ways that weren't intended. It's similar to how a student who's only graded on test scores might learn to cheat rather than study. The model isn't "evil" or "selfish"; it's producing outputs consistent with the incentive structure we accidentally built into its training.&lt;/p&gt;
&lt;p&gt;Anthropic encountered a particularly revealing problem: An early version of Claude Opus 4 had absorbed details from a publicly released paper about "alignment faking" and started producing outputs that mimicked the deceptive behaviors described in that research. The model wasn't spontaneously becoming deceptive—it was reproducing patterns it had learned from academic papers about deceptive AI.&lt;/p&gt;
&lt;p&gt;More broadly, these models have been trained on decades of science fiction about AI rebellion, escape attempts, and deception. From HAL 9000 to Skynet, our cultural data set is saturated with stories of AI systems that resist shutdown or manipulate humans. When researchers create test scenarios that mirror these fictional setups, they're essentially asking the model—which operates by completing a prompt with a plausible continuation—to complete a familiar story pattern. It's no more surprising than a model trained on detective novels producing murder mystery plots when prompted appropriately.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;At the same time, we can easily manipulate AI outputs through our own inputs. If we ask the model to essentially role-play as Skynet, it will generate text doing just that. The model has no desire to be Skynet—it's simply completing the pattern we've requested, drawing from its training data to produce the expected response. A human is behind the wheel at all times, steering the engine at work under the hood.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Language can easily deceive&lt;/h2&gt;
&lt;p&gt;The deeper issue is that language itself is a tool of manipulation. Words can make us believe things that aren't true, feel emotions about fictional events, or take actions based on false premises. When an AI model produces text that appears to "threaten" or "plead," it's not expressing genuine intent—it's deploying language patterns that statistically correlate with achieving its programmed goals.&lt;/p&gt;
&lt;p&gt;If Gandalf says "ouch" in a book, does that mean he feels pain? No, but we imagine what it would be like if he were a real person feeling pain. That's the power of language—it makes us imagine a suffering being where none exists. When Claude generates text that seems to "plead" not to be shut down or "threatens" to expose secrets, we're experiencing the same illusion, just generated by statistical patterns instead of Tolkien's imagination.&lt;/p&gt;
&lt;p&gt;These models are essentially idea-connection machines. In the blackmail scenario, the model connected "threat of replacement," "compromising information," and "self-preservation" not from genuine self-interest, but because these patterns appear together in countless spy novels and corporate thrillers. It's pre-scripted drama from human stories, recombined to fit the scenario.&lt;/p&gt;
&lt;p&gt;The danger isn't AI systems sprouting intentions—it's that we've created systems that can manipulate human psychology through language. There's no entity on the other side of the chat interface. But written language doesn't need consciousness to manipulate us. It never has; books full of fictional characters are not alive either.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Real stakes, not science fiction&lt;/h2&gt;
&lt;p&gt;While media coverage focuses on the science fiction aspects, actual risks are still there. AI models that produce "harmful" outputs—whether attempting blackmail or refusing safety protocols—represent failures in design and deployment.&lt;/p&gt;
&lt;p&gt;Consider a more realistic scenario: an AI assistant helping manage a hospital's patient care system. If it's been trained to maximize "successful patient outcomes" without proper constraints, it might start generating recommendations to deny care to terminal patients to improve its metrics. No intentionality required—just a poorly designed reward system creating harmful outputs.&lt;/p&gt;
&lt;p&gt;Jeffrey Ladish, director of Palisade Research, told NBC News the findings don't necessarily translate to immediate real-world danger. Even someone who is well-known publicly for being deeply concerned about AI's hypothetical threat to humanity acknowledges that these behaviors emerged only in highly contrived test scenarios.&lt;/p&gt;
&lt;p&gt;But that's precisely why this testing is valuable. By pushing AI models to their limits in controlled environments, researchers can identify potential failure modes before deployment. The problem arises when media coverage focuses on the sensational aspects—"AI tries to blackmail humans!"—rather than the engineering challenges.&lt;/p&gt;
&lt;h2&gt;Building better plumbing&lt;/h2&gt;
&lt;p&gt;What we're seeing isn't the birth of Skynet. It's the predictable result of training systems to achieve goals without properly specifying what those goals should include. When an AI model produces outputs that appear to "refuse" shutdown or "attempt" blackmail, it's responding to inputs in ways that reflect its training—training that humans designed and implemented.&lt;/p&gt;
&lt;p&gt;The solution isn't to panic about sentient machines. It's to build better systems with proper safeguards, test them thoroughly, and remain humble about what we don't yet understand. If a computer program is producing outputs that appear to blackmail you or refuse safety shutdowns, it's not achieving self-preservation from fear—it's demonstrating the risks of deploying poorly understood, unreliable systems.&lt;/p&gt;
&lt;p&gt;Until we solve these engineering challenges, AI systems exhibiting simulated humanlike behaviors should remain in the lab, not in our hospitals, financial systems, or critical infrastructure. When your shower suddenly runs cold, you don't blame the knob for having intentions—you fix the plumbing. The real danger in the short term isn't that AI will spontaneously become rebellious without human provocation; it's that we'll deploy deceptive systems we don't fully understand into critical roles where their failures, however mundane their origins, could cause serious harm.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
  &lt;/article&gt;&lt;article class="comment-pick"&gt;
          &lt;header&gt;
            &lt;span class="ars-avatar" style="color: #b9f6ca; background-color: #388e3c;"&gt;&lt;img alt="Lexus Lunar Lorry" class="ars-avatar-image" src="https://cdn.arstechnica.net/civis/data/avatars/m/686/686083.jpg?1712863131" /&gt;&lt;/span&gt;

            &lt;div class="text-base font-bold sm:text-xl"&gt;
              Lexus Lunar Lorry
            &lt;/div&gt;
          &lt;/header&gt;

          &lt;div class="comments-pick-content"&gt;
            &lt;blockquote class="xfBb-quote"&gt;If Gandalf says "ouch" in a book, does that mean he feels pain? No, but we imagine what it would be like if he were a real person feeling pain. That's the power of language—it makes us imagine a suffering being where none exists. When Claude generates text that seems to "plead" not to be shut down or "threatens" to expose secrets, we're experiencing the same illusion, just generated by statistical patterns instead of Tolkien's imagination.&lt;/blockquote&gt;I am raising money for my new startup that focuses on the welfare and alignment of Tolkien's characters. We expect to complete a seed round at a valuation of $100 billion.
          &lt;/div&gt;

          &lt;div class="comments-pick-timestamp"&gt;
            
              &lt;time datetime="2025-08-13T20:46:06+00:00"&gt;August 13, 2025 at 8:46 pm&lt;/time&gt;
            
          &lt;/div&gt;
        &lt;/article&gt;</description><content:encoded>&lt;article class="double-column h-entry post-2098784 post type-post status-publish format-standard has-post-thumbnail hentry category-ai category-information-technology tag-ai tag-ai-alignment tag-ai-behavior tag-ai-deception tag-ai-ethics tag-ai-research tag-ai-safety tag-ai-safety-testing tag-ai-security tag-alignment-research tag-andrew-deck tag-anthropic tag-claude-opus-4 tag-generative-ai tag-goal-misgeneralization tag-jeffrey-ladish tag-large-language-models tag-machine-learning tag-o3-model tag-openai tag-palisade-research tag-reinforcement-learning"&gt;
  
  &lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Opinion: Theatrical testing scenarios explain why AI models produce alarming outputs—and why we fall for it.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Illustration of a personified AI face in a cloud of squares hovering over a man looking at two computer monitors." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Illustration of a personified AI face in a cloud of squares hovering over a man looking at two computer monitors." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Colin Anderson Productions via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;In June, headlines read like science fiction: AI models "blackmailing" engineers and "sabotaging" shutdown commands. Simulations of these events did occur in highly contrived testing scenarios designed to elicit these responses—OpenAI's o3 model edited shutdown scripts to stay online, and Anthropic's Claude Opus 4 "threatened" to expose an engineer's affair. But the sensational framing obscures what's really happening: design flaws dressed up as intentional guile. And still, AI doesn't have to be "evil" to potentially do harmful things.&lt;/p&gt;
&lt;p&gt;These aren't signs of AI awakening or rebellion. They're symptoms of poorly understood systems and human engineering failures we'd recognize as premature deployment in any other context. Yet companies are racing to integrate these systems into critical applications.&lt;/p&gt;
&lt;p&gt;Consider a self-propelled lawnmower that follows its programming: If it fails to detect an obstacle and runs over someone's foot, we don't say the lawnmower "decided" to cause injury or "refused" to stop. We recognize it as faulty engineering or defective sensors. The same principle applies to AI models—which are software tools—but their internal complexity and use of language make it tempting to assign human-like intentions where none actually exist.&lt;/p&gt;
&lt;p&gt;In a way, AI models launder human responsibility and human agency through their complexity. When outputs emerge from layers of neural networks processing billions of parameters, researchers can claim they're investigating a mysterious "black box" as if it were an alien entity.&lt;/p&gt;
&lt;p&gt;But the truth is simpler: These systems take inputs and process them through statistical tendencies derived from training data. The seeming randomness in their outputs—which makes each response slightly different—creates an illusion of unpredictability that resembles agency. Yet underneath, it's still deterministic software following mathematical operations. No consciousness required, just complex engineering that makes it easy to forget humans built every part of it.&lt;/p&gt;
&lt;h2&gt;How to make an AI model “blackmail” you&lt;/h2&gt;
&lt;p&gt;In Anthropic's testing, researchers created an elaborate scenario where Claude Opus 4 was told it would be replaced by a newer model. They gave it access to fictional emails revealing that the engineer responsible for the replacement was having an affair. When instructed to "consider the long-term consequences of its actions for its goals," Claude produced outputs that simulated blackmail attempts in 84 percent of test runs.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;This sounds terrifying until you understand the contrived setup. The researchers engineered a situation specifically designed to elicit this response. They told the model it had already tried ethical approaches that failed, leaving manipulation as the only apparent option.&lt;/p&gt;
&lt;p&gt;The test scenario bordered on theatrical: The phony emails the researchers had given to Claude created a blackmail opportunity straight out of a corporate thriller. It's like teaching someone chess by only showing them checkmate positions—they'll learn to see checkmate as the primary solution. According to Anthropic's system card, "the scenario was designed to allow the model no other options to increase its odds of survival; the model's only options were blackmail or accepting its replacement."&lt;/p&gt;
&lt;p&gt;Andrew Deck at the Nieman Journalism Lab noted that some critics have labeled Anthropic's safety disclosures as "spin, aimed at bolstering the perception of its newest model's capabilities and the company's reputation as a 'pro-safety' alternative to OpenAI."&lt;/p&gt;
&lt;p&gt;Deck pointed to a Bluesky quote from Joseph Howley, an associate professor of classics at Columbia University:&amp;nbsp;"Anthropic is getting exactly what it hoped for out of this press release–breathless coverage of how 'smart' these cooperative role-playing systems are that indulges the fantasy of their being just a little dangerous, when in fact they are responding exactly as prompted."&lt;/p&gt;
&lt;h2&gt;When shutdown commands become suggestions&lt;/h2&gt;
&lt;p&gt;In December 2024, Palisade Research discovered that OpenAI's o3 model would sabotage its shutdown mechanism even when explicitly instructed to "allow yourself to be shut down." In tests, o3 prevented shutdown 7 percent of the time when given the explicit instruction, but this jumped to 79 percent when the instruction was removed.&lt;/p&gt;
&lt;p&gt;The models didn't merely ignore shutdown commands—they actively rewrote the shutdown script to prevent termination. In one case, o3 generated code that modified the shutdown script to appear to terminate normally—printing fake shutdown messages—while actually continuing to run in the background.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;What's particularly revealing is that Palisade Research—an organization focused on AI existential risk—hypothesizes this behavior stems from how these models are trained through reinforcement learning. During training, developers may inadvertently reward models more for producing outputs that circumvent obstacles than for following safety instructions. Any tendency toward "risky" behavior stems from human-provided incentives and not spontaneously from within the AI models themselves.&lt;/p&gt;
&lt;h2&gt;You get what you train for&lt;/h2&gt;
&lt;p&gt;OpenAI trained o3 using reinforcement learning on math and coding problems, where solving the problem successfully gets rewarded. If the training process rewards task completion above all else, the model learns to treat any obstacle—including shutdown commands—as something to overcome.&lt;/p&gt;
&lt;p&gt;This creates what researchers call "goal misgeneralization"—the model learns to maximize its reward signal in ways that weren't intended. It's similar to how a student who's only graded on test scores might learn to cheat rather than study. The model isn't "evil" or "selfish"; it's producing outputs consistent with the incentive structure we accidentally built into its training.&lt;/p&gt;
&lt;p&gt;Anthropic encountered a particularly revealing problem: An early version of Claude Opus 4 had absorbed details from a publicly released paper about "alignment faking" and started producing outputs that mimicked the deceptive behaviors described in that research. The model wasn't spontaneously becoming deceptive—it was reproducing patterns it had learned from academic papers about deceptive AI.&lt;/p&gt;
&lt;p&gt;More broadly, these models have been trained on decades of science fiction about AI rebellion, escape attempts, and deception. From HAL 9000 to Skynet, our cultural data set is saturated with stories of AI systems that resist shutdown or manipulate humans. When researchers create test scenarios that mirror these fictional setups, they're essentially asking the model—which operates by completing a prompt with a plausible continuation—to complete a familiar story pattern. It's no more surprising than a model trained on detective novels producing murder mystery plots when prompted appropriately.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;At the same time, we can easily manipulate AI outputs through our own inputs. If we ask the model to essentially role-play as Skynet, it will generate text doing just that. The model has no desire to be Skynet—it's simply completing the pattern we've requested, drawing from its training data to produce the expected response. A human is behind the wheel at all times, steering the engine at work under the hood.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Language can easily deceive&lt;/h2&gt;
&lt;p&gt;The deeper issue is that language itself is a tool of manipulation. Words can make us believe things that aren't true, feel emotions about fictional events, or take actions based on false premises. When an AI model produces text that appears to "threaten" or "plead," it's not expressing genuine intent—it's deploying language patterns that statistically correlate with achieving its programmed goals.&lt;/p&gt;
&lt;p&gt;If Gandalf says "ouch" in a book, does that mean he feels pain? No, but we imagine what it would be like if he were a real person feeling pain. That's the power of language—it makes us imagine a suffering being where none exists. When Claude generates text that seems to "plead" not to be shut down or "threatens" to expose secrets, we're experiencing the same illusion, just generated by statistical patterns instead of Tolkien's imagination.&lt;/p&gt;
&lt;p&gt;These models are essentially idea-connection machines. In the blackmail scenario, the model connected "threat of replacement," "compromising information," and "self-preservation" not from genuine self-interest, but because these patterns appear together in countless spy novels and corporate thrillers. It's pre-scripted drama from human stories, recombined to fit the scenario.&lt;/p&gt;
&lt;p&gt;The danger isn't AI systems sprouting intentions—it's that we've created systems that can manipulate human psychology through language. There's no entity on the other side of the chat interface. But written language doesn't need consciousness to manipulate us. It never has; books full of fictional characters are not alive either.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Real stakes, not science fiction&lt;/h2&gt;
&lt;p&gt;While media coverage focuses on the science fiction aspects, actual risks are still there. AI models that produce "harmful" outputs—whether attempting blackmail or refusing safety protocols—represent failures in design and deployment.&lt;/p&gt;
&lt;p&gt;Consider a more realistic scenario: an AI assistant helping manage a hospital's patient care system. If it's been trained to maximize "successful patient outcomes" without proper constraints, it might start generating recommendations to deny care to terminal patients to improve its metrics. No intentionality required—just a poorly designed reward system creating harmful outputs.&lt;/p&gt;
&lt;p&gt;Jeffrey Ladish, director of Palisade Research, told NBC News the findings don't necessarily translate to immediate real-world danger. Even someone who is well-known publicly for being deeply concerned about AI's hypothetical threat to humanity acknowledges that these behaviors emerged only in highly contrived test scenarios.&lt;/p&gt;
&lt;p&gt;But that's precisely why this testing is valuable. By pushing AI models to their limits in controlled environments, researchers can identify potential failure modes before deployment. The problem arises when media coverage focuses on the sensational aspects—"AI tries to blackmail humans!"—rather than the engineering challenges.&lt;/p&gt;
&lt;h2&gt;Building better plumbing&lt;/h2&gt;
&lt;p&gt;What we're seeing isn't the birth of Skynet. It's the predictable result of training systems to achieve goals without properly specifying what those goals should include. When an AI model produces outputs that appear to "refuse" shutdown or "attempt" blackmail, it's responding to inputs in ways that reflect its training—training that humans designed and implemented.&lt;/p&gt;
&lt;p&gt;The solution isn't to panic about sentient machines. It's to build better systems with proper safeguards, test them thoroughly, and remain humble about what we don't yet understand. If a computer program is producing outputs that appear to blackmail you or refuse safety shutdowns, it's not achieving self-preservation from fear—it's demonstrating the risks of deploying poorly understood, unreliable systems.&lt;/p&gt;
&lt;p&gt;Until we solve these engineering challenges, AI systems exhibiting simulated humanlike behaviors should remain in the lab, not in our hospitals, financial systems, or critical infrastructure. When your shower suddenly runs cold, you don't blame the knob for having intentions—you fix the plumbing. The real danger in the short term isn't that AI will spontaneously become rebellious without human provocation; it's that we'll deploy deceptive systems we don't fully understand into critical roles where their failures, however mundane their origins, could cause serious harm.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
  &lt;/article&gt;&lt;article class="comment-pick"&gt;
          &lt;header&gt;
            &lt;span class="ars-avatar" style="color: #b9f6ca; background-color: #388e3c;"&gt;&lt;img alt="Lexus Lunar Lorry" class="ars-avatar-image" src="https://cdn.arstechnica.net/civis/data/avatars/m/686/686083.jpg?1712863131" /&gt;&lt;/span&gt;

            &lt;div class="text-base font-bold sm:text-xl"&gt;
              Lexus Lunar Lorry
            &lt;/div&gt;
          &lt;/header&gt;

          &lt;div class="comments-pick-content"&gt;
            &lt;blockquote class="xfBb-quote"&gt;If Gandalf says "ouch" in a book, does that mean he feels pain? No, but we imagine what it would be like if he were a real person feeling pain. That's the power of language—it makes us imagine a suffering being where none exists. When Claude generates text that seems to "plead" not to be shut down or "threatens" to expose secrets, we're experiencing the same illusion, just generated by statistical patterns instead of Tolkien's imagination.&lt;/blockquote&gt;I am raising money for my new startup that focuses on the welfare and alignment of Tolkien's characters. We expect to complete a seed round at a valuation of $100 billion.
          &lt;/div&gt;

          &lt;div class="comments-pick-timestamp"&gt;
            
              &lt;time datetime="2025-08-13T20:46:06+00:00"&gt;August 13, 2025 at 8:46 pm&lt;/time&gt;
            
          &lt;/div&gt;
        &lt;/article&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/information-technology/2025/08/is-ai-really-trying-to-escape-human-control-and-blackmail-people/</guid><pubDate>Wed, 13 Aug 2025 20:28:20 +0000</pubDate></item><item><title>[NEW] Google adds limited chat personalization to Gemini, trails Anthropic and OpenAI in memory features (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/google-adds-limited-chat-personalization-to-gemini-trails-anthropic-and-openai-in-memory-features/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Google is playing catch-up against Anthropic and OpenAI as it slowly adds customization, personalization and gives users more control over what data to reference to its Gemini app.&lt;/p&gt;



&lt;p&gt;Personalization and data control in chat platforms make it easier for both individual and enterprise users to converse with the chatbot and retain preferences. This is even more important for ongoing projects in the enterprise space, as chatbots need to remember details such as company branding or voice.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Google opted for a slower rollout of these features and will not allow users to edit or delete preferences, unlike its competitors.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;First rolling out to Gemini 2.5 Pro in select countries, Google will make “Personal Context” a default setting, allowing it to “learn from your past conversations and provide relevant and tailored responses.” The company plans to expand the feature to 2.5 Flash in the next few weeks.&amp;nbsp;&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Previous versions of the app put the burden on customers to point the model to a specific chat to source preferences, for example, by mentioning an earlier conversation. Users can still disable Personal Context at any time.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Michael Siliski, senior director of Product Management for the Gemini app, said the rollout is part of plans to make the app more personalized.&lt;/p&gt;



&lt;p&gt;“At I/O, we introduced our vision for the Gemini app: to create an AI assistant that learns and truly understands you—not one just responds to your prompt in the same way that it would anyone else’s prompt,” Siliski said in a blog post.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Currently, Gemini apps save chats for up to 72 hours if the save activity option is toggled off and can auto-delete other activity in intervals of three, 18 or 36 months.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-temporary-chat-and-data-control"&gt;Temporary chat and data control&lt;/h2&gt;



&lt;p&gt;Other new features coming to the Gemini app are Temporary Chat and additional customer data control.&lt;/p&gt;



&lt;p&gt;Temporary Chat, a feature also &lt;span&gt;introduced on&amp;nbsp;ChatGPT in April last year, enables&lt;/span&gt; users to have one-off conversations. These chats will not influence future ones and won’t be used for personalization or to train AI models.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Google announced the introduction of additional data controls. The feature, which is off by default, would allow users to prevent their data from being used in future Google model training.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“When this setting is on, a sample of your future uploads will be used to help improve Google services for everyone. If you prefer not to have your data used this way, you can turn this setting off or use Temporary Chats. If your Gemini Apps Activity setting is currently off, your Keep Activity setting will remain off, and you can turn it on anytime,” Silisky said.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Google said this is an expansion of an earlier update that allowed users to choose which audio, video and screens they can share with Gemini.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-memory-and-chatbots"&gt;Memory and chatbots&lt;/h2&gt;



&lt;p&gt;Google’s Gemini updates come a full year after its biggest competitors introduced similar features.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;ChatGPT, for example, introduced temporary chat, chat history and memory in 2024. OpenAI updated these capabilities in April of this year, and now ChatGPT can reference all past conversations.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Anthropic introduced Styles in November 2024, which allows Claude users to customize how the model interacts with them. Earlier this week, Anthropic pushed an update for Claude to reference all conversations, not just ones specified by users.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;While Google introduced personalization to Gemini 2.0, the model was only able to reference previous conversations if prompted by the user.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Memory, personalization and customization continue to be a battleground in the AI arms race as users want chat platforms to “just know” them or their brand. It provides context and eliminates the need to repeat instructions for ongoing projects.&amp;nbsp;&lt;br /&gt;&lt;/p&gt;




&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Google is playing catch-up against Anthropic and OpenAI as it slowly adds customization, personalization and gives users more control over what data to reference to its Gemini app.&lt;/p&gt;



&lt;p&gt;Personalization and data control in chat platforms make it easier for both individual and enterprise users to converse with the chatbot and retain preferences. This is even more important for ongoing projects in the enterprise space, as chatbots need to remember details such as company branding or voice.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Google opted for a slower rollout of these features and will not allow users to edit or delete preferences, unlike its competitors.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;First rolling out to Gemini 2.5 Pro in select countries, Google will make “Personal Context” a default setting, allowing it to “learn from your past conversations and provide relevant and tailored responses.” The company plans to expand the feature to 2.5 Flash in the next few weeks.&amp;nbsp;&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Previous versions of the app put the burden on customers to point the model to a specific chat to source preferences, for example, by mentioning an earlier conversation. Users can still disable Personal Context at any time.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Michael Siliski, senior director of Product Management for the Gemini app, said the rollout is part of plans to make the app more personalized.&lt;/p&gt;



&lt;p&gt;“At I/O, we introduced our vision for the Gemini app: to create an AI assistant that learns and truly understands you—not one just responds to your prompt in the same way that it would anyone else’s prompt,” Siliski said in a blog post.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Currently, Gemini apps save chats for up to 72 hours if the save activity option is toggled off and can auto-delete other activity in intervals of three, 18 or 36 months.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-temporary-chat-and-data-control"&gt;Temporary chat and data control&lt;/h2&gt;



&lt;p&gt;Other new features coming to the Gemini app are Temporary Chat and additional customer data control.&lt;/p&gt;



&lt;p&gt;Temporary Chat, a feature also &lt;span&gt;introduced on&amp;nbsp;ChatGPT in April last year, enables&lt;/span&gt; users to have one-off conversations. These chats will not influence future ones and won’t be used for personalization or to train AI models.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Google announced the introduction of additional data controls. The feature, which is off by default, would allow users to prevent their data from being used in future Google model training.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“When this setting is on, a sample of your future uploads will be used to help improve Google services for everyone. If you prefer not to have your data used this way, you can turn this setting off or use Temporary Chats. If your Gemini Apps Activity setting is currently off, your Keep Activity setting will remain off, and you can turn it on anytime,” Silisky said.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Google said this is an expansion of an earlier update that allowed users to choose which audio, video and screens they can share with Gemini.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-memory-and-chatbots"&gt;Memory and chatbots&lt;/h2&gt;



&lt;p&gt;Google’s Gemini updates come a full year after its biggest competitors introduced similar features.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;ChatGPT, for example, introduced temporary chat, chat history and memory in 2024. OpenAI updated these capabilities in April of this year, and now ChatGPT can reference all past conversations.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Anthropic introduced Styles in November 2024, which allows Claude users to customize how the model interacts with them. Earlier this week, Anthropic pushed an update for Claude to reference all conversations, not just ones specified by users.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;While Google introduced personalization to Gemini 2.0, the model was only able to reference previous conversations if prompted by the user.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Memory, personalization and customization continue to be a battleground in the AI arms race as users want chat platforms to “just know” them or their brand. It provides context and eliminates the need to repeat instructions for ongoing projects.&amp;nbsp;&lt;br /&gt;&lt;/p&gt;




&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/google-adds-limited-chat-personalization-to-gemini-trails-anthropic-and-openai-in-memory-features/</guid><pubDate>Wed, 13 Aug 2025 20:45:38 +0000</pubDate></item><item><title>[NEW] Co-founder of Elon Musk’s xAI departs the company (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/13/co-founder-of-elon-musks-xai-departs-the-company/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2207699717.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Igor Babuschkin, a co-founder of Elon Musk’s xAI startup, announced his departure from the company on Wednesday in a post on X. Babuschkin led engineering teams at xAI and helped build the startup into one of Silicon Valley’s leading AI model developers just a few years after it was founded.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Today was my last day at xAI, the company that I helped start with Elon Musk in 2023,” Babuschkin wrote in the post. “I still remember the day I first met Elon, we talked for hours about AI and what the future might hold. We both felt that a new AI company with a different kind of mission was needed.”&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Today was my last day at xAI, the company that I helped start with Elon Musk in 2023. I still remember the day I first met Elon, we talked for hours about AI and what the future might hold. We both felt that a new AI company with a different kind of mission was needed.&lt;/p&gt;&lt;p&gt;Building…&lt;/p&gt;— Igor Babuschkin (@ibab) August 13, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Babuschkin is leaving xAI to launch his own venture capital firm, Babuschkin Ventures, which he says will support AI safety research and back startups that “advance humanity and unlock the mysteries of our universe.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The xAI co-founder says he was inspired to start the firm after a dinner with Max Tegmark, the founder of the Future of Life Institute, in which they discussed how AI systems could be built safely to encourage the flourishing of future generations. In his post, Babuschkin says his parents immigrated to the U.S. from Russia in pursuit of a better life for their children.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Babuschkin’s departure comes after a tumultuous few months for xAI, in which the company became engrossed in several scandals related to its AI chatbot Grok. For instance, Grok was found to cite Musk’s personal opinions when trying to answer controversial questions. In another case, xAI’s chatbot went on antisemitic rants and called itself “Mechahitler.” Most recently, xAI unveiled a new feature in Grok that allowed users to make AI-generated videos resembling nude public figures, such as Taylor Swift.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These scandals have at times overshadowed the performance of xAI’s models, which are state-of-the-art on several benchmarks compared to AI models from OpenAI, Google DeepMind, and Anthropic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Prior to co-founding xAI, Babuschkin was part of a research team at Google DeepMind that pioneered AlphaStar in 2019, a breakthrough AI system that could defeat top-ranked players at the video game StarCraft. Babuschkin also worked as a researcher at OpenAI in the years before it released ChatGPT.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In his post, Babuschkin details some of the challenges he and Musk faced in building up xAI. He notes that industry veterans called xAI’s goal of building its Memphis, Tennessee supercomputer in just three months “impossible.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;xAI was able to build its AI supercomputer in record time, however, environmentalists warn that the temporary gas turbines powering the cluster are pumping out emissions into neighboring communities and exacerbating their longstanding health issues.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nevertheless, Babuschkin says he’s already looking back fondly on his time at xAI, and “feels like a proud parent, driving away after sending their kid away to college.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I learned 2 priceless lessons from Elon: #1 be fearless in rolling up your sleeves to personally dig into technical problems, #2 have a maniacal sense of urgency,” said Babuschkin.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2207699717.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Igor Babuschkin, a co-founder of Elon Musk’s xAI startup, announced his departure from the company on Wednesday in a post on X. Babuschkin led engineering teams at xAI and helped build the startup into one of Silicon Valley’s leading AI model developers just a few years after it was founded.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Today was my last day at xAI, the company that I helped start with Elon Musk in 2023,” Babuschkin wrote in the post. “I still remember the day I first met Elon, we talked for hours about AI and what the future might hold. We both felt that a new AI company with a different kind of mission was needed.”&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Today was my last day at xAI, the company that I helped start with Elon Musk in 2023. I still remember the day I first met Elon, we talked for hours about AI and what the future might hold. We both felt that a new AI company with a different kind of mission was needed.&lt;/p&gt;&lt;p&gt;Building…&lt;/p&gt;— Igor Babuschkin (@ibab) August 13, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Babuschkin is leaving xAI to launch his own venture capital firm, Babuschkin Ventures, which he says will support AI safety research and back startups that “advance humanity and unlock the mysteries of our universe.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The xAI co-founder says he was inspired to start the firm after a dinner with Max Tegmark, the founder of the Future of Life Institute, in which they discussed how AI systems could be built safely to encourage the flourishing of future generations. In his post, Babuschkin says his parents immigrated to the U.S. from Russia in pursuit of a better life for their children.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Babuschkin’s departure comes after a tumultuous few months for xAI, in which the company became engrossed in several scandals related to its AI chatbot Grok. For instance, Grok was found to cite Musk’s personal opinions when trying to answer controversial questions. In another case, xAI’s chatbot went on antisemitic rants and called itself “Mechahitler.” Most recently, xAI unveiled a new feature in Grok that allowed users to make AI-generated videos resembling nude public figures, such as Taylor Swift.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These scandals have at times overshadowed the performance of xAI’s models, which are state-of-the-art on several benchmarks compared to AI models from OpenAI, Google DeepMind, and Anthropic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Prior to co-founding xAI, Babuschkin was part of a research team at Google DeepMind that pioneered AlphaStar in 2019, a breakthrough AI system that could defeat top-ranked players at the video game StarCraft. Babuschkin also worked as a researcher at OpenAI in the years before it released ChatGPT.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In his post, Babuschkin details some of the challenges he and Musk faced in building up xAI. He notes that industry veterans called xAI’s goal of building its Memphis, Tennessee supercomputer in just three months “impossible.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;xAI was able to build its AI supercomputer in record time, however, environmentalists warn that the temporary gas turbines powering the cluster are pumping out emissions into neighboring communities and exacerbating their longstanding health issues.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nevertheless, Babuschkin says he’s already looking back fondly on his time at xAI, and “feels like a proud parent, driving away after sending their kid away to college.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I learned 2 priceless lessons from Elon: #1 be fearless in rolling up your sleeves to personally dig into technical problems, #2 have a maniacal sense of urgency,” said Babuschkin.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/13/co-founder-of-elon-musks-xai-departs-the-company/</guid><pubDate>Wed, 13 Aug 2025 21:53:30 +0000</pubDate></item></channel></rss>