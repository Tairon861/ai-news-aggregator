<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 19 Aug 2025 06:33:33 +0000</lastBuildDate><item><title>Researchers glimpse the inner workings of protein language models (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/researchers-glimpse-inner-workings-protein-language-models-0818</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202508/MIT-model-interpret-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Within the past few years, models that can predict the structure or function of proteins have been widely used for a variety of biological applications, such as identifying drug targets and designing new therapeutic antibodies.&lt;/p&gt;&lt;p&gt;These models, which are based on large language models (LLMs), can make very accurate predictions of a protein’s suitability for a given application. However, there’s no way to determine how these models make their predictions or which protein features play the most important role in those decisions.&lt;/p&gt;&lt;p&gt;In a new study, MIT researchers have used a novel technique to open up that “black box” and allow them to determine what features a protein language model takes into account when making predictions. Understanding what is happening inside that black box&amp;nbsp;could help researchers to choose better models for a particular task, helping to streamline the process of identifying new drugs or vaccine targets.&lt;/p&gt;&lt;p&gt;“Our work has broad implications for enhanced explainability in downstream tasks that rely on these representations,” says Bonnie Berger, the Simons Professor of Mathematics, head of the Computation and Biology group in MIT’s Computer Science and Artificial Intelligence Laboratory, and the senior author of the study. “Additionally, identifying features that protein language models track has the potential to reveal novel biological insights from these representations.”&lt;/p&gt;&lt;p&gt;Onkar Gujral, an MIT graduate student, is the lead author of the study, which appears this week in the &lt;em&gt;Proceedings of the National Academy of Sciences.&lt;/em&gt; Mihir Bafna, an MIT graduate student, and Eric Alm, an MIT professor of biological engineering, are also authors of the paper.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Opening the black box&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In 2018, Berger and former MIT graduate student Tristan Bepler PhD ’20 introduced the first protein language model. Their model, like subsequent protein models that accelerated the development of&amp;nbsp;AlphaFold, such as&amp;nbsp;ESM2 and OmegaFold, was based on LLMs. These models, which include ChatGPT, can analyze huge amounts of text and figure out which words are most likely to appear together.&lt;/p&gt;&lt;p&gt;Protein language models use a similar approach, but instead of analyzing words, they analyze amino acid sequences. Researchers have used these models to predict the structure and function of proteins, and for applications such as identifying proteins that might bind to particular drugs.&lt;/p&gt;&lt;p&gt;In a&amp;nbsp;2021 study, Berger and colleagues used a protein language model to predict which sections of viral surface proteins are less likely to mutate in a way that enables viral escape. This allowed them to identify possible targets for vaccines against influenza, HIV, and SARS-CoV-2.&lt;/p&gt;&lt;p&gt;However, in all of these studies, it has been impossible to know how the models were making their predictions.&lt;/p&gt;&lt;p&gt;“We would get out some prediction at the end, but we had absolutely no idea what was happening in the individual components of this black box,” Berger says.&lt;/p&gt;&lt;p&gt;In the new study, the researchers wanted to dig into how protein language models make their predictions. Just like LLMs, protein language models encode information as representations that consist of a pattern of activation of different “nodes” within a neural network. These nodes are analogous to the networks of neurons that store memories and other information within the brain.&lt;/p&gt;&lt;p&gt;The inner workings of LLMs are not easy to interpret, but within the past couple of years, researchers have begun using a type of algorithm known as a sparse autoencoder to help shed some light on how those models make their predictions. The new study from Berger’s lab is the first to use this algorithm on protein language models.&lt;/p&gt;&lt;p&gt;Sparse autoencoders work by adjusting how a protein is represented within a neural network. Typically, a given protein will be represented by a pattern of activation of a constrained number of neurons, for example, 480. A sparse autoencoder will expand that representation into a much larger number of nodes, say 20,000.&lt;/p&gt;&lt;p&gt;When information about a protein is encoded by only 480 neurons, each node lights up for multiple features, making it very difficult to know what features each node is encoding. However, when the neural network is expanded to 20,000 nodes, this extra space along with a sparsity constraint gives the information room to “spread out.” Now, a feature of the protein that was previously encoded by multiple nodes can occupy a single node.&lt;/p&gt;&lt;p&gt;“In a sparse representation, the neurons lighting up are doing so in a more meaningful manner,” Gujral says. “Before the sparse representations are created, the networks pack information so tightly together that it's hard to interpret the neurons.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Interpretable models&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Once the researchers obtained sparse representations of many proteins, they used an AI assistant called Claude (related to the popular Anthropic chatbot of the same name), to analyze the representations. In this case, they asked Claude to compare the sparse representations with the known features of each protein, such as molecular function, protein family, or location within a cell.&lt;/p&gt;&lt;p&gt;By analyzing thousands of representations, Claude can determine which nodes correspond to specific protein features, then describe them in plain English. For example, the algorithm might say, “This neuron appears to be detecting proteins involved in transmembrane transport of ions or amino acids, particularly those located in the plasma membrane.”&lt;/p&gt;&lt;p&gt;This process makes the nodes far more “interpretable,” meaning the researchers can tell what each node is encoding. They found that the features most likely to be encoded by these nodes were protein family and certain functions, including several different metabolic and biosynthetic processes.&lt;/p&gt;&lt;p&gt;“When you train a sparse autoencoder, you aren’t training it to be interpretable, but it turns out that by incentivizing the representation to be really sparse, that ends up resulting in interpretability,” Gujral says.&lt;/p&gt;&lt;p&gt;Understanding what features a particular protein model is encoding could help researchers choose the right model for a particular task, or tweak the type of input they give the model, to generate the best results. Additionally, analyzing the features that a model encodes could one day help biologists to learn more about the proteins that they are studying.&lt;/p&gt;&lt;p&gt;“At some point when the models get a lot more powerful, you could learn more biology than you already know, from opening up the models,” Gujral says.&lt;/p&gt;&lt;p&gt;The research was funded by the National Institutes of Health.&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202508/MIT-model-interpret-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Within the past few years, models that can predict the structure or function of proteins have been widely used for a variety of biological applications, such as identifying drug targets and designing new therapeutic antibodies.&lt;/p&gt;&lt;p&gt;These models, which are based on large language models (LLMs), can make very accurate predictions of a protein’s suitability for a given application. However, there’s no way to determine how these models make their predictions or which protein features play the most important role in those decisions.&lt;/p&gt;&lt;p&gt;In a new study, MIT researchers have used a novel technique to open up that “black box” and allow them to determine what features a protein language model takes into account when making predictions. Understanding what is happening inside that black box&amp;nbsp;could help researchers to choose better models for a particular task, helping to streamline the process of identifying new drugs or vaccine targets.&lt;/p&gt;&lt;p&gt;“Our work has broad implications for enhanced explainability in downstream tasks that rely on these representations,” says Bonnie Berger, the Simons Professor of Mathematics, head of the Computation and Biology group in MIT’s Computer Science and Artificial Intelligence Laboratory, and the senior author of the study. “Additionally, identifying features that protein language models track has the potential to reveal novel biological insights from these representations.”&lt;/p&gt;&lt;p&gt;Onkar Gujral, an MIT graduate student, is the lead author of the study, which appears this week in the &lt;em&gt;Proceedings of the National Academy of Sciences.&lt;/em&gt; Mihir Bafna, an MIT graduate student, and Eric Alm, an MIT professor of biological engineering, are also authors of the paper.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Opening the black box&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In 2018, Berger and former MIT graduate student Tristan Bepler PhD ’20 introduced the first protein language model. Their model, like subsequent protein models that accelerated the development of&amp;nbsp;AlphaFold, such as&amp;nbsp;ESM2 and OmegaFold, was based on LLMs. These models, which include ChatGPT, can analyze huge amounts of text and figure out which words are most likely to appear together.&lt;/p&gt;&lt;p&gt;Protein language models use a similar approach, but instead of analyzing words, they analyze amino acid sequences. Researchers have used these models to predict the structure and function of proteins, and for applications such as identifying proteins that might bind to particular drugs.&lt;/p&gt;&lt;p&gt;In a&amp;nbsp;2021 study, Berger and colleagues used a protein language model to predict which sections of viral surface proteins are less likely to mutate in a way that enables viral escape. This allowed them to identify possible targets for vaccines against influenza, HIV, and SARS-CoV-2.&lt;/p&gt;&lt;p&gt;However, in all of these studies, it has been impossible to know how the models were making their predictions.&lt;/p&gt;&lt;p&gt;“We would get out some prediction at the end, but we had absolutely no idea what was happening in the individual components of this black box,” Berger says.&lt;/p&gt;&lt;p&gt;In the new study, the researchers wanted to dig into how protein language models make their predictions. Just like LLMs, protein language models encode information as representations that consist of a pattern of activation of different “nodes” within a neural network. These nodes are analogous to the networks of neurons that store memories and other information within the brain.&lt;/p&gt;&lt;p&gt;The inner workings of LLMs are not easy to interpret, but within the past couple of years, researchers have begun using a type of algorithm known as a sparse autoencoder to help shed some light on how those models make their predictions. The new study from Berger’s lab is the first to use this algorithm on protein language models.&lt;/p&gt;&lt;p&gt;Sparse autoencoders work by adjusting how a protein is represented within a neural network. Typically, a given protein will be represented by a pattern of activation of a constrained number of neurons, for example, 480. A sparse autoencoder will expand that representation into a much larger number of nodes, say 20,000.&lt;/p&gt;&lt;p&gt;When information about a protein is encoded by only 480 neurons, each node lights up for multiple features, making it very difficult to know what features each node is encoding. However, when the neural network is expanded to 20,000 nodes, this extra space along with a sparsity constraint gives the information room to “spread out.” Now, a feature of the protein that was previously encoded by multiple nodes can occupy a single node.&lt;/p&gt;&lt;p&gt;“In a sparse representation, the neurons lighting up are doing so in a more meaningful manner,” Gujral says. “Before the sparse representations are created, the networks pack information so tightly together that it's hard to interpret the neurons.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Interpretable models&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Once the researchers obtained sparse representations of many proteins, they used an AI assistant called Claude (related to the popular Anthropic chatbot of the same name), to analyze the representations. In this case, they asked Claude to compare the sparse representations with the known features of each protein, such as molecular function, protein family, or location within a cell.&lt;/p&gt;&lt;p&gt;By analyzing thousands of representations, Claude can determine which nodes correspond to specific protein features, then describe them in plain English. For example, the algorithm might say, “This neuron appears to be detecting proteins involved in transmembrane transport of ions or amino acids, particularly those located in the plasma membrane.”&lt;/p&gt;&lt;p&gt;This process makes the nodes far more “interpretable,” meaning the researchers can tell what each node is encoding. They found that the features most likely to be encoded by these nodes were protein family and certain functions, including several different metabolic and biosynthetic processes.&lt;/p&gt;&lt;p&gt;“When you train a sparse autoencoder, you aren’t training it to be interpretable, but it turns out that by incentivizing the representation to be really sparse, that ends up resulting in interpretability,” Gujral says.&lt;/p&gt;&lt;p&gt;Understanding what features a particular protein model is encoding could help researchers choose the right model for a particular task, or tweak the type of input they give the model, to generate the best results. Additionally, analyzing the features that a model encodes could one day help biologists to learn more about the proteins that they are studying.&lt;/p&gt;&lt;p&gt;“At some point when the models get a lot more powerful, you could learn more biology than you already know, from opening up the models,” Gujral says.&lt;/p&gt;&lt;p&gt;The research was funded by the National Institutes of Health.&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/researchers-glimpse-inner-workings-protein-language-models-0818</guid><pubDate>Mon, 18 Aug 2025 19:00:00 +0000</pubDate></item><item><title>TensorZero nabs $7.3M seed to solve the messy world of enterprise LLM development (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/tensorzero-nabs-7-3m-seed-to-solve-the-messy-world-of-enterprise-llm-development/</link><description>&lt;p&gt;TensorZero, a startup building open-source infrastructure for large language model applications, announced Monday it has raised $7.3 million in seed funding led by FirstMark, with participation from Bessemer Venture Partners, Bedrock, DRW, Coalition, and dozens of strategic angel investors.&lt;/p&gt;&lt;p&gt;The funding comes as the 18-month-old company experiences explosive growth in the developer community. TensorZero’s open-source repository recently achieved the “#1 trending repository of the week” spot globally on GitHub, jumping from roughly 3,000 to over 9,700 stars in recent months as enterprises grapple with the complexity of building production-ready AI applications.&lt;/p&gt;&lt;p&gt;“Despite all the noise in the industry, companies building LLM applications still lack the right tools to meet complex cognitive and infrastructure needs, and resort to stitching together whatever early solutions are available on the market,” said Matt Turck, General Partner at FirstMark, who led the investment. “TensorZero provides production-grade, enterprise-ready components for building LLM applications that natively work together in a self-reinforcing loop, out of the box.”&lt;/p&gt;&lt;p&gt;The Brooklyn-based company addresses a growing pain point for enterprises deploying AI applications at scale. While large language models like GPT-5 and Claude have demonstrated remarkable capabilities, translating these into reliable business applications requires orchestrating multiple complex systems for model access, monitoring, optimization, and experimentation.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-how-nuclear-fusion-research-shaped-a-breakthrough-ai-optimization-platform"&gt;How nuclear fusion research shaped a breakthrough AI optimization platform&lt;/h2&gt;



&lt;p&gt;TensorZero’s approach stems from co-founder and CTO Viraj Mehta’s unconventional background in reinforcement learning for nuclear fusion reactors. During his PhD at Carnegie Mellon, Mehta worked on Department of Energy research projects where data collection cost “like a car per data point — $30,000 for 5 seconds of data,” he explained in a recent interview with VentureBeat.&lt;/p&gt;



&lt;p&gt;“That problem leads to a huge amount of concern about where to focus our limited resources,” Mehta said. “We were going to only get to run a handful of trials total, so the question became: what is the marginally most valuable place we can collect data from?” This experience shaped TensorZero’s core philosophy: maximizing the value of every data point to continuously improve AI systems.&lt;/p&gt;



&lt;p&gt;The insight led Mehta and co-founder Gabriel Bianconi, former chief product officer at Ondo Finance (a decentralized finance project with over $1 billion in assets under management), to reconceptualize LLM applications as reinforcement learning problems where systems learn from real-world feedback.&lt;/p&gt;



&lt;p&gt;“LLM applications in their broader context feel like reinforcement learning problems,” Mehta explained. “You make many calls to a machine learning model with structured inputs, get structured outputs, and eventually receive some form of reward or feedback. This looks to me like a partially observable Markov decision process.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-enterprises-are-ditching-complex-vendor-integrations-for-unified-ai-infrastructure"&gt;Why enterprises are ditching complex vendor integrations for unified AI infrastructure&lt;/h2&gt;



&lt;p&gt;Traditional approaches to building LLM applications require companies to integrate numerous specialized tools from different vendors — model gateways, observability platforms, evaluation frameworks, and fine-tuning services. TensorZero unifies these capabilities into a single open-source stack designed to work together seamlessly.&lt;/p&gt;



&lt;p&gt;“Most companies didn’t go through the hassle of integrating all these different tools, and even the ones that did ended up with fragmented solutions, because those tools weren’t designed to work well with each other,” Bianconi said. “So we realized there was an opportunity to build a product that enables this feedback loop in production.”&lt;/p&gt;



&lt;p&gt;The platform’s core innovation is creating what the founders call a “data and learning flywheel” — a feedback loop that turns production metrics and human feedback into smarter, faster, and cheaper models. Built in Rust for performance, TensorZero achieves sub-millisecond latency overhead while supporting all major LLM providers through a unified API.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-major-banks-and-ai-startups-are-already-building-production-systems-on-tensorzero"&gt;Major banks and AI startups are already building production systems on TensorZero&lt;/h2&gt;



&lt;p&gt;The approach has already attracted significant enterprise adoption. One of Europe’s largest banks is using TensorZero to automate code changelog generation, while numerous AI-first startups from Series A to Series B stage have integrated the platform across diverse industries including healthcare, finance, and consumer applications.&lt;/p&gt;



&lt;p&gt;“The surge in adoption from both the open-source community and enterprises has been incredible,” Bianconi said. “We’re fortunate to have received contributions from dozens of developers worldwide, and it’s exciting to see TensorZero already powering cutting-edge LLM applications at frontier AI startups and large organizations.”&lt;/p&gt;



&lt;p&gt;The company’s customer base spans organizations from startups to major financial institutions, drawn by both the technical capabilities and the open-source nature of the platform. For enterprises with strict compliance requirements, the ability to run TensorZero within their own infrastructure provides crucial control over sensitive data.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-tensorzero-outperforms-langchain-and-other-ai-frameworks-at-enterprise-scale"&gt;How TensorZero outperforms LangChain and other AI frameworks at enterprise scale&lt;/h2&gt;



&lt;p&gt;TensorZero differentiates itself from existing solutions like LangChain and LiteLLM through its end-to-end approach and focus on production-grade deployments. While many frameworks excel at rapid prototyping, they often hit scalability ceilings that force companies to rebuild their infrastructure.&lt;/p&gt;



&lt;p&gt;“There are two dimensions to think about,” Bianconi explained. “First, there are a number of projects out there that are very good to get started quickly, and you can put a prototype out there very quickly. But often companies will hit a ceiling with many of those products and need to churn and go for something else.”&lt;/p&gt;



&lt;p&gt;The platform’s structured approach to data collection also enables more sophisticated optimization techniques. Unlike traditional observability tools that store raw text inputs and outputs, TensorZero maintains structured data about the variables that go into each inference, making it easier to retrain models and experiment with different approaches.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-rust-powered-performance-delivers-sub-millisecond-latency-at-10-000-queries-per-second"&gt;Rust-powered performance delivers sub-millisecond latency at 10,000+ queries per second&lt;/h2&gt;



&lt;p&gt;Performance has been a key design consideration. In benchmarks, TensorZero’s Rust-based gateway adds less than 1 millisecond of latency at 99th percentile while handling over 10,000 queries per second. This compares favorably to Python-based alternatives like LiteLLM, which can add 25-100x more latency at much lower throughput levels.&lt;/p&gt;



&lt;p&gt;“LiteLLM (Python) at 100 QPS adds 25-100x+ more P99 latency than our gateway at 10,000 QPS,” the founders noted in their announcement, highlighting the performance advantages of their Rust implementation.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-open-source-strategy-designed-to-eliminate-ai-vendor-lock-in-fears"&gt;The open-source strategy designed to eliminate AI vendor lock-in fears&lt;/h2&gt;



&lt;p&gt;TensorZero has committed to keeping its core platform entirely open source, with no paid features — a strategy designed to build trust with enterprise customers wary of vendor lock-in. The company plans to monetize through a managed service that automates the more complex aspects of LLM optimization, such as GPU management for custom model training and proactive optimization recommendations.&lt;/p&gt;



&lt;p&gt;“We realized very early on that we needed to make this open source, to give [enterprises] the confidence to do this,” Bianconi said. “In the future, at least a year from now realistically, we’ll come back with a complementary managed service.”&lt;/p&gt;



&lt;p&gt;The managed service will focus on automating the computationally intensive aspects of LLM optimization while maintaining the open-source core. This includes handling GPU infrastructure for fine-tuning, running automated experiments, and providing proactive suggestions for improving model performance.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-s-next-for-the-company-reshaping-enterprise-ai-infrastructure"&gt;What’s next for the company reshaping enterprise AI infrastructure&lt;/h2&gt;



&lt;p&gt;The announcement positions TensorZero at the forefront of a growing movement to solve the “LLMOps” challenge — the operational complexity of running AI applications in production. As enterprises increasingly view AI as critical business infrastructure rather than experimental technology, the demand for production-ready tooling continues to accelerate.&lt;/p&gt;



&lt;p&gt;With the new funding, TensorZero plans to accelerate development of its open-source infrastructure while building out its team. The company is currently hiring in New York and welcomes open-source contributions from the developer community. The founders are particularly excited about developing research tools that will enable faster experimentation across different AI applications.&lt;/p&gt;



&lt;p&gt;“Our ultimate vision is to enable a data and learning flywheel for optimizing LLM applications—a feedback loop that turns production metrics and human feedback into smarter, faster, and cheaper models and agents,” Mehta said. “As AI models grow smarter and take on more complex workflows, you can’t reason about them in a vacuum; you have to do so in the context of their real-world consequences.”&lt;/p&gt;



&lt;p&gt;TensorZero’s rapid GitHub growth and early enterprise traction suggest strong product-market fit in addressing one of the most pressing challenges in modern AI development. The company’s open-source approach and focus on enterprise-grade performance could prove decisive advantages in a market where developer adoption often precedes enterprise sales.&lt;/p&gt;



&lt;p&gt;For enterprises still struggling to move AI applications from prototype to production, TensorZero’s unified approach offers a compelling alternative to the current patchwork of specialized tools. As one industry observer noted, the difference between building AI demos and building AI businesses often comes down to infrastructure — and TensorZero is betting that unified, performance-oriented infrastructure will be the foundation upon which the next generation of AI companies is built.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;p&gt;TensorZero, a startup building open-source infrastructure for large language model applications, announced Monday it has raised $7.3 million in seed funding led by FirstMark, with participation from Bessemer Venture Partners, Bedrock, DRW, Coalition, and dozens of strategic angel investors.&lt;/p&gt;&lt;p&gt;The funding comes as the 18-month-old company experiences explosive growth in the developer community. TensorZero’s open-source repository recently achieved the “#1 trending repository of the week” spot globally on GitHub, jumping from roughly 3,000 to over 9,700 stars in recent months as enterprises grapple with the complexity of building production-ready AI applications.&lt;/p&gt;&lt;p&gt;“Despite all the noise in the industry, companies building LLM applications still lack the right tools to meet complex cognitive and infrastructure needs, and resort to stitching together whatever early solutions are available on the market,” said Matt Turck, General Partner at FirstMark, who led the investment. “TensorZero provides production-grade, enterprise-ready components for building LLM applications that natively work together in a self-reinforcing loop, out of the box.”&lt;/p&gt;&lt;p&gt;The Brooklyn-based company addresses a growing pain point for enterprises deploying AI applications at scale. While large language models like GPT-5 and Claude have demonstrated remarkable capabilities, translating these into reliable business applications requires orchestrating multiple complex systems for model access, monitoring, optimization, and experimentation.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-how-nuclear-fusion-research-shaped-a-breakthrough-ai-optimization-platform"&gt;How nuclear fusion research shaped a breakthrough AI optimization platform&lt;/h2&gt;



&lt;p&gt;TensorZero’s approach stems from co-founder and CTO Viraj Mehta’s unconventional background in reinforcement learning for nuclear fusion reactors. During his PhD at Carnegie Mellon, Mehta worked on Department of Energy research projects where data collection cost “like a car per data point — $30,000 for 5 seconds of data,” he explained in a recent interview with VentureBeat.&lt;/p&gt;



&lt;p&gt;“That problem leads to a huge amount of concern about where to focus our limited resources,” Mehta said. “We were going to only get to run a handful of trials total, so the question became: what is the marginally most valuable place we can collect data from?” This experience shaped TensorZero’s core philosophy: maximizing the value of every data point to continuously improve AI systems.&lt;/p&gt;



&lt;p&gt;The insight led Mehta and co-founder Gabriel Bianconi, former chief product officer at Ondo Finance (a decentralized finance project with over $1 billion in assets under management), to reconceptualize LLM applications as reinforcement learning problems where systems learn from real-world feedback.&lt;/p&gt;



&lt;p&gt;“LLM applications in their broader context feel like reinforcement learning problems,” Mehta explained. “You make many calls to a machine learning model with structured inputs, get structured outputs, and eventually receive some form of reward or feedback. This looks to me like a partially observable Markov decision process.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-enterprises-are-ditching-complex-vendor-integrations-for-unified-ai-infrastructure"&gt;Why enterprises are ditching complex vendor integrations for unified AI infrastructure&lt;/h2&gt;



&lt;p&gt;Traditional approaches to building LLM applications require companies to integrate numerous specialized tools from different vendors — model gateways, observability platforms, evaluation frameworks, and fine-tuning services. TensorZero unifies these capabilities into a single open-source stack designed to work together seamlessly.&lt;/p&gt;



&lt;p&gt;“Most companies didn’t go through the hassle of integrating all these different tools, and even the ones that did ended up with fragmented solutions, because those tools weren’t designed to work well with each other,” Bianconi said. “So we realized there was an opportunity to build a product that enables this feedback loop in production.”&lt;/p&gt;



&lt;p&gt;The platform’s core innovation is creating what the founders call a “data and learning flywheel” — a feedback loop that turns production metrics and human feedback into smarter, faster, and cheaper models. Built in Rust for performance, TensorZero achieves sub-millisecond latency overhead while supporting all major LLM providers through a unified API.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-major-banks-and-ai-startups-are-already-building-production-systems-on-tensorzero"&gt;Major banks and AI startups are already building production systems on TensorZero&lt;/h2&gt;



&lt;p&gt;The approach has already attracted significant enterprise adoption. One of Europe’s largest banks is using TensorZero to automate code changelog generation, while numerous AI-first startups from Series A to Series B stage have integrated the platform across diverse industries including healthcare, finance, and consumer applications.&lt;/p&gt;



&lt;p&gt;“The surge in adoption from both the open-source community and enterprises has been incredible,” Bianconi said. “We’re fortunate to have received contributions from dozens of developers worldwide, and it’s exciting to see TensorZero already powering cutting-edge LLM applications at frontier AI startups and large organizations.”&lt;/p&gt;



&lt;p&gt;The company’s customer base spans organizations from startups to major financial institutions, drawn by both the technical capabilities and the open-source nature of the platform. For enterprises with strict compliance requirements, the ability to run TensorZero within their own infrastructure provides crucial control over sensitive data.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-tensorzero-outperforms-langchain-and-other-ai-frameworks-at-enterprise-scale"&gt;How TensorZero outperforms LangChain and other AI frameworks at enterprise scale&lt;/h2&gt;



&lt;p&gt;TensorZero differentiates itself from existing solutions like LangChain and LiteLLM through its end-to-end approach and focus on production-grade deployments. While many frameworks excel at rapid prototyping, they often hit scalability ceilings that force companies to rebuild their infrastructure.&lt;/p&gt;



&lt;p&gt;“There are two dimensions to think about,” Bianconi explained. “First, there are a number of projects out there that are very good to get started quickly, and you can put a prototype out there very quickly. But often companies will hit a ceiling with many of those products and need to churn and go for something else.”&lt;/p&gt;



&lt;p&gt;The platform’s structured approach to data collection also enables more sophisticated optimization techniques. Unlike traditional observability tools that store raw text inputs and outputs, TensorZero maintains structured data about the variables that go into each inference, making it easier to retrain models and experiment with different approaches.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-rust-powered-performance-delivers-sub-millisecond-latency-at-10-000-queries-per-second"&gt;Rust-powered performance delivers sub-millisecond latency at 10,000+ queries per second&lt;/h2&gt;



&lt;p&gt;Performance has been a key design consideration. In benchmarks, TensorZero’s Rust-based gateway adds less than 1 millisecond of latency at 99th percentile while handling over 10,000 queries per second. This compares favorably to Python-based alternatives like LiteLLM, which can add 25-100x more latency at much lower throughput levels.&lt;/p&gt;



&lt;p&gt;“LiteLLM (Python) at 100 QPS adds 25-100x+ more P99 latency than our gateway at 10,000 QPS,” the founders noted in their announcement, highlighting the performance advantages of their Rust implementation.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-open-source-strategy-designed-to-eliminate-ai-vendor-lock-in-fears"&gt;The open-source strategy designed to eliminate AI vendor lock-in fears&lt;/h2&gt;



&lt;p&gt;TensorZero has committed to keeping its core platform entirely open source, with no paid features — a strategy designed to build trust with enterprise customers wary of vendor lock-in. The company plans to monetize through a managed service that automates the more complex aspects of LLM optimization, such as GPU management for custom model training and proactive optimization recommendations.&lt;/p&gt;



&lt;p&gt;“We realized very early on that we needed to make this open source, to give [enterprises] the confidence to do this,” Bianconi said. “In the future, at least a year from now realistically, we’ll come back with a complementary managed service.”&lt;/p&gt;



&lt;p&gt;The managed service will focus on automating the computationally intensive aspects of LLM optimization while maintaining the open-source core. This includes handling GPU infrastructure for fine-tuning, running automated experiments, and providing proactive suggestions for improving model performance.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-s-next-for-the-company-reshaping-enterprise-ai-infrastructure"&gt;What’s next for the company reshaping enterprise AI infrastructure&lt;/h2&gt;



&lt;p&gt;The announcement positions TensorZero at the forefront of a growing movement to solve the “LLMOps” challenge — the operational complexity of running AI applications in production. As enterprises increasingly view AI as critical business infrastructure rather than experimental technology, the demand for production-ready tooling continues to accelerate.&lt;/p&gt;



&lt;p&gt;With the new funding, TensorZero plans to accelerate development of its open-source infrastructure while building out its team. The company is currently hiring in New York and welcomes open-source contributions from the developer community. The founders are particularly excited about developing research tools that will enable faster experimentation across different AI applications.&lt;/p&gt;



&lt;p&gt;“Our ultimate vision is to enable a data and learning flywheel for optimizing LLM applications—a feedback loop that turns production metrics and human feedback into smarter, faster, and cheaper models and agents,” Mehta said. “As AI models grow smarter and take on more complex workflows, you can’t reason about them in a vacuum; you have to do so in the context of their real-world consequences.”&lt;/p&gt;



&lt;p&gt;TensorZero’s rapid GitHub growth and early enterprise traction suggest strong product-market fit in addressing one of the most pressing challenges in modern AI development. The company’s open-source approach and focus on enterprise-grade performance could prove decisive advantages in a market where developer adoption often precedes enterprise sales.&lt;/p&gt;



&lt;p&gt;For enterprises still struggling to move AI applications from prototype to production, TensorZero’s unified approach offers a compelling alternative to the current patchwork of specialized tools. As one industry observer noted, the difference between building AI demos and building AI businesses often comes down to infrastructure — and TensorZero is betting that unified, performance-oriented infrastructure will be the foundation upon which the next generation of AI companies is built.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/tensorzero-nabs-7-3m-seed-to-solve-the-messy-world-of-enterprise-llm-development/</guid><pubDate>Mon, 18 Aug 2025 19:13:32 +0000</pubDate></item><item><title>At Gamescom 2025, NVIDIA DLSS 4 and Ray Tracing Come to This Year’s Biggest Titles (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/gamescom-2025-dlss-4-ray-tracing/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/gamescom-2025-dlss.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;With over 175 games now supporting NVIDIA DLSS 4 — a suite of advanced, AI-powered neural rendering technologies — gamers and tech enthusiasts everywhere can experience breakthrough performance in this year’s most anticipated titles, including &lt;i&gt;Borderlands 4&lt;/i&gt;, &lt;i&gt;Hell Is Us&lt;/i&gt; and &lt;i&gt;Fate Trigger&lt;/i&gt;.&lt;/p&gt;
&lt;p&gt;Plus, path tracing is making its way to &lt;i&gt;Resident Evil Requiem&lt;/i&gt; and &lt;i&gt;Directive 8020&lt;/i&gt;, as well as ray tracing in upcoming releases like &lt;i&gt;Phantom Blade Zero&lt;/i&gt;, &lt;i&gt;PRAGMATA&lt;/i&gt; and &lt;i&gt;CINDER CITY&lt;/i&gt; — enabling crystal-clear visuals for more immersive gameplay&lt;/p&gt;
&lt;p&gt;“DLSS 4 and path tracing are no longer cutting-edge graphical experiments — they’re the foundation of modern PC gaming titles,” said Matt Wuebbling, vice president of global GeForce marketing at NVIDIA. “Developers are embracing AI-powered rendering to unlock stunning visuals and massive performance gains, enabling gamers everywhere to experience the future of real-time graphics today.”&lt;/p&gt;
&lt;p&gt;These announcements come alongside a new NVIDIA GeForce RTX 50 Series bundle for &lt;i&gt;Borderlands 4&lt;/i&gt; and updates to the NVIDIA app — a companion platform for content creators, gamers and AI enthusiasts using NVIDIA GeForce RTX GPUs.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;DLSS 4 Now Accelerating Over 175 Games and Applications&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Launched with the GeForce RTX 50 Series earlier this year, DLSS 4 with Multi Frame Generation uses AI to generate up to three frames for every traditionally rendered frame, delivering performance boosts of up to 8x over traditional rendering.&lt;/p&gt;
&lt;p&gt;In addition to Multi Frame Generation, DLSS 4 titles include support for DLSS Super Resolution, Ray Reconstruction and NVIDIA Reflex technology — unlocking incredible performance gains and responsive gameplay for every GeForce RTX 50 Series owner.&lt;/p&gt;
&lt;p&gt;New titles announced at Gamescom that will support the latest RTX technologies include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Directive 8020&lt;/i&gt; and &lt;i&gt;Resident Evil Requiem&lt;/i&gt;, which are launching with DLSS 4 and path tracing&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Black State&lt;/i&gt;, &lt;i&gt;CINDER CITY&lt;/i&gt; (formerly &lt;i&gt;Project LLL&lt;/i&gt;), &lt;i&gt;Cronos: The New Dawn&lt;/i&gt;, &lt;i&gt;Dying Light: The Beast&lt;/i&gt;, &lt;i&gt;Honeycomb: The World Beyond&lt;/i&gt;, &lt;i&gt;Lost Soul Aside&lt;/i&gt;, &lt;i&gt;The Outer Worlds 2&lt;/i&gt;, &lt;i&gt;Phantom Blade Zero&lt;/i&gt; and &lt;i&gt;PRAGMATA&lt;/i&gt;, which are launching with DLSS 4 and ray tracing&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Borderlands 4&lt;/i&gt; and &lt;i&gt;Fate Trigger&lt;/i&gt;, which are launching with DLSS 4 with Multi Frame Generation&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Indiana Jones and the Great Circle, &lt;/i&gt;which in September will add support for RTX Hair, a technology that uses new hardware capabilities in RTX 50 Series GPUs to model hair with greater path-traced detail and realism&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Many of these RTX titles will also launch on the GeForce NOW cloud gaming platform, including&lt;i&gt; Borderlands 4, CINDER CITY &lt;/i&gt;(formerly&lt;i&gt; Project LLL&lt;/i&gt;)&lt;i&gt;, Hell Is Us &lt;/i&gt;and &lt;i&gt;The Outer Worlds 2.&lt;/i&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA App Adds Global DLSS Overrides and Software Updates&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The NVIDIA app is the essential companion for NVIDIA GeForce RTX GPU users, simplifying the process of keeping PCs updated with the latest GeForce Game Ready and NVIDIA Studio Drivers.&lt;/p&gt;
&lt;p&gt;New updates to the NVIDIA app include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Global DLSS Overrides:&lt;/b&gt; Easily enable DLSS Multi-Frame Generation or DLSS Super Resolution profiles globally across hundreds of DLSS Override titles, instead of needing to configure per title.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Project G-Assist Upgrades: &lt;/b&gt;The latest update to Project G-Assist — an on-device AI assistant that lets users control and tune their RTX systems with voice and text commands — introduces a significantly more efficient AI model that uses 40% less memory. Despite its smaller footprint, it responds to queries faster and more accurately calls the right tools.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Highly Requested Legacy 3D Settings:&lt;/b&gt; Use easily configurable control panel settings — including anisotropic filtering, anti-aliasing and ambient occlusion — to enhance classic games.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The NVIDIA app beta update launches Tuesday, Aug. 19, at 9 a.m. PT, with full availability coming the following week.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA ACE Enhances Voice-Driven Gaming Experiences&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA ACE — a suite of generative AI technologies that power lifelike non-playable character interactions in games like Krafton’s &lt;i&gt;inZOI&lt;/i&gt; — now features in Iconic Interactive’s &lt;i&gt;The Oversight Bureau&lt;/i&gt;, a darkly comic, voice-driven puzzle game.&lt;/p&gt;
&lt;p&gt;Using speech-to-text technology powered by ACE, players can interact naturally with in-game characters using speech, with Iconic’s Narrative Engine interpreting the input and determining and delivering the pre-recorded character dialogue that best fits the story and situation.&lt;/p&gt;
&lt;p&gt;This system keeps developers in creative control while offering players real agency in games — all running locally on RTX AI PCs with sub-second latency.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;The Oversight Bureau&lt;/i&gt; launches later this year and will be playable at NVIDIA’s Gamescom B2B press suite.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA RTX Remix Evolves With Community Expansions and New Particle System&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA RTX Remix, an open-source modding platform for remastering classic games with path tracing and neural rendering, continues to grow thanks to its passionate community.&lt;/p&gt;
&lt;p&gt;Modders have been using large language models to extend RTX Remix’s capabilities. For example, one modder “vibe coded” a plug-in that connects RTX Remix to Adobe Substance 3D, the industry-standard tool for 3D texturing and materials. Another modder made it possible for RTX Remix to use classic game data to instantly make objects glow with emissive effects.&lt;/p&gt;
&lt;p&gt;RTX Remix’s open-source community has even expanded compatibility to allow many new titles to be remastered, including iconic games like &lt;i&gt;Call Of Duty 4: Modern Warfare&lt;/i&gt;, &lt;i&gt;Knights Of The Old Republic&lt;/i&gt;, &lt;i&gt;Doom 3&lt;/i&gt;, &lt;i&gt;Half-Life: Black Mesa&lt;/i&gt; and &lt;i&gt;Bioshock.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Some of these games were featured in the RTX Remix’s $50K Mod Contest, which wrapped up at Gamescom. &lt;i&gt;Painkiller RTX&lt;/i&gt; by Merry Pencil Studios won numerous awards, including “Best Overall RTX Remix Mod.” Explore all mod submissions on ModDB.com.&lt;/p&gt;
&lt;p&gt;At Gamescom, NVIDIA also unveiled a new RTX Remix particle system that brings dynamic, realistically lit and physically accurate particles to 165 classic games — the majority of which have never had a particle editor.&lt;/p&gt;
&lt;p&gt;Modders can use the system to change the look, size, quantity, light emission, turbulence and even gravity of particles in games. The new particle system will be available in September.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;‘Borderlands 4’ GeForce RTX 50 Series Bundle Available Now&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;To celebrate Gearbox’s &lt;i&gt;Borderlands 4&lt;/i&gt;, which will be enhanced by DLSS 4 with Multi Frame Generation and NVIDIA Reflex, NVIDIA is introducing a new GeForce RTX 50 Series bundle.&lt;/p&gt;
&lt;p&gt;Players who purchase a GeForce RTX 5090, 5080, 5070 Ti, or 5070 desktop system or graphics card — or laptops with a GeForce RTX™ 5090 Laptop GPU, RTX 5080 Laptop GPU, RTX 5070 Ti Laptop GPU or RTX 5070 Laptop GPU from participating retailers — will receive a copy of &lt;i&gt;Borderlands 4&lt;/i&gt; and &lt;i&gt;The Gilded Glory Pack&lt;/i&gt; DLC. The offer is available through Monday, Sept. 22.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about GeForce announcements at &lt;/i&gt;&lt;i&gt;Gamescom&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/gamescom-2025-dlss.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;With over 175 games now supporting NVIDIA DLSS 4 — a suite of advanced, AI-powered neural rendering technologies — gamers and tech enthusiasts everywhere can experience breakthrough performance in this year’s most anticipated titles, including &lt;i&gt;Borderlands 4&lt;/i&gt;, &lt;i&gt;Hell Is Us&lt;/i&gt; and &lt;i&gt;Fate Trigger&lt;/i&gt;.&lt;/p&gt;
&lt;p&gt;Plus, path tracing is making its way to &lt;i&gt;Resident Evil Requiem&lt;/i&gt; and &lt;i&gt;Directive 8020&lt;/i&gt;, as well as ray tracing in upcoming releases like &lt;i&gt;Phantom Blade Zero&lt;/i&gt;, &lt;i&gt;PRAGMATA&lt;/i&gt; and &lt;i&gt;CINDER CITY&lt;/i&gt; — enabling crystal-clear visuals for more immersive gameplay&lt;/p&gt;
&lt;p&gt;“DLSS 4 and path tracing are no longer cutting-edge graphical experiments — they’re the foundation of modern PC gaming titles,” said Matt Wuebbling, vice president of global GeForce marketing at NVIDIA. “Developers are embracing AI-powered rendering to unlock stunning visuals and massive performance gains, enabling gamers everywhere to experience the future of real-time graphics today.”&lt;/p&gt;
&lt;p&gt;These announcements come alongside a new NVIDIA GeForce RTX 50 Series bundle for &lt;i&gt;Borderlands 4&lt;/i&gt; and updates to the NVIDIA app — a companion platform for content creators, gamers and AI enthusiasts using NVIDIA GeForce RTX GPUs.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;DLSS 4 Now Accelerating Over 175 Games and Applications&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Launched with the GeForce RTX 50 Series earlier this year, DLSS 4 with Multi Frame Generation uses AI to generate up to three frames for every traditionally rendered frame, delivering performance boosts of up to 8x over traditional rendering.&lt;/p&gt;
&lt;p&gt;In addition to Multi Frame Generation, DLSS 4 titles include support for DLSS Super Resolution, Ray Reconstruction and NVIDIA Reflex technology — unlocking incredible performance gains and responsive gameplay for every GeForce RTX 50 Series owner.&lt;/p&gt;
&lt;p&gt;New titles announced at Gamescom that will support the latest RTX technologies include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Directive 8020&lt;/i&gt; and &lt;i&gt;Resident Evil Requiem&lt;/i&gt;, which are launching with DLSS 4 and path tracing&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Black State&lt;/i&gt;, &lt;i&gt;CINDER CITY&lt;/i&gt; (formerly &lt;i&gt;Project LLL&lt;/i&gt;), &lt;i&gt;Cronos: The New Dawn&lt;/i&gt;, &lt;i&gt;Dying Light: The Beast&lt;/i&gt;, &lt;i&gt;Honeycomb: The World Beyond&lt;/i&gt;, &lt;i&gt;Lost Soul Aside&lt;/i&gt;, &lt;i&gt;The Outer Worlds 2&lt;/i&gt;, &lt;i&gt;Phantom Blade Zero&lt;/i&gt; and &lt;i&gt;PRAGMATA&lt;/i&gt;, which are launching with DLSS 4 and ray tracing&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Borderlands 4&lt;/i&gt; and &lt;i&gt;Fate Trigger&lt;/i&gt;, which are launching with DLSS 4 with Multi Frame Generation&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Indiana Jones and the Great Circle, &lt;/i&gt;which in September will add support for RTX Hair, a technology that uses new hardware capabilities in RTX 50 Series GPUs to model hair with greater path-traced detail and realism&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Many of these RTX titles will also launch on the GeForce NOW cloud gaming platform, including&lt;i&gt; Borderlands 4, CINDER CITY &lt;/i&gt;(formerly&lt;i&gt; Project LLL&lt;/i&gt;)&lt;i&gt;, Hell Is Us &lt;/i&gt;and &lt;i&gt;The Outer Worlds 2.&lt;/i&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA App Adds Global DLSS Overrides and Software Updates&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The NVIDIA app is the essential companion for NVIDIA GeForce RTX GPU users, simplifying the process of keeping PCs updated with the latest GeForce Game Ready and NVIDIA Studio Drivers.&lt;/p&gt;
&lt;p&gt;New updates to the NVIDIA app include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Global DLSS Overrides:&lt;/b&gt; Easily enable DLSS Multi-Frame Generation or DLSS Super Resolution profiles globally across hundreds of DLSS Override titles, instead of needing to configure per title.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Project G-Assist Upgrades: &lt;/b&gt;The latest update to Project G-Assist — an on-device AI assistant that lets users control and tune their RTX systems with voice and text commands — introduces a significantly more efficient AI model that uses 40% less memory. Despite its smaller footprint, it responds to queries faster and more accurately calls the right tools.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Highly Requested Legacy 3D Settings:&lt;/b&gt; Use easily configurable control panel settings — including anisotropic filtering, anti-aliasing and ambient occlusion — to enhance classic games.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The NVIDIA app beta update launches Tuesday, Aug. 19, at 9 a.m. PT, with full availability coming the following week.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA ACE Enhances Voice-Driven Gaming Experiences&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA ACE — a suite of generative AI technologies that power lifelike non-playable character interactions in games like Krafton’s &lt;i&gt;inZOI&lt;/i&gt; — now features in Iconic Interactive’s &lt;i&gt;The Oversight Bureau&lt;/i&gt;, a darkly comic, voice-driven puzzle game.&lt;/p&gt;
&lt;p&gt;Using speech-to-text technology powered by ACE, players can interact naturally with in-game characters using speech, with Iconic’s Narrative Engine interpreting the input and determining and delivering the pre-recorded character dialogue that best fits the story and situation.&lt;/p&gt;
&lt;p&gt;This system keeps developers in creative control while offering players real agency in games — all running locally on RTX AI PCs with sub-second latency.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;The Oversight Bureau&lt;/i&gt; launches later this year and will be playable at NVIDIA’s Gamescom B2B press suite.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA RTX Remix Evolves With Community Expansions and New Particle System&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA RTX Remix, an open-source modding platform for remastering classic games with path tracing and neural rendering, continues to grow thanks to its passionate community.&lt;/p&gt;
&lt;p&gt;Modders have been using large language models to extend RTX Remix’s capabilities. For example, one modder “vibe coded” a plug-in that connects RTX Remix to Adobe Substance 3D, the industry-standard tool for 3D texturing and materials. Another modder made it possible for RTX Remix to use classic game data to instantly make objects glow with emissive effects.&lt;/p&gt;
&lt;p&gt;RTX Remix’s open-source community has even expanded compatibility to allow many new titles to be remastered, including iconic games like &lt;i&gt;Call Of Duty 4: Modern Warfare&lt;/i&gt;, &lt;i&gt;Knights Of The Old Republic&lt;/i&gt;, &lt;i&gt;Doom 3&lt;/i&gt;, &lt;i&gt;Half-Life: Black Mesa&lt;/i&gt; and &lt;i&gt;Bioshock.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Some of these games were featured in the RTX Remix’s $50K Mod Contest, which wrapped up at Gamescom. &lt;i&gt;Painkiller RTX&lt;/i&gt; by Merry Pencil Studios won numerous awards, including “Best Overall RTX Remix Mod.” Explore all mod submissions on ModDB.com.&lt;/p&gt;
&lt;p&gt;At Gamescom, NVIDIA also unveiled a new RTX Remix particle system that brings dynamic, realistically lit and physically accurate particles to 165 classic games — the majority of which have never had a particle editor.&lt;/p&gt;
&lt;p&gt;Modders can use the system to change the look, size, quantity, light emission, turbulence and even gravity of particles in games. The new particle system will be available in September.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;‘Borderlands 4’ GeForce RTX 50 Series Bundle Available Now&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;To celebrate Gearbox’s &lt;i&gt;Borderlands 4&lt;/i&gt;, which will be enhanced by DLSS 4 with Multi Frame Generation and NVIDIA Reflex, NVIDIA is introducing a new GeForce RTX 50 Series bundle.&lt;/p&gt;
&lt;p&gt;Players who purchase a GeForce RTX 5090, 5080, 5070 Ti, or 5070 desktop system or graphics card — or laptops with a GeForce RTX™ 5090 Laptop GPU, RTX 5080 Laptop GPU, RTX 5070 Ti Laptop GPU or RTX 5070 Laptop GPU from participating retailers — will receive a copy of &lt;i&gt;Borderlands 4&lt;/i&gt; and &lt;i&gt;The Gilded Glory Pack&lt;/i&gt; DLC. The offer is available through Monday, Sept. 22.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about GeForce announcements at &lt;/i&gt;&lt;i&gt;Gamescom&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/gamescom-2025-dlss-4-ray-tracing/</guid><pubDate>Mon, 18 Aug 2025 19:30:01 +0000</pubDate></item><item><title>New Lightweight AI Model for Project G-Assist Brings Support for 6GB NVIDIA GeForce RTX and RTX PRO GPUs (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/rtx-ai-garage-gamescom-g-assist-rtx-remix/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;At Gamescom, NVIDIA is releasing its first major update to Project G‑Assist — an experimental on-device AI assistant that allows users to tune their NVIDIA RTX systems with voice and text commands.&lt;/p&gt;
&lt;p&gt;The update brings a new AI model that uses 40% less VRAM, improves tool-calling intelligence and extends G-Assist support to all RTX GPUs with 6GB or more VRAM, including laptops. Plus, a new G-Assist Plug-In Hub enables users to easily discover and download plug-ins to enable more G-Assist features.&lt;/p&gt;
&lt;p&gt;NVIDIA also announced a new path-traced particle system, coming in September to the NVIDIA RTX Remix modding platform, that brings fully simulated physics, dynamic shadows and realistic reflections to visual effects.&lt;/p&gt;
&lt;p&gt;In addition, NVIDIA named the winners of the NVIDIA and ModDB RTX Remix Mod Contest. Check out the winners and finalist RTX mods in the RTX Remix GeForce article.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;G-Assist Gets Smarter, Expands to More RTX PCs&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The modern PC is a powerhouse, but unlocking its full potential means navigating a complex maze of settings across system software, GPU and peripheral utilities, control panels and more.&lt;/p&gt;
&lt;p&gt;Project G-Assist is a free, on-device AI assistant built to cut through that complexity. It acts as a central command center, providing easy access to functions previously buried in menus through voice or text commands. Users can ask the assistant to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run diagnostics to optimize game performance&lt;/li&gt;
&lt;li&gt;Display or chart frame rates, latency and GPU temperatures&lt;/li&gt;
&lt;li&gt;Adjust GPU or even peripheral settings, such as keyboard lighting&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;The G-Assist update also introduces a new, significantly more efficient AI model that’s faster and uses 40% less memory while maintaining response accuracy. The more efficient model means that G-Assist can now run on all RTX GPUs with 6GB or more VRAM, including laptops.&lt;/p&gt;
&lt;p&gt;Getting started is simple: install the NVIDIA app and the latest Game Ready Driver on Aug. 19, download the G-Assist update from the app’s home screen and press Alt+G to activate.&lt;/p&gt;
&lt;p&gt;Another G-Assist update coming in September will introduce support for laptop-specific commands for features like NVIDIA BatteryBoost and Battery OPS.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Introducing the G-Assist Plug-In Hub With Mod.io&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA is collaborating with mod.io to launch the G-Assist Plug-In Hub, which allows users to easily access G-Assist plug-ins, as well as discover and download community-created ones.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_83855"&gt;&lt;img alt="alt" class="size-full wp-image-83855" height="611" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/mod.io-plug-in.jpg" width="777" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-83855"&gt;With the mod.io plug-in, users can ask G-Assist to discover and install new plug-ins.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;With the latest update, users can also directly ask G-Assist what new plug-ins are available in the hub and install them using natural language, thanks to a mod.io plug-in.&lt;/p&gt;
&lt;p&gt;The recent G-Assist Plug-In Hackathon showcased the incredible creativity of the G-Assist community. Here’s a sneak peek of what they came up with:&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Some finalists include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Omniplay&lt;/b&gt; — allows gamers to use G-Assist to research lore from online wikis or take notes in real time while gaming&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Launchpad&lt;/b&gt; — lets gamers set, launch and toggle custom app groups on the fly to boost productivity&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Flux NIM Microservice for G-Assist&lt;/b&gt; — allows gamers to easily generate AI images from within G-Assist, using on-device NVIDIA NIM microservices&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The winners of the hackathon will be announced on Wednesday, Aug. 20.&lt;/p&gt;
&lt;p&gt;Building custom plug-ins is simple. They’re based on a foundation of JSON and Python scripts — and the Project G-Assist Plug-In Builder helps further simplify development by enabling users to code plug-ins with natural language.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Mod It Like It’s Hot With RTX Remix&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Classic PC games remain beloved for their unforgettable stories, characters and gameplay — but their dated graphics can be a barrier for new and longtime players.&lt;/p&gt;
&lt;p&gt;NVIDIA RTX Remix enables modders to revitalize these timeless titles with the latest NVIDIA gaming technologies — bridging nostalgic gameplay with modern visuals.&lt;/p&gt;
&lt;p&gt;Since the platform’s release, the RTX Remix modding community has grown with over 350 active projects and over 100 mods released. The mods span a catalog of beloved games like &lt;i&gt;Half-Life 2&lt;/i&gt;, &lt;i&gt;Need for Speed: Underground&lt;/i&gt;, &lt;i&gt;Portal 2 &lt;/i&gt;and &lt;i&gt;Deus Ex — &lt;/i&gt;and have amassed over 2 million downloads.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;In May, NVIDIA invited modders to participate in the NVIDIA and ModDB RTX Remix Mod Contest for a chance to win $50,000 in cash prizes. At Gamescom, NVIDIA announced the winners:&lt;/p&gt;

&lt;p&gt;These modders tapped RTX Remix and generative AI to bring their creations to life — from enhancing textures to quickly creating images and 3D assets.&lt;/p&gt;
&lt;p&gt;For example, the Merry Pencil Studios modder team used a workflow that seamlessly connected RTX Remix and ComfyUI, allowing them to simply select textures in the RTX Remix viewport and, with a single click in ComfyUI, restore them.&lt;/p&gt;
&lt;p&gt;The results are stunning, with each texture meticulously recreated with physically based materials layered with grime and rust. With a fully path-traced lighting system, the game’s gothic horror atmosphere has never felt more immersive to play through.&lt;/p&gt;
&lt;p&gt;All mods submitted to the RTX Remix Modding Contest, as well as 100 more Remix mods, are available to download from ModDB. For a sneak peek at RTX Remix projects under active development, check out the RTX Remix Showcase Discord server.&lt;/p&gt;
&lt;p&gt;Another RTX Remix update coming in September will allow modders to create new particles that match the look of those found in modern titles. This opens the door for over 165 RTX Remix-compatible games to have particles for the first time.&lt;/p&gt;
&lt;p&gt;To get started creating RTX mods, download NVIDIA RTX Remix from the home screen of the NVIDIA app. Read the RTX Remix article to learn more about the contest and winners.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Each week, the &lt;/i&gt;&lt;i&gt;RTX AI Garage&lt;/i&gt; &lt;i&gt;blog series features community-driven AI innovations and content for those looking to learn more about NVIDIA NIM microservices and AI Blueprints, as well as building &lt;/i&gt;&lt;i&gt;AI agents&lt;/i&gt;&lt;i&gt;, creative workflows, productivity apps and more on AI PCs and workstations.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; — and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;. Join NVIDIA’s &lt;/i&gt;&lt;i&gt;Discord server&lt;/i&gt;&lt;i&gt; to connect with community developers and AI enthusiasts for discussions on what’s possible with RTX AI.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;At Gamescom, NVIDIA is releasing its first major update to Project G‑Assist — an experimental on-device AI assistant that allows users to tune their NVIDIA RTX systems with voice and text commands.&lt;/p&gt;
&lt;p&gt;The update brings a new AI model that uses 40% less VRAM, improves tool-calling intelligence and extends G-Assist support to all RTX GPUs with 6GB or more VRAM, including laptops. Plus, a new G-Assist Plug-In Hub enables users to easily discover and download plug-ins to enable more G-Assist features.&lt;/p&gt;
&lt;p&gt;NVIDIA also announced a new path-traced particle system, coming in September to the NVIDIA RTX Remix modding platform, that brings fully simulated physics, dynamic shadows and realistic reflections to visual effects.&lt;/p&gt;
&lt;p&gt;In addition, NVIDIA named the winners of the NVIDIA and ModDB RTX Remix Mod Contest. Check out the winners and finalist RTX mods in the RTX Remix GeForce article.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;G-Assist Gets Smarter, Expands to More RTX PCs&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The modern PC is a powerhouse, but unlocking its full potential means navigating a complex maze of settings across system software, GPU and peripheral utilities, control panels and more.&lt;/p&gt;
&lt;p&gt;Project G-Assist is a free, on-device AI assistant built to cut through that complexity. It acts as a central command center, providing easy access to functions previously buried in menus through voice or text commands. Users can ask the assistant to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run diagnostics to optimize game performance&lt;/li&gt;
&lt;li&gt;Display or chart frame rates, latency and GPU temperatures&lt;/li&gt;
&lt;li&gt;Adjust GPU or even peripheral settings, such as keyboard lighting&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;The G-Assist update also introduces a new, significantly more efficient AI model that’s faster and uses 40% less memory while maintaining response accuracy. The more efficient model means that G-Assist can now run on all RTX GPUs with 6GB or more VRAM, including laptops.&lt;/p&gt;
&lt;p&gt;Getting started is simple: install the NVIDIA app and the latest Game Ready Driver on Aug. 19, download the G-Assist update from the app’s home screen and press Alt+G to activate.&lt;/p&gt;
&lt;p&gt;Another G-Assist update coming in September will introduce support for laptop-specific commands for features like NVIDIA BatteryBoost and Battery OPS.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Introducing the G-Assist Plug-In Hub With Mod.io&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA is collaborating with mod.io to launch the G-Assist Plug-In Hub, which allows users to easily access G-Assist plug-ins, as well as discover and download community-created ones.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_83855"&gt;&lt;img alt="alt" class="size-full wp-image-83855" height="611" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/mod.io-plug-in.jpg" width="777" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-83855"&gt;With the mod.io plug-in, users can ask G-Assist to discover and install new plug-ins.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;With the latest update, users can also directly ask G-Assist what new plug-ins are available in the hub and install them using natural language, thanks to a mod.io plug-in.&lt;/p&gt;
&lt;p&gt;The recent G-Assist Plug-In Hackathon showcased the incredible creativity of the G-Assist community. Here’s a sneak peek of what they came up with:&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Some finalists include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Omniplay&lt;/b&gt; — allows gamers to use G-Assist to research lore from online wikis or take notes in real time while gaming&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Launchpad&lt;/b&gt; — lets gamers set, launch and toggle custom app groups on the fly to boost productivity&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Flux NIM Microservice for G-Assist&lt;/b&gt; — allows gamers to easily generate AI images from within G-Assist, using on-device NVIDIA NIM microservices&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The winners of the hackathon will be announced on Wednesday, Aug. 20.&lt;/p&gt;
&lt;p&gt;Building custom plug-ins is simple. They’re based on a foundation of JSON and Python scripts — and the Project G-Assist Plug-In Builder helps further simplify development by enabling users to code plug-ins with natural language.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Mod It Like It’s Hot With RTX Remix&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Classic PC games remain beloved for their unforgettable stories, characters and gameplay — but their dated graphics can be a barrier for new and longtime players.&lt;/p&gt;
&lt;p&gt;NVIDIA RTX Remix enables modders to revitalize these timeless titles with the latest NVIDIA gaming technologies — bridging nostalgic gameplay with modern visuals.&lt;/p&gt;
&lt;p&gt;Since the platform’s release, the RTX Remix modding community has grown with over 350 active projects and over 100 mods released. The mods span a catalog of beloved games like &lt;i&gt;Half-Life 2&lt;/i&gt;, &lt;i&gt;Need for Speed: Underground&lt;/i&gt;, &lt;i&gt;Portal 2 &lt;/i&gt;and &lt;i&gt;Deus Ex — &lt;/i&gt;and have amassed over 2 million downloads.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;In May, NVIDIA invited modders to participate in the NVIDIA and ModDB RTX Remix Mod Contest for a chance to win $50,000 in cash prizes. At Gamescom, NVIDIA announced the winners:&lt;/p&gt;

&lt;p&gt;These modders tapped RTX Remix and generative AI to bring their creations to life — from enhancing textures to quickly creating images and 3D assets.&lt;/p&gt;
&lt;p&gt;For example, the Merry Pencil Studios modder team used a workflow that seamlessly connected RTX Remix and ComfyUI, allowing them to simply select textures in the RTX Remix viewport and, with a single click in ComfyUI, restore them.&lt;/p&gt;
&lt;p&gt;The results are stunning, with each texture meticulously recreated with physically based materials layered with grime and rust. With a fully path-traced lighting system, the game’s gothic horror atmosphere has never felt more immersive to play through.&lt;/p&gt;
&lt;p&gt;All mods submitted to the RTX Remix Modding Contest, as well as 100 more Remix mods, are available to download from ModDB. For a sneak peek at RTX Remix projects under active development, check out the RTX Remix Showcase Discord server.&lt;/p&gt;
&lt;p&gt;Another RTX Remix update coming in September will allow modders to create new particles that match the look of those found in modern titles. This opens the door for over 165 RTX Remix-compatible games to have particles for the first time.&lt;/p&gt;
&lt;p&gt;To get started creating RTX mods, download NVIDIA RTX Remix from the home screen of the NVIDIA app. Read the RTX Remix article to learn more about the contest and winners.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Each week, the &lt;/i&gt;&lt;i&gt;RTX AI Garage&lt;/i&gt; &lt;i&gt;blog series features community-driven AI innovations and content for those looking to learn more about NVIDIA NIM microservices and AI Blueprints, as well as building &lt;/i&gt;&lt;i&gt;AI agents&lt;/i&gt;&lt;i&gt;, creative workflows, productivity apps and more on AI PCs and workstations.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; — and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;. Join NVIDIA’s &lt;/i&gt;&lt;i&gt;Discord server&lt;/i&gt;&lt;i&gt; to connect with community developers and AI enthusiasts for discussions on what’s possible with RTX AI.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/rtx-ai-garage-gamescom-g-assist-rtx-remix/</guid><pubDate>Mon, 18 Aug 2025 19:30:40 +0000</pubDate></item><item><title>GEPA optimizes LLMs without costly reinforcement learning (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/gepa-optimizes-llms-without-costly-reinforcement-learning/</link><description>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Researchers from the University of California, Berkeley, Stanford University and Databricks have introduced a new AI optimization method called GEPA that significantly outperforms traditional reinforcement learning (RL) techniques for adapting large language models (LLMs) to specialized tasks.&lt;/p&gt;&lt;p&gt;GEPA removes the popular paradigm of learning through thousands of trial-and-error attempts guided by simple numerical scores. Instead, it uses an LLM’s own language understanding to reflect on its performance, diagnose errors, and iteratively evolve its instructions. In addition to being more accurate than established techniques, GEPA is significantly more efficient, achieving superior results with up to 35 times fewer trial runs.&lt;/p&gt;&lt;p&gt;For businesses building complex AI agents and workflows, this translates directly into faster development cycles, substantially lower computational costs, and more performant, reliable applications.&lt;/p&gt;&lt;p&gt;Modern enterprise AI applications are rarely a single call to an LLM. They are often “compound AI systems,” complex workflows that chain multiple LLM modules, external tools such as databases or code interpreters, and custom logic to perform sophisticated tasks, including multi-step research and data analysis.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;A popular way to optimize these systems is through reinforcement learning methods&lt;span&gt;, such as&amp;nbsp;Group Relative Policy Optimization&amp;nbsp;(GRPO), a technique employed in popular reasoning models, including&lt;/span&gt; DeepSeek-R1. This method treats the system as a black box; it runs a task, gets a simple success metric (a “scalar reward,” like a score of 7/10), and uses this feedback to slowly nudge the model’s parameters in the right direction.&lt;/p&gt;



&lt;p&gt;The major drawback of RL is its sample inefficiency. To learn effectively from these sparse numerical scores, RL methods often require tens of thousands, or even hundreds of thousands, of trial runs, known as “rollouts.” For any real-world enterprise application that involves expensive tool calls (e.g., API queries, code compilation) or uses powerful proprietary models, this process is prohibitively slow and costly.&lt;/p&gt;



&lt;p&gt;As Lakshya A Agrawal, co-author of the paper and doctoral student at UC Berkeley, told VentureBeat, this complexity is a major barrier for many companies. “For many teams, RL is not practical due to its cost and complexity—and their go-to approach so far would often just be prompt engineering by hand,” Agrawal said. He noted that GEPA is designed for teams that need to optimize systems built on top-tier models that often can’t be fine-tuned, allowing them to improve performance without managing custom GPU clusters.&lt;/p&gt;



&lt;p&gt;The researchers frame this challenge as follows: “How can we extract maximal learning signal from every expensive rollout to enable effective adaptation of complex, modular AI systems in low-data or budget-constrained settings?”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-an-optimizer-that-learns-with-language"&gt;An optimizer that learns with language&lt;/h2&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3015654" height="464" src="https://venturebeat.com/wp-content/uploads/2025/08/image_b1ddfc.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;GEPA framework Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;GEPA (Genetic-Pareto) is a prompt optimizer that tackles this challenge by replacing sparse rewards with rich, natural language feedback. It leverages the fact that the entire execution of an AI system (including its reasoning steps, tool calls, and even error messages) can be serialized into text that an LLM can read and understand. GEPA’s methodology is built on three core pillars.&lt;/p&gt;



&lt;p&gt;First is “genetic prompt evolution,” where GEPA treats a population of prompts like a gene pool. It iteratively “mutates” prompts to create new, potentially better versions. This mutation is an intelligent process driven by the second pillar: “reflection with natural language feedback.” After a few rollouts, GEPA provides an LLM with the full execution trace (what the system tried to do) and the outcome (what went right or wrong). The LLM then “reflects” on this feedback in natural language to diagnose the problem and write an improved, more detailed prompt. For instance, instead of just seeing a low score on a code generation task, it might analyze a compiler error and conclude the prompt needs to specify a particular library version.&lt;/p&gt;



&lt;p&gt;The third pillar is “Pareto-based selection,” which ensures smart exploration. Instead of focusing only on the single best-performing prompt, which can lead to getting stuck in a suboptimal solution (a “local optimum”), GEPA maintains a diverse roster of “specialist” prompts. It tracks which prompts perform best on different individual examples, creating a list of top candidates. By sampling from this diverse set of winning strategies, GEPA ensures it explores more solutions and is more likely to discover a prompt that generalizes well across a wide range of inputs.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015655" height="412" src="https://venturebeat.com/wp-content/uploads/2025/08/image_25fed4.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Selecting a single best candidate (left) can result in models getting stuck in local minima while Pareto selection (right) can explore more options and find optimal solutions Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The effectiveness of this entire process hinges on what the researchers call “feedback engineering.” Agrawal explains that the key is to surface the rich, textual details that systems already produce but often discard. “Traditional pipelines often reduce this detail to a single numerical reward, obscuring why particular outcomes occur,” he said. “GEPA’s core guidance is to structure feedback that surfaces not only outcomes but also intermediate trajectories and errors in plain text—the same evidence a human would use to diagnose system behavior.”&lt;/p&gt;



&lt;p&gt;For example, for a document retrieval system, this means listing which documents were retrieved correctly and which were missed, rather than just calculating a final score.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-gepa-in-action"&gt;GEPA in action&lt;/h2&gt;



&lt;p&gt;The researchers evaluated GEPA across four diverse tasks, including multi-hop question answering (HotpotQA) and privacy-preserving queries (PUPA). They used both open-source (Qwen3 8B) and proprietary (GPT-4.1 mini) models, comparing GEPA against the RL-based GRPO and the state-of-the-art prompt optimizer MIPROv2.&lt;/p&gt;



&lt;p&gt;Across all tasks, GEPA substantially outperformed GRPO, achieving up to a 19% higher score while using up to 35 times fewer rollouts. Agrawal provided a concrete example of this efficiency gain: “We used GEPA to optimize a QA system in ~3 hours versus GRPO’s 24 hours—an 8x reduction in development time, while also achieving 20% higher performance,” he explained. “RL-based optimization of the same scenario in our test cost about $300 in GPU time, while GEPA cost less than $20 for better results—15x savings in our experiments.”&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015656" height="289" src="https://venturebeat.com/wp-content/uploads/2025/08/image_1b53c1.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;GEPA outperforms other baselines on key benchmarks Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Beyond raw performance, the researchers found that GEPA-optimized systems are more reliable when faced with new, unseen data. This is measured by the “generalization gap” (the difference between performance on training data and final test data). Agrawal hypothesizes that this is because GEPA learns from richer feedback. “GEPA’s smaller generalization gap may stem from its use of rich natural-language feedback on each outcome—what worked, what failed, and why—rather than relying solely on a single scalar reward,” he said. “This may encourage the system to develop instructions and strategies grounded in a broader understanding of success, instead of merely learning patterns specific to the training data.” For enterprises, this improved reliability means less brittle, more adaptable AI applications in customer-facing roles.&lt;/p&gt;



&lt;p&gt;A major practical benefit is that GEPA’s instruction-based prompts are up to 9.2 times shorter than prompts produced by optimizers like MIPROv2, which include many few-shot examples. Shorter prompts decrease latency and reduce costs for API-based models. This makes the final application faster and cheaper to run in production.&lt;/p&gt;



&lt;p&gt;The paper also presents promising results for utilizing GEPA as an “inference-time” search strategy, transforming the AI from a single-answer generator into an iterative problem solver. Agrawal described a scenario where GEPA could be integrated into a company’s CI/CD pipeline. When new code is committed, GEPA could automatically generate and refine multiple optimized versions, test them for performance, and open a pull request with the best-performing variant for engineers to review. “This turns optimization into a continuous, automated process—rapidly generating solutions that often match or surpass expert hand-tuning,” Agrawal noted. In their experiments on CUDA code generation, this approach boosted performance on 20% of tasks to an expert level, compared to 0% for a single-shot attempt from GPT-4o.&lt;/p&gt;



&lt;p&gt;The paper’s authors believe GEPA is a foundational step toward a new paradigm of AI development. But beyond creating more human-like AI, its most immediate impact may be in who gets to build high-performing systems.&lt;/p&gt;



&lt;p&gt;“We expect GEPA to enable a positive shift in AI system building—making the optimization of such systems approachable by end-users, who often have the domain expertise relevant to the task, but not necessarily the time and willingness to learn complex RL specifics,” Agrawal said. “It gives power directly to the stakeholders with the exact task-specific domain knowledge.”&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Researchers from the University of California, Berkeley, Stanford University and Databricks have introduced a new AI optimization method called GEPA that significantly outperforms traditional reinforcement learning (RL) techniques for adapting large language models (LLMs) to specialized tasks.&lt;/p&gt;&lt;p&gt;GEPA removes the popular paradigm of learning through thousands of trial-and-error attempts guided by simple numerical scores. Instead, it uses an LLM’s own language understanding to reflect on its performance, diagnose errors, and iteratively evolve its instructions. In addition to being more accurate than established techniques, GEPA is significantly more efficient, achieving superior results with up to 35 times fewer trial runs.&lt;/p&gt;&lt;p&gt;For businesses building complex AI agents and workflows, this translates directly into faster development cycles, substantially lower computational costs, and more performant, reliable applications.&lt;/p&gt;&lt;p&gt;Modern enterprise AI applications are rarely a single call to an LLM. They are often “compound AI systems,” complex workflows that chain multiple LLM modules, external tools such as databases or code interpreters, and custom logic to perform sophisticated tasks, including multi-step research and data analysis.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;A popular way to optimize these systems is through reinforcement learning methods&lt;span&gt;, such as&amp;nbsp;Group Relative Policy Optimization&amp;nbsp;(GRPO), a technique employed in popular reasoning models, including&lt;/span&gt; DeepSeek-R1. This method treats the system as a black box; it runs a task, gets a simple success metric (a “scalar reward,” like a score of 7/10), and uses this feedback to slowly nudge the model’s parameters in the right direction.&lt;/p&gt;



&lt;p&gt;The major drawback of RL is its sample inefficiency. To learn effectively from these sparse numerical scores, RL methods often require tens of thousands, or even hundreds of thousands, of trial runs, known as “rollouts.” For any real-world enterprise application that involves expensive tool calls (e.g., API queries, code compilation) or uses powerful proprietary models, this process is prohibitively slow and costly.&lt;/p&gt;



&lt;p&gt;As Lakshya A Agrawal, co-author of the paper and doctoral student at UC Berkeley, told VentureBeat, this complexity is a major barrier for many companies. “For many teams, RL is not practical due to its cost and complexity—and their go-to approach so far would often just be prompt engineering by hand,” Agrawal said. He noted that GEPA is designed for teams that need to optimize systems built on top-tier models that often can’t be fine-tuned, allowing them to improve performance without managing custom GPU clusters.&lt;/p&gt;



&lt;p&gt;The researchers frame this challenge as follows: “How can we extract maximal learning signal from every expensive rollout to enable effective adaptation of complex, modular AI systems in low-data or budget-constrained settings?”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-an-optimizer-that-learns-with-language"&gt;An optimizer that learns with language&lt;/h2&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3015654" height="464" src="https://venturebeat.com/wp-content/uploads/2025/08/image_b1ddfc.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;GEPA framework Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;GEPA (Genetic-Pareto) is a prompt optimizer that tackles this challenge by replacing sparse rewards with rich, natural language feedback. It leverages the fact that the entire execution of an AI system (including its reasoning steps, tool calls, and even error messages) can be serialized into text that an LLM can read and understand. GEPA’s methodology is built on three core pillars.&lt;/p&gt;



&lt;p&gt;First is “genetic prompt evolution,” where GEPA treats a population of prompts like a gene pool. It iteratively “mutates” prompts to create new, potentially better versions. This mutation is an intelligent process driven by the second pillar: “reflection with natural language feedback.” After a few rollouts, GEPA provides an LLM with the full execution trace (what the system tried to do) and the outcome (what went right or wrong). The LLM then “reflects” on this feedback in natural language to diagnose the problem and write an improved, more detailed prompt. For instance, instead of just seeing a low score on a code generation task, it might analyze a compiler error and conclude the prompt needs to specify a particular library version.&lt;/p&gt;



&lt;p&gt;The third pillar is “Pareto-based selection,” which ensures smart exploration. Instead of focusing only on the single best-performing prompt, which can lead to getting stuck in a suboptimal solution (a “local optimum”), GEPA maintains a diverse roster of “specialist” prompts. It tracks which prompts perform best on different individual examples, creating a list of top candidates. By sampling from this diverse set of winning strategies, GEPA ensures it explores more solutions and is more likely to discover a prompt that generalizes well across a wide range of inputs.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015655" height="412" src="https://venturebeat.com/wp-content/uploads/2025/08/image_25fed4.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Selecting a single best candidate (left) can result in models getting stuck in local minima while Pareto selection (right) can explore more options and find optimal solutions Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The effectiveness of this entire process hinges on what the researchers call “feedback engineering.” Agrawal explains that the key is to surface the rich, textual details that systems already produce but often discard. “Traditional pipelines often reduce this detail to a single numerical reward, obscuring why particular outcomes occur,” he said. “GEPA’s core guidance is to structure feedback that surfaces not only outcomes but also intermediate trajectories and errors in plain text—the same evidence a human would use to diagnose system behavior.”&lt;/p&gt;



&lt;p&gt;For example, for a document retrieval system, this means listing which documents were retrieved correctly and which were missed, rather than just calculating a final score.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-gepa-in-action"&gt;GEPA in action&lt;/h2&gt;



&lt;p&gt;The researchers evaluated GEPA across four diverse tasks, including multi-hop question answering (HotpotQA) and privacy-preserving queries (PUPA). They used both open-source (Qwen3 8B) and proprietary (GPT-4.1 mini) models, comparing GEPA against the RL-based GRPO and the state-of-the-art prompt optimizer MIPROv2.&lt;/p&gt;



&lt;p&gt;Across all tasks, GEPA substantially outperformed GRPO, achieving up to a 19% higher score while using up to 35 times fewer rollouts. Agrawal provided a concrete example of this efficiency gain: “We used GEPA to optimize a QA system in ~3 hours versus GRPO’s 24 hours—an 8x reduction in development time, while also achieving 20% higher performance,” he explained. “RL-based optimization of the same scenario in our test cost about $300 in GPU time, while GEPA cost less than $20 for better results—15x savings in our experiments.”&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015656" height="289" src="https://venturebeat.com/wp-content/uploads/2025/08/image_1b53c1.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;GEPA outperforms other baselines on key benchmarks Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Beyond raw performance, the researchers found that GEPA-optimized systems are more reliable when faced with new, unseen data. This is measured by the “generalization gap” (the difference between performance on training data and final test data). Agrawal hypothesizes that this is because GEPA learns from richer feedback. “GEPA’s smaller generalization gap may stem from its use of rich natural-language feedback on each outcome—what worked, what failed, and why—rather than relying solely on a single scalar reward,” he said. “This may encourage the system to develop instructions and strategies grounded in a broader understanding of success, instead of merely learning patterns specific to the training data.” For enterprises, this improved reliability means less brittle, more adaptable AI applications in customer-facing roles.&lt;/p&gt;



&lt;p&gt;A major practical benefit is that GEPA’s instruction-based prompts are up to 9.2 times shorter than prompts produced by optimizers like MIPROv2, which include many few-shot examples. Shorter prompts decrease latency and reduce costs for API-based models. This makes the final application faster and cheaper to run in production.&lt;/p&gt;



&lt;p&gt;The paper also presents promising results for utilizing GEPA as an “inference-time” search strategy, transforming the AI from a single-answer generator into an iterative problem solver. Agrawal described a scenario where GEPA could be integrated into a company’s CI/CD pipeline. When new code is committed, GEPA could automatically generate and refine multiple optimized versions, test them for performance, and open a pull request with the best-performing variant for engineers to review. “This turns optimization into a continuous, automated process—rapidly generating solutions that often match or surpass expert hand-tuning,” Agrawal noted. In their experiments on CUDA code generation, this approach boosted performance on 20% of tasks to an expert level, compared to 0% for a single-shot attempt from GPT-4o.&lt;/p&gt;



&lt;p&gt;The paper’s authors believe GEPA is a foundational step toward a new paradigm of AI development. But beyond creating more human-like AI, its most immediate impact may be in who gets to build high-performing systems.&lt;/p&gt;



&lt;p&gt;“We expect GEPA to enable a positive shift in AI system building—making the optimization of such systems approachable by end-users, who often have the domain expertise relevant to the task, but not necessarily the time and willingness to learn complex RL specifics,” Agrawal said. “It gives power directly to the stakeholders with the exact task-specific domain knowledge.”&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/gepa-optimizes-llms-without-costly-reinforcement-learning/</guid><pubDate>Mon, 18 Aug 2025 20:41:22 +0000</pubDate></item><item><title>Hugging Face: 5 ways enterprises can slash AI costs without sacrificing performance (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/hugging-face-5-ways-enterprises-can-slash-ai-costs-without-sacrificing-performance/</link><description>&lt;p&gt;Enterprises seem to accept it as a basic fact: AI models require a significant amount of compute; they simply have to find ways to obtain more of it.&amp;nbsp;&lt;/p&gt;&lt;p&gt;But it doesn’t have to be that way, according to Sasha Luccioni, AI and climate lead at Hugging Face. What if there’s a smarter way to use AI? What if, instead of striving for more (often unnecessary) compute and ways to power it, they can focus on improving model performance and accuracy?&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ultimately, model makers and enterprises are focusing on the wrong issue: They should be computing &lt;em&gt;smarter&lt;/em&gt;, not harder or doing more, Luccioni says.&amp;nbsp;&lt;/p&gt;&lt;p&gt;“There are smarter ways of doing things that we’re currently under-exploring, because we’re so blinded by: We need more FLOPS, we need more GPUs, we need more time,” she said.&amp;nbsp;&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Here are five key learnings from Hugging Face that can help enterprises of all sizes use AI more efficiently.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-1-right-size-the-model-to-the-task-nbsp"&gt;1: Right-size the model to the task&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Avoid defaulting to giant, general-purpose models for every use case. Task-specific or distilled models can match, or even &lt;span&gt;surpass,&amp;nbsp;larger models&amp;nbsp;in terms of accuracy for targeted workloads — at a lower cost and with reduced energy consumption&lt;/span&gt;.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Luccioni, in fact, has found in testing that a task-specific model uses 20 to 30 times less energy than a general-purpose one. “Because it’s a model that can do that one task, as opposed to any task that you throw at it, which is often the case with large language models,” she said.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Distillation is key here; a full model could initially be trained from scratch and then refined for a specific task. DeepSeek R1, for instance, is “so huge that most organizations can’t afford to use it” because you need at least 8 GPUs, Luccioni noted. By contrast, distilled versions can be 10, 20 or even 30X smaller and run on a single GPU.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In general, open-source models help with efficiency, she noted, as they don’t need to be trained from scratch. That’s compared to just a few years ago, when enterprises were wasting resources because they couldn’t find the model they needed; nowadays, they can start out with a base model and fine-tune and adapt it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“It provides incremental shared innovation, as opposed to siloed, everyone’s training their models on their datasets and essentially wasting compute in the process,” said Luccioni.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It’s becoming clear that companies are quickly getting disillusioned with gen AI, as costs are not yet proportionate to the benefits. Generic use cases, such as writing emails or transcribing meeting notes, are genuinely helpful. However, task-specific models still require “a lot of work” because out-of-the-box models don’t cut it and are also more costly, said Luccioni.&lt;/p&gt;



&lt;p&gt;This is the next frontier of added value. “A lot of companies do want a specific task done,” Luccioni noted. “They don’t want AGI, they want specific intelligence. And that’s the gap that needs to be bridged.”&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-2-make-efficiency-the-default"&gt;2. Make efficiency the default&lt;/h2&gt;



&lt;p&gt;Adopt “nudge theory” in system design, set conservative reasoning budgets, limit always-on generative features and require opt-in for high-cost compute modes.&lt;/p&gt;



&lt;p&gt;In cognitive science, “nudge theory” is a behavioral change management approach designed to influence human behavior subtly. The “canonical example,” Luccioni noted, is adding cutlery to takeout: Having people decide whether they want plastic utensils, rather than automatically including them with every order, can significantly reduce waste.&lt;/p&gt;



&lt;p&gt;“Just getting people to opt into something versus opting out of something is actually a very powerful mechanism for changing people’s behavior,” said Luccioni.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Default mechanisms are also unnecessary, as they increase use and, therefore, costs because models are doing more work than they need to. For instance, with popular search engines such as Google, a gen AI summary automatically populates at the top by default. Luccioni also noted that, when she recently used&amp;nbsp;OpenAI’s GPT-5, the model automatically worked in full reasoning mode on “very simple questions.”&lt;/p&gt;



&lt;p&gt;“For me, it should be the exception,” she said. “Like, ‘what’s the meaning of life, then sure, I want a gen AI summary.’ But with ‘What’s the weather like in Montreal,’ or ‘What are the opening hours of my local pharmacy?’ I do not need a generative AI summary, yet it’s the default. I think that the default mode should be no reasoning.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-3-optimize-hardware-utilization"&gt;3. Optimize hardware utilization&lt;/h2&gt;



&lt;p&gt;Use batching; adjust precision and fine-tune batch sizes for specific hardware generation to minimize wasted memory and power draw.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For instance, enterprises should ask themselves: Does the model need to be on all the time? Will people be pinging it in real time, 100 requests at once? In that case, always-on optimization is necessary, Luccioni noted. However, in many others, it’s not; the model can be run periodically to optimize memory usage, and batching can ensure optimal memory utilization.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“It’s kind of like an engineering challenge, but a very specific one, so it’s hard to say, ‘Just distill all the models,’ or ‘change the precision on all the models,’” said Luccioni.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In one of her recent studies, she found that batch size depends on hardware, even down to the specific type or version. Going from one batch size to plus-one can increase energy use because models need more memory bars.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“This is something that people don’t really look at, they’re just like, ‘Oh, I’m gonna maximize the batch size,’ but it really comes down to tweaking all these different things, and all of a sudden it’s super efficient, but it only works in your specific context,” Luccioni explained.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-4-incentivize-energy-transparency"&gt;4. Incentivize energy transparency&lt;/h2&gt;



&lt;p&gt;It always helps when people are incentivized; to this end, Hugging Face earlier this year launched AI Energy Score. It’s a novel way to promote more energy efficiency, utilizing a 1- to 5-star rating system, with the most efficient models earning a “five-star” status.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It could be considered the “Energy Star for AI,” and was inspired by the potentially-soon-to-be-defunct federal program, which set energy efficiency specifications and branded qualifying appliances with an Energy Star logo.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“For a couple of decades, it was really a positive motivation, people wanted that star rating, right?,” said Luccioni. “Something similar with Energy Score would be great.”&lt;/p&gt;



&lt;p&gt;Hugging Face has a leaderboard up now, which it plans to update with new models (DeepSeek, GPT-oss) in September, and continually do so every 6 months or sooner as new models become available. The goal is that model builders will consider the rating as a “badge of honor,” Luccioni said. &lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-5-rethink-the-more-compute-is-better-mindset"&gt;5. Rethink the “more compute is better” mindset&lt;/h2&gt;



&lt;p&gt;Instead of chasing the largest GPU clusters, begin with the question: “What is the smartest way to achieve the result?” For many workloads, smarter architectures and better-curated data outperform brute-force scaling.&lt;/p&gt;



&lt;p&gt;“I think that people probably don’t need as many GPUs as they think they do,” said Luccioni. Instead of simply going for the biggest clusters, she urged enterprises to rethink the tasks GPUs will be completing and why they need them, how they performed those types of tasks before, and what adding extra GPUs will ultimately get them.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“It’s kind of this race to the bottom where we need a bigger cluster,” she said. “It’s thinking about what you’re using AI for, what technique do you need, what does that require?”&amp;nbsp;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;p&gt;Enterprises seem to accept it as a basic fact: AI models require a significant amount of compute; they simply have to find ways to obtain more of it.&amp;nbsp;&lt;/p&gt;&lt;p&gt;But it doesn’t have to be that way, according to Sasha Luccioni, AI and climate lead at Hugging Face. What if there’s a smarter way to use AI? What if, instead of striving for more (often unnecessary) compute and ways to power it, they can focus on improving model performance and accuracy?&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ultimately, model makers and enterprises are focusing on the wrong issue: They should be computing &lt;em&gt;smarter&lt;/em&gt;, not harder or doing more, Luccioni says.&amp;nbsp;&lt;/p&gt;&lt;p&gt;“There are smarter ways of doing things that we’re currently under-exploring, because we’re so blinded by: We need more FLOPS, we need more GPUs, we need more time,” she said.&amp;nbsp;&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Here are five key learnings from Hugging Face that can help enterprises of all sizes use AI more efficiently.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-1-right-size-the-model-to-the-task-nbsp"&gt;1: Right-size the model to the task&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Avoid defaulting to giant, general-purpose models for every use case. Task-specific or distilled models can match, or even &lt;span&gt;surpass,&amp;nbsp;larger models&amp;nbsp;in terms of accuracy for targeted workloads — at a lower cost and with reduced energy consumption&lt;/span&gt;.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Luccioni, in fact, has found in testing that a task-specific model uses 20 to 30 times less energy than a general-purpose one. “Because it’s a model that can do that one task, as opposed to any task that you throw at it, which is often the case with large language models,” she said.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Distillation is key here; a full model could initially be trained from scratch and then refined for a specific task. DeepSeek R1, for instance, is “so huge that most organizations can’t afford to use it” because you need at least 8 GPUs, Luccioni noted. By contrast, distilled versions can be 10, 20 or even 30X smaller and run on a single GPU.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In general, open-source models help with efficiency, she noted, as they don’t need to be trained from scratch. That’s compared to just a few years ago, when enterprises were wasting resources because they couldn’t find the model they needed; nowadays, they can start out with a base model and fine-tune and adapt it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“It provides incremental shared innovation, as opposed to siloed, everyone’s training their models on their datasets and essentially wasting compute in the process,” said Luccioni.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It’s becoming clear that companies are quickly getting disillusioned with gen AI, as costs are not yet proportionate to the benefits. Generic use cases, such as writing emails or transcribing meeting notes, are genuinely helpful. However, task-specific models still require “a lot of work” because out-of-the-box models don’t cut it and are also more costly, said Luccioni.&lt;/p&gt;



&lt;p&gt;This is the next frontier of added value. “A lot of companies do want a specific task done,” Luccioni noted. “They don’t want AGI, they want specific intelligence. And that’s the gap that needs to be bridged.”&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-2-make-efficiency-the-default"&gt;2. Make efficiency the default&lt;/h2&gt;



&lt;p&gt;Adopt “nudge theory” in system design, set conservative reasoning budgets, limit always-on generative features and require opt-in for high-cost compute modes.&lt;/p&gt;



&lt;p&gt;In cognitive science, “nudge theory” is a behavioral change management approach designed to influence human behavior subtly. The “canonical example,” Luccioni noted, is adding cutlery to takeout: Having people decide whether they want plastic utensils, rather than automatically including them with every order, can significantly reduce waste.&lt;/p&gt;



&lt;p&gt;“Just getting people to opt into something versus opting out of something is actually a very powerful mechanism for changing people’s behavior,” said Luccioni.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Default mechanisms are also unnecessary, as they increase use and, therefore, costs because models are doing more work than they need to. For instance, with popular search engines such as Google, a gen AI summary automatically populates at the top by default. Luccioni also noted that, when she recently used&amp;nbsp;OpenAI’s GPT-5, the model automatically worked in full reasoning mode on “very simple questions.”&lt;/p&gt;



&lt;p&gt;“For me, it should be the exception,” she said. “Like, ‘what’s the meaning of life, then sure, I want a gen AI summary.’ But with ‘What’s the weather like in Montreal,’ or ‘What are the opening hours of my local pharmacy?’ I do not need a generative AI summary, yet it’s the default. I think that the default mode should be no reasoning.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-3-optimize-hardware-utilization"&gt;3. Optimize hardware utilization&lt;/h2&gt;



&lt;p&gt;Use batching; adjust precision and fine-tune batch sizes for specific hardware generation to minimize wasted memory and power draw.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For instance, enterprises should ask themselves: Does the model need to be on all the time? Will people be pinging it in real time, 100 requests at once? In that case, always-on optimization is necessary, Luccioni noted. However, in many others, it’s not; the model can be run periodically to optimize memory usage, and batching can ensure optimal memory utilization.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“It’s kind of like an engineering challenge, but a very specific one, so it’s hard to say, ‘Just distill all the models,’ or ‘change the precision on all the models,’” said Luccioni.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In one of her recent studies, she found that batch size depends on hardware, even down to the specific type or version. Going from one batch size to plus-one can increase energy use because models need more memory bars.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“This is something that people don’t really look at, they’re just like, ‘Oh, I’m gonna maximize the batch size,’ but it really comes down to tweaking all these different things, and all of a sudden it’s super efficient, but it only works in your specific context,” Luccioni explained.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-4-incentivize-energy-transparency"&gt;4. Incentivize energy transparency&lt;/h2&gt;



&lt;p&gt;It always helps when people are incentivized; to this end, Hugging Face earlier this year launched AI Energy Score. It’s a novel way to promote more energy efficiency, utilizing a 1- to 5-star rating system, with the most efficient models earning a “five-star” status.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It could be considered the “Energy Star for AI,” and was inspired by the potentially-soon-to-be-defunct federal program, which set energy efficiency specifications and branded qualifying appliances with an Energy Star logo.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“For a couple of decades, it was really a positive motivation, people wanted that star rating, right?,” said Luccioni. “Something similar with Energy Score would be great.”&lt;/p&gt;



&lt;p&gt;Hugging Face has a leaderboard up now, which it plans to update with new models (DeepSeek, GPT-oss) in September, and continually do so every 6 months or sooner as new models become available. The goal is that model builders will consider the rating as a “badge of honor,” Luccioni said. &lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-5-rethink-the-more-compute-is-better-mindset"&gt;5. Rethink the “more compute is better” mindset&lt;/h2&gt;



&lt;p&gt;Instead of chasing the largest GPU clusters, begin with the question: “What is the smartest way to achieve the result?” For many workloads, smarter architectures and better-curated data outperform brute-force scaling.&lt;/p&gt;



&lt;p&gt;“I think that people probably don’t need as many GPUs as they think they do,” said Luccioni. Instead of simply going for the biggest clusters, she urged enterprises to rethink the tasks GPUs will be completing and why they need them, how they performed those types of tasks before, and what adding extra GPUs will ultimately get them.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“It’s kind of this race to the bottom where we need a bigger cluster,” she said. “It’s thinking about what you’re using AI for, what technique do you need, what does that require?”&amp;nbsp;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/hugging-face-5-ways-enterprises-can-slash-ai-costs-without-sacrificing-performance/</guid><pubDate>Mon, 18 Aug 2025 21:10:04 +0000</pubDate></item><item><title>Nvidia releases a new small, open model Nemotron-Nano-9B-v2 with toggle on/off reasoning (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/nvidia-releases-a-new-small-open-model-nemotron-nano-9b-v2-with-toggle-on-off-reasoning/</link><description>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Small models are having a moment. On the heels of the release of a new AI vision model small enough to fit on a smartwatch from MIT spinoff Liquid AI, and a model small enough to run on a smartphone from Google, &lt;strong&gt;Nvidia is joining the party today&lt;/strong&gt; with a new small language model (SLM) of its own, &lt;strong&gt;Nemotron-Nano-9B-V2&lt;/strong&gt;, which attained the highest performance in its class on selected benchmarks and comes with the ability for users to toggle on and off AI “reasoning,” that is, self-checking before outputting an answer.&lt;/p&gt;&lt;p&gt;While the 9 billion parameters are larger than some of the multimillion parameter small models VentureBeat has covered recently&lt;strong&gt;, Nvidia notes it is a meaningful reduction from its original size of 12 billion parameters&lt;/strong&gt; and is designed to fit on a &lt;strong&gt;single Nvidia A10 GPU&lt;/strong&gt;. &lt;/p&gt;&lt;p&gt;As Oleksii Kuchiaev, Nvidia Director of AI Model Post-Training, said on X in response to a question I submitted to him: “The 12B was pruned to 9B to specifically fit A10 which is a popular GPU choice for deployment.&lt;strong&gt; It is also a hybrid model which allows it to process a larger batch size and be up to 6x faster than similar sized transformer models.”&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;For context, many leading LLMs are in the 70+ billion parameter range (recall parameters refer to the internal settings governing the model’s behavior, with more generally denoting a larger and more capable, yet more compute intensive model).&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;The model handles multiple languages, including English, German, Spanish, French, Italian, Japanese, and in extended descriptions, Korean, Portuguese, Russian, and Chinese. It’s suitable for both &lt;strong&gt;instruction following and code generation.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Nemotron-Nano-9B-V2 and its pre-training datasets available right now on Hugging Face and through the company’s model catalog. &lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-a-fusion-of-transformer-and-mamba-architectures"&gt;A fusion of Transformer and Mamba architectures&lt;/h2&gt;



&lt;p&gt;It’s based on Nemotron-H, a set of hybrid Mamba-Transformer models that form the foundation for the company’s latest offerings.&lt;/p&gt;



&lt;p&gt;While most popular LLMs are pure “Transformer” models, which rely entirely on attention layers, they can become costly in memory and compute as sequence lengths grow.&lt;/p&gt;



&lt;p&gt;Instead, Nemotron-H models and others using the Mamba architecture developed by researchers at Carnegie Mellon University and Princeton, also &lt;strong&gt;weave in selective state space models (or SSMs), which can handle very long sequences of information in and out by maintaining state.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;These layers scale linearly with sequence length and can process contexts much longer than standard self-attention without the same memory and compute overhead.&lt;/p&gt;



&lt;p&gt;A h&lt;strong&gt;ybrid Mamba-Transformer reduces those costs by substituting most of the attention with linear-time state space layers, achieving up to 2–3× higher throughput on long contexts&lt;/strong&gt; with comparable accuracy.&lt;/p&gt;



&lt;p&gt;Other AI labs beyond Nvidia such as Ai2 have also released models based on the Mamba architecture.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-toggle-on-of-reasoning-using-language"&gt;Toggle on/of reasoning using language&lt;/h2&gt;



&lt;p&gt;Nemotron-Nano-9B-v2 is positioned as a unified, text-only chat and reasoning model trained from scratch. &lt;/p&gt;



&lt;p&gt;The &lt;strong&gt;system defaults to generating a reasoning trace before providing a final answer, though users can toggle this behavior &lt;/strong&gt;through simple control tokens such as /think or /no_think. &lt;/p&gt;



&lt;p&gt;The model also i&lt;strong&gt;ntroduces runtime “thinking budget” management&lt;/strong&gt;, which &lt;strong&gt;allows developers to cap the number of tokens &lt;/strong&gt;devoted to internal reasoning before the model completes a response. &lt;/p&gt;



&lt;p&gt;This mechanism is aimed at balancing accuracy with latency, &lt;strong&gt;particularly in applications like customer support or autonomous agents.&lt;/strong&gt;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-benchmarks-tell-a-promising-story"&gt;Benchmarks tell a promising story&lt;/h2&gt;



&lt;p&gt;Evaluation results highlight competitive accuracy against other open small-scale models. Tested in “reasoning on” mode using the NeMo-Skills suite,&lt;strong&gt; Nemotron-Nano-9B-v2 reaches 72.1 percent on AIME25&lt;/strong&gt;, &lt;strong&gt;97.8 percent on MATH500, 64.0 percent on GPQA&lt;/strong&gt;, and&lt;strong&gt; 71.1 percent on LiveCodeBench&lt;/strong&gt;. &lt;/p&gt;



&lt;p&gt;Scores on instruction following and long-context benchmarks are also reported: &lt;strong&gt;90.3 percent on IFEval, 78.9 percent on the RULER 128K test&lt;/strong&gt;, and smaller but measurable gains on BFCL v3 and the HLE benchmark. &lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015748" height="335" src="https://venturebeat.com/wp-content/uploads/2025/08/accuracy_chart.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;&lt;strong&gt;Across the board, Nano-9B-v2 shows higher accuracy than Qwen3-8B,&lt;/strong&gt; a common point of comparison.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015751" height="530" src="https://venturebeat.com/wp-content/uploads/2025/08/acc-vs-budget.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;Nvidia illustrates these results with accuracy-versus-budget curves that show how performance scales as the token allowance for reasoning increases. The company suggests that careful budget control can help developers optimize both quality and latency in production use cases.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-trained-on-synthetic-datasets"&gt;Trained on synthetic datasets&lt;/h2&gt;



&lt;p&gt;Both the Nano model and the Nemotron-H family rely on a mixture of curated, web-sourced, and synthetic training data. &lt;/p&gt;



&lt;p&gt;The corpora include general text, code, mathematics, science, legal, and financial documents, as well as alignment-style question-answering datasets. &lt;/p&gt;



&lt;p&gt;Nvidia confirms the use of synthetic reasoning traces generated by other large models to strengthen performance on complex benchmarks.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-licensing-and-commercial-use"&gt;Licensing and commercial use&lt;/h2&gt;



&lt;p&gt;The Nano-9B-v2 model is released under the Nvidia Open Model License Agreement, last updated in June 2025. &lt;/p&gt;



&lt;p&gt;The license is designed to be permissive and enterprise-friendly. &lt;strong&gt;Nvidia explicitly states that the models are &lt;em&gt;commercially usable&lt;/em&gt; out of the box&lt;/strong&gt;, and that&lt;strong&gt; developers are free to create and distribute derivative models. &lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Importantly, Nvidia does not claim ownership of any outputs generated by the model, leaving responsibility and rights with the developer or organization using it.&lt;/p&gt;



&lt;p&gt;For an enterprise developer, this means the model can be put into production immediately without negotiating a separate commercial license or paying fees tied to usage thresholds, revenue levels, or user counts. There are no clauses requiring a paid license once a company reaches a certain scale, unlike some tiered open licenses used by other providers.&lt;/p&gt;



&lt;p&gt;That said, the agreement does include several conditions enterprises must observe:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Guardrails&lt;/strong&gt;: Users cannot bypass or disable built-in safety mechanisms (referred to as “guardrails”) without implementing comparable replacements suited to their deployment.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Redistribution&lt;/strong&gt;: Any redistribution of the model or derivatives must include the Nvidia Open Model License text and attribution (“Licensed by Nvidia Corporation under the Nvidia Open Model License”).&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Compliance&lt;/strong&gt;: Users must comply with trade regulations and restrictions (e.g., U.S. export laws).&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Trustworthy AI terms&lt;/strong&gt;: Usage must align with Nvidia Trustworthy AI guidelines, which cover responsible deployment and ethical considerations.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Litigation clause&lt;/strong&gt;: If a user initiates copyright or patent litigation against another entity alleging infringement by the model, the license automatically terminates.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;These conditions focus on legal and responsible use rather than commercial scale. Enterprises do not need to seek additional permission or pay royalties to Nvidia simply for building products, monetizing them, or scaling their user base. Instead, they must make sure deployment practices respect safety, attribution, and compliance obligations.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-positioning-in-the-market"&gt;Positioning in the market&lt;/h2&gt;



&lt;p&gt;With Nemotron-Nano-9B-v2, Nvidia is targeting developers who need a balance of reasoning capability and deployment efficiency at smaller scales. &lt;/p&gt;



&lt;p&gt;The runtime budget control and reasoning-toggle features are meant to give system builders more flexibility in managing accuracy versus response speed. &lt;/p&gt;



&lt;p&gt;Their release on Hugging Face and Nvidia’s model catalog indicates that they are &lt;strong&gt;meant to be broadly accessible for experimentation and integration.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Nvidia’s release of Nemotron-Nano-9B-v2 showcase a continued focus on efficiency and controllable reasoning in language models. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;By combining hybrid architectures with new compression and training techniques&lt;/strong&gt;, the company is offering developers tools that seek to maintain accuracy while reducing costs and latency. &lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Small models are having a moment. On the heels of the release of a new AI vision model small enough to fit on a smartwatch from MIT spinoff Liquid AI, and a model small enough to run on a smartphone from Google, &lt;strong&gt;Nvidia is joining the party today&lt;/strong&gt; with a new small language model (SLM) of its own, &lt;strong&gt;Nemotron-Nano-9B-V2&lt;/strong&gt;, which attained the highest performance in its class on selected benchmarks and comes with the ability for users to toggle on and off AI “reasoning,” that is, self-checking before outputting an answer.&lt;/p&gt;&lt;p&gt;While the 9 billion parameters are larger than some of the multimillion parameter small models VentureBeat has covered recently&lt;strong&gt;, Nvidia notes it is a meaningful reduction from its original size of 12 billion parameters&lt;/strong&gt; and is designed to fit on a &lt;strong&gt;single Nvidia A10 GPU&lt;/strong&gt;. &lt;/p&gt;&lt;p&gt;As Oleksii Kuchiaev, Nvidia Director of AI Model Post-Training, said on X in response to a question I submitted to him: “The 12B was pruned to 9B to specifically fit A10 which is a popular GPU choice for deployment.&lt;strong&gt; It is also a hybrid model which allows it to process a larger batch size and be up to 6x faster than similar sized transformer models.”&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;For context, many leading LLMs are in the 70+ billion parameter range (recall parameters refer to the internal settings governing the model’s behavior, with more generally denoting a larger and more capable, yet more compute intensive model).&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;The model handles multiple languages, including English, German, Spanish, French, Italian, Japanese, and in extended descriptions, Korean, Portuguese, Russian, and Chinese. It’s suitable for both &lt;strong&gt;instruction following and code generation.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Nemotron-Nano-9B-V2 and its pre-training datasets available right now on Hugging Face and through the company’s model catalog. &lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-a-fusion-of-transformer-and-mamba-architectures"&gt;A fusion of Transformer and Mamba architectures&lt;/h2&gt;



&lt;p&gt;It’s based on Nemotron-H, a set of hybrid Mamba-Transformer models that form the foundation for the company’s latest offerings.&lt;/p&gt;



&lt;p&gt;While most popular LLMs are pure “Transformer” models, which rely entirely on attention layers, they can become costly in memory and compute as sequence lengths grow.&lt;/p&gt;



&lt;p&gt;Instead, Nemotron-H models and others using the Mamba architecture developed by researchers at Carnegie Mellon University and Princeton, also &lt;strong&gt;weave in selective state space models (or SSMs), which can handle very long sequences of information in and out by maintaining state.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;These layers scale linearly with sequence length and can process contexts much longer than standard self-attention without the same memory and compute overhead.&lt;/p&gt;



&lt;p&gt;A h&lt;strong&gt;ybrid Mamba-Transformer reduces those costs by substituting most of the attention with linear-time state space layers, achieving up to 2–3× higher throughput on long contexts&lt;/strong&gt; with comparable accuracy.&lt;/p&gt;



&lt;p&gt;Other AI labs beyond Nvidia such as Ai2 have also released models based on the Mamba architecture.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-toggle-on-of-reasoning-using-language"&gt;Toggle on/of reasoning using language&lt;/h2&gt;



&lt;p&gt;Nemotron-Nano-9B-v2 is positioned as a unified, text-only chat and reasoning model trained from scratch. &lt;/p&gt;



&lt;p&gt;The &lt;strong&gt;system defaults to generating a reasoning trace before providing a final answer, though users can toggle this behavior &lt;/strong&gt;through simple control tokens such as /think or /no_think. &lt;/p&gt;



&lt;p&gt;The model also i&lt;strong&gt;ntroduces runtime “thinking budget” management&lt;/strong&gt;, which &lt;strong&gt;allows developers to cap the number of tokens &lt;/strong&gt;devoted to internal reasoning before the model completes a response. &lt;/p&gt;



&lt;p&gt;This mechanism is aimed at balancing accuracy with latency, &lt;strong&gt;particularly in applications like customer support or autonomous agents.&lt;/strong&gt;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-benchmarks-tell-a-promising-story"&gt;Benchmarks tell a promising story&lt;/h2&gt;



&lt;p&gt;Evaluation results highlight competitive accuracy against other open small-scale models. Tested in “reasoning on” mode using the NeMo-Skills suite,&lt;strong&gt; Nemotron-Nano-9B-v2 reaches 72.1 percent on AIME25&lt;/strong&gt;, &lt;strong&gt;97.8 percent on MATH500, 64.0 percent on GPQA&lt;/strong&gt;, and&lt;strong&gt; 71.1 percent on LiveCodeBench&lt;/strong&gt;. &lt;/p&gt;



&lt;p&gt;Scores on instruction following and long-context benchmarks are also reported: &lt;strong&gt;90.3 percent on IFEval, 78.9 percent on the RULER 128K test&lt;/strong&gt;, and smaller but measurable gains on BFCL v3 and the HLE benchmark. &lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015748" height="335" src="https://venturebeat.com/wp-content/uploads/2025/08/accuracy_chart.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;&lt;strong&gt;Across the board, Nano-9B-v2 shows higher accuracy than Qwen3-8B,&lt;/strong&gt; a common point of comparison.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015751" height="530" src="https://venturebeat.com/wp-content/uploads/2025/08/acc-vs-budget.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;Nvidia illustrates these results with accuracy-versus-budget curves that show how performance scales as the token allowance for reasoning increases. The company suggests that careful budget control can help developers optimize both quality and latency in production use cases.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-trained-on-synthetic-datasets"&gt;Trained on synthetic datasets&lt;/h2&gt;



&lt;p&gt;Both the Nano model and the Nemotron-H family rely on a mixture of curated, web-sourced, and synthetic training data. &lt;/p&gt;



&lt;p&gt;The corpora include general text, code, mathematics, science, legal, and financial documents, as well as alignment-style question-answering datasets. &lt;/p&gt;



&lt;p&gt;Nvidia confirms the use of synthetic reasoning traces generated by other large models to strengthen performance on complex benchmarks.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-licensing-and-commercial-use"&gt;Licensing and commercial use&lt;/h2&gt;



&lt;p&gt;The Nano-9B-v2 model is released under the Nvidia Open Model License Agreement, last updated in June 2025. &lt;/p&gt;



&lt;p&gt;The license is designed to be permissive and enterprise-friendly. &lt;strong&gt;Nvidia explicitly states that the models are &lt;em&gt;commercially usable&lt;/em&gt; out of the box&lt;/strong&gt;, and that&lt;strong&gt; developers are free to create and distribute derivative models. &lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Importantly, Nvidia does not claim ownership of any outputs generated by the model, leaving responsibility and rights with the developer or organization using it.&lt;/p&gt;



&lt;p&gt;For an enterprise developer, this means the model can be put into production immediately without negotiating a separate commercial license or paying fees tied to usage thresholds, revenue levels, or user counts. There are no clauses requiring a paid license once a company reaches a certain scale, unlike some tiered open licenses used by other providers.&lt;/p&gt;



&lt;p&gt;That said, the agreement does include several conditions enterprises must observe:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Guardrails&lt;/strong&gt;: Users cannot bypass or disable built-in safety mechanisms (referred to as “guardrails”) without implementing comparable replacements suited to their deployment.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Redistribution&lt;/strong&gt;: Any redistribution of the model or derivatives must include the Nvidia Open Model License text and attribution (“Licensed by Nvidia Corporation under the Nvidia Open Model License”).&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Compliance&lt;/strong&gt;: Users must comply with trade regulations and restrictions (e.g., U.S. export laws).&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Trustworthy AI terms&lt;/strong&gt;: Usage must align with Nvidia Trustworthy AI guidelines, which cover responsible deployment and ethical considerations.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Litigation clause&lt;/strong&gt;: If a user initiates copyright or patent litigation against another entity alleging infringement by the model, the license automatically terminates.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;These conditions focus on legal and responsible use rather than commercial scale. Enterprises do not need to seek additional permission or pay royalties to Nvidia simply for building products, monetizing them, or scaling their user base. Instead, they must make sure deployment practices respect safety, attribution, and compliance obligations.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-positioning-in-the-market"&gt;Positioning in the market&lt;/h2&gt;



&lt;p&gt;With Nemotron-Nano-9B-v2, Nvidia is targeting developers who need a balance of reasoning capability and deployment efficiency at smaller scales. &lt;/p&gt;



&lt;p&gt;The runtime budget control and reasoning-toggle features are meant to give system builders more flexibility in managing accuracy versus response speed. &lt;/p&gt;



&lt;p&gt;Their release on Hugging Face and Nvidia’s model catalog indicates that they are &lt;strong&gt;meant to be broadly accessible for experimentation and integration.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Nvidia’s release of Nemotron-Nano-9B-v2 showcase a continued focus on efficiency and controllable reasoning in language models. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;By combining hybrid architectures with new compression and training techniques&lt;/strong&gt;, the company is offering developers tools that seek to maintain accuracy while reducing costs and latency. &lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/nvidia-releases-a-new-small-open-model-nemotron-nano-9b-v2-with-toggle-on-off-reasoning/</guid><pubDate>Mon, 18 Aug 2025 21:24:47 +0000</pubDate></item><item><title>[NEW] OpenAI launches a sub $5 ChatGPT plan in India (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/18/openai-launches-a-sub-5-chatgpt-plan-in-india/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/chatgpt-india.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI today launched a new, cheaper ChatGPT paid subscription plan in India called ChatGPT GO, priced at ₹399 per month ($4.60), which is more affordable than the ₹1,999 (about $23) per month Plus Plan.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company had turned on local currency pricing for all its plans a few days ago, and with this launch, it will also allow users to pay through UPI (Unified Payment Interface), India’s payment framework.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Nick Turley, VP at OpenAI and head of ChatGPT, said that this plan will increase the message, image generation, and file uploads by 10 times over the free tier. The ChatGPT Go plan will also enable better memory retention for more personalized responses, Turely said.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;We just launched ChatGPT Go in India, a new subscription tier that gives users in India more access to our most popular features: 10x higher message limits, 10x more image generations, 10x more file uploads, and 2x longer memory compared with our free tier. All for Rs. 399. 🇮🇳&lt;/p&gt;— Nick Turley (@nickaturley) August 19, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“Making ChatGPT more affordable has been a key ask from users! We’re rolling out Go in India first and will learn from feedback before expanding to other countries,” Turley said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;From a currency conversion standpoint, the Plus plan was higher than $20 for Indian users when offered in local currency. The new Go plan offers a more affordable alternative to people who are looking to use ChatGPT primarily for chat, image generation, and file processing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tibor&amp;nbsp;Blaho, a software engineer with a reputation for accurately leaking upcoming AI products, had previously teased this plan and its details. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the company is geo-restricting this plan to India, the company said on its support page that it is working to expand this plan to more regions. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Last month, Turley noted that ChatGPT now has more than 700 million weekly users worldwide — up from 500 million in March. OpenAI launched its updated image generator feature for ChatGPT in March, and since then, it has seen an uptick in usage in India. OpenAI CEO Sam Altman said in a recent podcast that India is the company’s second biggest market. With the Go plan, it wants to cash in on that. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to app analytics firm AppFigures, India has been the leading country in terms of ChatGPT app downloads across platforms, with over 29 million downloads coming from the country in the last 90 days. However, the app only made $3.6 million in this period from users in the country. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This move will likely nudge more consumers to subscribe to using ChatGPT more, given its pricing. Other AI companies have also made moves to attract users from the country’s internet user base of over 850 million. Last month, Perplexity partnered with network provider Airtel to offer free Perplexity Pro subscriptions. Google also dished up a free AI Pro plan for India-based students for one year.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While OpenAI’s move is not giving out any freebies, local and affordable pricing will likely result in a better subscription conversion rate for ChatGPT in India.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/chatgpt-india.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI today launched a new, cheaper ChatGPT paid subscription plan in India called ChatGPT GO, priced at ₹399 per month ($4.60), which is more affordable than the ₹1,999 (about $23) per month Plus Plan.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company had turned on local currency pricing for all its plans a few days ago, and with this launch, it will also allow users to pay through UPI (Unified Payment Interface), India’s payment framework.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Nick Turley, VP at OpenAI and head of ChatGPT, said that this plan will increase the message, image generation, and file uploads by 10 times over the free tier. The ChatGPT Go plan will also enable better memory retention for more personalized responses, Turely said.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;We just launched ChatGPT Go in India, a new subscription tier that gives users in India more access to our most popular features: 10x higher message limits, 10x more image generations, 10x more file uploads, and 2x longer memory compared with our free tier. All for Rs. 399. 🇮🇳&lt;/p&gt;— Nick Turley (@nickaturley) August 19, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“Making ChatGPT more affordable has been a key ask from users! We’re rolling out Go in India first and will learn from feedback before expanding to other countries,” Turley said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;From a currency conversion standpoint, the Plus plan was higher than $20 for Indian users when offered in local currency. The new Go plan offers a more affordable alternative to people who are looking to use ChatGPT primarily for chat, image generation, and file processing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tibor&amp;nbsp;Blaho, a software engineer with a reputation for accurately leaking upcoming AI products, had previously teased this plan and its details. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the company is geo-restricting this plan to India, the company said on its support page that it is working to expand this plan to more regions. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Last month, Turley noted that ChatGPT now has more than 700 million weekly users worldwide — up from 500 million in March. OpenAI launched its updated image generator feature for ChatGPT in March, and since then, it has seen an uptick in usage in India. OpenAI CEO Sam Altman said in a recent podcast that India is the company’s second biggest market. With the Go plan, it wants to cash in on that. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to app analytics firm AppFigures, India has been the leading country in terms of ChatGPT app downloads across platforms, with over 29 million downloads coming from the country in the last 90 days. However, the app only made $3.6 million in this period from users in the country. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This move will likely nudge more consumers to subscribe to using ChatGPT more, given its pricing. Other AI companies have also made moves to attract users from the country’s internet user base of over 850 million. Last month, Perplexity partnered with network provider Airtel to offer free Perplexity Pro subscriptions. Google also dished up a free AI Pro plan for India-based students for one year.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While OpenAI’s move is not giving out any freebies, local and affordable pricing will likely result in a better subscription conversion rate for ChatGPT in India.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/18/openai-launches-a-sub-5-chatgpt-plan-in-india/</guid><pubDate>Tue, 19 Aug 2025 03:57:15 +0000</pubDate></item></channel></rss>