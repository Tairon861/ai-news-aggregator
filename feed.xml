<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sat, 29 Nov 2025 06:32:07 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>Anthropic says it solved the long-running AI agent problem with a new multi-session Claude SDK (AI | VentureBeat)</title><link>https://venturebeat.com/ai/anthropic-says-it-solved-the-long-running-ai-agent-problem-with-a-new-multi</link><description>[unable to retrieve full-text content]&lt;p&gt;Agent memory remains a problem that enterprises want to fix, as agents forget some instructions or conversations the longer they run. &lt;/p&gt;&lt;p&gt;&lt;a href="https://www.anthropic.com/"&gt;&lt;u&gt;Anthropic&lt;/u&gt;&lt;/a&gt; believes it has solved this issue for its &lt;a href="https://venturebeat.com/ai/anthropics-new-claude-can-code-for-30-hours-think-of-it-as-your-ai-coworker"&gt;&lt;u&gt;Claude Agent SDK&lt;/u&gt;&lt;/a&gt;, developing a two-fold solution that allows an agent to work across different context windows.&lt;/p&gt;&lt;p&gt;“The core challenge of long-running agents is that they must work in discrete sessions, and each new session begins with no memory of what came before,” Anthropic wrote in &lt;a href="https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents"&gt;&lt;u&gt;a blog post&lt;/u&gt;&lt;/a&gt;. “Because context windows are limited, and because most complex projects cannot be completed within a single window, agents need a way to bridge the gap between coding sessions.”&lt;/p&gt;&lt;p&gt;Anthropic engineers proposed a two-fold approach for its Agent SDK: An initializer agent to set up the environment, and a coding agent to make incremental progress in each session and leave artifacts for the next.  &lt;/p&gt;&lt;h2&gt;The agent memory problem&lt;/h2&gt;&lt;p&gt;Since agents are built on foundation models, they remain constrained by the limited, although continually growing, context windows. For long-running agents, this could create a larger problem, leading the agent to forget instructions and behave abnormally while performing a task. &lt;a href="https://venturebeat.com/ai/enhancing-ai-agents-with-long-term-memory-insights-into-langmem-sdk-memobase-and-the-a-mem-framework"&gt;&lt;u&gt;Enhancing agent memory&lt;/u&gt;&lt;/a&gt; becomes essential for consistent, business-safe performance. &lt;/p&gt;&lt;p&gt;Several methods emerged over the past year, all attempting to bridge the gap between context windows and agent memory. &lt;a href="https://www.langchain.com/"&gt;&lt;u&gt;LangChain&lt;/u&gt;&lt;/a&gt;’s LangMem SDK, &lt;a href="https://www.memobase.io/"&gt;&lt;u&gt;Memobase&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://openai.com/"&gt;&lt;u&gt;OpenAI&lt;/u&gt;&lt;/a&gt;’s Swarm are examples of companies offering memory solutions. Research on agentic memory has also exploded recently, with proposed &lt;a href="https://venturebeat.com/ai/how-procedural-memory-can-cut-the-cost-and-complexity-of-ai-agents"&gt;&lt;u&gt;frameworks like Memp&lt;/u&gt;&lt;/a&gt; and the &lt;a href="https://venturebeat.com/ai/googles-nested-learning-paradigm-could-solve-ais-memory-and-continual"&gt;&lt;u&gt;Nested Learning Paradigm&lt;/u&gt;&lt;/a&gt; from &lt;a href="https://www.google.com/"&gt;&lt;u&gt;Google&lt;/u&gt;&lt;/a&gt; offering new alternatives to enhance memory. &lt;/p&gt;&lt;p&gt;Many of the current memory frameworks are open source and can ideally adapt to different large language models (LLMs) powering agents. Anthropic’s approach improves its Claude Agent SDK. &lt;/p&gt;&lt;h2&gt;How it works&lt;/h2&gt;&lt;p&gt;Anthropic identified that even though the Claude Agent SDK had context management capabilities and “should be possible for an agent to continue to do useful work for an arbitrarily long time,” it was not sufficient. The company said in its blog post that a model &lt;a href="https://venturebeat.com/ai/anthropics-claude-opus-4-5-is-here-cheaper-ai-infinite-chats-and-coding"&gt;&lt;u&gt;like Opus 4.5&lt;/u&gt;&lt;/a&gt; running the Claude Agent SDK can “fall short of building a production-quality web app if it’s only given a high-level prompt, such as &amp;#x27;build a clone of claude.ai.&amp;#x27;” &lt;/p&gt;&lt;p&gt;The failures manifested in two patterns, Anthropic said. First, the agent tried to do too much, causing the model to run out of context in the middle. The agent then has to guess what happened and cannot pass clear instructions to the next agent. The second failure occurs later on, after some features have already been built. The agent sees progress has been made and just declares the job done. &lt;/p&gt;&lt;p&gt;Anthropic researchers broke down the solution: Setting up an initial environment to lay the foundation for features and prompting each agent to make incremental progress towards a goal, while still leaving a clean slate at the end. &lt;/p&gt;&lt;p&gt;This is where the two-part solution of Anthropic&amp;#x27;s agent comes in. The initializer agent sets up the environment, logging what agents have done and which files have been added. The coding agent will then ask models to make incremental progress and leave structured updates. &lt;/p&gt;&lt;p&gt;“Inspiration for these practices came from knowing what effective software engineers do every day,” Anthropic said. &lt;/p&gt;&lt;p&gt;The researchers said they added testing tools to the coding agent, improving its ability to identify and fix bugs that weren’t obvious from the code alone. &lt;/p&gt;&lt;h2&gt;Future research&lt;/h2&gt;&lt;p&gt;Anthropic noted that its approach is “one possible set of solutions in a long-running agent harness.” However, this is just the beginning stage of what could become a wider research area for many in the AI space. &lt;/p&gt;&lt;p&gt;The company said its experiments to boost long-term memory for agents haven’t shown whether a single general-purpose coding agent works best across contexts or a multi-agent structure. &lt;/p&gt;&lt;p&gt;Its demo also focused on full-stack web app development, so other experiments should focus on generalizing the results across different tasks.&lt;/p&gt;&lt;p&gt;“It’s likely that some or all of these lessons can be applied to the types of long-running agentic tasks required in, for example, scientific research or financial modeling,” Anthropic said. &lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Agent memory remains a problem that enterprises want to fix, as agents forget some instructions or conversations the longer they run. &lt;/p&gt;&lt;p&gt;&lt;a href="https://www.anthropic.com/"&gt;&lt;u&gt;Anthropic&lt;/u&gt;&lt;/a&gt; believes it has solved this issue for its &lt;a href="https://venturebeat.com/ai/anthropics-new-claude-can-code-for-30-hours-think-of-it-as-your-ai-coworker"&gt;&lt;u&gt;Claude Agent SDK&lt;/u&gt;&lt;/a&gt;, developing a two-fold solution that allows an agent to work across different context windows.&lt;/p&gt;&lt;p&gt;“The core challenge of long-running agents is that they must work in discrete sessions, and each new session begins with no memory of what came before,” Anthropic wrote in &lt;a href="https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents"&gt;&lt;u&gt;a blog post&lt;/u&gt;&lt;/a&gt;. “Because context windows are limited, and because most complex projects cannot be completed within a single window, agents need a way to bridge the gap between coding sessions.”&lt;/p&gt;&lt;p&gt;Anthropic engineers proposed a two-fold approach for its Agent SDK: An initializer agent to set up the environment, and a coding agent to make incremental progress in each session and leave artifacts for the next.  &lt;/p&gt;&lt;h2&gt;The agent memory problem&lt;/h2&gt;&lt;p&gt;Since agents are built on foundation models, they remain constrained by the limited, although continually growing, context windows. For long-running agents, this could create a larger problem, leading the agent to forget instructions and behave abnormally while performing a task. &lt;a href="https://venturebeat.com/ai/enhancing-ai-agents-with-long-term-memory-insights-into-langmem-sdk-memobase-and-the-a-mem-framework"&gt;&lt;u&gt;Enhancing agent memory&lt;/u&gt;&lt;/a&gt; becomes essential for consistent, business-safe performance. &lt;/p&gt;&lt;p&gt;Several methods emerged over the past year, all attempting to bridge the gap between context windows and agent memory. &lt;a href="https://www.langchain.com/"&gt;&lt;u&gt;LangChain&lt;/u&gt;&lt;/a&gt;’s LangMem SDK, &lt;a href="https://www.memobase.io/"&gt;&lt;u&gt;Memobase&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://openai.com/"&gt;&lt;u&gt;OpenAI&lt;/u&gt;&lt;/a&gt;’s Swarm are examples of companies offering memory solutions. Research on agentic memory has also exploded recently, with proposed &lt;a href="https://venturebeat.com/ai/how-procedural-memory-can-cut-the-cost-and-complexity-of-ai-agents"&gt;&lt;u&gt;frameworks like Memp&lt;/u&gt;&lt;/a&gt; and the &lt;a href="https://venturebeat.com/ai/googles-nested-learning-paradigm-could-solve-ais-memory-and-continual"&gt;&lt;u&gt;Nested Learning Paradigm&lt;/u&gt;&lt;/a&gt; from &lt;a href="https://www.google.com/"&gt;&lt;u&gt;Google&lt;/u&gt;&lt;/a&gt; offering new alternatives to enhance memory. &lt;/p&gt;&lt;p&gt;Many of the current memory frameworks are open source and can ideally adapt to different large language models (LLMs) powering agents. Anthropic’s approach improves its Claude Agent SDK. &lt;/p&gt;&lt;h2&gt;How it works&lt;/h2&gt;&lt;p&gt;Anthropic identified that even though the Claude Agent SDK had context management capabilities and “should be possible for an agent to continue to do useful work for an arbitrarily long time,” it was not sufficient. The company said in its blog post that a model &lt;a href="https://venturebeat.com/ai/anthropics-claude-opus-4-5-is-here-cheaper-ai-infinite-chats-and-coding"&gt;&lt;u&gt;like Opus 4.5&lt;/u&gt;&lt;/a&gt; running the Claude Agent SDK can “fall short of building a production-quality web app if it’s only given a high-level prompt, such as &amp;#x27;build a clone of claude.ai.&amp;#x27;” &lt;/p&gt;&lt;p&gt;The failures manifested in two patterns, Anthropic said. First, the agent tried to do too much, causing the model to run out of context in the middle. The agent then has to guess what happened and cannot pass clear instructions to the next agent. The second failure occurs later on, after some features have already been built. The agent sees progress has been made and just declares the job done. &lt;/p&gt;&lt;p&gt;Anthropic researchers broke down the solution: Setting up an initial environment to lay the foundation for features and prompting each agent to make incremental progress towards a goal, while still leaving a clean slate at the end. &lt;/p&gt;&lt;p&gt;This is where the two-part solution of Anthropic&amp;#x27;s agent comes in. The initializer agent sets up the environment, logging what agents have done and which files have been added. The coding agent will then ask models to make incremental progress and leave structured updates. &lt;/p&gt;&lt;p&gt;“Inspiration for these practices came from knowing what effective software engineers do every day,” Anthropic said. &lt;/p&gt;&lt;p&gt;The researchers said they added testing tools to the coding agent, improving its ability to identify and fix bugs that weren’t obvious from the code alone. &lt;/p&gt;&lt;h2&gt;Future research&lt;/h2&gt;&lt;p&gt;Anthropic noted that its approach is “one possible set of solutions in a long-running agent harness.” However, this is just the beginning stage of what could become a wider research area for many in the AI space. &lt;/p&gt;&lt;p&gt;The company said its experiments to boost long-term memory for agents haven’t shown whether a single general-purpose coding agent works best across contexts or a multi-agent structure. &lt;/p&gt;&lt;p&gt;Its demo also focused on full-stack web app development, so other experiments should focus on generalizing the results across different tasks.&lt;/p&gt;&lt;p&gt;“It’s likely that some or all of these lessons can be applied to the types of long-running agentic tasks required in, for example, scientific research or financial modeling,” Anthropic said. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/anthropic-says-it-solved-the-long-running-ai-agent-problem-with-a-new-multi</guid><pubDate>Fri, 28 Nov 2025 19:30:00 +0000</pubDate></item><item><title>Supabase hit $5B by turning down million-dollar contracts. Here’s why. (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/video/supabase-hit-5b-by-turning-down-million-dollar-contracts-heres-why/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/09/postgres-bloat-1.jpeg?w=640" /&gt;&lt;/div&gt;&lt;div class="jwppp-video-box" id="jwppp-video-box-30711601"&gt;






&lt;span class="jwppp-instant"&gt;&lt;/span&gt;&lt;p&gt;Loading the player…&lt;/p&gt;
&lt;/div&gt;



&lt;p class="wp-block-paragraph"&gt;Vibe coding has taken the tech industry by storm, and&amp;nbsp;it’s&amp;nbsp;not just the&amp;nbsp;Lovables&amp;nbsp;and&amp;nbsp;Replits&amp;nbsp;of the world that&amp;nbsp;are&amp;nbsp;winning. The startups building the infrastructure behind them are cashing in too.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Supabase, the open-source database platform&amp;nbsp;that’s&amp;nbsp;become the backend of choice for the vibe-coding world,&amp;nbsp;raised $100 million at a&amp;nbsp;$5 billion&amp;nbsp;valuation&amp;nbsp;just months after closing $200 million at&amp;nbsp;$2 billion. But co-founder and CEO Paul&amp;nbsp;Copplestone&amp;nbsp;has a surprising strategy: he keeps turning down million-dollar enterprise contracts&amp;nbsp;from deep-pocketed but demanding customers.&amp;nbsp;He’s&amp;nbsp;betting instead that&amp;nbsp;if he sticks to his own product&amp;nbsp;vision, the world will come to him. So far,&amp;nbsp;he’s&amp;nbsp;been right.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Today on TechCrunch’s Equity podcast, Julie Bort sat down with&amp;nbsp;Copplestone&amp;nbsp;to explore&amp;nbsp;Supabase’s&amp;nbsp;rise and what it means for&amp;nbsp;vibe coding, developers and&amp;nbsp;the database&amp;nbsp;giants who have historically controlled this market.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on&amp;nbsp;X&amp;nbsp;and&amp;nbsp;Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/09/postgres-bloat-1.jpeg?w=640" /&gt;&lt;/div&gt;&lt;div class="jwppp-video-box" id="jwppp-video-box-30711601"&gt;






&lt;span class="jwppp-instant"&gt;&lt;/span&gt;&lt;p&gt;Loading the player…&lt;/p&gt;
&lt;/div&gt;



&lt;p class="wp-block-paragraph"&gt;Vibe coding has taken the tech industry by storm, and&amp;nbsp;it’s&amp;nbsp;not just the&amp;nbsp;Lovables&amp;nbsp;and&amp;nbsp;Replits&amp;nbsp;of the world that&amp;nbsp;are&amp;nbsp;winning. The startups building the infrastructure behind them are cashing in too.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Supabase, the open-source database platform&amp;nbsp;that’s&amp;nbsp;become the backend of choice for the vibe-coding world,&amp;nbsp;raised $100 million at a&amp;nbsp;$5 billion&amp;nbsp;valuation&amp;nbsp;just months after closing $200 million at&amp;nbsp;$2 billion. But co-founder and CEO Paul&amp;nbsp;Copplestone&amp;nbsp;has a surprising strategy: he keeps turning down million-dollar enterprise contracts&amp;nbsp;from deep-pocketed but demanding customers.&amp;nbsp;He’s&amp;nbsp;betting instead that&amp;nbsp;if he sticks to his own product&amp;nbsp;vision, the world will come to him. So far,&amp;nbsp;he’s&amp;nbsp;been right.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Today on TechCrunch’s Equity podcast, Julie Bort sat down with&amp;nbsp;Copplestone&amp;nbsp;to explore&amp;nbsp;Supabase’s&amp;nbsp;rise and what it means for&amp;nbsp;vibe coding, developers and&amp;nbsp;the database&amp;nbsp;giants who have historically controlled this market.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on&amp;nbsp;X&amp;nbsp;and&amp;nbsp;Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/video/supabase-hit-5b-by-turning-down-million-dollar-contracts-heres-why/</guid><pubDate>Fri, 28 Nov 2025 23:00:00 +0000</pubDate></item></channel></rss>