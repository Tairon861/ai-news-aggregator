<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 12 Aug 2025 12:46:53 +0000</lastBuildDate><item><title>[NEW] What you may have missed about GPT-5 (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/08/12/1121565/what-you-may-have-missed-about-gpt-5/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/GPT5-medical3.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Before OpenAI released GPT-5 last Thursday, CEO Sam Altman said its capabilities made him feel “useless relative to the AI.” He said working on it carries a weight he imagines the developers of the atom bomb must have felt.&lt;/p&gt;  &lt;p&gt;As tech giants converge on models that do more or less the same thing, OpenAI’s new offering was supposed to give a glimpse of AI’s newest frontier. It was meant to mark a leap toward the “artificial general intelligence” that tech’s evangelists have promised will transform humanity for the better.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Against those expectations, the model has mostly underwhelmed.&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;People have highlighted glaring mistakes in GPT-5’s responses, countering Altman’s claim made at the launch that it works like “a legitimate PhD-level expert in anything any area you need on demand.” Early testers have also found issues with OpenAI’s promise that GPT-5 automatically works out what type of AI model is best suited for your question—a reasoning model for more complicated queries, or a faster model for simpler ones. Altman seems to have conceded that this feature is flawed and takes away user control. However there is good news too: the model seems to have eased the problem of ChatGPT sucking up to users, with GPT-5 less likely to shower them with over the top compliments.&lt;/p&gt; 
 &lt;p&gt;Overall, as my colleague Grace Huckins pointed out, the new release represents more of a product update—providing slicker and prettier ways of conversing with ChatGPT—than a breakthrough that reshapes what is possible in AI.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But there’s one other thing to take from all this. For a while, AI companies didn’t make much effort to suggest how their models might be used. Instead, the plan was to simply build the smartest model possible—a brain of sorts—and trust that it would be good at lots of things. Writing poetry would come as naturally as organic chemistry. Getting there would be accomplished by bigger models, better training techniques, and technical breakthroughs.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;That has been changing: The play now is to push existing models into more places by hyping up specific applications. Companies have been more aggressive in their promises that their AI models can replace human coders, for example (even if the early evidence suggests otherwise). A possible explanation for this pivot is that tech giants simply have not made the breakthroughs they’ve expected. We might be stuck with only marginal improvements in large language models’ capabilities for the time being. That leaves AI companies with one option: Work with what you’ve got.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;The starkest example of this in the launch of GPT-5 is how much OpenAI is encouraging people to use it for health advice, one of AI’s most fraught arenas.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In the beginning, OpenAI mostly didn’t play ball with medical questions. If you tried to ask ChatGPT about your health, it gave lots of disclaimers warning you that it was not a doctor, and for some questions, it would refuse to give a response at all. But as I recently reported, those disclaimers began disappearing as OpenAI released new models. Its models will now not only interpret x-rays and mammograms for you but ask follow-up questions leading toward a diagnosis.&lt;/p&gt;  &lt;p&gt;In May, OpenAI signaled it would try to tackle medical questions head on.&lt;strong&gt; &lt;/strong&gt;It announced HealthBench, a way to evaluate how good AI systems are at handling health topics as measured against the opinions of physicians. In July, it published a study it participated in, reporting that a cohort of doctors in Kenya made fewer diagnostic mistakes when they were helped by an AI model.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;With the launch of GPT-5, OpenAI has begun explicitly telling people to use its models for health advice. At the launch event, Altman welcomed on stage Felipe Millon, an OpenAI employee, and his wife, Carolina Millon, who had recently been diagnosed with multiple forms of cancer. Carolina spoke about asking ChatGPT for help with her diagnoses, saying that she had uploaded copies of her biopsy results to ChatGPT to translate medical jargon and asked the AI for help making decisions about things like whether or not to pursue radiation. The trio called it an empowering example of shrinking the knowledge gap between doctors and patients.&lt;/p&gt;  &lt;p&gt;With this change in approach, OpenAI is wading into dangerous waters.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;For one, it’s using evidence that doctors can benefit from AI as a clinical tool, as in the Kenya study, to suggest that people without any medical background should ask the AI model for advice about their own health. The problem is that lots of people might ask for this advice without ever running it by a doctor (and are less likely to do so now that the chatbot rarely prompts them to).&lt;/p&gt;  &lt;p&gt;Indeed, two days before the launch of GPT-5, the &lt;em&gt;Annals of Internal Medicine&lt;/em&gt; published a paper about a man who stopped eating salt and began ingesting dangerous amounts of bromide following a conversation with ChatGPT. He developed bromide poisoning—which largely disappeared in the US after the Food and Drug Administration began curbing the use of bromide in over-the-counter medications in the 1970s—and then nearly died, spending weeks in the hospital.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;So what’s the point of all this? Essentially, it’s about accountability. When AI companies move from promising general intelligence to offering humanlike helpfulness in a specific field like health care, it raises a second, yet unanswered question about what will happen when mistakes are made. As things stand, there’s little indication tech companies will be made liable for the harm caused.&lt;/p&gt;  &lt;p&gt;“When doctors give you harmful medical advice due to error or prejudicial bias, you can sue them for malpractice and get recompense,” says Damien Williams, an assistant professor of data science and philosophy at the University of North Carolina Charlotte.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“When ChatGPT gives you harmful medical advice because it’s been trained on prejudicial data, or because ‘hallucinations’ are inherent in the operations of the system, what’s your recourse?”&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,&amp;nbsp;sign up here.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/GPT5-medical3.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Before OpenAI released GPT-5 last Thursday, CEO Sam Altman said its capabilities made him feel “useless relative to the AI.” He said working on it carries a weight he imagines the developers of the atom bomb must have felt.&lt;/p&gt;  &lt;p&gt;As tech giants converge on models that do more or less the same thing, OpenAI’s new offering was supposed to give a glimpse of AI’s newest frontier. It was meant to mark a leap toward the “artificial general intelligence” that tech’s evangelists have promised will transform humanity for the better.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Against those expectations, the model has mostly underwhelmed.&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;People have highlighted glaring mistakes in GPT-5’s responses, countering Altman’s claim made at the launch that it works like “a legitimate PhD-level expert in anything any area you need on demand.” Early testers have also found issues with OpenAI’s promise that GPT-5 automatically works out what type of AI model is best suited for your question—a reasoning model for more complicated queries, or a faster model for simpler ones. Altman seems to have conceded that this feature is flawed and takes away user control. However there is good news too: the model seems to have eased the problem of ChatGPT sucking up to users, with GPT-5 less likely to shower them with over the top compliments.&lt;/p&gt; 
 &lt;p&gt;Overall, as my colleague Grace Huckins pointed out, the new release represents more of a product update—providing slicker and prettier ways of conversing with ChatGPT—than a breakthrough that reshapes what is possible in AI.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But there’s one other thing to take from all this. For a while, AI companies didn’t make much effort to suggest how their models might be used. Instead, the plan was to simply build the smartest model possible—a brain of sorts—and trust that it would be good at lots of things. Writing poetry would come as naturally as organic chemistry. Getting there would be accomplished by bigger models, better training techniques, and technical breakthroughs.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;That has been changing: The play now is to push existing models into more places by hyping up specific applications. Companies have been more aggressive in their promises that their AI models can replace human coders, for example (even if the early evidence suggests otherwise). A possible explanation for this pivot is that tech giants simply have not made the breakthroughs they’ve expected. We might be stuck with only marginal improvements in large language models’ capabilities for the time being. That leaves AI companies with one option: Work with what you’ve got.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;The starkest example of this in the launch of GPT-5 is how much OpenAI is encouraging people to use it for health advice, one of AI’s most fraught arenas.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In the beginning, OpenAI mostly didn’t play ball with medical questions. If you tried to ask ChatGPT about your health, it gave lots of disclaimers warning you that it was not a doctor, and for some questions, it would refuse to give a response at all. But as I recently reported, those disclaimers began disappearing as OpenAI released new models. Its models will now not only interpret x-rays and mammograms for you but ask follow-up questions leading toward a diagnosis.&lt;/p&gt;  &lt;p&gt;In May, OpenAI signaled it would try to tackle medical questions head on.&lt;strong&gt; &lt;/strong&gt;It announced HealthBench, a way to evaluate how good AI systems are at handling health topics as measured against the opinions of physicians. In July, it published a study it participated in, reporting that a cohort of doctors in Kenya made fewer diagnostic mistakes when they were helped by an AI model.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;With the launch of GPT-5, OpenAI has begun explicitly telling people to use its models for health advice. At the launch event, Altman welcomed on stage Felipe Millon, an OpenAI employee, and his wife, Carolina Millon, who had recently been diagnosed with multiple forms of cancer. Carolina spoke about asking ChatGPT for help with her diagnoses, saying that she had uploaded copies of her biopsy results to ChatGPT to translate medical jargon and asked the AI for help making decisions about things like whether or not to pursue radiation. The trio called it an empowering example of shrinking the knowledge gap between doctors and patients.&lt;/p&gt;  &lt;p&gt;With this change in approach, OpenAI is wading into dangerous waters.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;For one, it’s using evidence that doctors can benefit from AI as a clinical tool, as in the Kenya study, to suggest that people without any medical background should ask the AI model for advice about their own health. The problem is that lots of people might ask for this advice without ever running it by a doctor (and are less likely to do so now that the chatbot rarely prompts them to).&lt;/p&gt;  &lt;p&gt;Indeed, two days before the launch of GPT-5, the &lt;em&gt;Annals of Internal Medicine&lt;/em&gt; published a paper about a man who stopped eating salt and began ingesting dangerous amounts of bromide following a conversation with ChatGPT. He developed bromide poisoning—which largely disappeared in the US after the Food and Drug Administration began curbing the use of bromide in over-the-counter medications in the 1970s—and then nearly died, spending weeks in the hospital.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;So what’s the point of all this? Essentially, it’s about accountability. When AI companies move from promising general intelligence to offering humanlike helpfulness in a specific field like health care, it raises a second, yet unanswered question about what will happen when mistakes are made. As things stand, there’s little indication tech companies will be made liable for the harm caused.&lt;/p&gt;  &lt;p&gt;“When doctors give you harmful medical advice due to error or prejudicial bias, you can sue them for malpractice and get recompense,” says Damien Williams, an assistant professor of data science and philosophy at the University of North Carolina Charlotte.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“When ChatGPT gives you harmful medical advice because it’s been trained on prejudicial data, or because ‘hallucinations’ are inherent in the operations of the system, what’s your recourse?”&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,&amp;nbsp;sign up here.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/08/12/1121565/what-you-may-have-missed-about-gpt-5/</guid><pubDate>Tue, 12 Aug 2025 07:52:23 +0000</pubDate></item><item><title>[NEW] NVIDIA latest: Blackwell GPU and software updates (AI News)</title><link>https://www.artificialintelligence-news.com/news/nvidia-expands-blackwell-powered-servers-with-new-ai-and-robotics-capabilities/</link><description>&lt;p&gt;NVIDIA’s latest RTX PRO 6000 Blackwell Server Edition GPU will soon be available in enterprise servers.&lt;/p&gt;&lt;p&gt;Systems from Cisco, Dell Technologies, HPE, Lenovo, and Supermicro will ship various configurations in 2U servers. Nvidia says they will offer higher performance and efficiency for AI, graphics, simulation, analytics, and industrial applications, and support tasks like AI model training, content creation, and scientific research.&lt;/p&gt;&lt;p&gt;“AI is reinventing computing for the first time in 60 years – what started in the cloud is now transforming the architecture of on-premises data centres,” said Jensen Huang, founder and CEO of NVIDIA. “With the world’s leading server providers, we’re making NVIDIA Blackwell RTX PRO Servers the standard platform for enterprise and industrial AI.”&lt;/p&gt;&lt;h3&gt;GPU acceleration for business workloads&lt;/h3&gt;&lt;p&gt;Millions of servers are sold each year for business operations, most still using ‘traditional’ CPUs. The new RTX PRO Servers give systems GPU acceleration, boosting performance in analytics, simulations, video processing, and rendering, the company says. NVIDIA says its Server Edition GPU can deliver up to 45 times better performance than CPU-only systems, with 18 times higher energy efficiency.&lt;/p&gt;&lt;p&gt;The RTX PRO line is aimed at companies building “AI factories” where space, power, and cooling may be limited. The servers also provide the infrastructure for NVIDIA’s AI Data Platform for storage systems. Dell, for example, is updating its AI Data Platform to use NVIDIA’s design; its PowerEdge R7725 servers comes with two RTX PRO 6000 GPUs, NVIDIA AI Enterprise software, and NVIDIA networking.&lt;/p&gt;&lt;p&gt;The new 2U servers, which can house up to eight GPU units, were among those announced in May at COMPUTEX.&lt;/p&gt;&lt;h3&gt;Blackwell architecture features&lt;/h3&gt;&lt;p&gt;The new servers are built around NVIDIA’s Blackwell architecture, which includes:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Fifth-generation Tensor Cores and a second-generation Transformer Engine with FP4 precision, capable of running inference at up to six times faster than the L40S GPU.&lt;/li&gt;&lt;li&gt;Fourth-generation RTX technology for photo rendering, with up to four times the performance of the L40S GPU.&lt;/li&gt;&lt;li&gt;Virtualisation and NVIDIA Multi-Instance GPU technology, allowing four separate workloads per GPU.&lt;/li&gt;&lt;li&gt;Improved energy efficiency for lower data centre power use.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;For physical AI and robotics&lt;/h3&gt;&lt;p&gt;NVIDIA’s Omniverse libraries and Cosmos world foundation models on RTX PRO Servers can run digital twin simulations, robot training routines, and large-scale synthetic data creation. They also support NVIDIA Metropolis blueprints for video search and summarisation and vision language models, among other tools for use in physical environments.&lt;/p&gt;&lt;p&gt;NVIDIA’s has updated its Omniverse and Cosmos offerings, with new Omniverse SDKs and added compatibility with MuJoCo (MJCF) and Universal Scene Description (OpenUSD). The company says this will allow over 250,000 MJCF developers to run robot simulations on its platforms. New Omniverse NuRec libraries bring ray-traced 3D Gaussian splatting for model construction from sensor data, while the updated Isaac Sim 5.0 and Isaac Lab 2.2 frameworks – available on GitHub – add neural rendering and new OpenUSD-based schemas for robots and sensors.&lt;/p&gt;&lt;p&gt;NuRec rendering is already integrated into the CARLA autonomous vehicle simulator and is being adopted by companies like Foretellix, which is using it for generating synthetic AV testing data. Voxel51’s FiftyOne data engine, used by automakers like Ford and Porsche, now supports NuRec. Boston Dynamics, Figure AI, Hexagon, and Amazon Devices &amp;amp; Services are among those already adopting the libraries and frameworks.&lt;/p&gt;&lt;p&gt;Cosmos WFMs has been downloaded over two million times. The software helps generate synthetic training data for robots using text, image, or video prompts. The new Cosmos Transfer-2 model speeds up image data generation from simulation scenes and spatial inputs like depth maps. Companies like Lightwheel, Moon Surgical, and Skild AI are using Cosmos have begun to produce training data at scale using Cosmos Transfer-2.&lt;/p&gt;&lt;p&gt;NVIDIA has also introduced Cosmos Reason, a 7-billion-parameter vision language model to help robots and AI agents combine prior knowledge and understanding of physics. It can automate dataset curation, supports multi-step robot task planning, and run video analytics systems. NVIDIA’s own robotics and DRIVE teams use Cosmos Reason for data filtering and annotation, and Uber and Magna have deployed it in autonomous vehicles, traffic monitoring, and industrial inspection systems.&lt;/p&gt;&lt;h3&gt;AI agents and large-scale deployments&lt;/h3&gt;&lt;p&gt;RTX PRO Servers can run the newly-announced Llama Nemotron Super model. When running with NVFP4 precision on a single RTX PRO 6000 GPU, they deliver up to three times better price performance than FP8 on NVIDIA’s H100 GPUs.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Nvidia)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Nvidia reclaims title of most valuable company on AI momentum&lt;/strong&gt;&lt;/p&gt;&lt;img alt="alt" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;NVIDIA’s latest RTX PRO 6000 Blackwell Server Edition GPU will soon be available in enterprise servers.&lt;/p&gt;&lt;p&gt;Systems from Cisco, Dell Technologies, HPE, Lenovo, and Supermicro will ship various configurations in 2U servers. Nvidia says they will offer higher performance and efficiency for AI, graphics, simulation, analytics, and industrial applications, and support tasks like AI model training, content creation, and scientific research.&lt;/p&gt;&lt;p&gt;“AI is reinventing computing for the first time in 60 years – what started in the cloud is now transforming the architecture of on-premises data centres,” said Jensen Huang, founder and CEO of NVIDIA. “With the world’s leading server providers, we’re making NVIDIA Blackwell RTX PRO Servers the standard platform for enterprise and industrial AI.”&lt;/p&gt;&lt;h3&gt;GPU acceleration for business workloads&lt;/h3&gt;&lt;p&gt;Millions of servers are sold each year for business operations, most still using ‘traditional’ CPUs. The new RTX PRO Servers give systems GPU acceleration, boosting performance in analytics, simulations, video processing, and rendering, the company says. NVIDIA says its Server Edition GPU can deliver up to 45 times better performance than CPU-only systems, with 18 times higher energy efficiency.&lt;/p&gt;&lt;p&gt;The RTX PRO line is aimed at companies building “AI factories” where space, power, and cooling may be limited. The servers also provide the infrastructure for NVIDIA’s AI Data Platform for storage systems. Dell, for example, is updating its AI Data Platform to use NVIDIA’s design; its PowerEdge R7725 servers comes with two RTX PRO 6000 GPUs, NVIDIA AI Enterprise software, and NVIDIA networking.&lt;/p&gt;&lt;p&gt;The new 2U servers, which can house up to eight GPU units, were among those announced in May at COMPUTEX.&lt;/p&gt;&lt;h3&gt;Blackwell architecture features&lt;/h3&gt;&lt;p&gt;The new servers are built around NVIDIA’s Blackwell architecture, which includes:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Fifth-generation Tensor Cores and a second-generation Transformer Engine with FP4 precision, capable of running inference at up to six times faster than the L40S GPU.&lt;/li&gt;&lt;li&gt;Fourth-generation RTX technology for photo rendering, with up to four times the performance of the L40S GPU.&lt;/li&gt;&lt;li&gt;Virtualisation and NVIDIA Multi-Instance GPU technology, allowing four separate workloads per GPU.&lt;/li&gt;&lt;li&gt;Improved energy efficiency for lower data centre power use.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;For physical AI and robotics&lt;/h3&gt;&lt;p&gt;NVIDIA’s Omniverse libraries and Cosmos world foundation models on RTX PRO Servers can run digital twin simulations, robot training routines, and large-scale synthetic data creation. They also support NVIDIA Metropolis blueprints for video search and summarisation and vision language models, among other tools for use in physical environments.&lt;/p&gt;&lt;p&gt;NVIDIA’s has updated its Omniverse and Cosmos offerings, with new Omniverse SDKs and added compatibility with MuJoCo (MJCF) and Universal Scene Description (OpenUSD). The company says this will allow over 250,000 MJCF developers to run robot simulations on its platforms. New Omniverse NuRec libraries bring ray-traced 3D Gaussian splatting for model construction from sensor data, while the updated Isaac Sim 5.0 and Isaac Lab 2.2 frameworks – available on GitHub – add neural rendering and new OpenUSD-based schemas for robots and sensors.&lt;/p&gt;&lt;p&gt;NuRec rendering is already integrated into the CARLA autonomous vehicle simulator and is being adopted by companies like Foretellix, which is using it for generating synthetic AV testing data. Voxel51’s FiftyOne data engine, used by automakers like Ford and Porsche, now supports NuRec. Boston Dynamics, Figure AI, Hexagon, and Amazon Devices &amp;amp; Services are among those already adopting the libraries and frameworks.&lt;/p&gt;&lt;p&gt;Cosmos WFMs has been downloaded over two million times. The software helps generate synthetic training data for robots using text, image, or video prompts. The new Cosmos Transfer-2 model speeds up image data generation from simulation scenes and spatial inputs like depth maps. Companies like Lightwheel, Moon Surgical, and Skild AI are using Cosmos have begun to produce training data at scale using Cosmos Transfer-2.&lt;/p&gt;&lt;p&gt;NVIDIA has also introduced Cosmos Reason, a 7-billion-parameter vision language model to help robots and AI agents combine prior knowledge and understanding of physics. It can automate dataset curation, supports multi-step robot task planning, and run video analytics systems. NVIDIA’s own robotics and DRIVE teams use Cosmos Reason for data filtering and annotation, and Uber and Magna have deployed it in autonomous vehicles, traffic monitoring, and industrial inspection systems.&lt;/p&gt;&lt;h3&gt;AI agents and large-scale deployments&lt;/h3&gt;&lt;p&gt;RTX PRO Servers can run the newly-announced Llama Nemotron Super model. When running with NVFP4 precision on a single RTX PRO 6000 GPU, they deliver up to three times better price performance than FP8 on NVIDIA’s H100 GPUs.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Nvidia)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Nvidia reclaims title of most valuable company on AI momentum&lt;/strong&gt;&lt;/p&gt;&lt;img alt="alt" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/nvidia-expands-blackwell-powered-servers-with-new-ai-and-robotics-capabilities/</guid><pubDate>Tue, 12 Aug 2025 09:16:33 +0000</pubDate></item><item><title>[NEW] SoundHound is giving its AI the power of sight (AI News)</title><link>https://www.artificialintelligence-news.com/news/soundhound-is-giving-its-ai-the-power-of-sight/</link><description>&lt;p&gt;SoundHound AI, already a major player in voice assistants, is now giving its technology a pair of eyes.&lt;/p&gt;&lt;p&gt;Imagine driving past a landmark and, without pulling out your phone, asking your car, “What’s that building over there?” and getting an instant answer. That’s what SoundHound AI is building.&amp;nbsp;&lt;/p&gt;&lt;p&gt;With the launch of Vision AI, SoundHound’s new system combines sight with sound to create a much smarter and more natural way to interact with technology. The idea is to mimic how we as humans operate; we don’t just listen to someone, we also see their gestures and what they’re looking at.&lt;/p&gt;&lt;p&gt;By bringing this same contextual understanding to AI, SoundHound hopes to smooth over the clunky and often frustrating experience we have with many of today’s smart devices. The company is targeting real-world applications where this combined sense could make a huge difference, whether that’s in your next car, at the restaurant drive-thru, or a factory floor.&lt;/p&gt;&lt;p&gt;Keyvan Mohajer, CEO of SoundHound AI, said: “At SoundHound, we believe the future of AI isn’t just multimodal—it’s deeply integrated, responsive, and built for real-world impact.&lt;/p&gt;&lt;p&gt;“With Vision AI, we’re extending our leadership in voice and conversational AI to redefine how humans interact with products and services offered and used by businesses.”&lt;/p&gt;&lt;p&gt;So, how does it work? Vision AI takes a live feed from a camera and fuses it with the company’s voice technology, which already excels at understanding natural speech. By processing what it sees and what it hears at the exact same time, the system can grasp the user’s true intent in a way a simple voice assistant never could.&lt;/p&gt;&lt;p&gt;Think of a mechanic wearing smart glasses who can simply look at an engine part and ask for instructions, receiving instant visual and audio guidance without ever putting down their tools. In a shop, a staff member could scan shelves just by looking at them to get a real-time inventory count. For the rest of us, it might mean a drive-thru kiosk that visually confirms our order on screen the moment we say it.&lt;/p&gt;&lt;p&gt;One of the biggest technical problems in creating such a system is ensuring the audio and visual elements are perfectly synchronised. Any lag would shatter the illusion of a natural conversation.&lt;/p&gt;&lt;p&gt;Pranav Singh, VP of Engineering at SoundHound AI, commented: “With Vision AI, we are fusing visual recognition and conversational intelligence into a single, synchronised flow. Every frame, every utterance, every intent is interpreted within the same ecosystem—ensuring faster, more natural user experiences that scale across surfaces from kiosks to embedded devices.&lt;/p&gt;&lt;p&gt;“This is innovation at the intersection of intelligence and execution, delivering AI that sees what you see, hears what you say, and responds in the moment.”&lt;/p&gt;&lt;p&gt;For the businesses adopting this tech, the promise is to provide faster service, fewer mistakes, and happier customers. It’s about removing friction and making technology feel less like a tool you have to operate and more like a partner that helps you get things done.&lt;/p&gt;&lt;p&gt;This new visual capability isn’t the only upgrade SoundHound is rolling out. The company also recently improved the “brain” of its system with a new update, Amelia 7.1. This enhancement makes its AI agents faster, more accurate, and gives businesses more control and transparency over how they work.&lt;/p&gt;&lt;p&gt;By combining sight and sound, SoundHound is aiming to push us closer to a world where interacting with AI feels as easy and intuitive as talking to another person.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Christian Lue)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Alan Turing Institute: Humanities are key to the future of AI&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;SoundHound AI, already a major player in voice assistants, is now giving its technology a pair of eyes.&lt;/p&gt;&lt;p&gt;Imagine driving past a landmark and, without pulling out your phone, asking your car, “What’s that building over there?” and getting an instant answer. That’s what SoundHound AI is building.&amp;nbsp;&lt;/p&gt;&lt;p&gt;With the launch of Vision AI, SoundHound’s new system combines sight with sound to create a much smarter and more natural way to interact with technology. The idea is to mimic how we as humans operate; we don’t just listen to someone, we also see their gestures and what they’re looking at.&lt;/p&gt;&lt;p&gt;By bringing this same contextual understanding to AI, SoundHound hopes to smooth over the clunky and often frustrating experience we have with many of today’s smart devices. The company is targeting real-world applications where this combined sense could make a huge difference, whether that’s in your next car, at the restaurant drive-thru, or a factory floor.&lt;/p&gt;&lt;p&gt;Keyvan Mohajer, CEO of SoundHound AI, said: “At SoundHound, we believe the future of AI isn’t just multimodal—it’s deeply integrated, responsive, and built for real-world impact.&lt;/p&gt;&lt;p&gt;“With Vision AI, we’re extending our leadership in voice and conversational AI to redefine how humans interact with products and services offered and used by businesses.”&lt;/p&gt;&lt;p&gt;So, how does it work? Vision AI takes a live feed from a camera and fuses it with the company’s voice technology, which already excels at understanding natural speech. By processing what it sees and what it hears at the exact same time, the system can grasp the user’s true intent in a way a simple voice assistant never could.&lt;/p&gt;&lt;p&gt;Think of a mechanic wearing smart glasses who can simply look at an engine part and ask for instructions, receiving instant visual and audio guidance without ever putting down their tools. In a shop, a staff member could scan shelves just by looking at them to get a real-time inventory count. For the rest of us, it might mean a drive-thru kiosk that visually confirms our order on screen the moment we say it.&lt;/p&gt;&lt;p&gt;One of the biggest technical problems in creating such a system is ensuring the audio and visual elements are perfectly synchronised. Any lag would shatter the illusion of a natural conversation.&lt;/p&gt;&lt;p&gt;Pranav Singh, VP of Engineering at SoundHound AI, commented: “With Vision AI, we are fusing visual recognition and conversational intelligence into a single, synchronised flow. Every frame, every utterance, every intent is interpreted within the same ecosystem—ensuring faster, more natural user experiences that scale across surfaces from kiosks to embedded devices.&lt;/p&gt;&lt;p&gt;“This is innovation at the intersection of intelligence and execution, delivering AI that sees what you see, hears what you say, and responds in the moment.”&lt;/p&gt;&lt;p&gt;For the businesses adopting this tech, the promise is to provide faster service, fewer mistakes, and happier customers. It’s about removing friction and making technology feel less like a tool you have to operate and more like a partner that helps you get things done.&lt;/p&gt;&lt;p&gt;This new visual capability isn’t the only upgrade SoundHound is rolling out. The company also recently improved the “brain” of its system with a new update, Amelia 7.1. This enhancement makes its AI agents faster, more accurate, and gives businesses more control and transparency over how they work.&lt;/p&gt;&lt;p&gt;By combining sight and sound, SoundHound is aiming to push us closer to a world where interacting with AI feels as easy and intuitive as talking to another person.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Christian Lue)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Alan Turing Institute: Humanities are key to the future of AI&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/soundhound-is-giving-its-ai-the-power-of-sight/</guid><pubDate>Tue, 12 Aug 2025 10:06:54 +0000</pubDate></item><item><title>[NEW] The Download: meet the judges using AI, and GPT-5’s health promises (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/08/12/1121658/the-download-meet-the-judges-using-ai-and-gpt-5s-health-promises/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Meet the early-adopter judges using AI&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;The propensity for AI systems to make mistakes that humans miss has been on full display in the US legal system as of late. The follies began when lawyers submitted documents citing cases that didn’t exist. Similar mistakes soon spread to other roles in the courts. Last December, a Stanford professor submitted sworn testimony containing hallucinations and errors in a case about deepfakes, despite being an expert on AI and misinformation himself.&lt;/p&gt;&lt;p&gt;Now, judges are experimenting with generative AI too. Some believe that with the right precautions, the technology can expedite legal research, summarize cases, draft routine orders, and overall help speed up the court system, which is badly backlogged in many parts of the US. Are they right to be so confident in it? Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—James O’Donnell&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;What you may have missed about GPT-5&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;OpenAI’s new GPT-5 model was supposed to give a glimpse of AI’s newest frontier. It was meant to mark a leap toward the “artificial general intelligence” that tech’s evangelists have promised will transform humanity for the better.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Against those expectations, the model has mostly underwhelmed. But there’s one other thing to take from all this. Among other suggestions for potential uses of its models, OpenAI has begun explicitly telling people to use them for health advice. It’s a change in approach that signals the company is wading into dangerous waters. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—James O’Donnell&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 The US has extended its China tariff truce by another 90 days&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;It’s yet again another example of Trump’s on-again, off-again policies. (CNBC)&lt;br /&gt;+ &lt;em&gt;China has succeeded in finding other markets to sell to anyway. &lt;/em&gt;(CNN)&lt;br /&gt;+ &lt;em&gt;Now we’ve got to wait until November 10 for the next round of tariffs. &lt;/em&gt;(BBC)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 Europe’s arms factories are rapidly expanding&lt;/strong&gt;&lt;br /&gt;As EU governments debate how to sustain weapons deliveries to Ukraine. (FT $)&lt;br /&gt;+ &lt;em&gt;Trump is due to meet with Vladimir Putin on Friday. &lt;/em&gt;(The Guardian)&lt;br /&gt;+ &lt;em&gt;Generative AI is learning to spy for the US military. &lt;/em&gt;(MIT Technology Review)&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;3 China has urged companies to avoid using Nvidia’s H20 chips&lt;/strong&gt;&lt;br /&gt;Which comes as a blow to the firm after it made a deal with the US government. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;Chinese officials fear that the US could embed “back doors” into them. &lt;/em&gt;(SCMP $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 Elon Musk has threatened legal action against Apple&lt;/strong&gt;&lt;br /&gt;He claims that OpenAI is the only AI firm able to top its App Store charts. (Reuters)&lt;br /&gt;+ &lt;em&gt;Grok is ranked a lowly sixth on its free listings. &lt;/em&gt;(FT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 AI is making sharing photos of children even riskier&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;Nudifying tools are making it easier than ever to manipulate images. (NYT $)&lt;br /&gt;+ &lt;em&gt;You need to talk to your kid about AI. Here are 6 things you should say. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 The future of food hinges on our land use&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;Can factory farming ease the burden? (Vox)&lt;br /&gt;+ &lt;em&gt;Africa fights rising hunger by looking to foods of the past. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;7 What does Palantir really do?&lt;/strong&gt;&lt;br /&gt;Even former workers don’t seem entirely sure. (Wired $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 Interest in AI majors is exploding&lt;br /&gt;&lt;/strong&gt;Among young students and older workers alike. (WP $)&lt;br /&gt;+ &lt;em&gt;We’re reliving a new dot com bubble updated for the AI age. &lt;/em&gt;(New Yorker $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 The in-person job interview is staging a comeback&lt;br /&gt;&lt;/strong&gt;AI has made it too easy to cheat remotely. (WSJ $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 This YouTube show attempts to turn internet discourse into live debate&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;Sounds absolutely terrible. (New Yorker $)&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt; 
 &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“It’s not banned but has kind of become a politically incorrect thing to do.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—An anonymous Chinese data center operator tells the Financial Times why purchasing Nvidia’s H20 chips has become so fraught in China.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcalZdRVHPX2-Bw7gCjuTwCv8vpkPsgRfTKTn9oA6GZ3_iVevVahGtfflNLyRPkpLrReeRLPb0EypnuW7n7ye8C-LWQGi0q9_u5lAyTTlfcQkfvh9yv_zaaJKXXQre0vjiIrP4Vxw?key=zZ0e1PSXqOlV2OsnZSiIxw" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;The search for extraterrestrial life is targeting Jupiter’s icy moon Europa&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Europa, Jupiter’s fourth-largest moon, is nothing like ours. Its surface is a vast saltwater ocean, encased in a blanket of cracked ice, one that seems to occasionally break open and spew watery plumes into the moon’s thin atmosphere.&lt;/p&gt;&lt;p&gt;For these reasons, Europa captivates planetary scientists. All that water and energy—and hints of elements essential for building organic molecules —point to another extraordinary possibility. Jupiter’s big, bright moon could host life.&lt;/p&gt;&lt;p&gt;They may eventually get some answers thanks to Europa Clipper, scheduled to reach Jupiter in 2030. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Stephen Ornes&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;br /&gt;&lt;/em&gt;&lt;br /&gt;+ There’s still plenty of time to decide on a song of the summer.&lt;br /&gt;+ Why we love to love horrendously bad films—particularly The Room.&lt;br /&gt;+ Lock up your daughters: these medieval bards were dangerously charismatic.&lt;br /&gt;+ How to instantly become better at pretty much anything.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Meet the early-adopter judges using AI&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;The propensity for AI systems to make mistakes that humans miss has been on full display in the US legal system as of late. The follies began when lawyers submitted documents citing cases that didn’t exist. Similar mistakes soon spread to other roles in the courts. Last December, a Stanford professor submitted sworn testimony containing hallucinations and errors in a case about deepfakes, despite being an expert on AI and misinformation himself.&lt;/p&gt;&lt;p&gt;Now, judges are experimenting with generative AI too. Some believe that with the right precautions, the technology can expedite legal research, summarize cases, draft routine orders, and overall help speed up the court system, which is badly backlogged in many parts of the US. Are they right to be so confident in it? Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—James O’Donnell&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;What you may have missed about GPT-5&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;OpenAI’s new GPT-5 model was supposed to give a glimpse of AI’s newest frontier. It was meant to mark a leap toward the “artificial general intelligence” that tech’s evangelists have promised will transform humanity for the better.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Against those expectations, the model has mostly underwhelmed. But there’s one other thing to take from all this. Among other suggestions for potential uses of its models, OpenAI has begun explicitly telling people to use them for health advice. It’s a change in approach that signals the company is wading into dangerous waters. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—James O’Donnell&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 The US has extended its China tariff truce by another 90 days&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;It’s yet again another example of Trump’s on-again, off-again policies. (CNBC)&lt;br /&gt;+ &lt;em&gt;China has succeeded in finding other markets to sell to anyway. &lt;/em&gt;(CNN)&lt;br /&gt;+ &lt;em&gt;Now we’ve got to wait until November 10 for the next round of tariffs. &lt;/em&gt;(BBC)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 Europe’s arms factories are rapidly expanding&lt;/strong&gt;&lt;br /&gt;As EU governments debate how to sustain weapons deliveries to Ukraine. (FT $)&lt;br /&gt;+ &lt;em&gt;Trump is due to meet with Vladimir Putin on Friday. &lt;/em&gt;(The Guardian)&lt;br /&gt;+ &lt;em&gt;Generative AI is learning to spy for the US military. &lt;/em&gt;(MIT Technology Review)&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;3 China has urged companies to avoid using Nvidia’s H20 chips&lt;/strong&gt;&lt;br /&gt;Which comes as a blow to the firm after it made a deal with the US government. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;Chinese officials fear that the US could embed “back doors” into them. &lt;/em&gt;(SCMP $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 Elon Musk has threatened legal action against Apple&lt;/strong&gt;&lt;br /&gt;He claims that OpenAI is the only AI firm able to top its App Store charts. (Reuters)&lt;br /&gt;+ &lt;em&gt;Grok is ranked a lowly sixth on its free listings. &lt;/em&gt;(FT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 AI is making sharing photos of children even riskier&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;Nudifying tools are making it easier than ever to manipulate images. (NYT $)&lt;br /&gt;+ &lt;em&gt;You need to talk to your kid about AI. Here are 6 things you should say. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 The future of food hinges on our land use&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;Can factory farming ease the burden? (Vox)&lt;br /&gt;+ &lt;em&gt;Africa fights rising hunger by looking to foods of the past. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;7 What does Palantir really do?&lt;/strong&gt;&lt;br /&gt;Even former workers don’t seem entirely sure. (Wired $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 Interest in AI majors is exploding&lt;br /&gt;&lt;/strong&gt;Among young students and older workers alike. (WP $)&lt;br /&gt;+ &lt;em&gt;We’re reliving a new dot com bubble updated for the AI age. &lt;/em&gt;(New Yorker $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 The in-person job interview is staging a comeback&lt;br /&gt;&lt;/strong&gt;AI has made it too easy to cheat remotely. (WSJ $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 This YouTube show attempts to turn internet discourse into live debate&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;Sounds absolutely terrible. (New Yorker $)&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt; 
 &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“It’s not banned but has kind of become a politically incorrect thing to do.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—An anonymous Chinese data center operator tells the Financial Times why purchasing Nvidia’s H20 chips has become so fraught in China.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcalZdRVHPX2-Bw7gCjuTwCv8vpkPsgRfTKTn9oA6GZ3_iVevVahGtfflNLyRPkpLrReeRLPb0EypnuW7n7ye8C-LWQGi0q9_u5lAyTTlfcQkfvh9yv_zaaJKXXQre0vjiIrP4Vxw?key=zZ0e1PSXqOlV2OsnZSiIxw" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;The search for extraterrestrial life is targeting Jupiter’s icy moon Europa&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Europa, Jupiter’s fourth-largest moon, is nothing like ours. Its surface is a vast saltwater ocean, encased in a blanket of cracked ice, one that seems to occasionally break open and spew watery plumes into the moon’s thin atmosphere.&lt;/p&gt;&lt;p&gt;For these reasons, Europa captivates planetary scientists. All that water and energy—and hints of elements essential for building organic molecules —point to another extraordinary possibility. Jupiter’s big, bright moon could host life.&lt;/p&gt;&lt;p&gt;They may eventually get some answers thanks to Europa Clipper, scheduled to reach Jupiter in 2030. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Stephen Ornes&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;br /&gt;&lt;/em&gt;&lt;br /&gt;+ There’s still plenty of time to decide on a song of the summer.&lt;br /&gt;+ Why we love to love horrendously bad films—particularly The Room.&lt;br /&gt;+ Lock up your daughters: these medieval bards were dangerously charismatic.&lt;br /&gt;+ How to instantly become better at pretty much anything.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/08/12/1121658/the-download-meet-the-judges-using-ai-and-gpt-5s-health-promises/</guid><pubDate>Tue, 12 Aug 2025 12:10:00 +0000</pubDate></item><item><title>[NEW] How a once-tiny research lab helped Nvidia become a $4 trillion-dollar company (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/12/how-a-once-tiny-research-lab-helped-nvidia-become-a-4-trillion-dollar-company/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;When Bill Dally joined Nvidia’s research lab in 2009, it employed only about a dozen people and was focused on ray tracing, a rendering technique used in computer graphics.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That once-small research lab now employs more than 400 people, who have helped transform Nvidia from a video game GPU startup in the nineties to a $4 trillion-dollar company fueling the artificial intelligence boom.&lt;/p&gt;&lt;p&gt;Now, the company’s research lab has its sights set on developing the tech needed to power robotics and AI. And some of that lab work is already showing up in products. The company unveiled Monday a new set world AI models, libraries, and other infrastructure for robotics developers.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Dally, now Nvidia’s chief scientist, started consulting for Nvidia in 2003 while he was working at Stanford. When he was ready to step down from being the department chair of Stanford’s computer science department a few years later, he planned to take a sabbatical. Nvidia had a different idea.&lt;/p&gt;

&lt;figure class="wp-block-image alignleft size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3036045" height="510" src="https://techcrunch.com/wp-content/uploads/2025/08/NEW-Bill-Dally.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Bill Dally / Nvidia&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;David Kirk, who was running the research lab at the time, and Nvidia CEO Jensen Huang, thought a more permanent position at the research lab was a better idea. Dally told TechCrunch the pair put on a “full-court press” on why he should join Nvidia’s research lab and eventually convinced him.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It wound up being kind of a perfect fit for my interests and my talents,” Dally said. “I think everybody’s always searching for the place in life where they can make the biggest, you know, contribution to the world. And I think for me, it’s definitely Nvidia.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When Dally took over the lab in 2009, expansion was first and foremost. Researchers started working on areas outside of ray tracing right away, including circuit design and VLSI, or very large-scale integration, a process that combines millions of transistors on a single chip.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The research lab hasn’t stopped expanding since.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“We try to figure out what will make the most positive difference for the company because we’re constantly seeing exciting new areas, but some of them, you know, they do great work, but we have trouble saying if [we’ll be] wildly successful at this,” Dally said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For a while that was building better GPUs for artificial intelligence. Nvidia was early to the future AI boom and started tinkering with the idea of AI GPUs in 2010 —&amp;nbsp;more than a decade before the current AI frenzy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We said this is amazing, this is gonna completely change the world,” Dally said. “We have to start doubling down on this and Jensen believed that when I told him that. We started specializing our GPUs for it and developing lots of software to support it, engaging with the researchers all around the world who were doing it, long before it was clearly relevant.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-physical-ai-focus"&gt;Physical AI focus&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Now, as Nvidia holds a commanding lead in the AI GPU market, the tech company has started to seek out new areas of demand beyond AI data centers. That search has led Nvidia to physical AI and robotics.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I think eventually robots are going to be a huge player in the world and we want to basically be making the brains of all the robots,” Dally said. “To do that we need to start, you know, developing the key technologies.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s where Sanja Fidler, the vice president of AI research at Nvidia, comes in. Fidler joined Nvidia’s research lab in 2018. At the time, she was already working on simulation models for robots with a team of students at MIT. When she told Huang about what they were working on at a researchers’ reception, he was interested.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I could not resist joining,” Fidler told TechCrunch in an interview. “It’s just such a, you know, it’s just such a great topic fit and at the same time was also such a great culture fit. You know, Jensen told me, come work with me, not with us, not for us, you know?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;She joined Nvidia and got to work creating a research lab in Toronto called Omniverse, an Nvidia platform, that was focused on building simulations for physical AI.&lt;/p&gt;

&lt;figure class="wp-block-image alignright size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3036047" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/SanjaFidler_1-2.jpg?w=544" width="544" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Sanja Fidler / Nvidia&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The first challenge to building these simulated worlds was finding the necessary 3D data, Fidler said. This included finding the proper volume of potential images to use and building the technology needed to turn these images into 3D renditions the simulators could use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We invested in this technology called differentiable rendering, which essentially makes rendering amendable to AI, right?” Fidler said. “You go [from] rendering means from 3D to image or video, right? And we want it to go the other way.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-world-models"&gt;World models&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Omniverse released the first version of its model that turns images into 3D models, GANverse3D, in 2021. Then it got to work on figuring out the same process for video. Fidler said they used videos from robots and self-driving cars to create these 3D models and simulations through its Neuric Neural Reconstruction Engine, which the company first announced in 2022.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;She added these technologies were the backbone of the company’s Cosmos family of world AI models that were announced at CES in January.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Now, the lab is focused on making these models faster. When you play a video game or simulation you want the tech to be able to respond in real time, Fidler said, for robots they are working to make the reaction time even faster.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The robot doesn’t need to watch the world in the same time, in the same way as the world works,” Fidler said. “It can watch it like 100x faster. So if we can make this model significantly faster than they are today, they’re going to be tremendously useful for robotic or physical AI applications.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company continues to make progress on this goal. Nvidia announced a fleet of new world AI models designed for creating synthetic data that can be used to train robots at the SIGGRAPH computer graphics conference on Monday. Nvidia also announced new libraries and infrastructure software aimed at robotics developers too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite the progress —&amp;nbsp;and the current hype about robots, especially humanoids —&amp;nbsp;the Nvidia research team remains realistic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both Dally and Fidler said the industry is still at least a few years off from having a humanoid in your home, with Fidler comparing it to the hype and timeline regarding autonomous vehicles.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re making huge progress and I think you know AI has really been the enabler here,” Dally said. “Starting with visual AI for the robot perception, and then you know generative AI, that’s being hugely valuable for task and motion planning and manipulation. As we solve each of these individual little problems and as the amount of data we have to train our networks grows, these robots are going to grow.”&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;When Bill Dally joined Nvidia’s research lab in 2009, it employed only about a dozen people and was focused on ray tracing, a rendering technique used in computer graphics.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That once-small research lab now employs more than 400 people, who have helped transform Nvidia from a video game GPU startup in the nineties to a $4 trillion-dollar company fueling the artificial intelligence boom.&lt;/p&gt;&lt;p&gt;Now, the company’s research lab has its sights set on developing the tech needed to power robotics and AI. And some of that lab work is already showing up in products. The company unveiled Monday a new set world AI models, libraries, and other infrastructure for robotics developers.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Dally, now Nvidia’s chief scientist, started consulting for Nvidia in 2003 while he was working at Stanford. When he was ready to step down from being the department chair of Stanford’s computer science department a few years later, he planned to take a sabbatical. Nvidia had a different idea.&lt;/p&gt;

&lt;figure class="wp-block-image alignleft size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3036045" height="510" src="https://techcrunch.com/wp-content/uploads/2025/08/NEW-Bill-Dally.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Bill Dally / Nvidia&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;David Kirk, who was running the research lab at the time, and Nvidia CEO Jensen Huang, thought a more permanent position at the research lab was a better idea. Dally told TechCrunch the pair put on a “full-court press” on why he should join Nvidia’s research lab and eventually convinced him.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It wound up being kind of a perfect fit for my interests and my talents,” Dally said. “I think everybody’s always searching for the place in life where they can make the biggest, you know, contribution to the world. And I think for me, it’s definitely Nvidia.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When Dally took over the lab in 2009, expansion was first and foremost. Researchers started working on areas outside of ray tracing right away, including circuit design and VLSI, or very large-scale integration, a process that combines millions of transistors on a single chip.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The research lab hasn’t stopped expanding since.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“We try to figure out what will make the most positive difference for the company because we’re constantly seeing exciting new areas, but some of them, you know, they do great work, but we have trouble saying if [we’ll be] wildly successful at this,” Dally said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For a while that was building better GPUs for artificial intelligence. Nvidia was early to the future AI boom and started tinkering with the idea of AI GPUs in 2010 —&amp;nbsp;more than a decade before the current AI frenzy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We said this is amazing, this is gonna completely change the world,” Dally said. “We have to start doubling down on this and Jensen believed that when I told him that. We started specializing our GPUs for it and developing lots of software to support it, engaging with the researchers all around the world who were doing it, long before it was clearly relevant.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-physical-ai-focus"&gt;Physical AI focus&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Now, as Nvidia holds a commanding lead in the AI GPU market, the tech company has started to seek out new areas of demand beyond AI data centers. That search has led Nvidia to physical AI and robotics.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I think eventually robots are going to be a huge player in the world and we want to basically be making the brains of all the robots,” Dally said. “To do that we need to start, you know, developing the key technologies.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s where Sanja Fidler, the vice president of AI research at Nvidia, comes in. Fidler joined Nvidia’s research lab in 2018. At the time, she was already working on simulation models for robots with a team of students at MIT. When she told Huang about what they were working on at a researchers’ reception, he was interested.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I could not resist joining,” Fidler told TechCrunch in an interview. “It’s just such a, you know, it’s just such a great topic fit and at the same time was also such a great culture fit. You know, Jensen told me, come work with me, not with us, not for us, you know?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;She joined Nvidia and got to work creating a research lab in Toronto called Omniverse, an Nvidia platform, that was focused on building simulations for physical AI.&lt;/p&gt;

&lt;figure class="wp-block-image alignright size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3036047" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/SanjaFidler_1-2.jpg?w=544" width="544" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Sanja Fidler / Nvidia&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The first challenge to building these simulated worlds was finding the necessary 3D data, Fidler said. This included finding the proper volume of potential images to use and building the technology needed to turn these images into 3D renditions the simulators could use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We invested in this technology called differentiable rendering, which essentially makes rendering amendable to AI, right?” Fidler said. “You go [from] rendering means from 3D to image or video, right? And we want it to go the other way.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-world-models"&gt;World models&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Omniverse released the first version of its model that turns images into 3D models, GANverse3D, in 2021. Then it got to work on figuring out the same process for video. Fidler said they used videos from robots and self-driving cars to create these 3D models and simulations through its Neuric Neural Reconstruction Engine, which the company first announced in 2022.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;She added these technologies were the backbone of the company’s Cosmos family of world AI models that were announced at CES in January.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Now, the lab is focused on making these models faster. When you play a video game or simulation you want the tech to be able to respond in real time, Fidler said, for robots they are working to make the reaction time even faster.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The robot doesn’t need to watch the world in the same time, in the same way as the world works,” Fidler said. “It can watch it like 100x faster. So if we can make this model significantly faster than they are today, they’re going to be tremendously useful for robotic or physical AI applications.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company continues to make progress on this goal. Nvidia announced a fleet of new world AI models designed for creating synthetic data that can be used to train robots at the SIGGRAPH computer graphics conference on Monday. Nvidia also announced new libraries and infrastructure software aimed at robotics developers too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite the progress —&amp;nbsp;and the current hype about robots, especially humanoids —&amp;nbsp;the Nvidia research team remains realistic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both Dally and Fidler said the industry is still at least a few years off from having a humanoid in your home, with Fidler comparing it to the hype and timeline regarding autonomous vehicles.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re making huge progress and I think you know AI has really been the enabler here,” Dally said. “Starting with visual AI for the robot perception, and then you know generative AI, that’s being hugely valuable for task and motion planning and manipulation. As we solve each of these individual little problems and as the amount of data we have to train our networks grows, these robots are going to grow.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/12/how-a-once-tiny-research-lab-helped-nvidia-become-a-4-trillion-dollar-company/</guid><pubDate>Tue, 12 Aug 2025 12:30:00 +0000</pubDate></item></channel></rss>