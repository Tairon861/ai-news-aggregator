<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 25 Sep 2025 18:30:37 +0000</lastBuildDate><item><title> ()</title><link>https://venturebeat.com/category/ai/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://venturebeat.com/category/ai/feed/</guid></item><item><title>How AI and Wikipedia have sent vulnerable languages into a doom spiral (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/09/25/1124005/ai-wikipedia-vulnerable-languages-doom-spiral/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/0924f.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;When Kenneth Wehr started managing the Greenlandic-language version of Wikipedia four years ago, his first act was to delete almost everything. It had to go, he thought, if it had any chance of surviving.&lt;/p&gt;  &lt;p&gt;Wehr, who’s 26, isn’t from Greenland—he grew up in Germany—but he had become obsessed with the island, an autonomous Danish territory, after visiting as a teenager. He’d spent years writing obscure Wikipedia articles in his native tongue on virtually everything to do with it. He even ended up moving to Copenhagen to study Greenlandic, a language spoken by some 57,000 mostly Indigenous Inuit people scattered across dozens of far-flung Arctic villages.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"&gt; &lt;p&gt;The Greenlandic-language edition was added to Wikipedia around 2003, just a few years after the site launched in English. By the time Wehr took its helm nearly 20 years later, hundreds of Wikipedians had contributed to it and had collectively written some 1,500 articles totaling over tens of thousands of words. It seemed to be an impressive vindication of the crowdsourcing approach that has made Wikipedia the go-to source for information online, demonstrating that it could work even in the unlikeliest places.&amp;nbsp;&lt;/p&gt;    &lt;p&gt;There was only one problem: The Greenlandic Wikipedia was a mirage.&amp;nbsp;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Virtually every single article had been published by people who did not actually speak the language. Wehr, who now teaches Greenlandic in Denmark, speculates that perhaps only one or two Greenlanders had ever contributed. But what worried him most was something else: Over time, he had noticed that a growing number of articles appeared to be copy-pasted into Wikipedia by people using machine translators. They were riddled with elementary mistakes—from grammatical blunders to meaningless words to more significant inaccuracies, like an entry that claimed Canada had only 41 inhabitants. Other pages sometimes contained random strings of letters spat out by machines that were unable to find suitable Greenlandic words to express themselves.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“It might have looked Greenlandic to [the authors], but they had no way of knowing,” complains Wehr. &lt;/p&gt; 
 &lt;p&gt;“Sentences wouldn’t make sense at all, or they would have obvious errors,” he adds. “AI translators are really bad at Greenlandic.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;What Wehr describes is not unique to the Greenlandic edition.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Wikipedia is the most ambitious multilingual project after the Bible: There are editions in over 340 languages, and a further 400 even more obscure ones are being developed and tested. Many of these smaller editions have been swamped with automatically translated content as AI has become increasingly accessible. Volunteers working on four African languages, for instance, estimated to &lt;em&gt;MIT Technology Review &lt;/em&gt;that between 40% and 60% of articles in their Wikipedia editions were uncorrected machine translations. And after auditing the Wikipedia edition in Inuktitut, an Indigenous language close to Greenlandic that’s spoken in Canada, &lt;em&gt;MIT Technology Review &lt;/em&gt;estimates that more than two-thirds of pages containing more than several sentences feature portions created this way.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This is beginning to cause a wicked problem. AI systems, from Google Translate to ChatGPT, learn to “speak” new languages by scraping huge quantities of text from the internet. Wikipedia is sometimes the largest source of online linguistic data for languages with few speakers—so any errors on those pages, grammatical or otherwise, can poison the wells that AI is expected to draw from. That can make the models’ translation of these languages particularly error-prone, which creates a sort of linguistic doom loop as people continue to add more and more poorly translated Wikipedia pages using those tools, and AI models continue to train from poorly translated pages. It’s a complicated problem, but it boils down to a simple concept: Garbage in, garbage out.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“These models are built on raw data,” says Kevin Scannell, a former professor of computer science at Saint Louis University who now builds computer software tailored for endangered languages. “They will try and learn everything about a language from scratch. There is no other input. There are no grammar books. There are no dictionaries. There is nothing other than the text that is inputted.”&lt;/p&gt;  &lt;p&gt;There isn’t perfect data on the scale of this problem, particularly because a lot of AI training data is kept confidential and the field continues to evolve rapidly. But back in 2020, Wikipedia was estimated to make up more than half the training data that was fed into AI models translating some languages spoken by millions across Africa, including Malagasy, Yoruba, and Shona. In 2022, a research team from Germany that looked into what data could be obtained by online scraping even found that Wikipedia was the sole easily accessible source of online linguistic data for 27 under-resourced languages.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;This could have significant repercussions in cases where Wikipedia is poorly written—potentially pushing the most vulnerable languages on Earth toward the precipice as future generations begin to turn away from them.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“Wikipedia will be reflected in the AI models for these languages,” says Trond Trosterud, a computational linguist at the University of Tromsø in Norway, who has been raising the alarm about the potentially harmful outcomes of badly run Wikipedia editions for years. “I find it hard to imagine it will not have consequences. And, of course, the more dominant position that Wikipedia has, the worse it will be.”&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Use responsibly&lt;/h3&gt;  &lt;p&gt;Automation has been built into Wikipedia since the very earliest days. Bots keep the platform operational: They repair broken links, fix bad formatting, and even correct spelling mistakes. These repetitive and mundane tasks can be automated away with little problem. There is even an army of bots that scurry around generating short articles about rivers, cities, or animals by slotting their names into formulaic phrases. They have generally made the platform better.&amp;nbsp;&lt;/p&gt;  &lt;div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"&gt; &lt;p&gt;But AI is different. Anybody can use it to cause massive damage with a few clicks.&amp;nbsp;&lt;/p&gt;    &lt;p&gt;Wikipedia has managed the onset of the AI era better than many other websites. It has not been flooded with AI bots or disinformation, as social media has been. It largely retains the innocence that characterized the earlier internet age. Wikipedia is open and free for anyone to use, edit, and pull from, and it’s run by the very same community it serves. It is transparent and easy to use. But community-run platforms live and die on the size of their communities. English has triumphed, while Greenlandic has sunk.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;“We need good Wikipedians. This is something that people take for granted. It is not magic,” says Amir Aharoni, a member of the volunteer Language Committee, which oversees requests to open or close Wikipedia editions. “If you use machine translation responsibly, it can be efficient and useful. Unfortunately, you cannot trust all people to use it responsibly.”&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;Trosterud has studied the behavior of users on small Wikipedia editions and says AI has empowered a subset that he terms “Wikipedia hijackers.” These users can range widely—from naive teenagers creating pages about their hometowns or their favorite YouTubers to well-meaning Wikipedians who think that by creating articles in minority languages they are in some way “helping” those communities.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“The problem with them nowadays is that they are armed with Google Translate,” Trosterud says, adding that this is allowing them to produce much longer and more plausible-looking content than they ever could before: “Earlier they were armed only with dictionaries.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This has effectively industrialized the acts of destruction—which affect vulnerable languages most, since AI translations are typically far less reliable for them. There can be lots of different reasons for this, but a meaningful part of the issue is the relatively small amount of source text that is available online. And sometimes models struggle to identify a language because it is similar to others, or because some, including Greenlandic and most Native American languages, have structures that make them badly suited to the way most machine translation systems work. (Wehr notes that in Greenlandic most words are agglutinative, meaning they are built by attaching prefixes and suffixes to stems. As a result, many words are extremely context specific and can express ideas that in other languages would take a full sentence.)&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Research produced by Google before a major expansion of Google Translate rolled out three years ago found that translation systems for lower-resourced languages were generally of a lower quality than those for better-resourced ones. Researchers found, for example, that their model would often mistranslate basic nouns across languages, including the names of animals and colors. (In a statement to &lt;em&gt;MIT&lt;/em&gt; &lt;em&gt;Technology Review&lt;/em&gt;, Google wrote that it is “committed to meeting a high standard of quality for all 249 languages” it supports “by rigorously testing and improving [its] systems, particularly for languages that may have limited public text resources on the web.”)&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;Wikipedia itself offers a built-in editing tool called Content Translate, which allows users to automatically translate articles from one language to another—the idea being that this will save time by preserving the references and fiddly formatting of the originals. But it piggybacks on external machine translation systems, so it’s largely plagued by the same weaknesses as other machine translators—a problem that the Wikimedia Foundation says is hard to solve. It’s up to each edition’s community to decide whether this tool is allowed, and some have decided against it. (Notably, English-language Wikipedia has largely banned its use, claiming that some 95% of articles created using Content Translate failed to meet an acceptable standard without significant additional work.) But it’s at least easy to tell when the program has been used; Content Translate adds a tag on the Wikipedia back end.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Other AI programs can be harder to monitor. Still, many Wikipedia editors I spoke with said that once their languages were added to major online translation tools, they noticed a corresponding spike in the frequency with which poor, likely machine-translated pages were created.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Some Wikipedians using AI to translate content do occasionally admit that they do not speak the target languages. They may see themselves as providing smaller communities with rough-cut articles that speakers can then fix—essentially following the same model that has worked well for more active Wikipedia editions.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Google Translate, for instance, says the Fulfulde word for January means June, while ChatGPT says it’s August or September. The programs also suggest the Fulfulde word for “harvest” means “fever” or “well-being,” among other possibilities.&amp;nbsp;&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt; &lt;/blockquote&gt;  &lt;p&gt;But once error-filled pages are produced in small languages, there is usually not an army of knowledgeable people who speak those languages standing ready to improve them. There are few readers of these editions, and sometimes not a single regular editor.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Yuet Man Lee, a Canadian teacher in his 20s, says that he used a mix of Google Translate and ChatGPT to translate a handful of articles that he had written for the English Wikipedia into Inuktitut, thinking it’d be nice to pitch in and help a smaller Wikipedia community. He says he added a note to one saying that it was only a rough translation. “I did not think that anybody would notice [the article],” he explains. “If you put something out there on the smaller Wikipedias—most of the time nobody does.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But at the same time, he says, he still thought “someone might see it and fix it up”—adding that he had wondered whether the Inuktitut translation that the AI systems generated was grammatically correct. Nobody has touched the article since he created it.&lt;/p&gt; 
 &lt;p&gt;Lee, who teaches social sciences in Vancouver and first started editing entries in the English Wikipedia a decade ago, says that users familiar with more active Wikipedias can fall victim to this mindset, which he terms a “bigger-Wikipedia arrogance”: When they try to contribute to smaller Wikipedia editions, they assume that others will come along to fix their mistakes. It can sometimes work. Lee says he had previously contributed several articles to Wikipedia in Tatar, a language spoken by several million people mainly in Russia, and at least one of those was eventually corrected. But the Inuktitut Wikipedia is, by comparison, a “barren wasteland.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;He emphasizes that his intentions had been good: He wanted to add more articles to an Indigenous Canadian Wikipedia. “I am now thinking that it may have been a bad idea. I did not consider that I could be contributing to a recursive loop,” he says. “It was about trying to get content out there, out of curiosity and for fun, without properly thinking about the consequences.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;h3 class="wp-block-heading"&gt;&amp;nbsp;“Totally, completely no future”&lt;/h3&gt;  &lt;p&gt;Wikipedia is a project that is driven by wide-eyed optimism. Editing can be a thankless task, involving weeks spent bickering with faceless, pseudonymous people, but devotees put in hours of unpaid labor because of a commitment to a higher cause. It is this commitment that drives many of the regular small-language editors I spoke with. They all feared what would happen if garbage continued to appear on their pages.&lt;/p&gt;  &lt;p&gt;Abdulkadir Abdulkadir, a 26-year-old agricultural planner who spoke with me over a crackling phone call from a busy roadside in northern Nigeria, said that he spends three hours every day fiddling with entries in his native Fulfulde, a language used mainly by pastoralists and farmers across the Sahel. “But the work is too much,” he said.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Abdulkadir sees an urgent need for the Fulfulde Wikipedia to work properly. He has been suggesting it as one of the few online resources for farmers in remote villages, potentially offering information on which seeds or crops might work best for their fields in a language they can understand. If you give them a machine-translated article, Abdulkadir told me, then it could “easily harm them,” as the information will probably not be translated correctly into Fulfulde.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Google Translate, for instance, says the Fulfulde word for January means June, while ChatGPT says it’s August or September. The programs also suggest the Fulfulde word for “harvest” means “fever” or “well-being,” among other possibilities.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Abdulkadir said he had recently been forced to correct an article about cowpeas, a foundational cash crop across much of Africa, after discovering that it was largely illegible.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;If someone wants to create pages on the Fulfulde Wikipedia, Abdulkadir said, they should be translated manually. Otherwise, “whoever will read your articles will [not] be able to get even basic knowledge,” he tells these Wikipedians. Nevertheless, he estimates that some 60% of articles are still uncorrected machine translations. Abdulkadir told me that unless something important changes with how AI systems learn and are deployed, then the outlook for Fulfulde looks bleak. “It is going to be terrible, honestly,” he said. “Totally, completely no future.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Across the country from Abdulkadir, Lucy Iwuala contributes to Wikipedia in Igbo, a language spoken by several million people in southeastern Nigeria. “The harm has already been done,” she told me, opening the two most recently created articles. Both had been automatically translated via Wikipedia’s Content Translate and contained so many mistakes that she said it would have given her a headache to continue reading them. “There are some terms that have not even been translated. They are still in English,” she pointed out. She recognized the username that had created the pages as a serial offender. “This one even includes letters that are not used in the Igbo language,” she said.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Iwuala began regularly contributing to Wikipedia three years ago out of concern that Igbo was being displaced by English. It is a worry that is common to many who are active on smaller Wikipedia editions. “This is my culture. This is who I am,” she told me. “That is the essence of it all: to ensure that you are not erased.”&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;Iwuala, who now works as a professional translator between English and Igbo, said the users doing the most damage are inexperienced and see AI translations as a way to quickly increase the profile of the Igbo Wikipedia. She often finds herself having to explain at online edit-a-thons she organizes, or over email to various error-prone editors, that the results can be the exact opposite, pushing users away: “You will be discouraged and you will no longer want to visit this place. You will just abandon it and go back to the English Wikipedia.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;These fears are echoed by Noah Ha‘alilio Solomon, an assistant professor of Hawaiian language at the University of Hawai‘i. He reports that some 35% of words on some pages in the Hawaiian Wikipedia are incomprehensible. “If this is the Hawaiian that is going to exist online, then it will do more harm than anything else,” he says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Hawaiian, which was teetering on the verge of extinction several decades ago, has been undergoing a recovery effort led by Indigenous activists and academics. Seeing such poor Hawaiian on such a widely used platform as Wikipedia is upsetting to Ha‘alilio Solomon.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“It is painful, because it reminds us of all the times that our culture and language has been appropriated,” he says. “We have been fighting tooth and nail in an uphill climb for language revitalization. There is nothing easy about that, and this can add extra impediments. People are going to think that this is an accurate representation of the Hawaiian language.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The consequences of all these Wikipedia errors can quickly become clear. AI translators that have undoubtedly ingested these pages in their training data are now assisting in the production, for instance, of error-strewn AI-generated books aimed at learners of languages as diverse as Inuktitut and Cree, Indigenous languages spoken in Canada, and Manx, a small Celtic language spoken on the Isle of Man. Many of these have been popping up for sale on Amazon. “It was just complete nonsense,” says Richard Compton, a linguist at the University of Quebec in Montreal, of a volume he reviewed that had purported to be an introductory phrasebook for Inuktitut.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Rather than making minority languages more accessible, AI is now creating an ever expanding minefield for students and speakers of those languages to navigate. “It is a slap in the face,” Compton says. He worries that younger generations in Canada, hoping to learn languages in communities that have fought uphill battles against discrimination to pass on their heritage, might turn to online tools such as ChatGPT or phrasebooks on Amazon and simply make matters worse. “It is fraud,” he says.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;A race against time&lt;/h3&gt;  &lt;p&gt;According to UNESCO, a language is declared extinct every two weeks. But whether the Wikimedia Foundation, which runs Wikipedia, has an obligation to the languages used on its platform is an open question. When I spoke to Runa Bhattacharjee, a senior director at the foundation, she said that it was up to the individual communities to make decisions about what content they wanted to exist on their Wikipedia. “Ultimately, the responsibility really lies with the community to see that there is no vandalism or unwanted activity, whether through machine translation or other means,” she said. Usually, Bhattacharjee added, editions were considered for closure only if a specific complaint was raised about them.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But if there is no active community, how can an edition be fixed or even have a complaint raised?&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;Bhattacharjee explained that the Wikimedia Foundation sees its role in such cases as about maintaining the Wikipedia platform in case someone comes along to revive it: “It is the space that we provide for them to grow and develop. That is where we are at.”&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Inari Saami, spoken in a single remote community in northern Finland, is a poster child for how people can take good advantage of Wikipedia. The language was headed toward extinction four decades ago; there were only four children who spoke it. Their parents created the Inari Saami Language Association in a last-ditch bid to keep it going. The efforts worked. There are now several hundred speakers, schools that use Inari Saami as a medium of instruction, and 6,400 Wikipedia articles in the language, each one copy-edited by a fluent speaker.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This success highlights how Wikipedia can indeed provide small and determined communities with a unique vehicle to promote their languages’ preservation. “We don’t care about quantity. We care about quality,” says Fabrizio Brecciaroli, a member of the Inari Saami Language Association. “We are planning to use Wikipedia as a repository for the written language. We need to provide tools that can be used by the younger generations. It is important for them to be able to use Inari Saami digitally.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt; &lt;p&gt;This has been such a success that Wikipedia has been integrated into the curriculum at the Inari Saami–speaking schools, Brecciaroli adds. He fields phone calls from teachers asking him to write up simple pages on topics from tornadoes to Saami folklore. Wikipedia has even offered a way to introduce words into Inari Saami. “We have to make up new words all the time,” Brecciaroli says. “Young people need them to speak about sports, politics, and video games. If they are unsure how to say something, they now check Wikipedia.”&lt;/p&gt;  &lt;p&gt;Wikipedia is a monumental intellectual experiment. What’s happening with Inari Saami suggests that with maximum care, it can work in smaller languages. “The ultimate goal is to make sure that Inari Saami survives,” Brecciaroli says. “It might be a good thing that there isn’t a Google Translate in Inari Saami.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That may be true—though large language models like ChatGPT can be made to translate phrases into languages that more traditional machine translation tools do not offer. Brecciaroli told me that ChatGPT isn’t great in Inari Saami but that the quality varies significantly depending on what you ask it to do; if you ask it a question in the language, then the answer will be filled with words from Finnish and even words it invents. But if you ask it something in English, Finnish, or Italian and then ask it to reply in Inari Saami, it will perform better.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In light of all this, creating as much high-quality content online as can possibly be written becomes a race against time. “ChatGPT only needs a lot of words,” Brecciaroli says. “If we keep putting good material in, then sooner or later, we will get something out. That is the hope.” This is an idea supported by multiple linguists I spoke with—that it may be possible to end the “garbage in, garbage out” cycle. (OpenAI, which operates ChatGPT, did not respond to a request for comment.)&lt;/p&gt;  &lt;p&gt;Still, the overall problem is likely to grow and grow, since many languages are not as lucky as Inari Saami—and their AI translators will most likely be trained on more and more AI slop. Wehr, unfortunately, seems far less optimistic about the future of his beloved Greenlandic.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_18"&gt;&lt;p&gt;Since deleting much of the Greenlandic-language Wikipedia, he has spent years trying to recruit speakers to help him revive it. He has appeared in Greenlandic media and made social media appeals. But he hasn’t gotten much of a response; he says it has been demoralizing.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“There is nobody in Greenland who is interested in this, or who wants to contribute,” he says. “There is completely no point in it, and that is why it should be closed.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Late last year, he began a process requesting that the Wikipedia Language Committee shut down the Greenlandic-language edition. Months of bitter debate followed between dozens of Wikipedia bureaucrats; some seemed to be surprised that a superficially healthy-seeming edition could be gripped by so many problems.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Then, earlier this month, Wehr’s proposal was accepted: Greenlandic Wikipedia is set to be shuttered, and any articles that remain will be moved into the Wikipedia Incubator, where new language editions are tested and built. Among the reasons cited by the Language Committee is the use of AI tools, which have “frequently produced nonsense that could misrepresent the language.”&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Nevertheless, it may be too late—mistakes in Greenlandic already seem to have become embedded in machine translators. If you prompt either Google Translate or ChatGPT to do something as simple as count to 10 in proper Greenlandic, neither program can deliver.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Jacob Judah is an investigative journalist based in London.&amp;nbsp;&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/0924f.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;When Kenneth Wehr started managing the Greenlandic-language version of Wikipedia four years ago, his first act was to delete almost everything. It had to go, he thought, if it had any chance of surviving.&lt;/p&gt;  &lt;p&gt;Wehr, who’s 26, isn’t from Greenland—he grew up in Germany—but he had become obsessed with the island, an autonomous Danish territory, after visiting as a teenager. He’d spent years writing obscure Wikipedia articles in his native tongue on virtually everything to do with it. He even ended up moving to Copenhagen to study Greenlandic, a language spoken by some 57,000 mostly Indigenous Inuit people scattered across dozens of far-flung Arctic villages.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"&gt; &lt;p&gt;The Greenlandic-language edition was added to Wikipedia around 2003, just a few years after the site launched in English. By the time Wehr took its helm nearly 20 years later, hundreds of Wikipedians had contributed to it and had collectively written some 1,500 articles totaling over tens of thousands of words. It seemed to be an impressive vindication of the crowdsourcing approach that has made Wikipedia the go-to source for information online, demonstrating that it could work even in the unlikeliest places.&amp;nbsp;&lt;/p&gt;    &lt;p&gt;There was only one problem: The Greenlandic Wikipedia was a mirage.&amp;nbsp;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Virtually every single article had been published by people who did not actually speak the language. Wehr, who now teaches Greenlandic in Denmark, speculates that perhaps only one or two Greenlanders had ever contributed. But what worried him most was something else: Over time, he had noticed that a growing number of articles appeared to be copy-pasted into Wikipedia by people using machine translators. They were riddled with elementary mistakes—from grammatical blunders to meaningless words to more significant inaccuracies, like an entry that claimed Canada had only 41 inhabitants. Other pages sometimes contained random strings of letters spat out by machines that were unable to find suitable Greenlandic words to express themselves.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“It might have looked Greenlandic to [the authors], but they had no way of knowing,” complains Wehr. &lt;/p&gt; 
 &lt;p&gt;“Sentences wouldn’t make sense at all, or they would have obvious errors,” he adds. “AI translators are really bad at Greenlandic.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;What Wehr describes is not unique to the Greenlandic edition.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Wikipedia is the most ambitious multilingual project after the Bible: There are editions in over 340 languages, and a further 400 even more obscure ones are being developed and tested. Many of these smaller editions have been swamped with automatically translated content as AI has become increasingly accessible. Volunteers working on four African languages, for instance, estimated to &lt;em&gt;MIT Technology Review &lt;/em&gt;that between 40% and 60% of articles in their Wikipedia editions were uncorrected machine translations. And after auditing the Wikipedia edition in Inuktitut, an Indigenous language close to Greenlandic that’s spoken in Canada, &lt;em&gt;MIT Technology Review &lt;/em&gt;estimates that more than two-thirds of pages containing more than several sentences feature portions created this way.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This is beginning to cause a wicked problem. AI systems, from Google Translate to ChatGPT, learn to “speak” new languages by scraping huge quantities of text from the internet. Wikipedia is sometimes the largest source of online linguistic data for languages with few speakers—so any errors on those pages, grammatical or otherwise, can poison the wells that AI is expected to draw from. That can make the models’ translation of these languages particularly error-prone, which creates a sort of linguistic doom loop as people continue to add more and more poorly translated Wikipedia pages using those tools, and AI models continue to train from poorly translated pages. It’s a complicated problem, but it boils down to a simple concept: Garbage in, garbage out.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“These models are built on raw data,” says Kevin Scannell, a former professor of computer science at Saint Louis University who now builds computer software tailored for endangered languages. “They will try and learn everything about a language from scratch. There is no other input. There are no grammar books. There are no dictionaries. There is nothing other than the text that is inputted.”&lt;/p&gt;  &lt;p&gt;There isn’t perfect data on the scale of this problem, particularly because a lot of AI training data is kept confidential and the field continues to evolve rapidly. But back in 2020, Wikipedia was estimated to make up more than half the training data that was fed into AI models translating some languages spoken by millions across Africa, including Malagasy, Yoruba, and Shona. In 2022, a research team from Germany that looked into what data could be obtained by online scraping even found that Wikipedia was the sole easily accessible source of online linguistic data for 27 under-resourced languages.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;This could have significant repercussions in cases where Wikipedia is poorly written—potentially pushing the most vulnerable languages on Earth toward the precipice as future generations begin to turn away from them.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“Wikipedia will be reflected in the AI models for these languages,” says Trond Trosterud, a computational linguist at the University of Tromsø in Norway, who has been raising the alarm about the potentially harmful outcomes of badly run Wikipedia editions for years. “I find it hard to imagine it will not have consequences. And, of course, the more dominant position that Wikipedia has, the worse it will be.”&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Use responsibly&lt;/h3&gt;  &lt;p&gt;Automation has been built into Wikipedia since the very earliest days. Bots keep the platform operational: They repair broken links, fix bad formatting, and even correct spelling mistakes. These repetitive and mundane tasks can be automated away with little problem. There is even an army of bots that scurry around generating short articles about rivers, cities, or animals by slotting their names into formulaic phrases. They have generally made the platform better.&amp;nbsp;&lt;/p&gt;  &lt;div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"&gt; &lt;p&gt;But AI is different. Anybody can use it to cause massive damage with a few clicks.&amp;nbsp;&lt;/p&gt;    &lt;p&gt;Wikipedia has managed the onset of the AI era better than many other websites. It has not been flooded with AI bots or disinformation, as social media has been. It largely retains the innocence that characterized the earlier internet age. Wikipedia is open and free for anyone to use, edit, and pull from, and it’s run by the very same community it serves. It is transparent and easy to use. But community-run platforms live and die on the size of their communities. English has triumphed, while Greenlandic has sunk.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;“We need good Wikipedians. This is something that people take for granted. It is not magic,” says Amir Aharoni, a member of the volunteer Language Committee, which oversees requests to open or close Wikipedia editions. “If you use machine translation responsibly, it can be efficient and useful. Unfortunately, you cannot trust all people to use it responsibly.”&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;Trosterud has studied the behavior of users on small Wikipedia editions and says AI has empowered a subset that he terms “Wikipedia hijackers.” These users can range widely—from naive teenagers creating pages about their hometowns or their favorite YouTubers to well-meaning Wikipedians who think that by creating articles in minority languages they are in some way “helping” those communities.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“The problem with them nowadays is that they are armed with Google Translate,” Trosterud says, adding that this is allowing them to produce much longer and more plausible-looking content than they ever could before: “Earlier they were armed only with dictionaries.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This has effectively industrialized the acts of destruction—which affect vulnerable languages most, since AI translations are typically far less reliable for them. There can be lots of different reasons for this, but a meaningful part of the issue is the relatively small amount of source text that is available online. And sometimes models struggle to identify a language because it is similar to others, or because some, including Greenlandic and most Native American languages, have structures that make them badly suited to the way most machine translation systems work. (Wehr notes that in Greenlandic most words are agglutinative, meaning they are built by attaching prefixes and suffixes to stems. As a result, many words are extremely context specific and can express ideas that in other languages would take a full sentence.)&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Research produced by Google before a major expansion of Google Translate rolled out three years ago found that translation systems for lower-resourced languages were generally of a lower quality than those for better-resourced ones. Researchers found, for example, that their model would often mistranslate basic nouns across languages, including the names of animals and colors. (In a statement to &lt;em&gt;MIT&lt;/em&gt; &lt;em&gt;Technology Review&lt;/em&gt;, Google wrote that it is “committed to meeting a high standard of quality for all 249 languages” it supports “by rigorously testing and improving [its] systems, particularly for languages that may have limited public text resources on the web.”)&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;Wikipedia itself offers a built-in editing tool called Content Translate, which allows users to automatically translate articles from one language to another—the idea being that this will save time by preserving the references and fiddly formatting of the originals. But it piggybacks on external machine translation systems, so it’s largely plagued by the same weaknesses as other machine translators—a problem that the Wikimedia Foundation says is hard to solve. It’s up to each edition’s community to decide whether this tool is allowed, and some have decided against it. (Notably, English-language Wikipedia has largely banned its use, claiming that some 95% of articles created using Content Translate failed to meet an acceptable standard without significant additional work.) But it’s at least easy to tell when the program has been used; Content Translate adds a tag on the Wikipedia back end.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Other AI programs can be harder to monitor. Still, many Wikipedia editors I spoke with said that once their languages were added to major online translation tools, they noticed a corresponding spike in the frequency with which poor, likely machine-translated pages were created.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Some Wikipedians using AI to translate content do occasionally admit that they do not speak the target languages. They may see themselves as providing smaller communities with rough-cut articles that speakers can then fix—essentially following the same model that has worked well for more active Wikipedia editions.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Google Translate, for instance, says the Fulfulde word for January means June, while ChatGPT says it’s August or September. The programs also suggest the Fulfulde word for “harvest” means “fever” or “well-being,” among other possibilities.&amp;nbsp;&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt; &lt;/blockquote&gt;  &lt;p&gt;But once error-filled pages are produced in small languages, there is usually not an army of knowledgeable people who speak those languages standing ready to improve them. There are few readers of these editions, and sometimes not a single regular editor.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Yuet Man Lee, a Canadian teacher in his 20s, says that he used a mix of Google Translate and ChatGPT to translate a handful of articles that he had written for the English Wikipedia into Inuktitut, thinking it’d be nice to pitch in and help a smaller Wikipedia community. He says he added a note to one saying that it was only a rough translation. “I did not think that anybody would notice [the article],” he explains. “If you put something out there on the smaller Wikipedias—most of the time nobody does.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But at the same time, he says, he still thought “someone might see it and fix it up”—adding that he had wondered whether the Inuktitut translation that the AI systems generated was grammatically correct. Nobody has touched the article since he created it.&lt;/p&gt; 
 &lt;p&gt;Lee, who teaches social sciences in Vancouver and first started editing entries in the English Wikipedia a decade ago, says that users familiar with more active Wikipedias can fall victim to this mindset, which he terms a “bigger-Wikipedia arrogance”: When they try to contribute to smaller Wikipedia editions, they assume that others will come along to fix their mistakes. It can sometimes work. Lee says he had previously contributed several articles to Wikipedia in Tatar, a language spoken by several million people mainly in Russia, and at least one of those was eventually corrected. But the Inuktitut Wikipedia is, by comparison, a “barren wasteland.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;He emphasizes that his intentions had been good: He wanted to add more articles to an Indigenous Canadian Wikipedia. “I am now thinking that it may have been a bad idea. I did not consider that I could be contributing to a recursive loop,” he says. “It was about trying to get content out there, out of curiosity and for fun, without properly thinking about the consequences.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;h3 class="wp-block-heading"&gt;&amp;nbsp;“Totally, completely no future”&lt;/h3&gt;  &lt;p&gt;Wikipedia is a project that is driven by wide-eyed optimism. Editing can be a thankless task, involving weeks spent bickering with faceless, pseudonymous people, but devotees put in hours of unpaid labor because of a commitment to a higher cause. It is this commitment that drives many of the regular small-language editors I spoke with. They all feared what would happen if garbage continued to appear on their pages.&lt;/p&gt;  &lt;p&gt;Abdulkadir Abdulkadir, a 26-year-old agricultural planner who spoke with me over a crackling phone call from a busy roadside in northern Nigeria, said that he spends three hours every day fiddling with entries in his native Fulfulde, a language used mainly by pastoralists and farmers across the Sahel. “But the work is too much,” he said.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Abdulkadir sees an urgent need for the Fulfulde Wikipedia to work properly. He has been suggesting it as one of the few online resources for farmers in remote villages, potentially offering information on which seeds or crops might work best for their fields in a language they can understand. If you give them a machine-translated article, Abdulkadir told me, then it could “easily harm them,” as the information will probably not be translated correctly into Fulfulde.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Google Translate, for instance, says the Fulfulde word for January means June, while ChatGPT says it’s August or September. The programs also suggest the Fulfulde word for “harvest” means “fever” or “well-being,” among other possibilities.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Abdulkadir said he had recently been forced to correct an article about cowpeas, a foundational cash crop across much of Africa, after discovering that it was largely illegible.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;If someone wants to create pages on the Fulfulde Wikipedia, Abdulkadir said, they should be translated manually. Otherwise, “whoever will read your articles will [not] be able to get even basic knowledge,” he tells these Wikipedians. Nevertheless, he estimates that some 60% of articles are still uncorrected machine translations. Abdulkadir told me that unless something important changes with how AI systems learn and are deployed, then the outlook for Fulfulde looks bleak. “It is going to be terrible, honestly,” he said. “Totally, completely no future.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Across the country from Abdulkadir, Lucy Iwuala contributes to Wikipedia in Igbo, a language spoken by several million people in southeastern Nigeria. “The harm has already been done,” she told me, opening the two most recently created articles. Both had been automatically translated via Wikipedia’s Content Translate and contained so many mistakes that she said it would have given her a headache to continue reading them. “There are some terms that have not even been translated. They are still in English,” she pointed out. She recognized the username that had created the pages as a serial offender. “This one even includes letters that are not used in the Igbo language,” she said.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Iwuala began regularly contributing to Wikipedia three years ago out of concern that Igbo was being displaced by English. It is a worry that is common to many who are active on smaller Wikipedia editions. “This is my culture. This is who I am,” she told me. “That is the essence of it all: to ensure that you are not erased.”&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;Iwuala, who now works as a professional translator between English and Igbo, said the users doing the most damage are inexperienced and see AI translations as a way to quickly increase the profile of the Igbo Wikipedia. She often finds herself having to explain at online edit-a-thons she organizes, or over email to various error-prone editors, that the results can be the exact opposite, pushing users away: “You will be discouraged and you will no longer want to visit this place. You will just abandon it and go back to the English Wikipedia.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;These fears are echoed by Noah Ha‘alilio Solomon, an assistant professor of Hawaiian language at the University of Hawai‘i. He reports that some 35% of words on some pages in the Hawaiian Wikipedia are incomprehensible. “If this is the Hawaiian that is going to exist online, then it will do more harm than anything else,” he says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Hawaiian, which was teetering on the verge of extinction several decades ago, has been undergoing a recovery effort led by Indigenous activists and academics. Seeing such poor Hawaiian on such a widely used platform as Wikipedia is upsetting to Ha‘alilio Solomon.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“It is painful, because it reminds us of all the times that our culture and language has been appropriated,” he says. “We have been fighting tooth and nail in an uphill climb for language revitalization. There is nothing easy about that, and this can add extra impediments. People are going to think that this is an accurate representation of the Hawaiian language.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The consequences of all these Wikipedia errors can quickly become clear. AI translators that have undoubtedly ingested these pages in their training data are now assisting in the production, for instance, of error-strewn AI-generated books aimed at learners of languages as diverse as Inuktitut and Cree, Indigenous languages spoken in Canada, and Manx, a small Celtic language spoken on the Isle of Man. Many of these have been popping up for sale on Amazon. “It was just complete nonsense,” says Richard Compton, a linguist at the University of Quebec in Montreal, of a volume he reviewed that had purported to be an introductory phrasebook for Inuktitut.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Rather than making minority languages more accessible, AI is now creating an ever expanding minefield for students and speakers of those languages to navigate. “It is a slap in the face,” Compton says. He worries that younger generations in Canada, hoping to learn languages in communities that have fought uphill battles against discrimination to pass on their heritage, might turn to online tools such as ChatGPT or phrasebooks on Amazon and simply make matters worse. “It is fraud,” he says.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;A race against time&lt;/h3&gt;  &lt;p&gt;According to UNESCO, a language is declared extinct every two weeks. But whether the Wikimedia Foundation, which runs Wikipedia, has an obligation to the languages used on its platform is an open question. When I spoke to Runa Bhattacharjee, a senior director at the foundation, she said that it was up to the individual communities to make decisions about what content they wanted to exist on their Wikipedia. “Ultimately, the responsibility really lies with the community to see that there is no vandalism or unwanted activity, whether through machine translation or other means,” she said. Usually, Bhattacharjee added, editions were considered for closure only if a specific complaint was raised about them.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But if there is no active community, how can an edition be fixed or even have a complaint raised?&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;Bhattacharjee explained that the Wikimedia Foundation sees its role in such cases as about maintaining the Wikipedia platform in case someone comes along to revive it: “It is the space that we provide for them to grow and develop. That is where we are at.”&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Inari Saami, spoken in a single remote community in northern Finland, is a poster child for how people can take good advantage of Wikipedia. The language was headed toward extinction four decades ago; there were only four children who spoke it. Their parents created the Inari Saami Language Association in a last-ditch bid to keep it going. The efforts worked. There are now several hundred speakers, schools that use Inari Saami as a medium of instruction, and 6,400 Wikipedia articles in the language, each one copy-edited by a fluent speaker.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This success highlights how Wikipedia can indeed provide small and determined communities with a unique vehicle to promote their languages’ preservation. “We don’t care about quantity. We care about quality,” says Fabrizio Brecciaroli, a member of the Inari Saami Language Association. “We are planning to use Wikipedia as a repository for the written language. We need to provide tools that can be used by the younger generations. It is important for them to be able to use Inari Saami digitally.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt; &lt;p&gt;This has been such a success that Wikipedia has been integrated into the curriculum at the Inari Saami–speaking schools, Brecciaroli adds. He fields phone calls from teachers asking him to write up simple pages on topics from tornadoes to Saami folklore. Wikipedia has even offered a way to introduce words into Inari Saami. “We have to make up new words all the time,” Brecciaroli says. “Young people need them to speak about sports, politics, and video games. If they are unsure how to say something, they now check Wikipedia.”&lt;/p&gt;  &lt;p&gt;Wikipedia is a monumental intellectual experiment. What’s happening with Inari Saami suggests that with maximum care, it can work in smaller languages. “The ultimate goal is to make sure that Inari Saami survives,” Brecciaroli says. “It might be a good thing that there isn’t a Google Translate in Inari Saami.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That may be true—though large language models like ChatGPT can be made to translate phrases into languages that more traditional machine translation tools do not offer. Brecciaroli told me that ChatGPT isn’t great in Inari Saami but that the quality varies significantly depending on what you ask it to do; if you ask it a question in the language, then the answer will be filled with words from Finnish and even words it invents. But if you ask it something in English, Finnish, or Italian and then ask it to reply in Inari Saami, it will perform better.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In light of all this, creating as much high-quality content online as can possibly be written becomes a race against time. “ChatGPT only needs a lot of words,” Brecciaroli says. “If we keep putting good material in, then sooner or later, we will get something out. That is the hope.” This is an idea supported by multiple linguists I spoke with—that it may be possible to end the “garbage in, garbage out” cycle. (OpenAI, which operates ChatGPT, did not respond to a request for comment.)&lt;/p&gt;  &lt;p&gt;Still, the overall problem is likely to grow and grow, since many languages are not as lucky as Inari Saami—and their AI translators will most likely be trained on more and more AI slop. Wehr, unfortunately, seems far less optimistic about the future of his beloved Greenlandic.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_18"&gt;&lt;p&gt;Since deleting much of the Greenlandic-language Wikipedia, he has spent years trying to recruit speakers to help him revive it. He has appeared in Greenlandic media and made social media appeals. But he hasn’t gotten much of a response; he says it has been demoralizing.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“There is nobody in Greenland who is interested in this, or who wants to contribute,” he says. “There is completely no point in it, and that is why it should be closed.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Late last year, he began a process requesting that the Wikipedia Language Committee shut down the Greenlandic-language edition. Months of bitter debate followed between dozens of Wikipedia bureaucrats; some seemed to be surprised that a superficially healthy-seeming edition could be gripped by so many problems.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Then, earlier this month, Wehr’s proposal was accepted: Greenlandic Wikipedia is set to be shuttered, and any articles that remain will be moved into the Wikipedia Incubator, where new language editions are tested and built. Among the reasons cited by the Language Committee is the use of AI tools, which have “frequently produced nonsense that could misrepresent the language.”&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Nevertheless, it may be too late—mistakes in Greenlandic already seem to have become embedded in machine translators. If you prompt either Google Translate or ChatGPT to do something as simple as count to 10 in proper Greenlandic, neither program can deliver.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Jacob Judah is an investigative journalist based in London.&amp;nbsp;&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/09/25/1124005/ai-wikipedia-vulnerable-languages-doom-spiral/</guid><pubDate>Thu, 25 Sep 2025 09:00:00 +0000</pubDate></item><item><title>Inside Huawei’s plan to make thousands of AI chips think like one computer (AI News)</title><link>https://www.artificialintelligence-news.com/news/huawei-ai-chips-superpod-technology/</link><description>&lt;p&gt;Imagine connecting thousands of powerful AI chips scattered in dozens of server cabinets and making them work together as if they were a single, massive computer. That is exactly what Huawei demonstrated at HUAWEI CONNECT 2025, where the company unveiled a breakthrough in AI infrastructure architecture that could reshape how the world builds and scales artificial intelligence systems.&lt;/p&gt;&lt;p&gt;Instead of traditional approaches where individual servers work somewhat independently, Huawei’s new SuperPoD technology creates what the company’s executives describe as a single logical machine made from thousands of separate processing units, allowing them, or it, to “learn, think, and reason as one.”&lt;/p&gt;&lt;p&gt;The implications extend beyond impressive technical specifications, representing a shift in how AI computing power can be organised, scaled, and deployed in industries.&lt;/p&gt;&lt;h3&gt;The technical foundation: UnifiedBus 2.0&lt;/h3&gt;&lt;p&gt;At the core of Huawei’s infrastructure approach is UnifiedBus (UB). Yang Chaobin, Huawei’s Director of the Board and CEO of the ICT Business Group, explained that “Huawei has developed the groundbreaking SuperPoD architecture based on our UnifiedBus interconnect protocol. The architecture deeply interconnects physical servers so that they can learn, think, and reason like a single logical server.”&lt;/p&gt;&lt;p&gt;The technical specifications reveal the scope of this achievement. The UnifiedBus protocol addresses two challenges that, historically, have limited large-scale AI computing: the reliability of long-range communications and bandwidth-latency. Traditional copper connections provide high bandwidth but only over short distances, typically connecting perhaps two cabinets.&lt;/p&gt;&lt;p&gt;Optical cables support longer range but suffer from reliability issues that become more problematic the greater the distance and scale. Eric Xu, Huawei’s Deputy Chairman and Rotating Chairman, said that solving these fundamental connectivity challenges was essential to the company’s AI infrastructure strategy.&lt;/p&gt;&lt;p&gt;Xu detailed the breakthrough solutions in terms of the OSI model: “We have built reliability into every layer of our interconnect protocol, from the physical layer and data link layer, all the way up to the network and transmission layers. There is 100-ns-level fault detection and protection switching on optical paths, making any intermittent disconnections or faults of optical modules imperceptible at the application layer.”&lt;/p&gt;&lt;h3&gt;SuperPoD architecture: Scale and performance&lt;/h3&gt;&lt;p&gt;The Atlas 950 SuperPoD represents the flagship implementation of this architecture, comprising of up to 8,192 Ascend 950DT chips in a configuration that Xu described as delivering “8 EFLOPS in FP8 and 16 EFLOPS in FP4. Its interconnect bandwidth will be 16 PB/s. This means that a single Atlas 950 SuperPoD will have an interconnect bandwidth over 10 times higher than the entire globe’s total peak internet bandwidth.”&lt;/p&gt;&lt;p&gt;The specifications are more than incremental improvements. The Atlas 950 SuperPoD occupies 160 cabinets in 1,000m&lt;sup&gt;2&lt;/sup&gt;, with 128 compute cabinets and 32 comms cabinets linked with all-optical interconnects. The system’s memory capacity reaches 1,152 TB and maintains what Huawei claims is 2.1-microsecond latency in the entire system.&lt;/p&gt;&lt;p&gt;Later in the production pipeline will be the Atlas 960 SuperPoD, which is set to incorporate 15,488 Ascend 960 chips in 220 cabinets covering 2,200m&lt;sup&gt;2&lt;/sup&gt;. Xu said it will deliver “30 EFLOPS in FP8 and 60 EFLOPS in FP4, and come with 4,460 TB of memory and 34 PB/s interconnect bandwidth.”&lt;/p&gt;&lt;h3&gt;Beyond AI: General-purpose computing applications&lt;/h3&gt;&lt;p&gt;The SuperPoD concept extends beyond AI workloads into general-purpose computing through the TaiShan 950 SuperPoD. Built on Kunpeng 950 processors, this system addresses enterprise challenges in replacing legacy mainframes and mid-range computers.&lt;/p&gt;&lt;p&gt;Xu positioned this as particularly relevant for the finance sector, where “the TaiShan 950 SuperPoD, combined with the distributed GaussDB, can serve as an ideal alternative, and replace — once and for all — mainframes, mid-range computers, and Oracle’s Exadata database servers.”&lt;/p&gt;&lt;h3&gt;Open architecture strategy&lt;/h3&gt;&lt;p&gt;Perhaps most significantly for the broader AI infrastructure market, Huawei announced the release of UnifiedBus 2.0 technical specifications as open standards. The decision reflects both strategic positioning and practical constraints.&lt;/p&gt;&lt;p&gt;Xu acknowledged that “the Chinese mainland will lag behind in semiconductor manufacturing process nodes for a relatively long time” and emphasised that “sustainable computing power can only be achieved with process nodes that are practically available.”&lt;/p&gt;&lt;p&gt;Yang framed the open approach as ecosystem building: “We are committed to our open-hardware and open-source-software approach that will help more partners develop their own industry-scenario-based SuperPoD solutions. This will accelerate developer innovation and foster a thriving ecosystem.”&lt;/p&gt;&lt;p&gt;The company is to open-source hardware and software components, with hardware including NPU modules, air-cooled and liquid-cooled blade servers, AI cards, CPU boards, and cascade cards. For software, Huawei committed to fully open-sourcing CANN compiler tools, Mind series application kits, and openPangu foundation models by 31 December 2025.&lt;/p&gt;&lt;h3&gt;Market deployment and ecosystem impact&lt;/h3&gt;&lt;p&gt;Real-world deployment provides validation for these technical claims. Over 300 Atlas 900 A3 SuperPoD units have already been shipped in 2025, which have been deployed for more than 20 customers from multiple sectors, including the Internet, finance, carrier, electricity, and manufacturing sectors.&lt;/p&gt;&lt;p&gt;The implications for the development of China’s AI infrastructure are substantial. By creating an open ecosystem around domestic technology, Huawei is addressing the challenges of building competitive AI infrastructure inside parameters set by constrained semiconductor manufacturing and availability. Its approach enables broader industry participation in developing AI infrastructure solutions without needing access to the most advanced process nodes.&lt;/p&gt;&lt;p&gt;For the global AI infrastructure market, Huawei’s open architecture strategy introduces an alternative to the tightly integrated, proprietary hardware and software approach dominant among Western competitors. Whether the ecosystem proposed by Huawei can achieve comparable performance and maintain commercial viability remains to be demonstrated at scale.&lt;/p&gt;&lt;p&gt;Ultimately, the SuperPoD architecture represents more than an incremental advance for AI computing. Huawei is proposing a fundamental of how massive computational resources are connected, managed, and scaled. The open-source release of its specifications and elements will test whether collaborative development can accelerate AI infrastructure innovation in an ecosystem of partners. That has the potential to reshape competitive dynamics in the global AI infrastructure market.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Huawei commits to training 30,000 Malaysian AI professionals as local tech ecosystem expands&lt;/strong&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt="Banner for the AI &amp;amp; Big Data Expo event series." src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Imagine connecting thousands of powerful AI chips scattered in dozens of server cabinets and making them work together as if they were a single, massive computer. That is exactly what Huawei demonstrated at HUAWEI CONNECT 2025, where the company unveiled a breakthrough in AI infrastructure architecture that could reshape how the world builds and scales artificial intelligence systems.&lt;/p&gt;&lt;p&gt;Instead of traditional approaches where individual servers work somewhat independently, Huawei’s new SuperPoD technology creates what the company’s executives describe as a single logical machine made from thousands of separate processing units, allowing them, or it, to “learn, think, and reason as one.”&lt;/p&gt;&lt;p&gt;The implications extend beyond impressive technical specifications, representing a shift in how AI computing power can be organised, scaled, and deployed in industries.&lt;/p&gt;&lt;h3&gt;The technical foundation: UnifiedBus 2.0&lt;/h3&gt;&lt;p&gt;At the core of Huawei’s infrastructure approach is UnifiedBus (UB). Yang Chaobin, Huawei’s Director of the Board and CEO of the ICT Business Group, explained that “Huawei has developed the groundbreaking SuperPoD architecture based on our UnifiedBus interconnect protocol. The architecture deeply interconnects physical servers so that they can learn, think, and reason like a single logical server.”&lt;/p&gt;&lt;p&gt;The technical specifications reveal the scope of this achievement. The UnifiedBus protocol addresses two challenges that, historically, have limited large-scale AI computing: the reliability of long-range communications and bandwidth-latency. Traditional copper connections provide high bandwidth but only over short distances, typically connecting perhaps two cabinets.&lt;/p&gt;&lt;p&gt;Optical cables support longer range but suffer from reliability issues that become more problematic the greater the distance and scale. Eric Xu, Huawei’s Deputy Chairman and Rotating Chairman, said that solving these fundamental connectivity challenges was essential to the company’s AI infrastructure strategy.&lt;/p&gt;&lt;p&gt;Xu detailed the breakthrough solutions in terms of the OSI model: “We have built reliability into every layer of our interconnect protocol, from the physical layer and data link layer, all the way up to the network and transmission layers. There is 100-ns-level fault detection and protection switching on optical paths, making any intermittent disconnections or faults of optical modules imperceptible at the application layer.”&lt;/p&gt;&lt;h3&gt;SuperPoD architecture: Scale and performance&lt;/h3&gt;&lt;p&gt;The Atlas 950 SuperPoD represents the flagship implementation of this architecture, comprising of up to 8,192 Ascend 950DT chips in a configuration that Xu described as delivering “8 EFLOPS in FP8 and 16 EFLOPS in FP4. Its interconnect bandwidth will be 16 PB/s. This means that a single Atlas 950 SuperPoD will have an interconnect bandwidth over 10 times higher than the entire globe’s total peak internet bandwidth.”&lt;/p&gt;&lt;p&gt;The specifications are more than incremental improvements. The Atlas 950 SuperPoD occupies 160 cabinets in 1,000m&lt;sup&gt;2&lt;/sup&gt;, with 128 compute cabinets and 32 comms cabinets linked with all-optical interconnects. The system’s memory capacity reaches 1,152 TB and maintains what Huawei claims is 2.1-microsecond latency in the entire system.&lt;/p&gt;&lt;p&gt;Later in the production pipeline will be the Atlas 960 SuperPoD, which is set to incorporate 15,488 Ascend 960 chips in 220 cabinets covering 2,200m&lt;sup&gt;2&lt;/sup&gt;. Xu said it will deliver “30 EFLOPS in FP8 and 60 EFLOPS in FP4, and come with 4,460 TB of memory and 34 PB/s interconnect bandwidth.”&lt;/p&gt;&lt;h3&gt;Beyond AI: General-purpose computing applications&lt;/h3&gt;&lt;p&gt;The SuperPoD concept extends beyond AI workloads into general-purpose computing through the TaiShan 950 SuperPoD. Built on Kunpeng 950 processors, this system addresses enterprise challenges in replacing legacy mainframes and mid-range computers.&lt;/p&gt;&lt;p&gt;Xu positioned this as particularly relevant for the finance sector, where “the TaiShan 950 SuperPoD, combined with the distributed GaussDB, can serve as an ideal alternative, and replace — once and for all — mainframes, mid-range computers, and Oracle’s Exadata database servers.”&lt;/p&gt;&lt;h3&gt;Open architecture strategy&lt;/h3&gt;&lt;p&gt;Perhaps most significantly for the broader AI infrastructure market, Huawei announced the release of UnifiedBus 2.0 technical specifications as open standards. The decision reflects both strategic positioning and practical constraints.&lt;/p&gt;&lt;p&gt;Xu acknowledged that “the Chinese mainland will lag behind in semiconductor manufacturing process nodes for a relatively long time” and emphasised that “sustainable computing power can only be achieved with process nodes that are practically available.”&lt;/p&gt;&lt;p&gt;Yang framed the open approach as ecosystem building: “We are committed to our open-hardware and open-source-software approach that will help more partners develop their own industry-scenario-based SuperPoD solutions. This will accelerate developer innovation and foster a thriving ecosystem.”&lt;/p&gt;&lt;p&gt;The company is to open-source hardware and software components, with hardware including NPU modules, air-cooled and liquid-cooled blade servers, AI cards, CPU boards, and cascade cards. For software, Huawei committed to fully open-sourcing CANN compiler tools, Mind series application kits, and openPangu foundation models by 31 December 2025.&lt;/p&gt;&lt;h3&gt;Market deployment and ecosystem impact&lt;/h3&gt;&lt;p&gt;Real-world deployment provides validation for these technical claims. Over 300 Atlas 900 A3 SuperPoD units have already been shipped in 2025, which have been deployed for more than 20 customers from multiple sectors, including the Internet, finance, carrier, electricity, and manufacturing sectors.&lt;/p&gt;&lt;p&gt;The implications for the development of China’s AI infrastructure are substantial. By creating an open ecosystem around domestic technology, Huawei is addressing the challenges of building competitive AI infrastructure inside parameters set by constrained semiconductor manufacturing and availability. Its approach enables broader industry participation in developing AI infrastructure solutions without needing access to the most advanced process nodes.&lt;/p&gt;&lt;p&gt;For the global AI infrastructure market, Huawei’s open architecture strategy introduces an alternative to the tightly integrated, proprietary hardware and software approach dominant among Western competitors. Whether the ecosystem proposed by Huawei can achieve comparable performance and maintain commercial viability remains to be demonstrated at scale.&lt;/p&gt;&lt;p&gt;Ultimately, the SuperPoD architecture represents more than an incremental advance for AI computing. Huawei is proposing a fundamental of how massive computational resources are connected, managed, and scaled. The open-source release of its specifications and elements will test whether collaborative development can accelerate AI infrastructure innovation in an ecosystem of partners. That has the potential to reshape competitive dynamics in the global AI infrastructure market.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Huawei commits to training 30,000 Malaysian AI professionals as local tech ecosystem expands&lt;/strong&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt="Banner for the AI &amp;amp; Big Data Expo event series." src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/huawei-ai-chips-superpod-technology/</guid><pubDate>Thu, 25 Sep 2025 09:23:08 +0000</pubDate></item><item><title>Fusion power plants don’t exist yet, but they’re making money anyway (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/09/25/1124050/fusion-future-funding/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/6-25-CFS-Magnet-Factory-in-Devens-1.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;This week, Commonwealth Fusion Systems announced it has another customer for its first commercial fusion power plant, in Virginia. Eni, one of the world’s largest oil and gas companies, signed a billion-dollar deal to buy electricity from the facility.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;One small detail? That reactor doesn’t exist yet. Neither does the smaller reactor Commonwealth is building first to demonstrate that its tokamak design will work as intended.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;This is a weird moment in fusion. Investors are pouring billions into the field to build power plants, and some companies are even signing huge agreements to purchase power from those still-nonexistent plants. All this comes before companies have actually completed a working reactor that can produce electricity. It takes money to develop a new technology, but all this funding could lead to some twisted expectations.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Nearly three years ago, the National Ignition Facility at Lawrence Livermore National Laboratory hit a major milestone for fusion power. With the help of the world’s most powerful lasers, scientists heated a pellet of fuel to 100 million °C. Hydrogen atoms in that fuel fused together, releasing more energy than the lasers put in.&lt;/p&gt; 
 &lt;p&gt;It was a game changer for the vibes in fusion. The NIF experiment finally showed that a fusion reactor could yield net energy. Plasma physicists’ models had certainly suggested that it should be true, but it was another thing to see it demonstrated in real life.&lt;/p&gt;  &lt;p&gt;But in some ways, the NIF results didn’t really change much for commercial fusion. That site’s lasers used a bonkers amount of energy, the setup was wildly complicated, and the whole thing lasted a fraction of a second. To operate a fusion power plant, not only do you have to achieve net energy, but you also need to do that on a somewhat constant basis and—crucially—do it economically.&lt;/p&gt; 
 &lt;p&gt;So in the wake of the NIF news, all eyes went to companies like Commonwealth, Helion, and Zap Energy. Who would be the first to demonstrate this milestone in a more commercially feasible reactor? Or better yet, who would be the first to get a power plant up and running?&lt;/p&gt;  &lt;p&gt;So far, the answer is none of them.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;To be fair, many fusion companies have made technical progress. Commonwealth has built and tested its high-temperature superconducting magnets and published research about that work. Zap Energy demonstrated three hours of continuous operation in its test system, a milestone validated by the US Department of Energy. Helion started construction of its power plant in Washington in July. (And that’s not to mention a thriving, publicly funded fusion industry in China.)&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;These are all important milestones, and these and other companies have seen many more. But as Ed Morse, a professor of nuclear engineering at Berkeley, summed it up to me: “They don’t have a reactor.” (He was speaking specifically about Commonwealth, but really, the same goes for the others.)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;And yet, the money pours in. Commonwealth raised over $800 million in funding earlier this year. And now it’s got two big customers signed on to buy electricity from this future power plant.&lt;/p&gt;  &lt;p&gt;Why buy electricity from a reactor that’s currently little more than ideas on paper? From the perspective of these particular potential buyers, such agreements can be something of a win-win, says Adam Stein, director of nuclear energy innovation at the Breakthrough Institute.&lt;/p&gt;  &lt;p&gt;By putting a vote of confidence behind Commonwealth, Eni could help the fusion startup get the capital it needs to actually build its plant. The company also directly invests in Commonwealth, so it stands to benefit from success. Getting a good rate on the capital needed to build the plant could also mean the electricity is ultimately cheaper for Eni, Stein says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Ultimately, fusion needs a lot of money. If fossil-fuel companies and tech giants want to provide it, all the better. One concern I have, though, is how outside observers are interpreting these big commitments.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;US Energy Secretary Chris Wright has been loud about his support for fusion and his expectations of the technology. Earlier this month, he told the BBC that it will soon power the world.&lt;/p&gt;  &lt;p&gt;He’s certainly not the first to have big dreams for fusion, and it is an exciting technology. But despite the jaw-dropping financial milestones, this industry is still very much in development.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And while Wright praises fusion, the Trump administration is slashing support for other energy technologies, including wind and solar power, and spreading disinformation about their safety, cost, and effectiveness.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;To meet the growing electricity demand and cut emissions from the power sector, we’ll need a whole range of technologies. It’s a risk and a distraction to put all our hopes on an unproven energy tech when there are plenty of options that actually exist.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark, &lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/6-25-CFS-Magnet-Factory-in-Devens-1.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;This week, Commonwealth Fusion Systems announced it has another customer for its first commercial fusion power plant, in Virginia. Eni, one of the world’s largest oil and gas companies, signed a billion-dollar deal to buy electricity from the facility.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;One small detail? That reactor doesn’t exist yet. Neither does the smaller reactor Commonwealth is building first to demonstrate that its tokamak design will work as intended.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;This is a weird moment in fusion. Investors are pouring billions into the field to build power plants, and some companies are even signing huge agreements to purchase power from those still-nonexistent plants. All this comes before companies have actually completed a working reactor that can produce electricity. It takes money to develop a new technology, but all this funding could lead to some twisted expectations.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Nearly three years ago, the National Ignition Facility at Lawrence Livermore National Laboratory hit a major milestone for fusion power. With the help of the world’s most powerful lasers, scientists heated a pellet of fuel to 100 million °C. Hydrogen atoms in that fuel fused together, releasing more energy than the lasers put in.&lt;/p&gt; 
 &lt;p&gt;It was a game changer for the vibes in fusion. The NIF experiment finally showed that a fusion reactor could yield net energy. Plasma physicists’ models had certainly suggested that it should be true, but it was another thing to see it demonstrated in real life.&lt;/p&gt;  &lt;p&gt;But in some ways, the NIF results didn’t really change much for commercial fusion. That site’s lasers used a bonkers amount of energy, the setup was wildly complicated, and the whole thing lasted a fraction of a second. To operate a fusion power plant, not only do you have to achieve net energy, but you also need to do that on a somewhat constant basis and—crucially—do it economically.&lt;/p&gt; 
 &lt;p&gt;So in the wake of the NIF news, all eyes went to companies like Commonwealth, Helion, and Zap Energy. Who would be the first to demonstrate this milestone in a more commercially feasible reactor? Or better yet, who would be the first to get a power plant up and running?&lt;/p&gt;  &lt;p&gt;So far, the answer is none of them.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;To be fair, many fusion companies have made technical progress. Commonwealth has built and tested its high-temperature superconducting magnets and published research about that work. Zap Energy demonstrated three hours of continuous operation in its test system, a milestone validated by the US Department of Energy. Helion started construction of its power plant in Washington in July. (And that’s not to mention a thriving, publicly funded fusion industry in China.)&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;These are all important milestones, and these and other companies have seen many more. But as Ed Morse, a professor of nuclear engineering at Berkeley, summed it up to me: “They don’t have a reactor.” (He was speaking specifically about Commonwealth, but really, the same goes for the others.)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;And yet, the money pours in. Commonwealth raised over $800 million in funding earlier this year. And now it’s got two big customers signed on to buy electricity from this future power plant.&lt;/p&gt;  &lt;p&gt;Why buy electricity from a reactor that’s currently little more than ideas on paper? From the perspective of these particular potential buyers, such agreements can be something of a win-win, says Adam Stein, director of nuclear energy innovation at the Breakthrough Institute.&lt;/p&gt;  &lt;p&gt;By putting a vote of confidence behind Commonwealth, Eni could help the fusion startup get the capital it needs to actually build its plant. The company also directly invests in Commonwealth, so it stands to benefit from success. Getting a good rate on the capital needed to build the plant could also mean the electricity is ultimately cheaper for Eni, Stein says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Ultimately, fusion needs a lot of money. If fossil-fuel companies and tech giants want to provide it, all the better. One concern I have, though, is how outside observers are interpreting these big commitments.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;US Energy Secretary Chris Wright has been loud about his support for fusion and his expectations of the technology. Earlier this month, he told the BBC that it will soon power the world.&lt;/p&gt;  &lt;p&gt;He’s certainly not the first to have big dreams for fusion, and it is an exciting technology. But despite the jaw-dropping financial milestones, this industry is still very much in development.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And while Wright praises fusion, the Trump administration is slashing support for other energy technologies, including wind and solar power, and spreading disinformation about their safety, cost, and effectiveness.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;To meet the growing electricity demand and cut emissions from the power sector, we’ll need a whole range of technologies. It’s a risk and a distraction to put all our hopes on an unproven energy tech when there are plenty of options that actually exist.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark, &lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/09/25/1124050/fusion-future-funding/</guid><pubDate>Thu, 25 Sep 2025 10:00:00 +0000</pubDate></item><item><title>[NEW] Towards better health conversations: Research insights on a “wayfinding” AI agent based on Gemini (The latest research from Google)</title><link>https://research.google/blog/towards-better-health-conversations-research-insights-on-a-wayfinding-ai-agent-based-on-gemini/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;The ability to find clear, relevant, and personalized health information is a cornerstone of empowerment for medical patients. Yet, navigating the world of online health information is often a confusing, overwhelming, and impersonal experience. We are met with a flood of generic information that does not account for our unique context, and it can be difficult to know what details are relevant.&lt;/p&gt;&lt;p&gt;Large language models (LLMs) have the potential to make this information more accessible and tailored. However, many AI tools today act as passive "question-answerers" — they provide a single, comprehensive answer to an initial query. But this isn't how an expert, like a doctor, helps someone navigate a complex topic. A health professional doesn't just provide a lecture; they ask clarifying questions to understand the full picture, discover a person's goals, and guide them through the information maze. Though this &lt;i&gt;context-seeking&lt;/i&gt; is critical, it's a significant design challenge for AI.&lt;/p&gt;&lt;p&gt;In “Towards Better Health Conversations: The Benefits of Context-Seeking”, we describe how we designed and tested our “Wayfinding AI”, an early-stage research prototype, based on Gemini, that explores a new approach. Our fundamental thesis is that by proactively asking clarifying questions, an AI agent can better discover a user's needs, guide them in articulating their concerns, and provide more helpful, tailored information. In a series of four mixed-method user experience studies with a total of 163 participants, we examined how people interact with AI for their health questions, and we iteratively designed an agent that users found to be significantly more helpful, relevant, and tailored to their needs than a baseline AI agent.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;The ability to find clear, relevant, and personalized health information is a cornerstone of empowerment for medical patients. Yet, navigating the world of online health information is often a confusing, overwhelming, and impersonal experience. We are met with a flood of generic information that does not account for our unique context, and it can be difficult to know what details are relevant.&lt;/p&gt;&lt;p&gt;Large language models (LLMs) have the potential to make this information more accessible and tailored. However, many AI tools today act as passive "question-answerers" — they provide a single, comprehensive answer to an initial query. But this isn't how an expert, like a doctor, helps someone navigate a complex topic. A health professional doesn't just provide a lecture; they ask clarifying questions to understand the full picture, discover a person's goals, and guide them through the information maze. Though this &lt;i&gt;context-seeking&lt;/i&gt; is critical, it's a significant design challenge for AI.&lt;/p&gt;&lt;p&gt;In “Towards Better Health Conversations: The Benefits of Context-Seeking”, we describe how we designed and tested our “Wayfinding AI”, an early-stage research prototype, based on Gemini, that explores a new approach. Our fundamental thesis is that by proactively asking clarifying questions, an AI agent can better discover a user's needs, guide them in articulating their concerns, and provide more helpful, tailored information. In a series of four mixed-method user experience studies with a total of 163 participants, we examined how people interact with AI for their health questions, and we iteratively designed an agent that users found to be significantly more helpful, relevant, and tailored to their needs than a baseline AI agent.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/towards-better-health-conversations-research-insights-on-a-wayfinding-ai-agent-based-on-gemini/</guid><pubDate>Thu, 25 Sep 2025 10:33:00 +0000</pubDate></item><item><title>DeepMind’s robotic ballet: An AI for coordinating manufacturing robots (AI – Ars Technica)</title><link>https://arstechnica.com/science/2025/09/deepminds-robotic-ballet-an-ai-for-coordinating-manufacturing-robots/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        An AI figures out how robots can get jobs done without getting in each other's way.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Image of an assembly line where two robots are coordinating the production of an automobile frame." class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-2223136363-640x427.jpg" width="640" /&gt;
                  &lt;img alt="Image of an assembly line where two robots are coordinating the production of an automobile frame." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-2223136363-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          China News Service 

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;A lot of the stuff we use today is largely made by robots—arms with multiple degrees of freedom positioned along conveyor belts that move in a spectacle of precisely synchronized motions. All this motion is usually programmed by hand, which can take hundreds to thousands of hours. Google’s DeepMind team has developed an AI system called RoboBallet that lets manufacturing robots figure out what to do on their own.&lt;/p&gt;
&lt;h2&gt;Traveling salesmen&lt;/h2&gt;
&lt;p&gt;Planning what manufacturing robots should do to get their jobs done efficiently is really hard to automate. You need to solve both task allocation and scheduling—deciding which task should be done by which robot in what order. It’s like the famous traveling salesman problem on steroids. On top of that, there is the question of motion planning; you need to make sure all these robotic arms won’t collide with each other or with all the gear standing around them.&lt;/p&gt;
&lt;p&gt;At the end, you’re facing myriad possible combinations where you’ve got to solve not one but three computationally hard problems at the same time. “There are some tools that let you automate motion planning, but task allocation and scheduling are usually done manually,” says Matthew Lai, a research engineer at Google DeepMind. “Solving all three of these problems combined is what we tackled in our work.”&lt;/p&gt;
&lt;p&gt;Lai’s team started by generating simulated samples of what are called work cells, areas where teams of robots perform their tasks on a product being manufactured. The work cells contained something called a workpiece, a product on which the robots do work, in this case something to be constructed of aluminum struts placed on a table. Around the table, there were up to eight randomly placed Franka Panda robotic arms, each with 7 degrees of freedom, that were supposed to complete up to 40 tasks on a workpiece. Every task required a robotic arm’s end effector to get within 2.5 centimeters of the right spot on the right strut, approached from the correct angle, then stay there, frozen, for a moment. The pause simulates doing some work.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;To make things harder, the team peppered every work cell with random obstacles the robots had to avoid. “We chose to work with up to eight robots, as this is around the sensible maximum for packing robots closely together without them blocking each other all the time,” Lai explains. Forcing the robots to perform 40 tasks on a workpiece was also something the team considered representative of what’s required at real factories.&lt;/p&gt;
&lt;p&gt;A setup like this would be a nightmare to tackle using even the most powerful reinforcement-learning algorithms. Lai and his colleagues found a way around it by turning it all into graphs.&lt;/p&gt;
&lt;h2&gt;Complex relationships&lt;/h2&gt;
&lt;p&gt;Graphs in Lai’s model comprised nodes and edges. Things like robots, tasks, and obstacles were treated as nodes. Relationships between them were encoded as either one- or bi-directional edges. One-directional edges connected robots with tasks and obstacles because the robots needed information about where the obstacles were and whether the tasks were completed or not. Bidirectional edges connected the robots to each other, because each robot had to know what other robots were doing at each time step to avoid collisions or duplicating tasks.&lt;/p&gt;
&lt;p&gt;To read and make sense of the graphs, the team used graph neural networks, a type of artificial intelligence designed to extract relationships between the nodes by passing messages along the edges of the connections among them. This decluttered the data, allowing the researchers to design a system that focused exclusively on what mattered most: finding the most efficient ways to complete tasks while navigating obstacles. After a few days of training on randomly generated work cells using a single Nvidia A100 GPU, the new industrial planning AI, called RoboBallet, could lay out seemingly viable trajectories through complex, previously unseen environments in a matter of seconds.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Most importantly, though, it scaled really well.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Economy of scale&lt;/h2&gt;
&lt;p&gt;The problem with applying traditional computational methods to complex problems like managing robots at a factory is that the challenge of computation grows exponentially with the number of items you have in your system. Computing the most optimal trajectories for one robot is relatively simple. Doing the same for two is considerably harder; when the number grows to eight, the problem becomes practically intractable.&lt;/p&gt;
&lt;p&gt;With RoboBallet, the complexity of computation also grew with the complexity of the system, but at a far slower rate. (The computations grew linearly with the growing number of tasks and obstacles, and quadratically with the number of robots.) According to the team, these computations should make the system feasible for industrial-scale use.&lt;/p&gt;
&lt;p&gt;The team wanted to test, however, whether the plans their AI was producing were any good. To check that, Lai and his colleagues computed the most optimal task allocations, schedules, and motions in a few simplified work cells and compared those with results delivered by RoboBallet. In terms of execution time, arguably the most important metric in manufacturing, the AI came very close to what human engineers could do. It wasn’t better than they were—it just provided an answer more quickly.&lt;/p&gt;
&lt;p&gt;The team also tested RoboBallet plans on a real-world physical setup of four Panda robots working on an aluminum workpiece, and they worked just as well as in simulations. But Lai says it can do more than just speed up the process of programming robots.&lt;/p&gt;
&lt;h2&gt;Limping along&lt;/h2&gt;
&lt;p&gt;RoboBallet, according to DeepMind’s team, also enables us to design better work cells. “Because it works so fast, it would be possible for a designer to try different layouts and different placement or selections of robots in almost real time,” Lai says. This way, engineers at factories would be able to see exactly how much time they would save by adding another robot to a cell or choosing a robot of a different type. Another thing RoboBallet can do is reprogram the work cell on the fly, allowing other robots to fill in when one of them breaks down.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Still, there are a few things that still need ironing out before RoboBallet can come to factories. “There are several simplifications we made,” Lai admits. The first was that the obstacles were decomposed into cuboids. Even the workpiece itself was cubical. While this was somewhat representative of the obstacles and equipment in real factories, there are lots of possible workpieces with more organic shapes. “It would be better to represent those in a more flexible way, like mesh graphs or point clouds,” Lai says. This, however, would likely mean a drop in RoboBallet’s blistering speed.&lt;/p&gt;
&lt;p&gt;Another thing is that the robots in Lai’s experiments were identical, while in a real-world work cell, robotic teams are quite often heterogeneous. “That’s why real-world applications would require additional research and engineering specific to the type of application,” Lai says. He adds, though, that the current RoboBallet is already designed with such adaptations in mind—it can be easily extended to support them. And once that’s done, his hope is that it will make factories faster and way more flexible.&lt;/p&gt;
&lt;p&gt;“The system would have to be given work cell models, the workpiece models, as well as the list of tasks that need to be done—based on that, RoboBallet would be able to generate a complete plan,” Lai says.&lt;/p&gt;
&lt;p&gt;Science Robotics, 2025. DOI: 10.1126/scirobotics.ads1204&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        An AI figures out how robots can get jobs done without getting in each other's way.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Image of an assembly line where two robots are coordinating the production of an automobile frame." class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-2223136363-640x427.jpg" width="640" /&gt;
                  &lt;img alt="Image of an assembly line where two robots are coordinating the production of an automobile frame." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-2223136363-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          China News Service 

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;A lot of the stuff we use today is largely made by robots—arms with multiple degrees of freedom positioned along conveyor belts that move in a spectacle of precisely synchronized motions. All this motion is usually programmed by hand, which can take hundreds to thousands of hours. Google’s DeepMind team has developed an AI system called RoboBallet that lets manufacturing robots figure out what to do on their own.&lt;/p&gt;
&lt;h2&gt;Traveling salesmen&lt;/h2&gt;
&lt;p&gt;Planning what manufacturing robots should do to get their jobs done efficiently is really hard to automate. You need to solve both task allocation and scheduling—deciding which task should be done by which robot in what order. It’s like the famous traveling salesman problem on steroids. On top of that, there is the question of motion planning; you need to make sure all these robotic arms won’t collide with each other or with all the gear standing around them.&lt;/p&gt;
&lt;p&gt;At the end, you’re facing myriad possible combinations where you’ve got to solve not one but three computationally hard problems at the same time. “There are some tools that let you automate motion planning, but task allocation and scheduling are usually done manually,” says Matthew Lai, a research engineer at Google DeepMind. “Solving all three of these problems combined is what we tackled in our work.”&lt;/p&gt;
&lt;p&gt;Lai’s team started by generating simulated samples of what are called work cells, areas where teams of robots perform their tasks on a product being manufactured. The work cells contained something called a workpiece, a product on which the robots do work, in this case something to be constructed of aluminum struts placed on a table. Around the table, there were up to eight randomly placed Franka Panda robotic arms, each with 7 degrees of freedom, that were supposed to complete up to 40 tasks on a workpiece. Every task required a robotic arm’s end effector to get within 2.5 centimeters of the right spot on the right strut, approached from the correct angle, then stay there, frozen, for a moment. The pause simulates doing some work.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;To make things harder, the team peppered every work cell with random obstacles the robots had to avoid. “We chose to work with up to eight robots, as this is around the sensible maximum for packing robots closely together without them blocking each other all the time,” Lai explains. Forcing the robots to perform 40 tasks on a workpiece was also something the team considered representative of what’s required at real factories.&lt;/p&gt;
&lt;p&gt;A setup like this would be a nightmare to tackle using even the most powerful reinforcement-learning algorithms. Lai and his colleagues found a way around it by turning it all into graphs.&lt;/p&gt;
&lt;h2&gt;Complex relationships&lt;/h2&gt;
&lt;p&gt;Graphs in Lai’s model comprised nodes and edges. Things like robots, tasks, and obstacles were treated as nodes. Relationships between them were encoded as either one- or bi-directional edges. One-directional edges connected robots with tasks and obstacles because the robots needed information about where the obstacles were and whether the tasks were completed or not. Bidirectional edges connected the robots to each other, because each robot had to know what other robots were doing at each time step to avoid collisions or duplicating tasks.&lt;/p&gt;
&lt;p&gt;To read and make sense of the graphs, the team used graph neural networks, a type of artificial intelligence designed to extract relationships between the nodes by passing messages along the edges of the connections among them. This decluttered the data, allowing the researchers to design a system that focused exclusively on what mattered most: finding the most efficient ways to complete tasks while navigating obstacles. After a few days of training on randomly generated work cells using a single Nvidia A100 GPU, the new industrial planning AI, called RoboBallet, could lay out seemingly viable trajectories through complex, previously unseen environments in a matter of seconds.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Most importantly, though, it scaled really well.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Economy of scale&lt;/h2&gt;
&lt;p&gt;The problem with applying traditional computational methods to complex problems like managing robots at a factory is that the challenge of computation grows exponentially with the number of items you have in your system. Computing the most optimal trajectories for one robot is relatively simple. Doing the same for two is considerably harder; when the number grows to eight, the problem becomes practically intractable.&lt;/p&gt;
&lt;p&gt;With RoboBallet, the complexity of computation also grew with the complexity of the system, but at a far slower rate. (The computations grew linearly with the growing number of tasks and obstacles, and quadratically with the number of robots.) According to the team, these computations should make the system feasible for industrial-scale use.&lt;/p&gt;
&lt;p&gt;The team wanted to test, however, whether the plans their AI was producing were any good. To check that, Lai and his colleagues computed the most optimal task allocations, schedules, and motions in a few simplified work cells and compared those with results delivered by RoboBallet. In terms of execution time, arguably the most important metric in manufacturing, the AI came very close to what human engineers could do. It wasn’t better than they were—it just provided an answer more quickly.&lt;/p&gt;
&lt;p&gt;The team also tested RoboBallet plans on a real-world physical setup of four Panda robots working on an aluminum workpiece, and they worked just as well as in simulations. But Lai says it can do more than just speed up the process of programming robots.&lt;/p&gt;
&lt;h2&gt;Limping along&lt;/h2&gt;
&lt;p&gt;RoboBallet, according to DeepMind’s team, also enables us to design better work cells. “Because it works so fast, it would be possible for a designer to try different layouts and different placement or selections of robots in almost real time,” Lai says. This way, engineers at factories would be able to see exactly how much time they would save by adding another robot to a cell or choosing a robot of a different type. Another thing RoboBallet can do is reprogram the work cell on the fly, allowing other robots to fill in when one of them breaks down.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Still, there are a few things that still need ironing out before RoboBallet can come to factories. “There are several simplifications we made,” Lai admits. The first was that the obstacles were decomposed into cuboids. Even the workpiece itself was cubical. While this was somewhat representative of the obstacles and equipment in real factories, there are lots of possible workpieces with more organic shapes. “It would be better to represent those in a more flexible way, like mesh graphs or point clouds,” Lai says. This, however, would likely mean a drop in RoboBallet’s blistering speed.&lt;/p&gt;
&lt;p&gt;Another thing is that the robots in Lai’s experiments were identical, while in a real-world work cell, robotic teams are quite often heterogeneous. “That’s why real-world applications would require additional research and engineering specific to the type of application,” Lai says. He adds, though, that the current RoboBallet is already designed with such adaptations in mind—it can be easily extended to support them. And once that’s done, his hope is that it will make factories faster and way more flexible.&lt;/p&gt;
&lt;p&gt;“The system would have to be given work cell models, the workpiece models, as well as the list of tasks that need to be done—based on that, RoboBallet would be able to generate a complete plan,” Lai says.&lt;/p&gt;
&lt;p&gt;Science Robotics, 2025. DOI: 10.1126/scirobotics.ads1204&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/science/2025/09/deepminds-robotic-ballet-an-ai-for-coordinating-manufacturing-robots/</guid><pubDate>Thu, 25 Sep 2025 11:15:40 +0000</pubDate></item><item><title>The Download: growing threats to vulnerable languages, and fact-checking Trump’s medical claims (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/09/25/1124079/the-download-threats-vulnerable-languages-and-trump-medical-claims/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;h3 class="wp-block-heading has-medium-font-size"&gt;&lt;strong&gt;How AI and Wikipedia have sent vulnerable languages into a doom spiral&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Wikipedia is the most ambitious multilingual project after the Bible: There are editions in over 340 languages, and a further 400 even more obscure ones are being developed. But many of these smaller editions are being swamped with AI-translated content. Volunteers working on four African languages, for instance, estimated to &lt;em&gt;MIT Technology Review &lt;/em&gt;that between 40% and 60% of articles in their Wikipedia editions were uncorrected machine translations.&lt;/p&gt;  &lt;p&gt;This is beginning to cause a wicked problem. AI systems learn new languages by scraping huge quantities of text from the internet. Wikipedia is sometimes the largest source of online linguistic data for languages with few speakers—so any errors on those pages can poison the wells that AI is expected to draw from. Volunteers are being forced to go to extreme lengths to fix the issue, even deleting certain languages from Wikipedia entirely. Read the full story.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;—&lt;em&gt;Jacob Judah&amp;nbsp;&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;&lt;em&gt;This story is part of our Big Story series: MIT Technology Review’s most important, ambitious reporting. These stories take a deep look at the technologies that are coming next and what they will mean for us and the world we live in. &lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;&lt;em&gt;Check out the rest of the series here&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;&lt;em&gt;.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; 
   &lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Trump is pushing leucovorin as a new treatment for autism. What is it?&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;  &lt;p&gt;On Monday, President Trump claimed that childhood vaccines and acetaminophen, the active ingredient in Tylenol, are to blame for the increasing prevalence of autism. He advised pregnant women against taking the medicine.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The administration also announced that the FDA would work to make a medication called leucovorin available as a treatment for children with autism. The president’s assertions left many dismayed. “The data cited do not support the claim that Tylenol causes autism and leucovorin is a cure, and only stoke fear and falsely suggest hope when there is no simple answer,” said the Coalition for Autism Researchers, a group of more than 250 scientists, in a statement. So what &lt;em&gt;does &lt;/em&gt;the evidence say? Read our story to find out.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;—&lt;em&gt;Cassandra Willyard&amp;nbsp;&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;This is part of our MIT Technology Review Explains series, where our writers untangle the complex, messy world of technology to help you understand what’s coming next. You can read &lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;&lt;em&gt;more from the series here&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;&lt;em&gt;.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;    &lt;h3 class="wp-block-heading has-medium-font-size"&gt;&lt;strong&gt;Fusion power plants don’t exist yet, but they’re making money anyway&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;This week, Commonwealth Fusion Systems announced it has another customer for its first commercial fusion power plant, in Virginia. Eni, one of the world’s largest oil and gas companies, signed a billion-dollar deal to buy electricity from the facility.&lt;/p&gt;  &lt;p&gt;One small detail? That reactor doesn’t exist yet. This is a weird moment in fusion. Investors are pouring billions into the field to build power plants, and companies are even signing huge agreements to purchase power from those still-nonexistent plants.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;But all this comes before companies have actually completed a working reactor that can produce electricity. It takes money to develop a new technology, but all this funding could lead to some twisted expectations. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Casey Crownhart&amp;nbsp;&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story is from The Spark, our weekly newsletter all about the latest in climate change and clean tech. &lt;/strong&gt;&lt;strong&gt;Sign up&lt;/strong&gt;&lt;strong&gt; to receive it in your inbox every Wednesday.&lt;/strong&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;The AI Hype Index: Cracking the chatbot code&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Millions of us use chatbots every day, even though we don’t really know how they work or how using them affects us. In a bid to address this, the FTC recently launched an inquiry into how chatbots affect children and teenagers. Elsewhere, OpenAI has started to shed more light on what people are actually using ChatGPT for, and why it thinks its LLMs are so prone to making stuff up.&lt;/p&gt;  &lt;p&gt;There’s still plenty we don’t know—but that isn’t stopping governments from forging ahead with AI projects. In the US, RFK Jr. is pushing his staffers to use ChatGPT, while Albania is using a chatbot for public contract procurement. Check out the latest edition of our AI Hype Index to help you sort AI reality from hyped-up fiction.&amp;nbsp;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Huntington’s disease has been treated successfully for the first time&lt;/strong&gt;&lt;br /&gt;Gene therapy managed to slow progress of the disease in patients by 75%. (The Economist $)&amp;nbsp;&lt;br /&gt;+ &lt;em&gt;Here’s how the gene editing tool CRISPR is changing lives&lt;/em&gt;. (MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 Google says 90% of tech workers are using AI&lt;/strong&gt;&lt;br /&gt;But most of them also say they don’t trust AI models’ outputs. (CNN)&lt;br /&gt;+ &lt;em&gt;Why does AI hallucinate?&lt;/em&gt; (MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3 A MAGA TikTok takeover is coming&lt;/strong&gt;&lt;br /&gt;Just as free speech protections in the US start to look worryingly fragile. (The Atlantic $)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;4 Chinese tech workers are returning from the US&lt;/strong&gt;&lt;br /&gt;There’s a whole bunch of complex factors both driving them to leave, and luring them back. (Rest of World)&lt;br /&gt;+&lt;em&gt; But it’s hard to say what the impact of the new $100,000 fee for H-1B visas will be on India’s tech sector.&lt;/em&gt; (WP $)&lt;br /&gt;+&lt;em&gt; Europe is hoping to nab more tech talent too.&lt;/em&gt; (The Verge)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;5 If AI can diagnose us, what are doctors for?&lt;/strong&gt;&lt;br /&gt;They need to prepare for the fact chatbot use is becoming more and more widespread among patients. (New Yorker $)&lt;br /&gt;+ &lt;em&gt;This medical startup uses LLMs to run appointments and make diagnoses. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;6 Drones have been spotted at four more airports in Denmark&lt;/strong&gt;&lt;br /&gt;It looks like a coordinated attack, but officials still haven’t worked out who is behind it. (FT $)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;7 TSMC has unveiled AI-designed chips that use less energy&lt;/strong&gt;&lt;br /&gt;The AI software found better solutions than TSMC’s own human engineers&lt;em&gt;—&lt;/em&gt;and did so much faster. (South China Morning Post)&lt;br /&gt;+ &lt;em&gt;These four charts sum up the state of AI and energy.&lt;/em&gt; (MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 How to find love on dating apps&amp;nbsp;💑&lt;/strong&gt;&lt;br /&gt;It’s not easy, but it &lt;em&gt;is &lt;/em&gt;possible. (The Guardian)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;9 AI models can’t cope with Persian social etiquette&lt;/strong&gt;&lt;br /&gt;It involves a lot of saying ‘no’ when you mean ‘yes’, which simply doesn’t wash with computers. (Ars Technica)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10 VR headsets are better than ever, but no one seems to care&lt;/strong&gt;&lt;br /&gt;The tech industry keeps overestimating how willing people are to strap computers to their faces. (Gizmodo)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“We are living through the most destructive arms race in human history.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Ukrainian president Volodymyr Zelenskyy tells world leaders gathered at the UN that they need to intervene to stop the escalating development of drone technology and AI, The Guardian reports.&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1081538" src="https://wp.technologyreview.com/wp-content/uploads/2023/10/2AIntellFinal2f_thumb.jpg" /&gt;&lt;div class="image-credit"&gt;STUART BRADFORD&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;&lt;strong&gt;The great AI consciousness conundrum&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;AI consciousness isn’t just a tricky intellectual puzzle; it’s a morally weighty problem. Fail to identify a conscious AI, and you might unintentionally subjugate a being whose interests ought to matter. Mistake an unconscious AI for a conscious one, and you risk compromising human safety and happiness for the sake of an unthinking, unfeeling hunk of silicon and code.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;Over the past few decades, a small research community has doggedly attacked the question of what consciousness is and how it works. The effort has yielded real progress. And now, with the rapid advance of AI technology, these insights could offer our only guide to the untested, morally fraught waters of artificial consciousness. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Grace Huckins&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ It’s Fat Bear Week! Who gets your vote this year?&lt;br /&gt;+ Learn about Lord Woodbine, the forgotten sixth Beatle.&amp;nbsp;&lt;br /&gt;+ There are some truly wild and wacky recipes in this Medieval Cookery collection. Venison porridge, anyone?&amp;nbsp;&lt;br /&gt;+ Pessimism about technology is as old as technology itself, as this archive shows.&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;h3 class="wp-block-heading has-medium-font-size"&gt;&lt;strong&gt;How AI and Wikipedia have sent vulnerable languages into a doom spiral&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Wikipedia is the most ambitious multilingual project after the Bible: There are editions in over 340 languages, and a further 400 even more obscure ones are being developed. But many of these smaller editions are being swamped with AI-translated content. Volunteers working on four African languages, for instance, estimated to &lt;em&gt;MIT Technology Review &lt;/em&gt;that between 40% and 60% of articles in their Wikipedia editions were uncorrected machine translations.&lt;/p&gt;  &lt;p&gt;This is beginning to cause a wicked problem. AI systems learn new languages by scraping huge quantities of text from the internet. Wikipedia is sometimes the largest source of online linguistic data for languages with few speakers—so any errors on those pages can poison the wells that AI is expected to draw from. Volunteers are being forced to go to extreme lengths to fix the issue, even deleting certain languages from Wikipedia entirely. Read the full story.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;—&lt;em&gt;Jacob Judah&amp;nbsp;&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;&lt;em&gt;This story is part of our Big Story series: MIT Technology Review’s most important, ambitious reporting. These stories take a deep look at the technologies that are coming next and what they will mean for us and the world we live in. &lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;&lt;em&gt;Check out the rest of the series here&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;&lt;em&gt;.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; 
   &lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Trump is pushing leucovorin as a new treatment for autism. What is it?&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;  &lt;p&gt;On Monday, President Trump claimed that childhood vaccines and acetaminophen, the active ingredient in Tylenol, are to blame for the increasing prevalence of autism. He advised pregnant women against taking the medicine.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The administration also announced that the FDA would work to make a medication called leucovorin available as a treatment for children with autism. The president’s assertions left many dismayed. “The data cited do not support the claim that Tylenol causes autism and leucovorin is a cure, and only stoke fear and falsely suggest hope when there is no simple answer,” said the Coalition for Autism Researchers, a group of more than 250 scientists, in a statement. So what &lt;em&gt;does &lt;/em&gt;the evidence say? Read our story to find out.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;—&lt;em&gt;Cassandra Willyard&amp;nbsp;&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;This is part of our MIT Technology Review Explains series, where our writers untangle the complex, messy world of technology to help you understand what’s coming next. You can read &lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;&lt;em&gt;more from the series here&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;&lt;em&gt;.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;    &lt;h3 class="wp-block-heading has-medium-font-size"&gt;&lt;strong&gt;Fusion power plants don’t exist yet, but they’re making money anyway&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;This week, Commonwealth Fusion Systems announced it has another customer for its first commercial fusion power plant, in Virginia. Eni, one of the world’s largest oil and gas companies, signed a billion-dollar deal to buy electricity from the facility.&lt;/p&gt;  &lt;p&gt;One small detail? That reactor doesn’t exist yet. This is a weird moment in fusion. Investors are pouring billions into the field to build power plants, and companies are even signing huge agreements to purchase power from those still-nonexistent plants.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;But all this comes before companies have actually completed a working reactor that can produce electricity. It takes money to develop a new technology, but all this funding could lead to some twisted expectations. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Casey Crownhart&amp;nbsp;&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story is from The Spark, our weekly newsletter all about the latest in climate change and clean tech. &lt;/strong&gt;&lt;strong&gt;Sign up&lt;/strong&gt;&lt;strong&gt; to receive it in your inbox every Wednesday.&lt;/strong&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;The AI Hype Index: Cracking the chatbot code&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Millions of us use chatbots every day, even though we don’t really know how they work or how using them affects us. In a bid to address this, the FTC recently launched an inquiry into how chatbots affect children and teenagers. Elsewhere, OpenAI has started to shed more light on what people are actually using ChatGPT for, and why it thinks its LLMs are so prone to making stuff up.&lt;/p&gt;  &lt;p&gt;There’s still plenty we don’t know—but that isn’t stopping governments from forging ahead with AI projects. In the US, RFK Jr. is pushing his staffers to use ChatGPT, while Albania is using a chatbot for public contract procurement. Check out the latest edition of our AI Hype Index to help you sort AI reality from hyped-up fiction.&amp;nbsp;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Huntington’s disease has been treated successfully for the first time&lt;/strong&gt;&lt;br /&gt;Gene therapy managed to slow progress of the disease in patients by 75%. (The Economist $)&amp;nbsp;&lt;br /&gt;+ &lt;em&gt;Here’s how the gene editing tool CRISPR is changing lives&lt;/em&gt;. (MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 Google says 90% of tech workers are using AI&lt;/strong&gt;&lt;br /&gt;But most of them also say they don’t trust AI models’ outputs. (CNN)&lt;br /&gt;+ &lt;em&gt;Why does AI hallucinate?&lt;/em&gt; (MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3 A MAGA TikTok takeover is coming&lt;/strong&gt;&lt;br /&gt;Just as free speech protections in the US start to look worryingly fragile. (The Atlantic $)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;4 Chinese tech workers are returning from the US&lt;/strong&gt;&lt;br /&gt;There’s a whole bunch of complex factors both driving them to leave, and luring them back. (Rest of World)&lt;br /&gt;+&lt;em&gt; But it’s hard to say what the impact of the new $100,000 fee for H-1B visas will be on India’s tech sector.&lt;/em&gt; (WP $)&lt;br /&gt;+&lt;em&gt; Europe is hoping to nab more tech talent too.&lt;/em&gt; (The Verge)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;5 If AI can diagnose us, what are doctors for?&lt;/strong&gt;&lt;br /&gt;They need to prepare for the fact chatbot use is becoming more and more widespread among patients. (New Yorker $)&lt;br /&gt;+ &lt;em&gt;This medical startup uses LLMs to run appointments and make diagnoses. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;6 Drones have been spotted at four more airports in Denmark&lt;/strong&gt;&lt;br /&gt;It looks like a coordinated attack, but officials still haven’t worked out who is behind it. (FT $)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;7 TSMC has unveiled AI-designed chips that use less energy&lt;/strong&gt;&lt;br /&gt;The AI software found better solutions than TSMC’s own human engineers&lt;em&gt;—&lt;/em&gt;and did so much faster. (South China Morning Post)&lt;br /&gt;+ &lt;em&gt;These four charts sum up the state of AI and energy.&lt;/em&gt; (MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 How to find love on dating apps&amp;nbsp;💑&lt;/strong&gt;&lt;br /&gt;It’s not easy, but it &lt;em&gt;is &lt;/em&gt;possible. (The Guardian)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;9 AI models can’t cope with Persian social etiquette&lt;/strong&gt;&lt;br /&gt;It involves a lot of saying ‘no’ when you mean ‘yes’, which simply doesn’t wash with computers. (Ars Technica)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10 VR headsets are better than ever, but no one seems to care&lt;/strong&gt;&lt;br /&gt;The tech industry keeps overestimating how willing people are to strap computers to their faces. (Gizmodo)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“We are living through the most destructive arms race in human history.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Ukrainian president Volodymyr Zelenskyy tells world leaders gathered at the UN that they need to intervene to stop the escalating development of drone technology and AI, The Guardian reports.&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1081538" src="https://wp.technologyreview.com/wp-content/uploads/2023/10/2AIntellFinal2f_thumb.jpg" /&gt;&lt;div class="image-credit"&gt;STUART BRADFORD&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;&lt;strong&gt;The great AI consciousness conundrum&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;AI consciousness isn’t just a tricky intellectual puzzle; it’s a morally weighty problem. Fail to identify a conscious AI, and you might unintentionally subjugate a being whose interests ought to matter. Mistake an unconscious AI for a conscious one, and you risk compromising human safety and happiness for the sake of an unthinking, unfeeling hunk of silicon and code.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;Over the past few decades, a small research community has doggedly attacked the question of what consciousness is and how it works. The effort has yielded real progress. And now, with the rapid advance of AI technology, these insights could offer our only guide to the untested, morally fraught waters of artificial consciousness. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Grace Huckins&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ It’s Fat Bear Week! Who gets your vote this year?&lt;br /&gt;+ Learn about Lord Woodbine, the forgotten sixth Beatle.&amp;nbsp;&lt;br /&gt;+ There are some truly wild and wacky recipes in this Medieval Cookery collection. Venison porridge, anyone?&amp;nbsp;&lt;br /&gt;+ Pessimism about technology is as old as technology itself, as this archive shows.&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/09/25/1124079/the-download-threats-vulnerable-languages-and-trump-medical-claims/</guid><pubDate>Thu, 25 Sep 2025 12:10:00 +0000</pubDate></item><item><title>[NEW] Samsung benchmarks real productivity of enterprise AI models (AI News)</title><link>https://www.artificialintelligence-news.com/news/samsung-benchmarks-real-productivity-enterprise-ai-models/</link><description>&lt;p&gt;Samsung is overcoming limitations of existing benchmarks to better assess the real-world productivity of AI models in enterprise settings. The new system, developed by Samsung Research and named TRUEBench, aims to address the growing disparity between theoretical AI performance and its actual utility in the workplace.&lt;/p&gt;&lt;p&gt;As businesses worldwide accelerate their adoption of large language models (LLMs) to improve their operations, a challenge has emerged: how to accurately gauge their effectiveness. Many existing benchmarks focus on academic or general knowledge tests, often limited to English and simple question and answer formats. This has created a gap that leaves enterprises without a reliable method for evaluating how an AI model will perform on complex, multilingual, and context-rich business tasks.&lt;/p&gt;&lt;p&gt;Samsung’s TRUEBench, short for Trustworthy Real-world Usage Evaluation Benchmark, has been developed to fill this void. It provides a comprehensive suite of metrics that assesses LLMs based on scenarios and tasks directly relevant to real-world corporate environments. The benchmark draws upon Samsung’s own extensive internal enterprise use of AI models, ensuring the evaluation criteria are grounded in genuine workplace demands.&lt;/p&gt;&lt;p&gt;The framework evaluates common enterprise functions such as creating content, analysing data, summarising lengthy documents, and translating materials. These are broken down into 10 distinct categories and 46 sub-categories, providing a granular view of an AI’s productivity capabilities.&lt;/p&gt;&lt;p&gt;“Samsung Research brings deep expertise and a competitive edge through its real-world AI experience,” said Paul (Kyungwhoon) Cheun, CTO of the DX Division at Samsung Electronics and Head of Samsung Research. “We expect TRUEBench to establish evaluation standards for productivity.”&lt;/p&gt;&lt;p&gt;To tackle the limitations of older benchmarks, TRUEBench is built upon a foundation of 2,485 diverse test sets spanning 12 different languages and supporting cross-linguistic scenarios. This multilingual approach is critical for global corporations where information flows across different regions. The test materials themselves reflect the variety of workplace requests, ranging from brief instructions of just eight characters to the complex analysis of documents exceeding 20,000 characters.&lt;/p&gt;&lt;p&gt;Samsung recognised that in a real business context, a user’s full intent is not always explicitly stated in their initial prompt. The benchmark is therefore designed to assess an AI model’s ability to understand and fulfil these implicit enterprise needs, moving beyond simple accuracy to a more nuanced measure of helpfulness and relevance.&lt;/p&gt;&lt;p&gt;To achieve this, Samsung Research developed a unique collaborative process between human experts and AI to create the productivity scoring criteria. Initially, human annotators establish the evaluation standards for a given task. An AI then reviews these standards, checking for potential errors, internal contradictions, or unnecessary constraints that might not reflect a realistic user expectation. Following the AI’s feedback, the human annotators refine the criteria. This iterative loop ensures the final evaluation standards are precise and reflective of a high-quality outcome.&lt;/p&gt;&lt;p&gt;This cross-verified process delivers an automated evaluation system that scores the performance of LLMs. By using AI to apply these refined criteria, the system minimises the subjective bias that can occur with human-only scoring, ensuring consistency and reliability across all tests. TRUEBench also employs a strict scoring model where an AI model must satisfy every condition associated with a test to receive a passing mark. This all or nothing approach for individual conditions enables a more detailed and exacting assessment of the performance of AI models across different enterprise tasks.&lt;/p&gt;&lt;p&gt;To boost transparency and encourage wider adoption, Samsung has made TRUEBench’s data samples and leaderboards publicly available on the global open-source platform Hugging Face. This allows developers, researchers, and enterprises to directly compare the productivity performance of up to five different AI models simultaneously. The platform provides a clear, at a glance overview of how various AIs stack up against each other on practical tasks.&lt;/p&gt;&lt;p&gt;As of writing, here are the top 20 models by overall ranking based on Samsung’s AI benchmark:&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Current top 20 models by overall ranking based on Samsung’s AI benchmark that assesses the real-world productivity of AI models in enterprise settings." class="wp-image-109591" height="322" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/09/image-15-1024x322.png" width="1024" /&gt;&lt;/figure&gt;&lt;p&gt;The full published data also includes the average length of the AI-generated responses. This allows for a simultaneous comparison of not only performance but also efficiency, a key consideration for businesses weighing operational costs and speed.&lt;/p&gt;&lt;p&gt;With the launch of TRUEBench, Samsung is not merely releasing another tool but is aiming to change how the industry thinks about AI performance. By moving the goalposts from abstract knowledge to tangible productivity, Samsung’s benchmark could play a role in helping organisations make better decisions about which enterprise AI models to integrate into their workflows and bridge the gap between an AI’s potential and its proven value.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Inside Huawei’s plan to make thousands of AI chips think like one computer&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Banner for the AI &amp;amp; Big Data Expo event series." class="wp-image-109137" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Samsung is overcoming limitations of existing benchmarks to better assess the real-world productivity of AI models in enterprise settings. The new system, developed by Samsung Research and named TRUEBench, aims to address the growing disparity between theoretical AI performance and its actual utility in the workplace.&lt;/p&gt;&lt;p&gt;As businesses worldwide accelerate their adoption of large language models (LLMs) to improve their operations, a challenge has emerged: how to accurately gauge their effectiveness. Many existing benchmarks focus on academic or general knowledge tests, often limited to English and simple question and answer formats. This has created a gap that leaves enterprises without a reliable method for evaluating how an AI model will perform on complex, multilingual, and context-rich business tasks.&lt;/p&gt;&lt;p&gt;Samsung’s TRUEBench, short for Trustworthy Real-world Usage Evaluation Benchmark, has been developed to fill this void. It provides a comprehensive suite of metrics that assesses LLMs based on scenarios and tasks directly relevant to real-world corporate environments. The benchmark draws upon Samsung’s own extensive internal enterprise use of AI models, ensuring the evaluation criteria are grounded in genuine workplace demands.&lt;/p&gt;&lt;p&gt;The framework evaluates common enterprise functions such as creating content, analysing data, summarising lengthy documents, and translating materials. These are broken down into 10 distinct categories and 46 sub-categories, providing a granular view of an AI’s productivity capabilities.&lt;/p&gt;&lt;p&gt;“Samsung Research brings deep expertise and a competitive edge through its real-world AI experience,” said Paul (Kyungwhoon) Cheun, CTO of the DX Division at Samsung Electronics and Head of Samsung Research. “We expect TRUEBench to establish evaluation standards for productivity.”&lt;/p&gt;&lt;p&gt;To tackle the limitations of older benchmarks, TRUEBench is built upon a foundation of 2,485 diverse test sets spanning 12 different languages and supporting cross-linguistic scenarios. This multilingual approach is critical for global corporations where information flows across different regions. The test materials themselves reflect the variety of workplace requests, ranging from brief instructions of just eight characters to the complex analysis of documents exceeding 20,000 characters.&lt;/p&gt;&lt;p&gt;Samsung recognised that in a real business context, a user’s full intent is not always explicitly stated in their initial prompt. The benchmark is therefore designed to assess an AI model’s ability to understand and fulfil these implicit enterprise needs, moving beyond simple accuracy to a more nuanced measure of helpfulness and relevance.&lt;/p&gt;&lt;p&gt;To achieve this, Samsung Research developed a unique collaborative process between human experts and AI to create the productivity scoring criteria. Initially, human annotators establish the evaluation standards for a given task. An AI then reviews these standards, checking for potential errors, internal contradictions, or unnecessary constraints that might not reflect a realistic user expectation. Following the AI’s feedback, the human annotators refine the criteria. This iterative loop ensures the final evaluation standards are precise and reflective of a high-quality outcome.&lt;/p&gt;&lt;p&gt;This cross-verified process delivers an automated evaluation system that scores the performance of LLMs. By using AI to apply these refined criteria, the system minimises the subjective bias that can occur with human-only scoring, ensuring consistency and reliability across all tests. TRUEBench also employs a strict scoring model where an AI model must satisfy every condition associated with a test to receive a passing mark. This all or nothing approach for individual conditions enables a more detailed and exacting assessment of the performance of AI models across different enterprise tasks.&lt;/p&gt;&lt;p&gt;To boost transparency and encourage wider adoption, Samsung has made TRUEBench’s data samples and leaderboards publicly available on the global open-source platform Hugging Face. This allows developers, researchers, and enterprises to directly compare the productivity performance of up to five different AI models simultaneously. The platform provides a clear, at a glance overview of how various AIs stack up against each other on practical tasks.&lt;/p&gt;&lt;p&gt;As of writing, here are the top 20 models by overall ranking based on Samsung’s AI benchmark:&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Current top 20 models by overall ranking based on Samsung’s AI benchmark that assesses the real-world productivity of AI models in enterprise settings." class="wp-image-109591" height="322" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/09/image-15-1024x322.png" width="1024" /&gt;&lt;/figure&gt;&lt;p&gt;The full published data also includes the average length of the AI-generated responses. This allows for a simultaneous comparison of not only performance but also efficiency, a key consideration for businesses weighing operational costs and speed.&lt;/p&gt;&lt;p&gt;With the launch of TRUEBench, Samsung is not merely releasing another tool but is aiming to change how the industry thinks about AI performance. By moving the goalposts from abstract knowledge to tangible productivity, Samsung’s benchmark could play a role in helping organisations make better decisions about which enterprise AI models to integrate into their workflows and bridge the gap between an AI’s potential and its proven value.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Inside Huawei’s plan to make thousands of AI chips think like one computer&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Banner for the AI &amp;amp; Big Data Expo event series." class="wp-image-109137" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/samsung-benchmarks-real-productivity-enterprise-ai-models/</guid><pubDate>Thu, 25 Sep 2025 12:49:10 +0000</pubDate></item><item><title>[NEW] Pilots Wanted: Stream ‘Mecha BREAK’ on GeForce NOW (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/geforce-now-thursday-mecha-break/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Suit up and head for the cloud. &lt;i&gt;Mecha BREAK&lt;/i&gt;, the popular third-person shooter, is now available to stream on GeForce NOW with NVIDIA DLSS 4 technology.&lt;/p&gt;
&lt;p&gt;Catch it this week as part of 10 new titles joining the nearly 5,000 games supported on GeForce NOW, in addition to Capcom’s &lt;i&gt;Phoenix Wright: Ace Attorney Trilogy.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;This week, &lt;i&gt;Alan Wake 2&lt;/i&gt; and &lt;i&gt;S.T.A.L.K.E.R 2: Heart of Chornobyl&lt;/i&gt; join the lineup of GeForce RTX 5080-ready titles, both already available on the service. Look for the “GeForce RTX 5080 Ready” row in the app or check out the full list.&lt;/p&gt;
&lt;p&gt;Plus, LG is celebrating being the first to offer OLED TVs that can stream GeForce NOW natively at 4K 120 frames per second, powered by the new GeForce RTX 5080 servers. To mark the occasion, LG is giving away 100 one-month GeForce NOW Ultimate membership codes for free. Head over to the r/LG_UserHub Reddit page to enter.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Smashing Good Time&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Mecha BREAK&lt;/i&gt; is a multiplayer mech game where players can choose from diverse mechs, customize appearances and battle colossal war machines on treacherous terrain&lt;i&gt;. &lt;/i&gt;The game pits pilots against each other in high-octane, team-based mech battles across vast, destructible arenas. Pick a Striker, master its unique abilities and outplay rivals in aerial and ground combat for pulse-pounding victory. Whether gearing up for 3v3, 6v6 or chaotic PvPvE matches, there’s a mech for every play style — from agile snipers to heavy frontline brutes.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Mecha BREAK&lt;/em&gt; is now available for streaming on GeForce NOW, making it easy to jump into the action wherever, whenever. With DLSS 4, Ultimate members get to experience the crispest visuals and smoothest frame rates, all from the cloud.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Hold It!&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_85309"&gt;&lt;img alt="Phoenix Wright: Ace Attorney trilogy on GeForce NOW" class="size-large wp-image-85309" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/GFN_Thursday-Phoenix_Wright_Ace_Attorney_Trilogy-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-85309"&gt;&lt;em&gt;No objections to the cloud here.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;In Capcom’s iconic &lt;i&gt;Phoenix Wright: Ace Attorney Trilogy&lt;/i&gt;, players become Phoenix Wright and experience the thrill of battle in the fight to save their innocent clients in a court of law. Play all 14 episodes, spanning the first three games, in one gorgeous collection.&lt;/p&gt;
&lt;p&gt;Investigate crime scenes, gather evidence, cross-examine witnesses with razor-sharp wit and deliver iconic “Objection!” moments to uncover the truth. With a mix of quirky characters, twisting storylines and surprising humor, the trilogy captures everything fans love about the legendary courtroom adventure.&lt;/p&gt;
&lt;p&gt;On GeForce NOW, it’s never been easier to bring justice to the courtroom. Stream the &lt;i&gt;Phoenix Wright: Ace Attorney Trilogy&lt;/i&gt; across devices instantly, with crisp visuals and smooth performance that lets every “Hold it!” land with dramatic flair. Whether gamers are on the go or settled in, the courtroom is always in session.&lt;/p&gt;
&lt;p&gt;In addition, members can look for the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;ENDLESS Legend 2 &lt;/i&gt;(New release on Steam and Xbox, available on PC Game Pass, Sept. 22)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Renown&lt;/i&gt; (New release on Steam, Sept. 22)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Baby Steps &lt;/i&gt;(New release on Steam, Sept. 23)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Aztecs: The Last Sun &lt;/i&gt;(New release on Steam, Sept. 23)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;SWORN&lt;/i&gt; (New release on Xbox, available on PC Game Pass, Sept. 23)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Lost Rift &lt;/i&gt;(New release on Steam, Sept. 25)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;CloverPit &lt;/i&gt;(New release on Steam, Sept. 26)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Mecha BREAK&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Phoenix Wright: Ace Attorney Trilogy &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Predecessor &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;New GeForce RTX 5080-ready games:&lt;/p&gt;

&lt;p&gt;What are you planning to play this weekend? Let us know on X or in the comments below.&lt;/p&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p dir="ltr" lang="en"&gt;Giveaway alert 📣@LGGamingGlobal is celebrating being the world’s first OLED TVs to stream GeForce NOW natively at 4K 120 fps by giving away 100 one-month GeForce NOW Ultimate membership codes.&lt;/p&gt;
&lt;p&gt;Enter here 👉 https://t.co/Tdd9hxdp1t&lt;/p&gt;
&lt;p&gt;— 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) September 24, 2025&lt;/p&gt;&lt;/blockquote&gt;


		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Suit up and head for the cloud. &lt;i&gt;Mecha BREAK&lt;/i&gt;, the popular third-person shooter, is now available to stream on GeForce NOW with NVIDIA DLSS 4 technology.&lt;/p&gt;
&lt;p&gt;Catch it this week as part of 10 new titles joining the nearly 5,000 games supported on GeForce NOW, in addition to Capcom’s &lt;i&gt;Phoenix Wright: Ace Attorney Trilogy.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;This week, &lt;i&gt;Alan Wake 2&lt;/i&gt; and &lt;i&gt;S.T.A.L.K.E.R 2: Heart of Chornobyl&lt;/i&gt; join the lineup of GeForce RTX 5080-ready titles, both already available on the service. Look for the “GeForce RTX 5080 Ready” row in the app or check out the full list.&lt;/p&gt;
&lt;p&gt;Plus, LG is celebrating being the first to offer OLED TVs that can stream GeForce NOW natively at 4K 120 frames per second, powered by the new GeForce RTX 5080 servers. To mark the occasion, LG is giving away 100 one-month GeForce NOW Ultimate membership codes for free. Head over to the r/LG_UserHub Reddit page to enter.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Smashing Good Time&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Mecha BREAK&lt;/i&gt; is a multiplayer mech game where players can choose from diverse mechs, customize appearances and battle colossal war machines on treacherous terrain&lt;i&gt;. &lt;/i&gt;The game pits pilots against each other in high-octane, team-based mech battles across vast, destructible arenas. Pick a Striker, master its unique abilities and outplay rivals in aerial and ground combat for pulse-pounding victory. Whether gearing up for 3v3, 6v6 or chaotic PvPvE matches, there’s a mech for every play style — from agile snipers to heavy frontline brutes.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Mecha BREAK&lt;/em&gt; is now available for streaming on GeForce NOW, making it easy to jump into the action wherever, whenever. With DLSS 4, Ultimate members get to experience the crispest visuals and smoothest frame rates, all from the cloud.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Hold It!&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_85309"&gt;&lt;img alt="Phoenix Wright: Ace Attorney trilogy on GeForce NOW" class="size-large wp-image-85309" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/GFN_Thursday-Phoenix_Wright_Ace_Attorney_Trilogy-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-85309"&gt;&lt;em&gt;No objections to the cloud here.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;In Capcom’s iconic &lt;i&gt;Phoenix Wright: Ace Attorney Trilogy&lt;/i&gt;, players become Phoenix Wright and experience the thrill of battle in the fight to save their innocent clients in a court of law. Play all 14 episodes, spanning the first three games, in one gorgeous collection.&lt;/p&gt;
&lt;p&gt;Investigate crime scenes, gather evidence, cross-examine witnesses with razor-sharp wit and deliver iconic “Objection!” moments to uncover the truth. With a mix of quirky characters, twisting storylines and surprising humor, the trilogy captures everything fans love about the legendary courtroom adventure.&lt;/p&gt;
&lt;p&gt;On GeForce NOW, it’s never been easier to bring justice to the courtroom. Stream the &lt;i&gt;Phoenix Wright: Ace Attorney Trilogy&lt;/i&gt; across devices instantly, with crisp visuals and smooth performance that lets every “Hold it!” land with dramatic flair. Whether gamers are on the go or settled in, the courtroom is always in session.&lt;/p&gt;
&lt;p&gt;In addition, members can look for the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;ENDLESS Legend 2 &lt;/i&gt;(New release on Steam and Xbox, available on PC Game Pass, Sept. 22)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Renown&lt;/i&gt; (New release on Steam, Sept. 22)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Baby Steps &lt;/i&gt;(New release on Steam, Sept. 23)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Aztecs: The Last Sun &lt;/i&gt;(New release on Steam, Sept. 23)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;SWORN&lt;/i&gt; (New release on Xbox, available on PC Game Pass, Sept. 23)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Lost Rift &lt;/i&gt;(New release on Steam, Sept. 25)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;CloverPit &lt;/i&gt;(New release on Steam, Sept. 26)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Mecha BREAK&lt;/i&gt; (Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Phoenix Wright: Ace Attorney Trilogy &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Predecessor &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;New GeForce RTX 5080-ready games:&lt;/p&gt;

&lt;p&gt;What are you planning to play this weekend? Let us know on X or in the comments below.&lt;/p&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p dir="ltr" lang="en"&gt;Giveaway alert 📣@LGGamingGlobal is celebrating being the world’s first OLED TVs to stream GeForce NOW natively at 4K 120 fps by giving away 100 one-month GeForce NOW Ultimate membership codes.&lt;/p&gt;
&lt;p&gt;Enter here 👉 https://t.co/Tdd9hxdp1t&lt;/p&gt;
&lt;p&gt;— 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) September 24, 2025&lt;/p&gt;&lt;/blockquote&gt;


		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/geforce-now-thursday-mecha-break/</guid><pubDate>Thu, 25 Sep 2025 13:00:00 +0000</pubDate></item><item><title>[NEW] Databricks will bake OpenAI models into its products in $100M bet to spur enterprise adoption (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/25/databricks-will-bake-openai-models-into-its-products-in-100m-bet-to-spur-enterprise-adoption/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/12/GettyImages-2021258442.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Databricks said on Thursday that it is incorporating OpenAI’s models, including GPT-5, into its data platform as well as its AI product, Agent Bricks, as part of a $100 million multi-year deal that bets on the AI company’s ability to bring in enterprise customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The deal highlights the accelerating race to bring generative AI into the enterprise stack, as companies foresee demand for AI tools that can tap into corporate data securely.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Agent Bricks lets organizations build AI apps and agents on top of their enterprise data using a range of AI models. OpenAI’s latest models are now part of that menu — accessible in SQL or via API — and GPT-5 is being offered as a flagship model for Databricks customers. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The news comes nearly two months after Databricks added OpenAI’s open-weight models, gpt-oss 20B and gpt-oss 120B, to the platform. Agent Bricks can now measure how accurately the different models perform on specific tasks and fine-tunes accordingly to produce more tailored results.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our partnership with Databricks brings our most advanced models to where secure enterprise data already lives, making it easier for businesses to experiment, deploy, and scale AI agents with real impact,” Brad Lightcap, chief operating officer of OpenAI, said in a statement. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With this deal, Databricks is betting that enterprise customers will flock to OpenAI’s models. Under the terms, the company is on the hook to pay&lt;em&gt; &lt;/em&gt;a minimum of $100 million to OpenAI whether or not the AI firm’s models generate that much in revenue over the life of the deal. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Neither company disclosed how long the agreement will run. If revenue ends up exceeding $100 million, OpenAI will earn more, but if it falls short, Databricks will have to pay the full amount anyway. For Databricks, it means potential downside risk; for OpenAI, it means predictable income at a time when the company is trying to rapidly build out more data centers. &amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The deal is similar to the one Databricks reached with Anthropic earlier this year, setting a revenue target of $100 million over five years.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A Databricks spokesperson told TechCrunch that the firm has already seen overwhelming demand from customers, including Mastercard, for native access to OpenAI’s models on the platform.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/12/GettyImages-2021258442.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Databricks said on Thursday that it is incorporating OpenAI’s models, including GPT-5, into its data platform as well as its AI product, Agent Bricks, as part of a $100 million multi-year deal that bets on the AI company’s ability to bring in enterprise customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The deal highlights the accelerating race to bring generative AI into the enterprise stack, as companies foresee demand for AI tools that can tap into corporate data securely.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Agent Bricks lets organizations build AI apps and agents on top of their enterprise data using a range of AI models. OpenAI’s latest models are now part of that menu — accessible in SQL or via API — and GPT-5 is being offered as a flagship model for Databricks customers. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The news comes nearly two months after Databricks added OpenAI’s open-weight models, gpt-oss 20B and gpt-oss 120B, to the platform. Agent Bricks can now measure how accurately the different models perform on specific tasks and fine-tunes accordingly to produce more tailored results.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our partnership with Databricks brings our most advanced models to where secure enterprise data already lives, making it easier for businesses to experiment, deploy, and scale AI agents with real impact,” Brad Lightcap, chief operating officer of OpenAI, said in a statement. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With this deal, Databricks is betting that enterprise customers will flock to OpenAI’s models. Under the terms, the company is on the hook to pay&lt;em&gt; &lt;/em&gt;a minimum of $100 million to OpenAI whether or not the AI firm’s models generate that much in revenue over the life of the deal. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Neither company disclosed how long the agreement will run. If revenue ends up exceeding $100 million, OpenAI will earn more, but if it falls short, Databricks will have to pay the full amount anyway. For Databricks, it means potential downside risk; for OpenAI, it means predictable income at a time when the company is trying to rapidly build out more data centers. &amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The deal is similar to the one Databricks reached with Anthropic earlier this year, setting a revenue target of $100 million over five years.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A Databricks spokesperson told TechCrunch that the firm has already seen overwhelming demand from customers, including Mastercard, for native access to OpenAI’s models on the platform.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/25/databricks-will-bake-openai-models-into-its-products-in-100m-bet-to-spur-enterprise-adoption/</guid><pubDate>Thu, 25 Sep 2025 13:14:16 +0000</pubDate></item><item><title>[NEW] Less than 48 hours to grab your TechCrunch Disrupt 2025 ticket savings (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/25/these-are-the-last-2-days-for-techcrunch-disrupt-2025-ticket-savings/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Ticktock! Less than 2 days remain to secure savings of up to $668 on your &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; pass. This offer ends tomorrow, &lt;strong&gt;September 26, at 11:59 p.m. PT&lt;/strong&gt;. If you haven’t registered yet, now’s the moment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Join 10,000+ founders, investors, tech innovators, and visionaries at Disrupt 2025, taking place October 27–29 at Moscone West in San Francisco. From fundraising and product building to hiring, strategy insights, or discovering your next startup investment, Disrupt delivers for everyone at the forefront of tech. &lt;strong&gt;Grab your ticket now and save.&lt;/strong&gt;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 2 days left" class="wp-image-3011644" height="383" src="https://techcrunch.com/wp-content/uploads/2025/05/TC25_2Days-16X9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-wherever-you-are-on-your-journey-disrupt-accelerates-growth"&gt;Wherever you are on your journey, Disrupt accelerates growth&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="SAN FRANCISCO, CALIFORNIA - OCTOBER 28: (L-R) Connie Loizos, Editor-in-Chief &amp;amp; GM of TechCrunch, and Vinod Khosla, Founder of Khosla Ventures, speak onstage during TechCrunch Disrupt 2024 Day 1 at Moscone Center on October 28, 2024 in San Francisco, California. (Photo by Kimberly White/Getty Images for TechCrunch)" class="wp-image-2907129" height="453" src="https://techcrunch.com/wp-content/uploads/2024/10/GettyImages-2181599446.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White/Getty Images for TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Explore &lt;strong&gt;200+ sessions&lt;/strong&gt; across five industry stages, Q&amp;amp;A breakouts, and dynamic roundtables to give you the insights, inspiration, and actionable takeaways to make moves and connections — on your own terms. With content flowing all three days, there’s no shortage of opportunities to learn and act.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="view at TechCrunch Disrupt 2015" class="wp-image-1356940" height="454" src="https://techcrunch.com/wp-content/uploads/2016/07/21596646922_656ce9d531_k.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Jeff Bottari / Flickr &lt;span class="screen-reader-text"&gt;(opens in a new window)&lt;/span&gt;&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Don’t miss content you can’t get anywhere else. &lt;strong&gt;Startup Battlefield 200&lt;/strong&gt; takes the main stage with 20 TechCrunch-vetted early-stage startups competing for glory. Watch VCs critique pitches in real time, sharing candid insights on what makes a startup truly successful.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Salva Health Co-Founder &amp;amp; CEO Valentina Agudelo Vargas, winner of the Startup Battlefield 2024, poses onstage during TechCrunch Disrupt 2024 Day 3 at Moscone Center on October 30, 2024 in San Francisco." class="wp-image-2913234" height="453" src="https://techcrunch.com/wp-content/uploads/2024/11/54105085427_2cae9d0502_o.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;See demos and live product reveals all over the floor. The Expo Hall, the beating heart of Disrupt, brings attendees together to discover and interact with tomorrow’s innovations. With 100+ startups on display, it’s your go-to spot for hands-on exploration.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-968068" height="453" src="https://techcrunch.com/wp-content/uploads/2014/03/8697162730_891d808f92_o.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Build a year’s worth of connections in just a few days. Thousands of VCs, operators, innovators, and creators are here to meet, partner, or invest in you. Discover your next mentor, partner, or investment — whether in curated meetings or chance encounters that spark growth.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 class="wp-block-heading" id="h-don-t-wait-for-ticket-rates-to-increase"&gt;Don’t wait for ticket rates to increase&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;If you’re serious about accelerating your startup, refining your strategy, or simply catching up on what’s coming next in tech, this is one conference you can’t afford to skip. And if you’ve been a longtime TechCrunch reader, join us for the 20th anniversary of TechCrunch at Disrupt 2025.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Secure up to $668 in ticket savings before tomorrow ends.&lt;/strong&gt; Prices will jump on September 26 at 11:59 p.m. PT.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Ticktock! Less than 2 days remain to secure savings of up to $668 on your &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; pass. This offer ends tomorrow, &lt;strong&gt;September 26, at 11:59 p.m. PT&lt;/strong&gt;. If you haven’t registered yet, now’s the moment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Join 10,000+ founders, investors, tech innovators, and visionaries at Disrupt 2025, taking place October 27–29 at Moscone West in San Francisco. From fundraising and product building to hiring, strategy insights, or discovering your next startup investment, Disrupt delivers for everyone at the forefront of tech. &lt;strong&gt;Grab your ticket now and save.&lt;/strong&gt;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 2 days left" class="wp-image-3011644" height="383" src="https://techcrunch.com/wp-content/uploads/2025/05/TC25_2Days-16X9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-wherever-you-are-on-your-journey-disrupt-accelerates-growth"&gt;Wherever you are on your journey, Disrupt accelerates growth&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="SAN FRANCISCO, CALIFORNIA - OCTOBER 28: (L-R) Connie Loizos, Editor-in-Chief &amp;amp; GM of TechCrunch, and Vinod Khosla, Founder of Khosla Ventures, speak onstage during TechCrunch Disrupt 2024 Day 1 at Moscone Center on October 28, 2024 in San Francisco, California. (Photo by Kimberly White/Getty Images for TechCrunch)" class="wp-image-2907129" height="453" src="https://techcrunch.com/wp-content/uploads/2024/10/GettyImages-2181599446.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White/Getty Images for TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Explore &lt;strong&gt;200+ sessions&lt;/strong&gt; across five industry stages, Q&amp;amp;A breakouts, and dynamic roundtables to give you the insights, inspiration, and actionable takeaways to make moves and connections — on your own terms. With content flowing all three days, there’s no shortage of opportunities to learn and act.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="view at TechCrunch Disrupt 2015" class="wp-image-1356940" height="454" src="https://techcrunch.com/wp-content/uploads/2016/07/21596646922_656ce9d531_k.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Jeff Bottari / Flickr &lt;span class="screen-reader-text"&gt;(opens in a new window)&lt;/span&gt;&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Don’t miss content you can’t get anywhere else. &lt;strong&gt;Startup Battlefield 200&lt;/strong&gt; takes the main stage with 20 TechCrunch-vetted early-stage startups competing for glory. Watch VCs critique pitches in real time, sharing candid insights on what makes a startup truly successful.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Salva Health Co-Founder &amp;amp; CEO Valentina Agudelo Vargas, winner of the Startup Battlefield 2024, poses onstage during TechCrunch Disrupt 2024 Day 3 at Moscone Center on October 30, 2024 in San Francisco." class="wp-image-2913234" height="453" src="https://techcrunch.com/wp-content/uploads/2024/11/54105085427_2cae9d0502_o.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;See demos and live product reveals all over the floor. The Expo Hall, the beating heart of Disrupt, brings attendees together to discover and interact with tomorrow’s innovations. With 100+ startups on display, it’s your go-to spot for hands-on exploration.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-968068" height="453" src="https://techcrunch.com/wp-content/uploads/2014/03/8697162730_891d808f92_o.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Build a year’s worth of connections in just a few days. Thousands of VCs, operators, innovators, and creators are here to meet, partner, or invest in you. Discover your next mentor, partner, or investment — whether in curated meetings or chance encounters that spark growth.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 class="wp-block-heading" id="h-don-t-wait-for-ticket-rates-to-increase"&gt;Don’t wait for ticket rates to increase&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;If you’re serious about accelerating your startup, refining your strategy, or simply catching up on what’s coming next in tech, this is one conference you can’t afford to skip. And if you’ve been a longtime TechCrunch reader, join us for the 20th anniversary of TechCrunch at Disrupt 2025.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Secure up to $668 in ticket savings before tomorrow ends.&lt;/strong&gt; Prices will jump on September 26 at 11:59 p.m. PT.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/25/these-are-the-last-2-days-for-techcrunch-disrupt-2025-ticket-savings/</guid><pubDate>Thu, 25 Sep 2025 14:00:00 +0000</pubDate></item><item><title>[NEW] Clarifai’s new reasoning engine makes AI models faster and less expensive (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/25/clarifais-new-reasoning-engine-makes-ai-models-faster-and-less-expensive/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/03/GettyImages-1862779720.jpg?resize=1200,738" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Thursday, the AI platform Clarifai announced a new reasoning engine that it claims will make running AI models twice as fast and 40% less expensive. Designed to be adaptable to a variety of models and cloud hosts, the system employs a range of optimizations to get more inference power out of the same hardware.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s a variety of different types of optimizations, all the way down to CUDA kernels to advanced speculative decoding techniques,” said CEO Matthew Zeiler. “You can get more out of the same cards, basically.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The results were verified by a string of benchmark tests by the third-party firm Artificial Analysis, which recorded industry-best records for both throughput and latency.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The process focuses specifically on inference, the computing demands of operating an AI model that has already been trained. That computing load has grown particularly intense with the rise of agentic and reasoning models, which require multiple steps in response to a single command.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;First launched as a computer vision service, Clarifai has grown increasingly focused on compute orchestration as the AI boom has drastically increased demand for both GPUs and the data centers that house them. The company first announced its compute platform at AWS re:Invent in December, but the new reasoning engine is the first product specifically tailored for multi-step agentic models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The product comes amid intense pressure on AI infrastructure, which has spurred a string of billion-dollar deals. OpenAI has laid out plans for as much as $1 trillion in new data center spending, projecting nearly limitless future demand for compute. But while the hardware buildout has been intense, Clarifai’s CEO believes there is more to be done in optimizing the infrastructure we already have.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There’s software tricks that take a good model like this further, like the Clarifai reasoning engine,” Zeiler says, “but there’s also algorithm improvements that can help combat the need for gigawatt data centers. And I don’t think we’re at the end of the algorithm innovations.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/03/GettyImages-1862779720.jpg?resize=1200,738" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Thursday, the AI platform Clarifai announced a new reasoning engine that it claims will make running AI models twice as fast and 40% less expensive. Designed to be adaptable to a variety of models and cloud hosts, the system employs a range of optimizations to get more inference power out of the same hardware.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s a variety of different types of optimizations, all the way down to CUDA kernels to advanced speculative decoding techniques,” said CEO Matthew Zeiler. “You can get more out of the same cards, basically.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The results were verified by a string of benchmark tests by the third-party firm Artificial Analysis, which recorded industry-best records for both throughput and latency.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The process focuses specifically on inference, the computing demands of operating an AI model that has already been trained. That computing load has grown particularly intense with the rise of agentic and reasoning models, which require multiple steps in response to a single command.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;First launched as a computer vision service, Clarifai has grown increasingly focused on compute orchestration as the AI boom has drastically increased demand for both GPUs and the data centers that house them. The company first announced its compute platform at AWS re:Invent in December, but the new reasoning engine is the first product specifically tailored for multi-step agentic models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The product comes amid intense pressure on AI infrastructure, which has spurred a string of billion-dollar deals. OpenAI has laid out plans for as much as $1 trillion in new data center spending, projecting nearly limitless future demand for compute. But while the hardware buildout has been intense, Clarifai’s CEO believes there is more to be done in optimizing the infrastructure we already have.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There’s software tricks that take a good model like this further, like the Clarifai reasoning engine,” Zeiler says, “but there’s also algorithm improvements that can help combat the need for gigawatt data centers. And I don’t think we’re at the end of the algorithm innovations.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/25/clarifais-new-reasoning-engine-makes-ai-models-faster-and-less-expensive/</guid><pubDate>Thu, 25 Sep 2025 14:13:43 +0000</pubDate></item><item><title>[NEW] AI system learns from many types of scientific information and runs experiments to discover new materials (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/ai-system-learns-many-types-scientific-information-and-runs-experiments-discovering-new-materials-0925</link><description>&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Machine-learning models can speed up the discovery of new materials by making predictions and suggesting experiments. But most models today only consider a few specific types of data or variables. Compare that with human scientists, who work in a collaborative environment and consider experimental results, the broader scientific literature, imaging and structural analysis, personal experience or intuition, and input from colleagues and peer reviewers.&lt;/p&gt;&lt;p&gt;Now, MIT researchers have developed a method for optimizing materials recipes and planning experiments that incorporates information from diverse sources like insights from the literature, chemical compositions, microstructural images, and more. The approach is part of a new platform, named Copilot for Real-world Experimental Scientists (CRESt), that also uses robotic equipment for high-throughput materials testing, the results of which are fed back into large multimodal models to further optimize materials recipes.&lt;/p&gt;&lt;p&gt;Human researchers can converse with the system in natural language, with no coding required, and the system makes its own observations and hypotheses along the way. Cameras and visual language models also allow the system to monitor experiments, detect issues, and suggest corrections.&lt;/p&gt;&lt;p&gt;“In the field of AI for science, the key is designing new experiments,” says Ju Li, School of Engineering Carl Richard Soderberg Professor of Power Engineering. “We use multimodal feedback — for example information from previous literature on how palladium behaved in fuel cells at this temperature, and human feedback — to complement experimental data and design new experiments. We also use robots to synthesize and characterize the material’s structure and to test performance.”&lt;/p&gt;&lt;p&gt;The system is described in a paper published in &lt;em&gt;Nature&lt;/em&gt;. The researchers used CRESt to explore more than 900 chemistries and conduct 3,500 electrochemical tests, leading to the discovery of a catalyst material that delivered record power density in a fuel cell that runs on formate salt to produce electricity.&lt;/p&gt;&lt;p&gt;Joining Li on the paper as first authors are&amp;nbsp;PhD student Zhen Zhang, Zhichu Ren PhD ’24, PhD student Chia-Wei Hsu, and postdoc&amp;nbsp;Weibin Chen. Their coauthors are MIT Assistant Professor Iwnetim Abate; Associate Professor Pulkit Agrawal; JR East Professor of Engineering Yang Shao-Horn; MIT.nano researcher Aubrey Penn; Zhang-Wei Hong PhD ’25, Hongbin Xu PhD ’25; Daniel Zheng PhD ’25; MIT graduate students Shuhan Miao and Hugh Smith; MIT postdocs Yimeng Huang, Weiyin Chen, Yungsheng Tian, Yifan Gao, and Yaoshen Niu; former MIT postdoc Sipei Li; and collaborators including Chi-Feng Lee, Yu-Cheng Shao, Hsiao-Tsu Wang, and Ying-Rui Lu.&lt;/p&gt;        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-youtube-video paragraph--view-mode--default"&gt;
          

            
   

  &lt;div class="news-article--inline-video"&gt;
        &lt;div class="news-article--inline-video--container"&gt;

                             &lt;div class="news-article--inline-video--cover-image"&gt;
                      &lt;img alt="Video thumbnail" height="480" src="https://i1.ytimg.com/vi/0cV5zbilK4Y/maxresdefault.jpg" width="640" /&gt;
                    
          Play video
        &lt;/div&gt;
                

                      &lt;/div&gt;
          &lt;/div&gt;  
        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;&lt;strong&gt;A smarter system&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Materials science experiments can be time-consuming and expensive. They require researchers to carefully design workflows, make new material, and run a series of tests and analysis to understand what happened. Those results are then used to decide how to improve the material.&lt;/p&gt;&lt;p&gt;To improve the process, some researchers have turned to a machine-learning strategy known as active learning to make efficient use of previous experimental data points and explore or exploit those data. When paired with a statistical technique known as Bayesian optimization (BO), active learning has helped researchers identify new materials for things like batteries and advanced semiconductors.&lt;/p&gt;&lt;p&gt;“Bayesian optimization is like Netflix recommending the next movie to watch based on your viewing history, except instead it recommends the next experiment to do,” Li explains. “But basic Bayesian optimization is too simplistic. It uses a boxed-in design space, so if I say I’m going to use platinum, palladium, and iron, it only changes the ratio of those elements in this small space. But real materials have a lot more dependencies, and BO often gets lost.”&lt;/p&gt;&lt;p&gt;Most active learning approaches also rely on single data streams that don’t capture everything that goes on in an experiment. To equip computational systems with more human-like knowledge, while still taking advantage of the speed and control of automated systems, Li and his collaborators built CRESt.&lt;/p&gt;&lt;p&gt;CRESt’s robotic equipment includes a liquid-handling robot, a carbothermal shock system to rapidly synthesize materials, an automated electrochemical workstation for testing, characterization equipment including automated electron microscopy and optical microscopy, and auxiliary devices such as pumps and gas valves, which can also be remotely controlled.&amp;nbsp; Many processing parameters can also be tuned.&lt;/p&gt;&lt;p&gt;With the user interface, researchers can chat with CRESt and tell it to use active learning to find promising materials recipes for different projects. CRESt can include up to 20 precursor molecules and substrates into its recipe. To guide material designs, CRESt’s models search through scientific papers for descriptions of elements or precursor molecules that might be useful. When human researchers tell CRESt to pursue new recipes, it kicks off a robotic symphony of sample preparation, characterization, and testing. The researcher can also ask CRESt to perform image analysis from scanning electron microscopy imaging, X-ray diffraction, and other sources.&lt;/p&gt;&lt;p&gt;Information from those processes is used to train the active learning models, which use both literature knowledge and current experimental results to suggest further experiments and accelerate materials discovery.&lt;/p&gt;&lt;p&gt;“For each recipe we use previous literature text or databases, and it creates these huge representations of every recipe based on the previous knowledge base before even doing the experiment,” says Li. “We perform principal component analysis in this knowledge embedding space to get a reduced search space that captures most of the performance variability. Then we use Bayesian optimization in this reduced space to design the new experiment. After the new experiment, we feed newly acquired multimodal experimental data and human feedback into a large language model to augment the knowledgebase and redefine the reduced search space, which gives us a big boost in active learning efficiency.”&lt;/p&gt;&lt;p&gt;Materials science experiments can also face reproducibility challenges. To address the problem, CRESt monitors its experiments with cameras, looking for potential problems and suggesting solutions via text and voice to human researchers.&lt;/p&gt;&lt;p&gt;The researchers used CRESt to develop an electrode material for an advanced type of high-density fuel cell known as a direct formate fuel cell. After exploring more than 900 chemistries over three months, CRESt discovered a catalyst material made from eight elements that achieved a 9.3-fold improvement in power density per dollar over pure palladium, an expensive precious metal. In further tests, CRESTs material was used to deliver a record power density to a working direct formate fuel cell even though the cell contained just one-fourth of the precious metals of previous devices.&lt;/p&gt;&lt;p&gt;The results show the potential for CRESt to find solutions to real-world energy problems that have plagued the materials science and engineering community for decades.&lt;/p&gt;&lt;p&gt;“A significant challenge for fuel-cell catalysts is the use of precious metal,” says Zhang. “For fuel cells, researchers have used various precious metals like palladium and platinum. We used a multielement catalyst that also incorporates many other cheap elements to create the optimal coordination environment for catalytic activity and resistance to poisoning species such as carbon monoxide and adsorbed hydrogen atom. People have been searching low-cost options for many years. This system greatly accelerated our search for these catalysts.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A helpful assistant&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Early on, poor reproducibility emerged as a major problem that limited the researchers’ ability to perform their new active learning technique on experimental datasets. Material properties can be influenced by the way the precursors are mixed and processed, and any number of problems can subtly alter experimental conditions, requiring careful inspection to correct.&lt;/p&gt;&lt;p&gt;To partially automate the process, the researchers coupled computer vision and vision language models with domain knowledge from the scientific literature, which allowed the system to hypothesize sources of irreproducibility and propose solutions. For example, the models can notice when there’s a millimeter-sized deviation in a sample’s shape or when a pipette moves something out of place. The researchers incorporated some of the model’s suggestions, leading to improved consistency, suggesting the models already make good experimental assistants.&lt;/p&gt;&lt;p&gt;The researchers noted that humans still performed most of the debugging in their experiments.&lt;/p&gt;&lt;p&gt;“CREST is an assistant, not a replacement, for human researchers,” Li says. “Human researchers are still indispensable. In fact, we use natural language so the system can explain what it is doing and present observations and hypotheses. But this is a step toward more flexible, self-driving labs.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Machine-learning models can speed up the discovery of new materials by making predictions and suggesting experiments. But most models today only consider a few specific types of data or variables. Compare that with human scientists, who work in a collaborative environment and consider experimental results, the broader scientific literature, imaging and structural analysis, personal experience or intuition, and input from colleagues and peer reviewers.&lt;/p&gt;&lt;p&gt;Now, MIT researchers have developed a method for optimizing materials recipes and planning experiments that incorporates information from diverse sources like insights from the literature, chemical compositions, microstructural images, and more. The approach is part of a new platform, named Copilot for Real-world Experimental Scientists (CRESt), that also uses robotic equipment for high-throughput materials testing, the results of which are fed back into large multimodal models to further optimize materials recipes.&lt;/p&gt;&lt;p&gt;Human researchers can converse with the system in natural language, with no coding required, and the system makes its own observations and hypotheses along the way. Cameras and visual language models also allow the system to monitor experiments, detect issues, and suggest corrections.&lt;/p&gt;&lt;p&gt;“In the field of AI for science, the key is designing new experiments,” says Ju Li, School of Engineering Carl Richard Soderberg Professor of Power Engineering. “We use multimodal feedback — for example information from previous literature on how palladium behaved in fuel cells at this temperature, and human feedback — to complement experimental data and design new experiments. We also use robots to synthesize and characterize the material’s structure and to test performance.”&lt;/p&gt;&lt;p&gt;The system is described in a paper published in &lt;em&gt;Nature&lt;/em&gt;. The researchers used CRESt to explore more than 900 chemistries and conduct 3,500 electrochemical tests, leading to the discovery of a catalyst material that delivered record power density in a fuel cell that runs on formate salt to produce electricity.&lt;/p&gt;&lt;p&gt;Joining Li on the paper as first authors are&amp;nbsp;PhD student Zhen Zhang, Zhichu Ren PhD ’24, PhD student Chia-Wei Hsu, and postdoc&amp;nbsp;Weibin Chen. Their coauthors are MIT Assistant Professor Iwnetim Abate; Associate Professor Pulkit Agrawal; JR East Professor of Engineering Yang Shao-Horn; MIT.nano researcher Aubrey Penn; Zhang-Wei Hong PhD ’25, Hongbin Xu PhD ’25; Daniel Zheng PhD ’25; MIT graduate students Shuhan Miao and Hugh Smith; MIT postdocs Yimeng Huang, Weiyin Chen, Yungsheng Tian, Yifan Gao, and Yaoshen Niu; former MIT postdoc Sipei Li; and collaborators including Chi-Feng Lee, Yu-Cheng Shao, Hsiao-Tsu Wang, and Ying-Rui Lu.&lt;/p&gt;        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-youtube-video paragraph--view-mode--default"&gt;
          

            
   

  &lt;div class="news-article--inline-video"&gt;
        &lt;div class="news-article--inline-video--container"&gt;

                             &lt;div class="news-article--inline-video--cover-image"&gt;
                      &lt;img alt="Video thumbnail" height="480" src="https://i1.ytimg.com/vi/0cV5zbilK4Y/maxresdefault.jpg" width="640" /&gt;
                    
          Play video
        &lt;/div&gt;
                

                      &lt;/div&gt;
          &lt;/div&gt;  
        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;&lt;strong&gt;A smarter system&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Materials science experiments can be time-consuming and expensive. They require researchers to carefully design workflows, make new material, and run a series of tests and analysis to understand what happened. Those results are then used to decide how to improve the material.&lt;/p&gt;&lt;p&gt;To improve the process, some researchers have turned to a machine-learning strategy known as active learning to make efficient use of previous experimental data points and explore or exploit those data. When paired with a statistical technique known as Bayesian optimization (BO), active learning has helped researchers identify new materials for things like batteries and advanced semiconductors.&lt;/p&gt;&lt;p&gt;“Bayesian optimization is like Netflix recommending the next movie to watch based on your viewing history, except instead it recommends the next experiment to do,” Li explains. “But basic Bayesian optimization is too simplistic. It uses a boxed-in design space, so if I say I’m going to use platinum, palladium, and iron, it only changes the ratio of those elements in this small space. But real materials have a lot more dependencies, and BO often gets lost.”&lt;/p&gt;&lt;p&gt;Most active learning approaches also rely on single data streams that don’t capture everything that goes on in an experiment. To equip computational systems with more human-like knowledge, while still taking advantage of the speed and control of automated systems, Li and his collaborators built CRESt.&lt;/p&gt;&lt;p&gt;CRESt’s robotic equipment includes a liquid-handling robot, a carbothermal shock system to rapidly synthesize materials, an automated electrochemical workstation for testing, characterization equipment including automated electron microscopy and optical microscopy, and auxiliary devices such as pumps and gas valves, which can also be remotely controlled.&amp;nbsp; Many processing parameters can also be tuned.&lt;/p&gt;&lt;p&gt;With the user interface, researchers can chat with CRESt and tell it to use active learning to find promising materials recipes for different projects. CRESt can include up to 20 precursor molecules and substrates into its recipe. To guide material designs, CRESt’s models search through scientific papers for descriptions of elements or precursor molecules that might be useful. When human researchers tell CRESt to pursue new recipes, it kicks off a robotic symphony of sample preparation, characterization, and testing. The researcher can also ask CRESt to perform image analysis from scanning electron microscopy imaging, X-ray diffraction, and other sources.&lt;/p&gt;&lt;p&gt;Information from those processes is used to train the active learning models, which use both literature knowledge and current experimental results to suggest further experiments and accelerate materials discovery.&lt;/p&gt;&lt;p&gt;“For each recipe we use previous literature text or databases, and it creates these huge representations of every recipe based on the previous knowledge base before even doing the experiment,” says Li. “We perform principal component analysis in this knowledge embedding space to get a reduced search space that captures most of the performance variability. Then we use Bayesian optimization in this reduced space to design the new experiment. After the new experiment, we feed newly acquired multimodal experimental data and human feedback into a large language model to augment the knowledgebase and redefine the reduced search space, which gives us a big boost in active learning efficiency.”&lt;/p&gt;&lt;p&gt;Materials science experiments can also face reproducibility challenges. To address the problem, CRESt monitors its experiments with cameras, looking for potential problems and suggesting solutions via text and voice to human researchers.&lt;/p&gt;&lt;p&gt;The researchers used CRESt to develop an electrode material for an advanced type of high-density fuel cell known as a direct formate fuel cell. After exploring more than 900 chemistries over three months, CRESt discovered a catalyst material made from eight elements that achieved a 9.3-fold improvement in power density per dollar over pure palladium, an expensive precious metal. In further tests, CRESTs material was used to deliver a record power density to a working direct formate fuel cell even though the cell contained just one-fourth of the precious metals of previous devices.&lt;/p&gt;&lt;p&gt;The results show the potential for CRESt to find solutions to real-world energy problems that have plagued the materials science and engineering community for decades.&lt;/p&gt;&lt;p&gt;“A significant challenge for fuel-cell catalysts is the use of precious metal,” says Zhang. “For fuel cells, researchers have used various precious metals like palladium and platinum. We used a multielement catalyst that also incorporates many other cheap elements to create the optimal coordination environment for catalytic activity and resistance to poisoning species such as carbon monoxide and adsorbed hydrogen atom. People have been searching low-cost options for many years. This system greatly accelerated our search for these catalysts.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A helpful assistant&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Early on, poor reproducibility emerged as a major problem that limited the researchers’ ability to perform their new active learning technique on experimental datasets. Material properties can be influenced by the way the precursors are mixed and processed, and any number of problems can subtly alter experimental conditions, requiring careful inspection to correct.&lt;/p&gt;&lt;p&gt;To partially automate the process, the researchers coupled computer vision and vision language models with domain knowledge from the scientific literature, which allowed the system to hypothesize sources of irreproducibility and propose solutions. For example, the models can notice when there’s a millimeter-sized deviation in a sample’s shape or when a pipette moves something out of place. The researchers incorporated some of the model’s suggestions, leading to improved consistency, suggesting the models already make good experimental assistants.&lt;/p&gt;&lt;p&gt;The researchers noted that humans still performed most of the debugging in their experiments.&lt;/p&gt;&lt;p&gt;“CREST is an assistant, not a replacement, for human researchers,” Li says. “Human researchers are still indispensable. In fact, we use natural language so the system can explain what it is doing and present observations and hypotheses. But this is a step toward more flexible, self-driving labs.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/ai-system-learns-many-types-scientific-information-and-runs-experiments-discovering-new-materials-0925</guid><pubDate>Thu, 25 Sep 2025 15:00:00 +0000</pubDate></item><item><title>[NEW] What top VCs want from AI founders: Inside the investor lens with Jon McNeill, Aileen Lee, and Steve Jang at TechCrunch Disrupt 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/25/what-top-vcs-want-from-ai-founders-inside-the-investor-lens-with-jon-mcneill-aileen-lee-and-steve-jang-at-techcrunch-disrupt-2025/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;From model infrastructure to niche applications, AI is producing a new breed of founders and a new set of investor expectations. In this candid conversation on the &lt;strong&gt;AI Stage&lt;/strong&gt;, at &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; — taking place on October 27-29 in San Francisco — top VCs will share what’s catching their eye (and what’s not), how they’re thinking about defensibility in a world of AI monopolies, and what founders need to show to get that next term sheet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;If you are scaling or planning to scale&lt;/strong&gt; an AI startup, you cannot miss this session. Learn what it really takes to build a viable AI company. &lt;strong&gt;Register now and save up to $668&lt;/strong&gt; before savings end on September 26 at 11:59 p.m. PT.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Or, are you an investor who backs AI breakthroughs? &lt;strong&gt;See what an Investor Pass can do for you&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt Aileen Lee, Steve Jang, Jon McNeill" class="wp-image-3050145" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/TC25_McNeill-Lee-Jang-Speaker-16x9-Dark-1.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-who-s-speaking"&gt;Who’s speaking&lt;/h2&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Jon McNeill&lt;/strong&gt;, CEO and co-founder, DVx Ventures — Jon has launched 12 companies through his venture studio, built and scaled businesses delivering multifold returns, and led operations at Tesla and Lyft during periods of dramatic growth.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Aileen Lee&lt;/strong&gt;, founder and managing partner, Cowboy Ventures — Aileen invests in early-stage enterprise and consumer software companies like Dollar Shave Club, Drata Security, Guild, and Ironclad. She is the person who coined the term “unicorn,” co-founded All Raise, and is a longtime voice for women in tech.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Steve Jang&lt;/strong&gt;, founder and managing partner, Kindred Ventures — Steve is a Midas List investor in AI, decentralized systems, infrastructure, and frontier tech. His portfolio includes early bets in Uber, Coinbase, Perplexity, Fal, and PlayAI.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="SAN FRANCISCO, CALIFORNIA - SEPTEMBER 19: (L-R) Accel Partner Sara Ittelson, Kindred Ventures Founder &amp;amp; Managing Partner Steve Jang, and Breakthrough Energy Ventures Partner Libby Wayman speak onstage during TechCrunch Disrupt 2023 at Moscone Center on September 19, 2023 in San Francisco, California. (Photo by Kimberly White/Getty Images for TechCrunch)" class="wp-image-2602892" height="383" src="https://techcrunch.com/wp-content/uploads/2023/09/GettyImages-1691159208.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-what-you-ll-learn"&gt;What you’ll learn&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;This session&lt;/strong&gt; goes beyond pitch decks to reveal what investors are watching. Expect insights on:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;What makes an AI startup defensible in 2025.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Signals investors look for before writing the check.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;How to distinguish yourself in a crowded landscape.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 class="wp-block-heading" id="h-investor-clarity-for-ai-innovation"&gt;Investor clarity for AI innovation&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Join 10,000+ startup and VC leaders October 27-29 at San Francisco’s Moscone West. From deep technical sessions to founder stories, Disrupt 2025 is where you gain clarity for growth and opportunity. &lt;strong&gt;Register now to save up to $668&lt;/strong&gt; before prices increase after tomorrow, September 26.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;From model infrastructure to niche applications, AI is producing a new breed of founders and a new set of investor expectations. In this candid conversation on the &lt;strong&gt;AI Stage&lt;/strong&gt;, at &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; — taking place on October 27-29 in San Francisco — top VCs will share what’s catching their eye (and what’s not), how they’re thinking about defensibility in a world of AI monopolies, and what founders need to show to get that next term sheet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;If you are scaling or planning to scale&lt;/strong&gt; an AI startup, you cannot miss this session. Learn what it really takes to build a viable AI company. &lt;strong&gt;Register now and save up to $668&lt;/strong&gt; before savings end on September 26 at 11:59 p.m. PT.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Or, are you an investor who backs AI breakthroughs? &lt;strong&gt;See what an Investor Pass can do for you&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt Aileen Lee, Steve Jang, Jon McNeill" class="wp-image-3050145" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/TC25_McNeill-Lee-Jang-Speaker-16x9-Dark-1.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-who-s-speaking"&gt;Who’s speaking&lt;/h2&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Jon McNeill&lt;/strong&gt;, CEO and co-founder, DVx Ventures — Jon has launched 12 companies through his venture studio, built and scaled businesses delivering multifold returns, and led operations at Tesla and Lyft during periods of dramatic growth.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Aileen Lee&lt;/strong&gt;, founder and managing partner, Cowboy Ventures — Aileen invests in early-stage enterprise and consumer software companies like Dollar Shave Club, Drata Security, Guild, and Ironclad. She is the person who coined the term “unicorn,” co-founded All Raise, and is a longtime voice for women in tech.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Steve Jang&lt;/strong&gt;, founder and managing partner, Kindred Ventures — Steve is a Midas List investor in AI, decentralized systems, infrastructure, and frontier tech. His portfolio includes early bets in Uber, Coinbase, Perplexity, Fal, and PlayAI.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="SAN FRANCISCO, CALIFORNIA - SEPTEMBER 19: (L-R) Accel Partner Sara Ittelson, Kindred Ventures Founder &amp;amp; Managing Partner Steve Jang, and Breakthrough Energy Ventures Partner Libby Wayman speak onstage during TechCrunch Disrupt 2023 at Moscone Center on September 19, 2023 in San Francisco, California. (Photo by Kimberly White/Getty Images for TechCrunch)" class="wp-image-2602892" height="383" src="https://techcrunch.com/wp-content/uploads/2023/09/GettyImages-1691159208.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-what-you-ll-learn"&gt;What you’ll learn&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;This session&lt;/strong&gt; goes beyond pitch decks to reveal what investors are watching. Expect insights on:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;What makes an AI startup defensible in 2025.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Signals investors look for before writing the check.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;How to distinguish yourself in a crowded landscape.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 class="wp-block-heading" id="h-investor-clarity-for-ai-innovation"&gt;Investor clarity for AI innovation&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Join 10,000+ startup and VC leaders October 27-29 at San Francisco’s Moscone West. From deep technical sessions to founder stories, Disrupt 2025 is where you gain clarity for growth and opportunity. &lt;strong&gt;Register now to save up to $668&lt;/strong&gt; before prices increase after tomorrow, September 26.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/25/what-top-vcs-want-from-ai-founders-inside-the-investor-lens-with-jon-mcneill-aileen-lee-and-steve-jang-at-techcrunch-disrupt-2025/</guid><pubDate>Thu, 25 Sep 2025 15:00:00 +0000</pubDate></item><item><title>[NEW] Love, lies, and algorithms: Is AI really helping us find ‘the one’? Live at TechCrunch Disrupt 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/25/love-lies-and-algorithms-is-ai-really-helping-us-find-the-one/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;What happens when technology takes the wheel in our love lives?&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;From dating apps and AI-powered matchmaking to full-on digital companionship, artificial intelligence is rapidly becoming a third party in our most personal relationships. But is it truly helping us find deeper connection — or just reshaping romance into an algorithmic illusion?&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Only on the &lt;strong&gt;AI Stage&lt;/strong&gt; at &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, happening October 27-29 in San Francisco’s Moscone West, we’re bringing together three powerhouse voices to unpack the future of love, trust, and tech.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Dr. Amanda Gesselman, Mark Kantor, Eugenia Kuyda" class="wp-image-3048925" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/TC25_Gesselman-Kantor-Kuyda-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Eugenia Kuyda&lt;/strong&gt;, founder of Replika, the world’s leading AI companion platform with over 35 million users. Her mission: build AI that supports human emotional well-being — not replaces it.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Mark Kantor&lt;/strong&gt;, head of product at Tinder and former GM at Zynga. He’s seen how features can drive feelings — and how behavioral design shapes who we match, message, and meet.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Dr. Amanda Gesselman&lt;/strong&gt;, Kinsey Institute research scientist, whose work explores how digital intimacy, platforms, and emotional outsourcing are redefining relationships in the AI era.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 class="wp-block-heading" id="h-in-this-session-we-ll-dive-into"&gt;In this session, we’ll dive into:&lt;/h2&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;How recommendation engines shape who we date and why.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;The psychological impact of digital companionship and AI “love.”&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;What’s being lost — or gained — when intimacy is optimized.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Ethical red flags and design choices that deserve more scrutiny.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;Whether you’re building an AI product, navigating the dating landscape, or curious about the emotional side of emerging tech, this panel will challenge everything you think you know about connection in a digital-first world.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-don-t-miss-savings-of-up-to-668"&gt;Don’t miss savings of up to $668&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch Disrupt 2025 takes place October 27–29 at Moscone West in San Francisco, where the celebration of the 20th anniversary of TechCrunch happens. Savings ends September 26 at 11:59 p.m. PT. &lt;strong&gt;Save up to $668 when you register now&lt;/strong&gt; so you don’t miss this session and everything else happening across one of the largest tech epicenters of the year.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;What happens when technology takes the wheel in our love lives?&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;From dating apps and AI-powered matchmaking to full-on digital companionship, artificial intelligence is rapidly becoming a third party in our most personal relationships. But is it truly helping us find deeper connection — or just reshaping romance into an algorithmic illusion?&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Only on the &lt;strong&gt;AI Stage&lt;/strong&gt; at &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, happening October 27-29 in San Francisco’s Moscone West, we’re bringing together three powerhouse voices to unpack the future of love, trust, and tech.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Dr. Amanda Gesselman, Mark Kantor, Eugenia Kuyda" class="wp-image-3048925" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/TC25_Gesselman-Kantor-Kuyda-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Eugenia Kuyda&lt;/strong&gt;, founder of Replika, the world’s leading AI companion platform with over 35 million users. Her mission: build AI that supports human emotional well-being — not replaces it.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Mark Kantor&lt;/strong&gt;, head of product at Tinder and former GM at Zynga. He’s seen how features can drive feelings — and how behavioral design shapes who we match, message, and meet.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Dr. Amanda Gesselman&lt;/strong&gt;, Kinsey Institute research scientist, whose work explores how digital intimacy, platforms, and emotional outsourcing are redefining relationships in the AI era.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 class="wp-block-heading" id="h-in-this-session-we-ll-dive-into"&gt;In this session, we’ll dive into:&lt;/h2&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;How recommendation engines shape who we date and why.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;The psychological impact of digital companionship and AI “love.”&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;What’s being lost — or gained — when intimacy is optimized.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Ethical red flags and design choices that deserve more scrutiny.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;Whether you’re building an AI product, navigating the dating landscape, or curious about the emotional side of emerging tech, this panel will challenge everything you think you know about connection in a digital-first world.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-don-t-miss-savings-of-up-to-668"&gt;Don’t miss savings of up to $668&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch Disrupt 2025 takes place October 27–29 at Moscone West in San Francisco, where the celebration of the 20th anniversary of TechCrunch happens. Savings ends September 26 at 11:59 p.m. PT. &lt;strong&gt;Save up to $668 when you register now&lt;/strong&gt; so you don’t miss this session and everything else happening across one of the largest tech epicenters of the year.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/25/love-lies-and-algorithms-is-ai-really-helping-us-find-the-one/</guid><pubDate>Thu, 25 Sep 2025 15:30:00 +0000</pubDate></item><item><title>[NEW] Gemini Robotics 1.5 brings AI agents into the physical world (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://lh3.googleusercontent.com/0s-zLFC0-vi8maL45QgUS93WJwkDdrwItNJiaoHb4rh6_Eru1sg43UJEax5twdcItCR8lK8bW-0hqsPjsxe-O1dduxTdzsv_Av66SY1psiPuTnC4gg=w1200-h630-n-nu" /&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Acknowledgements&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;This work was developed by the Gemini Robotics team: Abbas Abdolmaleki, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Ashwin Balakrishna, Nathan Batchelor, Alex Bewley, Jeff Bingham, Michael Bloesch, Konstantinos Bousmalis, Philemon Brakel, Anthony Brohan, Thomas Buschmann, Arunkumar Byravan, Serkan Cabi, Ken Caluwaerts, Federico Casarini, Christine Chan, Oscar Chang, London Chappellet-Volpini, Jose Enrique Chen, Xi Chen, Hao-Tien Lewis Chiang, Krzysztof Choromanski, Adrian Collister, David B. D'Ambrosio, Sudeep Dasari, Todor Davchev, Meet Kirankumar Dave, Coline Devin, Norman Di Palo, Tianli Ding, Carl Doersch, Adil Dostmohamed, Yilun Du, Debidatta Dwibedi, Sathish Thoppay Egambaram, Michael Elabd, Tom Erez, Xiaolin Fang, Claudio Fantacci, Cody Fong, Erik Frey, Chuyuan Fu, Ruiqi Gao, Marissa Giustina, Keerthana Gopalakrishnan, Laura Graesser, Oliver Groth, Agrim Gupta, Roland Hafner, Steven Hansen, Leonard Hasenclever, Sam Haves, Nicolas Heess, Brandon Hernaez, Alex Hofer, Jasmine Hsu, Lu Huang, Sandy H. Huang, Atil Iscen, Mithun George Jacob, Deepali Jain, Sally Jesmonth, Abhishek Jindal, Ryan Julian, Dmitry Kalashnikov, Stefani Karp, Matija Kecman, J. Chase Kew, Donnie Kim, Frank Kim, Junkyung Kim, Thomas Kipf, Sean Kirmani, Ksenia Konyushkova, Yuheng Kuang, Thomas Lampe, Antoine Laurens, Tuan Anh Le, Isabel Leal, Alex X. Lee, Tsang-Wei Edward Lee, Guy Lever, Jacky Liang, Li-Heng Lin, Fangchen Liu, Shangbang Long, Caden Lu, Sharath Maddineni, Anirudha Majumdar, Kevis-Kokitsi Maninis, Andrew Marmon, Sergio Martinez, Assaf Hurwitz Michaely, Niko Milonopoulos, Joss Moore, Robert Moreno, Michael Neunert, Francesco Nori, Joy Ortiz, Kenneth Oslund, Carolina Parada, Emilio Parisotto, Peter Pastor Sampedro, Acorn Pooley, Thomas Power, Alessio Quaglino, Haroon Qureshi, Rajkumar Vasudeva Raju, Helen Ran, Dushyant Rao, Kanishka Rao, Isaac Reid, David Rendleman, Krista Reymann, Miguel Rivas, Francesco Romano, Yulia Rubanova, Pannag R Sanketi, Dhruv Shah, Mohit Sharma, Kathryn Shea, Mohit Shridhar, Charles Shu, Vikas Sindhwani, Sumeet Singh, Radu Soricut, Rachel Sterneck, Ian Storz, Razvan Surdulescu, Jie Tan, Jonathan Tompson, Saran Tunyasuvunakool, Jake Varley, Grace Vesom, Giulia Vezzani, Maria Bauza Villalonga, Oriol Vinyals, René Wagner, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Chengda Wu, Markus Wulfmeier, Fei Xia, Ted Xiao, Annie Xie, Jinyu Xie, Peng Xu, Sichun Xu, Ying Xu, Zhuo Xu, Jimmy Yan, Sherry Yang, Skye Yang, Yuxiang Yang, Hiu Hong Yu, Wenhao Yu, Li Yang Ku, Wentao Yuan, Yuan Yuan, Jingwei Zhang, Tingnan Zhang, Zhiyuan Zhang, Allan Zhou, Guangyao Zhou and Yuxiang Zhou.&lt;/p&gt;&lt;p&gt;We’d also like to thank: Amy Nommeots-Nomm, Ashley Gibb, Bhavya Sukhija, Bryan Gale, Catarina Barros, Christy Koh, Clara Barbu, Demetra Brady, Hiroki Furuta, Jennie Lees, Kendra Byrne, Keran Rong, Kevin Murphy, Kieran Connell, Kuang-Huei Lee, M. Emre Karagozler, Martina Zambelli, Matthew Jackson, Michael Noseworthy, Miguel Lázaro-Gredilla, Mili Sanwalka, Mimi Jasarevic, Nimrod Gileadi, Rebeca Santamaria-Fernandez, Rui Yao, Siobhan Mcloughlin, Sophie Bridgers, Stefano Saliceti, Steven Bohez, Svetlana Grant, Tim Hertweck, Verena Rieser, Yandong Ji.&lt;/p&gt;&lt;p&gt;For their leadership and support of this effort, we’d like to thank: Jean-Baptiste Alayrac, Zoubin Ghahramani, Koray Kavukcuoglu and Demis Hassabis. We’d like to recognize the many teams across Google and Google DeepMind that have contributed to this effort including Legal, Marketing, Communications, Responsibility and Safety Council, Responsible Development and Innovation, Policy, Strategy and Operations, and our Business and Corporate Development teams. We’d like to thank everyone on the Robotics team not explicitly mentioned above for their continued support and guidance. Finally, we’d like to thank the Apptronik team for their support.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://lh3.googleusercontent.com/0s-zLFC0-vi8maL45QgUS93WJwkDdrwItNJiaoHb4rh6_Eru1sg43UJEax5twdcItCR8lK8bW-0hqsPjsxe-O1dduxTdzsv_Av66SY1psiPuTnC4gg=w1200-h630-n-nu" /&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Acknowledgements&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;This work was developed by the Gemini Robotics team: Abbas Abdolmaleki, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Ashwin Balakrishna, Nathan Batchelor, Alex Bewley, Jeff Bingham, Michael Bloesch, Konstantinos Bousmalis, Philemon Brakel, Anthony Brohan, Thomas Buschmann, Arunkumar Byravan, Serkan Cabi, Ken Caluwaerts, Federico Casarini, Christine Chan, Oscar Chang, London Chappellet-Volpini, Jose Enrique Chen, Xi Chen, Hao-Tien Lewis Chiang, Krzysztof Choromanski, Adrian Collister, David B. D'Ambrosio, Sudeep Dasari, Todor Davchev, Meet Kirankumar Dave, Coline Devin, Norman Di Palo, Tianli Ding, Carl Doersch, Adil Dostmohamed, Yilun Du, Debidatta Dwibedi, Sathish Thoppay Egambaram, Michael Elabd, Tom Erez, Xiaolin Fang, Claudio Fantacci, Cody Fong, Erik Frey, Chuyuan Fu, Ruiqi Gao, Marissa Giustina, Keerthana Gopalakrishnan, Laura Graesser, Oliver Groth, Agrim Gupta, Roland Hafner, Steven Hansen, Leonard Hasenclever, Sam Haves, Nicolas Heess, Brandon Hernaez, Alex Hofer, Jasmine Hsu, Lu Huang, Sandy H. Huang, Atil Iscen, Mithun George Jacob, Deepali Jain, Sally Jesmonth, Abhishek Jindal, Ryan Julian, Dmitry Kalashnikov, Stefani Karp, Matija Kecman, J. Chase Kew, Donnie Kim, Frank Kim, Junkyung Kim, Thomas Kipf, Sean Kirmani, Ksenia Konyushkova, Yuheng Kuang, Thomas Lampe, Antoine Laurens, Tuan Anh Le, Isabel Leal, Alex X. Lee, Tsang-Wei Edward Lee, Guy Lever, Jacky Liang, Li-Heng Lin, Fangchen Liu, Shangbang Long, Caden Lu, Sharath Maddineni, Anirudha Majumdar, Kevis-Kokitsi Maninis, Andrew Marmon, Sergio Martinez, Assaf Hurwitz Michaely, Niko Milonopoulos, Joss Moore, Robert Moreno, Michael Neunert, Francesco Nori, Joy Ortiz, Kenneth Oslund, Carolina Parada, Emilio Parisotto, Peter Pastor Sampedro, Acorn Pooley, Thomas Power, Alessio Quaglino, Haroon Qureshi, Rajkumar Vasudeva Raju, Helen Ran, Dushyant Rao, Kanishka Rao, Isaac Reid, David Rendleman, Krista Reymann, Miguel Rivas, Francesco Romano, Yulia Rubanova, Pannag R Sanketi, Dhruv Shah, Mohit Sharma, Kathryn Shea, Mohit Shridhar, Charles Shu, Vikas Sindhwani, Sumeet Singh, Radu Soricut, Rachel Sterneck, Ian Storz, Razvan Surdulescu, Jie Tan, Jonathan Tompson, Saran Tunyasuvunakool, Jake Varley, Grace Vesom, Giulia Vezzani, Maria Bauza Villalonga, Oriol Vinyals, René Wagner, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Chengda Wu, Markus Wulfmeier, Fei Xia, Ted Xiao, Annie Xie, Jinyu Xie, Peng Xu, Sichun Xu, Ying Xu, Zhuo Xu, Jimmy Yan, Sherry Yang, Skye Yang, Yuxiang Yang, Hiu Hong Yu, Wenhao Yu, Li Yang Ku, Wentao Yuan, Yuan Yuan, Jingwei Zhang, Tingnan Zhang, Zhiyuan Zhang, Allan Zhou, Guangyao Zhou and Yuxiang Zhou.&lt;/p&gt;&lt;p&gt;We’d also like to thank: Amy Nommeots-Nomm, Ashley Gibb, Bhavya Sukhija, Bryan Gale, Catarina Barros, Christy Koh, Clara Barbu, Demetra Brady, Hiroki Furuta, Jennie Lees, Kendra Byrne, Keran Rong, Kevin Murphy, Kieran Connell, Kuang-Huei Lee, M. Emre Karagozler, Martina Zambelli, Matthew Jackson, Michael Noseworthy, Miguel Lázaro-Gredilla, Mili Sanwalka, Mimi Jasarevic, Nimrod Gileadi, Rebeca Santamaria-Fernandez, Rui Yao, Siobhan Mcloughlin, Sophie Bridgers, Stefano Saliceti, Steven Bohez, Svetlana Grant, Tim Hertweck, Verena Rieser, Yandong Ji.&lt;/p&gt;&lt;p&gt;For their leadership and support of this effort, we’d like to thank: Jean-Baptiste Alayrac, Zoubin Ghahramani, Koray Kavukcuoglu and Demis Hassabis. We’d like to recognize the many teams across Google and Google DeepMind that have contributed to this effort including Legal, Marketing, Communications, Responsibility and Safety Council, Responsible Development and Innovation, Policy, Strategy and Operations, and our Business and Corporate Development teams. We’d like to thank everyone on the Robotics team not explicitly mentioned above for their continued support and guidance. Finally, we’d like to thank the Apptronik team for their support.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/</guid><pubDate>Thu, 25 Sep 2025 15:59:29 +0000</pubDate></item><item><title>[NEW] Google DeepMind unveils its first “thinking” robotics AI (AI – Ars Technica)</title><link>https://arstechnica.com/google/2025/09/google-deepmind-unveils-its-first-thinking-robotics-ai/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        DeepMind researchers believe this is the dawn of agentic robots.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Gemini Robotics Apollo" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/Gemini-Robotics-1.5_Agentic-Capabilitiies-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Gemini Robotics Apollo" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/Gemini-Robotics-1.5_Agentic-Capabilitiies-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Generative AI systems that create text, images, audio, and even video are becoming commonplace. In the same way AI models output those data types, they can also be used to output robot actions. That's the foundation of Google DeepMind's Gemini Robotics project, which has announced a pair of new models that work together to create the first robots that "think" before acting. Traditional LLMs have their own set of problems, but the introduction of simulated reasoning did significantly upgrade their capabilities, and now the same could be happening with AI robotics.&lt;/p&gt;
&lt;p&gt;The team at DeepMind contends that generative AI is a uniquely important technology for robotics because it unlocks general functionality. Current robots have to be trained intensively on specific tasks, and they are typically bad at doing anything else. "Robots today are highly bespoke and difficult to deploy, often taking many months in order to install a single cell that can do a single task," said Carolina Parada, head of robotics at Google DeepMind.&lt;/p&gt;
&lt;p&gt;The fundamentals of generative systems make AI-powered robots more general. They can be presented with entirely new situations and workspaces without needing to be reprogrammed. DeepMind's current approach to robotics relies on two models: one that thinks and one that does.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Gemini Robotics 1.5: Learning Across Embodiments

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;The two new models are known as Gemini Robotics 1.5 and Gemini Robotics-ER 1.5. The former is a vision-language-action (VLA) model, meaning it uses visual and text data to generate robot actions. The "ER" in the other model stands for embodied reasoning. This is a vision-language model (VLM) that takes visual and text input to generate the steps needed to complete a complex task.&lt;/p&gt;
&lt;h2&gt;The thinking machines&lt;/h2&gt;
&lt;p&gt;Gemini Robotics-ER 1.5 is the first robotics AI capable of simulated reasoning like modern text-based chatbots—Google likes to call this "thinking," but that's a bit of a misnomer in the realm of generative AI. DeepMind says the ER model achieves top marks in both academic and internal benchmarks, which shows that it can make accurate decisions about how to interact with a physical space. It doesn't undertake any actions, though. That's where Gemini Robotics 1.5 comes in.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Imagine that you want a robot to sort a pile of laundry into whites and colors. Gemini Robotics-ER 1.5 would process the request along with images of the physical environment (a pile of clothing). This AI can also call tools like Google search to gather more data. The ER model then generates natural language instructions, specific steps that the robot should follow to complete the given task.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2118932 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Gemin iRobotics thinking" class="fullwidth full" height="1384" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GeminiRobotics-Agentic-System.png" width="2464" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The two new models work together to "think" about how to complete a task.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Gemini Robotics 1.5 (the action model) takes these instructions from the ER model and generates robot actions while using visual input to guide its movements. But it also goes through its own thinking process to consider how to approach each step. "There are all these kinds of intuitive thoughts that help [a person] guide this task, but robots don't have this intuition," said DeepMind's Kanishka Rao. "One of the major advancements that we've made with 1.5 in the VLA is its ability to think before it acts."&lt;/p&gt;
&lt;p&gt;Both of DeepMind's new robotic AIs are built on the Gemini foundation models but have been fine-tuned with data that adapts them to operating in a physical space. This approach, the team says, gives robots the ability to undertake more complex multi-stage tasks, bringing agentic capabilities to robotics.&lt;/p&gt;
&lt;p&gt;The DeepMind team tests Gemini robotics with a few different machines, like the two-armed Aloha 2 and the humanoid Apollo. In the past, AI researchers had to create customized models for each robot, but that's no longer necessary. DeepMind says that Gemini Robotics 1.5 can learn across different embodiments, transferring skills learned from Aloha 2's grippers to the more intricate hands on Apollo with no specialized tuning.&lt;/p&gt;
&lt;p&gt;All this talk of physical agents powered by AI is fun, but we're still a long way from a robot you can order to do your laundry. Gemini Robotics 1.5, the model that actually controls robots, is still only available to trusted testers. However, the thinking ER model is now rolling out in Google AI Studio, allowing developers to generate robotic instructions for their own physically embodied robotic experiments.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        DeepMind researchers believe this is the dawn of agentic robots.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Gemini Robotics Apollo" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/Gemini-Robotics-1.5_Agentic-Capabilitiies-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Gemini Robotics Apollo" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/Gemini-Robotics-1.5_Agentic-Capabilitiies-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Generative AI systems that create text, images, audio, and even video are becoming commonplace. In the same way AI models output those data types, they can also be used to output robot actions. That's the foundation of Google DeepMind's Gemini Robotics project, which has announced a pair of new models that work together to create the first robots that "think" before acting. Traditional LLMs have their own set of problems, but the introduction of simulated reasoning did significantly upgrade their capabilities, and now the same could be happening with AI robotics.&lt;/p&gt;
&lt;p&gt;The team at DeepMind contends that generative AI is a uniquely important technology for robotics because it unlocks general functionality. Current robots have to be trained intensively on specific tasks, and they are typically bad at doing anything else. "Robots today are highly bespoke and difficult to deploy, often taking many months in order to install a single cell that can do a single task," said Carolina Parada, head of robotics at Google DeepMind.&lt;/p&gt;
&lt;p&gt;The fundamentals of generative systems make AI-powered robots more general. They can be presented with entirely new situations and workspaces without needing to be reprogrammed. DeepMind's current approach to robotics relies on two models: one that thinks and one that does.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Gemini Robotics 1.5: Learning Across Embodiments

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;The two new models are known as Gemini Robotics 1.5 and Gemini Robotics-ER 1.5. The former is a vision-language-action (VLA) model, meaning it uses visual and text data to generate robot actions. The "ER" in the other model stands for embodied reasoning. This is a vision-language model (VLM) that takes visual and text input to generate the steps needed to complete a complex task.&lt;/p&gt;
&lt;h2&gt;The thinking machines&lt;/h2&gt;
&lt;p&gt;Gemini Robotics-ER 1.5 is the first robotics AI capable of simulated reasoning like modern text-based chatbots—Google likes to call this "thinking," but that's a bit of a misnomer in the realm of generative AI. DeepMind says the ER model achieves top marks in both academic and internal benchmarks, which shows that it can make accurate decisions about how to interact with a physical space. It doesn't undertake any actions, though. That's where Gemini Robotics 1.5 comes in.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Imagine that you want a robot to sort a pile of laundry into whites and colors. Gemini Robotics-ER 1.5 would process the request along with images of the physical environment (a pile of clothing). This AI can also call tools like Google search to gather more data. The ER model then generates natural language instructions, specific steps that the robot should follow to complete the given task.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2118932 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Gemin iRobotics thinking" class="fullwidth full" height="1384" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GeminiRobotics-Agentic-System.png" width="2464" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The two new models work together to "think" about how to complete a task.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Gemini Robotics 1.5 (the action model) takes these instructions from the ER model and generates robot actions while using visual input to guide its movements. But it also goes through its own thinking process to consider how to approach each step. "There are all these kinds of intuitive thoughts that help [a person] guide this task, but robots don't have this intuition," said DeepMind's Kanishka Rao. "One of the major advancements that we've made with 1.5 in the VLA is its ability to think before it acts."&lt;/p&gt;
&lt;p&gt;Both of DeepMind's new robotic AIs are built on the Gemini foundation models but have been fine-tuned with data that adapts them to operating in a physical space. This approach, the team says, gives robots the ability to undertake more complex multi-stage tasks, bringing agentic capabilities to robotics.&lt;/p&gt;
&lt;p&gt;The DeepMind team tests Gemini robotics with a few different machines, like the two-armed Aloha 2 and the humanoid Apollo. In the past, AI researchers had to create customized models for each robot, but that's no longer necessary. DeepMind says that Gemini Robotics 1.5 can learn across different embodiments, transferring skills learned from Aloha 2's grippers to the more intricate hands on Apollo with no specialized tuning.&lt;/p&gt;
&lt;p&gt;All this talk of physical agents powered by AI is fun, but we're still a long way from a robot you can order to do your laundry. Gemini Robotics 1.5, the model that actually controls robots, is still only available to trusted testers. However, the thinking ER model is now rolling out in Google AI Studio, allowing developers to generate robotic instructions for their own physically embodied robotic experiments.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2025/09/google-deepmind-unveils-its-first-thinking-robotics-ai/</guid><pubDate>Thu, 25 Sep 2025 16:00:59 +0000</pubDate></item><item><title>[NEW] Elon Musk’s xAI offers Grok to federal government for 42 cents (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/25/elon-musks-xai-offers-grok-to-federal-government-for-42-cents/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2218892225.jpg?resize=1200,804" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Elon Musk’s xAI has reached an agreement with the U.S. government’s purchasing arm to sell its AI chatbot Grok to the federal government for under a dollar, pitting it against OpenAI and Anthropic.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Under the agreement between xAI and the General Services Administration (GSA), federal agencies will be charged 42 cents to use xAI’s chatbot Grok for a year and a half. OpenAI and Anthropic are offering their enterprise and government versions of ChatGPT and Claude, respectively, for $1 for a year. &amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The steep discount for federal agencies includes access to xAI engineers to help integrate the technology.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The price point is either part of a running joke Musk has of using variations of 420, a marijuana reference, or a nod to one of Musk’s favorite books, “The Hitchhiker’s Guide to the Galaxy,” which references the number 42 as the answer to the meaning of life and the universe. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this year, xAI had been close to being approved as a GSA vendor, but after Grok began generating antisemitic posts and calling itself “MechaHitler” on X, the planned partnership reportedly fell through. In late August, internal emails obtained by Wired revealed the White House had instructed the GSA to add xAI’s Grok to the approved vendor list “ASAP.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company was also one of several AI firms, including Anthropic, Google, and OpenAI, to be selected for a $200 million contract with the Pentagon.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After President Donald Trump’s inauguration, Musk formed and led the Department of Government Efficiency, or DOGE, in a rampant cost-cutting spree that has seen mixed results. During that time, Musk placed several aides at the GSA and other government agencies responsible for regulating or awarding government contracts in industries in which Musk has business.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2218892225.jpg?resize=1200,804" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Elon Musk’s xAI has reached an agreement with the U.S. government’s purchasing arm to sell its AI chatbot Grok to the federal government for under a dollar, pitting it against OpenAI and Anthropic.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Under the agreement between xAI and the General Services Administration (GSA), federal agencies will be charged 42 cents to use xAI’s chatbot Grok for a year and a half. OpenAI and Anthropic are offering their enterprise and government versions of ChatGPT and Claude, respectively, for $1 for a year. &amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The steep discount for federal agencies includes access to xAI engineers to help integrate the technology.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The price point is either part of a running joke Musk has of using variations of 420, a marijuana reference, or a nod to one of Musk’s favorite books, “The Hitchhiker’s Guide to the Galaxy,” which references the number 42 as the answer to the meaning of life and the universe. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this year, xAI had been close to being approved as a GSA vendor, but after Grok began generating antisemitic posts and calling itself “MechaHitler” on X, the planned partnership reportedly fell through. In late August, internal emails obtained by Wired revealed the White House had instructed the GSA to add xAI’s Grok to the approved vendor list “ASAP.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company was also one of several AI firms, including Anthropic, Google, and OpenAI, to be selected for a $200 million contract with the Pentagon.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After President Donald Trump’s inauguration, Musk formed and led the Department of Government Efficiency, or DOGE, in a rampant cost-cutting spree that has seen mixed results. During that time, Musk placed several aides at the GSA and other government agencies responsible for regulating or awarding government contracts in industries in which Musk has business.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/25/elon-musks-xai-offers-grok-to-federal-government-for-42-cents/</guid><pubDate>Thu, 25 Sep 2025 16:06:15 +0000</pubDate></item><item><title>[NEW] OpenAI says GPT-5 stacks up to humans in a wide range of jobs (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/25/openai-says-gpt-5-stacks-up-to-humans-in-a-wide-range-of-jobs/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI released a new benchmark on Thursday that tests how its AI models perform compared to human professionals across a wide range of industries and jobs. The test, GDPval, is an early attempt at understanding how close OpenAI’s systems are to outperforming humans at economically valuable work — a key part of the company’s founding mission to develop artificial general intelligence, or AGI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI says its found that its GPT-5 model and Anthropic’s Claude Opus 4.1 “are already approaching the quality of work produced by industry experts.” &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That’s not to say that OpenAI’s models are going to start replacing humans in their jobs immediately. Despite predictions by some CEOs that AI will take the jobs of humans in just a few years, OpenAI admits that GDPval today covers a very limited number of tasks people do in their real jobs. However, it is one of the latest ways the company is measuring AI’s progress toward this milestone.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GDPval is based on nine industries that contribute the most to America’s gross domestic product, including domains such as healthcare, finance, manufacturing, and government. The benchmark tests an AI model’s performance in 44 occupations among those industries, ranging from software engineers to nurses to journalists.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For OpenAI’s first version of the test, GDPval-v0, OpenAI asked experienced professionals to compare AI-generated reports with those produced by other professionals, and then choose the best one. For example, one prompt asked investment bankers to create a competitor landscape for the last-mile delivery industry and compare them to AI-generated reports. OpenAI then averages an AI model’s “win rate” against the human reports across all 44 occupations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For GPT-5-high, a souped-up version of GPT-5 with extra computational power, the company says the AI model was ranked as better than or on par with industry experts 40.6% of the time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI also tested Anthropic’s Claude Opus 4.1 model, which was ranked as better than or on par with industry experts in 49% of tasks. OpenAI says that it believes Claude scored so high because of its tendency to make pleasing graphics, rather than sheer performance.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3050395" height="493" src="https://techcrunch.com/wp-content/uploads/2025/09/Screenshot-2025-09-25-at-9.10.47AM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;OpenAI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;It’s worth noting that most working professionals do a lot more than submit research reports to their boss, which is all that GDPval-v0 tests for. OpenAI acknowledges this and says it plans to create more robust tests in the future that can account for more industries and interactive workflows.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nonetheless, the company sees the progress on GDPval as notable.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In an interview with TechCrunch, OpenAI’s chief economist Dr. Aaron Chatterji said GDPval’s results suggest that people in these jobs can now use AI models to spend time on more meaningful tasks.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“[Because] the model is getting good at some of these things,” Chatterji says, “people in those jobs can now use the model, increasingly as capabilities get better, to offload some of their work and do potentially higher value things.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s evaluations lead Tejal Patwardhan tells TechCrunch that she’s encouraged by the rate of progress on GDPval. OpenAI’s GPT-4o model scored just 13.7% (wins and ties versus humans), which was released roughly 15 months ago. Now GPT-5 scores nearly triple that, a trend Patwardhan expects to continue.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Silicon Valley has a wide range of benchmarks it uses to measure the progress of AI models and assess whether a given model is state-of-the-art. Among the most popular are AIME 2025 (a test of competitive math problems) and GPQA Diamond (a test of PhD-level science questions). However, several AI models are nearing saturation on some of these benchmarks, and many AI researchers have cited the need for better tests that can measure AI’s proficiency on real-world tasks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Benchmarks like GDPval could become increasingly important in that conversation, as OpenAI makes the case that its AI models are valuable for a wide range of industries. But OpenAI may need a more comprehensive version of the test to definitively say its AI models can outperform humans.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI released a new benchmark on Thursday that tests how its AI models perform compared to human professionals across a wide range of industries and jobs. The test, GDPval, is an early attempt at understanding how close OpenAI’s systems are to outperforming humans at economically valuable work — a key part of the company’s founding mission to develop artificial general intelligence, or AGI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI says its found that its GPT-5 model and Anthropic’s Claude Opus 4.1 “are already approaching the quality of work produced by industry experts.” &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That’s not to say that OpenAI’s models are going to start replacing humans in their jobs immediately. Despite predictions by some CEOs that AI will take the jobs of humans in just a few years, OpenAI admits that GDPval today covers a very limited number of tasks people do in their real jobs. However, it is one of the latest ways the company is measuring AI’s progress toward this milestone.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GDPval is based on nine industries that contribute the most to America’s gross domestic product, including domains such as healthcare, finance, manufacturing, and government. The benchmark tests an AI model’s performance in 44 occupations among those industries, ranging from software engineers to nurses to journalists.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For OpenAI’s first version of the test, GDPval-v0, OpenAI asked experienced professionals to compare AI-generated reports with those produced by other professionals, and then choose the best one. For example, one prompt asked investment bankers to create a competitor landscape for the last-mile delivery industry and compare them to AI-generated reports. OpenAI then averages an AI model’s “win rate” against the human reports across all 44 occupations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For GPT-5-high, a souped-up version of GPT-5 with extra computational power, the company says the AI model was ranked as better than or on par with industry experts 40.6% of the time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI also tested Anthropic’s Claude Opus 4.1 model, which was ranked as better than or on par with industry experts in 49% of tasks. OpenAI says that it believes Claude scored so high because of its tendency to make pleasing graphics, rather than sheer performance.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3050395" height="493" src="https://techcrunch.com/wp-content/uploads/2025/09/Screenshot-2025-09-25-at-9.10.47AM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;OpenAI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;It’s worth noting that most working professionals do a lot more than submit research reports to their boss, which is all that GDPval-v0 tests for. OpenAI acknowledges this and says it plans to create more robust tests in the future that can account for more industries and interactive workflows.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nonetheless, the company sees the progress on GDPval as notable.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In an interview with TechCrunch, OpenAI’s chief economist Dr. Aaron Chatterji said GDPval’s results suggest that people in these jobs can now use AI models to spend time on more meaningful tasks.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“[Because] the model is getting good at some of these things,” Chatterji says, “people in those jobs can now use the model, increasingly as capabilities get better, to offload some of their work and do potentially higher value things.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s evaluations lead Tejal Patwardhan tells TechCrunch that she’s encouraged by the rate of progress on GDPval. OpenAI’s GPT-4o model scored just 13.7% (wins and ties versus humans), which was released roughly 15 months ago. Now GPT-5 scores nearly triple that, a trend Patwardhan expects to continue.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Silicon Valley has a wide range of benchmarks it uses to measure the progress of AI models and assess whether a given model is state-of-the-art. Among the most popular are AIME 2025 (a test of competitive math problems) and GPQA Diamond (a test of PhD-level science questions). However, several AI models are nearing saturation on some of these benchmarks, and many AI researchers have cited the need for better tests that can measure AI’s proficiency on real-world tasks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Benchmarks like GDPval could become increasingly important in that conversation, as OpenAI makes the case that its AI models are valuable for a wide range of industries. But OpenAI may need a more comprehensive version of the test to definitively say its AI models can outperform humans.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/25/openai-says-gpt-5-stacks-up-to-humans-in-a-wide-range-of-jobs/</guid><pubDate>Thu, 25 Sep 2025 16:11:34 +0000</pubDate></item><item><title>[NEW] Steph Curry’s VC firm just backed an AI startup that wants to fix food supply chains (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/25/steph-currys-vc-firm-just-backed-an-ai-startup-that-wants-to-fix-food-supply-chains/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/7a2af4da-c373-4aff-8bd5-3f7a68ac26d8_4b1d70.jpeg?w=720" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Food supply chains are notoriously messy. Orders arrive through different channels, staff spend hours manually entering them into clunky enterprise software systems, and compliance often depends on spreadsheets.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For decades, software vendors have tried, with mixed success, to modernize the workflows behind the global movement of perishable goods.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Now, a Y Combinator startup called Burnt thinks AI agents — software that can automatically handle tasks typically done by humans — can succeed where traditional enterprise software hasn’t in the trillion-dollar U.S. food market.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company, which automates back-office supply chain tasks with AI, has raised $3.8 million in seed funding led by Penny Jar Capital, the venture firm backed by NBA star Steph Curry, with participation from Scribble Ventures, Formation VC, and angel investors, including Dan Scheinman.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Burnt co-founder and CEO Joseph Jacob grew up around food factories. He says his great-grandfather was the first to export shrimp from India to the U.S. in the 1930s. Since then, each generation of his family has worked somewhere along the seafood supply chain, including farming, processing, exporting, and importing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jacob moved to India during his formative years and, after college, worked on the factory floor of a shrimp processor in a rural area. The experience introduced him to the intricacies of the food and restaurant business.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When he returned to the U.S. and began managing large volumes of seafood imports, he noticed major inefficiencies.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“I ended up buying hundreds of millions of pounds of seafood, but everything was tracked on Excel sheets and a 20-year-old ERP system,” Jacob told TechCrunch. “In a business with razor-thin margins, it’s nearly impossible to succeed without good supply chain management. We went through multiple software implementations, but two rollouts failed. That’s when I realized I wanted to build software for this industry, not just work in it.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jacob’s experience isn’t an isolated one. Enterprise vendors have long tried to sell distributors on large rollouts that drag on for years, cost millions, and frustrate the small and mid-sized players that dominate the market. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After two decades of missed software adoption in the industry, Jacob believes&lt;strong&gt; &lt;/strong&gt;Burnt’s approach of layering AI agents on top of existing systems rather than replacing them represents a massive opportunity.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Everyone we talk to calls their ERP a necessary evil,” said the chief executive. “Traditional software forced teams to rip out old processes and adopt new ones. With AI, you don’t need to change the process; you just get the work done.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Here’s how things tend to work today: Sales reps at food distributors receive orders via email, phone calls, WhatsApp, voicemails, texts, and even faxes. Each order then has to be keyed in manually. While critical, the process eats up hours that could be spent on higher-value work like winning new customers or upselling existing ones.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Burnt’s first agent, Ozai, automates and manages this order-entry process. In fact, Jacob claims it can handle up to 80% of workflows that are currently stuck in legacy systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since launching in January, the startup has processed more than $10 million in monthly orders across seafood, specialty goods, and packaged food distributors. One of the U.K.’s largest food conglomerates, with billions in revenue, is currently implementing Burnt’s system. The company is already generating six-figure revenue and growing “steadily” month-on-month, though Jacob declined to share exact numbers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While building AI for food supply chains may sound unglamorous, Jacob says that’s the point. He argues that decades of failed tech rollouts have left operators skeptical of “tech tourists” with no industry experience.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His background, as well as that of his co-founders, has helped Burnt gain trust in a sector where relationships matter. Chief Product Officer Rhea Karimpanal — Jacob’s childhood friend and now wife — comes from a family that ran restaurants, while CTO Chandru Shanmugasundaram built software systems for restaurant applications.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jacob previously worked at Rekki, a Benchmark-backed B2B marketplace for restaurants and suppliers, where he saw firsthand how brittle supply chain tech could be and how AI might transform it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, winning investors wasn’t straightforward. AI agents may be hot, but convincing VCs to back one for food distributors required a different pitch. Many lacked conviction in the market despite its size, he said.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That’s where Curry’s Penny Jar Capital came in. The firm’s thesis is centered on backing founders who are building in “overlooked” industries where tech adoption lags.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Two decades of missed software adoption is a massive opportunity. Investors who understand this know it can be huge if executed right,” Jacob said.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/7a2af4da-c373-4aff-8bd5-3f7a68ac26d8_4b1d70.jpeg?w=720" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Food supply chains are notoriously messy. Orders arrive through different channels, staff spend hours manually entering them into clunky enterprise software systems, and compliance often depends on spreadsheets.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For decades, software vendors have tried, with mixed success, to modernize the workflows behind the global movement of perishable goods.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Now, a Y Combinator startup called Burnt thinks AI agents — software that can automatically handle tasks typically done by humans — can succeed where traditional enterprise software hasn’t in the trillion-dollar U.S. food market.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company, which automates back-office supply chain tasks with AI, has raised $3.8 million in seed funding led by Penny Jar Capital, the venture firm backed by NBA star Steph Curry, with participation from Scribble Ventures, Formation VC, and angel investors, including Dan Scheinman.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Burnt co-founder and CEO Joseph Jacob grew up around food factories. He says his great-grandfather was the first to export shrimp from India to the U.S. in the 1930s. Since then, each generation of his family has worked somewhere along the seafood supply chain, including farming, processing, exporting, and importing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jacob moved to India during his formative years and, after college, worked on the factory floor of a shrimp processor in a rural area. The experience introduced him to the intricacies of the food and restaurant business.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When he returned to the U.S. and began managing large volumes of seafood imports, he noticed major inefficiencies.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“I ended up buying hundreds of millions of pounds of seafood, but everything was tracked on Excel sheets and a 20-year-old ERP system,” Jacob told TechCrunch. “In a business with razor-thin margins, it’s nearly impossible to succeed without good supply chain management. We went through multiple software implementations, but two rollouts failed. That’s when I realized I wanted to build software for this industry, not just work in it.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jacob’s experience isn’t an isolated one. Enterprise vendors have long tried to sell distributors on large rollouts that drag on for years, cost millions, and frustrate the small and mid-sized players that dominate the market. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After two decades of missed software adoption in the industry, Jacob believes&lt;strong&gt; &lt;/strong&gt;Burnt’s approach of layering AI agents on top of existing systems rather than replacing them represents a massive opportunity.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Everyone we talk to calls their ERP a necessary evil,” said the chief executive. “Traditional software forced teams to rip out old processes and adopt new ones. With AI, you don’t need to change the process; you just get the work done.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Here’s how things tend to work today: Sales reps at food distributors receive orders via email, phone calls, WhatsApp, voicemails, texts, and even faxes. Each order then has to be keyed in manually. While critical, the process eats up hours that could be spent on higher-value work like winning new customers or upselling existing ones.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Burnt’s first agent, Ozai, automates and manages this order-entry process. In fact, Jacob claims it can handle up to 80% of workflows that are currently stuck in legacy systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since launching in January, the startup has processed more than $10 million in monthly orders across seafood, specialty goods, and packaged food distributors. One of the U.K.’s largest food conglomerates, with billions in revenue, is currently implementing Burnt’s system. The company is already generating six-figure revenue and growing “steadily” month-on-month, though Jacob declined to share exact numbers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While building AI for food supply chains may sound unglamorous, Jacob says that’s the point. He argues that decades of failed tech rollouts have left operators skeptical of “tech tourists” with no industry experience.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His background, as well as that of his co-founders, has helped Burnt gain trust in a sector where relationships matter. Chief Product Officer Rhea Karimpanal — Jacob’s childhood friend and now wife — comes from a family that ran restaurants, while CTO Chandru Shanmugasundaram built software systems for restaurant applications.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jacob previously worked at Rekki, a Benchmark-backed B2B marketplace for restaurants and suppliers, where he saw firsthand how brittle supply chain tech could be and how AI might transform it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, winning investors wasn’t straightforward. AI agents may be hot, but convincing VCs to back one for food distributors required a different pitch. Many lacked conviction in the market despite its size, he said.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That’s where Curry’s Penny Jar Capital came in. The firm’s thesis is centered on backing founders who are building in “overlooked” industries where tech adoption lags.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Two decades of missed software adoption is a massive opportunity. Investors who understand this know it can be huge if executed right,” Jacob said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/25/steph-currys-vc-firm-just-backed-an-ai-startup-that-wants-to-fix-food-supply-chains/</guid><pubDate>Thu, 25 Sep 2025 16:30:04 +0000</pubDate></item><item><title>[NEW] Juicebox raises $30M from Sequoia to revolutionize hiring with LLM-powered search (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/25/juicebox-raises-30m-from-sequoia-to-revolutionize-hiring-with-llm-powered-search/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/david_and_ishan.jpg?resize=1200,857" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;For years, recruiters used machine learning to find potential hires by searching for keywords in résumés and LinkedIn profiles. Although this method helps to narrow the candidate pool, recruiters still have to manually review each profile to determine the best fit for the job.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;David Paffenholz (pictured left) and Ishan Gupta, then just 22 and 19, respectively, realized that LLMs could find talent faster and more efficiently. They built Juicebox, an AI-powered search engine that uses natural language to analyze professional profiles, personal websites, and other publicly available information to identify the most qualified candidates.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;After attending startup accelerator Y Combinator in the summer of 2022, Paffenholz and Gupta spent a couple more years refining their product. When their AI search engine, PeopleGPT, was ready in late 2023, it was quickly adopted by a wide range of customers, from small startups to large companies like Cognition, Ramp, and Perplexity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a short period of time, it was serving over 2,500 customers and achieving more than $10 million in annual recurring revenue (ARR).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Thursday, Juicebox announced that it had raised $36 million in total funding, including a $30 million Series A round led by Sequoia.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sequoia partner David Cahn learned about the company while catching up with an early-stage startup founder who said that he is using Juicebox for all of his recruiting efforts. Cahn told TechCrunch that the founder has hired over a dozen people without using a professional recruiter, something that was previously very difficult to do.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Such a rave review of Juicebox piqued Cahn’s interest. Shortly after, Cahn learned that Sequoia’s internal recruiter was also trying Juicebox to help the firm with its own hiring efforts, which made him even more excited about the startup’s growth potential. &amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;When Cahn eventually met with Paffenholz and Gupta, he was even more impressed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I’m not sure I’ve ever in my career seen a company with four people that got to 2,000 customers with that small of a team,” Cahn told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Although Juicebox has since hired eight additional employees, the company continues to attract customers without a sales team.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Customers are flocking to Juicebox in part because hiring speed is extremely important for companies racing to build AI functionalities.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What sets Juicebox’s search engine apart is its ability to infer information about candidates much like a human would.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We help find net new candidates that wouldn’t be found elsewhere, because the profiles might not have the keywords or the types of things that we’d expect them to have in the regular searches,” Paffenholz told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup’s product is popular not only with small companies that lack a dedicated recruiter, but also with talent teams at large corporations. By automating the candidate search, the tool frees up internal recruiters to focus more on building relationships with potential hires.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Once Juicebox identifies candidates, its agent can automatically email them and schedule initial calls.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Juicebox is growing quickly, older talent acquisition startups like Eightfold are also adding AI-powered search functionality to their offerings.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, Cahn is convinced that Paffenholz and Gupta can transform Juicebox into an essential product for every startup’s technology stack.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’ve invested in a number of businesses that become defaults for startups, like Stripe,” he said. “I think Juicebox has a chance to be a default where, every single startup, [it’s] the first thing they use to hire their first employees.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/david_and_ishan.jpg?resize=1200,857" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;For years, recruiters used machine learning to find potential hires by searching for keywords in résumés and LinkedIn profiles. Although this method helps to narrow the candidate pool, recruiters still have to manually review each profile to determine the best fit for the job.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;David Paffenholz (pictured left) and Ishan Gupta, then just 22 and 19, respectively, realized that LLMs could find talent faster and more efficiently. They built Juicebox, an AI-powered search engine that uses natural language to analyze professional profiles, personal websites, and other publicly available information to identify the most qualified candidates.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;After attending startup accelerator Y Combinator in the summer of 2022, Paffenholz and Gupta spent a couple more years refining their product. When their AI search engine, PeopleGPT, was ready in late 2023, it was quickly adopted by a wide range of customers, from small startups to large companies like Cognition, Ramp, and Perplexity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a short period of time, it was serving over 2,500 customers and achieving more than $10 million in annual recurring revenue (ARR).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Thursday, Juicebox announced that it had raised $36 million in total funding, including a $30 million Series A round led by Sequoia.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sequoia partner David Cahn learned about the company while catching up with an early-stage startup founder who said that he is using Juicebox for all of his recruiting efforts. Cahn told TechCrunch that the founder has hired over a dozen people without using a professional recruiter, something that was previously very difficult to do.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Such a rave review of Juicebox piqued Cahn’s interest. Shortly after, Cahn learned that Sequoia’s internal recruiter was also trying Juicebox to help the firm with its own hiring efforts, which made him even more excited about the startup’s growth potential. &amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;When Cahn eventually met with Paffenholz and Gupta, he was even more impressed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I’m not sure I’ve ever in my career seen a company with four people that got to 2,000 customers with that small of a team,” Cahn told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Although Juicebox has since hired eight additional employees, the company continues to attract customers without a sales team.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Customers are flocking to Juicebox in part because hiring speed is extremely important for companies racing to build AI functionalities.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What sets Juicebox’s search engine apart is its ability to infer information about candidates much like a human would.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We help find net new candidates that wouldn’t be found elsewhere, because the profiles might not have the keywords or the types of things that we’d expect them to have in the regular searches,” Paffenholz told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup’s product is popular not only with small companies that lack a dedicated recruiter, but also with talent teams at large corporations. By automating the candidate search, the tool frees up internal recruiters to focus more on building relationships with potential hires.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Once Juicebox identifies candidates, its agent can automatically email them and schedule initial calls.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Juicebox is growing quickly, older talent acquisition startups like Eightfold are also adding AI-powered search functionality to their offerings.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, Cahn is convinced that Paffenholz and Gupta can transform Juicebox into an essential product for every startup’s technology stack.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’ve invested in a number of businesses that become defaults for startups, like Stripe,” he said. “I think Juicebox has a chance to be a default where, every single startup, [it’s] the first thing they use to hire their first employees.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/25/juicebox-raises-30m-from-sequoia-to-revolutionize-hiring-with-llm-powered-search/</guid><pubDate>Thu, 25 Sep 2025 16:31:13 +0000</pubDate></item><item><title>[NEW] OpenAI launches ChatGPT Pulse to proactively write you morning briefs (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/25/openai-launches-chatgpt-pulse-to-proactively-write-you-morning-briefs/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI is launching a new feature inside of ChatGPT called Pulse, which generates personalized reports for users while they sleep. Pulse offers users five to ten briefs that can get them up to speed on their day, and is aimed at encouraging users to check ChatGPT first thing in the morning — much like they would check social media or a news app.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Pulse is part of a broader shift in OpenAI’s consumer products, which are lately being designed to work for users asynchronously instead of responding to questions. Features like ChatGPT Agent or Codex aim to make ChatGPT feel more like an assistant rather than a chatbot. With Pulse, OpenAI seemingly wants ChatGPT to be more proactive.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We’re building AI that lets us take the level of support that only the wealthiest have been able to afford and make it available to everyone over time,” said OpenAI’s new CEO of Applications, Fidji Simo, in a blog post. “And ChatGPT Pulse is the first step in that direction – starting with Pro users today, but with the goal of rolling out this intelligence to all.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI CEO Sam Altman said earlier this week that some of ChatGPT’s new “compute-intensive” products would be limited to the company’s most expensive subscription plan — which is the case for Pulse. OpenAI has previously said it’s severely limited in the number of servers it has to power ChatGPT, and it’s rapidly building out AI data centers with partners like Oracle and SoftBank to increase its capacity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Starting Thursday, OpenAI will roll out Pulse for subscribers to its $200-a-month Pro plan, for whom it will appear as a new tab in the ChatGPT app. The company says it would like to launch Pulse to all ChatGPT users in the future, with Plus subscribers to get access soon, but it first needs to make the product more efficient.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Pulse’s reports can be roundups of news articles on a specific topic — like updates on a specific sports team — as well as more personalized briefs based on a user’s context.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a demo for TechCrunch, OpenAI product lead Adam Fry showed several reports Pulse had made for him: a roundup of news about British soccer team Arsenal; group Halloween costume suggestions for his wife and kids; and a toddler-friendly travel itinerary for his family’s upcoming trip to Sedona, Arizona.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3050481" height="200" src="https://techcrunch.com/wp-content/uploads/2025/09/ChatGPT-Pulse-examples.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;An example of Pulse’s cards and reports (Credit: OpenAI)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Each report is displayed as a “card” featuring AI-generated images and text. Users can click on each one to get the full report, and can then query ChatGPT about the contents. Pulse will proactively generate some reports, but users can also ask Pulse for new automated reports or offer feedback on existing ones.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A core part of Pulse is that it stops after generating a few reports and shows a message: “Great, that’s it for today.” According to Fry, that’s an intentional design choice to make the service different from engagement-optimized social media apps. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Pulse is compatible with ChatGPT’s Connectors, so users can connect apps like Google Calendar and Gmail. Once that’s set up, Pulse will parse through your email overnight to surface the most important messages in the morning, or access your calendar to generate an agenda for upcoming events.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;If users have ChatGPT’s memory features turned on, Pulse will also pull in context from previous chats to improve your reports. OpenAI’s personalization lead, Christina Wadsworth Kaplan, gave an example of how Pulse automatically picked up on her love of running to create an itinerary for her upcoming trip to London that included running routes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Wadsworth Kaplan described Pulse as a “net-new functionality” for a consumer product. As a pescatarian, she says Pulse takes dinner reservations on her calendar and finds menu items that work with her diet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But it’s hard to overlook how Pulse could compete with existing news products, such as Apple News, paid newsletters or traditional journalism outlets. Fry doesn’t expect Pulse to replace the various news apps people use, and the feature cites its sources with links in the same way ChatGPT Search does.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It remains to be seen if Pulse is worth the computational power it requires to work. Fry says the service can “vary tremendously” in how much computing power it spends on a given task — for some projects, it’s fairly efficient, but others may require searching the web and synthesizing lots of documents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Eventually, OpenAI would like to make Pulse more agentic, to the point where it could make restaurant reservations on a user’s behalf, or draft emails that users could approve to be sent. But such features may be a long way out, and would likely require OpenAI’s agentic models to improve a great deal before users would trust it with such decisions.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI is launching a new feature inside of ChatGPT called Pulse, which generates personalized reports for users while they sleep. Pulse offers users five to ten briefs that can get them up to speed on their day, and is aimed at encouraging users to check ChatGPT first thing in the morning — much like they would check social media or a news app.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Pulse is part of a broader shift in OpenAI’s consumer products, which are lately being designed to work for users asynchronously instead of responding to questions. Features like ChatGPT Agent or Codex aim to make ChatGPT feel more like an assistant rather than a chatbot. With Pulse, OpenAI seemingly wants ChatGPT to be more proactive.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We’re building AI that lets us take the level of support that only the wealthiest have been able to afford and make it available to everyone over time,” said OpenAI’s new CEO of Applications, Fidji Simo, in a blog post. “And ChatGPT Pulse is the first step in that direction – starting with Pro users today, but with the goal of rolling out this intelligence to all.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI CEO Sam Altman said earlier this week that some of ChatGPT’s new “compute-intensive” products would be limited to the company’s most expensive subscription plan — which is the case for Pulse. OpenAI has previously said it’s severely limited in the number of servers it has to power ChatGPT, and it’s rapidly building out AI data centers with partners like Oracle and SoftBank to increase its capacity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Starting Thursday, OpenAI will roll out Pulse for subscribers to its $200-a-month Pro plan, for whom it will appear as a new tab in the ChatGPT app. The company says it would like to launch Pulse to all ChatGPT users in the future, with Plus subscribers to get access soon, but it first needs to make the product more efficient.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Pulse’s reports can be roundups of news articles on a specific topic — like updates on a specific sports team — as well as more personalized briefs based on a user’s context.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a demo for TechCrunch, OpenAI product lead Adam Fry showed several reports Pulse had made for him: a roundup of news about British soccer team Arsenal; group Halloween costume suggestions for his wife and kids; and a toddler-friendly travel itinerary for his family’s upcoming trip to Sedona, Arizona.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3050481" height="200" src="https://techcrunch.com/wp-content/uploads/2025/09/ChatGPT-Pulse-examples.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;An example of Pulse’s cards and reports (Credit: OpenAI)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Each report is displayed as a “card” featuring AI-generated images and text. Users can click on each one to get the full report, and can then query ChatGPT about the contents. Pulse will proactively generate some reports, but users can also ask Pulse for new automated reports or offer feedback on existing ones.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A core part of Pulse is that it stops after generating a few reports and shows a message: “Great, that’s it for today.” According to Fry, that’s an intentional design choice to make the service different from engagement-optimized social media apps. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Pulse is compatible with ChatGPT’s Connectors, so users can connect apps like Google Calendar and Gmail. Once that’s set up, Pulse will parse through your email overnight to surface the most important messages in the morning, or access your calendar to generate an agenda for upcoming events.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;If users have ChatGPT’s memory features turned on, Pulse will also pull in context from previous chats to improve your reports. OpenAI’s personalization lead, Christina Wadsworth Kaplan, gave an example of how Pulse automatically picked up on her love of running to create an itinerary for her upcoming trip to London that included running routes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Wadsworth Kaplan described Pulse as a “net-new functionality” for a consumer product. As a pescatarian, she says Pulse takes dinner reservations on her calendar and finds menu items that work with her diet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But it’s hard to overlook how Pulse could compete with existing news products, such as Apple News, paid newsletters or traditional journalism outlets. Fry doesn’t expect Pulse to replace the various news apps people use, and the feature cites its sources with links in the same way ChatGPT Search does.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It remains to be seen if Pulse is worth the computational power it requires to work. Fry says the service can “vary tremendously” in how much computing power it spends on a given task — for some projects, it’s fairly efficient, but others may require searching the web and synthesizing lots of documents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Eventually, OpenAI would like to make Pulse more agentic, to the point where it could make restaurant reservations on a user’s behalf, or draft emails that users could approve to be sent. But such features may be a long way out, and would likely require OpenAI’s agentic models to improve a great deal before users would trust it with such decisions.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/25/openai-launches-chatgpt-pulse-to-proactively-write-you-morning-briefs/</guid><pubDate>Thu, 25 Sep 2025 17:00:00 +0000</pubDate></item><item><title>[NEW] Shoplifters could soon be chased down by drones (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/09/25/1124088/shoplifters-could-soon-be-chased-down-by-drones/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/flock-shopper.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Flock Safety, whose drones were once reserved for police departments, is now offering them for private-sector security, the company announced today, with potential customers including including businesses intent on curbing shoplifting.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Companies in the US can now place Flock’s drone docking stations on their premises. If the company has a waiver from the Federal Aviation Administration to fly beyond visual line of sight (these are becoming easier to get), its security team can fly the drones within a certain radius, often a few miles.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“Instead of a 911 call [that triggers the drone], it’s an alarm call,” says Keith Kauffman, a former police chief who now directs Flock’s drone program. “It’s still the same type of response.”&lt;/p&gt;  &lt;p&gt;Kauffman walked through how the drone program might work in the case of retail theft: If the security team at a store like Home Depot, for example, saw shoplifters leave the store, then the drone, equipped with cameras, could be activated from its docking station on the roof.&lt;/p&gt; 
 &lt;p&gt;“The drone follows the people. The people get in a car. You click a button,” he says, “and you track the vehicle with the drone, and the drone just follows the car.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The video feed of that drone might go to the company’s security team, but it could also be automatically transmitted directly to police departments.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;The company says it’s in talks with large retailers but doesn’t yet have any signed contracts. The only private-sector company Kauffman named as a customer is Morning Star, a California tomato processor that uses drones to secure its distribution facilities. Flock will also pitch the drones to hospital campuses, warehouse sites, and oil and gas facilities.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It’s worth noting that the FAA is currently drafting new rules for how it grants approval to pilots flying drones out of sight, and it’s not clear if Flock’s use case would be allowed under the currently proposed guidance.&lt;/p&gt;  &lt;p&gt;The company’s expansion to the private sector follows the rise of programs launched by police departments around the country to deploy drones as first responders. In such programs, law enforcement sends drones to a scene to provide visuals faster than an officer can get there.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Flock has arguably led this push, and police departments have claimed drone-enabled successes, like a supply drop to a boy lost in the Colorado wilderness. But the programs have also sparked privacy worries, concerns about overpolicing in minority neighborhoods, and lawsuits charging that police departments should not block public access to drone footage.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;Other technologies Flock offers, like license plate readers, have drawn recent criticism for the ease with which federal US immigration agencies, including ICE and CBP, could look at data collected by local police departments amid President Trump’s mass deportation efforts.&lt;/p&gt;  &lt;p&gt;Flock’s expansion into private-sector security is “a logical step, but in the wrong direction,” says Rebecca Williams, senior strategist for the ACLU’s privacy and data governance unit.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Williams cited a growing erosion of Fourth Amendment protections—which prevent unlawful search and seizure—in the online era, in which the government can purchase private data that it would otherwise need a warrant to acquire. Proposed legislation to curb that practice has stalled, and Flock’s expansion into the private sector would exacerbate the issue, Williams says.&lt;/p&gt;  &lt;p&gt;“Flock is the Meta of surveillance technology now,” Williams says, referring to the amount of personal data that company has acquired and monetized. “This expansion is very scary.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/flock-shopper.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Flock Safety, whose drones were once reserved for police departments, is now offering them for private-sector security, the company announced today, with potential customers including including businesses intent on curbing shoplifting.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Companies in the US can now place Flock’s drone docking stations on their premises. If the company has a waiver from the Federal Aviation Administration to fly beyond visual line of sight (these are becoming easier to get), its security team can fly the drones within a certain radius, often a few miles.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“Instead of a 911 call [that triggers the drone], it’s an alarm call,” says Keith Kauffman, a former police chief who now directs Flock’s drone program. “It’s still the same type of response.”&lt;/p&gt;  &lt;p&gt;Kauffman walked through how the drone program might work in the case of retail theft: If the security team at a store like Home Depot, for example, saw shoplifters leave the store, then the drone, equipped with cameras, could be activated from its docking station on the roof.&lt;/p&gt; 
 &lt;p&gt;“The drone follows the people. The people get in a car. You click a button,” he says, “and you track the vehicle with the drone, and the drone just follows the car.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The video feed of that drone might go to the company’s security team, but it could also be automatically transmitted directly to police departments.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;The company says it’s in talks with large retailers but doesn’t yet have any signed contracts. The only private-sector company Kauffman named as a customer is Morning Star, a California tomato processor that uses drones to secure its distribution facilities. Flock will also pitch the drones to hospital campuses, warehouse sites, and oil and gas facilities.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It’s worth noting that the FAA is currently drafting new rules for how it grants approval to pilots flying drones out of sight, and it’s not clear if Flock’s use case would be allowed under the currently proposed guidance.&lt;/p&gt;  &lt;p&gt;The company’s expansion to the private sector follows the rise of programs launched by police departments around the country to deploy drones as first responders. In such programs, law enforcement sends drones to a scene to provide visuals faster than an officer can get there.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Flock has arguably led this push, and police departments have claimed drone-enabled successes, like a supply drop to a boy lost in the Colorado wilderness. But the programs have also sparked privacy worries, concerns about overpolicing in minority neighborhoods, and lawsuits charging that police departments should not block public access to drone footage.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;Other technologies Flock offers, like license plate readers, have drawn recent criticism for the ease with which federal US immigration agencies, including ICE and CBP, could look at data collected by local police departments amid President Trump’s mass deportation efforts.&lt;/p&gt;  &lt;p&gt;Flock’s expansion into private-sector security is “a logical step, but in the wrong direction,” says Rebecca Williams, senior strategist for the ACLU’s privacy and data governance unit.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Williams cited a growing erosion of Fourth Amendment protections—which prevent unlawful search and seizure—in the online era, in which the government can purchase private data that it would otherwise need a warrant to acquire. Proposed legislation to curb that practice has stalled, and Flock’s expansion into the private sector would exacerbate the issue, Williams says.&lt;/p&gt;  &lt;p&gt;“Flock is the Meta of surveillance technology now,” Williams says, referring to the amount of personal data that company has acquired and monetized. “This expansion is very scary.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/09/25/1124088/shoplifters-could-soon-be-chased-down-by-drones/</guid><pubDate>Thu, 25 Sep 2025 17:34:26 +0000</pubDate></item><item><title>[NEW] Experts urge caution about using ChatGPT to pick stocks (AI – Ars Technica)</title><link>https://arstechnica.com/information-technology/2025/09/experts-urge-caution-about-using-chatgpt-to-pick-stocks/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        AI-selected portfolios might perform well in a growing market, but experts warn of downturn risks.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Robot showing stock market financial growth chart" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/robot_stocks-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Robot showing stock market financial growth chart" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/robot_stocks-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Yuichiro Chino via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;With AI chatbots growing in popular usage, it was only a matter of time before large numbers of people began applying them to the stock market. In fact, at least 1 in 10 retail investors now consult ChatGPT or other AI chatbots for stock-picking advice, according to a Reuters report published Thursday.&lt;/p&gt;
&lt;p&gt;Data from a survey by trading platform eToro of 11,000 retail investors worldwide suggests that 13 percent of individual investors already use AI tools like ChatGPT or Google's Gemini for stock selection, while about half say they would consider using these tools for portfolio decisions.&lt;/p&gt;
&lt;p&gt;Unlike algorithmic trading, where computers automatically execute thousands of trades per second, investors are using ChatGPT as an advisory tool in place of human experts. They type questions, read the AI model's analysis, and then manually decide whether to place trades through their brokers.&lt;/p&gt;
&lt;p&gt;Reuters spoke with Jeremy Leung, who analyzed companies for investment bank UBS for almost two decades. Leung now relies on ChatGPT for his multi-asset portfolio. "I no longer have the luxury of a Bloomberg terminal, or those kinds of market-data services which are very, very expensive," Leung told Reuters. "Even the simple ChatGPT tool can do a lot and replicate a lot of the workflows that I used to do."&lt;/p&gt;
&lt;p&gt;Reuters reports that financial products comparison website Finder asked ChatGPT in March 2023 to select stocks from high-quality businesses based on criteria like debt levels and sustained growth. Since then, the resulting 38-stock portfolio has reportedly grown in value nearly 55 percent. That performance beat the average of the UK's 10 most popular funds by almost 19 percentage points.&lt;/p&gt;
&lt;p&gt;But there's a huge caveat to that kind of AI success story: US stocks sit near record highs, Reuters notes, with the S&amp;amp;P 500 index up 13 percent this year after surging 23 percent last year. Those are conditions that can make almost any stock-picking strategy look smart.&lt;/p&gt;
&lt;p&gt;Reuters frames the AI trading advice trend as a case of new technology tools "democratizing," or opening up, investment analysis once reserved for institutional investors with expensive data terminals. But experts warn that AI models can confabulate financial data and lack access to real-time market information, making them risky substitutes for professional advice.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;"AI models can be brilliant," Dan Moczulski, UK managing director at eToro, told Reuters. "The risk comes when people treat generic models like ChatGPT or Gemini as crystal balls." He noted that general AI models "can misquote figures and dates, lean too hard on a pre-established narrative, and overly rely on past price action to attempt to predict the future."&lt;/p&gt;
&lt;h2&gt;The hazards of AI stock picking&lt;/h2&gt;
&lt;p&gt;Using AI to trade stocks at home feels like it might be the next step in a long series of technological advances that have democratized individual retail investing, for better or for worse. Computer-based stock trading for individuals dates back to 1984, when Charles Schwab introduced electronic trading services for dial-up customers. E-Trade launched in 1992, and by the late 1990s, online brokerages had transformed retail investing, dropping commission fees from hundreds of dollars per trade to under $10.&lt;/p&gt;
&lt;p&gt;The first "robo-advisors" appeared after the 2008 financial crisis, which began the rise of automated online services that use algorithms to manage and rebalance portfolios based on a client's goals. Services like Betterment launched in 2010, and Wealthfront followed in 2011, using algorithms to automatically rebalance portfolios. By the end of 2015, robo-advisors from nearly 100 companies globally were managing $60 billion in client assets.&lt;/p&gt;
&lt;p&gt;The arrival of ChatGPT in November 2022 arguably marked a new phase where retail investors could directly query an AI model for stock picks rather than relying on pre-programmed algorithms. But Leung acknowledged that ChatGPT cannot access data behind paywalls, potentially missing crucial analyses available through professional services. To get better results, he creates specific prompts like "assume you're a short analyst, what is the short thesis for this stock?" or "use only credible sources, such as SEC filings."&lt;/p&gt;
&lt;p&gt;Beyond chatbots, reliance on financial algorithms is growing. The "robo-advisory" market, which includes all companies providing automated, algorithm-driven financial advice from fintech startups to established banks, is forecast to grow roughly 600 percent by 2029, according to data-analysis firm Research and Markets.&lt;/p&gt;
&lt;p&gt;But as more retail investors turn to AI tools for investment decisions,&amp;nbsp;it's also potential trouble waiting to happen.&lt;/p&gt;
&lt;p&gt;"If people get comfortable investing using AI and they're making money, they may not be able to manage in a crisis or downturn," Leung warned Reuters. The concern extends beyond individual losses to whether retail investors using AI tools understand risk management or have strategies for when markets turn bearish.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        AI-selected portfolios might perform well in a growing market, but experts warn of downturn risks.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Robot showing stock market financial growth chart" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/robot_stocks-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Robot showing stock market financial growth chart" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/robot_stocks-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Yuichiro Chino via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;With AI chatbots growing in popular usage, it was only a matter of time before large numbers of people began applying them to the stock market. In fact, at least 1 in 10 retail investors now consult ChatGPT or other AI chatbots for stock-picking advice, according to a Reuters report published Thursday.&lt;/p&gt;
&lt;p&gt;Data from a survey by trading platform eToro of 11,000 retail investors worldwide suggests that 13 percent of individual investors already use AI tools like ChatGPT or Google's Gemini for stock selection, while about half say they would consider using these tools for portfolio decisions.&lt;/p&gt;
&lt;p&gt;Unlike algorithmic trading, where computers automatically execute thousands of trades per second, investors are using ChatGPT as an advisory tool in place of human experts. They type questions, read the AI model's analysis, and then manually decide whether to place trades through their brokers.&lt;/p&gt;
&lt;p&gt;Reuters spoke with Jeremy Leung, who analyzed companies for investment bank UBS for almost two decades. Leung now relies on ChatGPT for his multi-asset portfolio. "I no longer have the luxury of a Bloomberg terminal, or those kinds of market-data services which are very, very expensive," Leung told Reuters. "Even the simple ChatGPT tool can do a lot and replicate a lot of the workflows that I used to do."&lt;/p&gt;
&lt;p&gt;Reuters reports that financial products comparison website Finder asked ChatGPT in March 2023 to select stocks from high-quality businesses based on criteria like debt levels and sustained growth. Since then, the resulting 38-stock portfolio has reportedly grown in value nearly 55 percent. That performance beat the average of the UK's 10 most popular funds by almost 19 percentage points.&lt;/p&gt;
&lt;p&gt;But there's a huge caveat to that kind of AI success story: US stocks sit near record highs, Reuters notes, with the S&amp;amp;P 500 index up 13 percent this year after surging 23 percent last year. Those are conditions that can make almost any stock-picking strategy look smart.&lt;/p&gt;
&lt;p&gt;Reuters frames the AI trading advice trend as a case of new technology tools "democratizing," or opening up, investment analysis once reserved for institutional investors with expensive data terminals. But experts warn that AI models can confabulate financial data and lack access to real-time market information, making them risky substitutes for professional advice.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;"AI models can be brilliant," Dan Moczulski, UK managing director at eToro, told Reuters. "The risk comes when people treat generic models like ChatGPT or Gemini as crystal balls." He noted that general AI models "can misquote figures and dates, lean too hard on a pre-established narrative, and overly rely on past price action to attempt to predict the future."&lt;/p&gt;
&lt;h2&gt;The hazards of AI stock picking&lt;/h2&gt;
&lt;p&gt;Using AI to trade stocks at home feels like it might be the next step in a long series of technological advances that have democratized individual retail investing, for better or for worse. Computer-based stock trading for individuals dates back to 1984, when Charles Schwab introduced electronic trading services for dial-up customers. E-Trade launched in 1992, and by the late 1990s, online brokerages had transformed retail investing, dropping commission fees from hundreds of dollars per trade to under $10.&lt;/p&gt;
&lt;p&gt;The first "robo-advisors" appeared after the 2008 financial crisis, which began the rise of automated online services that use algorithms to manage and rebalance portfolios based on a client's goals. Services like Betterment launched in 2010, and Wealthfront followed in 2011, using algorithms to automatically rebalance portfolios. By the end of 2015, robo-advisors from nearly 100 companies globally were managing $60 billion in client assets.&lt;/p&gt;
&lt;p&gt;The arrival of ChatGPT in November 2022 arguably marked a new phase where retail investors could directly query an AI model for stock picks rather than relying on pre-programmed algorithms. But Leung acknowledged that ChatGPT cannot access data behind paywalls, potentially missing crucial analyses available through professional services. To get better results, he creates specific prompts like "assume you're a short analyst, what is the short thesis for this stock?" or "use only credible sources, such as SEC filings."&lt;/p&gt;
&lt;p&gt;Beyond chatbots, reliance on financial algorithms is growing. The "robo-advisory" market, which includes all companies providing automated, algorithm-driven financial advice from fintech startups to established banks, is forecast to grow roughly 600 percent by 2029, according to data-analysis firm Research and Markets.&lt;/p&gt;
&lt;p&gt;But as more retail investors turn to AI tools for investment decisions,&amp;nbsp;it's also potential trouble waiting to happen.&lt;/p&gt;
&lt;p&gt;"If people get comfortable investing using AI and they're making money, they may not be able to manage in a crisis or downturn," Leung warned Reuters. The concern extends beyond individual losses to whether retail investors using AI tools understand risk management or have strategies for when markets turn bearish.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/information-technology/2025/09/experts-urge-caution-about-using-chatgpt-to-pick-stocks/</guid><pubDate>Thu, 25 Sep 2025 18:10:50 +0000</pubDate></item></channel></rss>