<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 05 Nov 2025 06:33:10 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>Sora is now available on Android in the US, Canada, and other regions (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/04/sora-is-now-available-on-android-in-the-us-canada-and-other-regions/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/sora-app-GettyImages-2240278671.jpeg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Sora, the AI video generator from OpenAI, is now officially available for Android users in the U.S., Canada, Japan, Korea, Taiwan, Thailand, and Vietnam.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Initially launched as an iOS app in September, Sora quickly rose to the top of the App Store charts, amassing over 1 million downloads in a week. With its arrival on the Google Play Store, Sora is expected to attract a larger user base, likely resulting in a surge in downloads.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Android version retains all the features of its iOS counterpart, including the “Cameos” feature, which allows users to generate videos of themselves performing various activities using their own likeness.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The videos can be shared in a feed reminiscent of TikTok, allowing users to discover and engage with content from others. This appears to be a strategic move by OpenAI to strengthen its position in the competitive landscape of short-form video sharing. The AI giant aims to rival major players like Meta, which has recently launched its own AI video feed called Vibes, as well as existing platforms such as TikTok and Instagram.&lt;/p&gt;

&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;The Sora app is now available on Android in:&lt;/p&gt;&lt;p&gt;Canada&lt;br /&gt;Japan&lt;br /&gt;Korea&lt;br /&gt;Taiwan&lt;br /&gt;Thailand&lt;br /&gt;US&lt;br /&gt;Vietnam pic.twitter.com/wmx5KU4VM1&lt;/p&gt;— Sora (@soraofficialapp) November 4, 2025&lt;/blockquote&gt; 

&lt;p class="wp-block-paragraph"&gt;However, the app has faced criticism for its handling of deepfakes. After its initial launch, users began uploading disrespectful videos of historical figures, including Martin Luther King Jr. As a result, Sora paused the generation of content depicting Dr. King last month and strengthened its guardrails.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company also recently addressed the backlash surrounding copyrighted characters, such as SpongeBob and Pikachu, by changing its policy for the Sora app from an “opt-out” approach to an “opt-in” system for rights holders.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, it’s currently involved in a legal dispute with celebrity video maker Cameo regarding the name of Sora’s flagship feature, “Cameo.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Looking ahead, OpenAI plans to introduce additional features to Sora. These include character cameos, letting users create AI-generated videos featuring their pets and inanimate objects. Basic video editing tools are also on the way, including the ability to stitch multiple clips together. Sora also plans to help users customize their social feeds, focusing on content from selected individuals rather than a large audience.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/sora-app-GettyImages-2240278671.jpeg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Sora, the AI video generator from OpenAI, is now officially available for Android users in the U.S., Canada, Japan, Korea, Taiwan, Thailand, and Vietnam.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Initially launched as an iOS app in September, Sora quickly rose to the top of the App Store charts, amassing over 1 million downloads in a week. With its arrival on the Google Play Store, Sora is expected to attract a larger user base, likely resulting in a surge in downloads.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Android version retains all the features of its iOS counterpart, including the “Cameos” feature, which allows users to generate videos of themselves performing various activities using their own likeness.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The videos can be shared in a feed reminiscent of TikTok, allowing users to discover and engage with content from others. This appears to be a strategic move by OpenAI to strengthen its position in the competitive landscape of short-form video sharing. The AI giant aims to rival major players like Meta, which has recently launched its own AI video feed called Vibes, as well as existing platforms such as TikTok and Instagram.&lt;/p&gt;

&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;The Sora app is now available on Android in:&lt;/p&gt;&lt;p&gt;Canada&lt;br /&gt;Japan&lt;br /&gt;Korea&lt;br /&gt;Taiwan&lt;br /&gt;Thailand&lt;br /&gt;US&lt;br /&gt;Vietnam pic.twitter.com/wmx5KU4VM1&lt;/p&gt;— Sora (@soraofficialapp) November 4, 2025&lt;/blockquote&gt; 

&lt;p class="wp-block-paragraph"&gt;However, the app has faced criticism for its handling of deepfakes. After its initial launch, users began uploading disrespectful videos of historical figures, including Martin Luther King Jr. As a result, Sora paused the generation of content depicting Dr. King last month and strengthened its guardrails.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company also recently addressed the backlash surrounding copyrighted characters, such as SpongeBob and Pikachu, by changing its policy for the Sora app from an “opt-out” approach to an “opt-in” system for rights holders.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, it’s currently involved in a legal dispute with celebrity video maker Cameo regarding the name of Sora’s flagship feature, “Cameo.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Looking ahead, OpenAI plans to introduce additional features to Sora. These include character cameos, letting users create AI-generated videos featuring their pets and inanimate objects. Basic video editing tools are also on the way, including the ability to stitch multiple clips together. Sora also plans to help users customize their social feeds, focusing on content from selected individuals rather than a large audience.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/04/sora-is-now-available-on-android-in-the-us-canada-and-other-regions/</guid><pubDate>Tue, 04 Nov 2025 19:35:51 +0000</pubDate></item><item><title>Attention ISN'T all you need?! New Qwen3 variant Brumby-14B-Base leverages Power Retention technique (AI | VentureBeat)</title><link>https://venturebeat.com/ai/attention-isnt-all-you-need-new-qwen3-variant-brumby-14b-base-leverages</link><description>[unable to retrieve full-text content]&lt;p&gt;When the transformer architecture was introduced in 2017 in the now seminal Google paper &amp;quot;&lt;a href="https://arxiv.org/abs/1706.03762"&gt;Attention Is All You Need&lt;/a&gt;,&amp;quot; it became an instant cornerstone of modern artificial intelligence. &lt;/p&gt;&lt;p&gt;Every major large language model (LLM) — from OpenAI&amp;#x27;s GPT series to Anthropic&amp;#x27;s Claude, Google&amp;#x27;s Gemini, and Meta&amp;#x27;s Llama — has been built on some variation of its central mechanism: &lt;b&gt;attention&lt;/b&gt;, the mathematical operation that allows a model to look back across its entire input and decide what information matters most.&lt;/p&gt;&lt;p&gt;Eight years later, the same mechanism that defined AI’s golden age is now showing its limits. Attention is powerful, but it is also expensive — its computational and memory costs scale quadratically with context length, creating an increasingly unsustainable bottleneck for both research and industry. As models aim to reason across documents, codebases, or video streams lasting hours or days, attention becomes the architecture’s Achilles’ heel.&lt;/p&gt;&lt;p&gt;On October 28, 2025, the little-known AI startup &lt;a href="https://manifestai.com/articles/release-brumby-14b/"&gt;Manifest AI introduced a radical alternative&lt;/a&gt;. Their new model, &lt;b&gt;Brumby-14B-Base&lt;/b&gt;, is a &lt;b&gt;retrained variant of Qwen3-14B-Base&lt;/b&gt;, one of the leading open-source transformer models.&lt;/p&gt;&lt;p&gt;But while many variants of Qwen have been trained already, Brumby-14B-Base is novel in that it abandons attention altogether. &lt;/p&gt;&lt;p&gt;Instead, Brumby replaces those layers with a novel mechanism called &lt;b&gt;Power Retention&lt;/b&gt;—a recurrent, hardware-efficient architecture that stores and updates information over arbitrarily long contexts without the exponential memory growth of attention.&lt;/p&gt;&lt;p&gt;Trained at a stated cost of just $4,000, the 14-billion-parameter Brumby model performs on par with established transformer models like Qwen3-14B and GLM-4.5-Air, achieving near-state-of-the-art accuracy on a range of reasoning and comprehension benchmarks.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;From Attention to Retention: The Architectural Shift&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The core of Manifest AI’s innovation lies in what they call the Power Retention layer. &lt;/p&gt;&lt;p&gt;In a traditional transformer, every token computes a set of queries (Q), keys (K), and values (V), then performs a matrix operation that measures the similarity between every token and every other token—essentially a full pairwise comparison across the sequence. &lt;/p&gt;&lt;p&gt;This is what gives attention its flexibility, but also what makes it so costly: processing a sequence twice as long takes roughly four times the compute and memory.&lt;/p&gt;&lt;p&gt;Power Retention keeps the same inputs (Q, K, V), but replaces the global similarity operation with a recurrent state update. &lt;/p&gt;&lt;p&gt;Each layer maintains a memory matrix S, which is updated at each time step according to the incoming key, value, and a learned gating signal. &lt;/p&gt;&lt;p&gt;The process looks more like an RNN (Recurrent Neural Network) than a transformer: instead of recomputing attention over the entire context, the model continuously compresses past information into a fixed-size latent state.&lt;/p&gt;&lt;p&gt;This means the computational cost of Power Retention &lt;b&gt;does not grow with context length&lt;/b&gt;. Whether the model is processing 1,000 or 1,000,000 tokens, the &lt;b&gt;per-token cost remains constant.&lt;/b&gt; &lt;/p&gt;&lt;p&gt;That property alone—constant-time per-token computation—marks a profound departure from transformer behavior.&lt;/p&gt;&lt;p&gt;At the same time, Power Retention preserves the expressive power that made attention successful. Because the recurrence involves tensor powers of the input (hence the name “power retention”), it can represent higher-order dependencies between past and present tokens. &lt;/p&gt;&lt;p&gt;The result is an architecture that can theoretically retain long-term dependencies indefinitely, while remaining as efficient as an RNN and as expressive as a transformer.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Retraining, Not Rebuilding&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Perhaps the most striking aspect of Brumby-14B’s training process is its efficiency. Manifest AI trained the model for only 60 hours on 32 Nvidia H100 GPUs, at a cost of roughly $4,000 — less than 2% of what a conventional model of this scale would cost to train from scratch.&lt;/p&gt;&lt;p&gt;However, since it relied on a transformer-based model, it&amp;#x27;s safe to say that this advance alone will not end the transformer AI-era.&lt;/p&gt;&lt;p&gt;As Jacob Buckman, founder of Manifest AI, clarified in an email to VentureBeat: “The ability to train for $4,000 is indeed only possible when leveraging an existing transformer model,” he said. “Brumby could not be trained from scratch for that price.”&lt;/p&gt;&lt;p&gt;Still, Buckman emphasized the significance of that result: “The reason this is important is that the ability to build on the weights of the previous generation of model architectures is a critical accelerant for the adoption of a new modeling paradigm.” &lt;/p&gt;&lt;p&gt;He argues this demonstrates how attention-free systems can catch up to transformer performance “for orders-of-magnitude less” investment.&lt;/p&gt;&lt;p&gt;In the loss curves released by Manifest AI, Brumby’s training loss quickly converges to that of the Qwen3 baseline within 3,000 training steps, even as the architecture diverges significantly from its transformer origins. &lt;/p&gt;&lt;p&gt;Although Brumby-14B-Base began life as Qwen3-14B-Base, it did not remain identical for long. Manifest AI fundamentally altered Qwen3’s architecture by removing its attention layers—the mathematical engine that defines how a transformer model processes information—and replacing them with their new “power retention” mechanism. This change restructured the model’s internal wiring, effectively giving it a new brain while preserving much of its prior knowledge.&lt;/p&gt;&lt;p&gt;Because of that architectural swap, the existing Qwen3 weights no longer fit perfectly. They were trained to operate within a transformer’s attention dynamics, not the new retention-based system. As a result, the Brumby model initially “forgot” how to apply some of its learned knowledge effectively. The retraining process—about &lt;b&gt;3,000 steps&lt;/b&gt; of additional learning—served to recalibrate those weights, aligning them with the power retention framework without having to start from zero.&lt;/p&gt;&lt;p&gt;A helpful way to think about this is to imagine taking a world-class pianist and handing them a guitar. They already understand rhythm, harmony, and melody, but their hands must learn entirely new patterns to produce the same music. Similarly, Brumby had to relearn how to use its existing knowledge through a new computational instrument. Those 3,000 training steps were, in effect, its crash course in guitar lessons.&lt;/p&gt;&lt;p&gt;By the end of this short retraining phase, Brumby had regained its full performance, reaching the same accuracy as the original Qwen3 model. That quick recovery is what makes the result so significant: it shows that an attention-free system can inherit and adapt the capabilities of a transformer model with only a fraction of the training time and cost.&lt;/p&gt;&lt;p&gt;The benchmark progression plots show a similar trend: the model rapidly approaches its target accuracy on core evaluations like GSM8K, HellaSwag, and MMLU after only a few thousand steps, matching or even slightly surpassing Qwen3 on several tasks.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Benchmarking the Brumby&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Across standard evaluation tasks, &lt;b&gt;Brumby-14B-Base&lt;/b&gt; consistently performs at or near parity with transformer baselines of comparable scale.&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Task&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Brumby-14B&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Qwen3-14B&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GLM-4.5-Air&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Nemotron Nano (12B)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ARC&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.89&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.94&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.92&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.93&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GSM8K&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.88&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.84&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.83&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.84&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GSM8K (Platinum)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.87&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.88&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.85&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.87&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;HellaSwag&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.77&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.81&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.85&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.82&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;MATH&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.62&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.54&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.47&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.26&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;MBPP&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.57&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.75&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.73&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.71&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;MMLU&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.71&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.78&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.77&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.78&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;MMLU (Pro)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.36&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.55&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.51&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.53&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;While it lags slightly behind transformers on knowledge-heavy evaluations like &lt;b&gt;MMLU-Pro&lt;/b&gt;, it matches or outperforms them on &lt;b&gt;mathematical reasoning&lt;/b&gt; and &lt;b&gt;long-context reasoning&lt;/b&gt; tasks—precisely where attention architectures tend to falter. This pattern reinforces the idea that recurrent or retention-based systems may hold a structural advantage for reasoning over extended temporal or logical dependencies.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Hardware Efficiency and Inference Performance&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Brumby’s power retention design offers another major advantage: hardware efficiency.&lt;/p&gt;&lt;p&gt;Because the state update involves only local matrix operations, inference can be implemented with linear complexity in sequence length. &lt;/p&gt;&lt;p&gt;Manifest AI reports that their fastest kernels, developed through their in-house CUDA framework Vidrial, can deliver hundreds-fold speedups over attention on very long contexts.&lt;/p&gt;&lt;p&gt;Buckman said the alpha-stage Power Retention kernels “achieve typical hardware utilization of 80–85%, which is higher than FlashAttention2’s 70–75% or Mamba’s 50–60%.” &lt;/p&gt;&lt;p&gt;(Mamba is another emerging “post-transformer” architecture developed by Carnegie Mellon scientists back in 2023 that, like Power Retention, seeks to eliminate the computational bottleneck of attention. It replaces attention with a &lt;i&gt;state-space&lt;/i&gt; mechanism that processes sequences linearly — updating an internal state over time rather than comparing every token to every other one. This makes it far more efficient for long inputs, though it typically achieves lower hardware utilization than Power Retention in early tests.)&lt;/p&gt;&lt;p&gt;Both Power Retention and Mamba, he added, “expend meaningfully fewer total FLOPs than FlashAttention2 on long contexts, as well as far less memory.” &lt;/p&gt;&lt;p&gt;According to Buckman, the reported 100× speedup comes from this combined improvement in utilization and computational efficiency, though he noted that “we have not yet stress-tested it on production-scale workloads.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Training and Scaling Economics&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Perhaps no statistic in the Brumby release generated more attention than the training cost.&lt;/p&gt;&lt;p&gt;A 14-billion-parameter model, trained for $4,000, represents a two-order-of-magnitude reduction in the cost of foundation model development.&lt;/p&gt;&lt;p&gt;Buckman confirmed that the low cost reflects a broader scaling pattern. “Far from diminishing returns, we have found that ease of retraining improves with scale,” he said. “The number of steps required to successfully retrain a model decreases with its parameter count.” &lt;/p&gt;&lt;p&gt;Manifest has not yet validated the cost of retraining models at 700B parameters, but Buckman projected a range of $10,000–$20,000 for models of that magnitude—still far below transformer training budgets.&lt;/p&gt;&lt;p&gt;He also reiterated that this approach could democratize large-scale experimentation by allowing smaller research groups or companies to retrain or repurpose existing transformer checkpoints without prohibitive compute costs.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Integration and Deployment&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;According to Buckman, converting an existing transformer into a Power Retention model is designed to be simple. &lt;/p&gt;&lt;p&gt;“It is straightforward for any company that is already retraining, post-training, or fine-tuning open-source models,” he said. “Simply pip install retention, change one line of your architecture code, and resume training where you left off.”&lt;/p&gt;&lt;p&gt;He added that after only a small number of GPU-hours, the model typically recovers its original performance—at which point it gains the efficiency benefits of the attention-free design. &lt;/p&gt;&lt;p&gt;“The resulting architecture will permit far faster long-context training and inference than previously,” Buckman noted.&lt;/p&gt;&lt;p&gt;On infrastructure, Buckman said the main Brumby kernels are written in Triton, compatible with both NVIDIA and AMD accelerators. Specialized CUDA kernels are also available through the team’s in-house Vidrial framework. Integration with vLLM and other inference engines remains a work in progress: “We have not yet integrated Power Retention into inference engines, but doing so is a major ongoing initiative at Manifest.”&lt;/p&gt;&lt;p&gt;As for distributed inference, Buckman dismissed concerns about instability: “We have not found this difficulty to be exacerbated in any way by our recurrent-state architecture. In fact, context-parallel training and GPU partitioning for multi-user inference both become significantly cleaner technically when using our approach.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Mission and Long-Term Vision&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Beyond the engineering details, Buckman also described Manifest’s broader mission. “Our mission is to train a neural network to model all human output,” he said. &lt;/p&gt;&lt;p&gt;The team’s goal, he explained, is to move beyond modeling “artifacts of intelligence” toward modeling “the intelligent processes that generated them.” This shift, he argued, requires “fundamentally rethinking” how models are designed and trained—work that Power Retention represents only the beginning of.&lt;/p&gt;&lt;p&gt;The Brumby-14B release, he said, is “one step forward in a long march” toward architectures that can model thought processes continuously and efficiently.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Public Debate and Industry Reception&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The launch of Brumby-14B sparked immediate discussion on X (formerly Twitter), where researchers debated the framing of Manifest AI’s announcement. &lt;/p&gt;&lt;p&gt;Some, including Meta researcher &lt;a href="https://x.com/redtachyon/status/1983819957583421606"&gt;Ariel (@redtachyon)&lt;/a&gt;, argued that the “$4,000 foundation model” tagline was misleading, since the training involved reusing pretrained transformer weights rather than training from scratch.&lt;/p&gt;&lt;p&gt;“They shuffled around the weights of Qwen, fine-tuned it a bit, and called it ‘training a foundation model for $4k,’” Ariel &lt;a href="https://x.com/redtachyon/status/1983820872461722075"&gt;wrote&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Buckman responded publicly, clarifying that the initial tweet had been part of a longer thread explaining the retraining approach. “It’s not like I was being deceptive about it,” he &lt;a href="https://x.com/jacobmbuckman/status/1983875586175996413"&gt;wrote&lt;/a&gt;. “I broke it up into separate tweets, and now everyone is mad about the first one.”&lt;/p&gt;&lt;p&gt;In a follow-up email, Buckman took a measured view of the controversy. “The end of the transformer era is not yet here,” he reiterated, “but the march has begun.” &lt;/p&gt;&lt;p&gt;He also acknowledged that the $4,000 claim, though technically accurate in context, had drawn attention precisely because it challenged expectations about what it costs to experiment at frontier scale.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Conclusion: A Crack in the Transformer’s Wall?&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The release of Brumby-14B-Base is more than an engineering milestone; it is a proof of concept that the transformer’s dominance may finally face credible competition. &lt;/p&gt;&lt;p&gt;By replacing attention with power retention, Manifest AI has demonstrated that performance parity with state-of-the-art transformers is possible at a fraction of the computational cost—and that the long-context bottleneck can be broken without exotic hardware.&lt;/p&gt;&lt;p&gt;The broader implications are twofold. First, the economics of training and serving large models could shift dramatically, lowering the barrier to entry for open research and smaller organizations. &lt;/p&gt;&lt;p&gt;Second, the architectural diversity of AI models may expand again, reigniting theoretical and empirical exploration after half a decade of transformer monoculture.&lt;/p&gt;&lt;p&gt;As Buckman put it: “The end of the transformer era is not yet here. Our release is just one step forward in a long march toward the future.”&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;When the transformer architecture was introduced in 2017 in the now seminal Google paper &amp;quot;&lt;a href="https://arxiv.org/abs/1706.03762"&gt;Attention Is All You Need&lt;/a&gt;,&amp;quot; it became an instant cornerstone of modern artificial intelligence. &lt;/p&gt;&lt;p&gt;Every major large language model (LLM) — from OpenAI&amp;#x27;s GPT series to Anthropic&amp;#x27;s Claude, Google&amp;#x27;s Gemini, and Meta&amp;#x27;s Llama — has been built on some variation of its central mechanism: &lt;b&gt;attention&lt;/b&gt;, the mathematical operation that allows a model to look back across its entire input and decide what information matters most.&lt;/p&gt;&lt;p&gt;Eight years later, the same mechanism that defined AI’s golden age is now showing its limits. Attention is powerful, but it is also expensive — its computational and memory costs scale quadratically with context length, creating an increasingly unsustainable bottleneck for both research and industry. As models aim to reason across documents, codebases, or video streams lasting hours or days, attention becomes the architecture’s Achilles’ heel.&lt;/p&gt;&lt;p&gt;On October 28, 2025, the little-known AI startup &lt;a href="https://manifestai.com/articles/release-brumby-14b/"&gt;Manifest AI introduced a radical alternative&lt;/a&gt;. Their new model, &lt;b&gt;Brumby-14B-Base&lt;/b&gt;, is a &lt;b&gt;retrained variant of Qwen3-14B-Base&lt;/b&gt;, one of the leading open-source transformer models.&lt;/p&gt;&lt;p&gt;But while many variants of Qwen have been trained already, Brumby-14B-Base is novel in that it abandons attention altogether. &lt;/p&gt;&lt;p&gt;Instead, Brumby replaces those layers with a novel mechanism called &lt;b&gt;Power Retention&lt;/b&gt;—a recurrent, hardware-efficient architecture that stores and updates information over arbitrarily long contexts without the exponential memory growth of attention.&lt;/p&gt;&lt;p&gt;Trained at a stated cost of just $4,000, the 14-billion-parameter Brumby model performs on par with established transformer models like Qwen3-14B and GLM-4.5-Air, achieving near-state-of-the-art accuracy on a range of reasoning and comprehension benchmarks.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;From Attention to Retention: The Architectural Shift&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The core of Manifest AI’s innovation lies in what they call the Power Retention layer. &lt;/p&gt;&lt;p&gt;In a traditional transformer, every token computes a set of queries (Q), keys (K), and values (V), then performs a matrix operation that measures the similarity between every token and every other token—essentially a full pairwise comparison across the sequence. &lt;/p&gt;&lt;p&gt;This is what gives attention its flexibility, but also what makes it so costly: processing a sequence twice as long takes roughly four times the compute and memory.&lt;/p&gt;&lt;p&gt;Power Retention keeps the same inputs (Q, K, V), but replaces the global similarity operation with a recurrent state update. &lt;/p&gt;&lt;p&gt;Each layer maintains a memory matrix S, which is updated at each time step according to the incoming key, value, and a learned gating signal. &lt;/p&gt;&lt;p&gt;The process looks more like an RNN (Recurrent Neural Network) than a transformer: instead of recomputing attention over the entire context, the model continuously compresses past information into a fixed-size latent state.&lt;/p&gt;&lt;p&gt;This means the computational cost of Power Retention &lt;b&gt;does not grow with context length&lt;/b&gt;. Whether the model is processing 1,000 or 1,000,000 tokens, the &lt;b&gt;per-token cost remains constant.&lt;/b&gt; &lt;/p&gt;&lt;p&gt;That property alone—constant-time per-token computation—marks a profound departure from transformer behavior.&lt;/p&gt;&lt;p&gt;At the same time, Power Retention preserves the expressive power that made attention successful. Because the recurrence involves tensor powers of the input (hence the name “power retention”), it can represent higher-order dependencies between past and present tokens. &lt;/p&gt;&lt;p&gt;The result is an architecture that can theoretically retain long-term dependencies indefinitely, while remaining as efficient as an RNN and as expressive as a transformer.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Retraining, Not Rebuilding&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Perhaps the most striking aspect of Brumby-14B’s training process is its efficiency. Manifest AI trained the model for only 60 hours on 32 Nvidia H100 GPUs, at a cost of roughly $4,000 — less than 2% of what a conventional model of this scale would cost to train from scratch.&lt;/p&gt;&lt;p&gt;However, since it relied on a transformer-based model, it&amp;#x27;s safe to say that this advance alone will not end the transformer AI-era.&lt;/p&gt;&lt;p&gt;As Jacob Buckman, founder of Manifest AI, clarified in an email to VentureBeat: “The ability to train for $4,000 is indeed only possible when leveraging an existing transformer model,” he said. “Brumby could not be trained from scratch for that price.”&lt;/p&gt;&lt;p&gt;Still, Buckman emphasized the significance of that result: “The reason this is important is that the ability to build on the weights of the previous generation of model architectures is a critical accelerant for the adoption of a new modeling paradigm.” &lt;/p&gt;&lt;p&gt;He argues this demonstrates how attention-free systems can catch up to transformer performance “for orders-of-magnitude less” investment.&lt;/p&gt;&lt;p&gt;In the loss curves released by Manifest AI, Brumby’s training loss quickly converges to that of the Qwen3 baseline within 3,000 training steps, even as the architecture diverges significantly from its transformer origins. &lt;/p&gt;&lt;p&gt;Although Brumby-14B-Base began life as Qwen3-14B-Base, it did not remain identical for long. Manifest AI fundamentally altered Qwen3’s architecture by removing its attention layers—the mathematical engine that defines how a transformer model processes information—and replacing them with their new “power retention” mechanism. This change restructured the model’s internal wiring, effectively giving it a new brain while preserving much of its prior knowledge.&lt;/p&gt;&lt;p&gt;Because of that architectural swap, the existing Qwen3 weights no longer fit perfectly. They were trained to operate within a transformer’s attention dynamics, not the new retention-based system. As a result, the Brumby model initially “forgot” how to apply some of its learned knowledge effectively. The retraining process—about &lt;b&gt;3,000 steps&lt;/b&gt; of additional learning—served to recalibrate those weights, aligning them with the power retention framework without having to start from zero.&lt;/p&gt;&lt;p&gt;A helpful way to think about this is to imagine taking a world-class pianist and handing them a guitar. They already understand rhythm, harmony, and melody, but their hands must learn entirely new patterns to produce the same music. Similarly, Brumby had to relearn how to use its existing knowledge through a new computational instrument. Those 3,000 training steps were, in effect, its crash course in guitar lessons.&lt;/p&gt;&lt;p&gt;By the end of this short retraining phase, Brumby had regained its full performance, reaching the same accuracy as the original Qwen3 model. That quick recovery is what makes the result so significant: it shows that an attention-free system can inherit and adapt the capabilities of a transformer model with only a fraction of the training time and cost.&lt;/p&gt;&lt;p&gt;The benchmark progression plots show a similar trend: the model rapidly approaches its target accuracy on core evaluations like GSM8K, HellaSwag, and MMLU after only a few thousand steps, matching or even slightly surpassing Qwen3 on several tasks.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Benchmarking the Brumby&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Across standard evaluation tasks, &lt;b&gt;Brumby-14B-Base&lt;/b&gt; consistently performs at or near parity with transformer baselines of comparable scale.&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Task&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Brumby-14B&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Qwen3-14B&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GLM-4.5-Air&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Nemotron Nano (12B)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ARC&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.89&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.94&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.92&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.93&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GSM8K&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.88&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.84&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.83&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.84&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GSM8K (Platinum)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.87&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.88&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.85&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.87&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;HellaSwag&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.77&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.81&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.85&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.82&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;MATH&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.62&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.54&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.47&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.26&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;MBPP&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.57&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.75&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.73&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.71&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;MMLU&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.71&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.78&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.77&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.78&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;MMLU (Pro)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.36&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.55&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.51&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.53&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;While it lags slightly behind transformers on knowledge-heavy evaluations like &lt;b&gt;MMLU-Pro&lt;/b&gt;, it matches or outperforms them on &lt;b&gt;mathematical reasoning&lt;/b&gt; and &lt;b&gt;long-context reasoning&lt;/b&gt; tasks—precisely where attention architectures tend to falter. This pattern reinforces the idea that recurrent or retention-based systems may hold a structural advantage for reasoning over extended temporal or logical dependencies.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Hardware Efficiency and Inference Performance&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Brumby’s power retention design offers another major advantage: hardware efficiency.&lt;/p&gt;&lt;p&gt;Because the state update involves only local matrix operations, inference can be implemented with linear complexity in sequence length. &lt;/p&gt;&lt;p&gt;Manifest AI reports that their fastest kernels, developed through their in-house CUDA framework Vidrial, can deliver hundreds-fold speedups over attention on very long contexts.&lt;/p&gt;&lt;p&gt;Buckman said the alpha-stage Power Retention kernels “achieve typical hardware utilization of 80–85%, which is higher than FlashAttention2’s 70–75% or Mamba’s 50–60%.” &lt;/p&gt;&lt;p&gt;(Mamba is another emerging “post-transformer” architecture developed by Carnegie Mellon scientists back in 2023 that, like Power Retention, seeks to eliminate the computational bottleneck of attention. It replaces attention with a &lt;i&gt;state-space&lt;/i&gt; mechanism that processes sequences linearly — updating an internal state over time rather than comparing every token to every other one. This makes it far more efficient for long inputs, though it typically achieves lower hardware utilization than Power Retention in early tests.)&lt;/p&gt;&lt;p&gt;Both Power Retention and Mamba, he added, “expend meaningfully fewer total FLOPs than FlashAttention2 on long contexts, as well as far less memory.” &lt;/p&gt;&lt;p&gt;According to Buckman, the reported 100× speedup comes from this combined improvement in utilization and computational efficiency, though he noted that “we have not yet stress-tested it on production-scale workloads.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Training and Scaling Economics&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Perhaps no statistic in the Brumby release generated more attention than the training cost.&lt;/p&gt;&lt;p&gt;A 14-billion-parameter model, trained for $4,000, represents a two-order-of-magnitude reduction in the cost of foundation model development.&lt;/p&gt;&lt;p&gt;Buckman confirmed that the low cost reflects a broader scaling pattern. “Far from diminishing returns, we have found that ease of retraining improves with scale,” he said. “The number of steps required to successfully retrain a model decreases with its parameter count.” &lt;/p&gt;&lt;p&gt;Manifest has not yet validated the cost of retraining models at 700B parameters, but Buckman projected a range of $10,000–$20,000 for models of that magnitude—still far below transformer training budgets.&lt;/p&gt;&lt;p&gt;He also reiterated that this approach could democratize large-scale experimentation by allowing smaller research groups or companies to retrain or repurpose existing transformer checkpoints without prohibitive compute costs.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Integration and Deployment&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;According to Buckman, converting an existing transformer into a Power Retention model is designed to be simple. &lt;/p&gt;&lt;p&gt;“It is straightforward for any company that is already retraining, post-training, or fine-tuning open-source models,” he said. “Simply pip install retention, change one line of your architecture code, and resume training where you left off.”&lt;/p&gt;&lt;p&gt;He added that after only a small number of GPU-hours, the model typically recovers its original performance—at which point it gains the efficiency benefits of the attention-free design. &lt;/p&gt;&lt;p&gt;“The resulting architecture will permit far faster long-context training and inference than previously,” Buckman noted.&lt;/p&gt;&lt;p&gt;On infrastructure, Buckman said the main Brumby kernels are written in Triton, compatible with both NVIDIA and AMD accelerators. Specialized CUDA kernels are also available through the team’s in-house Vidrial framework. Integration with vLLM and other inference engines remains a work in progress: “We have not yet integrated Power Retention into inference engines, but doing so is a major ongoing initiative at Manifest.”&lt;/p&gt;&lt;p&gt;As for distributed inference, Buckman dismissed concerns about instability: “We have not found this difficulty to be exacerbated in any way by our recurrent-state architecture. In fact, context-parallel training and GPU partitioning for multi-user inference both become significantly cleaner technically when using our approach.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Mission and Long-Term Vision&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Beyond the engineering details, Buckman also described Manifest’s broader mission. “Our mission is to train a neural network to model all human output,” he said. &lt;/p&gt;&lt;p&gt;The team’s goal, he explained, is to move beyond modeling “artifacts of intelligence” toward modeling “the intelligent processes that generated them.” This shift, he argued, requires “fundamentally rethinking” how models are designed and trained—work that Power Retention represents only the beginning of.&lt;/p&gt;&lt;p&gt;The Brumby-14B release, he said, is “one step forward in a long march” toward architectures that can model thought processes continuously and efficiently.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Public Debate and Industry Reception&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The launch of Brumby-14B sparked immediate discussion on X (formerly Twitter), where researchers debated the framing of Manifest AI’s announcement. &lt;/p&gt;&lt;p&gt;Some, including Meta researcher &lt;a href="https://x.com/redtachyon/status/1983819957583421606"&gt;Ariel (@redtachyon)&lt;/a&gt;, argued that the “$4,000 foundation model” tagline was misleading, since the training involved reusing pretrained transformer weights rather than training from scratch.&lt;/p&gt;&lt;p&gt;“They shuffled around the weights of Qwen, fine-tuned it a bit, and called it ‘training a foundation model for $4k,’” Ariel &lt;a href="https://x.com/redtachyon/status/1983820872461722075"&gt;wrote&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Buckman responded publicly, clarifying that the initial tweet had been part of a longer thread explaining the retraining approach. “It’s not like I was being deceptive about it,” he &lt;a href="https://x.com/jacobmbuckman/status/1983875586175996413"&gt;wrote&lt;/a&gt;. “I broke it up into separate tweets, and now everyone is mad about the first one.”&lt;/p&gt;&lt;p&gt;In a follow-up email, Buckman took a measured view of the controversy. “The end of the transformer era is not yet here,” he reiterated, “but the march has begun.” &lt;/p&gt;&lt;p&gt;He also acknowledged that the $4,000 claim, though technically accurate in context, had drawn attention precisely because it challenged expectations about what it costs to experiment at frontier scale.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Conclusion: A Crack in the Transformer’s Wall?&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The release of Brumby-14B-Base is more than an engineering milestone; it is a proof of concept that the transformer’s dominance may finally face credible competition. &lt;/p&gt;&lt;p&gt;By replacing attention with power retention, Manifest AI has demonstrated that performance parity with state-of-the-art transformers is possible at a fraction of the computational cost—and that the long-context bottleneck can be broken without exotic hardware.&lt;/p&gt;&lt;p&gt;The broader implications are twofold. First, the economics of training and serving large models could shift dramatically, lowering the barrier to entry for open research and smaller organizations. &lt;/p&gt;&lt;p&gt;Second, the architectural diversity of AI models may expand again, reigniting theoretical and empirical exploration after half a decade of transformer monoculture.&lt;/p&gt;&lt;p&gt;As Buckman put it: “The end of the transformer era is not yet here. Our release is just one step forward in a long march toward the future.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/attention-isnt-all-you-need-new-qwen3-variant-brumby-14b-base-leverages</guid><pubDate>Tue, 04 Nov 2025 19:37:00 +0000</pubDate></item><item><title>Databricks research reveals that building better AI judges isn't just a technical concern, it's a people problem (AI | VentureBeat)</title><link>https://venturebeat.com/ai/databricks-research-reveals-that-building-better-ai-judges-isnt-just-a</link><description>[unable to retrieve full-text content]&lt;p&gt;The intelligence of AI models isn&amp;#x27;t what&amp;#x27;s blocking enterprise deployments. It&amp;#x27;s the inability to define and measure quality in the first place.&lt;/p&gt;&lt;p&gt;That&amp;#x27;s where AI judges are now playing an increasingly important role. In AI evaluation, a &amp;quot;judge&amp;quot; is an AI system that scores outputs from another AI system. &lt;/p&gt;&lt;p&gt;Judge Builder is Databricks&amp;#x27; framework for creating judges and was first deployed as part of the company&amp;#x27;s&lt;a href="https://venturebeat.com/ai/why-most-enterprise-ai-agents-never-reach-production-and-how-databricks-plans-t"&gt; &lt;u&gt;Agent Bricks&lt;/u&gt;&lt;/a&gt; technology earlier this year. The framework has evolved significantly since its initial launch in response to direct user feedback and deployments.&lt;/p&gt;&lt;p&gt;Early versions focused on technical implementation but customer feedback revealed the real bottleneck was organizational alignment. Databricks now offers a structured workshop process that guides teams through three core challenges: getting stakeholders to agree on quality criteria, capturing domain expertise from limited subject matter experts and deploying evaluation systems at scale.&lt;/p&gt;&lt;p&gt;&amp;quot;The intelligence of the model is typically not the bottleneck, the models are really smart,&amp;quot; Jonathan Frankle, Databricks&amp;#x27; chief AI scientist, told VentureBeat in an exclusive briefing. &amp;quot;Instead, it&amp;#x27;s really about asking, how do we get the models to do what we want, and how do we know if they did what we wanted?&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The &amp;#x27;Ouroboros problem&amp;#x27; of AI evaluation&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Judge Builder addresses what Pallavi Koppol, a Databricks research scientist who led the development, calls the &amp;quot;Ouroboros problem.&amp;quot;  An Ouroboros is an ancient symbol that depicts a snake eating its own tail. &lt;/p&gt;&lt;p&gt;Using AI systems to evaluate AI systems creates a circular validation challenge.&lt;/p&gt;&lt;p&gt;&amp;quot;You want a judge to see if your system is good, if your AI system is good, but then your judge is also an AI system,&amp;quot; Koppol explained. &amp;quot;And now you&amp;#x27;re saying like, well, how do I know this judge is good?&amp;quot;&lt;/p&gt;&lt;p&gt;The solution is measuring &amp;quot;distance to human expert ground truth&amp;quot; as the primary scoring function. By minimizing the gap between how an AI judge scores outputs versus how domain experts would score them, organizations can trust these judges as scalable proxies for human evaluation.&lt;/p&gt;&lt;p&gt;This approach differs fundamentally from traditional&lt;a href="https://venturebeat.com/ai/beyond-detection-why-automatically-correcting-hallucinations-could-transform-enterprise-ai-adoption"&gt; &lt;u&gt;guardrail systems&lt;/u&gt;&lt;/a&gt; or single-metric evaluations. Rather than asking whether an AI output passed or failed on a generic quality check, Judge Builder creates highly specific evaluation criteria tailored to each organization&amp;#x27;s domain expertise and business requirements.&lt;/p&gt;&lt;p&gt;The technical implementation also sets it apart. Judge Builder integrates with Databricks&amp;#x27; MLflow and &lt;a href="https://venturebeat.com/ai/the-usd100m-openai-partnership-is-nice-but-databricks-real-breakthrough"&gt;&lt;u&gt;prompt optimization&lt;/u&gt;&lt;/a&gt; tools and can work with any underlying model. Teams can version control their judges, track performance over time and deploy multiple judges simultaneously across different quality dimensions.&lt;/p&gt;&lt;h2&gt;Lessons learned: Building judges that actually work&lt;/h2&gt;&lt;p&gt;Databricks&amp;#x27; work with enterprise customers revealed three critical lessons that apply to anyone building AI judges.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Lesson one: Your experts don&amp;#x27;t agree as much as you think.&lt;/b&gt; When quality is subjective, organizations discover that even their own subject matter experts disagree on what constitutes acceptable output. A customer service response might be factually correct but use an inappropriate tone. A financial summary might be comprehensive but too technical for the intended audience.&lt;/p&gt;&lt;p&gt;&amp;quot;One of the biggest lessons of this whole process is that all problems become people problems,&amp;quot; Frankle said. &amp;quot;The hardest part is getting an idea out of a person&amp;#x27;s brain and into something explicit. And the harder part is that companies are not one brain, but many brains.&amp;quot;&lt;/p&gt;&lt;p&gt;The fix is batched annotation with inter-rater reliability checks. Teams annotate examples in small groups, then measure agreement scores before proceeding. This catches misalignment early. In one case, three experts gave ratings of 1, 5 and neutral for the same output before discussion revealed they were interpreting the evaluation criteria differently.&lt;/p&gt;&lt;p&gt;Companies using this approach achieve inter-rater reliability scores as high as 0.6 compared to typical scores of 0.3 from external annotation services. Higher agreement translates directly to better judge performance because the training data contains less noise.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Lesson two: Break down vague criteria into specific judges.&lt;/b&gt; Instead of one judge evaluating whether a response is &amp;quot;relevant, factual and concise,&amp;quot; create three separate judges. Each targets a specific quality aspect. This granularity matters because a failing &amp;quot;overall quality&amp;quot; score reveals something is wrong but not what to fix.&lt;/p&gt;&lt;p&gt;The best results come from combining top-down requirements such as regulatory constraints, stakeholder priorities, with bottom-up discovery of observed failure patterns. One customer built a top-down judge for correctness but discovered through data analysis that correct responses almost always cited the top two retrieval results. This insight became a new production-friendly judge that could proxy for correctness without requiring ground-truth labels.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Lesson three: You need fewer examples than you think.&lt;/b&gt; Teams can create robust judges from just 20-30 well-chosen examples. The key is selecting edge cases that expose disagreement rather than obvious examples where everyone agrees.&lt;/p&gt;&lt;p&gt;&amp;quot;We&amp;#x27;re able to run this process with some teams in as little as three hours, so it doesn&amp;#x27;t really take that long to start getting a good judge,&amp;quot; Koppol said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Production results: From pilots to seven-figure deployments&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Frankle shared three metrics Databricks uses to measure Judge Builder&amp;#x27;s success: whether customers want to use it again, whether they increase AI spending and whether they progress further in their AI journey.&lt;/p&gt;&lt;p&gt;On the first metric, one customer created more than a dozen judges after their initial workshop. &amp;quot;This customer made more than a dozen judges after we walked them through doing this in a rigorous way for the first time with this framework,&amp;quot; Frankle said. &amp;quot;They really went to town on judges and are now measuring everything.&amp;quot;&lt;/p&gt;&lt;p&gt;For the second metric, the business impact is clear. &amp;quot;There are multiple customers who have gone through this workshop and have become seven-figure spenders on GenAI at Databricks in a way that they weren&amp;#x27;t before,&amp;quot; Frankle said.&lt;/p&gt;&lt;p&gt;The third metric reveals Judge Builder&amp;#x27;s strategic value. Customers who previously hesitated to use advanced techniques like reinforcement learning now feel confident deploying them because they can measure whether improvements actually occurred.&lt;/p&gt;&lt;p&gt;&amp;quot;There are customers who have gone and done very advanced things after having had these judges where they were reluctant to do so before,&amp;quot; Frankle said. &amp;quot;They&amp;#x27;ve moved from doing a little bit of prompt engineering to doing reinforcement learning with us. Why spend the money on reinforcement learning, and why spend the energy on reinforcement learning if you don&amp;#x27;t know whether it actually made a difference?&amp;quot;&lt;/p&gt;&lt;h2&gt;What enterprises should do now&lt;/h2&gt;&lt;p&gt;The teams successfully moving AI from pilot to production treat judges not as one-time artifacts but as evolving assets that grow with their systems.&lt;/p&gt;&lt;p&gt;Databricks recommends three practical steps. First, focus on high-impact judges by identifying one critical regulatory requirement plus one observed failure mode. These become your initial judge portfolio.&lt;/p&gt;&lt;p&gt;Second, create lightweight workflows with subject matter experts. A few hours reviewing 20-30 edge cases provides sufficient calibration for most judges. Use batched annotation and inter-rater reliability checks to denoise your data.&lt;/p&gt;&lt;p&gt;Third, schedule regular judge reviews using production data. New failure modes will emerge as your system evolves. Your judge portfolio should evolve with them.&lt;/p&gt;&lt;p&gt;&amp;quot;A judge is a way to evaluate a model, it&amp;#x27;s also a way to create guardrails, it&amp;#x27;s also a way to have a metric against which you can do prompt optimization and it&amp;#x27;s also a way to have a metric against which you can do reinforcement learning,&amp;quot; Frankle said. &amp;quot;Once you have a judge that you know represents your human taste in an empirical form that you can query as much as you want, you can use it in 10,000 different ways to measure or improve your agents.&amp;quot;&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;The intelligence of AI models isn&amp;#x27;t what&amp;#x27;s blocking enterprise deployments. It&amp;#x27;s the inability to define and measure quality in the first place.&lt;/p&gt;&lt;p&gt;That&amp;#x27;s where AI judges are now playing an increasingly important role. In AI evaluation, a &amp;quot;judge&amp;quot; is an AI system that scores outputs from another AI system. &lt;/p&gt;&lt;p&gt;Judge Builder is Databricks&amp;#x27; framework for creating judges and was first deployed as part of the company&amp;#x27;s&lt;a href="https://venturebeat.com/ai/why-most-enterprise-ai-agents-never-reach-production-and-how-databricks-plans-t"&gt; &lt;u&gt;Agent Bricks&lt;/u&gt;&lt;/a&gt; technology earlier this year. The framework has evolved significantly since its initial launch in response to direct user feedback and deployments.&lt;/p&gt;&lt;p&gt;Early versions focused on technical implementation but customer feedback revealed the real bottleneck was organizational alignment. Databricks now offers a structured workshop process that guides teams through three core challenges: getting stakeholders to agree on quality criteria, capturing domain expertise from limited subject matter experts and deploying evaluation systems at scale.&lt;/p&gt;&lt;p&gt;&amp;quot;The intelligence of the model is typically not the bottleneck, the models are really smart,&amp;quot; Jonathan Frankle, Databricks&amp;#x27; chief AI scientist, told VentureBeat in an exclusive briefing. &amp;quot;Instead, it&amp;#x27;s really about asking, how do we get the models to do what we want, and how do we know if they did what we wanted?&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The &amp;#x27;Ouroboros problem&amp;#x27; of AI evaluation&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Judge Builder addresses what Pallavi Koppol, a Databricks research scientist who led the development, calls the &amp;quot;Ouroboros problem.&amp;quot;  An Ouroboros is an ancient symbol that depicts a snake eating its own tail. &lt;/p&gt;&lt;p&gt;Using AI systems to evaluate AI systems creates a circular validation challenge.&lt;/p&gt;&lt;p&gt;&amp;quot;You want a judge to see if your system is good, if your AI system is good, but then your judge is also an AI system,&amp;quot; Koppol explained. &amp;quot;And now you&amp;#x27;re saying like, well, how do I know this judge is good?&amp;quot;&lt;/p&gt;&lt;p&gt;The solution is measuring &amp;quot;distance to human expert ground truth&amp;quot; as the primary scoring function. By minimizing the gap between how an AI judge scores outputs versus how domain experts would score them, organizations can trust these judges as scalable proxies for human evaluation.&lt;/p&gt;&lt;p&gt;This approach differs fundamentally from traditional&lt;a href="https://venturebeat.com/ai/beyond-detection-why-automatically-correcting-hallucinations-could-transform-enterprise-ai-adoption"&gt; &lt;u&gt;guardrail systems&lt;/u&gt;&lt;/a&gt; or single-metric evaluations. Rather than asking whether an AI output passed or failed on a generic quality check, Judge Builder creates highly specific evaluation criteria tailored to each organization&amp;#x27;s domain expertise and business requirements.&lt;/p&gt;&lt;p&gt;The technical implementation also sets it apart. Judge Builder integrates with Databricks&amp;#x27; MLflow and &lt;a href="https://venturebeat.com/ai/the-usd100m-openai-partnership-is-nice-but-databricks-real-breakthrough"&gt;&lt;u&gt;prompt optimization&lt;/u&gt;&lt;/a&gt; tools and can work with any underlying model. Teams can version control their judges, track performance over time and deploy multiple judges simultaneously across different quality dimensions.&lt;/p&gt;&lt;h2&gt;Lessons learned: Building judges that actually work&lt;/h2&gt;&lt;p&gt;Databricks&amp;#x27; work with enterprise customers revealed three critical lessons that apply to anyone building AI judges.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Lesson one: Your experts don&amp;#x27;t agree as much as you think.&lt;/b&gt; When quality is subjective, organizations discover that even their own subject matter experts disagree on what constitutes acceptable output. A customer service response might be factually correct but use an inappropriate tone. A financial summary might be comprehensive but too technical for the intended audience.&lt;/p&gt;&lt;p&gt;&amp;quot;One of the biggest lessons of this whole process is that all problems become people problems,&amp;quot; Frankle said. &amp;quot;The hardest part is getting an idea out of a person&amp;#x27;s brain and into something explicit. And the harder part is that companies are not one brain, but many brains.&amp;quot;&lt;/p&gt;&lt;p&gt;The fix is batched annotation with inter-rater reliability checks. Teams annotate examples in small groups, then measure agreement scores before proceeding. This catches misalignment early. In one case, three experts gave ratings of 1, 5 and neutral for the same output before discussion revealed they were interpreting the evaluation criteria differently.&lt;/p&gt;&lt;p&gt;Companies using this approach achieve inter-rater reliability scores as high as 0.6 compared to typical scores of 0.3 from external annotation services. Higher agreement translates directly to better judge performance because the training data contains less noise.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Lesson two: Break down vague criteria into specific judges.&lt;/b&gt; Instead of one judge evaluating whether a response is &amp;quot;relevant, factual and concise,&amp;quot; create three separate judges. Each targets a specific quality aspect. This granularity matters because a failing &amp;quot;overall quality&amp;quot; score reveals something is wrong but not what to fix.&lt;/p&gt;&lt;p&gt;The best results come from combining top-down requirements such as regulatory constraints, stakeholder priorities, with bottom-up discovery of observed failure patterns. One customer built a top-down judge for correctness but discovered through data analysis that correct responses almost always cited the top two retrieval results. This insight became a new production-friendly judge that could proxy for correctness without requiring ground-truth labels.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Lesson three: You need fewer examples than you think.&lt;/b&gt; Teams can create robust judges from just 20-30 well-chosen examples. The key is selecting edge cases that expose disagreement rather than obvious examples where everyone agrees.&lt;/p&gt;&lt;p&gt;&amp;quot;We&amp;#x27;re able to run this process with some teams in as little as three hours, so it doesn&amp;#x27;t really take that long to start getting a good judge,&amp;quot; Koppol said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Production results: From pilots to seven-figure deployments&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Frankle shared three metrics Databricks uses to measure Judge Builder&amp;#x27;s success: whether customers want to use it again, whether they increase AI spending and whether they progress further in their AI journey.&lt;/p&gt;&lt;p&gt;On the first metric, one customer created more than a dozen judges after their initial workshop. &amp;quot;This customer made more than a dozen judges after we walked them through doing this in a rigorous way for the first time with this framework,&amp;quot; Frankle said. &amp;quot;They really went to town on judges and are now measuring everything.&amp;quot;&lt;/p&gt;&lt;p&gt;For the second metric, the business impact is clear. &amp;quot;There are multiple customers who have gone through this workshop and have become seven-figure spenders on GenAI at Databricks in a way that they weren&amp;#x27;t before,&amp;quot; Frankle said.&lt;/p&gt;&lt;p&gt;The third metric reveals Judge Builder&amp;#x27;s strategic value. Customers who previously hesitated to use advanced techniques like reinforcement learning now feel confident deploying them because they can measure whether improvements actually occurred.&lt;/p&gt;&lt;p&gt;&amp;quot;There are customers who have gone and done very advanced things after having had these judges where they were reluctant to do so before,&amp;quot; Frankle said. &amp;quot;They&amp;#x27;ve moved from doing a little bit of prompt engineering to doing reinforcement learning with us. Why spend the money on reinforcement learning, and why spend the energy on reinforcement learning if you don&amp;#x27;t know whether it actually made a difference?&amp;quot;&lt;/p&gt;&lt;h2&gt;What enterprises should do now&lt;/h2&gt;&lt;p&gt;The teams successfully moving AI from pilot to production treat judges not as one-time artifacts but as evolving assets that grow with their systems.&lt;/p&gt;&lt;p&gt;Databricks recommends three practical steps. First, focus on high-impact judges by identifying one critical regulatory requirement plus one observed failure mode. These become your initial judge portfolio.&lt;/p&gt;&lt;p&gt;Second, create lightweight workflows with subject matter experts. A few hours reviewing 20-30 edge cases provides sufficient calibration for most judges. Use batched annotation and inter-rater reliability checks to denoise your data.&lt;/p&gt;&lt;p&gt;Third, schedule regular judge reviews using production data. New failure modes will emerge as your system evolves. Your judge portfolio should evolve with them.&lt;/p&gt;&lt;p&gt;&amp;quot;A judge is a way to evaluate a model, it&amp;#x27;s also a way to create guardrails, it&amp;#x27;s also a way to have a metric against which you can do prompt optimization and it&amp;#x27;s also a way to have a metric against which you can do reinforcement learning,&amp;quot; Frankle said. &amp;quot;Once you have a judge that you know represents your human taste in an empirical form that you can query as much as you want, you can use it in 10,000 different ways to measure or improve your agents.&amp;quot;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/databricks-research-reveals-that-building-better-ai-judges-isnt-just-a</guid><pubDate>Tue, 04 Nov 2025 20:00:00 +0000</pubDate></item><item><title>Google’s AI Mode gets new agentic capabilities to help book event tickets and beauty appointments (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/04/googles-ai-mode-gets-new-agentic-capabilities-to-help-book-event-tickets-and-beauty-appointments/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Tuesday that it’s launching new agentic capabilities in AI Mode, its feature that allows users to ask complex questions and follow-ups to dig deeper on a topic directly within Search. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users can now get help with booking event tickets and beauty and wellness appointments in AI Mode.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For example, you can say, “Find me two cheap tickets for the Shaboozey concert coming up. Prefer standing floor tickets.” AI Mode will then search across multiple websites to find real-time ticket options that meet your specific requests. It will then present you with a curated list of ticket prices to choose from. AI Mode links you directly to the booking page so you can finalize your purchase.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new agentic capabilities are available to all users opted into Google’s experimental arm, Search Labs, in the U.S. The company notes that Google AI Pro and Ultra subscribers have access to high limits.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google first brought agentic capabilities to AI Mode back in August, when it started letting people use the feature to find restaurant reservations.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="a selection of screenshots showing AI mode." class="wp-image-3064998" height="742" src="https://techcrunch.com/wp-content/uploads/2025/11/Screenshot-2025-11-04-at-2.50.08PM.png" width="1696" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;With this capability, you can request dinner reservations based on multiple preferences, such as party size, date, time, location, and preferred cuisine. For example, you could ask, “Find me a dinner reservation for three people this Friday after 6 p.m. around Logan Square. Craving ramen or bibimbap.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI Mode will then search across different reservation platforms to find real-time availability for restaurants that match the inquiry. It then surfaces a curated list of options to choose from. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our priority in Google Search is connecting you with high-quality information you can rely on,” Google explained on its Search Labs page. “This new mode is rooted in our core quality and safety systems, but it’s still an early experiment and may make mistakes.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google first launched AI Mode in March to take on popular services like Perplexity AI and OpenAI’s ChatGPT Search. Since then, the tech giant has brought AI Mode to more than 180 countries and has been building it out with new functionalities. For instance, AI Mode recently got access to a Canvas feature that helps you build study plans and organize information over multiple sessions in a side panel. It also now lets you use Google Lens to ask about what’s on your desktop screen.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Tuesday that it’s launching new agentic capabilities in AI Mode, its feature that allows users to ask complex questions and follow-ups to dig deeper on a topic directly within Search. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users can now get help with booking event tickets and beauty and wellness appointments in AI Mode.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For example, you can say, “Find me two cheap tickets for the Shaboozey concert coming up. Prefer standing floor tickets.” AI Mode will then search across multiple websites to find real-time ticket options that meet your specific requests. It will then present you with a curated list of ticket prices to choose from. AI Mode links you directly to the booking page so you can finalize your purchase.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new agentic capabilities are available to all users opted into Google’s experimental arm, Search Labs, in the U.S. The company notes that Google AI Pro and Ultra subscribers have access to high limits.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google first brought agentic capabilities to AI Mode back in August, when it started letting people use the feature to find restaurant reservations.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="a selection of screenshots showing AI mode." class="wp-image-3064998" height="742" src="https://techcrunch.com/wp-content/uploads/2025/11/Screenshot-2025-11-04-at-2.50.08PM.png" width="1696" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;With this capability, you can request dinner reservations based on multiple preferences, such as party size, date, time, location, and preferred cuisine. For example, you could ask, “Find me a dinner reservation for three people this Friday after 6 p.m. around Logan Square. Craving ramen or bibimbap.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI Mode will then search across different reservation platforms to find real-time availability for restaurants that match the inquiry. It then surfaces a curated list of options to choose from. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our priority in Google Search is connecting you with high-quality information you can rely on,” Google explained on its Search Labs page. “This new mode is rooted in our core quality and safety systems, but it’s still an early experiment and may make mistakes.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google first launched AI Mode in March to take on popular services like Perplexity AI and OpenAI’s ChatGPT Search. Since then, the tech giant has brought AI Mode to more than 180 countries and has been building it out with new functionalities. For instance, AI Mode recently got access to a Canvas feature that helps you build study plans and organize information over multiple sessions in a side panel. It also now lets you use Google Lens to ask about what’s on your desktop screen.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/04/googles-ai-mode-gets-new-agentic-capabilities-to-help-book-event-tickets-and-beauty-appointments/</guid><pubDate>Tue, 04 Nov 2025 20:36:18 +0000</pubDate></item><item><title>Meet Project Suncatcher, Google’s plan to put AI data centers in space (AI – Ars Technica)</title><link>https://arstechnica.com/google/2025/11/meet-project-suncatcher-googles-plan-to-put-ai-data-centers-in-space/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google is already zapping TPUs with radiation to get ready.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Project Suncatcher" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Suncatcher_hero-640x360.png" width="640" /&gt;
                  &lt;img alt="Project Suncatcher" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Suncatcher_hero-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The tech industry is on a tear, building data centers for AI as quickly as they can buy up the land. The sky-high energy costs and logistical headaches of managing all those data centers have prompted interest in space-based infrastructure. Moguls like Jeff Bezos and Elon Musk have mused about putting GPUs in space, and now Google confirms it’s working on its own version of the technology. The company’s latest “moonshot” is known as Project Suncatcher, and if all goes as planned, Google hopes it will lead to scalable networks of orbiting TPUs.&lt;/p&gt;
&lt;p&gt;The space around Earth has changed a lot in the last few years. A new generation of satellite constellations like Starlink has shown it’s feasible to relay Internet communication via orbital systems. Deploying high-performance AI accelerators in space along similar lines would be a boon to the industry’s never-ending build-out. Google notes that space may be “the best place to scale AI compute.”&lt;/p&gt;
&lt;p&gt;Google’s vision for scalable orbiting data centers relies on solar-powered satellites with free-space optical links connecting the nodes into a distributed network. Naturally, there are numerous engineering challenges to solve before Project Suncatcher is real. As a reference, Google points to the long road from its first moonshot self-driving cars 15 years ago to the Waymo vehicles that are almost fully autonomous today.&lt;/p&gt;
&lt;h2&gt;Taking AI to space&lt;/h2&gt;
&lt;p&gt;Some of the benefits are obvious. Google’s vision for Suncatcher, as explained in a pre-print study (PDF), would place the satellites in a dawn-dusk sun-synchronous low-earth orbit. That ensures they would get almost constant sunlight exposure (hence the name). The cost of electricity on Earth is a problem for large data centers, and even moving them all to solar power wouldn’t get the job done. Google notes solar panels are up to eight times more efficient in orbit than they are on the surface of Earth. Lots of uninterrupted sunlight at higher efficiency means more power for data processing.&lt;/p&gt;
&lt;p&gt;A major sticking point is how you can keep satellites connected at high speeds as they orbit. On Earth, the nodes in a data center communicate via blazing-fast optical interconnect chips. Maintaining high-speed communication among the orbiting servers will require wireless solutions that can operate at tens of terabits per second. Early testing on Earth has demonstrated bidirectional speeds up to 1.6 Tbps—Google believes this can be scaled up over time.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="496" id="video-2125616-1" preload="metadata" width="490"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Suncatcher-1.mp4?_=1" type="video/mp4" /&gt;Google’s proposed free-fall (“no thrust”) constellation for linked satellites; arrow pointing toward Earth.&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
    &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Google’s proposed free-fall (“no thrust”) constellation for linked satellites; arrow pointing toward Earth.

          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;However, there is the problem of physics. Received power decreases with the square of distance, so Google notes the satellites would have to maintain proximity of a kilometer or less. That would require a tighter formation than any currently operational constellation, but it should be workable. Google has developed analytical models suggesting that satellites positioned several hundred meters apart would require only “modest station-keeping maneuvers.”&lt;/p&gt;
&lt;p&gt;Hardware designed for space is expensive and often less capable compared to terrestrial systems because the former needs to be hardened against extreme temperatures and radiation. Google’s approach to Project Suncatcher is to reuse the components used on Earth, which might not be very robust when you stuff them in a satellite. However, innovations like the Snapdragon-powered Mars Ingenuity helicopter have shown that off-the-shelf hardware may survive longer in space than we thought.&lt;/p&gt;
&lt;p&gt;Google says Suncatcher only works if TPUs can run for at least five years, which works out to 750 rad. The company is testing this by blasting its latest v6e Cloud TPU (Trillium) in a 67MeV proton beam. Google says that while the memory was most vulnerable to damage, the experiments showed that TPUs can handle about three times as much radiation (almost 2 krad) before data corruption was detected.&lt;/p&gt;
&lt;p&gt;Google hopes to launch a pair of prototype satellites with TPUs by early 2027. It expects the launch cost of these first AI orbiters to be quite high. However, Google is planning for the mid-2030s when launch costs are projected to drop to as little as $200 per kilogram. At that level, space-based data centers could become as economical as the terrestrial versions.&lt;/p&gt;
&lt;p&gt;The fact is, terrestrial data centers are dirty, noisy, and ravenous for power and water. This has led many communities to oppose plans to build them near the places where people live and work. Putting them in space could solve everyone’s problems (unless you’re an astronomer).&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google is already zapping TPUs with radiation to get ready.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Project Suncatcher" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Suncatcher_hero-640x360.png" width="640" /&gt;
                  &lt;img alt="Project Suncatcher" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Suncatcher_hero-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The tech industry is on a tear, building data centers for AI as quickly as they can buy up the land. The sky-high energy costs and logistical headaches of managing all those data centers have prompted interest in space-based infrastructure. Moguls like Jeff Bezos and Elon Musk have mused about putting GPUs in space, and now Google confirms it’s working on its own version of the technology. The company’s latest “moonshot” is known as Project Suncatcher, and if all goes as planned, Google hopes it will lead to scalable networks of orbiting TPUs.&lt;/p&gt;
&lt;p&gt;The space around Earth has changed a lot in the last few years. A new generation of satellite constellations like Starlink has shown it’s feasible to relay Internet communication via orbital systems. Deploying high-performance AI accelerators in space along similar lines would be a boon to the industry’s never-ending build-out. Google notes that space may be “the best place to scale AI compute.”&lt;/p&gt;
&lt;p&gt;Google’s vision for scalable orbiting data centers relies on solar-powered satellites with free-space optical links connecting the nodes into a distributed network. Naturally, there are numerous engineering challenges to solve before Project Suncatcher is real. As a reference, Google points to the long road from its first moonshot self-driving cars 15 years ago to the Waymo vehicles that are almost fully autonomous today.&lt;/p&gt;
&lt;h2&gt;Taking AI to space&lt;/h2&gt;
&lt;p&gt;Some of the benefits are obvious. Google’s vision for Suncatcher, as explained in a pre-print study (PDF), would place the satellites in a dawn-dusk sun-synchronous low-earth orbit. That ensures they would get almost constant sunlight exposure (hence the name). The cost of electricity on Earth is a problem for large data centers, and even moving them all to solar power wouldn’t get the job done. Google notes solar panels are up to eight times more efficient in orbit than they are on the surface of Earth. Lots of uninterrupted sunlight at higher efficiency means more power for data processing.&lt;/p&gt;
&lt;p&gt;A major sticking point is how you can keep satellites connected at high speeds as they orbit. On Earth, the nodes in a data center communicate via blazing-fast optical interconnect chips. Maintaining high-speed communication among the orbiting servers will require wireless solutions that can operate at tens of terabits per second. Early testing on Earth has demonstrated bidirectional speeds up to 1.6 Tbps—Google believes this can be scaled up over time.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="496" id="video-2125616-1" preload="metadata" width="490"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Suncatcher-1.mp4?_=1" type="video/mp4" /&gt;Google’s proposed free-fall (“no thrust”) constellation for linked satellites; arrow pointing toward Earth.&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
    &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Google’s proposed free-fall (“no thrust”) constellation for linked satellites; arrow pointing toward Earth.

          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;However, there is the problem of physics. Received power decreases with the square of distance, so Google notes the satellites would have to maintain proximity of a kilometer or less. That would require a tighter formation than any currently operational constellation, but it should be workable. Google has developed analytical models suggesting that satellites positioned several hundred meters apart would require only “modest station-keeping maneuvers.”&lt;/p&gt;
&lt;p&gt;Hardware designed for space is expensive and often less capable compared to terrestrial systems because the former needs to be hardened against extreme temperatures and radiation. Google’s approach to Project Suncatcher is to reuse the components used on Earth, which might not be very robust when you stuff them in a satellite. However, innovations like the Snapdragon-powered Mars Ingenuity helicopter have shown that off-the-shelf hardware may survive longer in space than we thought.&lt;/p&gt;
&lt;p&gt;Google says Suncatcher only works if TPUs can run for at least five years, which works out to 750 rad. The company is testing this by blasting its latest v6e Cloud TPU (Trillium) in a 67MeV proton beam. Google says that while the memory was most vulnerable to damage, the experiments showed that TPUs can handle about three times as much radiation (almost 2 krad) before data corruption was detected.&lt;/p&gt;
&lt;p&gt;Google hopes to launch a pair of prototype satellites with TPUs by early 2027. It expects the launch cost of these first AI orbiters to be quite high. However, Google is planning for the mid-2030s when launch costs are projected to drop to as little as $200 per kilogram. At that level, space-based data centers could become as economical as the terrestrial versions.&lt;/p&gt;
&lt;p&gt;The fact is, terrestrial data centers are dirty, noisy, and ravenous for power and water. This has led many communities to oppose plans to build them near the places where people live and work. Putting them in space could solve everyone’s problems (unless you’re an astronomer).&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2025/11/meet-project-suncatcher-googles-plan-to-put-ai-data-centers-in-space/</guid><pubDate>Tue, 04 Nov 2025 20:59:02 +0000</pubDate></item><item><title>Rivian creates another spinoff company called Mind Robotics (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/04/rivian-creates-another-spinoff-company-called-mind-robotics/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/Rivian-QUAD-R1S-sunset.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Rivian has created its second spinoff company this year: an industrial AI and robotics venture called Mind Robotics.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new effort will be focused around using “industrial AI to reshape how physical world businesses operate and leverage Rivian operations data as the foundation for a robotics data flywheel,” according to the company’s third-quarter shareholder letter published Tuesday.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That’s a mouthful of buzzwords, and Rivian declined to clarify beyond that explanation. On an investor call Tuesday, Rivian CEO RJ Scaringe said his company realized it had the chance to “develop products and robotic solutions that allow us to run and operate our manufacturing plants more efficiently.” Scaringe will serve as chairman of the board of directors for Mind Robotics, according to a filing, and Rivian is a shareholder, he said on the call.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As much as we’ve seen AI shift how we operate and run our businesses through the wide-ranging applications for LLMs, the potential for AI to really shift how we think about operating in the physical world is, in some ways, unimaginably large,” Scaringe said on the call. “So the creation of this company is ultimately the culmination of us coming to the view that we wanted to have direct control and direct influence over the design and development of advanced AI robotics that would be very focused on industrial applications.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mind Robotics has already raised a $115 million seed round, which was led by VC firm Eclipse. Jiten Behl, a partner at Eclipse who also used to work at Rivian, revealed the investment in a LinkedIn post after TechCrunch previously reported the firm’s involvement in an earlier version of this story.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The launch of Mind Robotics marks the second time this year that Rivian has created a new stand-alone company. In March, the company spun out its skunkworks micromobility division into a startup called Also Inc. That new company was funded in part by money from Eclipse, with additional funding from Greenoaks Capital.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s unclear if Rivian employees are moving over to Mind Robotics, like was the case with Also. A Rivian spokesperson declined to say. But the company hinted at the possibility in Tuesday’s letter.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“With our strong bench of technology talent and an innovation-driven culture, we have been able to identify additional areas of value to accelerate our mission on a wider scale while maintaining Rivian’s focus,” Scaringe wrote.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Robotics and industrial AI are hot areas for investment right now. There is a slew of humanoid robotics companies raising money and trying to ship products, including Tesla. General Motors is working on its own robotics and AI division, too. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond Rivian’s announcement Tuesday, though, very little is known about what Mind Robotics will get up to. There is essentially no digital footprint for the company yet, save for the trademark application. That application is very broadly targeted and says Mind Robotics could use the trademark for everything from machinery, to vehicles, to “incubators for eggs.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story has been updated with new information from a regulatory filing in the third paragraph, the LinkedIn post in the fifth paragraph, and from the investor call throughout.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/Rivian-QUAD-R1S-sunset.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Rivian has created its second spinoff company this year: an industrial AI and robotics venture called Mind Robotics.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new effort will be focused around using “industrial AI to reshape how physical world businesses operate and leverage Rivian operations data as the foundation for a robotics data flywheel,” according to the company’s third-quarter shareholder letter published Tuesday.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That’s a mouthful of buzzwords, and Rivian declined to clarify beyond that explanation. On an investor call Tuesday, Rivian CEO RJ Scaringe said his company realized it had the chance to “develop products and robotic solutions that allow us to run and operate our manufacturing plants more efficiently.” Scaringe will serve as chairman of the board of directors for Mind Robotics, according to a filing, and Rivian is a shareholder, he said on the call.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As much as we’ve seen AI shift how we operate and run our businesses through the wide-ranging applications for LLMs, the potential for AI to really shift how we think about operating in the physical world is, in some ways, unimaginably large,” Scaringe said on the call. “So the creation of this company is ultimately the culmination of us coming to the view that we wanted to have direct control and direct influence over the design and development of advanced AI robotics that would be very focused on industrial applications.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mind Robotics has already raised a $115 million seed round, which was led by VC firm Eclipse. Jiten Behl, a partner at Eclipse who also used to work at Rivian, revealed the investment in a LinkedIn post after TechCrunch previously reported the firm’s involvement in an earlier version of this story.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The launch of Mind Robotics marks the second time this year that Rivian has created a new stand-alone company. In March, the company spun out its skunkworks micromobility division into a startup called Also Inc. That new company was funded in part by money from Eclipse, with additional funding from Greenoaks Capital.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s unclear if Rivian employees are moving over to Mind Robotics, like was the case with Also. A Rivian spokesperson declined to say. But the company hinted at the possibility in Tuesday’s letter.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“With our strong bench of technology talent and an innovation-driven culture, we have been able to identify additional areas of value to accelerate our mission on a wider scale while maintaining Rivian’s focus,” Scaringe wrote.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Robotics and industrial AI are hot areas for investment right now. There is a slew of humanoid robotics companies raising money and trying to ship products, including Tesla. General Motors is working on its own robotics and AI division, too. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond Rivian’s announcement Tuesday, though, very little is known about what Mind Robotics will get up to. There is essentially no digital footprint for the company yet, save for the trademark application. That application is very broadly targeted and says Mind Robotics could use the trademark for everything from machinery, to vehicles, to “incubators for eggs.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story has been updated with new information from a regulatory filing in the third paragraph, the LinkedIn post in the fifth paragraph, and from the investor call throughout.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/04/rivian-creates-another-spinoff-company-called-mind-robotics/</guid><pubDate>Tue, 04 Nov 2025 21:03:43 +0000</pubDate></item><item><title>People Inc. forges AI licensing deal with Microsoft as Google traffic drops (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/04/people-inc-forges-ai-licensing-deal-with-microsoft-as-google-traffic-drops/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/Neil-Vogel.jpg?resize=1200,593" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;People Inc., one of the largest media publishers in the U.S., has signed an AI licensing deal with Microsoft. The media giant (formerly known as Dotdash Meredith) made the announcement Tuesday as a part of parent company IAC’s third-quarter earnings. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Under the deal, People Inc. will become a launch partner in Microsoft’s publisher content marketplace. This is the company’s second AI deal following its earlier agreement with OpenAI last year.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;People Inc. CEO Neil Vogel described the new marketplace as “essentially a pay-per-use market where AI players directly can compensate publishers for use of their content on, sort of like an ‘a la carte’ basis.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He also praised Microsoft for being committed to paying for content to support its AI efforts, adding that Microsoft’s Copilot would be the first buyer for the marketplace. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s a very strong endorsement of us to be in the room with them and a very strong endorsement of the publishing marketplace and the value of content to make AI that is of high value,” Vogel said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The announcement of the Microsoft deal was shared during IAC’s earnings, alongside the news that Google Search’s AI Overviews has been hurting the publisher’s traffic. For the first time, People Inc. shared data with investors that showed how Google Search, which accounted for 54% of its traffic two years ago, had dropped to 24% of its traffic during the past quarter.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The agreement differs from the OpenAI deal, which Vogel characterized as more of an “all-you-can-eat” model, but said People Inc. was happy with either model. What matters to the company is that its work is “respected and paid for,” he said. The company didn’t share the specific deal terms, though. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;People Inc. has taken issue with the way AI companies have ingested media without paying to fuel their AI products and train their models. Recently, Vogel criticized Google, calling the tech giant a “bad actor” because it uses the same bot to crawl websites for its Google search engine and its AI features. Publishers can’t block the bot, as Google search still accounts for a large percentage of their traffic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, People Inc. uses the technology from web infrastructure provider Cloudflare to block other AI crawlers, prompting AI players to approach it with content deals. In September, Vogel attributed its decision to leverage Cloudflare’s tech as a way to push AI companies to the negotiating table, noting that its progress on deals was “much further along” after adopting the solution.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He reiterated those comments on today’s earnings call with investors, saying that blocking AI crawlers has been “very effective” and “brought almost everyone to the table.” Vogel suggested that more deals would be announced in time, as well. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;IAC reported that People Inc. grew its digital revenue 9% to $269 million in the quarter, driven by performance marketing and licensing, which saw 38% and 24% growth, respectively. It also noted its acquisition of a food-focused media publisher and influencer network Feedfeed.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/Neil-Vogel.jpg?resize=1200,593" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;People Inc., one of the largest media publishers in the U.S., has signed an AI licensing deal with Microsoft. The media giant (formerly known as Dotdash Meredith) made the announcement Tuesday as a part of parent company IAC’s third-quarter earnings. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Under the deal, People Inc. will become a launch partner in Microsoft’s publisher content marketplace. This is the company’s second AI deal following its earlier agreement with OpenAI last year.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;People Inc. CEO Neil Vogel described the new marketplace as “essentially a pay-per-use market where AI players directly can compensate publishers for use of their content on, sort of like an ‘a la carte’ basis.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He also praised Microsoft for being committed to paying for content to support its AI efforts, adding that Microsoft’s Copilot would be the first buyer for the marketplace. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s a very strong endorsement of us to be in the room with them and a very strong endorsement of the publishing marketplace and the value of content to make AI that is of high value,” Vogel said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The announcement of the Microsoft deal was shared during IAC’s earnings, alongside the news that Google Search’s AI Overviews has been hurting the publisher’s traffic. For the first time, People Inc. shared data with investors that showed how Google Search, which accounted for 54% of its traffic two years ago, had dropped to 24% of its traffic during the past quarter.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The agreement differs from the OpenAI deal, which Vogel characterized as more of an “all-you-can-eat” model, but said People Inc. was happy with either model. What matters to the company is that its work is “respected and paid for,” he said. The company didn’t share the specific deal terms, though. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;People Inc. has taken issue with the way AI companies have ingested media without paying to fuel their AI products and train their models. Recently, Vogel criticized Google, calling the tech giant a “bad actor” because it uses the same bot to crawl websites for its Google search engine and its AI features. Publishers can’t block the bot, as Google search still accounts for a large percentage of their traffic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, People Inc. uses the technology from web infrastructure provider Cloudflare to block other AI crawlers, prompting AI players to approach it with content deals. In September, Vogel attributed its decision to leverage Cloudflare’s tech as a way to push AI companies to the negotiating table, noting that its progress on deals was “much further along” after adopting the solution.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He reiterated those comments on today’s earnings call with investors, saying that blocking AI crawlers has been “very effective” and “brought almost everyone to the table.” Vogel suggested that more deals would be announced in time, as well. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;IAC reported that People Inc. grew its digital revenue 9% to $269 million in the quarter, driven by performance marketing and licensing, which saw 38% and 24% growth, respectively. It also noted its acquisition of a food-focused media publisher and influencer network Feedfeed.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/04/people-inc-forges-ai-licensing-deal-with-microsoft-as-google-traffic-drops/</guid><pubDate>Tue, 04 Nov 2025 22:30:28 +0000</pubDate></item><item><title>Google’s new hurricane model was breathtakingly good this season (AI – Ars Technica)</title><link>https://arstechnica.com/science/2025/11/googles-new-weather-model-impressed-during-its-first-hurricane-season/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Meanwhile, the US Global Forecasting System continues to get worse.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="640" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/1000x1000-640x640.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/1000x1000-1000x648.jpg" width="1000" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Hurricane Melissa, with a well-defined eye, can be seen in this satellite image from Monday morning.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          NOAA

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The Atlantic hurricane season is drawing to a close, and with the tropics quieting down for a winter slumber, the focus of forecasters turns to evaluating what worked and what did not during the preceding season.&lt;/p&gt;
&lt;p&gt;This year, the answers are clear. Although Google DeepMind’s Weather Lab only started releasing cyclone track forecasts in June, the company’s AI forecasting service performed exceptionally well. By contrast, the Global Forecast System model, operated by the US National Weather Service and is based on traditional physics and runs on powerful supercomputers, performed abysmally.&lt;/p&gt;
&lt;p&gt;The official data comparing forecast model performance will not be published by the National Hurricane Center for a few months. However, Brian McNoldy, a senior researcher at the University of Miami, has already done some preliminary number crunching.&lt;/p&gt;
&lt;p&gt;The results are stunning:&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2125648 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="720" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/bafkreia2ecszfb2iz2hican5os3ivjtnx3bkodspx4epbuqnyc3hjnum5m.jpg" width="1080" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      2025 Atlantic season hurricane model performance on track accuracy.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Brian McNoldy

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;A little help in reading the graphic is in order. This chart sums up the track forecast accuracy for all 13 named storms in the Atlantic Basin this season, measuring the mean position error at various hours in the forecast, from 0 to 120 hours (five days). On this chart, the lower a line is, the better a model has performed.&lt;/p&gt;
&lt;h2&gt;A new champion&lt;/h2&gt;
&lt;p&gt;The dotted black line shows the average forecast error for official forecasts from the 2022 to 2024 seasons. What jumps out is that the United States’ premier global model, the GFS (denoted here as AVNI), is by far the worst-performing model. Meanwhile, at the bottom of the chart, in maroon, is the Google DeepMind model (GDMI), performing the best at nearly all forecast hours.&lt;/p&gt;
&lt;p&gt;The difference in errors between the US GFS model and Google’s DeepMind is remarkable. At five days, the Google forecast had an error of 165 nautical miles compared to 360 nautical miles for the GFS model, more than &lt;em&gt;twice&lt;/em&gt; as bad. This is the kind of error that causes forecasters to completely disregard one model in favor of another.&lt;/p&gt;
&lt;p&gt;But there’s more. Google’s model was so good that it regularly beat the official forecast from the National Hurricane Center (OFCL), which is produced by human experts looking at a broad array of model data. The AI-based model also beat highly regarded “consensus models,” including the TVCN and HCCA products. For more information on various models and their designations, see here.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;This early model comparison does not include the “gold standard” traditional, physics-based model produced by the European Centre for Medium-Range Weather Forecasts. However, the ECMWF model typically does not do better on hurricane track forecasts than the hurricane center or consensus models, which weigh&amp;nbsp;several different model outputs. So it is unlikely to be superior to Google’s DeepMind.&lt;/p&gt;
&lt;h2&gt;This will change forecasting forever&lt;/h2&gt;
&lt;p&gt;It’s worth noting that DeepMind also did exceptionally well at intensity forecasting, which is the fluctuations in the strength of a hurricane. So in its first season, it nailed both hurricane tracks and intensity.&lt;/p&gt;
&lt;p&gt;As a forecaster who has relied on traditional physics-based models for a quarter of a century, it is difficult to say how gobsmacking these results are. Going forward, it is safe to say that we will rely heavily on Google and other AI weather models, which are likely to improve in the coming years, as they are relatively new and have room for improvement.&lt;/p&gt;
&lt;p&gt;“The beauty of DeepMind and other similar data-driven, AI-based weather models is how much more quickly they produce a forecast compared to their traditional physics-based counterparts that require some of the most expensive and advanced supercomputers in the world,” noted Michael Lowry, a hurricane specialist and author of the Eye on the Tropics newsletter, about the model performance. “Beyond that, these ‘smart’ models with their neural network architectures have the ability to learn from their mistakes and correct on-the-fly.”&lt;/p&gt;
&lt;h2&gt;What about the North American model?&lt;/h2&gt;
&lt;p&gt;As for the GFS model, it is difficult to explain why it performed so poorly this season. In the past, it has been, at worst, worthy of consideration in making a forecast. But this year, myself and other forecasters often disregarded it.&lt;/p&gt;
&lt;p&gt;“It’s not immediately clear why the GFS performed so poorly this hurricane season,” Lowry wrote. “Some have speculated the lapse in data collection from DOGE-related government cuts this year could have been a contributing factor, but presumably such a factor would have affected other global physics-based models as well, not just the American GFS.”&lt;/p&gt;
&lt;p&gt;With the US government in shutdown mode, we probably cannot expect many answers soon. But it seems clear that the massive upgrade of the model’s dynamic core, which began in 2019, has largely been a failure. If the GFS was a little bit behind some competitors a decade ago, it is now fading further and faster.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Meanwhile, the US Global Forecasting System continues to get worse.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="640" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/1000x1000-640x640.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/1000x1000-1000x648.jpg" width="1000" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Hurricane Melissa, with a well-defined eye, can be seen in this satellite image from Monday morning.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          NOAA

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The Atlantic hurricane season is drawing to a close, and with the tropics quieting down for a winter slumber, the focus of forecasters turns to evaluating what worked and what did not during the preceding season.&lt;/p&gt;
&lt;p&gt;This year, the answers are clear. Although Google DeepMind’s Weather Lab only started releasing cyclone track forecasts in June, the company’s AI forecasting service performed exceptionally well. By contrast, the Global Forecast System model, operated by the US National Weather Service and is based on traditional physics and runs on powerful supercomputers, performed abysmally.&lt;/p&gt;
&lt;p&gt;The official data comparing forecast model performance will not be published by the National Hurricane Center for a few months. However, Brian McNoldy, a senior researcher at the University of Miami, has already done some preliminary number crunching.&lt;/p&gt;
&lt;p&gt;The results are stunning:&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2125648 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="720" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/bafkreia2ecszfb2iz2hican5os3ivjtnx3bkodspx4epbuqnyc3hjnum5m.jpg" width="1080" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      2025 Atlantic season hurricane model performance on track accuracy.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Brian McNoldy

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;A little help in reading the graphic is in order. This chart sums up the track forecast accuracy for all 13 named storms in the Atlantic Basin this season, measuring the mean position error at various hours in the forecast, from 0 to 120 hours (five days). On this chart, the lower a line is, the better a model has performed.&lt;/p&gt;
&lt;h2&gt;A new champion&lt;/h2&gt;
&lt;p&gt;The dotted black line shows the average forecast error for official forecasts from the 2022 to 2024 seasons. What jumps out is that the United States’ premier global model, the GFS (denoted here as AVNI), is by far the worst-performing model. Meanwhile, at the bottom of the chart, in maroon, is the Google DeepMind model (GDMI), performing the best at nearly all forecast hours.&lt;/p&gt;
&lt;p&gt;The difference in errors between the US GFS model and Google’s DeepMind is remarkable. At five days, the Google forecast had an error of 165 nautical miles compared to 360 nautical miles for the GFS model, more than &lt;em&gt;twice&lt;/em&gt; as bad. This is the kind of error that causes forecasters to completely disregard one model in favor of another.&lt;/p&gt;
&lt;p&gt;But there’s more. Google’s model was so good that it regularly beat the official forecast from the National Hurricane Center (OFCL), which is produced by human experts looking at a broad array of model data. The AI-based model also beat highly regarded “consensus models,” including the TVCN and HCCA products. For more information on various models and their designations, see here.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;This early model comparison does not include the “gold standard” traditional, physics-based model produced by the European Centre for Medium-Range Weather Forecasts. However, the ECMWF model typically does not do better on hurricane track forecasts than the hurricane center or consensus models, which weigh&amp;nbsp;several different model outputs. So it is unlikely to be superior to Google’s DeepMind.&lt;/p&gt;
&lt;h2&gt;This will change forecasting forever&lt;/h2&gt;
&lt;p&gt;It’s worth noting that DeepMind also did exceptionally well at intensity forecasting, which is the fluctuations in the strength of a hurricane. So in its first season, it nailed both hurricane tracks and intensity.&lt;/p&gt;
&lt;p&gt;As a forecaster who has relied on traditional physics-based models for a quarter of a century, it is difficult to say how gobsmacking these results are. Going forward, it is safe to say that we will rely heavily on Google and other AI weather models, which are likely to improve in the coming years, as they are relatively new and have room for improvement.&lt;/p&gt;
&lt;p&gt;“The beauty of DeepMind and other similar data-driven, AI-based weather models is how much more quickly they produce a forecast compared to their traditional physics-based counterparts that require some of the most expensive and advanced supercomputers in the world,” noted Michael Lowry, a hurricane specialist and author of the Eye on the Tropics newsletter, about the model performance. “Beyond that, these ‘smart’ models with their neural network architectures have the ability to learn from their mistakes and correct on-the-fly.”&lt;/p&gt;
&lt;h2&gt;What about the North American model?&lt;/h2&gt;
&lt;p&gt;As for the GFS model, it is difficult to explain why it performed so poorly this season. In the past, it has been, at worst, worthy of consideration in making a forecast. But this year, myself and other forecasters often disregarded it.&lt;/p&gt;
&lt;p&gt;“It’s not immediately clear why the GFS performed so poorly this hurricane season,” Lowry wrote. “Some have speculated the lapse in data collection from DOGE-related government cuts this year could have been a contributing factor, but presumably such a factor would have affected other global physics-based models as well, not just the American GFS.”&lt;/p&gt;
&lt;p&gt;With the US government in shutdown mode, we probably cannot expect many answers soon. But it seems clear that the massive upgrade of the model’s dynamic core, which began in 2019, has largely been a failure. If the GFS was a little bit behind some competitors a decade ago, it is now fading further and faster.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/science/2025/11/googles-new-weather-model-impressed-during-its-first-hurricane-season/</guid><pubDate>Tue, 04 Nov 2025 22:50:29 +0000</pubDate></item><item><title>Amazon sends legal threats to Perplexity over agentic browsing (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/04/amazon-sends-legal-threats-to-perplexity-over-agentic-browsing/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-2181996346.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon has told Perplexity to get its agentic browser out of its online store, the companies both confirmed publicly on Tuesday. After warning Perplexity multiple times that Comet, its AI-powered shopping assistant, was violating Amazon’s terms of service by not identifying itself as an agent, the e-commerce giant sent the AI search engine startup a sternly worded cease-and-desist letter, Perplexity wrote in a blog post titled “Bullying is not innovation.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This week, Perplexity received an aggressive legal threat from Amazon, demanding we prohibit Comet users from using their AI assistants on Amazon. This is Amazon’s first legal salvo against an AI company, and it is a threat to all internet users,” Perplexity lamented in the blog post.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Perplexity’s argument is that, since its agent is acting on behalf of a human user’s direction, the agent automatically has the “same permissions” as the human user. The implication is that it doesn’t have to identify itself as an agent.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon’s response points out that other third-party agents working at the behest of human users do identify themselves. “It is how others operate, including food delivery apps and the restaurants they take orders for, delivery service apps and the stores they shop from, and online travel agencies and the airlines they book tickets with for customers,” Amazon’s statement explains.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If Amazon is to be believed, then Perplexity could simply identify its agent and start shopping. Of course, the risk is that Amazon, which has its own shopping bot called Rufus, could also block Comet — or any other third-party agentic shopper — from its site.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon suggests as much as its statement, which also says, “We think it’s fairly straightforward that third-party applications that offer to make purchases on behalf of customers from other businesses should operate openly and respect service provider decisions whether or not to participate.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Perplexity claims that Amazon would block the shopping bot because Amazon wants to sell advertising and product placements. Unlike human shoppers, a bot tasked with buying a new laundry basket presumably wouldn’t find itself buying a more expensive one, or getting lured into buying the latest Brandon Sanderson novel and a new set of earphones (on sale!).&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;If all of this sounds a bit familiar, that’s because it is. A few months ago, Cloudflare published research accusing Perplexity of scraping websites while specifically defying requests from websites blocking AI bots. Interestingly, many people came to Perplexity’s defense that time, because this wasn’t a clear-cut case of web crawler bad behavior. Cloudflare documented how the AI was accessing a specific public website when its user asked about that specific website.&amp;nbsp;Perplexity fans argued that this is exactly what every human-operated web browser does.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On the other hand, Perplexity was using some questionable methods to do that accessing when a website opted out of bots, like hiding its identity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As TechCrunch reported at the time, the Cloudflare incident foreshadowed the challenges to come if the agentic world materializes as Silicon Valley predicts it will. If consumers and companies outsource their shopping, travel bookings, and restaurant reservations to bots, will it be in the best interest of websites to block bots entirely? How will they allow and work with them?&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Perplexity may be right in that Amazon is setting a precedent. As the 800-pound gorilla in e-commerce, it is clearly saying that the way this should work is for an agent to identify itself and let the website decide.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-2181996346.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon has told Perplexity to get its agentic browser out of its online store, the companies both confirmed publicly on Tuesday. After warning Perplexity multiple times that Comet, its AI-powered shopping assistant, was violating Amazon’s terms of service by not identifying itself as an agent, the e-commerce giant sent the AI search engine startup a sternly worded cease-and-desist letter, Perplexity wrote in a blog post titled “Bullying is not innovation.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This week, Perplexity received an aggressive legal threat from Amazon, demanding we prohibit Comet users from using their AI assistants on Amazon. This is Amazon’s first legal salvo against an AI company, and it is a threat to all internet users,” Perplexity lamented in the blog post.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Perplexity’s argument is that, since its agent is acting on behalf of a human user’s direction, the agent automatically has the “same permissions” as the human user. The implication is that it doesn’t have to identify itself as an agent.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon’s response points out that other third-party agents working at the behest of human users do identify themselves. “It is how others operate, including food delivery apps and the restaurants they take orders for, delivery service apps and the stores they shop from, and online travel agencies and the airlines they book tickets with for customers,” Amazon’s statement explains.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If Amazon is to be believed, then Perplexity could simply identify its agent and start shopping. Of course, the risk is that Amazon, which has its own shopping bot called Rufus, could also block Comet — or any other third-party agentic shopper — from its site.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon suggests as much as its statement, which also says, “We think it’s fairly straightforward that third-party applications that offer to make purchases on behalf of customers from other businesses should operate openly and respect service provider decisions whether or not to participate.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Perplexity claims that Amazon would block the shopping bot because Amazon wants to sell advertising and product placements. Unlike human shoppers, a bot tasked with buying a new laundry basket presumably wouldn’t find itself buying a more expensive one, or getting lured into buying the latest Brandon Sanderson novel and a new set of earphones (on sale!).&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;If all of this sounds a bit familiar, that’s because it is. A few months ago, Cloudflare published research accusing Perplexity of scraping websites while specifically defying requests from websites blocking AI bots. Interestingly, many people came to Perplexity’s defense that time, because this wasn’t a clear-cut case of web crawler bad behavior. Cloudflare documented how the AI was accessing a specific public website when its user asked about that specific website.&amp;nbsp;Perplexity fans argued that this is exactly what every human-operated web browser does.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On the other hand, Perplexity was using some questionable methods to do that accessing when a website opted out of bots, like hiding its identity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As TechCrunch reported at the time, the Cloudflare incident foreshadowed the challenges to come if the agentic world materializes as Silicon Valley predicts it will. If consumers and companies outsource their shopping, travel bookings, and restaurant reservations to bots, will it be in the best interest of websites to block bots entirely? How will they allow and work with them?&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Perplexity may be right in that Amazon is setting a precedent. As the 800-pound gorilla in e-commerce, it is clearly saying that the way this should work is for an agent to identify itself and let the website decide.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/04/amazon-sends-legal-threats-to-perplexity-over-agentic-browsing/</guid><pubDate>Tue, 04 Nov 2025 23:05:04 +0000</pubDate></item><item><title>[NEW] NVIDIA, Qualcomm join U.S., Indian VCs to help build India’s next deep tech startups (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/04/nvidia-qualcomm-join-u-s-indian-vcs-to-help-build-indias-next-deep-tech-startups/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;NVIDIA and Qualcomm Ventures have joined a growing coalition of U.S. and Indian investors backing India’s deep tech startups. The group launched in September with more than $1 billion in commitments, timing that aligns with India’s new ₹1 trillion (around $12 billion) research and development initiative.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;NVIDIA has joined the coalition as a strategic technical advisor, without any financial commitments, while Qualcomm Ventures has come on board alongside six Indian venture firms, bringing additional capital commitments totaling more than $850 million.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;India is home to more than 180,000 startups and over 120 unicorns. In its early years, much of the ecosystem closely mirrored Western business models before evolving into SaaS companies that serve global clients, especially those in the U.S. In recent years, however, India’s focus has shifted to building ventures that tackle harder, infrastructure-scale problems — from launching satellites and electrifying transportation to designing semiconductors. The Indian government has sought to accelerate this shift as major economies race to secure technological sovereignty. Yet capital for such ventures remains scarce, as they require a longer gestation period than traditional sectors and most VCs favor proven, lower-risk models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In September, Silicon Valley- and India-based Celesta Capital spearheaded the launch of the India Deep Tech Alliance (IDTA) to bridge that gap, bringing together seven major U.S. and Indian investors — Accel, Blume Ventures, Premji Invest, Gaja Capital, Ideaspring Capital, Tenacity Ventures, and Venture Catalysts. The latest addition includes Indian venture firms Activate AI, Chiratae Ventures, InfoEdge Ventures, Kalaari Capital, Singularity Holdings, and YourNest Venture Capital.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The coalition aims to invest capital and provide mentorship and network access to Indian deep-tech startups over the next five to ten years. It also plans to collaborate with the Indian government on its policy initiatives, including the recently introduced Research, Development and Innovation (RDI) scheme.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s a coalition of the willing, wanting to support the development of the Indian deep tech ecosystem,” Sriram Viswanathan, founding managing partner of Celesta Capital and founding executive council member of the IDTA, said in an interview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Approved by the Indian cabinet earlier this year and rolled out by Prime Minister Narendra Modi this week, the ₹1 trillion RDI scheme will fund projects in areas such as energy security and transition, quantum computing, robotics, space tech, biotech, and AI through long-term loans, equity infusions, and allocations to deep-tech funds of funds. The venture firms participating in the alliance plan to leverage the initiative to back Indian-domiciled deep-tech startups.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“This is, in a way, the most seminal moment where the Indian government’s action will drive creation and the formation of many of these deep tech companies and will be supported by a number of VCs in India that are really looking at developing this ecosystem,” Viswanathan told TechCrunch. “There is a turning point in the Indian entrepreneurial ecosystem in favor of deep tech, and that’s what we’re all excited about.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-3065055" height="1099" src="https://techcrunch.com/wp-content/uploads/2025/11/india-deep-tech-alliance.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;IDTA&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The alliance has brought in NVIDIA to provide strategic and technical guidance to its members and emerging startups. The U.S. chipmaker — whose market value has surged amid the global boom in AI — will advise on best practices for integrating NVIDIA’s AI and accelerated computing platforms, offer technical talks and training through the NVIDIA Deep Learning Institute, and contribute to policy dialogues between industry and the government to advance India’s deep-tech capabilities, the alliance said in a statement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Although NVIDIA will not participate financially, Vishal Dhupar, NVIDIA’s managing director for South Asia, said the company will share technical insights and scalable computing resources with Indian startups in the coalition.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“NVIDIA’s support is a pretty significant validation of the ecosystem, and them joining the IDTA is an endorsement of our collective objective that there is an opportunity for India to start seeing a burgeoning growth of this ecosystem,” Viswanathan told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unlike NVIDIA, Qualcomm is joining the alliance with an investment focus. The San Diego–based chipmaker made its first India investment in 2008, with early bets including Google Maps rival MapmyIndia, which went public in late 2021. Qualcomm and Celesta also backed Indian drone maker IdeaForge, which has been a publicly listed company since 2023.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, Qualcomm’s participation will extend beyond capital, said Rama Bethmangalkar, India managing director at Qualcomm Ventures. The firm plans to help startups connect with its portfolio companies, partner networks, and internal teams within Qualcomm, he told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If you are like minded and other VCs have allocated certain portion of their resources, dollars, time, and network, it helps each other and then collectively to work with the government, to be aligned with what the government is thinking on certain areas, whether it is quantum, semiconductors, AI, or emerging technologies, it is very important to be part of that group,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That said, the success of the IDTA remains to be seen. Viswanathan described the alliance as a “loose coalition of the willing,” noting that participating investors continue to run their own programs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re collaborating to share knowledge, to share deal flow, and all of that,” he said when asked about the progress since the alliance’s launch in September.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It is also unclear how much of the capital each participant will contribute.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re just collectively estimating what the total commitment is to this ecosystem,” Viswanathan said. “This alliance is not a fund. There’s no obligation, no allocation, if you will, of any deal. If Rama finds a deal, he will do it. If Rama finds it appropriate to bring in other investors, he’ll share the deal with other investors that he thinks are relevant for that investment.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;India’s deep-tech funding rose 78% year-over-year to $1.6 billion in 2024, according to a report by IT industry body Nasscom and global consulting firm Zinnov released in April. While the growth is promising, the capital raised still trails far behind that in developed markets, especially the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The alliance may help increase that figure, but more importantly, it is expected to draw global attention — and, in turn, more investors and corporate venture funds — to India’s startup ecosystem.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“What we need are role models to begin with,” said Bethmangalkar. “People are going to jump in. Entrepreneurs are going to get the confidence capital… In ten years, you’ll start seeing these as the companies listed on the main boards of our exchanges — deeply science- and tech-oriented firms.”&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;NVIDIA and Qualcomm Ventures have joined a growing coalition of U.S. and Indian investors backing India’s deep tech startups. The group launched in September with more than $1 billion in commitments, timing that aligns with India’s new ₹1 trillion (around $12 billion) research and development initiative.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;NVIDIA has joined the coalition as a strategic technical advisor, without any financial commitments, while Qualcomm Ventures has come on board alongside six Indian venture firms, bringing additional capital commitments totaling more than $850 million.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;India is home to more than 180,000 startups and over 120 unicorns. In its early years, much of the ecosystem closely mirrored Western business models before evolving into SaaS companies that serve global clients, especially those in the U.S. In recent years, however, India’s focus has shifted to building ventures that tackle harder, infrastructure-scale problems — from launching satellites and electrifying transportation to designing semiconductors. The Indian government has sought to accelerate this shift as major economies race to secure technological sovereignty. Yet capital for such ventures remains scarce, as they require a longer gestation period than traditional sectors and most VCs favor proven, lower-risk models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In September, Silicon Valley- and India-based Celesta Capital spearheaded the launch of the India Deep Tech Alliance (IDTA) to bridge that gap, bringing together seven major U.S. and Indian investors — Accel, Blume Ventures, Premji Invest, Gaja Capital, Ideaspring Capital, Tenacity Ventures, and Venture Catalysts. The latest addition includes Indian venture firms Activate AI, Chiratae Ventures, InfoEdge Ventures, Kalaari Capital, Singularity Holdings, and YourNest Venture Capital.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The coalition aims to invest capital and provide mentorship and network access to Indian deep-tech startups over the next five to ten years. It also plans to collaborate with the Indian government on its policy initiatives, including the recently introduced Research, Development and Innovation (RDI) scheme.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s a coalition of the willing, wanting to support the development of the Indian deep tech ecosystem,” Sriram Viswanathan, founding managing partner of Celesta Capital and founding executive council member of the IDTA, said in an interview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Approved by the Indian cabinet earlier this year and rolled out by Prime Minister Narendra Modi this week, the ₹1 trillion RDI scheme will fund projects in areas such as energy security and transition, quantum computing, robotics, space tech, biotech, and AI through long-term loans, equity infusions, and allocations to deep-tech funds of funds. The venture firms participating in the alliance plan to leverage the initiative to back Indian-domiciled deep-tech startups.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“This is, in a way, the most seminal moment where the Indian government’s action will drive creation and the formation of many of these deep tech companies and will be supported by a number of VCs in India that are really looking at developing this ecosystem,” Viswanathan told TechCrunch. “There is a turning point in the Indian entrepreneurial ecosystem in favor of deep tech, and that’s what we’re all excited about.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-3065055" height="1099" src="https://techcrunch.com/wp-content/uploads/2025/11/india-deep-tech-alliance.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;IDTA&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The alliance has brought in NVIDIA to provide strategic and technical guidance to its members and emerging startups. The U.S. chipmaker — whose market value has surged amid the global boom in AI — will advise on best practices for integrating NVIDIA’s AI and accelerated computing platforms, offer technical talks and training through the NVIDIA Deep Learning Institute, and contribute to policy dialogues between industry and the government to advance India’s deep-tech capabilities, the alliance said in a statement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Although NVIDIA will not participate financially, Vishal Dhupar, NVIDIA’s managing director for South Asia, said the company will share technical insights and scalable computing resources with Indian startups in the coalition.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“NVIDIA’s support is a pretty significant validation of the ecosystem, and them joining the IDTA is an endorsement of our collective objective that there is an opportunity for India to start seeing a burgeoning growth of this ecosystem,” Viswanathan told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unlike NVIDIA, Qualcomm is joining the alliance with an investment focus. The San Diego–based chipmaker made its first India investment in 2008, with early bets including Google Maps rival MapmyIndia, which went public in late 2021. Qualcomm and Celesta also backed Indian drone maker IdeaForge, which has been a publicly listed company since 2023.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, Qualcomm’s participation will extend beyond capital, said Rama Bethmangalkar, India managing director at Qualcomm Ventures. The firm plans to help startups connect with its portfolio companies, partner networks, and internal teams within Qualcomm, he told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If you are like minded and other VCs have allocated certain portion of their resources, dollars, time, and network, it helps each other and then collectively to work with the government, to be aligned with what the government is thinking on certain areas, whether it is quantum, semiconductors, AI, or emerging technologies, it is very important to be part of that group,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That said, the success of the IDTA remains to be seen. Viswanathan described the alliance as a “loose coalition of the willing,” noting that participating investors continue to run their own programs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re collaborating to share knowledge, to share deal flow, and all of that,” he said when asked about the progress since the alliance’s launch in September.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It is also unclear how much of the capital each participant will contribute.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re just collectively estimating what the total commitment is to this ecosystem,” Viswanathan said. “This alliance is not a fund. There’s no obligation, no allocation, if you will, of any deal. If Rama finds a deal, he will do it. If Rama finds it appropriate to bring in other investors, he’ll share the deal with other investors that he thinks are relevant for that investment.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;India’s deep-tech funding rose 78% year-over-year to $1.6 billion in 2024, according to a report by IT industry body Nasscom and global consulting firm Zinnov released in April. While the growth is promising, the capital raised still trails far behind that in developed markets, especially the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The alliance may help increase that figure, but more importantly, it is expected to draw global attention — and, in turn, more investors and corporate venture funds — to India’s startup ecosystem.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“What we need are role models to begin with,” said Bethmangalkar. “People are going to jump in. Entrepreneurs are going to get the confidence capital… In ten years, you’ll start seeing these as the companies listed on the main boards of our exchanges — deeply science- and tech-oriented firms.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/04/nvidia-qualcomm-join-u-s-indian-vcs-to-help-build-indias-next-deep-tech-startups/</guid><pubDate>Wed, 05 Nov 2025 03:30:00 +0000</pubDate></item><item><title>[NEW] Goldman Sachs doubles down on MoEngage in new round to fuel global expansion (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/04/goldman-sachs-doubles-down-on-moengage-in-100m-round-to-fuel-global-expansion/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;MoEngage, a customer engagement platform that works with consumer brands across 75 countries, says it has raised new funding led by its existing investor, Goldman Sachs Alternatives, to ramp up global growth and infuse more AI into its platform.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;All told, $100 million in shares just traded hands, split roughly 60% primary and 40% secondary, as part of MoEngage’s Series F round. The round marks the entry of Indian venture firm A91 Partners as a new investor. According to MoEngage, it has now raised $250 million in funding altogether.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As consumer brands increasingly rely on digital channels to reach customers, competition for attention has intensified. That’s pushed companies to use the customer data they already have to deliver more personalized marketing. While established marketing platforms continue to serve this space, brands are now seeking AI-driven tools that can automate decision-making and reduce manual labor. MoEngage positions itself in this segment with its Merlin AI suite, which helps marketing and product teams launch campaigns faster and improve targeting efficiency.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We help B2C brands engage more effectively with their customers by leveraging the first-party data they already have,” Raviteja Dodda (pictured above), co-founder and CEO of MoEngage, said in an interview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The 11-year-old startup spent its first seven years focusing largely on India and Southeast Asia. Over the past four years, it has expanded its reach to new markets, particularly North America, which now contributes more than 30% of its revenue, Dodda told TechCrunch. About 25% of the business comes from Europe and the Middle East, and the remaining 45% from India and Southeast Asia.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Goldman Sachs’ backing in the latest funding will help further bolster MoEngage’s global presence. The investment bank also co-led the startup’s Series E round of $77 million along with B Capital in June 2022.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The current investors know the most about the company, in terms of how the company performs, and they know everything good and bad,” said Dodda. “[Goldman Sachs] leading the round is a strong validation of our fundamentals.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Over the past two to three years, MoEngage has invested heavily in generative AI and decisioning AI capabilities. These efforts are reflected in its Merlin AI suite, which Dodda said includes a range of AI agents built for marketing use cases.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some of these agents act like copywriters, helping consumer brands draft marketing messages, create multiple variants of a campaign, or generate text in natural language along with relevant images. The suite also includes decisioning AI tools that help brands determine which customers should receive a particular message or offer, on which channel, and at what time, Dodda said.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-3064885" height="1177" src="https://techcrunch.com/wp-content/uploads/2025/11/moengage-merlin-ai.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;MoEngage’s Merlin AI suite&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;MoEngage&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;MoEngage currently serves over 1,350 consumer brands worldwide, including SoundCloud, McAfee, Kayak, Domino’s, Deutsche Telekom, and Travelodge, as well as prominent Indian household names such as Swiggy, Flipkart, Ola, Airtel, and Tata. About 60% of the company’s business comes from traditional enterprises, while the remaining 40% is from internet-focused firms. The platform also works with more than 25 global banks and several large insurers, including JPMorgan Chase, Citibank, and India’s largest insurer, Life Insurance Corporation (LIC).&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Some of these brands previously used marketing platforms from incumbents such as Adobe, Oracle, and Salesforce. MoEngage has since won over more than 300 of them, helping drive growth in North America and the EMEA regions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In one instance, SoundCloud migrated over 120 million users to MoEngage within 12 weeks, utilizing AI-driven insights to accelerate product launches and enhance retention among its paid users, said Hope Barrett, senior director of martech at SoundCloud.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Several of MoEngage’s customers also relied on multiple point solutions to handle specific tasks. The company helped consolidate those tools into a unified platform to cut costs and streamline marketing operations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If you look at all of our brands, whether it’s a bank or an e-commerce company, they leverage MoEngage to unify all their customer data from all the touchpoints. It could be their offline stores, website, mobile app [or other channels],” Dodda told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Without disclosing exact figures, Dodda said MoEngage grew about 40% year-over-year last year and aims to maintain a 35% compound annual growth rate (CAGR) over the next three years. The company also expects to become adjusted EBITDA-positive on a quarterly basis by the end of the current fiscal year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;MoEngage sees companies such as Braze and CleverTap, as well as legacy marketing clouds by Adobe, Oracle, and Salesforce, among its key competitors. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup has about 800 employees across its 15 offices worldwide. It plans to expand its workforce, particularly in North America and Europe, by scaling its customer success, support, sales, and marketing teams to deepen its presence in those markets. MoEngage also intends to build additional AI capabilities and hire more talent to support that effort.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;MoEngage plans to become IPO-ready within the next couple of years, Dodda told TechCrunch, without sharing a specific timeline for going public.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We see an opportunity to build a multi-billion dollar revenue company in our space,” he stated.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;MoEngage, a customer engagement platform that works with consumer brands across 75 countries, says it has raised new funding led by its existing investor, Goldman Sachs Alternatives, to ramp up global growth and infuse more AI into its platform.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;All told, $100 million in shares just traded hands, split roughly 60% primary and 40% secondary, as part of MoEngage’s Series F round. The round marks the entry of Indian venture firm A91 Partners as a new investor. According to MoEngage, it has now raised $250 million in funding altogether.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As consumer brands increasingly rely on digital channels to reach customers, competition for attention has intensified. That’s pushed companies to use the customer data they already have to deliver more personalized marketing. While established marketing platforms continue to serve this space, brands are now seeking AI-driven tools that can automate decision-making and reduce manual labor. MoEngage positions itself in this segment with its Merlin AI suite, which helps marketing and product teams launch campaigns faster and improve targeting efficiency.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We help B2C brands engage more effectively with their customers by leveraging the first-party data they already have,” Raviteja Dodda (pictured above), co-founder and CEO of MoEngage, said in an interview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The 11-year-old startup spent its first seven years focusing largely on India and Southeast Asia. Over the past four years, it has expanded its reach to new markets, particularly North America, which now contributes more than 30% of its revenue, Dodda told TechCrunch. About 25% of the business comes from Europe and the Middle East, and the remaining 45% from India and Southeast Asia.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Goldman Sachs’ backing in the latest funding will help further bolster MoEngage’s global presence. The investment bank also co-led the startup’s Series E round of $77 million along with B Capital in June 2022.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The current investors know the most about the company, in terms of how the company performs, and they know everything good and bad,” said Dodda. “[Goldman Sachs] leading the round is a strong validation of our fundamentals.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Over the past two to three years, MoEngage has invested heavily in generative AI and decisioning AI capabilities. These efforts are reflected in its Merlin AI suite, which Dodda said includes a range of AI agents built for marketing use cases.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some of these agents act like copywriters, helping consumer brands draft marketing messages, create multiple variants of a campaign, or generate text in natural language along with relevant images. The suite also includes decisioning AI tools that help brands determine which customers should receive a particular message or offer, on which channel, and at what time, Dodda said.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-3064885" height="1177" src="https://techcrunch.com/wp-content/uploads/2025/11/moengage-merlin-ai.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;MoEngage’s Merlin AI suite&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;MoEngage&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;MoEngage currently serves over 1,350 consumer brands worldwide, including SoundCloud, McAfee, Kayak, Domino’s, Deutsche Telekom, and Travelodge, as well as prominent Indian household names such as Swiggy, Flipkart, Ola, Airtel, and Tata. About 60% of the company’s business comes from traditional enterprises, while the remaining 40% is from internet-focused firms. The platform also works with more than 25 global banks and several large insurers, including JPMorgan Chase, Citibank, and India’s largest insurer, Life Insurance Corporation (LIC).&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Some of these brands previously used marketing platforms from incumbents such as Adobe, Oracle, and Salesforce. MoEngage has since won over more than 300 of them, helping drive growth in North America and the EMEA regions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In one instance, SoundCloud migrated over 120 million users to MoEngage within 12 weeks, utilizing AI-driven insights to accelerate product launches and enhance retention among its paid users, said Hope Barrett, senior director of martech at SoundCloud.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Several of MoEngage’s customers also relied on multiple point solutions to handle specific tasks. The company helped consolidate those tools into a unified platform to cut costs and streamline marketing operations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If you look at all of our brands, whether it’s a bank or an e-commerce company, they leverage MoEngage to unify all their customer data from all the touchpoints. It could be their offline stores, website, mobile app [or other channels],” Dodda told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Without disclosing exact figures, Dodda said MoEngage grew about 40% year-over-year last year and aims to maintain a 35% compound annual growth rate (CAGR) over the next three years. The company also expects to become adjusted EBITDA-positive on a quarterly basis by the end of the current fiscal year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;MoEngage sees companies such as Braze and CleverTap, as well as legacy marketing clouds by Adobe, Oracle, and Salesforce, among its key competitors. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup has about 800 employees across its 15 offices worldwide. It plans to expand its workforce, particularly in North America and Europe, by scaling its customer success, support, sales, and marketing teams to deepen its presence in those markets. MoEngage also intends to build additional AI capabilities and hire more talent to support that effort.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;MoEngage plans to become IPO-ready within the next couple of years, Dodda told TechCrunch, without sharing a specific timeline for going public.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We see an opportunity to build a multi-billion dollar revenue company in our space,” he stated.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/04/goldman-sachs-doubles-down-on-moengage-in-100m-round-to-fuel-global-expansion/</guid><pubDate>Wed, 05 Nov 2025 05:20:00 +0000</pubDate></item></channel></rss>